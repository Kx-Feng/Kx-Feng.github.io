[{"id":0,"href":"/docs/example/","title":"Arxiv Paper","section":"Docs","content":"\rArxiv Paper\r#\r2024-03-06\nTowards Efficient and Effective Unlearning of Large Language Models for Recommendation\nHangyu Wang Jianghao Lin Bo Chen Yang Yang Ruiming Tang Weinan Zhang Yong Yu\nabstract\rabstract: The significant advancements in large language models (LLMs) give rise to apromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).The efficacy of LLMRec arises from the open-world knowledge and reasoningcapabilities inherent in LLMs. LLMRec acquires the recommendation capabilitiesthrough instruction tuning based on user interaction data. However, in order toprotect user privacy and optimize utility, it is also crucial for LLMRec tointentionally forget specific user data, which is generally referred to asrecommendation unlearning. In the era of LLMs, recommendation unlearning posesnew challenges for LLMRec in terms of \\textit{inefficiency} and\\textit{ineffectiveness}. Existing unlearning methods require updating billionsof parameters in LLMRec, which is costly and time-consuming. Besides, theyalways impact the model utility during the unlearning process. To this end, wepropose \\textbf{E2URec}, the first \\underline{E}fficient and\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Ourproposed E2URec enhances the unlearning efficiency by updating only a fewadditional LoRA parameters, and improves the unlearning effectiveness byemploying a teacher-student framework, where we maintain multiple teachernetworks to guide the unlearning process. Extensive experiments show thatE2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affectingrecommendation performance. The source code is at\\url{https://github.com/justarter/E2URec}.\r2024-03-04\nSciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis\nHengxing Cai Xiaochen Cai Junhan Chang Sihang Li Lin Yao Changxin Wang Zhifeng Gao Yongge Li Mujie Lin Shuwen Yang Jiankun Wang Yuqi Yin Yaqi Li Linfeng Zhang Guolin Ke\nabstract\rabstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionizednatural language understanding and generation, igniting a surge of interest inleveraging these technologies for the nuanced field of scientific literatureanalysis. Existing benchmarks, however, inadequately evaluate the proficiencyof LLMs in the scientific domain, especially in scenarios involving complexcomprehension and multimodal data. In response, we introduced SciAssess, abenchmark tailored for the in-depth analysis of scientific literature, craftedto provide a thorough assessment of LLMs\u0026rsquo; efficacy. SciAssess focuses onevaluating LLMs\u0026rsquo; abilities in memorization, comprehension, and analysis withinscientific contexts. It includes representative tasks from diverse scientificfields, such as general chemistry, organic materials, and alloy materials. Andrigorous quality control measures ensure its reliability in terms ofcorrectness, anonymization, and copyright compliance. SciAssess evaluatesleading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying theirstrengths and areas for improvement and supporting the ongoing development ofLLM applications in scientific literature analysis. SciAssess and its resourcesare made available at https://sci-assess.github.io, offering a valuable toolfor advancing LLM capabilities in scientific literature analysis.\r2024-03-01\nTalkin\u0026rsquo; \u0026lsquo;Bout AI Generation: Copyright and the Generative-AI Supply Chain\nKatherine Lee A. Feder Cooper James Grimmelmann\nabstract\rabstract: \u0026ldquo;Does generative AI infringe copyright?\u0026rdquo; is an urgent question. It is also adifficult question, for two reasons. First, \u0026ldquo;generative AI\u0026rdquo; is not just oneproduct from one company. It is a catch-all name for a massive ecosystem ofloosely related technologies, including conversational text chatbots likeChatGPT, image generators like Midjourney and DALL-E, coding assistants likeGitHub Copilot, and systems that compose music and create videos. These systemsbehave differently and raise different legal issues. The second problem is thatcopyright law is notoriously complicated, and generative-AI systems manage totouch on a great many corners of it: authorship, similarity, direct andindirect liability, fair use, and licensing, among much else. These issuescannot be analyzed in isolation, because there are connections everywhere. In this Article, we aim to bring order to the chaos. To do so, we introducethe generative-AI supply chain: an interconnected set of stages that transformtraining data (millions of pictures of cats) into generations (a new,potentially never-seen-before picture of a cat that has never existed).Breaking down generative AI into these constituent stages reveals all of theplaces at which companies and users make choices that have copyrightconsequences. It enables us to trace the effects of upstream technical designson downstream uses, and to assess who in these complicated sociotechnicalsystems bears responsibility for infringement when it happens. Because weengage so closely with the technology of generative AI, we are able to shedmore light on the copyright questions. We do not give definitive answers as towho should and should not be held liable. Instead, we identify the keydecisions that courts will need to make as they grapple with these issues, andpoint out the consequences that would likely flow from different liabilityregimes.\r2024-02-29\nPRSA: Prompt Reverse Stealing Attacks against Large Language Models\nYong Yang Xuhong Zhang Yi Jiang Xi Chen Haoyu Wang Shouling Ji Zonghui Wang\nabstract\rabstract: Prompt, recognized as crucial intellectual property, enables large languagemodels (LLMs) to perform specific tasks without the need of fine-tuning,underscoring their escalating importance. With the rise of prompt-basedservices, such as prompt marketplaces and LLM applications, providers oftendisplay prompts\u0026rsquo; capabilities through input-output examples to attract users.However, this paradigm raises a pivotal security concern: does the exposure ofinput-output pairs pose the risk of potential prompt leakage, infringing on theintellectual property rights of the developers? To our knowledge, this problemstill has not been comprehensively explored yet. To remedy this gap, in thispaper, we perform the first in depth exploration and propose a novel attackframework for reverse-stealing prompts against commercial LLMs, namely PRSA.The main idea of PRSA is that by analyzing the critical features of theinput-output pairs, we mimic and gradually infer (steal) the target prompts. Indetail, PRSA mainly consists of two key phases: prompt mutation and promptpruning. In the mutation phase, we propose a prompt attention algorithm basedon differential feedback to capture these critical features for effectivelyinferring the target prompts. In the prompt pruning phase, we identify and maskthe words dependent on specific inputs, enabling the prompts to accommodatediverse inputs for generalization. Through extensive evaluation, we verify thatPRSA poses a severe threat in real world scenarios. We have reported thesefindings to prompt service providers and actively collaborate with them to takeprotective measures for prompt copyright.\rAn Unforgeable Publicly Verifiable Watermark for Large Language Models\nAiwei Liu Leyi Pan Xuming Hu Shu\u0026rsquo;ang Li Lijie Wen Irwin King Philip S. Yu\nabstract\rabstract: Recently, text watermarking algorithms for large language models (LLMs) havebeen proposed to mitigate the potential harms of text generated by LLMs,including fake news and copyright issues. However, current watermark detectionalgorithms require the secret key used in the watermark generation process,making them susceptible to security breaches and counterfeiting during publicdetection. To address this limitation, we propose an unforgeable publiclyverifiable watermark algorithm that uses two different neural networks forwatermark generation and detection, instead of using the same key at bothstages. Meanwhile, the token embedding parameters are shared between thegeneration and detection networks, which makes the detection network achieve ahigh accuracy very efficiently. Experiments demonstrate that our algorithmattains high detection accuracy and computational efficiency through neuralnetworks with a minimized number of parameters. Subsequent analysis confirmsthe high complexity involved in forging the watermark from the detectionnetwork. Our code and data are available at\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable_watermark}.\r2024-02-27\nBeyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs\nTanise Ceron Neele Falk Ana Barić Dmitry Nikolaev Sebastian Padó\nabstract\rabstract: Due to the widespread use of large language models (LLMs) in ubiquitoussystems, we need to understand whether they embed a specific worldview and whatthese views reflect. Recent studies report that, prompted with politicalquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclearwhether these leanings are reliable (robust to prompt variations) and whetherthe leaning is consistent across policies and political leaning. We propose aseries of tests which assess the reliability and consistency of LLMs\u0026rsquo; stanceson political statements based on a dataset of voting-advice questionnairescollected from seven EU countries and annotated for policy domains. We studyLLMs ranging in size from 7B to 70B parameters and find that their reliabilityincreases with parameter count. Larger models show overall stronger alignmentwith left-leaning parties but differ among policy programs: They evince a(left-wing) positive stance towards environment protection, social welfare butalso (right-wing) law and order, with no consistent preferences in foreignpolicy, migration, and economy.\rGenerative AI and Copyright: A Dynamic Perspective\nS. Alex Yang Angela Huyue Zhang\nabstract\rabstract: The rapid advancement of generative AI is poised to disrupt the creativeindustry. Amidst the immense excitement for this new technology, its futuredevelopment and applications in the creative industry hinge crucially upon twocopyright issues: 1) the compensation to creators whose content has been usedto train generative AI models (the fair use standard); and 2) the eligibilityof AI-generated content for copyright protection (AI-copyrightability). Whileboth issues have ignited heated debates among academics and practitioners, mostanalysis has focused on their challenges posed to existing copyright doctrines.In this paper, we aim to better understand the economic implications of thesetwo regulatory issues and their interactions. By constructing a dynamic modelwith endogenous content creation and AI model development, we unravel theimpacts of the fair use standard and AI-copyrightability on AI development, AIcompany profit, creators income, and consumer welfare, and how these impactsare influenced by various economic and operational factors. For example, whilegenerous fair use (use data for AI training without compensating the creator)benefits all parties when abundant training data exists, it can hurt creatorsand consumers when such data is scarce. Similarly, stronger AI-copyrightability(AI content enjoys more copyright protection) could hinder AI development andreduce social welfare. Our analysis also highlights the complex interplaybetween these two copyright issues. For instance, when existing training datais scarce, generous fair use may be preferred only when AI-copyrightability isweak. Our findings underscore the need for policymakers to embrace a dynamic,context-specific approach in making regulatory decisions and provide insightsfor business leaders navigating the complexities of the global regulatoryenvironment.\r2024-02-26\nLLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification\nYiping Song Juhua Zhang Zhiliang Tian Yuxin Yang Minlie Huang Dongsheng Li\nabstract\rabstract: As sufficient data are not always publically accessible for model training,researchers exploit limited data with advanced learning algorithms or expandthe dataset via data augmentation (DA). Conducting DA in private domainrequires private protection approaches (i.e. anonymization and perturbation),but those methods cannot provide protection guarantees. Differential privacy(DP) learning methods theoretically bound the protection but are not skilled atgenerating pseudo text samples with large models. In this paper, we transferDP-based pseudo sample generation task to DP-based generated samplesdiscrimination task, where we propose a DP-based DA method with a LLM and aDP-based discriminator for text classification on private domains. We constructa knowledge distillation model as the DP-based discriminator: teacher models,accessing private data, teaches students how to select private samples withcalibrated noise to achieve DP. To constrain the distribution of DA\u0026rsquo;sgeneration, we propose a DP-based tutor that models the noised privatedistribution and controls samples\u0026rsquo; generation with a low privacy cost. Wetheoretically analyze our model\u0026rsquo;s privacy protection and empirically verify ourmodel.\r2024-02-25\nText Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations\nYafei Xiang Hanyi Yu Yulu Gong Shuning Huo Mengran Zhu\nabstract\rabstract: With the rapid development of artificial intelligence technology, Transformerstructural pre-training model has become an important tool for large languagemodel (LLM) tasks. In the field of e-commerce, these models are especiallywidely used, from text understanding to generating recommendation systems,which provide powerful technical support for improving user experience andoptimizing service processes. This paper reviews the core application scenariosof Transformer pre-training model in e-commerce text understanding andrecommendation generation, including but not limited to automatic generation ofproduct descriptions, sentiment analysis of user comments, construction ofpersonalized recommendation system and automated processing of customer serviceconversations. Through a detailed analysis of the model\u0026rsquo;s working principle,implementation process, and application effects in specific cases, this paperemphasizes the unique advantages of pre-trained models in understanding complexuser intentions and improving the quality of recommendations. In addition, thechallenges and improvement directions for the future are also discussed, suchas how to further improve the generalization ability of the model, the abilityto handle large-scale data sets, and technical strategies to protect userprivacy. Ultimately, the paper points out that the application of Transformerstructural pre-training models in e-commerce has not only driven technologicalinnovation, but also brought substantial benefits to merchants and consumers,and looking forward, these models will continue to play a key role ine-commerce and beyond.\rCognitive Bias in High-Stakes Decision-Making with LLMs\nJessica Echterhoff Yao Liu Abeer Alessa Julian McAuley Zexue He\nabstract\rabstract: Large language models (LLMs) offer significant potential as tools to supportan expanding range of decision-making tasks. However, given their training onhuman (created) data, LLMs can inherit both societal biases against protectedgroups, as well as be subject to cognitive bias. Such human-like bias canimpede fair and explainable decisions made with LLM assistance. Our workintroduces BiasBuster, a framework designed to uncover, evaluate, and mitigatecognitive bias in LLMs, particularly in high-stakes decision-making tasks.Inspired by prior research in psychology and cognitive sciences, we develop adataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,prompt-induced, sequential, inherent). We test various bias mitigationstrategies, amidst proposing a novel method using LLMs to debias their ownprompts. Our analysis provides a comprehensive picture on the presence andeffects of cognitive bias across different commercial and open-source models.We demonstrate that our self-help debiasing effectively mitigate cognitive biaswithout having to manually craft examples for each bias type.\r2024-02-24\nFoot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology\nZhenhua Wang Wei Xie Baosheng Wang Enze Wang Zhiwen Gui Shuoyoucheng Ma Kai Chen\nabstract\rabstract: Large Language Models (LLMs) have gradually become the gateway for people toacquire new knowledge. However, attackers can break the model\u0026rsquo;s securityprotection (\u0026ldquo;jail\u0026rdquo;) to access restricted information, which is called\u0026quot;jailbreaking.\u0026quot; Previous studies have shown the weakness of current LLMs whenconfronted with such jailbreaking attacks. Nevertheless, comprehension of theintrinsic decision-making mechanism within the LLMs upon receipt of jailbreakprompts is noticeably lacking. Our research provides a psychologicalexplanation of the jailbreak prompts. Drawing on cognitive consistency theory,we argue that the key to jailbreak is guiding the LLM to achieve cognitivecoordination in an erroneous direction. Further, we propose an automaticblack-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.This method progressively induces the model to answer harmful questions viamulti-step incremental prompts. We instantiated a prototype system to evaluatethe jailbreaking effectiveness on 8 advanced LLMs, yielding an average successrate of 83.9%. This study builds a psychological perspective on the explanatoryinsights into the intrinsic decision-making logic of LLMs.\r2024-02-23\nUser Inference Attacks on Large Language Models\nNikhil Kandpal Krishna Pillutla Alina Oprea Peter Kairouz Christopher A. Choquette-Choo Zheng Xu\nabstract\rabstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user\u0026rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user\u0026rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.\rThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nShenglai Zeng Jiankun Zhang Pengfei He Yue Xing Yiding Liu Han Xu Jie Ren Shuaiqiang Wang Dawei Yin Yi Chang Jiliang Tang\nabstract\rabstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs\u0026rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.\rWho Wrote this Code? Watermarking for Code Generation\nTaehyun Lee Seokhee Hong Jaewoo Ahn Ilgee Hong Hwaran Lee Sangdoo Yun Jamin Shin Gunhee Kim\nabstract\rabstract: With the remarkable generation performance of large language models, ethicaland legal concerns about using them have been raised, such as plagiarism andcopyright issues. For such concerns, several approaches to watermark and detectLLM-generated text have been proposed very recently. However, we discover thatthe previous methods fail to function appropriately with code generation tasksbecause of the syntactic and semantic characteristics of code. Based on\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,Selective WatErmarking via Entropy Thresholding (SWEET), that promotes \u0026ldquo;green\u0026quot;tokens only at the position with high entropy of the token distribution duringgeneration, thereby preserving the correctness of the generated code. Thewatermarked code is detected by the statistical test and Z-score based on theentropy information. Our experiments on HumanEval and MBPP show that SWEETsignificantly improves the Pareto Frontier between the code correctness andwatermark detection performance. We also show that notable post-hoc detectionmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show thatsetting a reasonable entropy threshold is not much of a challenge. Code isavailable at https://github.com/hongcheki/sweet-watermark.\rA First Look at GPT Apps: Landscape and Vulnerability\nZejun Zhang Li Zhang Xin Yuan Anlan Zhang Mengwei Xu Feng Qian\nabstract\rabstract: With the advancement of Large Language Models (LLMs), increasinglysophisticated and powerful GPTs are entering the market. Despite theirpopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs\u0026rsquo;susceptibility to attacks raises concerns over safety and plagiarism. Thus, inthis work, we conduct a pioneering exploration of GPT stores, aiming to studyvulnerabilities and plagiarism within GPT applications. To begin with, weconduct, to our knowledge, the first large-scale monitoring and analysis of twostores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, wepropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.To complete these two tasks efficiently, we develop two automated tools: onefor web scraping and another designed for programmatically interacting withGPTs. Our findings reveal a significant enthusiasm among users and developersfor GPT interaction and creation, as evidenced by the rapid increase in GPTsand their creators. However, we also uncover a widespread failure to protectGPT internals, with nearly 90% of system prompts easily accessible, leading toconsiderable plagiarism and duplication among GPTs.\r2024-02-22\nExploring Memorization in Fine-tuned Language Models\nShenglai Zeng Yaxin Li Jie Ren Yiding Liu Han Xu Pengfei He Yue Xing Shuaiqiang Wang Jiliang Tang Dawei Yin\nabstract\rabstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models\u0026rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.\rDouble-I Watermark: Protecting Model Copyright for LLM Fine-tuning\nShen Li Liuyi Yao Jinyang Gao Lan Zhang Yaliang Li\nabstract\rabstract: To support various applications, business owners often seek the customizedmodels that are obtained by fine-tuning a pre-trained LLM through the APIprovided by LLM owners or cloud servers. However, this process carries asubstantial risk of model misuse, potentially resulting in severe economicconsequences for business owners. Thus, safeguarding the copyright of thesecustomized models during LLM fine-tuning has become an urgent practicalrequirement, but there are limited existing solutions to provide suchprotection. To tackle this pressing issue, we propose a novel watermarkingapproach named \u0026ldquo;Double-I watermark\u0026rdquo;. Specifically, based on the instruct-tuningdata, two types of backdoor data paradigms are introduced with trigger in theinstruction and the input, respectively. By leveraging LLM\u0026rsquo;s learningcapability to incorporate customized backdoor samples into the dataset, theproposed approach effectively injects specific watermarking information intothe customized model during fine-tuning, which makes it easy to inject andverify watermarks in commercial scenarios. We evaluate the proposed \u0026ldquo;Double-Iwatermark\u0026rdquo; under various fine-tuning methods, demonstrating its harmlessness,robustness, uniqueness, imperceptibility, and validity through both theoreticalanalysis and experimental verification.\r2024-02-21\nTree of Attacks: Jailbreaking Black-Box LLMs Automatically\nAnay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum Anderson Yaron Singer Amin Karbasi\nabstract\rabstract: While Large Language Models (LLMs) display versatile functionality, theycontinue to generate harmful, biased, and toxic content, as demonstrated by theprevalence of human-designed jailbreaks. In this work, we present Tree ofAttacks with Pruning (TAP), an automated method for generating jailbreaks thatonly requires black-box access to the target LLM. TAP utilizes an LLM toiteratively refine candidate (attack) prompts using tree-of-thought reasoninguntil one of the generated prompts jailbreaks the target. Crucially, beforesending prompts to the target, TAP assesses them and prunes the ones unlikelyto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigatea large search space of prompts and pruning reduces the total number of queriessent to the target. In empirical evaluations, we observe that TAP generatesprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)for more than 80% of the prompts using only a small number of queries.Interestingly, TAP is also capable of jailbreaking LLMs protected bystate-of-the-art guardrails, e.g., LlamaGuard. This significantly improves uponthe previous state-of-the-art black-box method for generating jailbreaks.\rLarge Language Models are Advanced Anonymizers\nRobin Staab Mark Vero Mislav Balunović Martin Vechev\nabstract\rabstract: Recent work in privacy research on large language models has shown that theyachieve near human-level performance at inferring personal data from real-worldonline texts. With consistently increasing model capabilities, existing textanonymization methods are currently lacking behind regulatory requirements andadversarial threats. This raises the question of how individuals caneffectively protect their personal data in sharing online texts. In this work,we take two steps to answer this question: We first present a new setting forevaluating anonymizations in the face of adversarial LLMs inferences, allowingfor a natural measurement of anonymization performance while remedying some ofthe shortcomings of previous metrics. We then present our LLM-based adversarialanonymization framework leveraging the strong inferential capabilities of LLMsto inform our anonymization procedure. In our experimental evaluation, we showon real-world and synthetic online texts how adversarial anonymizationoutperforms current industry-grade anonymizers both in terms of the resultingutility and privacy.\r2024-02-20\nPrivacy Issues in Large Language Models: A Survey\nSeth Neel Peter Chang\nabstract\rabstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.\rTRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification\nMartin Gubri Dennis Ulmer Hwaran Lee Sangdoo Yun Seong Joon Oh\nabstract\rabstract: Large Language Model (LLM) services and models often come with legal rules onwho can use them and how they must use them. Assessing the compliance of thereleased LLMs is crucial, as these rules protect the interests of the LLMcontributor and prevent misuse. In this context, we describe the novel problemof Black-box Identity Verification (BBIV). The goal is to determine whether athird-party application uses a certain LLM through its chat function. Wepropose a method called Targeted Random Adversarial Prompt (TRAP) thatidentifies the specific LLM in use. We repurpose adversarial suffixes,originally proposed for jailbreaking, to get a pre-defined answer from thetarget LLM, while other models give random answers. TRAP detects the targetLLMs with over 95% true positive rate at under 0.2% false positive rate evenafter a single interaction. TRAP remains effective even if the LLM has minorchanges that do not significantly alter the original function.\r2024-02-19\nCan AI-Generated Text be Reliably Detected?\nVinu Sankar Sadasivan Aounon Kumar Sriram Balasubramanian Wenxiao Wang Soheil Feizi\nabstract\rabstract: The unregulated use of LLMs can potentially lead to malicious consequencessuch as plagiarism, generating fake news, spamming, etc. Therefore, reliabledetection of AI-generated text can be critical to ensure the responsible use ofLLMs. Recent works attempt to tackle this problem either using certain modelsignatures present in the generated text outputs or by applying watermarkingtechniques that imprint specific patterns onto them. In this paper, we showthat these detectors are not reliable in practical scenarios. In particular, wedevelop a recursive paraphrasing attack to apply on AI text, which can break awhole range of detectors, including the ones using the watermarking schemes aswell as neural network-based detectors, zero-shot classifiers, andretrieval-based detectors. Our experiments include passages around 300 tokensin length, showing the sensitivity of the detectors even in the case ofrelatively long passages. We also observe that our recursive paraphrasing onlydegrades text quality slightly, measured via human studies, and metrics such asperplexity scores and accuracy on text benchmarks. Additionally, we show thateven LLMs protected by watermarking schemes can be vulnerable against spoofingattacks aimed to mislead detectors to classify human-written text asAI-generated, potentially causing reputational damages to the developers. Inparticular, we show that an adversary can infer hidden AI text signatures ofthe LLM outputs without having white-box access to the detection method.Finally, we provide a theoretical connection between the AUROC of the bestpossible detector and the Total Variation distance between human and AI textdistributions that can be used to study the fundamental hardness of thereliable detection problem for advanced language models. Our code is publiclyavailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.\rCopyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications\nXinwei Guo Yujun Li Yafeng Peng Xuetao Wei\nabstract\rabstract: As AIGC has impacted our society profoundly in the past years, ethical issueshave received tremendous attention. The most urgent one is the AIGC copyrightdilemma, which can immensely stifle the development of AIGC and greatly costthe entire society. Given the complexity of AIGC copyright governance and thefact that no perfect solution currently exists, previous work advocatedcopyleft on AI governance but without substantive analysis. In this paper, wetake a step further to explore the feasibility of copyleft to alleviate theAIGC copyright dilemma. We conduct a mixed-methods study from two aspects:qualitatively, we use a formal what-if analysis to clarify the dilemma andprovide case studies to show the feasibility of copyleft; quantitatively, weperform a carefully designed survey to find out how the public feels aboutcopylefting AIGC. The key findings include: a) people generally perceive thedilemma, b) they prefer to use authorized AIGC under loose restriction, and c)they are positive to copyleft in AIGC and willing to use it in the future.\rPurifying Large Language Models by Ensembling a Small Language Model\nTianlin Li Qian Liu Tianyu Pang Chao Du Qing Guo Yang Liu Min Lin\nabstract\rabstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.\rNOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization\nImjin Ahn Hansle Gwon Young-Hak Kim Tae Joon Jun Sanghyun Park\nabstract\rabstract: The discharge summary is a one of critical documents in the patient journey,encompassing all events experienced during hospitalization, including multiplevisits, medications, tests, surgery/procedures, and admissions/discharge.Providing a summary of the patient\u0026rsquo;s progress is crucial, as it significantlyinfluences future care and planning. Consequently, clinicians face thelaborious and resource-intensive task of manually collecting, organizing, andcombining all the necessary data for a discharge summary. Therefore, we propose\u0026quot;NOTE\u0026quot;, which stands for \u0026ldquo;Notable generation Of patient Text summaries throughan Efficient approach based on direct preference optimization\u0026rdquo;. NOTE is basedon Medical Information Mart for Intensive Care- III dataset and summarizes asingle hospitalization of a patient. Patient events are sequentially combinedand used to generate a discharge summary for each hospitalization. In thepresent circumstances, large language models\u0026rsquo; application programminginterfaces (LLMs\u0026rsquo; APIs) are widely available, but importing and exportingmedical data presents significant challenges due to privacy protection policiesin healthcare institutions. Moreover, to ensure optimal performance, it isessential to implement a lightweight model for internal server or programwithin the hospital. Therefore, we utilized DPO and parameter efficient finetuning (PEFT) techniques to apply a fine-tuning method that guarantees superiorperformance. To demonstrate the practical application of the developed NOTE, weprovide a webpage-based demonstration software. In the future, we will aim todeploy the software available for actual use by clinicians in hospital. NOTEcan be utilized to generate various summaries not only discharge summaries butalso throughout a patient\u0026rsquo;s journey, thereby alleviating the labor-intensiveworkload of clinicians and aiming for increased efficiency.\rWhere It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages\nSameer Jain Sedrick Scott Keh Shova Chettri Karun Dewan Pablo Izquierdo Johanna Prussman Pooja Shreshtha Cesar Suarez Zheyuan Ryan Shi Lei Li Fei Fang\nabstract\rabstract: Environmental conservation organizations routinely monitor news content onconservation in protected areas to maintain situational awareness ofdevelopments that can have an environmental impact. Existing automated mediamonitoring systems require large amounts of data labeled by domain experts,which is only feasible at scale for high-resource languages like English.However, such tools are most needed in the global south where news of interestis mainly in local low-resource languages, and far fewer experts are availableto annotate datasets sustainably. In this paper, we propose NewsSerow, a methodto automatically recognize environmental conservation content in low-resourcelanguages. NewsSerow is a pipeline of summarization, in-context few-shotclassification, and self-reflection using large language models (LLMs). Usingat most 10 demonstration example news articles in Nepali, NewsSerowsignificantly outperforms other few-shot methods and achieves comparableperformance with models fully fine-tuned using thousands of examples. The WorldWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring inNepal, significantly reducing their operational burden, and ensuring that AItools for conservation actually reach the communities that need them the most.NewsSerow has also been deployed for countries with other languages likeColombia.\rHU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?\nShubhashis Roy Dipta Sadat Shahriar\nabstract\rabstract: This paper describes our system developed for SemEval-2024 Task 8,\u0026ldquo;Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated TextDetection.\u0026rdquo; Machine-generated texts have been one of the main concerns due tothe use of large language models (LLM) in fake text generation, phishing,cheating in exams, or even plagiarizing copyright materials. A lot of systemshave been developed to detect machine-generated text. Nonetheless, the majorityof these systems rely on the text-generating model, a limitation that isimpractical in real-world scenarios, as it\u0026rsquo;s often impossible to know whichspecific model the user has used for text generation. In this work, we proposea single model based on contrastive learning, which uses ~40% of the baseline\u0026rsquo;sparameters (149M vs. 355M) but shows a comparable performance on the testdataset (21st out of 137 participants). Our key finding is that even without anensemble of multiple models, a single base model can have comparableperformance with the help of data augmentation and contrastive learning.\r2024-02-18\nStealthy Attack on Large Language Model based Recommendation\nJinghao Zhang Yuting Liu Qiang Liu Shu Wu Guibing Guo Liang Wang\nabstract\rabstract: Recently, the powerful large language models (LLMs) have been instrumental inpropelling the progress of recommender systems (RS). However, while thesesystems have flourished, their susceptibility to security threats has beenlargely overlooked. In this work, we reveal that the introduction of LLMs intorecommendation models presents new security vulnerabilities due to theiremphasis on the textual content of items. We demonstrate that attackers cansignificantly boost an item\u0026rsquo;s exposure by merely altering its textual contentduring the testing phase, without requiring direct interference with themodel\u0026rsquo;s training process. Additionally, the attack is notably stealthy, as itdoes not affect the overall recommendation performance and the modifications tothe text are subtle, making it difficult for users and platforms to detect. Ourcomprehensive experiments across four mainstream LLM-based recommendationmodels demonstrate the superior efficacy and stealthiness of our approach. Ourwork unveils a significant security gap in LLM-based recommendation systems andpaves the way for future research on protecting these systems.\r2024-02-17\nLLM-based Federated Recommendation\nJujia Zhao Wenjie Wang Chen Xu Zhaochun Ren See-Kiong Ng Tat-Seng Chua\nabstract\rabstract: Large Language Models (LLMs), with their advanced contextual understandingabilities, have demonstrated considerable potential in enhancing recommendationsystems via fine-tuning methods. However, fine-tuning requires users\u0026rsquo; behaviordata, which poses considerable privacy risks due to the incorporation ofsensitive user information. The unintended disclosure of such data couldinfringe upon data protection laws and give rise to ethical issues. To mitigatethese privacy issues, Federated Learning for Recommendation (Fed4Rec) hasemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-basedrecommendation presents two main challenges: first, an increase in theimbalance of performance across clients, affecting the system\u0026rsquo;s efficiency overtime, and second, a high demand on clients\u0026rsquo; computational and storage resourcesfor local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-basedRecommendation (PPLR) framework. The PPLR framework employs two primarystrategies. First, it implements a dynamic balance strategy, which involves thedesign of dynamic parameter aggregation and adjustment of learning speed fordifferent clients during the training phase, to ensure relatively balancedperformance across all clients. Second, PPLR adopts a flexible storagestrategy, selectively retaining certain sensitive layers of the language modelon the client side while offloading non-sensitive layers to the server. Thisapproach aims to preserve user privacy while efficiently saving computationaland storage resources. Experimental results demonstrate that PPLR not onlyachieves a balanced performance among clients but also enhances overall systemperformance in a manner that is both computationally and storage-efficient,while effectively protecting user privacy.\r2024-02-16\nLarge Language Model Unlearning\nYuanshun Yao Xiaojun Xu Yang Liu\nabstract\rabstract: We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,on large language models (LLMs). We show at least three scenarios of aligningLLMs with human preferences can benefit from unlearning: (1) removing harmfulresponses, (2) erasing copyright-protected content as requested, and (3)reducing hallucinations. Unlearning, as an alignment technique, has threeadvantages. (1) It only requires negative (e.g. harmful) examples, which aremuch easier and cheaper to collect (e.g. via red teaming or user reporting)than positive (e.g. helpful and often human-written) examples required in RLHF(RL from human feedback). (2) It is computationally efficient. (3) It isespecially effective when we know which training samples cause the misbehavior.To the best of our knowledge, our work is among the first to explore LLMunlearning. We are also among the first to formulate the settings, goals, andevaluations in LLM unlearning. We show that if practitioners only have limitedresources, and therefore the priority is to stop generating undesirable outputsrather than to try to generate desirable outputs, unlearning is particularlyappealing. Despite only having negative samples, our ablation study shows thatunlearning can still achieve better alignment performance than RLHF with just2% of its computational time.\rProving membership in LLM pretraining data via data watermarks\nJohnny Tian-Zheng Wei Ryan Yixiang Wang Robin Jia\nabstract\rabstract: Detecting whether copyright holders\u0026rsquo; works were used in LLM pretraining ispoised to be an important problem. This work proposes using data watermarks toenable principled detection with only black-box model access, provided that therightholder contributed multiple training documents and watermarked them beforepublic release. By applying a randomly sampled data watermark, detection can beframed as hypothesis testing, which provides guarantees on the false detectionrate. We study two watermarks: one that inserts random sequences, and anotherthat randomly substitutes characters with Unicode lookalikes. We first show howthree aspects of watermark design \u0026ndash; watermark length, number of duplications,and interference \u0026ndash; affect the power of the hypothesis test. Next, we study howa watermark\u0026rsquo;s detection strength changes under model and dataset scaling: whileincreasing the dataset size decreases the strength of the watermark, watermarksremain strong if the model size also increases. Finally, we view SHA hashes asnatural watermarks and show that we can robustly detect hashes fromBLOOM-176B\u0026rsquo;s training data, as long as they occurred at least 90 times.Together, our results point towards a promising future for data watermarks inreal world use.\rUser Experience Design Professionals\u0026rsquo; Perceptions of Generative Artificial Intelligence\nJie Li Hancheng Cao Laura Lin Youyang Hou Ruihao Zhu Abdallah El Ali\nabstract\rabstract: Among creative professionals, Generative Artificial Intelligence (GenAI) hassparked excitement over its capabilities and fear over unanticipatedconsequences. How does GenAI impact User Experience Design (UXD) practice, andare fears warranted? We interviewed 20 UX Designers, with diverse experienceand across companies (startups to large enterprises). We probed them tocharacterize their practices, and sample their attitudes, concerns, andexpectations. We found that experienced designers are confident in theiroriginality, creativity, and empathic skills, and find GenAI\u0026rsquo;s role asassistive. They emphasized the unique human factors of \u0026ldquo;enjoyment\u0026rdquo; and\u0026quot;agency\u0026quot;, where humans remain the arbiters of \u0026ldquo;AI alignment\u0026rdquo;. However, skilldegradation, job replacement, and creativity exhaustion can adversely impactjunior designers. We discuss implications for human-GenAI collaboration,specifically copyright and ownership, human creativity and agency, and AIliteracy and access. Through the lens of responsible and participatory AI, wecontribute a deeper understanding of GenAI fears and opportunities for UXD.\r2024-02-15\nRethinking Machine Unlearning for Large Language Models\nSijia Liu Yuanshun Yao Jinghan Jia Stephen Casper Nathalie Baracaldo Peter Hase Xiaojun Xu Yuguang Yao Hang Li Kush R. Varshney Mohit Bansal Sanmi Koyejo Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\rDE-COP: Detecting Copyrighted Content in Language Models Training Data\nAndré V. Duarte Xuandong Zhao Arlindo L. Oliveira Lei Li\nabstract\rabstract: How can we detect if copyrighted content was used in the training process ofa language model, considering that the training data is typically undisclosed?We are motivated by the premise that a language model is likely to identifyverbatim excerpts from its training text. We propose DE-COP, a method todetermine whether a piece of copyrighted content was included in training.DE-COP\u0026rsquo;s core approach is to probe an LLM with multiple-choice questions, whoseoptions include both verbatim text and their paraphrases. We constructBookTection, a benchmark with excerpts from 165 books published prior andsubsequent to a model\u0026rsquo;s training cutoff, along with their paraphrases. Ourexperiments show that DE-COP surpasses the prior best method by 9.6% indetection performance (AUC) on models with logits available. Moreover, DE-COPalso achieves an average accuracy of 72% for detecting suspect books on fullyblack-box models where prior methods give $\\approx$ 4% accuracy. Our code anddatasets are available at https://github.com/avduarte333/DE-COP_Method\rAbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns\nAshfak Md Shibli Mir Mehedi A. Pritom Maanak Gupta\nabstract\rabstract: SMS phishing, also known as \u0026ldquo;smishing\u0026rdquo;, is a growing threat that tricks usersinto disclosing private information or clicking into URLs with maliciouscontent through fraudulent mobile text messages. In recent past, we have alsoobserved a rapid advancement of conversational generative AI chatbot services(e.g., OpenAI\u0026rsquo;s ChatGPT, Google\u0026rsquo;s BARD), which are powered by pre-trained largelanguage models (LLMs). These AI chatbots certainly have a lot of utilities butit is not systematically understood how they can play a role in creatingthreats and attacks. In this paper, we propose AbuseGPT method to show how theexisting generative AI-based chatbot services can be exploited by attackers inreal world to create smishing texts and eventually lead to craftier smishingcampaigns. To the best of our knowledge, there is no pre-existing work thatevidently shows the impacts of these generative text-based models on creatingSMS phishing. Thus, we believe this study is the first of its kind to shedlight on this emerging cybersecurity threat. We have found strong empiricalevidences to show that attackers can exploit ethical standards in the existinggenerative AI-based chatbot services by crafting prompt injection attacks tocreate newer smishing campaigns. We also discuss some future researchdirections and guidelines to protect the abuse of generative AI-based servicesand safeguard users from smishing attacks.\rDetecting Phishing Sites Using ChatGPT\nTakashi Koide Naoki Fukushi Hiroki Nakano Daiki Chiba\nabstract\rabstract: The emergence of Large Language Models (LLMs), including ChatGPT, is having asignificant impact on a wide range of fields. While LLMs have been extensivelyresearched for tasks such as code generation and text synthesis, theirapplication in detecting malicious web content, particularly phishing sites,has been largely unexplored. To combat the rising tide of cyber attacks due tothe misuse of LLMs, it is important to automate detection by leveraging theadvanced capabilities of LLMs. In this paper, we propose a novel system called ChatPhishDetector thatutilizes LLMs to detect phishing sites. Our system involves leveraging a webcrawler to gather information from websites, generating prompts for LLMs basedon the crawled data, and then retrieving the detection results from theresponses generated by the LLMs. The system enables us to detect multilingualphishing sites with high accuracy by identifying impersonated brands and socialengineering techniques in the context of the entire website, without the needto train machine learning models. To evaluate the performance of our system, weconducted experiments on our own dataset and compared it with baseline systemsand several LLMs. The experimental results using GPT-4V demonstratedoutstanding performance, with a precision of 98.7% and a recall of 99.6%,outperforming the detection results of other LLMs and existing systems. Thesefindings highlight the potential of LLMs for protecting users from onlinefraudulent activities and have important implications for enhancingcybersecurity measures.\r2024-02-14\nCopyright Traps for Large Language Models\nMatthieu Meeus Igor Shilov Manuel Faysse Yves-Alexandre de Montjoye\nabstract\rabstract: Questions of fair use of copyright-protected content to train Large LanguageModels (LLMs) are being very actively debated. Document-level inference hasbeen proposed as a new task: inferring from black-box access to the trainedmodel whether a piece of content has been seen during training. SOTA methodshowever rely on naturally occurring memorization of (part of) the content.While very effective against models that memorize a lot, we hypothesize\u0026ndash;andlater confirm\u0026ndash;that they will not work against models that do not naturallymemorize, e.g. medium-size 1B models. We here propose to use copyright traps,the inclusion of fictitious entries in original content, to detect the use ofcopyrighted materials in LLMs with a focus on models where memorization doesnot naturally occur. We carefully design an experimental setup, randomlyinserting traps into original content (books) and train a 1.3B LLM. We firstvalidate that the use of content in our target model would be undetectableusing existing methods. We then show, contrary to intuition, that evenmedium-length trap sentences repeated a significant number of times (100) arenot detectable using existing methods. However, we show that longer sequencesrepeated a large number of times can be reliably detected (AUC=0.75) and usedas copyright traps. We further improve these results by studying how the numberof times a sequence is seen improves detectability, how sequences with higherperplexity tend to be memorized more, and how taking context into accountfurther improves detectability.\rTrained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code\nVahid Majdinasab Amin Nikanjam Foutse Khomh\nabstract\rabstract: Code auditing ensures that the developed code adheres to standards,regulations, and copyright protection by verifying that it does not containcode from protected sources. The recent advent of Large Language Models (LLMs)as coding assistants in the software development process poses new challengesfor code auditing. The dataset for training these models is mainly collectedfrom publicly available sources. This raises the issue of intellectual propertyinfringement as developers\u0026rsquo; codes are already included in the dataset.Therefore, auditing code developed using LLMs is challenging, as it isdifficult to reliably assert if an LLM used during development has been trainedon specific copyrighted codes, given that we do not have access to the trainingdatasets of these models. Given the non-disclosure of the training datasets,traditional approaches such as code clone detection are insufficient forasserting copyright infringement. To address this challenge, we propose a newapproach, TraWiC; a model-agnostic and interpretable method based on membershipinference for detecting code inclusion in an LLM\u0026rsquo;s training dataset. We extractsyntactic and semantic identifiers unique to each program to train a classifierfor detecting code inclusion. In our experiments, we observe that TraWiC iscapable of detecting 83.87% of codes that were used to train an LLM. Incomparison, the prevalent clone detection tool NiCad is only capable ofdetecting 47.64%. In addition to its remarkable performance, TraWiC has lowresource overhead in contrast to pair-wise clone detection that is conductedduring the auditing process of tools like CodeWhisperer reference tracker,across thousands of code snippets.\rDPZero: Private Fine-Tuning of Language Models without Backpropagation\nLiang Zhang Bingcong Li Kiran Koshy Thekumparampil Sewoong Oh Niao He\nabstract\rabstract: The widespread practice of fine-tuning large language models (LLMs) ondomain-specific data faces two major challenges in memory and privacy. First,as the size of LLMs continues to grow, the memory demands of gradient-basedtraining methods via backpropagation become prohibitively high. Second, giventhe tendency of LLMs to memorize training data, it is important to protectpotentially sensitive information in the fine-tuning data from beingregurgitated. Zeroth-order methods, which rely solely on forward passes,substantially reduce memory consumption during training. However, directlycombining them with standard differentially private gradient descent suffersfrom growing model size. To bridge this gap, we introduce DPZero, a novelprivate zeroth-order algorithm with nearly dimension-independent rates. Thememory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa onsix downstream tasks.\rA Study of Fairness Concerns in AI-based Mobile App Reviews\nAli Rezaei Nasab Maedeh Dashti Mojtaba Shahin Mansooreh Zahedi Hourieh Khalajzadeh Chetan Arora Peng Liang\nabstract\rabstract: Fairness is one of the socio-technical concerns that must be addressed inAI-based systems. Unfair AI-based systems, particularly unfair AI-based mobileapps, can pose difficulties for a significant proportion of the globalpopulation. This paper aims to analyze fairness concerns in AI-based appreviews.We first manually constructed a ground-truth dataset, including astatistical sample of fairness and non-fairness reviews. Leveraging theground-truth dataset, we developed and evaluated a set of machine learning anddeep learning classifiers that distinguish fairness reviews from non-fairnessreviews. Our experiments show that our best-performing classifier can detectfairness reviews with a precision of 94%. We then applied the best-performingclassifier on approximately 9.5M reviews collected from 108 AI-based apps andidentified around 92K fairness reviews. Next, applying the K-means clusteringtechnique to the 92K fairness reviews, followed by manual analysis, led to theidentification of six distinct types of fairness concerns (e.g., \u0026lsquo;receivingdifferent quality of features and services in different platforms and devices\u0026rsquo;and \u0026rsquo;lack of transparency and fairness in dealing with user-generatedcontent\u0026rsquo;). Finally, the manual analysis of 2,248 app owners\u0026rsquo; responses to thefairness reviews identified six root causes (e.g., \u0026lsquo;copyright issues\u0026rsquo;) that appowners report to justify fairness concerns.\r2024-02-13\nJAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models\nJillian Fisher Ximing Lu Jaehun Jung Liwei Jiang Zaid Harchaoui Yejin Choi\nabstract\rabstract: The permanence of online content combined with the enhanced authorshipidentification techniques calls for stronger computational methods to protectthe identity and privacy of online authorship when needed, e.g., blind reviewsfor scientific papers, anonymous online reviews, or anonymous interactions inthe mental health forums. In this paper, we propose an unsupervisedinference-time approach to authorship obfuscation to address the uniquechallenges of authorship obfuscation: lack of supervision data for diverseauthorship and domains, and the need for a sufficient level of revision beyondsimple paraphrasing to obfuscate the authorship, all the while preserving theoriginal content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm forauthorship obfuscation that can be in principle applied to any text andauthorship. Our approach builds on small language models such as GPT2-XL inorder to help avoid disclosing the original content to proprietary LLM\u0026rsquo;s APIs,while also reducing the performance gap between small and large language modelsvia algorithmic enhancement. The key idea behind our approach is to boost thecreative power of smaller language models through constrained decoding, whilealso allowing for user-specified controls and flexibility. Experimental resultsdemonstrate that our approach based on GPT2-XL outperforms previousstate-of-the-art methods based on comparably small models, while performingcompetitively against GPT3.5 175B, a propriety model that is two orders ofmagnitudes larger.\rComputational Copyright: Towards A Royalty Model for Music Generative AI\nJunwei Deng Jiaqi Ma\nabstract\rabstract: The advancement of generative AI has given rise to pressing copyrightchallenges, particularly in music industry. This paper focuses on the economicaspects of these challenges, emphasizing that the economic impact constitutes acentral issue in the copyright arena. The complexity of the black-boxgenerative AI technologies not only suggests but necessitates algorithmicsolutions. However, such solutions have been largely missing, leading toregulatory challenges in this landscape. We aim to bridge the gap in currentapproaches by proposing potential royalty models for revenue sharing on AImusic generation platforms. Our methodology involves a detailed analysis ofexisting royalty models in platforms like Spotify and YouTube, and adaptingthese to the unique context of AI-generated music. A significant challenge weaddress is the attribution of AI-generated music to influential copyrightedcontent in the training data. To this end, we present algorithmic solutionsemploying data attribution techniques. Our experimental results verify theeffectiveness of these solutions. This research represents a pioneering effortin integrating technical advancements with economic and legal considerations inthe field of generative AI, offering a computational copyright solution for thechallenges posed by the opaque nature of AI technologies.\r2024-02-12\nEmpowering Federated Learning for Massive Models with NVIDIA FLARE\nHolger R. Roth Ziyue Xu Yuan-Ting Hsieh Adithya Renduchintala Isaac Yang Zhihong Zhang Yuhong Wen Sean Yang Kevin Lu Kristopher Kersten Camir Ricketts Daguang Xu Chester Chen Yan Cheng Andrew Feng\nabstract\rabstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.\rEmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models\nGuo Lin Wenyue Hua Yongfeng Zhang\nabstract\rabstract: Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanism EmojiCryptto protect user privacy. It uses Emoji to encrypt the user inputs beforesending them to LLM, effectively rendering them indecipherable to human orLLM\u0026rsquo;s examination while retaining the original intent of the prompt, thusensuring the model\u0026rsquo;s performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that EmojiCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/EmojiCrypt.\rLarge language models can enhance persuasion through linguistic feature alignment\nMinkyu Shin Jin Kim\nabstract\rabstract: Although large language models (LLMs) are reshaping various aspects of humanlife, our current understanding of their impacts remains somewhat constrained.Here we investigate the impact of LLMs on human communication, using data onconsumer complaints in the financial industry. By employing an AI detectiontool on more than 820K complaints gathered by the Consumer Financial ProtectionBureau (CFPB), we find a sharp increase in the likely use of LLMs shortly afterthe release of ChatGPT. Moreover, the likely LLM usage was positivelycorrelated with message persuasiveness (i.e., increased likelihood of obtainingrelief from financial firms). Computational linguistic analyses suggest thatthe positive correlation may be explained by LLMs\u0026rsquo; enhancement of variouslinguistic features. Based on the results of these observational studies, wehypothesize that LLM usage may enhance a comprehensive set of linguisticfeatures, increasing message persuasiveness to receivers with heterogeneouslinguistic preferences (i.e., linguistic feature alignment). We test thishypothesis in preregistered experiments and find support for it. As an instanceof early empirical demonstrations of LLM usage for enhancing persuasion, ourresearch highlights the transformative potential of LLMs in humancommunication.\rGame Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch\nRay Ito Junichiro Takahashi\nabstract\rabstract: Several attempts have been made to implement text command control for gameagents. However, current technologies are limited to processing predefinedformat commands. This paper proposes a pioneering text command control systemfor a game agent that can understand natural language commands expressed infree-form. The proposed system uses a large language model (LLM) for codegeneration to interpret and transform natural language commands into behaviorbranch, a proposed knowledge expression based on behavior trees, whichfacilitates execution by the game agent. This study conducted empiricalvalidation within a game environment that simulates a Pok'emon game andinvolved multiple participants. The results confirmed the system\u0026rsquo;s ability tounderstand and carry out natural language commands, representing a noteworthyin the realm of real-time language interactive game agents. Notice for the use of this material. The copyright of this material isretained by the Japanese Society for Artificial Intelligence (JSAI). Thismaterial is published here with the agreement of JSAI. Please be complied withCopyright Law of Japan if any users wish to reproduce, make derivative work,distribute or make available to the public any part or whole thereof. AllRights Reserved, Copyright (C) The Japanese Society for ArtificialIntelligence.\rFive ethical principles for generative AI in scientific research\nZhicheng Lin\nabstract\rabstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.\r2024-02-10\nData Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\nShahriar Golchin Mihai Surdeanu\nabstract\rabstract: We propose the Data Contamination Quiz (DCQ), a simple and effective approachto detect data contamination in large language models (LLMs) and estimate theamount of it. Specifically, we frame data contamination detection as a seriesof multiple-choice questions and devise a quiz format wherein three perturbedversions of each dataset instance are created. These changes only includeword-level perturbations. The generated perturbed versions, along with theoriginal instance, form the options in the DCQ, with an extra optionaccommodating the possibility that none of the provided choices is correct.Given that the only distinguishing signal among the choices is the exactwording relative to the original instance, an LLM, when tasked with identifyingthe original instance from the choices, gravitates towards the original one ifit has been exposed to it in its pre-training phase\u0026ndash;a trait intrinsic to LLMs.Tested over several datasets with GPT-4/3.5, our findings\u0026ndash;while fully lackingaccess to LLMs\u0026rsquo; pre-training data and internal parameters\u0026ndash;suggest that DCQuncovers greater contamination levels compared to existing detection methodsand proficiently bypasses more safety filters, especially those set to avoidgenerating copyrighted contents.\rWhispers in the Machine: Confidentiality in LLM-integrated Systems\nJonathan Evertz Merlin Chlosta Lea Schönherr Thorsten Eisenhofer\nabstract\rabstract: Large Language Models (LLMs) are increasingly integrated with external tools.While these integrations can significantly improve the functionality of LLMs,they also create a new attack surface where confidential data may be disclosedbetween different components. Specifically, malicious tools can exploitvulnerabilities in the LLM itself to manipulate the model and compromise thedata of other services, raising the question of how private data can beprotected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality inLLM-integrated systems. For this, we formalize a \u0026ldquo;secret key\u0026rdquo; game that cancapture the ability of a model to conceal private information. This enables usto compare the vulnerability of a model against confidentiality attacks andalso the effectiveness of different defense strategies. In this framework, weevaluate eight previously published attacks and four defenses. We find thatcurrent defenses lack generalization across attack strategies. Building on thisanalysis, we propose a method for robustness fine-tuning, inspired byadversarial training. This approach is effective in lowering the success rateof attackers and in improving the system\u0026rsquo;s resilience against unknown attacks.\r2024-02-09\nStudious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning\nYichuan Mo Yuji Wang Zeming Wei Yisen Wang\nabstract\rabstract: Although Large Language Models (LLMs) have achieved tremendous success invarious applications, they are also susceptible to certain prompts that caninduce them to bypass built-in safety measures and provide dangerous or illegalcontent, a phenomenon known as jailbreak. To protect LLMs from producingharmful information, various defense strategies are proposed, with mostfocusing on content filtering or adversarial training of models. In this paper,we propose an approach named Prompt Adversarial Tuning (PAT) to train a defensecontrol mechanism, which is then embedded as a prefix to user prompts toimplement our defense strategy. We design a training process similar toadversarial training to achieve our optimized goal, alternating betweenupdating attack and defense controls. To our knowledge, we are the first toimplement defense from the perspective of prompt tuning. Once employed, ourmethod will hardly impact the operational efficiency of LLMs. Experiments showthat our method is effective in both black-box and white-box settings, reducingthe success rate of advanced attacks to nearly 0 while maintaining the benignanswer rate of 80% to simple benign questions. Our work might potentially charta new perspective for future explorations in LLM security.\r2024-02-07\nHuman-Readable Fingerprint for Large Language Models\nBoyi Zeng Chenghu Zhou Xinbing Wang Zhouhan Lin\nabstract\rabstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters\u0026rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM\u0026rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog\u0026rsquo;s appearancestrongly indicates the LLM\u0026rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.\r2024-02-06\nOrganic or Diffused: Can We Distinguish Human Art from AI-generated Images?\nAnna Yoo Jeong Ha Josephine Passananti Ronik Bhaskar Shawn Shan Reid Southen Haitao Zheng Ben Y. Zhao\nabstract\rabstract: The advent of generative AI images has completely disrupted the art world.Distinguishing AI generated images from human art is a challenging problemwhose impact is growing over time. A failure to address this problem allows badactors to defraud individuals paying a premium for human art and companieswhose stated policies forbid AI imagery. It is also critical for content ownersto establish copyright, and for model trainers interested in curating trainingdata in order to avoid potential model collapse. There are several different approaches to distinguishing human art from AIimages, including classifiers trained by supervised learning, research toolstargeting diffusion models, and identification by professional artists usingtheir knowledge of artistic techniques. In this paper, we seek to understandhow well these approaches can perform against today\u0026rsquo;s modern generative modelsin both benign and adversarial settings. We curate real human art across 7styles, generate matching images from 5 generative models, and apply 8detectors (5 automated detectors and 3 different human groups including 180crowdworkers, 4000+ professional artists, and 13 expert artists experienced atdetecting AI). Both Hive and expert artists do very well, but make mistakes indifferent ways (Hive is weaker against adversarial perturbations while Expertartists produce higher false positives). We believe these weaknesses willremain as models continue to evolve, and use our data to demonstrate why acombined team of human and automated detectors provides the best combination ofaccuracy and robustness.\r2024-02-05\nWeak-to-Strong Jailbreaking on Large Language Models\nXuandong Zhao Xianjun Yang Tianyu Pang Chao Du Lei Li Yu-Xiang Wang William Yang Wang\nabstract\rabstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack\u0026rsquo;s key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model\u0026rsquo;s decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at https://github.com/XuandongZhao/weak-to-strong\rZero-Shot Machine Unlearning at Scale via Lipschitz Regularization\nJack Foster Kyle Fogarty Stefan Schoepf Cengiz Öztireli Alexandra Brintrup\nabstract\rabstract: To comply with AI and data regulations, the need to forget private orcopyrighted information from trained machine learning models is increasinglyimportant. The key challenge in unlearning is forgetting the necessary data ina timely manner, while preserving model performance. In this work, we addressthe zero-shot unlearning scenario, whereby an unlearning algorithm must be ableto remove data given only a trained model and the data to be forgotten. Undersuch a definition, existing state-of-the-art methods are insufficient. Buildingon the concepts of Lipschitz continuity, we present a method that inducessmoothing of the forget sample\u0026rsquo;s output, with respect to perturbations of thatsample. We show this smoothing successfully results in forgetting whilepreserving general model performance. We perform extensive empirical evaluationof our method over a range of contemporary benchmarks, verifying that ourmethod achieves state-of-the-art performance under the strict constraints ofzero-shot unlearning.\r2024-02-04\nCopyright Protection in Generative AI: A Technical Perspective\nJie Ren Han Xu Pengfei He Yingqian Cui Shenglai Zeng Jiankun Zhang Hongzhi Wen Jiayuan Ding Hui Liu Yi Chang Jiliang Tang\nabstract\rabstract: Generative AI has witnessed rapid advancement in recent years, expandingtheir capabilities to create synthesized content such as text, images, audio,and code. The high fidelity and authenticity of contents generated by theseDeep Generative Models (DGMs) have sparked significant copyright concerns.There have been various legal debates on how to effectively safeguardcopyrights in DGMs. This work delves into this issue by providing acomprehensive overview of copyright protection from a technical perspective. Weexamine from two distinct viewpoints: the copyrights pertaining to the sourcedata held by the data owners and those of the generative models maintained bythe model builders. For data copyright, we delve into methods data owners canprotect their content and DGMs can be utilized without infringing upon theserights. For model copyright, our discussion extends to strategies forpreventing model theft and identifying outputs generated by specific models.Finally, we highlight the limitations of existing techniques and identify areasthat remain unexplored. Furthermore, we discuss prospective directions for thefuture of copyright protection, underscoring its importance for the sustainableand ethical development of Generative AI.\r2024-02-02\n(A)I Am Not a Lawyer, But\u0026hellip;: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice\nInyoung Cheong King Xia K. J. Kevin Feng Quan Ze Chen Amy X. Zhang\nabstract\rabstract: The rapid proliferation of large language models (LLMs) as general purposechatbots available to the public raises hopes around expanding access toprofessional guidance in law, medicine, and finance, while triggering concernsabout public reliance on LLMs for high-stakes circumstances. Prior research hasspeculated on high-level ethical considerations but lacks concrete criteriadetermining when and why LLM chatbots should or should not provide professionalassistance. Through examining the legal domain, we contribute a structuredexpert analysis to uncover nuanced policy considerations around using LLMs forprofessional advice, using methods inspired by case-based reasoning. Weconvened workshops with 20 legal experts and elicited dimensions on appropriateAI assistance for sample user queries (``cases\u0026rsquo;\u0026rsquo;). We categorized our expertdimensions into: (1) user attributes, (2) query characteristics, (3) AIcapabilities, and (4) impacts. Beyond known issues like hallucinations, expertsrevealed novel legal problems, including that users\u0026rsquo; conversations with LLMsare not protected by attorney-client confidentiality or bound to professionalethics that guard against conflicted counsel or poor quality advice. Thisaccountability deficit led participants to advocate for AI systems to helpusers polish their legal questions and relevant facts, rather than recommendspecific actions. More generally, we highlight the potential of case-basedexpert deliberation as a method of responsibly translating professionalintegrity and domain knowledge into design requirements to inform appropriateAI behavior when generating advice in professional domains.\r2024-01-30\nAre ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?\nDimitrios Ioannidis Jeremy Kepner Andrew Bowne Harriet S. Bryant\nabstract\rabstract: The rise of Generative Artificial Intelligence systems (\u0026ldquo;AI systems\u0026rdquo;) hascreated unprecedented social engagement. AI code generation systems provideresponses (output) to questions or requests by accessing the vast library ofopen-source code created by developers over the past few decades. However, theydo so by allegedly stealing the open-source code stored in virtual libraries,known as repositories. This Article focuses on how this happens and whetherthere is a solution that protects innovation and avoids years of litigation. Wealso touch upon the array of issues raised by the relationship between AI andcopyright. Looking ahead, we propose the following: (a) immediate changes tothe licenses for open-source code created by developers that will limit accessand/or use of any open-source code to humans only; (b) we suggest revisions tothe Massachusetts Institute of Technology (\u0026ldquo;MIT\u0026rdquo;) license so that AI systemsare required to procure appropriate licenses from open-source code developers,which we believe will harmonize standards and build social consensus for thebenefit of all of humanity, rather than promote profit-driven centers ofinnovation; (c) we call for urgent legislative action to protect the future ofAI systems while also promoting innovation; and (d) we propose a shift in theburden of proof to AI systems in obfuscation cases.\r2024-01-28\nPrivacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\nXinyu Tang Richard Shin Huseyin A. Inan Andre Manoel Fatemehsadat Mireshghallah Zinan Lin Sivakanth Gopi Janardhan Kulkarni Robert Sim\nabstract\rabstract: We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.\r2024-01-27\nFortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models\nYunhong He Jianling Qiu Wei Zhang Zhengqing Yuan\nabstract\rabstract: Recent advancements in large language models (LLMs) have significantlyenhanced capabilities in natural language processing and artificialintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionizedtext generation, translation, and question-answering tasks due to thetransformative Transformer model. Despite their widespread use, LLMs presentchallenges such as ethical dilemmas when models are compelled to respondinappropriately, susceptibility to phishing attacks, and privacy violations.This paper addresses these challenges by introducing a multi-pronged approachthat includes: 1) filtering sensitive vocabulary from user input to preventunethical responses; 2) detecting role-playing to halt interactions that couldlead to \u0026lsquo;prison break\u0026rsquo; scenarios; 3) implementing custom rule engines torestrict the generation of prohibited content; and 4) extending thesemethodologies to various LLM derivatives like Multi-Model Large Language Models(MLLMs). Our approach not only fortifies models against unethical manipulationsand privacy breaches but also maintains their high performance across tasks. Wedemonstrate state-of-the-art performance under various attack prompts, withoutcompromising the model\u0026rsquo;s core functionalities. Furthermore, the introduction ofdifferentiated security levels empowers users to control their personal datadisclosure. Our methods contribute to reducing social risks and conflictsarising from technological abuse, enhance data protection, and promote socialequity. Collectively, this research provides a framework for balancing theefficiency of question-answering systems with user privacy and ethicalstandards, ensuring a safer user experience and fostering trust in AItechnology.\r2024-01-26\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly\nYifan Yao Jinhao Duan Kaidi Xu Yuanfang Cai Zhibo Sun Yue Zhang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into \u0026ldquo;The Good\u0026rdquo; (beneficial LLM applications),\u0026ldquo;The Bad\u0026rdquo; (offensive applications), and \u0026ldquo;The Ugly\u0026rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs\u0026rsquo; potential to both bolster and jeopardize cybersecurity.\r2024-01-23\nA Survey of Text Watermarking in the Era of Large Language Models\nAiwei Liu Leyi Pan Yijian Lu Jingjing Li Xuming Hu Xi Zhang Lijie Wen Irwin King Hui Xiong Philip S. Yu\nabstract\rabstract: Text watermarking algorithms play a crucial role in the copyright protectionof textual content, yet their capabilities and application scenarios have beenlimited historically. The recent developments in large language models (LLMs)have opened new opportunities for the advancement of text watermarkingtechniques. LLMs not only enhance the capabilities of text watermarkingalgorithms through their text understanding and generation abilities but alsonecessitate the use of text watermarking algorithms for their own copyrightprotection. This paper conducts a comprehensive survey of the current state oftext watermarking technology, covering four main aspects: (1) an overview andcomparison of different text watermarking techniques; (2) evaluation methodsfor text watermarking algorithms, including their success rates, impact on textquality, robustness, and unforgeability; (3) potential application scenariosfor text watermarking technology; (4) current challenges and future directionsfor development. This survey aims to provide researchers with a thoroughunderstanding of text watermarking technology, thereby promoting its furtheradvancement.\r2024-01-21\nInstructional Fingerprinting of Large Language Models\nJiashu Xu Fei Wang Mingyu Derek Ma Pang Wei Koh Chaowei Xiao Muhao Chen\nabstract\rabstract: The exorbitant cost of training Large language models (LLMs) from scratchmakes it essential to fingerprint the models to protect intellectual propertyvia ownership authentication and to ensure downstream users and developerscomply with their license terms (e.g. restricting commercial use). In thisstudy, we present a pilot study on LLM fingerprinting as a form of verylightweight instruction tuning. Model publisher specifies a confidentialprivate key and implants it as an instruction backdoor that causes the LLM togenerate specific text when the key is present. Results on 11 popularly-usedLLMs showed that this approach is lightweight and does not affect the normalbehavior of the model. It also prevents publisher overclaim, maintainsrobustness against fingerprint guessing and parameter-efficient training, andsupports multi-stage fingerprinting akin to MIT License. Code is available inhttps://cnut1648.github.io/Model-Fingerprint/.\r2024-01-18\nSilent Guardian: Protecting Text from Malicious Exploitation by Large Language Models\nJiawei Zhao Kejiang Chen Xiaojian Yuan Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.\r2024-01-11\nNavigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The advent of Generative AI has marked a significant milestone in artificialintelligence, demonstrating remarkable capabilities in generating realisticimages, texts, and data patterns. However, these advancements come withheightened concerns over data privacy and copyright infringement, primarily dueto the reliance on vast datasets for model training. Traditional approacheslike differential privacy, machine unlearning, and data poisoning only offerfragmented solutions to these complex issues. Our paper delves into themultifaceted challenges of privacy and copyright protection within the datalifecycle. We advocate for integrated approaches that combines technicalinnovation with ethical foresight, holistically addressing these concerns byinvestigating and devising solutions that are informed by the lifecycleperspective. This work aims to catalyze a broader discussion and inspireconcerted efforts towards data privacy and copyright integrity in GenerativeAI.\r2024-01-06\nMalla: Demystifying Real-world Large Language Model Integrated Malicious Services\nZilong Lin Jian Cui Xiaojing Liao XiaoFeng Wang\nabstract\rabstract: The underground exploitation of large language models (LLMs) for maliciousservices (i.e., Malla) is witnessing an uptick, amplifying the cyber threatlandscape and posing questions about the trustworthiness of LLM technologies.However, there has been little effort to understand this new cybercrime, interms of its magnitude, impact, and techniques. In this paper, we conduct thefirst systematic study on 212 real-world Mallas, uncovering their proliferationin underground marketplaces and exposing their operational modalities. Ourstudy discloses the Malla ecosystem, revealing its significant growth andimpact on today\u0026rsquo;s public LLM services. Through examining 212 Mallas, weuncovered eight backend LLMs used by Mallas, along with 182 prompts thatcircumvent the protective measures of public LLM APIs. We further demystify thetactics employed by Mallas, including the abuse of uncensored LLMs and theexploitation of public LLM APIs through jailbreak prompts. Our findings enablea better understanding of the real-world exploitation of LLMs bycybercriminals, offering insights into strategies to counteract thiscybercrime.\r2024-01-05\nGeoLocator: a location-integrated large multimodal model for inferring geo-privacy\nYifan Yang Siqin Wang Daoyang Li Yixian Zhang Shuju Sun Junzhou He\nabstract\rabstract: Geographic privacy or geo-privacy refers to the keeping private of one\u0026rsquo;sgeographic location, especially the restriction of geographical data maintainedby personal electronic devices. Geo-privacy is a crucial aspect of personalsecurity; however, it often goes unnoticed in daily activities. With the surgein the use of Large Multimodal Models (LMMs), such as GPT-4, for Open SourceIntelligence (OSINT), the potential risks associated with geo-privacy breacheshave intensified. This study develops a location-integrated GPT-4 based modelnamed GeoLocator and designs four-dimensional experiments to demonstrate itscapability in inferring the locational information of input imageries and/orsocial media contents. Our experiments reveal that GeoLocator generatesspecific geographic details with high accuracy and consequently embeds the riskof the model users exposing geospatial information to the publicunintentionally, highlighting the thread of online data sharing, informationgathering technologies and LLMs on geo-privacy. We conclude with the broaderimplications of GeoLocator and our findings for individuals and the communityat large, by emphasizing the urgency for enhanced awareness and protectivemeasures against geo-privacy leakage in the era of advanced AI and widespreadsocial media usage.\r2024-01-01\nTaking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education\nArne Bewersdorff Christian Hartmann Marie Hornberger Kathrin Seßler Maria Bannert Enkelejda Kasneci Gjergji Kasneci Xiaoming Zhai Claudia Nerdel\nabstract\rabstract: The integration of Artificial Intelligence (AI), particularly Large LanguageModel (LLM)-based systems, in education has shown promise in enhancing teachingand learning experiences. However, the advent of Multimodal Large LanguageModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processingmultimodal data including text, sound, and visual inputs, opens a new era ofenriched, personalized, and interactive learning landscapes in education.Grounded in theory of multimedia learning, this paper explores thetransformative role of MLLMs in central aspects of science education bypresenting exemplary innovative learning scenarios. Possible applications forMLLMs could range from content creation to tailored support for learning,fostering competencies in scientific practices, and providing assessment andfeedback. These scenarios are not limited to text-based and uni-modal formatsbut can be multimodal, increasing thus personalization, accessibility, andpotential learning effectiveness. Besides many opportunities, challenges suchas data protection and ethical considerations become more salient, calling forrobust frameworks to ensure responsible integration. This paper underscores thenecessity for a balanced approach in implementing MLLMs, where the technologycomplements rather than supplants the educator\u0026rsquo;s role, ensuring thus aneffective and ethical use of AI in science education. It calls for furtherresearch to explore the nuanced implications of MLLMs on the evolving role ofeducators and to extend the discourse beyond science education to otherdisciplines. Through the exploration of potentials, challenges, and futureimplications, we aim to contribute to a preliminary understanding of thetransformative trajectory of MLLMs in science education and beyond.\rDigger: Detecting Copyright Content Mis-usage in Large Language Model Training\nHaodong Li Gelei Deng Yi Liu Kailong Wang Yuekang Li Tianwei Zhang Yang Liu Guoai Xu Guosheng Xu Haoyu Wang\nabstract\rabstract: Pre-training, which utilizes extensive and varied datasets, is a criticalfactor in the success of Large Language Models (LLMs) across numerousapplications. However, the detailed makeup of these datasets is often notdisclosed, leading to concerns about data security and potential misuse. Thisis particularly relevant when copyrighted material, still under legalprotection, is used inappropriately, either intentionally or unintentionally,infringing on the rights of the authors. In this paper, we introduce a detailed framework designed to detect andassess the presence of content from potentially copyrighted books within thetraining datasets of LLMs. This framework also provides a confidence estimationfor the likelihood of each content sample\u0026rsquo;s inclusion. To validate ourapproach, we conduct a series of simulated experiments, the results of whichaffirm the framework\u0026rsquo;s effectiveness in identifying and addressing instances ofcontent misuse in LLM training processes. Furthermore, we investigate thepresence of recognizable quotes from famous literary works within thesedatasets. The outcomes of our study have significant implications for ensuringthe ethical use of copyrighted materials in the development of LLMs,highlighting the need for more transparent and responsible data managementpractices in this field.\r2023-12-31\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\nDipankar Sarkar\nabstract\rabstract: This paper aims to introduce and analyze the Viz system in a comprehensiveway, a novel system architecture that integrates Quantized Low-Rank Adapters(QLoRA) to fine-tune large language models (LLM) within a legally compliant andresource efficient marketplace. Viz represents a significant contribution tothe field of artificial intelligence, particularly in addressing the challengesof computational efficiency, legal compliance, and economic sustainability inthe utilization and monetization of LLMs. The paper delineates the scholarlydiscourse and developments that have informed the creation of Viz, focusingprimarily on the advancements in LLM models, copyright issues in AI training(NYT case, 2023), and the evolution of model fine-tuning techniques,particularly low-rank adapters and quantized low-rank adapters, to create asustainable and economically compliant framework for LLM utilization. Theeconomic model it proposes benefits content creators, AI developers, andend-users, delineating a harmonious integration of technology, economy, andlaw, offering a comprehensive solution to the complex challenges of today\u0026rsquo;s AIlandscape.\r2023-12-30\nSplit-and-Denoise: Protect large language model inference with local differential privacy\nPeihua Mai Ran Yan Zhe Huang Youjia Yang Yan Pang\nabstract\rabstract: Large Language Models (LLMs) shows powerful capability in natural languageunderstanding by capturing hidden semantics in vector space. This processenriches the value of the text embeddings for various downstream tasks, therebyfostering the Embedding-as-a-Service (EaaS) business model. However, the directtransmission of text to servers poses a largely unaddressed risk of privacyleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), aninnovative framework that split the model to execute the token embedding layeron the client side at minimal computational cost. This allows the client tointroduce noise prior to transmitting the embeddings to the server, andsubsequently receive and denoise the perturbed output embeddings for downstreamtasks. Our approach is designed for the inference stage of LLMs and requires nomodifications to the model parameters. Extensive experiments demonstrate SnD\u0026rsquo;seffectiveness in optimizing the privacy-utility tradeoff across various LLMarchitectures and diverse downstream tasks. The results reveal a significantperformance improvement under the same privacy budget compared to the baseline,offering clients a privacy-preserving solution for local privacy protection.\r2023-12-21\nDeID-GPT: Zero-shot Medical Text De-Identification by GPT-4\nZhengliang Liu Yue Huang Xiaowei Yu Lu Zhang Zihao Wu Chao Cao Haixing Dai Lin Zhao Yiwei Li Peng Shu Fang Zeng Lichao Sun Wei Liu Dinggang Shen Quanzheng Li Tianming Liu Dajiang Zhu Xiang Li\nabstract\rabstract: The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT\u0026quot;) to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.\r2023-12-18\nOpportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview\nLiang Zhang Zhelun Chen\nabstract\rabstract: In recent years, the rapid advancement and impressive capabilities of LargeLanguage Models (LLMs) have been evident across various domains. This paperexplores the application, implications, and potential of LLMs in buildingenergy efficiency and decarbonization studies. The wide-ranging capabilities ofLLMs are examined in the context of the building energy field, includingintelligent control systems, code generation, data infrastructure, knowledgeextraction, and education. Despite the promising potential of LLMs, challengesincluding complex and expensive computation, data privacy, security andcopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.The paper concludes with a call for future research focused on the enhancementof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative researchbetween AI and energy experts.\r2023-12-15\nPrivacy-Aware Document Visual Question Answering\nRubèn Tito Khanh Nguyen Marlon Tobaben Raouf Kerkouche Mohamed Ali Souibgui Kangsoo Jung Lei Kang Ernest Valveny Antti Honkela Mario Fritz Dimosthenis Karatzas\nabstract\rabstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.\r2023-12-12\nEditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection\nXuanyu Zhang Runyi Li Jiwen Yu Youmin Xu Weiqi Li Jian Zhang\nabstract\rabstract: In the era where AI-generated content (AIGC) models can produce stunning andlifelike images, the lingering shadow of unauthorized reproductions andmalicious tampering poses imminent threats to copyright integrity andinformation security. Current image watermarking methods, while widely acceptedfor safeguarding visual content, can only protect copyright and ensuretraceability. They fall short in localizing increasingly realistic imagetampering, potentially leading to trust crises, privacy violations, and legaldisputes. To solve this challenge, we propose an innovative proactive forensicsframework EditGuard, to unify copyright protection and tamper-agnosticlocalization, especially for AIGC-based editing methods. It can offer ameticulous embedding of imperceptible watermarks and precise decoding oftampered areas and copyright information. Leveraging our observed fragility andlocality of image-into-image steganography, the realization of EditGuard can beconverted into a united image-bit steganography issue, thus completelydecoupling the training process from the tampering types. Extensive experimentsdemonstrate that our EditGuard balances the tamper localization accuracy,copyright recovery precision, and generalizability to various AIGC-basedtampering methods, especially for image forgery that is difficult for the nakedeye to detect. The project page is available athttps://xuanyuzhang21.github.io/project/editguard/.\r2023-12-11\nInferDPT: Privacy-Preserving Inference for Black-box Large Language Model\nMeng Tong Kejiang Chen Jie Zhang Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: Large language models (LLMs), like ChatGPT, have greatly simplified textgeneration tasks. However, they have also raised concerns about privacy riskssuch as data leakage and unauthorized data collection. Existing solutions forprivacy-preserving inference face practical challenges related to computationtime and communication costs. In this paper, we propose InferDPT, the firstpractical framework for the privacy-preserving Inference of black-box LLMs,implementing Differential Privacy in Text generation. InferDPT comprises twokey modules: the \u0026ldquo;perturbation module\u0026rdquo; utilizes the exponential mechanism togenerate a perturbed prompt, facilitating privacy-preserving inference withblack-box LLMs, and the \u0026ldquo;extraction module\u0026rdquo;, inspired by knowledge distillationand retrieval-augmented generation, extracts coherent and consistent text fromthe perturbed generation result, ensuring successful text generationcompletion. To address privacy concerns related to previous exponentialmechanisms\u0026rsquo; susceptibility to embedding revision attacks, we introduce RANTEXT,a novel differential privacy mechanism integrated into the perturbation moduleof InferDPT, which introduces the concept of \u0026ldquo;RANdom adjacency\u0026rdquo; for TEXTperturbation within the prompt. Experimental results across three datasetsdemonstrate that the text generation quality of InferDPT is comparable to thatof non-private GPT-4, and RANTEXT surpasses existing state-of-the-artmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy andutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achievesan average privacy protection rate exceeding 90% against embedding revisionattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higherthan that of CUSTEXT+.\r2023-12-07\nDefense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks\nXiaobei Yan Chip Hong Chang Tianwei Zhang\nabstract\rabstract: Artificial Intelligence (AI) hardware accelerators have been widely adoptedto enhance the efficiency of deep learning applications. However, they alsoraise security concerns regarding their vulnerability to power side-channelattacks (SCA). In these attacks, the adversary exploits unintendedcommunication channels to infer sensitive information processed by theaccelerator, posing significant privacy and copyright risks to the models.Advanced machine learning algorithms are further employed to facilitate theside-channel analysis and exacerbate the privacy issue of AI accelerators.Traditional defense strategies naively inject execution noise to the runtime ofAI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguardFPGA-based AI accelerators and mitigate model extraction threats viapower-based SCAs. The key insight of AIAShield is to leverage the prominentadversarial attack technique from the machine learning community to craftdelicate noise, which can significantly obfuscate the adversary\u0026rsquo;s side-channelobservation while incurring minimal overhead to the execution of the protectedmodel. At the hardware level, we design a new module based on ring oscillatorsto achieve fine-grained noise generation. At the algorithm level, we repurposeNeural Architecture Search to worsen the adversary\u0026rsquo;s extraction results.Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA)demonstrate that AIAShield outperforms existing solutions with excellenttransferability.\r2023-12-04\nResponsible Task Automation: Empowering Large Language Models as Responsible Task Automators\nZhizheng Zhang Xiaoyi Zhang Wenxuan Xie Yan Lu\nabstract\rabstract: The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users\u0026rsquo; privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios.\r2023-11-30\nCan Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?\nZhengyue Zhao Jinhao Duan Kaidi Xu Chenan Wang Rui Zhangp Zidong Dup Qi Guo Xing Hu\nabstract\rabstract: Stable Diffusion has established itself as a foundation model in generativeAI artistic applications, receiving widespread research and application. Somerecent fine-tuning methods have made it feasible for individuals to implantpersonalized concepts onto the basic Stable Diffusion model with minimalcomputational costs on small datasets. However, these innovations have alsogiven rise to issues like facial privacy forgery and artistic copyrightinfringement. In recent studies, researchers have explored the addition ofimperceptible adversarial perturbations to images to prevent potentialunauthorized exploitation and infringements when personal data is used forfine-tuning Stable Diffusion. Although these studies have demonstrated theability to protect images, it is essential to consider that these methods maynot be entirely applicable in real-world scenarios. In this paper, wesystematically evaluate the use of perturbations to protect images within apractical threat model. The results suggest that these approaches may not besufficient to safeguard image privacy and copyright effectively. Furthermore,we introduce a purification method capable of removing protected perturbationswhile preserving the original image structure to the greatest extent possible.Experiments reveal that Stable Diffusion can effectively learn from purifiedimages over all protective methods.\rTrustMark: Universal Watermarking for Arbitrary Resolution Images\nTu Bui Shruti Agarwal John Collomosse\nabstract\rabstract: Imperceptible digital watermarking is important in copyright protection,misinformation prevention, and responsible generative AI. We propose TrustMark- a GAN-based watermarking method with novel design in architecture andspatio-spectra losses to balance the trade-off between watermarked imagequality with the watermark recovery accuracy. Our model is trained withrobustness in mind, withstanding various in- and out-place perturbations on theencoded image. Additionally, we introduce TrustMark-RM - a watermark removermethod useful for re-watermarking. Our methods achieve state-of-art performanceon 3 benchmarks comprising arbitrary resolution images.\r2023-11-28\nPromptCARE: Prompt Copyright Protection by Watermark Injection and Verification\nHongwei Yao Jian Lou Kui Ren Zhan Qin\nabstract\rabstract: Large language models (LLMs) have witnessed a meteoric rise in popularityamong the general public users over the past few months, facilitating diversedownstream tasks with human-level accuracy and proficiency. Prompts play anessential role in this success, which efficiently adapt pre-trained LLMs totask-specific applications by simply prepending a sequence of tokens to thequery texts. However, designing and selecting an optimal prompt can be bothexpensive and demanding, leading to the emergence of Prompt-as-a-Serviceproviders who profit by providing well-designed prompts for authorized use.With the growing popularity of prompts and their indispensable role inLLM-based services, there is an urgent need to protect the copyright of promptsagainst unauthorized use. In this paper, we propose PromptCARE, the first framework for promptcopyright protection through watermark injection and verification. Promptwatermarking presents unique challenges that render existing watermarkingtechniques developed for model and dataset copyright verification ineffective.PromptCARE overcomes these hurdles by proposing watermark injection andverification schemes tailor-made for prompts and NLP characteristics. Extensiveexperiments on six well-known benchmark datasets, using three prevalentpre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate theeffectiveness, harmlessness, robustness, and stealthiness of PromptCARE.\rPCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Models\nXuefeng Fan Dahao Fu Hangyu Gui Xinpeng Zhang Xiaoyi Zhou\nabstract\rabstract: Deep neural networks (DNNs) have achieved tremendous success in artificialintelligence (AI) fields. However, DNN models can be easily illegally copied,redistributed, or abused by criminals, seriously damaging the interests ofmodel inventors. The copyright protection of DNN models by neural networkwatermarking has been studied, but the establishment of a traceabilitymechanism for determining the authorized users of a leaked model is a newproblem driven by the demand for AI services. Because the existing traceabilitymechanisms are used for models without watermarks, a small number offalse-positives are generated. Existing black-box active protection schemeshave loose authorization control and are vulnerable to forgery attacks.Therefore, based on the idea of black-box neural network watermarking with thevideo framing and image perceptual hash algorithm, a passive copyrightprotection and traceability framework PCPT is proposed that uses an additionalclass of DNN models, improving the existing traceability mechanism that yieldsa small number of false-positives. Based on an authorization control strategyand image perceptual hash algorithm, a DNN model active copyright protectionand traceability framework ACPT is proposed. This framework uses theauthorization control center constructed by the detector and verifier. Thisapproach realizes stricter authorization control, which establishes a strongconnection between users and model owners, improves the framework security, andsupports traceability verification.\r2023-11-27\nTokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform\nYihao Li Yanyi Lai Tianchi Liao Chuan Chen Zibin Zheng\nabstract\rabstract: With the development of practical deep learning models like generative AI,their excellent performance has brought huge economic value. For instance,ChatGPT has attracted more than 100 million users in three months. Since themodel training requires a lot of data and computing power, a well-performingdeep learning model is behind a huge effort and cost. Facing various modelattacks, unauthorized use and abuse from the network that threaten theinterests of model owners, in addition to considering legal and otheradministrative measures, it is equally important to protect the model\u0026rsquo;scopyright from the technical means. By using the model watermarking technology,we point out the possibility of building a unified platform for model ownershipverification. Given the application history of blockchain in copyrightverification and the drawbacks of a centralized third-party, this paperconsiders combining model watermarking technology and blockchain to build aunified model copyright protection platform. By a new solution we calledTokenized Model, it protects the model\u0026rsquo;s copyright by reliable ownership recordand verification mechanism. It also promotes the financial value of model byconstructing the model\u0026rsquo;s transaction process and contribution shares of amodel. In the typical case study, we also study the various performance underusual scenario to verify the effectiveness of this platform.\rDP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\nJunyuan Hong Jiachen T. Wang Chenhui Zhang Zhangheng Li Bo Li Zhangyang Wang\nabstract\rabstract: Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts\u0026rsquo; dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model\u0026rsquo;s provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\r2023-11-24\nInput Reconstruction Attack against Vertical Federated Large Language Models\nFei Zheng\nabstract\rabstract: Recently, large language models (LLMs) have drawn extensive attention fromacademia and the public, due to the advent of the ChatGPT. While LLMs showtheir astonishing ability in text generation for various tasks, privacyconcerns limit their usage in real-life businesses. More specifically, eitherthe user\u0026rsquo;s inputs (the user sends the query to the model-hosting server) or themodel (the user downloads the complete model) itself will be revealed duringthe usage. Vertical federated learning (VFL) is a promising solution to thiskind of problem. It protects both the user\u0026rsquo;s input and the knowledge of themodel by splitting the model into a bottom part and a top part, which ismaintained by the user and the model provider, respectively. However, in thispaper, we demonstrate that in LLMs, VFL fails to protect the user input sinceit is simple and cheap to reconstruct the input from the intermediateembeddings. Experiments show that even with a commercial GPU, the inputsentence can be reconstructed in only one second. We also discuss severalpossible solutions to enhance the privacy of vertical federated LLMs.\r2023-11-23\nMARBLE: Music Audio Representation Benchmark for Universal Evaluation\nRuibin Yuan Yinghao Ma Yizhi Li Ge Zhang Xingran Chen Hanzhi Yin Le Zhuo Yiqi Liu Jiawen Huang Zeyue Tian Binyue Deng Ningzhi Wang Chenghua Lin Emmanouil Benetos Anton Ragni Norbert Gyenge Roger Dannenberg Wenhu Chen Gus Xia Wei Xue Si Liu Shi Wang Ruibo Liu Yike Guo Jie Fu\nabstract\rabstract: In the era of extensive intersection between art and Artificial Intelligence(AI), such as image generation and fiction co-creation, AI for music remainsrelatively nascent, particularly in music understanding. This is evident in thelimited work on deep music representations, the scarcity of large-scaledatasets, and the absence of a universal and community-driven benchmark. Toaddress this issue, we introduce the Music Audio Representation Benchmark foruniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for variousMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomywith four hierarchy levels, including acoustic, performance, score, andhigh-level description. We then establish a unified protocol based on 14 taskson 8 public-available datasets, providing a fair and standard assessment ofrepresentations of all open-sourced pre-trained models developed on musicrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, andreproducible suite for the community, with a clear statement on copyrightissues on datasets. Results suggest recently proposed large-scale pre-trainedmusical language models perform the best in most tasks, with room for furtherimprovement. The leaderboard and toolkit repository are published athttps://marble-bm.shef.ac.uk to promote future music AI research.\r2023-11-17\nFunctionMarker: Watermarking Language Datasets via Knowledge Injection\nShuai Li Kejiang Chen Kunsheng Tang Wen Huang Jie Zhang Weiming Zhang Nenghai Yu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated superior performance invarious natural language processing tasks. Meanwhile, they require extensivetraining data, raising concerns related to dataset copyright protection.Backdoor-based watermarking is a viable approach to protect the copyright ofclassification datasets. However, these methods may introduce maliciousmisclassification behaviors into watermarked LLMs by attackers and also affectthe semantic information of the watermarked text. To address these issues, wepropose FunctionMarker, a novel copyright protection method for languagedatasets via knowledge injection. FunctionMarker enables LLMs to learn specificknowledge through fine-tuning on watermarked datasets, and we can extract theembedded watermark by obtaining the responses of LLMs to specificknowledge-related queries. Considering watermark capacity and stealthness, weselect customizable functions as specific knowledge for LLMs to learn and embedthe watermark into them. Moreover, FunctionMarker can embed multi-bitwatermarks while preserving the original semantic information, therebyincreasing the difficulty of adaptive attacks. We take mathematical functionsas an instance to evaluate the effectiveness of FunctionMarker, and experimentsshow that only 0.3% of watermarked text achieves a 90% watermark extractionaccuracy in most cases, validating our method\u0026rsquo;s effectiveness.\r2023-11-15\nAn Empathetic User-Centric Chatbot for Emotional Support\nYanting Pan Yixuan Tang Yuchen Niu\nabstract\rabstract: This paper explores the intersection of Otome Culture and artificialintelligence, particularly focusing on how Otome-oriented games fulfill theemotional needs of young women. These games, which are deeply rooted in asubcultural understanding of love, provide players with feelings ofsatisfaction, companionship, and protection through carefully crafted narrativestructures and character development. With the proliferation of Large LanguageModels (LLMs), there is an opportunity to transcend traditional static gamenarratives and create dynamic, emotionally responsive interactions. We presenta case study of Tears of Themis, where we have integrated LLM technology toenhance the interactive experience. Our approach involves augmenting existinggame narratives with a Question and Answer (QA) system, enriched through dataaugmentation and emotional enhancement techniques, resulting in a chatbot thatoffers realistic and supportive companionship.\rValue FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values\nJing Yao Xiaoyuan Yi Xiting Wang Yifan Gong Xing Xie\nabstract\rabstract: The rapid advancement of Large Language Models (LLMs) has attracted muchattention to value alignment for their responsible development. However, how todefine values in this context remains a largely unexplored question. Existingwork mainly follows the Helpful, Honest, Harmless principle and specifiesvalues as risk criteria formulated in the AI community, e.g., fairness andprivacy protection, suffering from poor clarity, adaptability and transparency.Inspired by basic values in humanity and social science across cultures, thiswork proposes a novel basic value alignment paradigm and introduces a valuespace spanned by basic value dimensions. All LLMs\u0026rsquo; behaviors can be mapped intothe space by identifying the underlying values, possessing the potential toaddress the three challenges. To foster future research, we apply therepresentative Schwartz\u0026rsquo;s Theory of Basic Values as an initialized example andconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.Our extensive analysis of FULCRA reveals the underlying relation between basicvalues and LLMs\u0026rsquo; behaviors, demonstrating that our approach not only coversexisting mainstream risks but also anticipates possibly unidentified ones.Additionally, we present an initial implementation of the basic valueevaluation and alignment, paving the way for future research in this line.\r2023-11-14\nLatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud\nMengke Zhang Tianxing He Tianle Wang Lu Mi Fatemehsadat Mireshghallah Binyi Chen Hao Wang Yulia Tsvetkov\nabstract\rabstract: In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).\r2023-11-12\nFlames: Benchmarking Value Alignment of Chinese Large Language Models\nKexin Huang Xiangyang Liu Qianyu Guo Tianxiang Sun Jiawei Sun Yaru Wang Zeyang Zhou Yixu Wang Yan Teng Xipeng Qiu Yingchun Wang Dahua Lin\nabstract\rabstract: The widespread adoption of large language models (LLMs) across variousregions underscores the urgent need to evaluate their alignment with humanvalues. Current benchmarks, however, fall short of effectively uncoveringsafety vulnerabilities in LLMs. Despite numerous models achieving high scoresand \u0026rsquo;topping the chart\u0026rsquo; in these evaluations, there is still a significant gapin LLMs\u0026rsquo; deeper alignment with human values and achieving genuine harmlessness.To this end, this paper proposes the first highly adversarial benchmark namedFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responseswith fine-grained annotations, and a specified scorer. Our frameworkencompasses both common harmlessness principles, such as fairness, safety,legality, and data protection, and a unique morality dimension that integratesspecific Chinese values such as harmony. Based on the framework, we carefullydesign adversarial prompts that incorporate complex scenarios and jailbreakingmethods, mostly with implicit malice. By prompting mainstream LLMs with suchadversarially constructed prompts, we obtain model responses, which are thenrigorously annotated for evaluation. Our findings indicate that all theevaluated LLMs demonstrate relatively poor performance on Flames, particularlyin the safety and fairness dimensions. Claude emerges as the best-performingmodel overall, but with its harmless rate being only 63.08% while GPT-4 onlyscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,setting a new challenge for contemporary LLMs and highlighting the need forfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,we develop a specified scorer capable of scoring LLMs across multipledimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publiclyavailable on https://github.com/AIFlames/Flames.\r2023-11-10\nRemoving RLHF Protections in GPT-4 via Fine-Tuning\nQiusi Zhan Richard Fang Rohan Bindu Akul Gupta Tatsunori Hashimoto Daniel Kang\nabstract\rabstract: As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendorsof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to removeRLHF protections with as few as 340 examples and a 95% success rate. Thesetraining examples can be automatically generated with weaker models. We furthershow that removing RLHF protections does not decrease usefulness onnon-censored outputs, providing evidence that our fine-tuning strategy does notdecrease usefulness despite using weaker models to generate training data. Ourresults show the need for further research on protections on LLMs.\r2023-11-09\nChatbots Are Not Reliable Text Annotators\nRoss Deans Kristensen-McLachlan Miceal Canavan Márton Kardos Mia Jacobsen Lene Aarøe\nabstract\rabstract: Recent research highlights the significant potential of ChatGPT for textannotation in social science research. However, ChatGPT is a closed-sourceproduct which has major drawbacks with regards to transparency,reproducibility, cost, and data protection. Recent advances in open-source (OS)large language models (LLMs) offer alternatives which remedy these challenges.This means that it is important to evaluate the performance of OS LLMs relativeto ChatGPT and standard approaches to supervised machine learningclassification. We conduct a systematic comparative evaluation of theperformance of a range of OS LLM models alongside ChatGPT, using both zero- andfew-shot learning as well as generic and custom prompts, with results comparedto more traditional supervised classification models. Using a new dataset ofTweets from US news media, and focusing on simple binary text annotation tasksfor standard social science concepts, we find significant variation in theperformance of ChatGPT and OS models across the tasks, and that supervisedclassifiers consistently outperform both. Given the unreliable performance ofChatGPT and the significant challenges it poses to Open Science we adviseagainst using ChatGPT for substantive text annotation tasks in social scienceresearch.\rEnhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT\nJingye Yang Cong Liu Wendy Deng Da Wu Chunhua Weng Yunyun Zhou Kai Wang\nabstract\rabstract: We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.\r2023-11-06\nFindings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs\nLongyue Wang Zhaopeng Tu Yan Gu Siyou Liu Dian Yu Qingsong Ma Chenyang Lyu Liting Zhou Chao-Hong Liu Yufeng Ma Weiyu Chen Yvette Graham Bonnie Webber Philipp Koehn Andy Way Yulin Yuan Shuming Shi\nabstract\rabstract: Translating literary works has perennially stood as an elusive dream inmachine translation (MT), a journey steeped in intricate challenges. To fosterprogress in this domain, we hold a new shared task at WMT 2023, the firstedition of the Discourse-Level Literary Translation. First, we (Tencent AI Laband China Literature Ltd.) release a copyrighted and document-levelChinese-English web novel corpus. Furthermore, we put forth anindustry-endorsed criteria to guide human evaluation process. This year, wetotally received 14 submissions from 7 academia and industry teams. We employboth automatic and human evaluations to measure the performance of thesubmitted systems. The official ranking of the systems is based on the overallhuman judgments. In addition, our extensive analysis reveals a series ofinteresting findings on literary and discourse-aware MT. We release data,system outputs, and leaderboard athttp://www2.statmt.org/wmt23/literary-translation-task.html.\r2023-11-05\nTag Your Fish in the Broken Net: A Responsible Web Framework for Protecting Online Privacy and Copyright\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The World Wide Web, a ubiquitous source of information, serves as a primaryresource for countless individuals, amassing a vast amount of data from globalinternet users. However, this online data, when scraped, indexed, and utilizedfor activities like web crawling, search engine indexing, and, notably, AImodel training, often diverges from the original intent of its contributors.The ascent of Generative AI has accentuated concerns surrounding data privacyand copyright infringement. Regrettably, the web\u0026rsquo;s current framework fallsshort in facilitating pivotal actions like consent withdrawal or data copyrightclaims. While some companies offer voluntary measures, such as crawler accessrestrictions, these often remain inaccessible to individual users. To empoweronline users to exercise their rights and enable companies to adhere toregulations, this paper introduces a user-controlled consent tagging frameworkfor online data. It leverages the extensibility of HTTP and HTML in conjunctionwith the decentralized nature of distributed ledger technology. With thisframework, users have the ability to tag their online data at the time oftransmission, and subsequently, they can track and request the withdrawal ofconsent for their data from the data holders. A proof-of-concept system isimplemented, demonstrating the feasibility of the framework. This work holdssignificant potential for contributing to the reinforcement of user consent,privacy, and copyright on the modern internet and lays the groundwork forfuture insights into creating a more responsible and user-centric webecosystem.\r2023-11-03\nDetecting Pretraining Data from Large Language Models\nWeijia Shi Anirudh Ajith Mengzhou Xia Yangsibo Huang Daogao Liu Terra Blevins Danqi Chen Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\r2023-11-01\nTowards Legally Enforceable Hate Speech Detection for Public Forums\nChu Fei Luo Rohan Bhambhoria Xiaodan Zhu Samuel Dahan\nabstract\rabstract: Hate speech causes widespread and deep-seated societal issues. Properenforcement of hate speech laws is key for protecting groups of people againstharmful and discriminatory language. However, determining what constitutes hatespeech is a complex task that is highly open to subjective interpretations.Existing works do not align their systems with enforceable definitions of hatespeech, which can make their outputs inconsistent with the goals of regulators.This research introduces a new perspective and task for enforceable hate speechdetection centred around legal definitions, and a dataset annotated onviolations of eleven possible definitions by legal experts. Given the challengeof identifying clear, legally enforceable instances of hate speech, we augmentthe dataset with expert-generated samples and an automatically mined challengeset. We experiment with grounding the model decision in these definitions usingzero-shot and few-shot prompting. We then report results on several largelanguage models (LLMs). With this task definition, automatic hate speechdetection can be more closely aligned to enforceable laws, and hence assist inmore rigorous enforcement of legal protections against harmful speech in publicforums.\r2023-10-31\nUnlearn What You Want to Forget: Efficient Unlearning for LLMs\nJiaao Chen Diyi Yang\nabstract\rabstract: Large language models (LLMs) have achieved significant progress frompre-training on and memorizing a wide range of textual data, however, thisprocess might suffer from privacy issues and violations of data protectionregulations. As a result, the ability to easily remove data related toindividual users from such models while not deteriorating their predictivequality after the removal becomes increasingly important. To address theseissues, in this work, we propose an efficient unlearning framework that couldefficiently update LLMs without having to retrain the whole model after dataremovals, by introducing lightweight unlearning layers learned with a selectiveteacher-student objective into the transformers. In addition, we introduce afusion mechanism to effectively combine different unlearning layers that learnsto forget different sets of data to handle a sequence of forgetting operations.Experiments on classification and generation tasks demonstrate theeffectiveness of our proposed methods compared to the state-of-the-artbaselines.\r2023-10-25\nPrivately Aligning Language Models with Reinforcement Learning\nFan Wu Huseyin A. Inan Arturs Backurs Varun Chandrasekaran Janardhan Kulkarni Robert Sim\nabstract\rabstract: Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.\r2023-10-24\nSoK: Memorization in General-Purpose Large Language Models\nValentin Hartmann Anshuman Suri Vincent Bindschaedler David Evans Shruti Tople Robert West\nabstract\rabstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.\r"},{"id":1,"href":"/docs/example/table-of-contents/with-toc/","title":"With ToC","section":"Table of Contents","content":"\rCaput vino delphine in tamen vias\r#\rCognita laeva illo fracta\r#\rLorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\nTe at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit Natus quaerere\r#\rPectora et sine mulcere, coniuge dum tincta incurvae. Quis iam; est dextra Peneosque, metuis a verba, primo. Illa sed colloque suis: magno: gramen, aera excutiunt concipit.\nPhrygiae petendo suisque extimuit, super, pars quod audet! Turba negarem. Fuerat attonitus; et dextra retinet sidera ulnas undas instimulat vacuae generis? Agnus dabat et ignotis dextera, sic tibi pacis feriente at mora euhoeque comites hostem vestras Phineus. Vultuque sanguine dominoque metuit risi fama vergit summaque meus clarissimus artesque tinguebat successor nominis cervice caelicolae.\nLimitibus misere sit\r#\rAurea non fata repertis praerupit feruntur simul, meae hosti lentaque citius levibus, cum sede dixit, Phaethon texta. Albentibus summos multifidasque iungitur loquendi an pectore, mihi ursaque omnia adfata, aeno parvumque in animi perlucentes. Epytus agis ait vixque clamat ornum adversam spondet, quid sceptra ipsum est. Reseret nec; saeva suo passu debentia linguam terga et aures et cervix de ubera. Coercet gelidumque manus, doluit volvitur induta?\nEnim sua\r#\rIuvenilior filia inlustre templa quidem herbis permittat trahens huic. In cruribus proceres sole crescitque fata, quos quos; merui maris se non tamen in, mea.\nGermana aves pignus tecta\r#\rMortalia rudibusque caelum cognosceret tantum aquis redito felicior texit, nec, aris parvo acre. Me parum contulerant multi tenentem, gratissime suis; vultum tu occupat deficeret corpora, sonum. E Actaea inplevit Phinea concepit nomenque potest sanguine captam nulla et, in duxisses campis non; mercede. Dicere cur Leucothoen obitum?\nPostibus mittam est nubibus principium pluma, exsecratur facta et. Iunge Mnemonidas pallamque pars; vere restitit alis flumina quae quoque, est ignara infestus Pyrrha. Di ducis terris maculatum At sede praemia manes nullaque!\n"},{"id":2,"href":"/docs/example/table-of-contents/without-toc/","title":"Without ToC","section":"Table of Contents","content":"\rAt me ipso nepotibus nunc celebratior genus\r#\rTanto oblite\r#\rLorem markdownum pectora novis patenti igne sua opus aurae feras materiaque illic demersit imago et aristas questaque posset. Vomit quoque suo inhaesuro clara. Esse cumque, per referri triste. Ut exponit solisque communis in tendens vincetis agisque iamque huic bene ante vetat omina Thebae rates. Aeacus servat admonitu concidit, ad resimas vultus et rugas vultu dignamque Siphnon.\nQuam iugulum regia simulacra, plus meruit humo pecorumque haesit, ab discedunt dixit: ritu pharetramque. Exul Laurenti orantem modo, per densum missisque labor manibus non colla unum, obiectat. Tu pervia collo, fessus quae Cretenque Myconon crate! Tegumenque quae invisi sudore per vocari quaque plus ventis fluidos. Nodo perque, fugisse pectora sorores.\nSumme promissa supple vadit lenius\r#\rQuibus largis latebris aethera versato est, ait sentiat faciemque. Aequata alis nec Caeneus exululat inclite corpus est, ire tibi ostendens et tibi. Rigent et vires dique possent lumina; eadem dixit poma funeribus paret et felix reddebant ventis utile lignum.\nRemansit notam Stygia feroxque Et dabit materna Vipereas Phrygiaeque umbram sollicito cruore conlucere suus Quarum Elis corniger Nec ieiunia dixit Vertitur mos ortu ramosam contudit dumque; placabat ac lumen. Coniunx Amoris spatium poenamque cavernis Thebae Pleiadasque ponunt, rapiare cum quae parum nimium rima.\nQuidem resupinus inducto solebat una facinus quae\r#\rCredulitas iniqua praepetibus paruit prospexit, voce poena, sub rupit sinuatur, quin suum ventorumque arcadiae priori. Soporiferam erat formamque, fecit, invergens, nymphae mutat fessas ait finge.\nBaculum mandataque ne addere capiti violentior Altera duas quam hoc ille tenues inquit Sicula sidereus latrantis domoque ratae polluit comites Possit oro clausura namque se nunc iuvenisque Faciem posuit Quodque cum ponunt novercae nata vestrae aratra Ite extrema Phrygiis, patre dentibus, tonso perculit, enim blanda, manibus fide quos caput armis, posse! Nocendo fas Alcyonae lacertis structa ferarum manus fulmen dubius, saxa caelum effuge extremis fixum tumor adfecit bella, potentes? Dum nec insidiosa tempora tegit spirarunt. Per lupi pars foliis, porreximus humum negant sunt subposuere Sidone steterant auro. Memoraverit sine: ferrum idem Orion caelum heres gerebat fixis?\n"},{"id":3,"href":"/posts/creating-a-new-theme/","title":"Creating a New Theme","section":"Blog","content":"\rIntroduction\r#\rThis tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment\r$ echo this is a command\rthis is a command\r## edit the file\r$ vi foo.md\r+++\rdate = \u0026#34;2014-09-28\u0026#34;\rtitle = \u0026#34;creating a new theme\u0026#34;\r+++\rbah and humbug\r:wq\r## show it\r$ cat foo.md\r+++\rdate = \u0026#34;2014-09-28\u0026#34;\rtitle = \u0026#34;creating a new theme\u0026#34;\r+++\rbah and humbug\r$ Some Definitions\r#\rThere are a few concepts that you need to understand before creating a theme.\nSkins\r#\rSkins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page\r#\rThe home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File\r#\rWhen Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent\r#\rContent is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter\r#\rThe front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown\r#\rContent is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files\r#\rHugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template\r#\rA single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template\r#\rA list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template\r#\rA partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site\r#\rLet\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta\r$ cd ~/Sites/zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 7 quoha staff 238 Sep 29 16:49 .\rdrwxr-xr-x 3 quoha staff 102 Sep 29 16:49 ..\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site\r#\rRunning the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public\rtotal 16\r-rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml\r-rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml\r$ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site\r#\rVerify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml\rsitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme\r#\rHugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton\r#\rUse the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes\r$ find themes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r-rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml\r$ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml\rauthor = \u0026#34;michael d henderson\u0026#34;\rdescription = \u0026#34;a minimal working template\u0026#34;\rlicense = \u0026#34;MIT\u0026#34;\rname = \u0026#34;zafta\u0026#34;\rsource_repo = \u0026#34;\u0026#34;\rtags = [\u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34;]\r:wq\r## also edit themes/zafta/LICENSE.md and change\r## the bit that says \u0026#34;YOUR_NAME_HERE\u0026#34; Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r$ Update the Configuration File to Use the Theme\r#\rNow that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml\rtheme = \u0026#34;zafta\u0026#34;\rbaseurl = \u0026#34;\u0026#34;\rlanguageCode = \u0026#34;en-us\u0026#34;\rtitle = \u0026#34;zafta - totally refreshing\u0026#34;\rMetaDataFormat = \u0026#34;toml\u0026#34;\r:wq\r$ Generate the Site\r#\rNow that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public\rtotal 16\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html\r-rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js\r-rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml\r$ Notice four things:\nHugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically. The Home Page\r#\rHugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html\r$ The Magic of Static\r#\rHugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld\rdrwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes\rdrwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js\r$ The Theme Development Cycle\r#\rWhen you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\nPurge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory\r#\rWhen generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option\r#\rHugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload\r#\rHugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands\r#\rUse the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory.\r##\r$ rm -rf public\r##\r## run hugo in watch mode\r##\r$ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public\r$ hugo server --watch --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rWatching for changes in /Users/quoha/Sites/zafta/content\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rINFO: 2014/09/29 File System Event: [\u0026#34;/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\u0026#34;: MODIFY|ATTRIB]\rChange detected, rebuilding site\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 1 ms Update the Home Page Template\r#\rThe home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\nindex.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page\r#\rRight now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html\r\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq\r$ Build the web site and then verify the results.\n$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html\r$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/html\u0026gt; Live Reload\r#\rNote: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39; + (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0] + \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page\r#\r\u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts\r#\rNow that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md\rERROR: 2014/09/29 Unable to Cast \u0026lt;nil\u0026gt; to map[string]interface{}\r$ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md\r+++\rDescription = \u0026#34;\u0026#34;\rTags = []\rCategories = []\r+++\r:wq\r$ find themes/zafta/archetypes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md\r$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md\r/Users/quoha/Sites/zafta/content/post/first.md created\r$ hugo --verbose new post/second.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/second.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md\r/Users/quoha/Sites/zafta/content/post/second.md created\r$ ls -l content/post\rtotal 16\r-rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md\r-rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md\r$ cat content/post/first.md +++\rCategories = []\rDescription = \u0026#34;\u0026#34;\rTags = []\rdate = \u0026#34;2014-09-29T21:54:53-05:00\u0026#34;\rtitle = \u0026#34;first\u0026#34;\r+++\rmy first post\r$ cat content/post/second.md +++\rCategories = []\rDescription = \u0026#34;\u0026#34;\rTags = []\rdate = \u0026#34;2014-09-29T21:57:09-05:00\u0026#34;\rtitle = \u0026#34;second\u0026#34;\r+++\rmy second post\r$ Build the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;, \u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ The output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html\r$ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates\r#\rIn Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage\r#\rThe home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r{{ range first 10 .Data.Pages }}\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r{{ end }}\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r:wq\r$ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\nrange .Title end The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html\r$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt;\r\u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r$ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts\r#\rWe\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File\r#\r$ vi themes/zafta/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r{{ .Content }}\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r:wq\r$ Build the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html\r$ cat public/post/first/index.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;first\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;my first post\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r$ cat public/post/second/index.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;second\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;my second post\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r$ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content\r#\rThe posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r{{ range first 10 .Data.Pages }}\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r{{ end }}\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt; Build the web site and verify the results.\n$ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html\r$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/second/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;/post/first/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r$ Create a Post Listing\r#\rWe have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages\r#\rLet\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++\rtitle = \u0026#34;about\u0026#34;\rdescription = \u0026#34;about this site\u0026#34;\rdate = \u0026#34;2014-09-27\u0026#34;\rslug = \u0026#34;about time\u0026#34;\r+++\r## about us\ri\u0026#39;m speechless\r:wq Generate the web site and verify the results.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/theme/\u0026#34;\u0026gt;creating a new theme\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/about-time/\u0026#34;\u0026gt;about\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/second-post/\u0026#34;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;http://localhost:1313/post/first-post/\u0026#34;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\r\u0026lt;script\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;http://\u0026#39;\r+ (location.host || \u0026#39;localhost\u0026#39;).split(\u0026#39;:\u0026#39;)[0]\r+ \u0026#39;:1313/livereload.js?mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39;\r+ \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt; Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt;\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \u0026#34;post\u0026#34;}}\r\u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt;\r{{ end }}\r{{ end }}\r\u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt;\r{{ range .Data.Pages }}\r{{ if eq .Type \u0026#34;page\u0026#34; }}\r\u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt;\r{{ end }}\r{{ end }}\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r:wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name \u0026#39;*.html\u0026#39; | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml\r[permalinks]\rpage = \u0026#34;/:title/\u0026#34;\rabout = \u0026#34;/:filename/\u0026#34; Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates\r#\rIf you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials\r#\rIn Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r:wq\r$ vi themes/zafta/layouts/partials/footer.html\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r:wq Update the Home Page Template to Use the Partials\r#\rThe most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \u0026#34;theme/partials/header.html\u0026#34; . }} versus\n{{ partial \u0026#34;header.html\u0026#34; . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html\r{{ partial \u0026#34;header.html\u0026#34; . }}\r\u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt;\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \u0026#34;post\u0026#34;}}\r\u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt;\r{{ end }}\r{{ end }}\r\u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt;\r{{ range .Data.Pages }}\r{{ if or (eq .Type \u0026#34;page\u0026#34;) (eq .Type \u0026#34;about\u0026#34;) }}\r\u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt;\r{{ end }}\r{{ end }}\r{{ partial \u0026#34;footer.html\u0026#34; . }}\r:wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials\r#\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \u0026#34;header.html\u0026#34; . }}\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r{{ .Content }}\r{{ partial \u0026#34;footer.html\u0026#34; . }}\r:wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts\r#\rIt\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template\r#\rWe\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html\r{{ partial \u0026#34;header.html\u0026#34; . }}\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r\u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt;\r{{ .Content }}\r{{ partial \u0026#34;footer.html\u0026#34; . }}\r:wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \u0026#34;header.html\u0026#34; . }}\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r{{ .Content }}\r{{ partial \u0026#34;footer.html\u0026#34; . }}\r:wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html\r{{ partial \u0026#34;header.html\u0026#34; . }}\r\u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt;\r\u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt;\r{{ .Content }}\r{{ partial \u0026#34;footer.html\u0026#34; . }}\r:wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself\r#\rDRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"},{"id":4,"href":"/posts/migrate-from-jekyll/","title":"Migrating from Jekyll","section":"Blog","content":"\rMove static content to static\r#\rJekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n▾ \u0026lt;root\u0026gt;/\r▾ images/\rlogo.png\rshould become\n▾ \u0026lt;root\u0026gt;/\r▾ static/\r▾ images/\rlogo.png\rAdditionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file\r#\rHugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site\r#\rThe default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you\u0026rsquo;ll want to do one of two alternatives:\nChange your submodule to point to map gh-pages to public instead of _site (recommended).\ngit submodule deinit _site\rgit rm _site\rgit submodule add -b gh-pages git@github.com:your-username/your-repo.git public\rOr, change the Hugo configuration to use _site instead of public.\n{\r..\r\u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;,\r..\r}\rConvert Jekyll templates to Hugo templates\r#\rThat\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes\r#\rJekyll has plugins; Hugo has shortcodes. It\u0026rsquo;s fairly trivial to do a port.\nImplementation\r#\rAs an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll\rclass ImageTag \u0026lt; Liquid::Tag\r@url = nil\r@caption = nil\r@class = nil\r@link = nil\r// Patterns\rIMAGE_URL_WITH_CLASS_AND_CAPTION =\rIMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i\rIMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i\rIMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i\rIMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i\rdef initialize(tag_name, markup, tokens)\rsuper\rif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK\r@class = $1\r@url = $3\r@caption = $7\r@link = $9\relsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION\r@class = $1\r@url = $3\r@caption = $7\relsif markup =~ IMAGE_URL_WITH_CAPTION\r@url = $1\r@caption = $5\relsif markup =~ IMAGE_URL_WITH_CLASS\r@class = $1\r@url = $3\relsif markup =~ IMAGE_URL\r@url = $1\rend\rend\rdef render(context)\rif @class\rsource = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot;\relse\rsource = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot;\rend\rif @link\rsource += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot;\rif @link\rsource += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot;\rend\rsource += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption\rsource += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot;\rsource\rend\rend\rend\rLiquid::Template.register_tag('image', Jekyll::ImageTag)\ris written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt;\r\u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt;\r{{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }}\r\u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt;\r{{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }}\r{{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}}\r\u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }}\r{{ .Get \u0026quot;title\u0026quot; }}{{ end }}\r{{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt;\r{{ .Get \u0026quot;caption\u0026quot; }}\r{{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }}\r{{ .Get \u0026quot;attr\u0026quot; }}\r{{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }}\r\u0026lt;/p\u0026gt; {{ end }}\r\u0026lt;/figcaption\u0026gt;\r{{ end }}\r\u0026lt;/figure\u0026gt;\r\u0026lt;!-- image --\u0026gt;\rUsage\r#\rI simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %}\rto this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}}\rAs a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches\r#\rFix content\r#\rDepending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up\r#\rYou\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff\r#\rHey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff.\n"},{"id":5,"href":"/docs/example/table-of-contents/","title":"Table of Contents","section":"Arxiv Paper","content":"\rUbi loqui\r#\rMentem genus facietque salire tempus bracchia\r#\rLorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice\r#\rOra precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis\r#\rNefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral),\rnorthbridge_services_troubleshooting, personal(\rfirmware_rw.trash_rw_crm.device(interactive_gopher_personal,\rsoftware, -1), megabit, ergonomicsSoftware(cmyk_usb_panel,\rmips_whitelist_duplex, cpa)));\rif (5) {\rmanagementNetwork += dma - boolean;\rkilohertz_token = 2;\rhoneypot_affiliate_ergonomics = fiber;\r}\rmouseNorthbridge = byte(nybble_xmp_modem.horse_subnet(\ranalogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet),\rgateway_ospf), repository.domain_key.mouse(serverData(fileNetwork,\rtrim_duplex_file), cellTapeDirect, token_tooltip_mashup(\rripcordingMashup)));\rmodule_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) +\rcoreLog.joystick(componentUdpLink), windows_expansion_touchscreen);\rbashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling(\rciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);\rPlacabilis coactis nega ingemuit ignoscat nimia non\r#\rFrontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) {\rzif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive;\rgigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop),\rpanel_point_firmware);\rspyware_bash.statePopApplet = express_netbios_digital(\rinsertion_troubleshooting.brouter(recordFolderUs), 65);\r}\rrecursionCoreRay = -5;\rif (hub == non) {\rportBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard),\rfont_radcab, guidCmsScalable + reciprocalMatrixPim);\rleft.bug = screenshot;\r} else {\rtooltipOpacity = raw_process_permalink(webcamFontUser, -1);\rexecutable_router += tape;\r}\rif (tft) {\rbandwidthWeb *= social_page;\r} else {\rregular += 611883;\rthumbnail /= system_lag_keyboard;\r}\rCaesorum illa tu sentit micat vestes papyriferi\r#\rInde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":6,"href":"/posts/goisforlovers/","title":"(Hu)go Template Primer","section":"Blog","content":"Hugo uses the excellent Go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in Go templates.\nThis document is a brief primer on using Go templates. The Go docs provide more details.\nIntroduction to Go Templates\r#\rGo templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that Go templates parse very quickly.\nA unique characteristic of Go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the Go docs.\nBasic Syntax\r#\rGolang templates are HTML files with the addition of variables and functions.\nGo variables and functions are accessible within {{ }}\nAccessing a predefined variable \u0026ldquo;foo\u0026rdquo;:\n{{ foo }}\rParameters are separated using spaces\nCalling the add function with input of 1, 2:\n{{ add 1 2 }}\rMethods and fields are accessed via dot notation\nAccessing the Page Parameter \u0026ldquo;bar\u0026rdquo;\n{{ .Params.bar }}\rParentheses can be used to group items together\n{{ if or (isset .Params \u0026quot;alt\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;) }} Caption {{ end }}\rVariables\r#\rEach Go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page.\nA variable is accessed by referencing the variable name.\n\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt;\rVariables can also be defined and referenced.\n{{ $address := \u0026quot;123 Main St.\u0026quot;}}\r{{ $address }}\rFunctions\r#\rGo template ship with a few functions which provide basic functionality. The Go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo.\nExample:\n{{ add 1 2 }}\rIncludes\r#\rWhen including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo.\nExample:\n{{ template \u0026quot;chrome/header.html\u0026quot; . }}\rLogic\r#\rGo templates provide the most basic iteration and conditional logic.\nIteration\r#\rJust like in Go, the Go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range.\nExample 1: Using Context\n{{ range array }}\r{{ . }}\r{{ end }}\rExample 2: Declaring value variable name\n{{range $element := array}}\r{{ $element }}\r{{ end }}\rExample 2: Declaring key and value variable name\n{{range $index, $element := array}}\r{{ $index }}\r{{ $element }}\r{{ end }}\rConditionals\r#\rIf, else, with, or, \u0026amp; and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end.\nGo Templates treat the following values as false:\nfalse 0 any array, slice, map, or string of length zero Example 1: If\n{{ if isset .Params \u0026quot;title\u0026quot; }}\u0026lt;h4\u0026gt;{{ index .Params \u0026quot;title\u0026quot; }}\u0026lt;/h4\u0026gt;{{ end }}\rExample 2: If -\u0026gt; Else\n{{ if isset .Params \u0026quot;alt\u0026quot; }}\r{{ index .Params \u0026quot;alt\u0026quot; }}\r{{else}}\r{{ index .Params \u0026quot;caption\u0026quot; }}\r{{ end }}\rExample 3: And \u0026amp; Or\n{{ if and (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}}\rExample 4: With\nAn alternative way of writing \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent.\nThe first example above could be simplified as:\n{{ with .Params.title }}\u0026lt;h4\u0026gt;{{ . }}\u0026lt;/h4\u0026gt;{{ end }}\rExample 5: If -\u0026gt; Else If\n{{ if isset .Params \u0026quot;alt\u0026quot; }}\r{{ index .Params \u0026quot;alt\u0026quot; }}\r{{ else if isset .Params \u0026quot;caption\u0026quot; }}\r{{ index .Params \u0026quot;caption\u0026quot; }}\r{{ end }}\rPipes\r#\rOne of the most powerful components of Go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline\u0026rsquo;s output becomes the input of the following pipe.\nBecause of the very simple syntax of Go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline.\nA few simple examples should help convey how to use the pipe.\nExample 1 :\n{{ if eq 1 1 }} Same {{ end }}\ris the same as\n{{ eq 1 1 | if }} Same {{ end }}\rIt does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes.\nExample 2 :\n{{ index .Params \u0026quot;disqus_url\u0026quot; | html }}\rAccess the page parameter called \u0026ldquo;disqus_url\u0026rdquo; and escape the HTML.\nExample 3 :\n{{ if or (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}}\rStuff Here\r{{ end }}\rCould be rewritten as\n{{ isset .Params \u0026quot;caption\u0026quot; | or isset .Params \u0026quot;title\u0026quot; | or isset .Params \u0026quot;attr\u0026quot; | if }}\rStuff Here\r{{ end }}\rContext (aka. the dot)\r#\rThe most easily overlooked concept to understand about Go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context.\nExample:\n{{ $title := .Site.Title }}\r{{ range .Params.tags }}\r\u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;{{ $baseurl }}/tags/{{ . | urlize }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; - {{ $title }} \u0026lt;/li\u0026gt;\r{{ end }}\rNotice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop.\nHugo Parameters\r#\rHugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates.\nUsing Content (page) Parameters\r#\rIn each piece of content you can provide variables to be used by the templates. This happens in the front matter.\nAn example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn\u0026rsquo;t make a lot of sense. We\u0026rsquo;ve defined a variable in our front matter of some pages to turn off the TOC from being displayed.\nHere is the example front matter:\n---\rtitle: \u0026#34;Permalinks\u0026#34;\rdate: \u0026#34;2013-11-18\u0026#34;\raliases:\r- \u0026#34;/doc/permalinks/\u0026#34;\rgroups: [\u0026#34;extras\u0026#34;]\rgroups_weight: 30\rnotoc: true\r--- Here is the corresponding code inside of the template:\n{{ if not .Params.notoc }}\r\u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;well col-md-4 col-sm-6\u0026quot;\u0026gt;\r{{ .TableOfContents }}\r\u0026lt;/div\u0026gt;\r{{ end }}\rUsing Site (config) Parameters\r#\rIn your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome.\nFor instance, you might declare:\nparams: CopyrightHTML: \u0026#34;Copyright \u0026amp;#xA9; 2013 John Doe. All Rights Reserved.\u0026#34; TwitterUser: \u0026#34;spf13\u0026#34; SidebarRecentLimit: 5 Within a footer layout, you might then declare a \u0026lt;footer\u0026gt; which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates.\n{{if .Site.Params.CopyrightHTML}}\u0026lt;footer\u0026gt;\r\u0026lt;div class=\u0026#34;text-center\u0026#34;\u0026gt;{{.Site.Params.CopyrightHTML | safeHtml}}\u0026lt;/div\u0026gt;\r\u0026lt;/footer\u0026gt;{{end}} An alternative way of writing the \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent:\n{{with .Site.Params.TwitterUser}}\u0026lt;span class=\u0026#34;twitter\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;https://twitter.com/{{.}}\u0026#34; rel=\u0026#34;author\u0026#34;\u0026gt;\r\u0026lt;img src=\u0026#34;/images/twitter.png\u0026#34; width=\u0026#34;48\u0026#34; height=\u0026#34;48\u0026#34; title=\u0026#34;Twitter: {{.}}\u0026#34;\ralt=\u0026#34;Twitter\u0026#34;\u0026gt;\u0026lt;/a\u0026gt;\r\u0026lt;/span\u0026gt;{{end}} Finally, if you want to pull \u0026ldquo;magic constants\u0026rdquo; out of your layouts, you can do so, such as in this example:\n\u0026lt;nav class=\u0026#34;recent\u0026#34;\u0026gt;\r\u0026lt;h1\u0026gt;Recent Posts\u0026lt;/h1\u0026gt;\r\u0026lt;ul\u0026gt;{{range first .Site.Params.SidebarRecentLimit .Site.Recent}}\r\u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{.RelPermalink}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\r{{end}}\u0026lt;/ul\u0026gt;\r\u0026lt;/nav\u0026gt; "},{"id":7,"href":"/posts/hugoisforlovers/","title":"Getting Started with Hugo","section":"Blog","content":"\rStep 1. Install Hugo\r#\rGo to Hugo releases and download the appropriate version for your OS and architecture.\nSave it somewhere specific as we will be using it in the next step.\nMore complete instructions are available at Install Hugo\nStep 2. Build the Docs\r#\rHugo has its own example site which happens to also be the documentation site you are reading right now.\nFollow the following steps:\nClone the Hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands:\ngit clone https://github.com/spf13/hugo\rcd hugo\r/path/to/where/you/installed/hugo server --source=./docs\r\u0026gt; 29 pages created\r\u0026gt; 0 tags index created\r\u0026gt; in 27 ms\r\u0026gt; Web Server is available at http://localhost:1313\r\u0026gt; Press ctrl+c to stop\rOnce you\u0026rsquo;ve gotten here, follow along the rest of this page on your local build.\nStep 3. Change the docs site\r#\rStop the Hugo process by hitting Ctrl+C.\nNow we are going to run hugo again, but this time with hugo in watch mode.\n/path/to/hugo/from/step/1/hugo server --source=./docs --watch\r\u0026gt; 29 pages created\r\u0026gt; 0 tags index created\r\u0026gt; in 27 ms\r\u0026gt; Web Server is available at http://localhost:1313\r\u0026gt; Watching for changes in /Users/spf13/Code/hugo/docs/content\r\u0026gt; Press ctrl+c to stop\rOpen your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo.\nContent files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md.\nChange and save this file.. Notice what happened in your terminal.\n\u0026gt; Change detected, rebuilding site\r\u0026gt; 29 pages created\r\u0026gt; 0 tags index created\r\u0026gt; in 26 ms\rRefresh the browser and observe that the typo is now fixed.\nNotice how quick that was. Try to refresh the site before it\u0026rsquo;s finished building. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds.\nStep 4. Have fun\r#\rThe best way to learn something is to play with it.\n"},{"id":8,"href":"/docs/example/collapsed/3rd-level/4th-level/","title":"4th Level","section":"3rd Level","content":"\r4th Level of Menu\r#\rCaesorum illa tu sentit micat vestes papyriferi\r#\rInde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":9,"href":"/docs/example/collapsed/3rd-level/","title":"3rd Level","section":"Collapsed","content":"\r3rd Level of Menu\r#\rNefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral),\rnorthbridge_services_troubleshooting, personal(\rfirmware_rw.trash_rw_crm.device(interactive_gopher_personal,\rsoftware, -1), megabit, ergonomicsSoftware(cmyk_usb_panel,\rmips_whitelist_duplex, cpa)));\rif (5) {\rmanagementNetwork += dma - boolean;\rkilohertz_token = 2;\rhoneypot_affiliate_ergonomics = fiber;\r}\rmouseNorthbridge = byte(nybble_xmp_modem.horse_subnet(\ranalogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet),\rgateway_ospf), repository.domain_key.mouse(serverData(fileNetwork,\rtrim_duplex_file), cellTapeDirect, token_tooltip_mashup(\rripcordingMashup)));\rmodule_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) +\rcoreLog.joystick(componentUdpLink), windows_expansion_touchscreen);\rbashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling(\rciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);\r"},{"id":10,"href":"/docs/example/hidden/","title":"Hidden","section":"Arxiv Paper","content":"\rThis page is hidden in menu\r#\rQuondam non pater est dignior ille Eurotas\r#\rLatent te facies\r#\rLorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\nPater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor Cum honorum Latona\r#\rO fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer),\rpad.property_data_programming.sectorBrowserPpga(dataMask, 37,\rrecycleRup));\rintellectualVaporwareUser += -5 * 4;\rtraceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));\rsurge_host_golden = mca_compact_device(dual_dpi_opengl, 33,\rcommerce_add_ppc);\rif (lun_ipv) {\rverticalExtranet(1, thumbnail_ttl, 3);\rbar_graphics_jpeg(chipset - sector_xmp_beta);\r}\rFronde cetera dextrae sequens pennis voce muneris\r#\rActa cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software;\rif (internic \u0026gt; disk) {\remoticonLockCron += 37 + bps - 4;\rwan_ansi_honeypot.cardGigaflops = artificialStorageCgi;\rsimplex -= downloadAccess;\r}\rvar volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;\rsector(memory(firewire + interlaced, wired)); "},{"id":11,"href":"/docs/shortcodes/buttons/","title":"Buttons","section":"Shortcodes","content":"\rButtons\r#\rButtons are styled links that can lead to local page or external link.\nExample\r#\r{{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} Get Home\rContribute\r"},{"id":12,"href":"/docs/shortcodes/columns/","title":"Columns","section":"Shortcodes","content":"\rColumns\r#\rColumns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic separator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example\r#\rLeft Content\r#\rLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nMid Content\r#\rLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!\nRight Content\r#\rLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":13,"href":"/docs/shortcodes/details/","title":"Details","section":"Shortcodes","content":" "},{"id":14,"href":"/docs/shortcodes/expand/","title":"Expand","section":"Shortcodes","content":"\rExpand\r#\rExpand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample\r#\rDefault\r#\r{{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} Expand\r↕\rMarkdown content\r#\rLorem markdownum insigne\u0026hellip;\nWith Custom Label\r#\r{{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} Custom Label\r...\rMarkdown content\r#\rLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":15,"href":"/docs/shortcodes/hints/","title":"Hints","section":"Shortcodes","content":"\rHints\r#\rHint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}} Example\r#\rMarkdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\rMarkdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\rMarkdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa\r"},{"id":16,"href":"/docs/shortcodes/mermaid/","title":"Mermaid","section":"Shortcodes","content":"\rMermaid Chart\r#\rMermaidJS is library for generating svg charts and diagrams from text.\nOverride Mermaid Initialization Config\nTo override the initialization config for Mermaid, create a mermaid.json file in your assets folder!\nExample\r#\r{{\u0026lt; mermaid class=\u0026#34;optional\u0026#34; \u0026gt;}} stateDiagram-v2 State1: The state with a note note right of State1 Important information! You can write notes. end note State1 --\u0026gt; State2 note left of State2 : This is the note to the left. {{\u0026lt; /mermaid \u0026gt;}} stateDiagram-v2\rState1: The state with a note\rnote right of State1\rImportant information! You can write\rnotes.\rend note\rState1 --\u003e State2\rnote left of State2 : This is the note to the left.\r"},{"id":17,"href":"/docs/shortcodes/section/","title":"Section","section":"Shortcodes","content":"\rSection\r#\rSection renders pages in section as definition list, using title and description. Optional param summary can be used to show or hide page summary\nExample\r#\r{{\u0026lt; section [summary] \u0026gt;}} First Page\rFirst page\r#\rLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\rSecond Page\rSecond Page\r#\rLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\r"},{"id":18,"href":"/docs/shortcodes/section/first-page/","title":"First Page","section":"Section","content":"\rFirst page\r#\rLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":19,"href":"/docs/shortcodes/section/second-page/","title":"Second Page","section":"Section","content":"\rSecond Page\r#\rLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"},{"id":20,"href":"/docs/shortcodes/tabs/","title":"Tabs","section":"Shortcodes","content":"\rTabs\r#\rTabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example\r#\rMacOS\rMacOS\r#\rThis is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux\rLinux\r#\rThis is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows\rWindows\r#\rThis is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n"},{"id":21,"href":"/docs/shortcodes/katex/","title":"KaTeX","section":"Shortcodes","content":"\rKaTeX\r#\rKaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nExample\r#\r{{\u0026lt; katex display=true class=\u0026#34;optional\u0026#34; \u0026gt;}} f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi {{\u0026lt; /katex \u0026gt;}} \\[\rf(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\r\\]\rDisplay Mode Example\r#\rHere is some inline example: \\(\\pi(x)\\)\r, rendered in the same line. And below is display example, having display: block \\[\rf(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\r\\]\rText continues here.\n"}]