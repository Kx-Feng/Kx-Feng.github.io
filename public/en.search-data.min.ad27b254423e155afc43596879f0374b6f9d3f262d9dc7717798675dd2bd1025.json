[{"id":0,"href":"/docs/about_me/","title":"Home","section":"Docs","content":"\rAbout Me\r#\r"},{"id":1,"href":"/docs/arxiv_papers/llm_copyright/","title":"LLM with Copyright","section":"Arxiv Papers","content":"\rArxiv Papers: LLM with Copyright\r#\r2024-03-15\nLost in Overlap: Exploring Watermark Collision in LLMs\nYiyang Luo, Ke Lin, Chao Gu\nabstract\rabstract: The proliferation of large language models (LLMs) in generating contentraises concerns about text copyright. Watermarking methods, particularlylogit-based approaches, embed imperceptible identifiers into text to addressthese challenges. However, the widespread use of watermarking across diverseLLMs has led to an inevitable issue known as watermark collision during commontasks like question answering and paraphrasing. This study focuses on dualwatermark collisions, where two watermarks are present simultaneously in thesame text. The research demonstrates that watermark collision poses a threat todetection performance for detectors of both upstream and downstream watermarkalgorithms.\r2024-03-13\nSecond-Order Information Matters: Revisiting Machine Unlearning for Large Language Models\nKang Gu, Md Rafi Ur Rashid, Najrin Sultana, Shagufta Mehnaz\nabstract\rabstract: With the rapid development of Large Language Models (LLMs), we have witnessedintense competition among the major LLM products like ChatGPT, LLaMa, andGemini. However, various issues (e.g. privacy leakage and copyright violation)of the training corpus still remain underexplored. For example, the Times suedOpenAI and Microsoft for infringing on its copyrights by using millions of itsarticles for training. From the perspective of LLM practitioners, handling suchunintended privacy violations can be challenging. Previous work addressed the``unlearning\u0026quot; problem of LLMs using gradient information, while they mostlyintroduced significant overheads like data preprocessing or lacked robustness.In this paper, contrasting with the methods based on first-order information,we revisit the unlearning problem via the perspective of second-orderinformation (Hessian). Our unlearning algorithms, which are inspired by classicNewton update, are not only data-agnostic/model-agnostic but also proven to berobust in terms of utility preservation or privacy guarantee. Through acomprehensive evaluation with four NLP datasets as well as a case study onreal-world datasets, our methods consistently show superiority over thefirst-order methods.\r2024-03-09\nDetecting Pretraining Data from Large Language Models\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\r2024-02-29\nAn Unforgeable Publicly Verifiable Watermark for Large Language Models\nAiwei Liu, Leyi Pan, Xuming Hu, Shu\u0026rsquo;ang Li, Lijie Wen, Irwin King, Philip S. Yu\nabstract\rabstract: Recently, text watermarking algorithms for large language models (LLMs) havebeen proposed to mitigate the potential harms of text generated by LLMs,including fake news and copyright issues. However, current watermark detectionalgorithms require the secret key used in the watermark generation process,making them susceptible to security breaches and counterfeiting during publicdetection. To address this limitation, we propose an unforgeable publiclyverifiable watermark algorithm that uses two different neural networks forwatermark generation and detection, instead of using the same key at bothstages. Meanwhile, the token embedding parameters are shared between thegeneration and detection networks, which makes the detection network achieve ahigh accuracy very efficiently. Experiments demonstrate that our algorithmattains high detection accuracy and computational efficiency through neuralnetworks with a minimized number of parameters. Subsequent analysis confirmsthe high complexity involved in forging the watermark from the detectionnetwork. Our code and data are available at\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable_watermark}.\rPRSA: Prompt Reverse Stealing Attacks against Large Language Models\nYong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang\nabstract\rabstract: Prompt, recognized as crucial intellectual property, enables large languagemodels (LLMs) to perform specific tasks without the need of fine-tuning,underscoring their escalating importance. With the rise of prompt-basedservices, such as prompt marketplaces and LLM applications, providers oftendisplay prompts\u0026rsquo; capabilities through input-output examples to attract users.However, this paradigm raises a pivotal security concern: does the exposure ofinput-output pairs pose the risk of potential prompt leakage, infringing on theintellectual property rights of the developers? To our knowledge, this problemstill has not been comprehensively explored yet. To remedy this gap, in thispaper, we perform the first in depth exploration and propose a novel attackframework for reverse-stealing prompts against commercial LLMs, namely PRSA.The main idea of PRSA is that by analyzing the critical features of theinput-output pairs, we mimic and gradually infer (steal) the target prompts. Indetail, PRSA mainly consists of two key phases: prompt mutation and promptpruning. In the mutation phase, we propose a prompt attention algorithm basedon differential feedback to capture these critical features for effectivelyinferring the target prompts. In the prompt pruning phase, we identify and maskthe words dependent on specific inputs, enabling the prompts to accommodatediverse inputs for generalization. Through extensive evaluation, we verify thatPRSA poses a severe threat in real world scenarios. We have reported thesefindings to prompt service providers and actively collaborate with them to takeprotective measures for prompt copyright.\r2024-02-23\nWho Wrote this Code? Watermarking for Code Generation\nTaehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim\nabstract\rabstract: With the remarkable generation performance of large language models, ethicaland legal concerns about using them have been raised, such as plagiarism andcopyright issues. For such concerns, several approaches to watermark and detectLLM-generated text have been proposed very recently. However, we discover thatthe previous methods fail to function appropriately with code generation tasksbecause of the syntactic and semantic characteristics of code. Based on\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,Selective WatErmarking via Entropy Thresholding (SWEET), that promotes \u0026ldquo;green\u0026quot;tokens only at the position with high entropy of the token distribution duringgeneration, thereby preserving the correctness of the generated code. Thewatermarked code is detected by the statistical test and Z-score based on theentropy information. Our experiments on HumanEval and MBPP show that SWEETsignificantly improves the Pareto Frontier between the code correctness andwatermark detection performance. We also show that notable post-hoc detectionmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show thatsetting a reasonable entropy threshold is not much of a challenge. Code isavailable at https://github.com/hongcheki/sweet-watermark.\r2024-02-22\nExploring Memorization in Fine-tuned Language Models\nShenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin\nabstract\rabstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models\u0026rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.\r2024-02-20\nPrivacy Issues in Large Language Models: A Survey\nSeth Neel, Peter Chang\nabstract\rabstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.\r2024-02-19\nPurifying Large Language Models by Ensembling a Small Language Model\nTianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin\nabstract\rabstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.\r2024-02-16\nLarge Language Model Unlearning\nYuanshun Yao, Xiaojun Xu, Yang Liu\nabstract\rabstract: We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,on large language models (LLMs). We show at least three scenarios of aligningLLMs with human preferences can benefit from unlearning: (1) removing harmfulresponses, (2) erasing copyright-protected content as requested, and (3)reducing hallucinations. Unlearning, as an alignment technique, has threeadvantages. (1) It only requires negative (e.g. harmful) examples, which aremuch easier and cheaper to collect (e.g. via red teaming or user reporting)than positive (e.g. helpful and often human-written) examples required in RLHF(RL from human feedback). (2) It is computationally efficient. (3) It isespecially effective when we know which training samples cause the misbehavior.To the best of our knowledge, our work is among the first to explore LLMunlearning. We are also among the first to formulate the settings, goals, andevaluations in LLM unlearning. We show that if practitioners only have limitedresources, and therefore the priority is to stop generating undesirable outputsrather than to try to generate desirable outputs, unlearning is particularlyappealing. Despite only having negative samples, our ablation study shows thatunlearning can still achieve better alignment performance than RLHF with just2% of its computational time.\r2024-02-15\nRethinking Machine Unlearning for Large Language Models\nSijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\r2024-02-14\nCopyright Traps for Large Language Models\nMatthieu Meeus, Igor Shilov, Manuel Faysse, Yves-Alexandre de Montjoye\nabstract\rabstract: Questions of fair use of copyright-protected content to train Large LanguageModels (LLMs) are being very actively debated. Document-level inference hasbeen proposed as a new task: inferring from black-box access to the trainedmodel whether a piece of content has been seen during training. SOTA methodshowever rely on naturally occurring memorization of (part of) the content.While very effective against models that memorize a lot, we hypothesize\u0026ndash;andlater confirm\u0026ndash;that they will not work against models that do not naturallymemorize, e.g. medium-size 1B models. We here propose to use copyright traps,the inclusion of fictitious entries in original content, to detect the use ofcopyrighted materials in LLMs with a focus on models where memorization doesnot naturally occur. We carefully design an experimental setup, randomlyinserting traps into original content (books) and train a 1.3B LLM. We firstvalidate that the use of content in our target model would be undetectableusing existing methods. We then show, contrary to intuition, that evenmedium-length trap sentences repeated a significant number of times (100) arenot detectable using existing methods. However, we show that longer sequencesrepeated a large number of times can be reliably detected (AUC=0.75) and usedas copyright traps. We further improve these results by studying how the numberof times a sequence is seen improves detectability, how sequences with higherperplexity tend to be memorized more, and how taking context into accountfurther improves detectability.\rTrained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code\nVahid Majdinasab, Amin Nikanjam, Foutse Khomh\nabstract\rabstract: Code auditing ensures that the developed code adheres to standards,regulations, and copyright protection by verifying that it does not containcode from protected sources. The recent advent of Large Language Models (LLMs)as coding assistants in the software development process poses new challengesfor code auditing. The dataset for training these models is mainly collectedfrom publicly available sources. This raises the issue of intellectual propertyinfringement as developers\u0026rsquo; codes are already included in the dataset.Therefore, auditing code developed using LLMs is challenging, as it isdifficult to reliably assert if an LLM used during development has been trainedon specific copyrighted codes, given that we do not have access to the trainingdatasets of these models. Given the non-disclosure of the training datasets,traditional approaches such as code clone detection are insufficient forasserting copyright infringement. To address this challenge, we propose a newapproach, TraWiC; a model-agnostic and interpretable method based on membershipinference for detecting code inclusion in an LLM\u0026rsquo;s training dataset. We extractsyntactic and semantic identifiers unique to each program to train a classifierfor detecting code inclusion. In our experiments, we observe that TraWiC iscapable of detecting 83.87% of codes that were used to train an LLM. Incomparison, the prevalent clone detection tool NiCad is only capable ofdetecting 47.64%. In addition to its remarkable performance, TraWiC has lowresource overhead in contrast to pair-wise clone detection that is conductedduring the auditing process of tools like CodeWhisperer reference tracker,across thousands of code snippets.\rEmpowering Federated Learning for Massive Models with NVIDIA FLARE\nHolger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng\nabstract\rabstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.\rFive ethical principles for generative AI in scientific research\nZhicheng Lin\nabstract\rabstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.\r2024-02-10\nData Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\nShahriar Golchin, Mihai Surdeanu\nabstract\rabstract: We propose the Data Contamination Quiz (DCQ), a simple and effective approachto detect data contamination in large language models (LLMs) and estimate theamount of it. Specifically, we frame data contamination detection as a seriesof multiple-choice questions and devise a quiz format wherein three perturbedversions of each dataset instance are created. These changes only includeword-level perturbations. The generated perturbed versions, along with theoriginal instance, form the options in the DCQ, with an extra optionaccommodating the possibility that none of the provided choices is correct.Given that the only distinguishing signal among the choices is the exactwording relative to the original instance, an LLM, when tasked with identifyingthe original instance from the choices, gravitates towards the original one ifit has been exposed to it in its pre-training phase\u0026ndash;a trait intrinsic to LLMs.Tested over several datasets with GPT-4/3.5, our findings\u0026ndash;while fully lackingaccess to LLMs\u0026rsquo; pre-training data and internal parameters\u0026ndash;suggest that DCQuncovers greater contamination levels compared to existing detection methodsand proficiently bypasses more safety filters, especially those set to avoidgenerating copyrighted contents.\r2024-02-07\nHuman-Readable Fingerprint for Large Language Models\nBoyi Zeng, Chenghu Zhou, Xinbing Wang, Zhouhan Lin\nabstract\rabstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters\u0026rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM\u0026rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog\u0026rsquo;s appearancestrongly indicates the LLM\u0026rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.\r2024-01-23\nA Survey of Text Watermarking in the Era of Large Language Models\nAiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, Philip S. Yu\nabstract\rabstract: Text watermarking algorithms play a crucial role in the copyright protectionof textual content, yet their capabilities and application scenarios have beenlimited historically. The recent developments in large language models (LLMs)have opened new opportunities for the advancement of text watermarkingtechniques. LLMs not only enhance the capabilities of text watermarkingalgorithms through their text understanding and generation abilities but alsonecessitate the use of text watermarking algorithms for their own copyrightprotection. This paper conducts a comprehensive survey of the current state oftext watermarking technology, covering four main aspects: (1) an overview andcomparison of different text watermarking techniques; (2) evaluation methodsfor text watermarking algorithms, including their success rates, impact on textquality, robustness, and unforgeability; (3) potential application scenariosfor text watermarking technology; (4) current challenges and future directionsfor development. This survey aims to provide researchers with a thoroughunderstanding of text watermarking technology, thereby promoting its furtheradvancement.\r2024-01-18\nSilent Guardian: Protecting Text from Malicious Exploitation by Large Language Models\nJiawei Zhao, Kejiang Chen, Xiaojian Yuan, Yuang Qi, Weiming Zhang, Nenghai Yu\nabstract\rabstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.\r2024-01-07\nThe Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\nHaonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi\nabstract\rabstract: The commercialization of diffusion models, renowned for their ability togenerate high-quality images that are often indistinguishable from real ones,brings forth potential copyright concerns. Although attempts have been made toimpede unauthorized access to copyrighted material during training and tosubsequently prevent DMs from generating copyrighted images, the effectivenessof these solutions remains unverified. This study explores the vulnerabilitiesassociated with copyright protection in DMs by introducing a backdoor datapoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.Our attack method operates without requiring access to or control over thediffusion model\u0026rsquo;s training or fine-tuning processes; it merely involves theinsertion of poisoning data into the clean training dataset. This data,comprising poisoning images equipped with prompts, is generated by leveragingthe powerful capabilities of multimodal large language models and text-guidedimage inpainting techniques. Our experimental results and analysis confirm themethod\u0026rsquo;s effectiveness. By integrating a minor portion ofnon-copyright-infringing stealthy poisoning data into the cleandataset-rendering it free from suspicion-we can prompt the finetuned diffusionmodels to produce copyrighted content when activated by specific triggerprompts. These findings underline potential pitfalls in the prevailingcopyright protection strategies and underscore the necessity for increasedscrutiny and preventative measures against the misuse of DMs.\r2024-01-02\nTowards Code Watermarking with Dual-Channel Transformations\nBorui Yang, Wei Li, Liyao Xiang, Bo Li\nabstract\rabstract: The expansion of the open source community and the rise of large languagemodels have raised ethical and security concerns on the distribution of sourcecode, such as misconduct on copyrighted code, distributions without properlicenses, or misuse of the code for malicious purposes. Hence it is importantto track the ownership of source code, in which watermarking is a majortechnique. Yet, drastically different from natural languages, source codewatermarking requires far stricter and more complicated rules to ensure thereadability as well as the functionality of the source code. Hence we introduceSrcMarker, a watermarking system to unobtrusively encode ID bitstrings intosource code, without affecting the usage and semantics of the code. To thisend, SrcMarker performs transformations on an AST-based intermediaterepresentation that enables unified transformations across differentprogramming languages. The core of the system utilizes learning-based embeddingand extraction modules to select rule-based transformations for watermarking.In addition, a novel feature-approximation technique is designed to tackle theinherent non-differentiability of rule selection, thus seamlessly integratingthe rule-based transformations and learning-based networks into aninterconnected system to enable end-to-end training. Extensive experimentsdemonstrate the superiority of SrcMarker over existing methods in variouswatermarking requirements.\r2024-01-01\nDigger: Detecting Copyright Content Mis-usage in Large Language Model Training\nHaodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang\nabstract\rabstract: Pre-training, which utilizes extensive and varied datasets, is a criticalfactor in the success of Large Language Models (LLMs) across numerousapplications. However, the detailed makeup of these datasets is often notdisclosed, leading to concerns about data security and potential misuse. Thisis particularly relevant when copyrighted material, still under legalprotection, is used inappropriately, either intentionally or unintentionally,infringing on the rights of the authors. In this paper, we introduce a detailed framework designed to detect andassess the presence of content from potentially copyrighted books within thetraining datasets of LLMs. This framework also provides a confidence estimationfor the likelihood of each content sample\u0026rsquo;s inclusion. To validate ourapproach, we conduct a series of simulated experiments, the results of whichaffirm the framework\u0026rsquo;s effectiveness in identifying and addressing instances ofcontent misuse in LLM training processes. Furthermore, we investigate thepresence of recognizable quotes from famous literary works within thesedatasets. The outcomes of our study have significant implications for ensuringthe ethical use of copyrighted materials in the development of LLMs,highlighting the need for more transparent and responsible data managementpractices in this field.\r2023-12-31\nViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\nDipankar Sarkar\nabstract\rabstract: This paper aims to introduce and analyze the Viz system in a comprehensiveway, a novel system architecture that integrates Quantized Low-Rank Adapters(QLoRA) to fine-tune large language models (LLM) within a legally compliant andresource efficient marketplace. Viz represents a significant contribution tothe field of artificial intelligence, particularly in addressing the challengesof computational efficiency, legal compliance, and economic sustainability inthe utilization and monetization of LLMs. The paper delineates the scholarlydiscourse and developments that have informed the creation of Viz, focusingprimarily on the advancements in LLM models, copyright issues in AI training(NYT case, 2023), and the evolution of model fine-tuning techniques,particularly low-rank adapters and quantized low-rank adapters, to create asustainable and economically compliant framework for LLM utilization. Theeconomic model it proposes benefits content creators, AI developers, andend-users, delineating a harmonious integration of technology, economy, andlaw, offering a comprehensive solution to the complex challenges of today\u0026rsquo;s AIlandscape.\r2023-12-15\nPrivacy-Aware Document Visual Question Answering\nRubèn Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas\nabstract\rabstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.\r2023-11-28\nPromptCARE: Prompt Copyright Protection by Watermark Injection and Verification\nHongwei Yao, Jian Lou, Kui Ren, Zhan Qin\nabstract\rabstract: Large language models (LLMs) have witnessed a meteoric rise in popularityamong the general public users over the past few months, facilitating diversedownstream tasks with human-level accuracy and proficiency. Prompts play anessential role in this success, which efficiently adapt pre-trained LLMs totask-specific applications by simply prepending a sequence of tokens to thequery texts. However, designing and selecting an optimal prompt can be bothexpensive and demanding, leading to the emergence of Prompt-as-a-Serviceproviders who profit by providing well-designed prompts for authorized use.With the growing popularity of prompts and their indispensable role inLLM-based services, there is an urgent need to protect the copyright of promptsagainst unauthorized use. In this paper, we propose PromptCARE, the first framework for promptcopyright protection through watermark injection and verification. Promptwatermarking presents unique challenges that render existing watermarkingtechniques developed for model and dataset copyright verification ineffective.PromptCARE overcomes these hurdles by proposing watermark injection andverification schemes tailor-made for prompts and NLP characteristics. Extensiveexperiments on six well-known benchmark datasets, using three prevalentpre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate theeffectiveness, harmlessness, robustness, and stealthiness of PromptCARE.\r2023-11-23\nMARBLE: Music Audio Representation Benchmark for Universal Evaluation\nRuibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu\nabstract\rabstract: In the era of extensive intersection between art and Artificial Intelligence(AI), such as image generation and fiction co-creation, AI for music remainsrelatively nascent, particularly in music understanding. This is evident in thelimited work on deep music representations, the scarcity of large-scaledatasets, and the absence of a universal and community-driven benchmark. Toaddress this issue, we introduce the Music Audio Representation Benchmark foruniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for variousMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomywith four hierarchy levels, including acoustic, performance, score, andhigh-level description. We then establish a unified protocol based on 14 taskson 8 public-available datasets, providing a fair and standard assessment ofrepresentations of all open-sourced pre-trained models developed on musicrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, andreproducible suite for the community, with a clear statement on copyrightissues on datasets. Results suggest recently proposed large-scale pre-trainedmusical language models perform the best in most tasks, with room for furtherimprovement. The leaderboard and toolkit repository are published athttps://marble-bm.shef.ac.uk to promote future music AI research.\r2023-11-21\nLyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT\nLe Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger Dannenberg, Jie Fu, Chenghua Lin, Emmanouil Benetos, Wenhu Chen, Wei Xue, Yike Guo\nabstract\rabstract: We introduce LyricWhiz, a robust, multilingual, and zero-shot automaticlyrics transcription method achieving state-of-the-art performance on variouslyrics transcription datasets, even in challenging genres such as rock andmetal. Our novel, training-free approach utilizes Whisper, a weakly supervisedrobust speech recognition model, and GPT-4, today\u0026rsquo;s most performant chat-basedlarge language model. In the proposed method, Whisper functions as the \u0026ldquo;ear\u0026rdquo; bytranscribing the audio, while GPT-4 serves as the \u0026ldquo;brain,\u0026rdquo; acting as anannotator with a strong performance for contextualized output selection andcorrection. Our experiments show that LyricWhiz significantly reduces WordError Rate compared to existing methods in English and can effectivelytranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz tocreate the first publicly available, large-scale, multilingual lyricstranscription dataset with a CC-BY-NC-SA copyright license, based onMTG-Jamendo, and offer a human-annotated subset for noise level estimation andevaluation. We anticipate that our proposed method and dataset will advance thedevelopment of multilingual lyrics transcription, a challenging and emergingtask.\r2023-11-17\nFunctionMarker: Watermarking Language Datasets via Knowledge Injection\nShuai Li, Kejiang Chen, Kunsheng Tang, Wen Huang, Jie Zhang, Weiming Zhang, Nenghai Yu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated superior performance invarious natural language processing tasks. Meanwhile, they require extensivetraining data, raising concerns related to dataset copyright protection.Backdoor-based watermarking is a viable approach to protect the copyright ofclassification datasets. However, these methods may introduce maliciousmisclassification behaviors into watermarked LLMs by attackers and also affectthe semantic information of the watermarked text. To address these issues, wepropose FunctionMarker, a novel copyright protection method for languagedatasets via knowledge injection. FunctionMarker enables LLMs to learn specificknowledge through fine-tuning on watermarked datasets, and we can extract theembedded watermark by obtaining the responses of LLMs to specificknowledge-related queries. Considering watermark capacity and stealthness, weselect customizable functions as specific knowledge for LLMs to learn and embedthe watermark into them. Moreover, FunctionMarker can embed multi-bitwatermarks while preserving the original semantic information, therebyincreasing the difficulty of adaptive attacks. We take mathematical functionsas an instance to evaluate the effectiveness of FunctionMarker, and experimentsshow that only 0.3% of watermarked text achieves a 90% watermark extractionaccuracy in most cases, validating our method\u0026rsquo;s effectiveness.\r2023-11-10\nWatermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service\nYuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu\nabstract\rabstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at https://github.com/Pter61/vlpmarker.\r2023-10-24\nSoK: Memorization in General-Purpose Large Language Models\nValentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West\nabstract\rabstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.\r2023-10-23\nH2O Open Ecosystem for State-of-the-art Large Language Models\nArno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos V. Conde\nabstract\rabstract: Large Language Models (LLMs) represent a revolution in AI. However, they alsopose many significant risks, such as the presence of biased, private,copyrighted or harmful text. For this reason we need open, transparent and safesolutions. We introduce a complete open-source ecosystem for developing andtesting LLMs. The goal of this project is to boost open alternatives toclosed-source approaches. We release h2oGPT, a family of fine-tuned LLMs ofdiverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUIdesigned for efficient fine-tuning, evaluation, and deployment of LLMs usingthe most recent state-of-the-art techniques. Our code and models are fullyopen-source. We believe this work helps to boost AI development and make itmore accessible, efficient and trustworthy. The demo is available at:https://gpt.h2o.ai/\rDid the Neurons Read your Book? Document-level Membership Inference for Large Language Models\nMatthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye\nabstract\rabstract: With large language models (LLMs) poised to become embedded in our dailylives, questions are starting to be raised about the dataset(s) they learnedfrom. These questions range from potential bias or misinformation LLMs couldretain from their training data to questions of copyright and fair use ofhuman-generated text. However, while these questions emerge, developers of therecent state-of-the-art LLMs become increasingly reluctant to disclose detailson their training corpus. We here introduce the task of document-levelmembership inference for real-world LLMs, i.e. inferring whether the LLM hasseen a given document during training or not. First, we propose a procedure forthe development and evaluation of document-level membership inference for LLMsby leveraging commonly used data sources for training and the model releasedate. We then propose a practical, black-box method to predict document-levelmembership and instantiate it on OpenLLaMA-7B with both books and academicpapers. We show our methodology to perform very well, reaching an impressiveAUC of 0.856 for books and 0.678 for papers. We then show our approach tooutperform the sentence-level membership inference attacks used in the privacyliterature for the document-level membership task. We finally evaluate whethersmaller models might be less sensitive to document-level inference and showOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.Taken together, our results show that accurate document-level membership can beinferred for LLMs, increasing the transparency of technology poised to changeour lives.\r2023-10-20\nCopyright Violations and Large Language Models\nAntonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard\nabstract\rabstract: Language models may memorize more than just facts, including entire chunks oftexts seen during training. Fair use exemptions to copyright laws typicallyallow for limited use of copyrighted material without permission from thecopyright holder, but typically for extraction of information from copyrightedmaterials, rather than {\\em verbatim} reproduction. This work explores theissue of copyright violations and large language models through the lens ofverbatim memorization, focusing on possible redistribution of copyrighted text.We present experiments with a range of language models over a collection ofpopular books and coding problems, providing a conservative characterization ofthe extent to which language models can redistribute these materials. Overall,this research highlights the need for further examination and the potentialimpact on future developments in natural language processing to ensureadherence to copyright regulations. Code is at\\url{https://github.com/coastalcph/CopyrightLLMs}.\r2023-10-10\nWatermarking Classification Dataset for Copyright Protection\nYixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun\nabstract\rabstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals\u0026rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.\r2023-10-04\nWho\u0026rsquo;s Harry Potter? Approximate Unlearning in LLMs\nRonen Eldan, Mark Russinovich\nabstract\rabstract: Large language models (LLMs) are trained on massive internet corpora thatoften contain copyrighted content. This poses legal and ethical challenges forthe developers and users of these models, as well as the original authors andpublishers. In this paper, we propose a novel technique for unlearning a subsetof the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter booksfrom the Llama2-7b model (a generative language model recently open-sourced byMeta). While the model took over 184K GPU-hours to pretrain, we show that inabout 1 GPU hour of finetuning, we effectively erase the model\u0026rsquo;s ability togenerate or recall Harry Potter-related content, while its performance oncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remainsalmost unaffected. We make our fine-tuned model publicly available onHuggingFace for community evaluation. To the best of our knowledge, this is thefirst paper to present an effective technique for unlearning in generativelanguage models. Our technique consists of three main components: First, we use a reinforcedmodel that is further trained on the target data to identify the tokens thatare most related to the unlearning target, by comparing its logits with thoseof a baseline model. Second, we replace idiosyncratic expressions in the targetdata with generic counterparts, and leverage the model\u0026rsquo;s own predictions togenerate alternative labels for every token. These labels aim to approximatethe next-token predictions of a model that has not been trained on the targetdata. Third, we finetune the model on these alternative labels, whicheffectively erases the original text from the model\u0026rsquo;s memory whenever it isprompted with its context.\r2023-08-23\nHow to Protect Copyright Data in Optimization of Large Language Models?\nTimothy Chu, Zhao Song, Chiwun Yang\nabstract\rabstract: Large language models (LLMs) and generative AI have played a transformativerole in computer research and applications. Controversy has arisen as towhether these models output copyrighted data, which can occur if the data themodels are trained on is copyrighted. LLMs are built on the transformer neuralnetwork architecture, which in turn relies on a mathematical computation calledAttention that uses the softmax function. In this paper, we show that large language model training and optimizationcan be seen as a softmax regression problem. We then establish a method ofefficiently performing softmax regression, in a way that prevents theregression function from generating copyright data. This establishes atheoretical method of training large language models in a way that avoidsgenerating copyright data.\r2023-08-19\nDUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization\nXiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang\nabstract\rabstract: Stable Diffusion (SD) customization approaches enable users to personalize SDmodel outputs, greatly enhancing the flexibility and diversity of AI art.However, they also allow individuals to plagiarize specific styles or subjectsfrom copyrighted images, which raises significant concerns about potentialcopyright infringement. To address this issue, we propose an invisibledata-free universal adversarial watermark (DUAW), aiming to protect a myriad ofcopyrighted images from different customization approaches across variousversions of SD models. First, DUAW is designed to disrupt the variationalautoencoder during SD customization. Second, DUAW operates in a data-freecontext, where it is trained on synthetic images produced by a Large LanguageModel (LLM) and a pretrained SD model. This approach circumvents the necessityof directly handling copyrighted images, thereby preserving theirconfidentiality. Once crafted, DUAW can be imperceptibly integrated intomassive copyrighted images, serving as a protective measure by inducingsignificant distortions in the images generated by customized SD models.Experimental results demonstrate that DUAW can effectively distort the outputsof fine-tuned SD models, rendering them discernible to both human observers anda simple classifier.\r2023-06-18\nShould ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era\nDong Zhang\nabstract\rabstract: With various AI tools such as ChatGPT becoming increasingly popular, we areentering a true AI era. We can foresee that exceptional AI tools will soon reapconsiderable profits. A crucial question arise: should AI tools share revenuewith their training data providers in additional to traditional stakeholdersand shareholders? The answer is Yes. Large AI tools, such as large languagemodels, always require more and better quality data to continuously improve,but current copyright laws limit their access to various types of data. Sharingrevenue between AI tools and their data providers could transform the currenthostile zero-sum game relationship between AI tools and a majority ofcopyrighted data owners into a collaborative and mutually beneficial one, whichis necessary to facilitate the development of a virtuous cycle among AI tools,their users and data providers that drives forward AI technology and builds ahealthy AI ecosystem. However, current revenue-sharing business models do notwork for AI tools in the forthcoming AI era, since the most widely used metricsfor website-based traffic and action, such as clicks, will be replaced by newmetrics such as prompts and cost per prompt for generative AI tools. Acompletely new revenue-sharing business model, which must be almost independentof AI tools and be easily explained to data providers, needs to establish aprompt-based scoring system to measure data engagement of each data provider.This paper systematically discusses how to build such a scoring system for alldata providers for AI tools based on classification and content similaritymodels, and outlines the requirements for AI tools or third parties to buildit. Sharing revenue with data providers using such a scoring system wouldencourage more data owners to participate in the revenue-sharing program. Thiswill be a utilitarian AI era where all parties benefit.\r2023-06-16\nh2oGPT: Democratizing Large Language Models\nArno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, Chun Ming Lee, Marcos V. Conde, Pasha Stetsenko, Olivier Grellier, SriSatish Ambati\nabstract\rabstract: Applications built on top of Large Language Models (LLMs) such as GPT-4represent a revolution in AI due to their human-level capabilities in naturallanguage processing. However, they also pose many significant risks such as thepresence of biased, private, or harmful text, and the unauthorized inclusion ofcopyrighted material. We introduce h2oGPT, a suite of open-source code repositories for thecreation and use of LLMs based on Generative Pretrained Transformers (GPTs).The goal of this project is to create the world\u0026rsquo;s best truly open-sourcealternative to closed-source approaches. In collaboration with and as part ofthe incredible and unstoppable open-source community, we open-source severalfine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercialuse under fully permissive Apache 2.0 licenses. Included in our release is100% private document search using natural language. Open-source language models help boost AI development and make it moreaccessible and trustworthy. They lower entry hurdles, allowing people andgroups to tailor these models to their needs. This openness increasesinnovation, transparency, and fairness. An open-source strategy is needed toshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.\r2023-06-09\nRobust Multi-bit Natural Language Watermarking through Invariant Features\nKiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak\nabstract\rabstract: Recent years have witnessed a proliferation of valuable original naturallanguage contents found in subscription-based media outlets, web novelplatforms, and outputs of large language models. However, these contents aresusceptible to illegal piracy and potential misuse without proper securitymeasures. This calls for a secure watermarking system to guarantee copyrightprotection through leakage tracing or ownership identification. To effectivelycombat piracy and protect copyrights, a multi-bit watermarking framework shouldbe able to embed adequate bits of information and extract the watermarks in arobust manner despite possible corruption. In this work, we explore ways toadvance both payload and robustness by following a well-known proposition fromimage watermarking and identify features in natural language that are invariantto minor corruption. Through a systematic analysis of the possible sources oferrors, we further propose a corruption-resistant infill model. Our full methodimproves upon the previous work on robustness by +16.8% point on average onfour datasets, three corruption types, and two corruption ratios. Codeavailable at https://github.com/bangawayoo/nlp-watermarking.\r2023-06-02\nAre You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark\nWenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie\nabstract\rabstract: Large language models (LLMs) have demonstrated powerful capabilities in bothtext understanding and generation. Companies have begun to offer Embedding as aService (EaaS) based on these LLMs, which can benefit various natural languageprocessing (NLP) tasks for customers. However, previous studies have shown thatEaaS is vulnerable to model extraction attacks, which can cause significantlosses for the owners of LLMs, as training these models is extremely expensive.To protect the copyright of LLMs for EaaS, we propose an Embedding Watermarkmethod called EmbMarker that implants backdoors on embeddings. Our methodselects a group of moderate-frequency words from a general text corpus to forma trigger set, then selects a target embedding as the watermark, and inserts itinto the embeddings of texts containing trigger words as the backdoor. Theweight of insertion is proportional to the number of trigger words included inthe text. This allows the watermark backdoor to be effectively transferred toEaaS-stealer\u0026rsquo;s model for copyright verification while minimizing the adverseimpact on the original embeddings\u0026rsquo; utility. Our extensive experiments onvarious datasets show that our method can effectively protect the copyright ofEaaS models without compromising service quality.\r2023-05-31\nSynthetic Pre-Training Tasks for Neural Machine Translation\nZexue He, Graeme Blackwood, Rameswar Panda, Julian McAuley, Rogerio Feris\nabstract\rabstract: Pre-training models with large crawled corpora can lead to issues such astoxicity and bias, as well as copyright and privacy concerns. A promising wayof alleviating such concerns is to conduct pre-training with synthetic tasksand data, since no real-world information is ingested by the model. Our goal inthis paper is to understand the factors that contribute to the effectiveness ofpre-training models when using synthetic resources, particularly in the contextof neural machine translation. We propose several novel approaches topre-training translation models that involve different levels of lexical andstructural knowledge, including: 1) generating obfuscated data from a largeparallel corpus 2) concatenating phrase pairs extracted from a smallword-aligned corpus, and 3) generating synthetic parallel data without realhuman language corpora. Our experiments on multiple language pairs reveal thatpre-training benefits can be realized even with high levels of obfuscation orpurely synthetic parallel data. We hope the findings from our comprehensiveempirical analysis will shed light on understanding what matters for NMTpre-training, as well as pave the way for the development of more efficient andless toxic models.\r2023-05-08\nDifferentially Private Attention Computation\nYeqi Gao, Zhao Song, Xin Yang\nabstract\rabstract: Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].\r2023-04-06\nWhose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics\nMadiha Zahrah Choksi, David Goedicke\nabstract\rabstract: Intelligent or generative writing tools rely on large language models thatrecognize, summarize, translate, and predict content. This position paperprobes the copyright interests of open data sets used to train large languagemodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumventthe copyright interests of the used data? We start by defining softwarecopyright and tracing its history. We rely on GitHub Copilot as a modern casestudy challenging software copyright. Our conclusion outlines obstacles thatgenerative writing assistants create for copyright, and offers a practical roadmap for copyright analysis for developers, software law experts, and generalusers to consider in the context of intelligent LLM-powered writing tools.\r2023-03-28\nSynthetically generated text for supervised text analysis\nAndrew Halterman\nabstract\rabstract: Supervised text models are a valuable tool for political scientists butpresent several obstacles to their use, including the expense of hand-labelingdocuments, the difficulty of retrieving rare relevant documents for annotation,and copyright and privacy concerns involved in sharing annotated documents.This article proposes a partial solution to these three issues, in the form ofcontrolled generation of synthetic text with large language models. I provide aconceptual overview of text generation, guidance on when researchers shouldprefer different techniques for generating synthetic text, a discussion ofethics, and a simple technique for improving the quality of synthetic text. Idemonstrate the usefulness of synthetic text with three applications:generating synthetic tweets describing the fighting in Ukraine, synthetic newsarticles describing specified political events for training an event detectionsystem, and a multilingual corpus of populist manifesto statements for traininga sentence-level populism classifier.\r2022-11-29\nPile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset\nPeter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, Daniel E. Ho\nabstract\rabstract: One concern with the rise of large language models lies with their potentialfor significant harm, particularly from pretraining on biased, obscene,copyrighted, and private information. Emerging ethical approaches haveattempted to filter pretraining material, but such approaches have been ad hocand failed to take context into account. We offer an approach to filteringgrounded in law, which has directly addressed the tradeoffs in filteringmaterial. First, we gather and make available the Pile of Law, a 256GB (andgrowing) dataset of open-source English-language legal and administrative data,covering court opinions, contracts, administrative rules, and legislativerecords. Pretraining on the Pile of Law may help with legal tasks that have thepromise to improve access to justice. Second, we distill the legal norms thatgovernments have developed to constrain the inclusion of toxic or privatecontent into actionable lessons for researchers and discuss how our datasetreflects these norms. Third, we show how the Pile of Law offers researchers theopportunity to learn such filtering rules directly from the data, providing anexciting new research direction in model-based processing.\r2022-08-03\nA Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval\nAlex Falcon, Giuseppe Serra, Oswald Lanz\nabstract\rabstract: Every hour, huge amounts of visual contents are posted on social media anduser-generated content platforms. To find relevant videos by means of a naturallanguage query, text-video retrieval methods have received increased attentionover the past few years. Data augmentation techniques were introduced toincrease the performance on unseen test examples by creating new trainingsamples with the application of semantics-preserving techniques, such as colorspace or geometric transformations on images. Yet, these techniques are usuallyapplied on raw data, leading to more resource-demanding solutions and alsorequiring the shareability of the raw data, which may not always be true, e.g.copyright issues with clips from movies or TV series. To address thisshortcoming, we propose a multimodal data augmentation technique which works inthe feature space and creates new videos and captions by mixing semanticallysimilar samples. We experiment our solution on a large scale public dataset,EPIC-Kitchens-100, and achieve considerable improvements over a baselinemethod, improved state-of-the-art performance, while at the same timeperforming multiple ablation studies. We release code and pretrained models onGithub at https://github.com/aranciokov/FSMMDA_VideoRetrieval.\r2021-05-11\nAddressing \u0026ldquo;Documentation Debt\u0026rdquo; in Machine Learning Research: A Retrospective Datasheet for BookCorpus\nJack Bandy, Nicholas Vincent\nabstract\rabstract: Recent literature has underscored the importance of dataset documentationwork for machine learning, and part of this work involves addressing\u0026quot;documentation debt\u0026quot; for datasets that have been used widely but documentedsparsely. This paper aims to help address documentation debt for BookCorpus, apopular text dataset for training large language models. Notably, researchershave used BookCorpus to train OpenAI\u0026rsquo;s GPT-N models and Google\u0026rsquo;s BERT models,even though little to no documentation exists about the dataset\u0026rsquo;s motivation,composition, collection process, etc. We offer a preliminary datasheet thatprovides key context and information about BookCorpus, highlighting severalnotable deficiencies. In particular, we find evidence that (1) BookCorpuslikely violates copyright restrictions for many books, (2) BookCorpus containsthousands of duplicated books, and (3) BookCorpus exhibits significant skews ingenre representation. We also find hints of other potential deficiencies thatcall for future research, including problematic content, potential skews inreligious representation, and lopsided author contributions. While more workremains, this initial effort to provide a datasheet for BookCorpus adds togrowing literature that urges more careful and systematic documentation formachine learning datasets.\r"},{"id":2,"href":"/docs/arxiv_papers/llm_privacy/","title":"LLM with Privacy","section":"Arxiv Papers","content":"\rArxiv Papers: LLM with Privacy\r#\r2024-03-20\nMELTing point: Mobile Evaluation of Language Transformers\nStefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi\nabstract\rabstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence\u0026rsquo;\u0026rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs). To achievethis, we have created our own automation infrastructure, MELT, which supportsthe headless execution and benchmarking of LLMs on device, supporting differentmodels, devices and frameworks, including Android, iOS and Nvidia Jetsondevices. We evaluate popular instruction fine-tuned LLMs and leverage differentframeworks to measure their end-to-end and granular performance, tracing theirmemory and energy requirements along the way. Our analysis is the first systematic study of on-device LLM execution,quantifying performance, energy efficiency and accuracy across variousstate-of-the-art models and showcases the state of on-device intelligence inthe era of hyperscale models. Results highlight the performance heterogeneityacross targets and corroborates that LLM inference is largely memory-bound.Quantization drastically reduces memory requirements and renders executionviable, but at a non-negligible accuracy cost. Drawing from its energyfootprint and thermal behavior, the continuous execution of LLMs remainselusive, as both factors negatively affect user experience. Last, ourexperience shows that the ecosystem is still in its infancy, and algorithmic aswell as hardware breakthroughs can significantly shift the execution cost. Weexpect NPU acceleration, and framework-hardware co-design to be the biggest bettowards efficient standalone execution, with the alternative of offloadingtailored towards edge deployments.\r2024-03-19\nRadiology-GPT: A Large Language Model for Radiology\nZhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, Tianming Liu\nabstract\rabstract: We introduce Radiology-GPT, a large language model for radiology. Using aninstruction tuning approach on an extensive dataset of radiology domainknowledge, Radiology-GPT demonstrates superior performance compared to generallanguage models such as StableLM, Dolly and LLaMA. It exhibits significantversatility in radiological diagnosis, research, and communication. This workserves as a catalyst for future developments in clinical NLP. The successfulimplementation of Radiology-GPT is indicative of the potential of localizinggenerative large language models, specifically tailored for distinctive medicalspecialties, while ensuring adherence to privacy standards such as HIPAA. Theprospect of developing individualized, large-scale language models that caterto specific needs of various hospitals presents a promising direction. Thefusion of conversational competence and domain-specific knowledge in thesemodels is set to foster future development in healthcare AI. A demo ofRadiology-GPT is available athttps://huggingface.co/spaces/allen-eric/radiology-gpt.\rSecuring Large Language Models: Threats, Vulnerabilities and Responsible Practices\nSara Abdali, Richard Anarfi, CJ Barberan, Jia He\nabstract\rabstract: Large language models (LLMs) have significantly transformed the landscape ofNatural Language Processing (NLP). Their impact extends across a diversespectrum of tasks, revolutionizing how we approach language understanding andgenerations. Nevertheless, alongside their remarkable utility, LLMs introducecritical security and risk considerations. These challenges warrant carefulexamination to ensure responsible deployment and safeguard against potentialvulnerabilities. This research paper thoroughly investigates security andprivacy concerns related to LLMs from five thematic perspectives: security andprivacy concerns, vulnerabilities against adversarial attacks, potential harmscaused by misuses of LLMs, mitigation strategies to address these challengeswhile identifying limitations of current strategies. Lastly, the paperrecommends promising avenues for future research to enhance the security andrisk management of LLMs.\rFederated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models\nSixing Yu, J. Pablo Muñoz, Ali Jannesari\nabstract\rabstract: Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, havedemonstrated remarkable success in a wide range of applications, driven bytheir ability to leverage vast amounts of data for pre-training. However,optimizing FMs often requires access to sensitive data, raising privacyconcerns and limiting their applicability in many domains. In this paper, wepropose the Federated Foundation Models (FFMs) paradigm, which combines thebenefits of FMs and Federated Learning (FL) to enable privacy-preserving andcollaborative learning across multiple end-users. We discuss the potentialbenefits and challenges of integrating FL into the lifespan of FMs, coveringpre-training, fine-tuning, and application. We further outline potential futureresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, andfederated prompt tuning, which allow the development of more personalized andcontext-aware models while ensuring data privacy. Moreover, we explore thepossibility of continual/lifelong learning in FFMs, as increased computationalpower at the edge may unlock the potential for optimizing FMs using newlygenerated private data close to the data source. The proposed FFM conceptsoffer a flexible and scalable framework for training large language models in aprivacy-preserving manner, setting the stage for subsequent advancements inboth FM training and federated learning.\r2024-03-18\nEnsuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models\nYi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong\nabstract\rabstract: Large Language Models (LLMs) exhibit impressive capabilities but also presentrisks such as biased content generation and privacy issues. One of the currentalignment techniques includes principle-driven integration, but it faceschallenges arising from the imprecision of manually crafted rules andinadequate risk perception in models without safety training. To address these,we introduce Guide-Align, a two-stage approach. Initially, a safety-trainedmodel identifies potential risks and formulates specific guidelines for variousinputs, thereby establishing a comprehensive library of guidelines and modelsfor input-guidelines retrieval. Subsequently, the retrieval model correlatesnew inputs with pertinent guidelines, guiding LLMs in response generation toensure safe and high-quality outputs, thus aligning with human values. Anadditional optional stage involves fine-tuning a model with new well-aligneddatasets generated through the process implemented in the second stage. Ourmethod customizes guidelines to accommodate diverse inputs, thereby enhancingthe fine-grainedness and comprehensiveness of the guideline library.Furthermore, it incorporates safety expertise from a safety-trained LLM througha lightweight retrieval model. We evaluated our approach on three benchmarks,demonstrating significant improvements in LLM security and quality. Notably,our fine-tuned model, Labrador, even at 13 billion parameters, outperformsGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.\rTrustLLM: Trustworthiness in Large Language Models\nLichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao\nabstract\rabstract: Large language models (LLMs), exemplified by ChatGPT, have gainedconsiderable attention for their excellent natural language processingcapabilities. Nonetheless, these LLMs present many challenges, particularly inthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMsemerges as an important topic. This paper introduces TrustLLM, a comprehensivestudy of trustworthiness in LLMs, including principles for different dimensionsof trustworthiness, established benchmark, evaluation, and analysis oftrustworthiness for mainstream LLMs, and discussion of open challenges andfuture directions. Specifically, we first propose a set of principles fortrustworthy LLMs that span eight different dimensions. Based on theseprinciples, we further establish a benchmark across six dimensions includingtruthfulness, safety, fairness, robustness, privacy, and machine ethics. Wethen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting ofover 30 datasets. Our findings firstly show that in general trustworthiness andutility (i.e., functional effectiveness) are positively related. Secondly, ourobservations reveal that proprietary LLMs generally outperform most open-sourcecounterparts in terms of trustworthiness, raising concerns about the potentialrisks of widely accessible open-source LLMs. However, a few open-source LLMscome very close to proprietary ones. Thirdly, it is important to note that someLLMs may be overly calibrated towards exhibiting trustworthiness, to the extentthat they compromise their utility by mistakenly treating benign prompts asharmful and consequently not responding. Finally, we emphasize the importanceof ensuring transparency not only in the models themselves but also in thetechnologies that underpin trustworthiness. Knowing the specific trustworthytechnologies that have been employed is crucial for analyzing theireffectiveness.\r2024-03-17\nDP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\nJunyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang\nabstract\rabstract: Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts\u0026rsquo; dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model\u0026rsquo;s provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\r2024-03-16\nNavigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning\nYunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang\nabstract\rabstract: Federated embodied agent learning protects the data privacy of individualvisual environments by keeping data locally at each client (the individualenvironment) during training. However, since the local data is inaccessible tothe server under federated learning, attackers may easily poison the trainingdata of the local client to build a backdoor in the agent without notice.Deploying such an agent raises the risk of potential harm to humans, as theattackers may easily navigate and control the agent as they wish via thebackdoor. Towards Byzantine-robust federated embodied agent learning, in thispaper, we study the attack and defense for the task of vision-and-languagenavigation (VLN), where the agent is required to follow natural languageinstructions to navigate indoor environments. First, we introduce a simple buteffective attack strategy, Navigation as Wish (NAW), in which the maliciousclient manipulates local trajectory data to implant a backdoor into the globalmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easilynavigate the deployed VLN agent regardless of the language instruction, withoutaffecting its performance on normal test sets. Then, we propose a newPrompt-Based Aggregation (PBA) to defend against the NAW attack in federatedVLN, which provides the server with a \u0026lsquo;\u0026lsquo;prompt\u0026rsquo;\u0026rsquo; of the vision-and-languagealignment variance between the benign and malicious clients so that they can bedistinguished during training. We validate the effectiveness of the PBA methodon protecting the global model from the NAW attack, which outperforms otherstate-of-the-art defense methods by a large margin in the defense metrics onR2R and RxR.\rExploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review\nLuoma Ke, Song Tong, Peng Cheng, Kaiping Peng\nabstract\rabstract: This paper explores the frontiers of large language models (LLMs) inpsychology applications. Psychology has undergone several theoretical changes,and the current use of Artificial Intelligence (AI) and Machine Learning,particularly LLMs, promises to open up new research directions. We provide adetailed exploration of how LLMs like ChatGPT are transforming psychologicalresearch. It discusses the impact of LLMs across various branches ofpsychology, including cognitive and behavioral, clinical and counseling,educational and developmental, and social and cultural psychology, highlightingtheir potential to simulate aspects of human cognition and behavior. The paperdelves into the capabilities of these models to emulate human-like textgeneration, offering innovative tools for literature review, hypothesisgeneration, experimental design, experimental subjects, data analysis, academicwriting, and peer review in psychology. While LLMs are essential in advancingresearch methodologies in psychology, the paper also cautions about theirtechnical and ethical challenges. There are issues like data privacy, theethical implications of using LLMs in psychological research, and the need fora deeper understanding of these models\u0026rsquo; limitations. Researchers shouldresponsibly use LLMs in psychological studies, adhering to ethical standardsand considering the potential consequences of deploying these technologies insensitive areas. Overall, the article provides a comprehensive overview of thecurrent state of LLMs in psychology, exploring potential benefits andchallenges. It serves as a call to action for researchers to leverage LLMs\u0026rsquo;advantages responsibly while addressing associated risks.\r2024-03-15\nBeyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning\nJianwei Li, Sheng Liu, Qi Lei\nabstract\rabstract: Language models trained via federated learning (FL) demonstrate impressivecapabilities in handling complex tasks while protecting user privacy. Recentstudies indicate that leveraging gradient information and prior knowledge canpotentially reveal training samples within FL setting. However, theseinvestigations have overlooked the potential privacy risks tied to theintrinsic architecture of the models. This paper presents a two-stage privacyattack strategy that targets the vulnerabilities in the architecture ofcontemporary language models, significantly enhancing attack performance byinitially recovering certain feature directions as additional supervisorysignals. Our comparative experiments demonstrate superior attack performanceacross various datasets and scenarios, highlighting the privacy leakage riskassociated with the increasingly complex architectures of language models. Wecall for the community to recognize and address these potential privacy risksin designing large language models.\rTriSum: Learning Summarization Ability from Large Language Models with Structured Rationale\nPengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han\nabstract\rabstract: The advent of large language models (LLMs) has significantly advanced naturallanguage processing tasks like text summarization. However, their large sizeand computational demands, coupled with privacy concerns in data transmission,limit their use in resource-constrained and privacy-centric settings. Toovercome this, we introduce TriSum, a framework for distilling LLMs\u0026rsquo; textsummarization abilities into a compact, local model. Initially, LLMs extract aset of aspect-triple rationales and summaries, which are refined using adual-scoring method for quality. Next, a smaller local model is trained withthese tasks, employing a curriculum learning strategy that evolves from simpleto complex tasks. Our method enhances local model performance on variousbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability byproviding insights into the summarization rationale.\rSocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores\nVidminas Vizgirda, Rui Zhao, Naman Goel\nabstract\rabstract: We present SocialGenPod, a decentralised and privacy-friendly way ofdeploying generative AI Web applications. Unlike centralised Web and dataarchitectures that keep user data tied to application and service providers, weshow how one can use Solid \u0026ndash; a decentralised Web specification \u0026ndash; to decoupleuser data from generative AI applications. We demonstrate SocialGenPod using aprototype that allows users to converse with different Large Language Models,optionally leveraging Retrieval Augmented Generation to generate answersgrounded in private documents stored in any Solid Pod that the user is allowedto access, directly or indirectly. SocialGenPod makes use of Solid accesscontrol mechanisms to give users full control of determining who has access todata stored in their Pods. SocialGenPod keeps all user data (chat history, appconfiguration, personal documents, etc) securely in the user\u0026rsquo;s personal Pod;separate from specific model or application providers. Besides better privacycontrols, this approach also enables portability across different services andapplications. Finally, we discuss challenges, posed by the large computerequirements of state-of-the-art models, that future research in this areashould address. Our prototype is open-source and available at:https://github.com/Vidminas/socialgenpod/.\rGenerative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity\nClaudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi\nabstract\rabstract: The advent of Generative AI, particularly through Large Language Models(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AIlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,thereby broadening their application scope. However, the complexity andemergent autonomy of these models introduce challenges in predictability andlegal compliance. This paper delves into the legal and regulatory implicationsof Generative AI and LLMs in the European Union context, analyzing aspects ofliability, privacy, intellectual property, and cybersecurity. It criticallyexamines the adequacy of the existing and proposed EU legislation, includingthe Artificial Intelligence Act (AIA) draft, in addressing the uniquechallenges posed by Generative AI in general and LLMs in particular. The paperidentifies potential gaps and shortcomings in the legislative framework andproposes recommendations to ensure the safe and compliant deployment ofgenerative models, ensuring they align with the EU\u0026rsquo;s evolving digital landscapeand legal standards.\rAraTrust: An Evaluation of Trustworthiness for LLMs in Arabic\nEmad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon\nabstract\rabstract: The swift progress and widespread acceptance of artificial intelligence (AI)systems highlight a pressing requirement to comprehend both the capabilitiesand potential risks associated with AI. Given the linguistic complexity,cultural richness, and underrepresented status of Arabic in AI research, thereis a pressing need to focus on Large Language Models (LLMs) performance andsafety for Arabic related tasks. Despite some progress in their development,there is a lack of comprehensive trustworthiness evaluation benchmarks whichpresents a major challenge in accurately assessing and improving the safety ofLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the firstcomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises516 human-written multiple-choice questions addressing diverse dimensionsrelated to truthfulness, ethics, safety, physical health, mental health,unfairness, illegal activities, privacy, and offensive language. We evaluated aset of LLMs against our benchmark to assess their trustworthiness. GPT-4 wasthe most trustworthy LLM, while open-source models, particularly AceGPT 7B andJais 13B, struggled to achieve a score of 60% in our benchmark.\r2024-03-14\nOn Protecting the Data Privacy of Large Language Models (LLMs): A Survey\nBiwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, Xiuzhen Cheng\nabstract\rabstract: Large language models (LLMs) are complex artificial intelligence systemscapable of understanding, generating and translating human language. They learnlanguage patterns by analyzing large amounts of text data, allowing them toperform writing, conversation, summarizing and other language tasks. When LLMsprocess and generate large amounts of data, there is a risk of leakingsensitive information, which may threaten data privacy. This paper concentrateson elucidating the data privacy concerns associated with LLMs to foster acomprehensive understanding. Specifically, a thorough investigation isundertaken to delineate the spectrum of data privacy threats, encompassing bothpassive privacy leakage and active privacy attacks within LLMs. Subsequently,we conduct an assessment of the privacy protection mechanisms employed by LLMsat various stages, followed by a detailed examination of their efficacy andconstraints. Finally, the discourse extends to delineate the challengesencountered and outline prospective directions for advancement in the realm ofLLM privacy protection.\rZero-shot and Few-shot Generation Strategies for Artificial Clinical Records\nErlend Frayling, Jake Lever, Graham McDonald\nabstract\rabstract: The challenge of accessing historical patient data for clinical research,while adhering to privacy regulations, is a significant obstacle in medicalscience. An innovative approach to circumvent this issue involves utilisingsynthetic medical records that mirror real patient data without compromisingindividual privacy. The creation of these synthetic datasets, particularlywithout using actual patient data to train Large Language Models (LLMs),presents a novel solution as gaining access to sensitive patient information totrain models is also a challenge. This study assesses the capability of theLlama 2 LLM to create synthetic medical records that accurately reflect realpatient information, employing zero-shot and few-shot prompting strategies forcomparison against fine-tuned methodologies that do require sensitive patientdata during training. We focus on generating synthetic narratives for theHistory of Present Illness section, utilising data from the MIMIC-IV datasetfor comparison. In this work introduce a novel prompting technique thatleverages a chain-of-thought approach, enhancing the model\u0026rsquo;s ability togenerate more accurate and contextually relevant medical narratives withoutprior fine-tuning. Our findings suggest that this chain-of-thought promptedapproach allows the zero-shot model to achieve results on par with those offine-tuned models, based on Rouge metrics evaluation.\rAnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection\nQihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen\nabstract\rabstract: Zero-shot anomaly detection (ZSAD) requires detection models trained usingauxiliary data to detect anomalies without any training sample in a targetdataset. It is a crucial task when training data is not accessible due tovarious concerns, eg, data privacy, yet it is challenging since the models needto generalize to anomalies across different domains where the appearance offoreground objects, abnormal regions, and background features, such asdefects/tumors on different products/organs, can vary significantly. Recentlylarge pre-trained vision-language models (VLMs), such as CLIP, havedemonstrated strong zero-shot recognition ability in various vision tasks,including anomaly detection. However, their ZSAD performance is weak since theVLMs focus more on modeling the class semantics of the foreground objectsrather than the abnormality/normality in the images. In this paper we introducea novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD acrossdifferent domains. The key insight of AnomalyCLIP is to learn object-agnostictext prompts that capture generic normality and abnormality in an imageregardless of its foreground objects. This allows our model to focus on theabnormal image regions rather than the object semantics, enabling generalizednormality and abnormality recognition on diverse types of objects. Large-scaleexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIPachieves superior zero-shot performance of detecting and segmenting anomaliesin datasets of highly diverse class semantics from various defect inspectionand medical imaging domains. Code will be made available athttps://github.com/zqhang/AnomalyCLIP.\r2024-03-13\nSoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks\nGuy Amit, Abigail Goldsteen, Ariel Farkash\nabstract\rabstract: Natural language processing models have experienced a significant upsurge inrecent years, with numerous applications being built upon them. Many of theseapplications require fine-tuning generic base models on customized, proprietarydatasets. This fine-tuning data is especially likely to contain personal orsensitive information about individuals, resulting in increased privacy risk.Membership inference attacks are the most commonly employed attack to assessthe privacy leakage of a machine learning model. However, limited research isavailable on the factors that affect the vulnerability of language models tothis kind of attack, or on the applicability of different defense strategies inthe language domain. We provide the first systematic review of thevulnerability of fine-tuned large language models to membership inferenceattacks, the various factors that come into play, and the effectiveness ofdifferent defense strategies. We find that some training methods providesignificantly reduced privacy risk, with the combination of differentialprivacy and low-rank adaptors achieving the best privacy protection againstthese attacks.\rSecond-Order Information Matters: Revisiting Machine Unlearning for Large Language Models\nKang Gu, Md Rafi Ur Rashid, Najrin Sultana, Shagufta Mehnaz\nabstract\rabstract: With the rapid development of Large Language Models (LLMs), we have witnessedintense competition among the major LLM products like ChatGPT, LLaMa, andGemini. However, various issues (e.g. privacy leakage and copyright violation)of the training corpus still remain underexplored. For example, the Times suedOpenAI and Microsoft for infringing on its copyrights by using millions of itsarticles for training. From the perspective of LLM practitioners, handling suchunintended privacy violations can be challenging. Previous work addressed the``unlearning\u0026quot; problem of LLMs using gradient information, while they mostlyintroduced significant overheads like data preprocessing or lacked robustness.In this paper, contrasting with the methods based on first-order information,we revisit the unlearning problem via the perspective of second-orderinformation (Hessian). Our unlearning algorithms, which are inspired by classicNewton update, are not only data-agnostic/model-agnostic but also proven to berobust in terms of utility preservation or privacy guarantee. Through acomprehensive evaluation with four NLP datasets as well as a case study onreal-world datasets, our methods consistently show superiority over thefirst-order methods.\rAGI: Artificial General Intelligence for Education\nEhsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, Xiaoming Zhai\nabstract\rabstract: Artificial general intelligence (AGI) has gained global recognition as afuture technology due to the emergence of breakthrough large language modelsand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventionalAI models, typically designed for a limited range of tasks, demand significantamounts of domain-specific data for training and may not always considerintricate interpersonal dynamics in education. AGI, driven by the recent largepre-trained models, represents a significant leap in the capability of machinesto perform tasks that require human-level intelligence, such as reasoning,problem-solving, decision-making, and even understanding human emotions andsocial interactions. This position paper reviews AGI\u0026rsquo;s key concepts,capabilities, scope, and potential within future education, including achievingfuture educational goals, designing pedagogy and curriculum, and performingassessments. It highlights that AGI can significantly improve intelligenttutoring systems, educational assessment, and evaluation procedures. AGIsystems can adapt to individual student needs, offering tailored learningexperiences. They can also provide comprehensive feedback on studentperformance and dynamically adjust teaching methods based on student progress.The paper emphasizes that AGI\u0026rsquo;s capabilities extend to understanding humanemotions and social interactions, which are critical in educational settings.The paper discusses that ethical issues in education with AGI include databias, fairness, and privacy and emphasizes the need for codes of conduct toensure responsible AGI use in academic settings like homework, teaching, andrecruitment. We also conclude that the development of AGI necessitatesinterdisciplinary collaborations between educators and AI engineers to advanceresearch and application efforts.\rTeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning\nShangding Gu, Alois Knoll, Ming Jin\nabstract\rabstract: The development of Large Language Models (LLMs) often confronts challengesstemming from the heavy reliance on human annotators in the reinforcementlearning with human feedback (RLHF) framework, or the frequent and costlyexternal queries tied to the self-instruct paradigm. In this work, we pivot toReinforcement Learning (RL) \u0026ndash; but with a twist. Diverging from the typicalRLHF, which refines LLMs following instruction data training, we use RL todirectly generate the foundational instruction dataset that alone suffices forfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations andrules, prioritizing the diversification of training datasets. It facilitatesthe generation of high-quality data without excessive reliance on externaladvanced models, paving the way for a single fine-tuning step and negating theneed for subsequent RLHF stages. Our findings highlight key advantages of ourapproach: reduced need for human involvement and fewer model queries (only$5.73%$ of WizardLM\u0026rsquo;s total), along with enhanced capabilities of LLMs incrafting and comprehending complex instructions compared to strong baselines,and substantially improved model privacy protection.\rEfficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification\nLong Lan, Fengxiang Wang, Shuyan Li, Xiangtao Zheng, Zengmao Wang, Xinwang Liu\nabstract\rabstract: Fine-grained ship classification in remote sensing (RS-FGSC) poses asignificant challenge due to the high similarity between classes and thelimited availability of labeled data, limiting the effectiveness of traditionalsupervised classification methods. Recent advancements in large pre-trainedVision-Language Models (VLMs) have demonstrated impressive capabilities infew-shot or zero-shot learning, particularly in understanding image content.This study delves into harnessing the potential of VLMs to enhanceclassification accuracy for unseen ship categories, which holds considerablesignificance in scenarios with restricted data due to cost or privacyconstraints. Directly fine-tuning VLMs for RS-FGSC often encounters thechallenge of overfitting the seen classes, resulting in suboptimalgeneralization to unseen classes, which highlights the difficulty indifferentiating complex backgrounds and capturing distinct ship features. Toaddress these issues, we introduce a novel prompt tuning technique that employsa hierarchical, multi-granularity prompt design. Our approach integrates remotesensing ship priors through bias terms, learned from a small trainable network.This strategy enhances the model\u0026rsquo;s generalization capabilities while improvingits ability to discern intricate backgrounds and learn discriminative shipfeatures. Furthermore, we contribute to the field by introducing acomprehensive dataset, FGSCM-52, significantly expanding existing datasets withmore extensive data and detailed annotations for less common ship classes.Extensive experimental evaluations demonstrate the superiority of our proposedmethod over current state-of-the-art techniques. The source code will be madepublicly available.\rOverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models\nHaomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan\nabstract\rabstract: The rapid development of Large Language Models (LLMs) has facilitated avariety of applications from different domains. In this technical report, weexplore the integration of LLMs and the popular academic writing tool,Overleaf, to enhance the efficiency and quality of academic writing. To achievethe above goal, there are three challenges: i) including seamless interactionbetween Overleaf and LLMs, ii) establishing reliable communication with the LLMprovider, and iii) ensuring user privacy. To address these challenges, wepresent OverleafCopilot, the first-ever tool (i.e., a browser extension) thatseamlessly integrates LLMs and Overleaf, enabling researchers to leverage thepower of LLMs while writing papers. Specifically, we first propose an effectiveframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, awebsite for researchers to easily find and share high-quality up-to-dateprompts. Thirdly, we propose an agent command system to help researchersquickly build their customizable agents. OverleafCopilot(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb) has been on the Chrome Extension Store, which now serves thousands ofresearchers. Additionally, the code of PromptGenius is released athttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has thepotential to revolutionize academic writing practices, empowering researchersto produce higher-quality papers in less time.\r2024-03-12\nA Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism\nZhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao, Dianhai Yu\nabstract\rabstract: As Large Language Models (LLMs) gain great success in real-worldapplications, an increasing number of users are seeking to develop and deploytheir customized LLMs through cloud services. Nonetheless, in some specificdomains, there are still concerns regarding cost and trade-offs between privacyissues and accuracy. In this study, we introduce a cost-effective andself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. Withcarefully designed horizontal and vertical shaking operators, we can achievecomparable accuracy results with SOTA privacy-preserving LLM schemes usingCryptography-based or Differential Privacy-based methods. Experiments also showthat with the CypherTalk framework, users can achieve reliable accuracy whenusing optimized shaking operator settings. To our best knowledge, this is thefirst work that considers cost, and trade-off between model utility and privacyin LLM scenarios.\rRobustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code\nZhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, David Lo\nabstract\rabstract: Large language models for code (LLM4Code), which demonstrate strongperformance (e.g., high accuracy) in processing source code, have significantlytransformed software engineering. Many studies separately investigate thenon-functional properties of LM4Code, but there is no systematic review of howthese properties are evaluated and enhanced. This paper fills this gap bythoroughly examining 146 relevant studies, thereby presenting the firstsystematic literature review to identify seven important properties beyondaccuracy, including robustness, security, privacy, explainability, efficiency,and usability. We discuss the current state-of-the-art methods and trends,identify gaps in existing research, and present promising directions for futurestudy.\rEfficient Language Model Architectures for Differentially Private Federated Learning\nJae Hun Ro, Srinadh Bhojanapalli, Zheng Xu, Yanxiang Zhang, Ananda Theertha Suresh\nabstract\rabstract: Cross-device federated learning (FL) is a technique that trains a model ondata distributed across typically millions of edge devices without data leavingthe devices. SGD is the standard client optimizer for on device training incross-device FL, favored for its memory and computational efficiency. However,in centralized training of neural language models, adaptive optimizers arepreferred as they offer improved stability and performance. In light of this,we ask if language models can be modified such that they can be efficientlytrained with SGD client optimizers and answer this affirmatively. We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrentnetwork by modifying the sigmoid and tanh activations in the recurrent cell andshow that this new model converges faster and achieves better utility than thestandard CIFG recurrent model in cross-device FL in large scale experiments. Wefurther show that the proposed scale invariant modification also helps infederated learning of larger transformer models. Finally, we demonstrate thescale invariant modification is also compatible with other non-adaptivealgorithms. Particularly, our results suggest an improved privacy utilitytrade-off in federated learning with differential privacy.\r2024-03-11\nSPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation\nYanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu, Mingbang Wang\nabstract\rabstract: Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require high computation cost andlarge memory cost. At the same time, LLMs may cause privacy leakage whentraining or prediction procedure contains sensitive information. In this paper,we propose SPA(Side Plugin Adaption), a lightweight architecture for faston-devices inference and privacy retaining on the constraints of stricton-devices computation and memory constraints. Compared with other on-devicesseq2seq generation, SPA could make a fast and stable inference on low-resourceconstraints, allowing it to obtain cost effiency. Our method establish aninteraction between a pretrained LLMs on-cloud and additive parameterson-devices, which could provide the knowledge on both pretrained LLMs andprivate personal feature.Further more, SPA provides a framework to keepfeature-base parameters on private guaranteed but low computational deviceswhile leave the parameters containing general information on the highcomputational devices.\r2024-03-10\nFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning\nZhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu\nabstract\rabstract: Instruction tuning has proven essential for enhancing the performance oflarge language models (LLMs) in generating human-aligned responses. However,collecting diverse, high-quality instruction data for tuning poses challenges,particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)has emerged as a solution, leveraging federated learning from multiple dataowners while preserving privacy. Yet, it faces challenges due to limitedinstruction data and vulnerabilities to training data extraction attacks. Toaddress these issues, we propose a novel federated algorithm, FedPIT, whichutilizes LLMs\u0026rsquo; in-context learning capability to self-generate task-specificsynthetic data for training autonomously. Our method employs parameter-isolatedtraining to maintain global parameters trained on synthetic data and localparameters trained on augmented local data, effectively thwarting dataextraction attacks. Extensive experiments on real-world medical datademonstrate the effectiveness of FedPIT in improving federated few-shotperformance while preserving privacy and robustness against data heterogeneity.\r2024-03-09\nDetecting Pretraining Data from Large Language Models\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\r2024-03-08\nDP-TabICL: In-Context Learning with Differentially Private Tabular Data\nAlycia N. Carey, Karuna Bhaila, Kennedy Edemacu, Xintao Wu\nabstract\rabstract: In-context learning (ICL) enables large language models (LLMs) to adapt tonew tasks by conditioning on demonstrations of question-answer pairs and it hasbeen shown to have comparable performance to costly model retraining andfine-tuning. Recently, ICL has been extended to allow tabular data to be usedas demonstration examples by serializing individual records into naturallanguage formats. However, it has been shown that LLMs can leak informationcontained in prompts, and since tabular data often contain sensitiveinformation, understanding how to protect the underlying tabular data used inICL is a critical area of research. This work serves as an initialinvestigation into how to use differential privacy (DP) \u0026ndash; the long-establishedgold standard for data privacy and anonymization \u0026ndash; to protect tabular dataused in ICL. Specifically, we investigate the application of DP mechanisms forprivate tabular ICL via data privatization prior to serialization andprompting. We formulate two private ICL frameworks with provable privacyguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenariosvia injecting noise into individual records or group statistics, respectively.We evaluate our DP-based frameworks on eight real-world tabular datasets andacross multiple ICL and DP settings. Our evaluations show that DP-based ICL canprotect the privacy of the underlying tabular data while achieving comparableperformance to non-LLM baselines, especially under high privacy regimes.\rSecGPT: An Execution Isolation Architecture for LLM-Based Systems\nYuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal\nabstract\rabstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begunsupporting third-party applications. These LLM apps leverage the de factonatural language-based automated execution paradigm of LLMs: that is, apps andtheir interactions are defined in natural language, provided access to userdata, and allowed to freely interact with each other and the system. These LLMapp ecosystems resemble the settings of earlier computing platforms, wherethere was insufficient isolation between apps and the system. Becausethird-party apps may not be trustworthy, and exacerbated by the imprecision ofthe natural language interfaces, the current designs pose security and privacyrisks for users. In this paper, we propose SecGPT, an architecture forLLM-based systems that aims to mitigate the security and privacy issues thatarise with the execution of third-party apps. SecGPT\u0026rsquo;s key idea is to isolatethe execution of apps and more precisely mediate their interactions outside oftheir isolated environments. We evaluate SecGPT against a number of case studyattacks and demonstrate that it protects against many security, privacy, andsafety issues that exist in non-isolated LLM-based systems. The performanceoverhead incurred by SecGPT to improve security is under 0.3x forthree-quarters of the tested queries. To foster follow-up research, we releaseSecGPT\u0026rsquo;s source code at https://github.com/llm-platform-security/SecGPT.\r2024-03-07\nMembership Inference Attacks and Privacy in Topic Modeling\nNico Manzonelli, Wanrong Zhang, Salil Vadhan\nabstract\rabstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.\rPrivacy-preserving Fine-tuning of Large Language Models through Flatness\nTiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei\nabstract\rabstract: The privacy concerns associated with the use of Large Language Models (LLMs)have grown recently with the development of LLMs such as ChatGPT. DifferentialPrivacy (DP) techniques are explored in existing work to mitigate their privacyrisks at the cost of generalization degradation. Our paper reveals that theflatness of DP-trained models\u0026rsquo; loss landscape plays an essential role in thetrade-off between their privacy and generalization. We further propose aholistic framework to enforce appropriate weight flatness, which substantiallyimproves model generalization with competitive privacy preservation. Itinnovates from three coarse-to-grained levels, including perturbation-awaremin-max optimization on model weights within a layer, flatness-guided sparseprefix-tuning on weights across layers, and weight knowledge distillationbetween DP \u0026amp; non-DP weights copies. Comprehensive experiments of bothblack-box and white-box scenarios are conducted to demonstrate theeffectiveness of our proposal in enhancing generalization and maintaining DPcharacteristics. For instance, on text classification dataset QNLI, DP-Flatachieves similar performance with non-private full fine-tuning but with DPguarantee under privacy budget $\\epsilon=3$, and even better performance givenhigher privacy budgets. Codes are provided in the supplement.\rFederated Recommendation via Hybrid Retrieval Augmented Generation\nHuimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang\nabstract\rabstract: Federated Recommendation (FR) emerges as a novel paradigm that enablesprivacy-preserving recommendations. However, traditional FR systems usuallyrepresent users/items with discrete identities (IDs), suffering fromperformance degradation due to the data sparsity and heterogeneity in FR. Onthe other hand, Large Language Models (LLMs) as recommenders have proveneffective across various recommendation scenarios. Yet, LLM-based recommendersencounter challenges such as low inference efficiency and potentialhallucination, compromising their performance in real-world scenarios. To thisend, we propose GPT-FedRec, a federated recommendation framework leveragingChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.GPT-FedRec is a two-stage solution. The first stage is a hybrid retrievalprocess, mining ID-based user patterns and text-based item features. Next, theretrieved results are converted into text prompts and fed into GPT forre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aimsto extract generalized features from data and exploit pretrained knowledgewithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, theRAG approach also prevents LLM hallucination, improving the recommendationperformance for real-world users. Experimental results on diverse benchmarkdatasets demonstrate the superior performance of GPT-FedRec againststate-of-the-art baseline methods.\r2024-03-06\nExplaining Genetic Programming Trees using Large Language Models\nPaula Maddigan, Andrew Lensen, Bing Xue\nabstract\rabstract: Genetic programming (GP) has the potential to generate explainable results,especially when used for dimensionality reduction. In this research, weinvestigate the potential of leveraging eXplainable AI (XAI) and large languagemodels (LLMs) like ChatGPT to improve the interpretability of GP-basednon-linear dimensionality reduction. Our study introduces a novel XAI dashboardnamed GP4NLDR, the first approach to combine state-of-the-art GP with anLLM-powered chatbot to provide comprehensive, user-centred explanations. Weshowcase the system\u0026rsquo;s ability to provide intuitive and insightful narratives onhigh-dimensional data reduction processes through case studies. Our studyhighlights the importance of prompt engineering in eliciting accurate andpertinent responses from LLMs. We also address important considerations arounddata privacy, hallucinatory outputs, and the rapid advancements in generativeAI. Our findings demonstrate its potential in advancing the explainability ofGP algorithms. This opens the door for future research into explaining GPmodels with LLMs.\rEnhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification\nRicardo Bigolin Lanfredi, Pritam Mukherjee, Ronald Summers\nabstract\rabstract: In chest X-ray (CXR) image analysis, rule-based systems are usually employedto extract labels from reports, but concerns exist about label quality. Thesedatasets typically offer only presence labels, sometimes with binaryuncertainty indicators, which limits their usefulness. In this work, we presentMAPLEZ (Medical report Annotations with Privacy-preserving Large language modelusing Expeditious Zero shot answers), a novel approach leveraging a locallyexecutable Large Language Model (LLM) to extract and enhance findings labels onCXR reports. MAPLEZ extracts not only binary labels indicating the presence orabsence of a finding but also the location, severity, and radiologists\u0026rsquo;uncertainty about the finding. Over eight abnormalities from five test sets, weshow that our method can extract these annotations with an increase of 5percentage points (pp) in F1 score for categorical presence annotations andmore than 30 pp increase in F1 score for the location annotations overcompeting labelers. Additionally, using these improved annotations inclassification supervision, we demonstrate substantial advancements in modelquality, with an increase of 1.7 pp in AUROC over models trained withannotations from the state-of-the-art approach. We share code and annotations.\rTowards Efficient and Effective Unlearning of Large Language Models for Recommendation\nHangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu\nabstract\rabstract: The significant advancements in large language models (LLMs) give rise to apromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).The efficacy of LLMRec arises from the open-world knowledge and reasoningcapabilities inherent in LLMs. LLMRec acquires the recommendation capabilitiesthrough instruction tuning based on user interaction data. However, in order toprotect user privacy and optimize utility, it is also crucial for LLMRec tointentionally forget specific user data, which is generally referred to asrecommendation unlearning. In the era of LLMs, recommendation unlearning posesnew challenges for LLMRec in terms of \\textit{inefficiency} and\\textit{ineffectiveness}. Existing unlearning methods require updating billionsof parameters in LLMRec, which is costly and time-consuming. Besides, theyalways impact the model utility during the unlearning process. To this end, wepropose \\textbf{E2URec}, the first \\underline{E}fficient and\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Ourproposed E2URec enhances the unlearning efficiency by updating only a fewadditional LoRA parameters, and improves the unlearning effectiveness byemploying a teacher-student framework, where we maintain multiple teachernetworks to guide the unlearning process. Extensive experiments show thatE2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affectingrecommendation performance. The source code is at\\url{https://github.com/justarter/E2URec}.\r2024-03-05\nCoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following\nKaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou\nabstract\rabstract: With the advancement of language models (LMs), their exposure to private datais increasingly inevitable, and their deployment (especially for smaller ones)on personal devices, such as PCs and smartphones, has become a prevailingtrend. In contexts laden with user information, enabling models to bothsafeguard user privacy and execute commands efficiently emerges as an essentialresearch imperative. In this paper, we propose CoGenesis, a collaborativegeneration framework integrating large (hosted on cloud infrastructure) andsmall models (deployed on local devices) to address privacy concerns logically.Initially, we design a pipeline to create personalized writing instructiondatasets enriched with extensive context details as the testbed of thisresearch issue. Subsequently, we introduce two variants of CoGenesis based onsketch and logits respectively. Our experimental findings, based on oursynthesized dataset and two additional open-source datasets, indicate that: 1)Large-scale models perform well when provided with user context but struggle inthe absence of such context. 2) While specialized smaller models fine-tuned onthe synthetic dataset show promise, they still lag behind their largercounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,showcases competitive performance, providing a feasible solution to privacyissues.\rPrivacy-Aware Semantic Cache for Large Language Models\nWaris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar\nabstract\rabstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2have revolutionized natural language processing and search engine dynamics.However, these models incur exceptionally high computational costs. Forinstance, GPT-3 consists of 175 billion parameters and inference on thesemodels also demands billions of floating-point operations. Caching is a naturalsolution to reduce LLM inference costs on repeated queries. However, existingcaching methods are incapable of finding semantic similarities among LLMqueries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a semantic cache for LLMs that identifiessemantically similar queries to determine cache hit or miss. Using MeanCache,the response to a user\u0026rsquo;s semantically similar query can be retrieved from alocal cache rather than re-querying the LLM, thus reducing costs, serviceprovider load, and environmental impact. MeanCache leverages Federated Learning(FL) to collaboratively train a query similarity model in a distributed manneracross numerous users without violating privacy. By placing a local cache ineach user\u0026rsquo;s device and using FL, MeanCache reduces the latency and costs andenhances model performance, resulting in lower cache false hit rates. Ourexperiments, benchmarked against the GPTCache, reveal that MeanCache attains anapproximately 17% higher F-score and a 20% increase in precision duringsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces thestorage requirement by 83% and accelerates semantic cache hit-and-missdecisions by 11%, while still surpassing GPTCache.\r2024-03-04\nDifferentially Private Synthetic Data via Foundation Model APIs 2: Text\nChulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin\nabstract\rabstract: Text data has become extremely valuable due to the emergence of machinelearning algorithms that learn from it. A lot of high-quality text datagenerated in the real world is private and therefore cannot be shared or usedfreely due to privacy concerns. Generating synthetic replicas of private textdata with a formal privacy guarantee, i.e., differential privacy (DP), offers apromising and scalable solution. However, existing methods necessitate DPfinetuning of large language models (LLMs) on private data to generate DPsynthetic data. This approach is not viable for proprietary LLMs (e.g.,GPT-3.5) and also demands considerable computational resources for open-sourceLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)algorithm to generate DP synthetic images with only API access to diffusionmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, thatapplies to the complex setting of text. We use API access to an LLM andgenerate DP synthetic text without any model training. We conduct comprehensiveexperiments on three benchmark datasets. Our results demonstrate that Aug-PEproduces DP synthetic text that yields competitive utility with the SOTA DPfinetuning baselines. This underscores the feasibility of relying solely on APIaccess of LLMs to produce high-quality DP synthetic texts, thereby facilitatingmore accessible routes to privacy-preserving LLM applications. Our code anddata are available at https://github.com/AI-secure/aug-pe.\rPushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities\nZheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen, Kaibin Huang\nabstract\rabstract: Large language models (LLMs), which have shown remarkable capabilities, arerevolutionizing AI development and potentially shaping our future. However,given their multimodality, the status quo cloud-based deployment faces somecritical challenges: 1) long response time; 2) high bandwidth costs; and 3) theviolation of data privacy. 6G mobile edge computing (MEC) systems may resolvethese pressing issues. In this article, we explore the potential of deployingLLMs at the 6G edge. We start by introducing killer applications powered bymultimodal LLMs, including robotics and healthcare, to highlight the need fordeploying LLMs in the vicinity of end users. Then, we identify the criticalchallenges for LLM deployment at the edge and envision the 6G MEC architecturefor LLMs. Furthermore, we delve into two design aspects, i.e., edge trainingand edge inference for LLMs. In both aspects, considering the inherent resourcelimitations at the edge, we discuss various cutting-edge techniques, includingsplit learning/inference, parameter-efficient fine-tuning, quantization, andparameter-sharing inference, to facilitate the efficient deployment of LLMs.This article serves as a position paper for thoroughly identifying themotivation, challenges, and pathway for empowering LLMs at the 6G edge.\r2024-03-03\nOn the Compressibility of Quantized Large Language Models\nYu Mao, Weilan Wang, Hongchao Du, Nan Guan, Chun Jason Xue\nabstract\rabstract: Deploying Large Language Models (LLMs) on edge or mobile devices offerssignificant benefits, such as enhanced data privacy and real-time processingcapabilities. However, it also faces critical challenges due to the substantialmemory requirement of LLMs. Quantization is an effective way of reducing themodel size while maintaining good performance. However, even afterquantization, LLMs may still be too big to fit entirely into the limited memoryof edge or mobile devices and have to be partially loaded from the storage tocomplete the inference. In this case, the I/O latency of model loading becomesthe bottleneck of the LLM inference latency. In this work, we take apreliminary step of studying applying data compression techniques to reducedata movement and thus speed up the inference of quantized LLM onmemory-constrained devices. In particular, we discussed the compressibility ofquantized LLMs, the trade-off between the compressibility and performance ofquantized LLMs, and opportunities to optimize both of them jointly.\rBreaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models\nArijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha\nabstract\rabstract: Large Language Models (LLMs) have become a cornerstone in the field ofNatural Language Processing (NLP), offering transformative capabilities inunderstanding and generating human-like text. However, with their risingprominence, the security and vulnerability aspects of these models havegarnered significant attention. This paper presents a comprehensive survey ofthe various forms of attacks targeting LLMs, discussing the nature andmechanisms of these attacks, their potential impacts, and current defensestrategies. We delve into topics such as adversarial attacks that aim tomanipulate model outputs, data poisoning that affects model training, andprivacy concerns related to training data exploitation. The paper also exploresthe effectiveness of different attack methodologies, the resilience of LLMsagainst these attacks, and the implications for model integrity and user trust.By examining the latest research, we provide insights into the currentlandscape of LLM vulnerabilities and defense mechanisms. Our objective is tooffer a nuanced understanding of LLM attacks, foster awareness within the AIcommunity, and inspire robust solutions to mitigate these risks in futuredevelopments.\rReMatch: Retrieval Enhanced Schema Matching with LLMs\nEitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha\nabstract\rabstract: Schema matching is a crucial task in data integration, involving thealignment of a source database schema with a target schema to establishcorrespondence between their elements. This task is challenging due to textualand semantic heterogeneity, as well as differences in schema sizes. Althoughmachine-learning-based solutions have been explored in numerous studies, theyoften suffer from low accuracy, require manual mapping of the schemas for modeltraining, or need access to source schema data which might be unavailable dueto privacy concerns. In this paper we present a novel method, named ReMatch,for matching schemas using retrieval-enhanced Large Language Models (LLMs). Ourmethod avoids the need for predefined mapping, any model training, or access todata in the source database. In the ReMatch method the tables of the targetschema and the attributes of the source schema are first represented asstructured passage-based documents. For each source attribute document, weretrieve $J$ documents, representing target schema tables, according to theirsemantic relevance. Subsequently, we create a prompt for every source table,comprising all its attributes and their descriptions, alongside all attributesfrom the set of top $J$ target tables retrieved previously. We employ LLMsusing this prompt for the matching task, yielding a ranked list of $K$potential matches for each source attribute. Our experimental results on largereal-world schemas demonstrate that ReMatch significantly improves matchingcapabilities and outperforms other machine learning approaches. By eliminatingthe requirement for training data, ReMatch becomes a viable solution forreal-world scenarios.\r2024-03-02\nAnalysis of Privacy Leakage in Federated Large Language Models\nMinh N. Vu, Truc Nguyen, Tre\u0026rsquo; R. Jeter, My T. Thai\nabstract\rabstract: With the rapid adoption of Federated Learning (FL) as the training and tuningprotocol for applications utilizing Large Language Models (LLMs), recentresearch highlights the need for significant modifications to FL to accommodatethe large-scale of LLMs. While substantial adjustments to the protocol havebeen introduced as a response, comprehensive privacy analysis for the adaptedFL protocol is currently lacking. To address this gap, our work delves into an extensive examination of theprivacy analysis of FL when used for training LLMs, both from theoretical andpractical perspectives. In particular, we design two active membershipinference attacks with guaranteed theoretical success rates to assess theprivacy leakages of various adapted FL configurations. Our theoretical findingsare translated into practical attacks, revealing substantial privacyvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, andOpenAI\u0026rsquo;s GPTs, across multiple real-world language datasets. Additionally, weconduct thorough experiments to evaluate the privacy leakage of these modelswhen data is protected by state-of-the-art differential privacy (DP)mechanisms.\rKnowledge Sanitization of Large Language Models\nYoichi Ishibashi, Hidetoshi Shimodaira\nabstract\rabstract: We explore a knowledge sanitization approach to mitigate the privacy concernsassociated with large language models (LLMs). LLMs trained on a large corpus ofWeb data can memorize and potentially reveal sensitive or confidentialinformation, raising critical security concerns. Our technique efficientlyfine-tunes these models using the Low-Rank Adaptation (LoRA) method, promptingthem to generate harmless responses such as ``I don\u0026rsquo;t know\u0026rsquo;\u0026rsquo; when queried aboutspecific information. Experimental results in a closed-book question-answeringtask show that our straightforward method not only minimizes particularknowledge leakage but also preserves the overall performance of LLMs. These twoadvantages strengthen the defense against extraction attacks and reduces theemission of harmful content such as hallucinations.\rInexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy\nJamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot\nabstract\rabstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel\u0026rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to theirU-MIA'' counterparts). We propose a categorization of existing U-MIAs intopopulation U-MIAs\u0026rsquo;\u0026rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs\u0026rsquo;\u0026rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.\rEvaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data\nAritra Hota, Soumyajit Chatterjee, Sandip Chakraborty\nabstract\rabstract: Traditional human-in-the-loop-based annotation for time-series data likeinertial data often requires access to alternate modalities like video or audiofrom the environment. These alternate sources provide the necessary informationto the human annotator, as the raw numeric data is often too obfuscated evenfor an expert. However, this traditional approach has many concerns surroundingoverall cost, efficiency, storage of additional modalities, time, scalability,and privacy. Interestingly, recent large language models (LLMs) are alsotrained with vast amounts of publicly available alphanumeric data, which allowsthem to comprehend and perform well on tasks beyond natural languageprocessing. Naturally, this opens up a potential avenue to explore LLMs asvirtual annotators where the LLMs will be directly provided the raw sensor datafor annotation instead of relying on any alternate modality. Naturally, thiscould mitigate the problems of the traditional human-in-the-loop approach.Motivated by this observation, we perform a detailed study in this paper toassess whether the state-of-the-art (SOTA) LLMs can be used as virtualannotators for labeling time-series physical sensing data. To perform this in aprincipled manner, we segregate the study into two major phases. In the firstphase, we investigate the challenges an LLM like GPT-4 faces in comprehendingraw sensor data. Considering the observations from phase 1, in the next phase,we investigate the possibility of encoding the raw sensor data using SOTA SSLapproaches and utilizing the projected time-series data to get annotations fromthe LLM. Detailed evaluation with four benchmark HAR datasets shows thatSSL-based encoding and metric-based guidance allow the LLM to make morereasonable decisions and provide accurate annotations without requiringcomputationally expensive fine-tuning or sophisticated prompt engineering.\r2024-03-01\nDifferentially Private Knowledge Distillation via Synthetic Text Generation\nJames Flemings, Murali Annavaram\nabstract\rabstract: Large Language models (LLMs) are achieving state-of-the-art performance inmany different downstream tasks. However, the increasing urgency of dataprivacy requires LLMs to train with Differential Privacy (DP) on private data.Concurrently it is also necessary to compress LLMs for real-life deployments onresource-constrained devices or latency-sensitive applications. Differentialprivacy and model compression generally must trade off utility loss to achievetheir objectives. Moreover, concurrently achieving both can result in even moreutility loss. To this end, we propose a novel differentially private knowledgedistillation algorithm that exploits synthetic data generated by adifferentially private LLM. The knowledge of a teacher model is transferredonto the student in two ways: one way from the synthetic data itself, the hardlabels, and the other way by the output distribution of the teacher modelevaluated on the synthetic data, the soft labels. Furthermore, if the teacherand student share a similar architectural structure, we can further distillknowledge by exploiting hidden representations. Our results show that ourframework substantially improves the utility over existing baselines withstrong privacy parameters, {\\epsilon} = 2, validating that we can successfullycompress autoregressive LLMs while preserving the privacy of training data.\rTeach LLMs to Phish: Stealing Private Information from Language Models\nAshwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang, Yaoqing Yang, Prateek Mittal\nabstract\rabstract: When large language models are trained on private data, it can be asignificant privacy risk for them to memorize and regurgitate sensitiveinformation. In this work, we propose a new practical data extraction attackthat we call \u0026ldquo;neural phishing\u0026rdquo;. This attack enables an adversary to target andextract sensitive or personally identifiable information (PII), e.g., creditcard numbers, from a model trained on user data with upwards of 10% attacksuccess rates, at times, as high as 50%. Our attack assumes only that anadversary can insert as few as 10s of benign-appearing sentences into thetraining dataset using only vague priors on the structure of the user data.\rBasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)\nSean Wellington\nabstract\rabstract: BasedAI is a distributed network of machines which introduces decentralizedinfrastructure capable of integrating Fully Homomorphic Encryption (FHE) withany large language model (LLM) connected to its network. The proposed frameworkembeds a default mechanism, called \u0026ldquo;Cerberus Squeezing\u0026rdquo;, into the miningprocess which enables the transformation of a standard LLMs into encryptedzero-knowledge LLMs, or \u0026ldquo;ZK-LLMs\u0026rdquo;, leveraging insights from generativeadversarial networks for data privacy. This novel quantization mechanismempowers BasedAI miners to process and respond to prompts derived from Userinteraction with LLMs without the need for decrypting either the queries ortheir corresponding responses. The introduction of Cerberus Squeezingsignificantly improves performance degradation caused by quantized functions incurrent FHE-compliant computing environments by proactively optimizing callsbetween users, miners, and validators.\r2024-02-29\nTowards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models\nChen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao\nabstract\rabstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Moststudies concentrate on fully pre-trained LLMs to better understand and improveLLMs\u0026rsquo; trustworthiness. In this paper, to reveal the untapped potential ofpre-training, we pioneer the exploration of LLMs\u0026rsquo; trustworthiness during thisperiod, focusing on five key dimensions: reliability, privacy, toxicity,fairness, and robustness. To begin with, we apply linear probing to LLMs. Thehigh probing accuracy suggests that \\textit{LLMs in early pre-training canalready distinguish concepts in each trustworthiness dimension}. Therefore, tofurther uncover the hidden possibilities of pre-training, we extract steeringvectors from a LLM\u0026rsquo;s pre-training checkpoints to enhance the LLM\u0026rsquo;strustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutualinformation estimation is bounded by linear probing accuracy, we also probeLLMs with mutual information to investigate the dynamics of trustworthinessduring pre-training. We are the first to observe a similar two-phasephenomenon: fitting and compression~\\citep{shwartz2017opening}. This researchprovides an initial exploration of trustworthiness modeling during LLMpre-training, seeking to unveil new insights and spur further developments inthe field. We will make our code publicly accessible at\\url{https://github.com/ChnQ/TracingLLM}.\r2024-02-28\nFedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing\nTerence Jie Chua, Wenhan Yu, Jun Zhao, Kwok-Yan Lam\nabstract\rabstract: The emergence of foundation models, including language and vision models, hasreshaped AI\u0026rsquo;s landscape, offering capabilities across various applications.Deploying and fine-tuning these large models, like GPT-3 and BERT, presentschallenges, especially in the current foundation model era. We introduceEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, weexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT usesadapters, emulators, and PEFT for federated model tuning, enhancing modelprivacy and memory efficiency. Adapters adjust pre-trained models, whileemulators give a compact representation of original models, addressing bothprivacy and efficiency. Adaptable to various neural networks, our approach alsouses deep reinforcement learning for hyper-parameter optimization. We testedFedPEAT in a unique scenario with a server participating in collaborativefederated tuning, showcasing its potential in tackling foundation modelchallenges.\rMedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices\nAbdul Basit, Khizar Hussain, Muhammad Abdullah Hanif, Muhammad Shafique\nabstract\rabstract: Large language models (LLMs) are revolutionizing various domains with theirremarkable natural language processing (NLP) abilities. However, deploying LLMsin resource-constrained edge computing and embedded systems presentssignificant challenges. Another challenge lies in delivering medical assistancein remote areas with limited healthcare facilities and infrastructure. Toaddress this, we introduce MedAide, an on-premise healthcare chatbot. Itleverages tiny-LLMs integrated with LangChain, providing efficient edge-basedpreliminary medical diagnostics and support. MedAide employs modeloptimizations for minimal memory footprint and latency on embedded edge deviceswithout server infrastructure. The training process is optimized using low-rankadaptation (LoRA). Additionally, the model is trained on diverse medicaldatasets, employing reinforcement learning from human feedback (RLHF) toenhance its domain-specific capabilities. The system is implemented on variousconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77%accuracy in medical consultations and scores 56 in USMLE benchmark, enabling anenergy-efficient healthcare assistance platform that alleviates privacyconcerns due to edge-based deployment, thereby empowering the community.\r2024-02-27\nBASES: Large-scale Web Search User Simulation with Large Language Model based Agents\nRuiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang\nabstract\rabstract: Due to the excellent capacities of large language models (LLMs), it becomesfeasible to develop LLM-based agents for reliable user simulation. Consideringthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,we conduct large-scale user simulation for web search, to improve the analysisand modeling of user search behavior. Specially, we propose BASES, a novel usersimulation framework with LLM-based agents, designed to facilitatecomprehensive simulations of web search user behaviors. Our simulationframework can generate unique user profiles at scale, which subsequently leadsto diverse search behaviors. To demonstrate the effectiveness of BASES, weconduct evaluation experiments based on two human benchmarks in both Chineseand English, demonstrating that BASES can effectively simulate large-scalehuman-like search behaviors. To further accommodate the research on web search,we develop WARRIORS, a new large-scale dataset encompassing web search userbehaviors, including both Chinese and English versions, which can greatlybolster research in the field of information retrieval. Our code and data willbe publicly released soon.\r2024-02-26\nPrivacy-Preserved Neural Graph Databases\nQi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song\nabstract\rabstract: In the era of large language models (LLMs), efficient and accurate dataretrieval has become increasingly crucial for the use of domain-specific orprivate data in the retrieval augmented generation (RAG). Neural graphdatabases (NGDBs) have emerged as a powerful paradigm that combines thestrengths of graph databases (GDBs) and neural networks to enable efficientstorage, retrieval, and analysis of graph-structured data which can beadaptively trained with LLMs. The usage of neural embedding storage and Complexneural logical Query Answering (CQA) provides NGDBs with generalizationability. When the graph is incomplete, by extracting latent patterns andrepresentations, neural graph databases can fill gaps in the graph structure,revealing hidden relationships and enabling accurate query answering.Nevertheless, this capability comes with inherent trade-offs, as it introducesadditional privacy risks to the domain-specific or private databases. Maliciousattackers can infer more sensitive information in the database usingwell-designed queries such as from the answer sets of where Turing Awardwinners born before 1950 and after 1940 lived, the living places of TuringAward winner Hinton are probably exposed, although the living places may havebeen deleted in the training stage due to the privacy concerns. In this work,we propose a privacy-preserved neural graph database (P-NGDB) framework toalleviate the risks of privacy leakage in NGDBs. We introduce adversarialtraining techniques in the training stage to enforce the NGDBs to generateindistinguishable answers when queried with private information, enhancing thedifficulty of inferring sensitive information through combinations of multipleinnocuous queries.\rCodeS: Towards Building Open-source Language Models for Text-to-SQL\nHaoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, Hong Chen\nabstract\rabstract: Language models have shown promising performance on the task of translatingnatural language questions into SQL queries (Text-to-SQL). However, most of thestate-of-the-art (SOTA) approaches rely on powerful yet closed-source largelanguage models (LLMs), such as ChatGPT and GPT-4, which may have thelimitations of unclear model architectures, data privacy risks, and expensiveinference overheads. To address the limitations, we introduce CodeS, a seriesof pre-trained language models with parameters ranging from 1B to 15B,specifically designed for the text-to-SQL task. CodeS is a fully open-sourcelanguage model, which achieves superior accuracy with much smaller parametersizes. This paper studies the research challenges in building CodeS. To enhancethe SQL generation abilities of CodeS, we adopt an incremental pre-trainingapproach using a specifically curated SQL-centric corpus. Based on this, weaddress the challenges of schema linking and rapid domain adaptation throughstrategic prompt construction and a bi-directional data augmentation technique.We conduct comprehensive evaluations on multiple datasets, including the widelyused Spider benchmark, the newly released BIRD benchmark, robustness-diagnosticbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, aswell as two real-world datasets created for financial and academicapplications. The experimental results show that our CodeS achieves new SOTAaccuracy and robustness on nearly all challenging text-to-SQL benchmarks.\rTricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks\nAbhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury\nabstract\rabstract: Recent explorations with commercial Large Language Models (LLMs) have shownthat non-expert users can jailbreak LLMs by simply manipulating their prompts;resulting in degenerate output behavior, privacy and security breaches,offensive outputs, and violations of content regulator policies. Limitedstudies have been conducted to formalize and analyze these attacks and theirmitigations. We bridge this gap by proposing a formalism and a taxonomy ofknown (and possible) jailbreaks. We survey existing jailbreak methods and theireffectiveness on open-source and commercial LLMs (such as GPT-based models,OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreakdetection in terms of their effectiveness against known attacks. For ouranalysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. Wewill make the dataset public along with the model outputs.\rA Paradigm Shift: The Future of Machine Translation Lies with Large Language Models\nChenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham Fikri Aji, Derek F. Wong, Longyue Wang\nabstract\rabstract: Machine Translation (MT) has greatly advanced over the years due to thedevelopments in deep neural networks. However, the emergence of Large LanguageModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MTdomain. In this context, we believe that the future of MT is intricately tiedto the capabilities of LLMs. These models not only offer vast linguisticunderstandings but also bring innovative methodologies, such as prompt-basedtechniques, that have the potential to further elevate MT. In this paper, weprovide an overview of the significant enhancements in MT that are influencedby LLMs and advocate for their pivotal role in upcoming MT research andimplementations. We highlight several new MT directions, emphasizing thebenefits of LLMs in scenarios such as Long-Document Translation, StylizedTranslation, and Interactive Translation. Additionally, we address theimportant concern of privacy in LLM-driven MT and suggest essentialprivacy-preserving strategies. By showcasing practical instances, we aim todemonstrate the advantages that LLMs offer, particularly in tasks liketranslating extended documents. We conclude by emphasizing the critical role ofLLMs in guiding the future evolution of MT and offer a roadmap for futureexploration in the sector.\rMobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\nOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan\nabstract\rabstract: \u0026ldquo;Bigger the better\u0026rdquo; has been the predominant trend in recent Large LanguageModels (LLMs) development. However, LLMs do not suit well for scenarios thatrequire on-device processing, energy efficiency, low memory footprint, andresponse efficiency. These requisites are crucial for privacy, security, andsustainable deployment. This paper explores the \u0026ldquo;less is more\u0026rdquo; paradigm byaddressing the challenge of designing accurate yet efficient Small LanguageModels (SLMs) for resource constrained devices. Our primary contribution is theintroduction of an accurate and fully transparent open-source 0.5 billion(0.5B) parameter SLM, named MobiLlama, catering to the specific needs ofresource-constrained computing with an emphasis on enhanced performance withreduced resource demands. MobiLlama is a SLM design that initiates from alarger model and applies a careful parameter sharing scheme to reduce both thepre-training and the deployment cost. Our work strives to not only bridge thegap in open-source SLMs but also ensures full transparency, where completetraining data pipeline, training code, model weights, and over 300 checkpointsalong with evaluation codes is available at :https://github.com/mbzuai-oryx/MobiLlama.\rKnowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking\nSamuel Kernan Freire, Chaofan Wang, Mina Foosherian, Stefan Wellsandt, Santiago Ruiz-Arenas, Evangelos Niforatos\nabstract\rabstract: Recent advances in natural language processing enable more intelligent waysto support knowledge sharing in factories. In manufacturing, operatingproduction lines has become increasingly knowledge-intensive, putting strain ona factory\u0026rsquo;s capacity to train and support new operators. This paper introducesa Large Language Model (LLM)-based system designed to retrieve information fromthe extensive knowledge contained in factory documentation and knowledge sharedby expert operators. The system aims to efficiently answer queries fromoperators and facilitate the sharing of new knowledge. We conducted a userstudy at a factory to assess its potential impact and adoption, elicitingseveral perceived benefits, namely, enabling quicker information retrieval andmore efficient resolution of issues. However, the study also highlighted apreference for learning from a human expert when such an option is available.Furthermore, we benchmarked several commercial and open-sourced LLMs for thissystem. The current state-of-the-art model, GPT-4, consistently outperformedits counterparts, with open-source models trailing closely, presenting anattractive option given their data privacy and customization benefits. Insummary, this work offers preliminary insights and a system design forfactories considering using LLM tools for knowledge management.\rPandora\u0026rsquo;s White-Box: Increased Training Data Leakage in Open LLMs\nJeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel\nabstract\rabstract: In this paper we undertake a systematic study of privacy attacks against opensource Large Language Models (LLMs), where an adversary has access to eitherthe model weights, gradients, or losses, and tries to exploit them to learnsomething about the underlying training data. Our headline results are thefirst membership inference attacks (MIAs) against pre-trained LLMs that areable to simultaneously achieve high TPRs and low FPRs, and a pipeline showingthat over $50%$ (!) of the fine-tuning dataset can be extracted from afine-tuned LLM in natural settings. We consider varying degrees of access tothe underlying model, customization of the language model, and resourcesavailable to the attacker. In the pre-trained setting, we propose three newwhite-box MIAs: an attack based on the gradient norm, a supervised neuralnetwork classifier, and a single step loss ratio attack. All outperformexisting black-box baselines, and our supervised attack closes the gap betweenMIA attack success against LLMs and other types of models. In fine-tuning, wefind that given access to the loss of the fine-tuned and base models, afine-tuned loss ratio attack FLoRA is able to achieve near perfect MIApeformance. We then leverage these MIAs to extract fine-tuning data fromfine-tuned language models. We find that the pipeline of generating fromfine-tuned models prompted with a small snippet of the prefix of each trainingexample, followed by using FLoRa to select the most likely training sample,succeeds the majority of the fine-tuning dataset after only $3$ epochs offine-tuning. Taken together, these findings show that highly effective MIAs areavailable in almost all LLM training settings, and highlight that great caremust be taken before LLMs are fine-tuned on highly sensitive data and thendeployed.\rDecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\nBoxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li\nabstract\rabstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance \u0026ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives \u0026ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .\rLLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery\nKexin Chen, Yuyang Du, Tao You, Mobarakol Islam, Ziyu Guo, Yueming Jin, Guangyong Chen, Pheng-Ann Heng\nabstract\rabstract: Visual question answering (VQA) can be fundamentally crucial for promotingrobotic-assisted surgical education. In practice, the needs of trainees areconstantly evolving, such as learning more surgical types, adapting todifferent robots, and learning new surgical instruments and techniques for onesurgery. Therefore, continually updating the VQA system by a sequential datastream from multiple resources is demanded in robotic surgery to address newtasks. In surgical scenarios, the storage cost and patient data privacy oftenrestrict the availability of old data when updating the model, necessitating anexemplar-free continual learning (CL) setup. However, prior studies overlookedtwo vital problems of the surgical domain: i) large domain shifts from diversesurgical operations collected from multiple departments or clinical centers,and ii) severe data imbalance arising from the uneven presence of surgicalinstruments or activities during surgical procedures. This paper proposes toaddress these two problems with a multimodal large language model (LLM) and anadaptive weight assignment methodology. We first develop a new multi-teacher CLframework that leverages a multimodal LLM as the additional teacher. The stronggeneralization ability of the LLM can bridge the knowledge gap when domainshifts and data imbalances occur. We then put forth a novel data processingmethod that transforms complex LLM embeddings into logits compatible with ourCL framework. We further design an adaptive weight assignment approach thatbalances the generalization ability of the LLM and the domain expertise of theold CL model. We construct a new dataset for surgical VQA tasks, providingvaluable data resources for future research. Extensive experimental results onthree datasets demonstrate the superiority of our method to other advanced CLmodels.\r2024-02-25\nText Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations\nYafei Xiang, Hanyi Yu, Yulu Gong, Shuning Huo, Mengran Zhu\nabstract\rabstract: With the rapid development of artificial intelligence technology, Transformerstructural pre-training model has become an important tool for large languagemodel (LLM) tasks. In the field of e-commerce, these models are especiallywidely used, from text understanding to generating recommendation systems,which provide powerful technical support for improving user experience andoptimizing service processes. This paper reviews the core application scenariosof Transformer pre-training model in e-commerce text understanding andrecommendation generation, including but not limited to automatic generation ofproduct descriptions, sentiment analysis of user comments, construction ofpersonalized recommendation system and automated processing of customer serviceconversations. Through a detailed analysis of the model\u0026rsquo;s working principle,implementation process, and application effects in specific cases, this paperemphasizes the unique advantages of pre-trained models in understanding complexuser intentions and improving the quality of recommendations. In addition, thechallenges and improvement directions for the future are also discussed, suchas how to further improve the generalization ability of the model, the abilityto handle large-scale data sets, and technical strategies to protect userprivacy. Ultimately, the paper points out that the application of Transformerstructural pre-training models in e-commerce has not only driven technologicalinnovation, but also brought substantial benefits to merchants and consumers,and looking forward, these models will continue to play a key role ine-commerce and beyond.\r2024-02-23\nThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nShenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang\nabstract\rabstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs\u0026rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.\rUser Inference Attacks on Large Language Models\nNikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu\nabstract\rabstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user\u0026rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user\u0026rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.\r2024-02-22\nExploring Memorization in Fine-tuned Language Models\nShenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin\nabstract\rabstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models\u0026rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.\rSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\nHuachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan\nabstract\rabstract: Developing specialized dialogue systems for mental health support requiresmulti-turn conversation data, which has recently garnered increasing attention.However, gathering and releasing large-scale and real-life multi-turnconversations to facilitate advancements in mental health presents challengesdue to data privacy protection, as well as the time and cost involved. Toaddress the challenges related to data scarcity, we introduce SMILE, asingle-turn to multi-turn inclusive language expansion technique that promptsChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our workbegins with the analysis of language transformation, validating the feasibilityof the proposed method when compared with other baseline methods. We thenconduct a study on dialogue diversity, including lexical features, semanticfeatures, and dialogue topics, demonstrating the effectiveness of our proposedmethod. Furthermore, we implement an expert evaluation and the resultsdemonstrate that the dialogues generated with our proposed method are of higherquality than those generated with other baseline methods. Thus, we employ ourmethod to generate a large-scale, diverse, and high-quality dialogue datasetnamed SmileChat, comprising 55,165 dialogues in total with an average of 10.4turns per dialogue. Finally, we utilize the collected corpus to develop amental health chatbot, MeChat. To better assess the overall quality ofSmileChat, we collect a real-life chat dataset comprising 82 counselingdialogues for model evaluation. Both automatic and human evaluationsdemonstrate that our trained dialogue system exhibits significant improvements,showcasing that SmileChat is high-quality and practical.\r2024-02-21\nPrivacy-Preserving Instructions for Aligning Large Language Models\nDa Yu, Peter Kairouz, Sewoong Oh, Zheng Xu\nabstract\rabstract: Service providers of large language model (LLM) applications collect userinstructions in the wild and use them in further aligning LLMs with users\u0026rsquo;intentions. These instructions, which potentially contain sensitiveinformation, are annotated by human workers in the process. This poses a newprivacy risk not addressed by the typical private optimization. To this end, wepropose using synthetic instructions to replace real instructions in dataannotation and model fine-tuning. Formal differential privacy is guaranteed bygenerating those synthetic instructions using privately fine-tuned generators.Crucial in achieving the desired utility is our novel filtering algorithm thatmatches the distribution of the synthetic instructions to that of the realones. In both supervised fine-tuning and reinforcement learning from humanfeedback, our extensive experiments demonstrate the high utility of the finalset of synthetic instructions by showing comparable results to realinstructions. In supervised fine-tuning, models trained with private syntheticinstructions outperform leading open-source models such as Vicuna.\rLarge Language Models are Advanced Anonymizers\nRobin Staab, Mark Vero, Mislav Balunović, Martin Vechev\nabstract\rabstract: Recent work in privacy research on large language models has shown that theyachieve near human-level performance at inferring personal data from real-worldonline texts. With consistently increasing model capabilities, existing textanonymization methods are currently lacking behind regulatory requirements andadversarial threats. This raises the question of how individuals caneffectively protect their personal data in sharing online texts. In this work,we take two steps to answer this question: We first present a new setting forevaluating anonymizations in the face of adversarial LLMs inferences, allowingfor a natural measurement of anonymization performance while remedying some ofthe shortcomings of previous metrics. We then present our LLM-based adversarialanonymization framework leveraging the strong inferential capabilities of LLMsto inform our anonymization procedure. In our experimental evaluation, we showon real-world and synthetic online texts how adversarial anonymizationoutperforms current industry-grade anonymizers both in terms of the resultingutility and privacy.\r2024-02-20\nReducing Privacy Risks in Online Self-Disclosures with Language Models\nYao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu\nabstract\rabstract: Self-disclosure, while being common and rewarding in social mediainteraction, also poses privacy risks. In this paper, we take the initiative toprotect the user-side privacy associated with online self-disclosure throughdetection and abstraction. We develop a taxonomy of 19 self-disclosurecategories and curate a large corpus consisting of 4.8K annotated disclosurespans. We then fine-tune a language model for detection, achieving over 65%partial span F$_1$. We further conduct an HCI user study, with 82% ofparticipants viewing the model positively, highlighting its real-worldapplicability. Motivated by the user feedback, we introduce the task ofself-disclosure abstraction, which is paraphrasing disclosures into lessspecific terms while preserving their utility, e.g., \u0026ldquo;Im 16F\u0026rdquo; to \u0026ldquo;I\u0026rsquo;m a teenagegirl\u0026rdquo;. We explore various fine-tuning strategies, and our best model cangenerate diverse abstractions that moderately reduce privacy risks whilemaintaining high utility according to human evaluation. To help users indeciding which disclosures to abstract, we present a task of rating theirimportance for context understanding. Our fine-tuned model achieves 80%accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we willonly release our corpus to researchers who agree to ethical guidelines.\rPrivacy Issues in Large Language Models: A Survey\nSeth Neel, Peter Chang\nabstract\rabstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.\rRTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution\nShang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, Zhiyao Xie\nabstract\rabstract: The automatic generation of RTL code (e.g., Verilog) using natural languageinstructions and large language models (LLMs) has attracted significantresearch interest recently. However, most existing approaches heavily rely oncommercial LLMs such as ChatGPT, while open-source LLMs tailored for thisspecific design generation task exhibit notably inferior performance. Theabsence of high-quality open-source solutions restricts the flexibility anddata privacy of this emerging technique. In this study, we present a newcustomized LLM solution with a modest parameter count of only 7B, achievingbetter performance than GPT-3.5 on two representative benchmarks for RTL codegeneration. This remarkable balance between accuracy and efficiency is madepossible by leveraging our new RTL code dataset and a customized LLM algorithm,both of which will be made fully open-source. Furthermore, we have successfullyquantized our LLM to 4-bit with a total size of 4GB, enabling it to function ona single laptop with only slight performance degradation. This efficiencyallows the RTL generator to serve as a local assistant for engineers, ensuringall design privacy concerns are addressed.\rOn the Convergence of Zeroth-Order Federated Tuning for Large Language Models\nZhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen\nabstract\rabstract: The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area.\r2024-02-19\nPurifying Large Language Models by Ensembling a Small Language Model\nTianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin\nabstract\rabstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.\rDistilling Large Language Models for Text-Attributed Graph Learning\nBo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao\nabstract\rabstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents.Graph models can efficiently learn TAGs, but their training heavily relies onhuman-annotated labels, which are scarce or even unavailable in manyapplications. Large language models (LLMs) have recently demonstratedremarkable capabilities in few-shot and zero-shot TAG learning, but they sufferfrom scalability, cost, and privacy issues. Therefore, in this work, we focuson synergizing LLMs and graph models with their complementary strengths bydistilling the power of LLMs to a local graph model on TAG learning. To addressthe inherent gaps between LLMs (generative models for texts) and graph models(discriminative models for graphs), we propose first to let LLMs teach aninterpreter with rich textual rationale and then let a student model mimic theinterpreter\u0026rsquo;s reasoning without LLMs\u0026rsquo; textual rationale. Extensive experimentsvalidate the efficacy of our proposed framework.\rKnowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion\nJinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen herring, Sujay Kumar Jauhar\nabstract\rabstract: Large Language Models (LLMs) excel at tackling various natural languagetasks. However, due to the significant costs involved in re-training orfine-tuning them, they remain largely static and difficult to personalize.Nevertheless, a variety of applications could benefit from generations that aretailored to users\u0026rsquo; preferences, goals, and knowledge. Among them is web search,where knowing what a user is trying to accomplish, what they care about, andwhat they know can lead to improved search experiences. In this work, wepropose a novel and general approach that augments an LLM with relevant contextfrom users\u0026rsquo; interaction histories with a search engine in order to personalizeits outputs. Specifically, we construct an entity-centric knowledge store foreach user based on their search and browsing activities on the web, which isthen leveraged to provide contextually relevant LLM prompt augmentations. Thisknowledge store is light-weight, since it only produces user-specific aggregateprojections of interests and knowledge onto public knowledge graphs, andleverages existing search log infrastructure, thereby mitigating the privacy,compliance, and scalability concerns associated with building deep userprofiles for personalization. We validate our approach on the task ofcontextual query suggestion, which requires understanding not only the user\u0026rsquo;scurrent search context but also what they historically know and care about.Through a number of experiments based on human evaluation, we show that ourapproach is significantly better than several other LLM-powered baselines,generating query suggestions that are contextually more relevant, personalized,and useful.\rIs Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports\nFelix J. Dorfner, Liv Jürgensen, Leonhard Donle, Fares Al Mohamad, Tobias R. Bodenmann, Mason C. Cleveland, Felix Busch, Lisa C. Adams, James Sato, Thomas Schultz, Albert E. Kim, Jameson Merkow, Keno K. Bressem, Christopher P. Bridge\nabstract\rabstract: Introduction: With the rapid advances in large language models (LLMs), therehave been numerous new open source as well as commercial models. While recentpublications have explored GPT-4 in its application to extracting informationof interest from radiology reports, there has not been a real-world comparisonof GPT-4 to different leading open-source models. Materials and Methods: Two different and independent datasets were used. Thefirst dataset consists of 540 chest x-ray reports that were created at theMassachusetts General Hospital between July 2019 and July 2021. The seconddataset consists of 500 chest x-ray reports from the ImaGenome dataset. We thencompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to theopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accuratelylabel the presence of multiple findings in x-ray text reports using differentprompting techniques. Results: On the ImaGenome dataset, the best performing open-source model wasLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shotprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,respectively. On the institutional dataset, the best performing open-sourcemodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- andfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and0.973, respectively. Conclusion: In this paper, we show that while GPT-4 is superior toopen-source models in zero-shot report labeling, the implementation of few-shotprompting can bring open-source models on par with GPT-4. This shows thatopen-source models could be a performant and privacy preserving alternative toGPT-4 for the task of radiology report classification.\rNOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization\nImjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park\nabstract\rabstract: The discharge summary is a one of critical documents in the patient journey,encompassing all events experienced during hospitalization, including multiplevisits, medications, tests, surgery/procedures, and admissions/discharge.Providing a summary of the patient\u0026rsquo;s progress is crucial, as it significantlyinfluences future care and planning. Consequently, clinicians face thelaborious and resource-intensive task of manually collecting, organizing, andcombining all the necessary data for a discharge summary. Therefore, we propose\u0026quot;NOTE\u0026quot;, which stands for \u0026ldquo;Notable generation Of patient Text summaries throughan Efficient approach based on direct preference optimization\u0026rdquo;. NOTE is basedon Medical Information Mart for Intensive Care- III dataset and summarizes asingle hospitalization of a patient. Patient events are sequentially combinedand used to generate a discharge summary for each hospitalization. In thepresent circumstances, large language models\u0026rsquo; application programminginterfaces (LLMs\u0026rsquo; APIs) are widely available, but importing and exportingmedical data presents significant challenges due to privacy protection policiesin healthcare institutions. Moreover, to ensure optimal performance, it isessential to implement a lightweight model for internal server or programwithin the hospital. Therefore, we utilized DPO and parameter efficient finetuning (PEFT) techniques to apply a fine-tuning method that guarantees superiorperformance. To demonstrate the practical application of the developed NOTE, weprovide a webpage-based demonstration software. In the future, we will aim todeploy the software available for actual use by clinicians in hospital. NOTEcan be utilized to generate various summaries not only discharge summaries butalso throughout a patient\u0026rsquo;s journey, thereby alleviating the labor-intensiveworkload of clinicians and aiming for increased efficiency.\r2024-02-18\nFederated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources\nJiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li\nabstract\rabstract: Federated Learning (FL) has recently been applied to the parameter-efficientfine-tuning of Large Language Models (LLMs). While promising, it raisessignificant challenges due to the heterogeneous resources and datadistributions of clients.This study introduces FlexLoRA, a simple yet effectiveaggregation scheme for LLM fine-tuning, which mitigates the \u0026ldquo;buckets effect\u0026rdquo; intraditional FL that restricts the potential of clients with ample resources bytying them to the capabilities of the least-resourced participants. FlexLoRAallows for dynamic adjustment of local LoRA ranks, fostering the development ofa global model imbued with broader, less task-specific knowledge. Bysynthesizing a full-size LoRA weight from individual client contributions andemploying Singular Value Decomposition (SVD) for weight redistribution,FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600clients performing diverse NLP tasks, our experiments validate the efficacy ofFlexLoRA, with the federated global model achieving up to a 3.1% averageimprovement in downstream NLP task performance. FlexLoRA\u0026rsquo;s practicality isfurther underscored by its seamless integration with existing LoRA-based FLmethods and theoretical analysis, offering a path toward scalable,privacy-preserving federated tuning for LLMs.\r2024-02-17\nLLM-based Federated Recommendation\nJujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, Tat-Seng Chua\nabstract\rabstract: Large Language Models (LLMs), with their advanced contextual understandingabilities, have demonstrated considerable potential in enhancing recommendationsystems via fine-tuning methods. However, fine-tuning requires users\u0026rsquo; behaviordata, which poses considerable privacy risks due to the incorporation ofsensitive user information. The unintended disclosure of such data couldinfringe upon data protection laws and give rise to ethical issues. To mitigatethese privacy issues, Federated Learning for Recommendation (Fed4Rec) hasemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-basedrecommendation presents two main challenges: first, an increase in theimbalance of performance across clients, affecting the system\u0026rsquo;s efficiency overtime, and second, a high demand on clients\u0026rsquo; computational and storage resourcesfor local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-basedRecommendation (PPLR) framework. The PPLR framework employs two primarystrategies. First, it implements a dynamic balance strategy, which involves thedesign of dynamic parameter aggregation and adjustment of learning speed fordifferent clients during the training phase, to ensure relatively balancedperformance across all clients. Second, PPLR adopts a flexible storagestrategy, selectively retaining certain sensitive layers of the language modelon the client side while offloading non-sensitive layers to the server. Thisapproach aims to preserve user privacy while efficiently saving computationaland storage resources. Experimental results demonstrate that PPLR not onlyachieves a balanced performance among clients but also enhances overall systemperformance in a manner that is both computationally and storage-efficient,while effectively protecting user privacy.\rKnowledge Editing on Black-box Large Language Models\nXiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Yutao Mou, Jinxu Zhao, Weiran Xu\nabstract\rabstract: Knowledge editing (KE) aims to efficiently and precisely modify the behaviorof large language models (LLMs) to update specific knowledge without negativelyinfluencing other knowledge. Current research primarily focuses on white-boxLLMs editing, overlooking an important scenario: black-box LLMs editing, whereLLMs are accessed through interfaces and only textual output is available. Inthis paper, we first officially introduce KE on black-box LLMs and then proposea comprehensive evaluation framework to overcome the limitations of existingevaluations that are not applicable to black-box LLMs editing and lackcomprehensiveness. To tackle privacy leaks of editing data and styleover-editing in current methods, we introduce a novel postEdit framework,resolving privacy concerns through downstream post-processing and maintainingtextual style consistency via fine-grained editing to original responses.Experiments and analysis on two benchmarks demonstrate that postEditoutperforms all baselines and achieves strong generalization, especially withhuge improvements on style retention (average $+20.82%\\uparrow$).\rA Platform for the Biomedical Application of Large Language Models\nSebastian Lobentanzer, Shaohong Feng, The BioChatter Consortium, Andreas Maier, Cankun Wang, Jan Baumbach, Nils Krehl, Qin Ma, Julio Saez-Rodriguez\nabstract\rabstract: Current-generation Large Language Models (LLMs) have stirred enormousinterest in recent months, yielding great potential for accessibility andautomation, while simultaneously posing significant challenges and risk ofmisuse. To facilitate interfacing with LLMs in the biomedical space, while atthe same time safeguarding their functionalities through sensible constraints,we propose a dedicated, open-source framework: BioChatter. Based on open-sourcesoftware packages, we synergise the many functionalities that are currentlydeveloping around LLMs, such as knowledge integration / retrieval-augmentedgeneration, model chaining, and benchmarking, resulting in an easy-to-use andinclusive framework for application in many use cases of biomedicine. We focuson robust and user-friendly implementation, including ways to deployprivacy-preserving local open-source LLMs. We demonstrate use cases via twomulti-purpose web apps (https://chat.biocypher.org), and provide documentation,support, and an open community.\rUnderstanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention\nEunkyung Jo, Yuin Jeong, SoHyun Park, Daniel A. Epstein, Young-Ho Kim\nabstract\rabstract: Recent large language models (LLMs) offer the potential to support publichealth monitoring by facilitating health disclosure through open-endedconversations but rarely preserve the knowledge gained about individuals acrossrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents anopportunity to improve engagement and self-disclosure, but we lack anunderstanding of how LTM impacts people\u0026rsquo;s interaction with LLM-driven chatbotsin public health interventions. We examine the case of CareCall \u0026ndash; anLLM-driven voice chatbot with LTM \u0026ndash; through the analysis of 1,252 call logsand interviews with nine users. We found that LTM enhanced health disclosureand fostered positive perceptions of the chatbot by offering familiarity.However, we also observed challenges in promoting self-disclosure through LTM,particularly around addressing chronic health conditions and privacy concerns.We discuss considerations for LTM integration in LLM-driven chatbots for publichealth monitoring, including carefully deciding what topics need to beremembered in light of public health goals.\r2024-02-16\nWhen Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\nMinrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, Khaled B. Letaief\nabstract\rabstract: AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.\r2024-02-15\nExploring the Adversarial Capabilities of Large Language Models\nLukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting\nabstract\rabstract: The proliferation of large language models (LLMs) has sparked widespread andgeneral interest due to their strong language generation capabilities, offeringgreat potential for both industry and research. While previous research delvedinto the security and privacy issues of LLMs, the extent to which these modelscan exhibit adversarial behavior remains largely unexplored. Addressing thisgap, we investigate whether common publicly available LLMs have inherentcapabilities to perturb text samples to fool safety measures, so-calledadversarial examples resp.~attacks. More specifically, we investigate whetherLLMs are inherently able to craft adversarial examples out of benign samples tofool existing safe rails. Our experiments, which focus on hate speechdetection, reveal that LLMs succeed in finding adversarial perturbations,effectively undermining hate speech detection systems. Our findings carrysignificant implications for (semi-)autonomous systems relying on LLMs,highlighting potential challenges in their interaction with existing systemsand safety measures.\rUnmemorization in Large Language Models via Self-Distillation and Deliberate Imagination\nYijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić\nabstract\rabstract: While displaying impressive generation capabilities across many tasks, LargeLanguage Models (LLMs) still struggle with crucial issues of privacy violationand unwanted exposure of sensitive data. This raises an essential question: howshould we prevent such undesired behavior of LLMs while maintaining theirstrong generation and natural language understanding (NLU) capabilities? Inthis work, we introduce a novel approach termed deliberate imagination in thecontext of LLM unlearning. Instead of trying to forget memorized data, weemploy a self-distillation framework, guiding LLMs to deliberately imaginealternative scenarios. As demonstrated in a wide range of experiments, theproposed method not only effectively unlearns targeted text but also preservesthe LLMs\u0026rsquo; capabilities in open-ended generation tasks as well as in NLU tasks.Our results demonstrate the usefulness of this approach across different modelsand sizes, and also with parameter-efficient fine-tuning, offering a novelpathway to addressing the challenges with private and sensitive data in LLMapplications.\rRethinking Machine Unlearning for Large Language Models\nSijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\r2024-02-14\nOnline Advertisements with LLMs: Opportunities and Challenges\nSoheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin\nabstract\rabstract: This paper explores the potential for leveraging Large Language Models (LLM)in the realm of online advertising systems. We delve into essentialrequirements including privacy, latency, reliability as well as thesatisfaction of users and advertisers which such a system must fulfill. Wefurther introduce a general framework for LLM advertisement, consisting ofmodification, bidding, prediction, and auction modules. Different designconsiderations for each module is presented, with an in-depth examination oftheir practicality and the technical challenges inherent to theirimplementation. Finally, we explore the prospect of LLM-based dynamic creativeoptimization as a means to significantly enhance the appeal of advertisementsto users and discuss its additional challenges.\rTowards Privacy-Aware Sign Language Translation at Scale\nPhillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, Jean Maillard\nabstract\rabstract: A major impediment to the advancement of sign language translation (SLT) isdata scarcity. Much of the sign language data currently available on the webcannot be used for training supervised models due to the lack of alignedcaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bearsprivacy risks due to the presence of biometric information, which theresponsible development of SLT technologies should account for. In this work,we propose a two-stage framework for privacy-aware SLT at scale that addressesboth of these issues. We introduce SSVP-SLT, which leverages self-supervisedvideo pretraining on anonymized and unannotated videos, followed by supervisedSLT finetuning on a curated parallel dataset. SSVP-SLT achievesstate-of-the-art finetuned and zero-shot gloss-free SLT performance on theHow2Sign dataset, outperforming the strongest respective baselines by over 3BLEU-4. Based on controlled experiments, we further discuss the advantages andlimitations of self-supervised pretraining and anonymization via facialobfuscation for SLT.\rDPZero: Private Fine-Tuning of Language Models without Backpropagation\nLiang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He\nabstract\rabstract: The widespread practice of fine-tuning large language models (LLMs) ondomain-specific data faces two major challenges in memory and privacy. First,as the size of LLMs continues to grow, the memory demands of gradient-basedtraining methods via backpropagation become prohibitively high. Second, giventhe tendency of LLMs to memorize training data, it is important to protectpotentially sensitive information in the fine-tuning data from beingregurgitated. Zeroth-order methods, which rely solely on forward passes,substantially reduce memory consumption during training. However, directlycombining them with standard differentially private gradient descent suffersfrom growing model size. To bridge this gap, we introduce DPZero, a novelprivate zeroth-order algorithm with nearly dimension-independent rates. Thememory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa onsix downstream tasks.\r2024-02-13\nJAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models\nJillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang, Zaid Harchaoui, Yejin Choi\nabstract\rabstract: The permanence of online content combined with the enhanced authorshipidentification techniques calls for stronger computational methods to protectthe identity and privacy of online authorship when needed, e.g., blind reviewsfor scientific papers, anonymous online reviews, or anonymous interactions inthe mental health forums. In this paper, we propose an unsupervisedinference-time approach to authorship obfuscation to address the uniquechallenges of authorship obfuscation: lack of supervision data for diverseauthorship and domains, and the need for a sufficient level of revision beyondsimple paraphrasing to obfuscate the authorship, all the while preserving theoriginal content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm forauthorship obfuscation that can be in principle applied to any text andauthorship. Our approach builds on small language models such as GPT2-XL inorder to help avoid disclosing the original content to proprietary LLM\u0026rsquo;s APIs,while also reducing the performance gap between small and large language modelsvia algorithmic enhancement. The key idea behind our approach is to boost thecreative power of smaller language models through constrained decoding, whilealso allowing for user-specified controls and flexibility. Experimental resultsdemonstrate that our approach based on GPT2-XL outperforms previousstate-of-the-art methods based on comparably small models, while performingcompetitively against GPT3.5 175B, a propriety model that is two orders ofmagnitudes larger.\rMapping the Ethics of Generative AI: A Comprehensive Scoping Review\nThilo Hagendorff\nabstract\rabstract: The advent of generative artificial intelligence and the widespread adoptionof it in society engendered intensive debates about its ethical implicationsand risks. These risks often differ from those associated with traditionaldiscriminative machine learning. To synthesize the recent discourse and map itsnormative concepts, we conducted a scoping review on the ethics of generativeartificial intelligence, including especially large language models andtext-to-image models. Our analysis provides a taxonomy of 378 normative issuesin 19 topic areas and ranks them according to their prevalence in theliterature. The study offers a comprehensive overview for scholars,practitioners, or policymakers, condensing the ethical debates surroundingfairness, safety, harmful content, hallucinations, privacy, interaction risks,security, alignment, societal impacts, and others. We discuss the results,evaluate imbalances in the literature, and explore unsubstantiated riskscenarios.\rBBox-Adapter: Lightweight Adapting for Black-Box Large Language Models\nHaotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai\nabstract\rabstract: Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Geminifor specific tasks is challenging. Due to the opacity in their parameters,embeddings, and even output probabilities, existing fine-tuning adaptationmethods are inapplicable. Consequently, adapting these black-box LLMs is onlypossible through their API services, raising concerns about transparency,privacy, and cost. To address these challenges, we introduce BBox-Adapter, anovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes targetand source domain data by treating target data as positive and source data asnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss topromote the likelihood of target domain data while penalizing that of thesource domain. Furthermore, it features an online adaptation mechanism, whichincorporates real-time positive data sampling from ground-truth, human, or AIfeedback, coupled with negative data from previous adaptations. Extensiveexperiments demonstrate BBox-Adapter\u0026rsquo;s effectiveness and cost efficiency. Itimproves model performance by up to 6.77% across diverse tasks and domains,while reducing training and inference costs by 31.30x and 1.84x, respectively.\r2024-02-12\nPANORAMIA: Privacy Auditing of Machine Learning Models without Retraining\nMishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien Gambs, Mathias Lécuyer\nabstract\rabstract: We introduce a privacy auditing scheme for ML models that relies onmembership inference attacks using generated data as \u0026ldquo;non-members\u0026rdquo;. Thisscheme, which we call PANORAMIA, quantifies the privacy leakage for large-scaleML models without control of the training process or model re-training and onlyrequires access to a subset of the training data. To demonstrate itsapplicability, we evaluate our auditing scheme across multiple ML domains,ranging from image and tabular data classification to large-scale languagemodels.\rEmpowering Federated Learning for Massive Models with NVIDIA FLARE\nHolger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng\nabstract\rabstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.\rUtilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code\nLiming Jiang\nabstract\rabstract: Mini-applications, commonly referred to as mini-apps, are compact softwareprograms embedded within larger applications or platforms, offering targetedfunctionality without the need for separate installations. Typically web-basedor cloud-hosted, these mini-apps streamline user experiences by providingfocused services accessible through web browsers or mobile apps. Theirsimplicity, speed, and integration capabilities make them valuable additions tomessaging platforms, social media networks, e-commerce sites, and variousdigital environments. WeChat Mini Programs, a prominent feature of China\u0026rsquo;sleading messaging app, exemplify this trend, offering users a seamless array ofservices without additional downloads. Leveraging WeChat\u0026rsquo;s extensive user baseand payment infrastructure, Mini Programs facilitate efficient transactions andbridge online and offline experiences, shaping China\u0026rsquo;s digital landscapesignificantly. This paper investigates the potential of employing LargeLanguage Models (LLMs) to detect privacy breaches within WeChat Mini Programs.Given the widespread use of Mini Programs and growing concerns about dataprivacy, this research seeks to determine if LLMs can effectively identifyinstances of privacy leakage within this ecosystem. Through meticulous analysisand experimentation, we aim to highlight the efficacy of LLMs in safeguardinguser privacy and security within the WeChat Mini Program environment, therebycontributing to a more secure digital landscape.\rRetrieval-Augmented Thought Process as Sequential Decision Making\nThomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar\nabstract\rabstract: Large Language Models (LLMs) have demonstrated their strong ability to assistpeople and show \u0026ldquo;sparks of intelligence\u0026rdquo;. However, several open challengeshinder their wider application: such as concerns over privacy, tendencies toproduce hallucinations, and difficulties in handling long contexts. In thiswork, we address those challenges by introducing the Retrieval-AugmentedThought Process (RATP). Given access to external knowledge, RATP formulates thethought generation of LLMs as a multiple-step decision process. To optimizesuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns aQ-value estimator that permits cost-efficient inference. In addressing the taskof question-answering with private data, where ethical and security concernslimit LLM training methods, RATP achieves a 50% improvement over existingin-context retrieval-augmented language models.\rEmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models\nGuo Lin, Wenyue Hua, Yongfeng Zhang\nabstract\rabstract: Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanism EmojiCryptto protect user privacy. It uses Emoji to encrypt the user inputs beforesending them to LLM, effectively rendering them indecipherable to human orLLM\u0026rsquo;s examination while retaining the original intent of the prompt, thusensuring the model\u0026rsquo;s performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that EmojiCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/EmojiCrypt.\rFive ethical principles for generative AI in scientific research\nZhicheng Lin\nabstract\rabstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.\rSecret Collusion Among Generative AI Agents\nSumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt\nabstract\rabstract: Recent capability increases in large language models (LLMs) open upapplications in which teams of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both the AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.\r2024-02-11\nDifferentially Private Training of Mixture of Experts Models\nPierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim\nabstract\rabstract: This position paper investigates the integration of Differential Privacy (DP)in the training of Mixture of Experts (MoE) models within the field of naturallanguage processing. As Large Language Models (LLMs) scale to billions ofparameters, leveraging expansive datasets, they exhibit enhanced linguisticcapabilities and emergent abilities. However, this growth raises significantcomputational and privacy concerns. Our study addresses these issues byexploring the potential of MoE models, known for their computationalefficiency, and the application of DP, a standard for privacy preservation. Wepresent the first known attempt to train MoE models under the constraints ofDP, addressing the unique challenges posed by their architecture and thecomplexities of DP integration. Our initial experimental studies demonstratethat MoE models can be effectively trained with DP, achieving performance thatis competitive with their non-private counterparts. This initial study aims toprovide valuable insights and ignite further research in the domain ofprivacy-preserving MoE models, softly laying the groundwork for prospectivedevelopments in this evolving field.\r2024-02-10\nOpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning\nRui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen\nabstract\rabstract: Trained on massive publicly available data, large language models (LLMs) havedemonstrated tremendous success across various fields. While more datacontributes to better performance, a disconcerting reality is that high-qualitypublic data will be exhausted in a few years. In this paper, we offer apotential next step for contemporary LLMs: collaborative and privacy-preservingLLM training on the underutilized distributed private data via federatedlearning (FL), where multiple data owners collaboratively train a shared modelwithout transmitting raw data. To achieve this, we build a concise, integrated,and research-friendly framework/codebase, named OpenFedLLM. It covers federatedinstruction tuning for enhancing instruction-following capability, federatedvalue alignment for aligning with human values, and 7 representative FLalgorithms. Besides, OpenFedLLM supports training on diverse domains, where wecover 8 training datasets; and provides comprehensive evaluations, where wecover 30+ evaluation metrics. Through extensive experiments, we observe thatall FL algorithms outperform local training on training LLMs, demonstrating aclear performance improvement across a variety of settings. Notably, in afinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm canoutperform GPT-4 by a significant margin while the model obtained throughindividual training cannot, demonstrating strong motivation for clients toparticipate in FL. The code is available athttps://github.com/rui-ye/OpenFedLLM.\r2024-02-09\nTowards Principled Assessment of Tabular Data Synthesis Algorithms\nYuntao Du, Ninghui Li\nabstract\rabstract: Data synthesis has been advocated as an important approach for utilizing datawhile protecting data privacy. A large number of tabular data synthesisalgorithms (which we call synthesizers) have been proposed. Some synthesizerssatisfy Differential Privacy, while others aim to provide privacy in aheuristic fashion. A comprehensive understanding of the strengths andweaknesses of these synthesizers remains elusive due to lacking principledevaluation metrics and missing head-to-head comparisons of newly developedsynthesizers that take advantage of diffusion models and large language modelswith state-of-the-art marginal-based synthesizers. In this paper, we present a principled and systematic evaluation frameworkfor assessing tabular data synthesis algorithms. Specifically, we examine andcritique existing evaluation metrics, and introduce a set of new metrics interms of fidelity, privacy, and utility to address their limitations. Based onthe proposed metrics, we also devise a unified objective for tuning, which canconsistently improve the quality of synthetic data for all methods. Weconducted extensive evaluations of 8 different types of synthesizers on 12datasets and identified some interesting findings, which offer new directionsfor privacy-preserving data synthesis.\rExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs\nFernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira\nabstract\rabstract: ExaRanker recently introduced an approach to training information retrieval(IR) models, incorporating natural language explanations as additional labels.The method addresses the challenge of limited labeled examples, leading toimprovements in the effectiveness of IR models. However, the initial resultswere based on proprietary language models such as GPT-3.5, which posedconstraints on dataset size due to its cost and data privacy. In this paper, weintroduce ExaRanker-Open, where we adapt and explore the use of open-sourcelanguage models to generate explanations. The method has been tested usingdifferent LLMs and datasets sizes to better comprehend the effectivecontribution of data augmentation. Our findings reveal that incorporatingexplanations consistently enhances neural rankers, with benefits escalating asthe LLM size increases. Notably, the data augmentation method provesadvantageous even with large datasets, as evidenced by ExaRanker surpassing thetarget baseline by 0.6 nDCG@10 points in our study. To encourage furtheradvancements by the research community, we have open-sourced both the code anddatasets at https://github.com/unicamp-dl/ExaRanker.\r2024-02-08\nSocial Learning: Towards Collaborative Learning with Large Language Models\nAmirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka, Matt Sharifi, Blaise Aguera y Arcas\nabstract\rabstract: We introduce the framework of \u0026ldquo;social learning\u0026rdquo; in the context of largelanguage models (LLMs), whereby models share knowledge with each other in aprivacy-aware manner using natural language. We present and evaluate twoapproaches for knowledge transfer between LLMs. In the first scenario, we allowthe model to generate abstract prompts aiming to teach the task. In our secondapproach, models transfer knowledge by generating synthetic examples. Weevaluate these methods across diverse datasets and quantify memorization as aproxy for privacy loss. These techniques inspired by social learning yieldpromising results with low memorization of the original data. In particular, weshow that performance using these methods is comparable to results with the useof original labels and prompts. Our work demonstrates the viability of sociallearning for LLMs, establishes baseline approaches and highlights severalunexplored areas for future work.\rRevolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices\nMohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Merouane Debbah, Thierry Lestable, Narinderjit Singh Thandi\nabstract\rabstract: The field of Natural Language Processing (NLP) is currently undergoing arevolutionary transformation driven by the power of pre-trained Large LanguageModels (LLMs) based on groundbreaking Transformer architectures. As thefrequency and diversity of cybersecurity attacks continue to rise, theimportance of incident detection has significantly increased. IoT devices areexpanding rapidly, resulting in a growing need for efficient techniques toautonomously identify network-based attacks in IoT networks with both highprecision and minimal computational requirements. This paper presentsSecurityBERT, a novel architecture that leverages the Bidirectional EncoderRepresentations from Transformers (BERT) model for cyber threat detection inIoT networks. During the training of SecurityBERT, we incorporated a novelprivacy-preserving encoding technique called Privacy-Preserving Fixed-LengthEncoding (PPFLE). We effectively represented network traffic data in astructured format by combining PPFLE with the Byte-level Byte-Pair Encoder(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperformstraditional Machine Learning (ML) and Deep Learning (DL) methods, such asConvolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), incyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, ourexperimental analysis shows that SecurityBERT achieved an impressive 98.2%overall accuracy in identifying fourteen distinct attack types, surpassingprevious records set by hybrid solutions such as GAN-Transformer-basedarchitectures and CNN-LSTM models. With an inference time of less than 0.15seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERTis ideally suited for real-life traffic analysis and a suitable choice fordeployment on resource-constrained IoT devices.\rLLMs Among Us: Generative AI Participating in Digital Discourse\nKristina Radivojevic, Nicholas Clark, Paul Brenner\nabstract\rabstract: The emergence of Large Language Models (LLMs) has great potential to reshapethe landscape of many social media platforms. While this can bring promisingopportunities, it also raises many threats, such as biases and privacyconcerns, and may contribute to the spread of propaganda by malicious actors.We developed the \u0026ldquo;LLMs Among Us\u0026rdquo; experimental framework on top of the Mastodonsocial media platform for bot and human participants to communicate withoutknowing the ratio or nature of bot and human participants. We built 10 personaswith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted threerounds of the experiment and surveyed participants after each round to measurethe ability of LLMs to pose as human participants without human detection. Wefound that participants correctly identified the nature of other users in theexperiment only 42% of the time despite knowing the presence of both bots andhumans. We also found that the choice of persona had substantially more impacton human perception than the choice of mainstream LLMs.\r2024-02-07\nDe-amplifying Bias from Differential Privacy in Language Model Fine-tuning\nSanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell\nabstract\rabstract: Fairness and privacy are two important values machine learning (ML)practitioners often seek to operationalize in models. Fairness aims to reducemodel bias for social/demographic sub-groups. Privacy via differential privacy(DP) mechanisms, on the other hand, limits the impact of any individual\u0026rsquo;straining data on the resulting model. The trade-offs between privacy andfairness goals of trustworthy ML pose a challenge to those wishing to addressboth. We show that DP amplifies gender, racial, and religious bias whenfine-tuning large language models (LLMs), producing models more biased thanones fine-tuned without DP. We find the cause of the amplification to be adisparity in convergence of gradients across sub-groups. Through the case ofbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),a known method for addressing bias, also mitigates bias amplification by DP. Asa consequence, DP and CDA together can be used to fine-tune models whilemaintaining both fairness and privacy.\rDefending Our Privacy With Backdoors\nDominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting\nabstract\rabstract: The proliferation of large AI models trained on uncurated, often sensitiveweb-scraped data has raised significant privacy concerns. One of the concernsis that adversaries can extract information about the training data usingprivacy attacks. Unfortunately, the task of removing specific information fromthe models without sacrificing performance is not straightforward and hasproven to be challenging. We propose a rather easy yet effective defense basedon backdoor attacks to remove private information such as names and faces ofindividuals from vision-language models by fine-tuning them for only a fewminutes instead of re-training them from scratch. Specifically, throughstrategic insertion of backdoors into text encoders, we align the embeddings ofsensitive phrases with those of neutral terms-\u0026ldquo;a person\u0026rdquo; instead of theperson\u0026rsquo;s actual name. For image encoders, we map embeddings of individuals tobe removed from the model to a universal, anonymous embedding. Our empiricalresults demonstrate the effectiveness of our backdoor-based defense on CLIP byassessing its performance using a specialized privacy attack for zero-shotclassifiers. Our approach provides not only a new \u0026ldquo;dual-use\u0026rdquo; perspective onbackdoor attacks, but also presents a promising avenue to enhance the privacyof individuals within models trained on uncurated web-scraped data.\r2024-02-06\nEmbedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy\nEfe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci\nabstract\rabstract: Recent developments in computer graphics, hardware, artificial intelligence(AI), and human-computer interaction likely lead to extended reality (XR)devices and setups being more pervasive. While these devices and setups provideusers with interactive, engaging, and immersive experiences with differentsensing modalities, such as eye and hand trackers, many non-player charactersare utilized in a pre-scripted way or by conventional AI techniques. In thispaper, we argue for using large language models (LLMs) in XR by embedding themin virtual avatars or as narratives to facilitate more inclusive experiencesthrough prompt engineering according to user profiles and fine-tuning the LLMsfor particular purposes. We argue that such inclusion will facilitate diversityfor XR use. In addition, we believe that with the versatile conversationalcapabilities of LLMs, users will engage more with XR environments, which mighthelp XR be more used in everyday life. Lastly, we speculate that combining theinformation provided to LLM-powered environments by the users and the biometricdata obtained through the sensors might lead to novel privacy invasions. Whilestudying such possible privacy invasions, user privacy concerns and preferencesshould also be investigated. In summary, despite some challenges, embeddingLLMs into XR is a promising and novel research area with several opportunities.\rLLsM: Generative Linguistic Steganography with Large Language Model\nYihao Wang, Ruiqi Song, Ru Zhang, Jianyi Liu, Lingxiao Li\nabstract\rabstract: Linguistic Steganography (LS) tasks aim to generate steganographic text(stego) based on secret information. Only authorized recipients can perceivethe existence of secrets in the texts and extract them, thereby preservingprivacy. However, the controllability of the stego generated by existingschemes is poor, and the stego is difficult to contain specific discoursecharacteristics such as style. As a result, the stego is easily detectable,compromising covert communication. To address these problems, this paperproposes LLsM, the first LS with the Large Language Model (LLM). We fine-tunedthe LLaMA2 with a large-scale constructed dataset encompassing rich discoursecharacteristics, which enables the fine-tuned LLM to generate texts withspecific discourse in a controllable manner. Then the discourse is used asguiding information and inputted into the fine-tuned LLM in the form of thePrompt together with secret. On this basis, the constructed candidate pool willbe range encoded and use secret to determine the interval. The same prefix ofthis interval\u0026rsquo;s beginning and ending is the secret embedded at this moment.Experiments show that LLsM performs superior to prevalent LS-task andrelated-task baselines regarding text quality, statistical analysis, discoursematching, and anti-steganalysis. In particular, LLsM\u0026rsquo;s MAUVE matric surpassessome baselines by 70%-80%, and its anti-steganalysis performance is 30%-40%higher. Notably, we also present examples of longer stegos generated by LLsM,showing its potential superiority in long LS tasks.\rDemocratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\nZhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng Jiang\nabstract\rabstract: Personalization in large language models (LLMs) is increasingly important,aiming to align LLM\u0026rsquo;s interactions, content, and recommendations withindividual user preferences. Recent advances in LLM personalization havespotlighted effective prompt design, by enriching user queries withnon-parametric knowledge through behavior history retrieval and textualprofiles. However, these approaches were limited due to a lack of modelownership, resulting in constrained customization and privacy issues. Moreover,they often failed to accurately capture user behavior patterns, especially incases where user data were complex and dynamic. To address these shortcomings,we introduce One PEFT Per User (OPPU), which employs personalizedparameter-efficient fine-tuning (PEFT) modules, to store user-specific behaviorpatterns and preferences. By plugging in users\u0026rsquo; personal PEFT parameters, theycan own and use their LLMs personally. OPPU integrates parametric userknowledge in the personal PEFT parameters with the non-parametric knowledgeacquired through retrieval and profile. This integration adapts individual LLMsto user behavior shifts. Experimental results demonstrate that OPPUsignificantly outperforms existing prompt-based methods across seven diversetasks in the LaMP benchmark. Further in-depth studies reveal OPPU\u0026rsquo;s enhancedcapabilities in handling user behavior shifts, modeling users at differentactive levels, maintaining robustness across various user history formats, anddisplaying versatility with different PEFT methods.\r2024-02-05\nPsychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach\nSergi Blanco-Cuaresma\nabstract\rabstract: This study explores the use of Large Language Models (LLMs) to analyze textcomments from Reddit users, aiming to achieve two primary objectives: firstly,to pinpoint critical excerpts that support a predefined psychologicalassessment of suicidal risk; and secondly, to summarize the material tosubstantiate the preassigned suicidal risk level. The work is circumscribed tothe use of \u0026ldquo;open-source\u0026rdquo; LLMs that can be run locally, thereby enhancing dataprivacy. Furthermore, it prioritizes models with low computationalrequirements, making it accessible to both individuals and institutionsoperating on limited computing budgets. The implemented strategy only relies ona carefully crafted prompt and a grammar to guide the LLM\u0026rsquo;s text completion.Despite its simplicity, the evaluation metrics show outstanding results, makingit a valuable privacy-focused and cost-effective approach. This work is part ofthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 sharedtask.\rConversation Reconstruction Attack Against GPT Models\nJunjie Chu, Zeyang Sha, Michael Backes, Yang Zhang\nabstract\rabstract: In recent times, significant advancements have been made in the field oflarge language models (LLMs), represented by GPT series models. To optimizetask execution, users often engage in multi-round conversations with GPT modelshosted in cloud environments. These multi-round conversations, potentiallyreplete with private information, require transmission and storage within thecloud. However, this operational paradigm introduces additional attacksurfaces. In this paper, we first introduce a specific ConversationReconstruction Attack targeting GPT models. Our introduced ConversationReconstruction Attack is composed of two steps: hijacking a session andreconstructing the conversations. Subsequently, we offer an exhaustiveevaluation of the privacy risks inherent in conversations when GPT models aresubjected to the proposed attack. However, GPT-4 demonstrates certainrobustness to the proposed attacks. We then introduce two advanced attacksaimed at better reconstructing previous conversations, specifically the UNRattack and the PBU attack. Our experimental findings indicate that the PBUattack yields substantial performance across all models, achieving semanticsimilarity scores exceeding 0.60, while the UNR attack is effective solely onGPT-3.5. Our results reveal the concern about privacy risks associated withconversations involving GPT models and aim to draw the community\u0026rsquo;s attention toprevent the potential misuse of these models\u0026rsquo; remarkable capabilities. We willresponsibly disclose our findings to the suppliers of related large languagemodels.\rPutting Context in Context: the Impact of Discussion Structure on Text Classification\nNicolò Penzo, Antonio Longa, Bruno Lepri, Sara Tonelli, Marco Guerini\nabstract\rabstract: Current text classification approaches usually focus on the content to beclassified. Contextual aspects (both linguistic and extra-linguistic) areusually neglected, even in tasks based on online discussions. Still in manycases the multi-party and multi-turn nature of the context from which theseelements are selected can be fruitfully exploited. In this work, we propose aseries of experiments on a large dataset for stance detection in English, inwhich we evaluate the contribution of different types of contextualinformation, i.e. linguistic, structural and temporal, by feeding them asnatural language input into a transformer-based model. We also experiment withdifferent amounts of training data and analyse the topology of local discussionnetworks in a privacy-compliant way. Results show that structural informationcan be highly beneficial to text classification but only under certaincircumstances (e.g. depending on the amount of training data and on discussionchain complexity). Indeed, we show that contextual information on smallerdatasets from other classification tasks does not yield significantimprovements. Our framework, based on local discussion networks, allows theintegration of structural information, while minimising user profiling, thuspreserving their privacy.\r2024-02-04\nA Survey of Large Language Models in Finance (FinLLMs)\nJean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song\nabstract\rabstract: Large Language Models (LLMs) have shown remarkable capabilities across a widevariety of Natural Language Processing (NLP) tasks and have attracted attentionfrom multiple domains, including financial services. Despite the extensiveresearch into general-domain LLMs, and their immense potential in finance,Financial LLM (FinLLM) research remains limited. This survey provides acomprehensive overview of FinLLMs, including their history, techniques,performance, and opportunities and challenges. Firstly, we present achronological overview of general-domain Pre-trained Language Models (PLMs)through to current FinLLMs, including the GPT-series, selected open-sourceLLMs, and financial LMs. Secondly, we compare five techniques used acrossfinancial PLMs and FinLLMs, including training methods, training data, andfine-tuning methods. Thirdly, we summarize the performance evaluations of sixbenchmark tasks and datasets. In addition, we provide eight advanced financialNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, wediscuss the opportunities and the challenges facing FinLLMs, such ashallucination, privacy, and efficiency. To support AI research in finance, wecompile a collection of accessible datasets and evaluation benchmarks onGitHub.\r2024-02-03\nHuman-Centered Privacy Research in the Age of Large Language Models\nTianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, Zhiping Zhang\nabstract\rabstract: The emergence of large language models (LLMs), and their increased use inuser-facing systems, has led to substantial privacy concerns. To date, researchon these privacy concerns has been model-centered: exploring how LLMs lead toprivacy risks like memorization, or can be used to infer personalcharacteristics about people from their content. We argue that there is a needfor more research focusing on the human aspect of these privacy issues: e.g.,research on how design paradigms for LLMs affect users\u0026rsquo; disclosure behaviors,users\u0026rsquo; mental models and preferences for privacy controls, and the design oftools, systems, and artifacts that empower end-users to reclaim ownership overtheir personal data. To build usable, efficient, and privacy-friendly systemspowered by these models with imperfect privacy properties, our goal is toinitiate discussions to outline an agenda for conducting human-centeredresearch on privacy issues in LLM-powered systems. This Special Interest Group(SIG) aims to bring together researchers with backgrounds in usable securityand privacy, human-AI collaboration, NLP, or any other related domains to sharetheir perspectives and experiences on this problem, to help our communityestablish a collective understanding of the challenges, research opportunities,research methods, and strategies to collaborate with researchers outside ofHCI.\r2024-02-02\nDTS-SQL: Decomposed Text-to-SQL with Small Large Language Models\nMohammadreza Pourreza, Davood Rafiei\nabstract\rabstract: Leading models for the text-to-SQL task heavily rely on proprietary LargeLanguage Models (LLMs), posing concerns over data privacy. Closing theperformance gap between small open-source models and large proprietary modelsis crucial to mitigate this reliance. To this end, we introduce a noveltwo-stage fine-tuning approach that decomposes the task into two simpler tasks.Through comprehensive evaluation on two large cross-domain datasets and twosmall LLMs, we show that this approach improves execution accuracy by 3 to 7percent, effectively aligning the performance of open-source models with theirproprietary counterparts.\rDigits micro-model for accurate and secure transactions\nChirag Chhablani, Nikhita Sharma, Jordan Hosier, Vijay K. Gurbani\nabstract\rabstract: Automatic Speech Recognition (ASR) systems are used in the financial domainto enhance the caller experience by enabling natural language understanding andfacilitating efficient and intuitive interactions. Increasing use of ASRsystems requires that such systems exhibit very low error rates. Thepredominant ASR models to collect numeric data are large, general-purposecommercial models \u0026ndash; Google Speech-to-text (STT), or Amazon Transcribe \u0026ndash; oropen source (OpenAI\u0026rsquo;s Whisper). Such ASR models are trained on hundreds ofthousands of hours of audio data and require considerable resources to run.Despite recent progress large speech recognition models, we highlight thepotential of smaller, specialized \u0026ldquo;micro\u0026rdquo; models. Such light models can betrained perform well on number recognition specific tasks, competing withgeneral models like Whisper or Google STT while using less than 80 minutes oftraining time and occupying at least an order of less memory resources. Also,unlike larger speech recognition models, micro-models are trained on carefullyselected and curated datasets, which makes them highly accurate, agile, andeasy to retrain, while using low compute resources. We present our work oncreating micro models for multi-digit number recognition that handle diversespeaking styles reflecting real-world pronunciation patterns. Our workcontributes to domain-specific ASR models, improving digit recognitionaccuracy, and privacy of data. An added advantage, their low resourceconsumption allows them to be hosted on-premise, keeping private data localinstead uploading to an external cloud. Our results indicate that ourmicro-model makes less errors than the best-of-breed commercial or open-sourceASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for ourmodel versus 11 GB VRAM for Whisper).\r2024-02-01\nHR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent\nWeijie Xu, Zicheng Huang, Wenxiang Hu, Xi Fang, Rajesh Kumar Cherukuri, Naumaan Nayyar, Lorenzo Malandri, Srinivasan H. Sengamedu\nabstract\rabstract: Recent advancements in Large Language Models (LLMs) have been reshapingNatural Language Processing (NLP) task in several domains. Their use in thefield of Human Resources (HR) has still room for expansions and could bebeneficial for several time consuming tasks. Examples such as time-offsubmissions, medical claims filing, and access requests are noteworthy, butthey are by no means the sole instances. However, the aforementioneddevelopments must grapple with the pivotal challenge of constructing ahigh-quality training dataset. On one hand, most conversation datasets aresolving problems for customers not employees. On the other hand, gatheringconversations with HR could raise privacy concerns. To solve it, we introduceHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HRdomains to evaluate LLM Agent. Our work has the following contributions: (1) Itis the first labeled open-sourced conversation dataset in the HR domain for NLPresearch. (2) It provides a detailed recipe for the data generation procedurealong with data analysis and human evaluations. The data generation pipeline istransferable and can be easily adapted for labeled conversation data generationin other domains. (3) The proposed data-collection pipeline is mostly based onLLMs with minimal human involvement for annotation, which is time andcost-efficient.\r2024-01-31\nDe-identification is not always enough\nAtiquer Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, Xiaoqian Jiang\nabstract\rabstract: For sharing privacy-sensitive data, de-identification is commonly regarded asadequate for safeguarding privacy. Synthetic data is also being considered as aprivacy-preserving alternative. Recent successes with numerical and tabulardata generative models and the breakthroughs in large generative languagemodels raise the question of whether synthetically generated clinical notescould be a viable alternative to real notes for research purposes. In thiswork, we demonstrated that (i) de-identification of real clinical notes doesnot protect records against a membership inference attack, (ii) proposed anovel approach to generate synthetic clinical notes using the currentstate-of-the-art large language models, (iii) evaluated the performance of thesynthetically generated notes in a clinical domain task, and (iv) proposed away to mount a membership inference attack where the target model is trainedwith synthetic data. We observed that when synthetically generated notesclosely match the performance of real data, they also exhibit similar privacyconcerns to the real data. Whether other approaches to synthetically generatedclinical notes could offer better trade-offs and become a better alternative tosensitive real notes warrants further investigation.\rFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nZhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng\nabstract\rabstract: Pre-trained large language models (LLMs) need fine-tuning to improve theirresponsiveness to natural language instructions. Federated learning offers away to fine-tune LLMs using the abundant data on end devices withoutcompromising data privacy. Most existing federated fine-tuning methods for LLMsrely on parameter-efficient fine-tuning techniques, which may not reach theperformance height possible with full-parameter tuning. However, federatedfull-parameter tuning of LLMs is a non-trivial problem due to the immensecommunication cost. This work introduces FedKSeed that employs zeroth-orderoptimization with a finite set of random seeds. It significantly reducestransmission requirements between the server and clients to just a few randomseeds and scalar gradients, amounting to only a few thousand bytes, makingfederated full-parameter tuning of billion-sized LLMs possible on devices.Building on it, we develop a strategy enabling probability-differentiated seedsampling, prioritizing perturbations with greater impact on model accuracy.Experiments across six scenarios with various LLMs, datasets and datapartitions demonstrate that our approach outperforms existing federated LLMfine-tuning methods in both communication efficiency and new taskgeneralization.\r2024-01-30\nSecurity and Privacy Challenges of Large Language Models: A Survey\nBadhan Chandra Das, M. Hadi Amini, Yanzhao Wu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.\rAalap: AI Assistant for Legal \u0026amp; Paralegal Functions in India\nAman Tiwari, Prathamesh Kalamkar, Atreyo Banerjee, Saurabh Karn, Varun Hemachandran, Smita Gupta\nabstract\rabstract: Using proprietary Large Language Models on legal tasks poses challenges dueto data privacy issues, domain data heterogeneity, domain knowledgesophistication, and domain objectives uniqueness. We created Aalalp, afine-tuned Mistral 7B model on instructions data related to specific Indianlegal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31% ofour test data and obtains an equivalent score in 34% of the test data asevaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoningrather than legal recall. Aalap is definitely helpful for the day-to-dayactivities of lawyers, judges, or anyone working in legal systems.\r2024-01-29\nTowards Building the Federated GPT: Federated Instruction Tuning\nJianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Yufan Zhou, Guoyin Wang, Yiran Chen\nabstract\rabstract: While \u0026ldquo;instruction-tuned\u0026rdquo; generative large language models (LLMs) havedemonstrated an impressive ability to generalize to new tasks, the trainingphases heavily rely on large amounts of diverse and high-quality instructiondata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,especially when it comes to human-written data, can pose significant challengesboth in terms of cost and accessibility. Moreover, concerns related to privacycan further limit access to such data, making the process of obtaining it acomplex and nuanced undertaking. Consequently, this hinders the generality ofthe tuned models and may restrict their effectiveness in certain contexts. Totackle this issue, our study introduces a new approach called FederatedInstruction Tuning (FedIT), which leverages federated learning (FL) as thelearning framework for the instruction tuning of LLMs. This marks the firstexploration of FL-based instruction tuning for LLMs. This is especiallyimportant since text data is predominantly generated by end users. Therefore,it is imperative to design and adapt FL approaches to effectively leveragethese users\u0026rsquo; diverse instructions stored on local devices, while preservingprivacy and ensuring data security. In the current paper, by conducting widelyused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneousand diverse sets of instructions on the client\u0026rsquo;s end with the proposedframework FedIT, we improved the performance of LLMs compared to centralizedtraining with only limited local instructions. Further, in this paper, wedeveloped a Github repository named Shepherd. This repository offers afoundational framework for exploring federated fine-tuning of LLMs usingheterogeneous instructions across diverse categories.\r2024-01-28\nPrivacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\nXinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim\nabstract\rabstract: We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.\rAI as a Medical Ally: Evaluating ChatGPT\u0026rsquo;s Usage and Impact in Indian Healthcare\nAryaman Raina, Prateek Mishra, Harshit goyal, Dhruv Kumar\nabstract\rabstract: This study investigates the integration and impact of Large Language Models(LLMs), like ChatGPT, in India\u0026rsquo;s healthcare sector. Our research employs a dualapproach, engaging both general users and medical professionals through surveysand interviews respectively. Our findings reveal that healthcare professionalsvalue ChatGPT in medical education and preliminary clinical settings, butexercise caution due to concerns about reliability, privacy, and the need forcross-verification with medical references. General users show a preference forAI interactions in healthcare, but concerns regarding accuracy and trustpersist. The study underscores the need for these technologies to complement,not replace, human medical expertise, highlighting the importance of developingLLMs in collaboration with healthcare providers. This paper enhances theunderstanding of LLMs in healthcare, detailing current usage, user trust, andimprovement areas. Our insights inform future research and development,underscoring the need for ethically compliant, user-focused LLM advancementsthat address healthcare-specific challenges.\rData-Free Generalized Zero-Shot Learning\nBowen Tang, Long Yan, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu\nabstract\rabstract: Deep learning models have the ability to extract rich knowledge fromlarge-scale datasets. However, the sharing of data has become increasinglychallenging due to concerns regarding data copyright and privacy. Consequently,this hampers the effective transfer of knowledge from existing data to noveldownstream tasks and concepts. Zero-shot learning (ZSL) approaches aim torecognize new classes by transferring semantic knowledge learned from baseclasses. However, traditional generative ZSL methods often require access toreal images from base classes and rely on manually annotated attributes, whichpresents challenges in terms of data restrictions and model scalability. Tothis end, this paper tackles a challenging and practical problem dubbed asdata-free zero-shot learning (DFZSL), where only the CLIP-based base classesdata pre-trained classifier is available for zero-shot classification.Specifically, we propose a generic framework for DFZSL, which consists of threemain components. Firstly, to recover the virtual features of the base data, wemodel the CLIP features of base class images as samples from a von Mises-Fisher(vMF) distribution based on the pre-trained classifier. Secondly, we leveragethe text features of CLIP as low-cost semantic information and propose afeature-language prompt tuning (FLPT) method to further align the virtual imagefeatures and textual features. Thirdly, we train a conditional generative modelusing the well-aligned virtual image features and corresponding semantic textfeatures, enabling the generation of new classes features and achieve betterzero-shot generalization. Our framework has been evaluated on five commonlyused benchmarks for generalized ZSL, as well as 11 benchmarks for thebase-to-new ZSL. The results demonstrate the superiority and effectiveness ofour approach. Our code is available in https://github.com/ylong4/DFZSL\r2024-01-27\nFortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models\nYunhong He, Jianling Qiu, Wei Zhang, Zhengqing Yuan\nabstract\rabstract: Recent advancements in large language models (LLMs) have significantlyenhanced capabilities in natural language processing and artificialintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionizedtext generation, translation, and question-answering tasks due to thetransformative Transformer model. Despite their widespread use, LLMs presentchallenges such as ethical dilemmas when models are compelled to respondinappropriately, susceptibility to phishing attacks, and privacy violations.This paper addresses these challenges by introducing a multi-pronged approachthat includes: 1) filtering sensitive vocabulary from user input to preventunethical responses; 2) detecting role-playing to halt interactions that couldlead to \u0026lsquo;prison break\u0026rsquo; scenarios; 3) implementing custom rule engines torestrict the generation of prohibited content; and 4) extending thesemethodologies to various LLM derivatives like Multi-Model Large Language Models(MLLMs). Our approach not only fortifies models against unethical manipulationsand privacy breaches but also maintains their high performance across tasks. Wedemonstrate state-of-the-art performance under various attack prompts, withoutcompromising the model\u0026rsquo;s core functionalities. Furthermore, the introduction ofdifferentiated security levels empowers users to control their personal datadisclosure. Our methods contribute to reducing social risks and conflictsarising from technological abuse, enhance data protection, and promote socialequity. Collectively, this research provides a framework for balancing theefficiency of question-answering systems with user privacy and ethicalstandards, ensuring a safer user experience and fostering trust in AItechnology.\rDataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure\nJunyi Ye, Mengnan Du, Guiling Wang\nabstract\rabstract: This paper introduces DataFrame question answering (QA), a novel task thatutilizes large language models (LLMs) to generate Pandas queries forinformation retrieval and data analysis on dataframes, emphasizing safe andnon-revealing data handling. Our method, which solely relies on dataframecolumn names, not only ensures data privacy but also significantly reduces thecontext window in the prompt, streamlining information processing andaddressing major challenges in LLM-based data analysis. We propose DataFrame QAas a comprehensive framework that includes safe Pandas query generation andcode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1metric on the renowned WikiSQL and our newly developed \u0026lsquo;UCI-DataFrameQA\u0026rsquo;,tailored for complex data analysis queries. Our findings indicate that GPT-4achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,underscoring its capability in securely retrieving and aggregating dataframevalues and conducting sophisticated data analyses. This approach, deployable ina zero-shot manner without prior training or adjustments, proves to be highlyadaptable and secure for diverse applications.\r2024-01-26\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into \u0026ldquo;The Good\u0026rdquo; (beneficial LLM applications),\u0026ldquo;The Bad\u0026rdquo; (offensive applications), and \u0026ldquo;The Ugly\u0026rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs\u0026rsquo; potential to both bolster and jeopardize cybersecurity.\r2024-01-25\nLLM on FHIR \u0026ndash; Demystifying Health Records\nPaul Schmiedmayer, Adrit Rao, Philipp Zagar, Vishnu Ravi, Aydin Zahedivash, Arash Fereydooni, Oliver Aalami\nabstract\rabstract: Objective: To enhance health literacy and accessibility of health informationfor a diverse patient population by developing a patient-centered artificialintelligence (AI) solution using large language models (LLMs) and FastHealthcare Interoperability Resources (FHIR) application programming interfaces(APIs). Materials and Methods: The research involved developing LLM on FHIR, anopen-source mobile application allowing users to interact with their healthrecords using LLMs. The app is built on Stanford\u0026rsquo;s Spezi ecosystem and usesOpenAI\u0026rsquo;s GPT-4. A pilot study was conducted with the SyntheticMass patientdataset and evaluated by medical experts to assess the app\u0026rsquo;s effectiveness inincreasing health literacy. The evaluation focused on the accuracy, relevance,and understandability of the LLM\u0026rsquo;s responses to common patient questions.Results: LLM on FHIR demonstrated varying but generally high degrees ofaccuracy and relevance in providing understandable health information topatients. The app effectively translated medical data into patient-friendlylanguage and was able to adapt its responses to different patient profiles.However, challenges included variability in LLM responses and the need forprecise filtering of health data. Discussion and Conclusion: LLMs offersignificant potential in improving health literacy and making health recordsmore accessible. LLM on FHIR, as a pioneering application in this field,demonstrates the feasibility and challenges of integrating LLMs into patientcare. While promising, the implementation and pilot also highlight risks suchas inconsistent responses and the importance of replicable output. Futuredirections include better resource identification mechanisms and executing LLMson-device to enhance privacy and reduce costs.\r2024-01-23\nMitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement\nChenghao Li, Dake Chen, Yuke Zhang, Peter A. Beerel\nabstract\rabstract: While diffusion models demonstrate a remarkable capability for generatinghigh-quality images, their tendency to `replicate\u0026rsquo; training data raises privacyconcerns. Although recent research suggests that this replication may stem fromthe insufficient generalization of training data captions and duplication oftraining images, effective mitigation strategies remain elusive. To addressthis gap, our paper first introduces a generality score that measures thecaption generality and employ large language model (LLM) to generalize trainingcaptions. Subsequently, we leverage generalized captions and propose a noveldual fusion enhancement approach to mitigate the replication of diffusionmodels. Our empirical results demonstrate that our proposed methods cansignificantly reduce replication by 43.5% compared to the original diffusionmodel while maintaining the diversity and quality of generations. Code isavailable at https://github.com/HowardLi0816/dual-fusion-diffusion.\r\u0026ldquo;The teachers are confused as well\u0026rdquo;: A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education\nKyrie Zhixuan Zhou, Zachary Kilhoffer, Madelyn Rose Sanfilippo, Ted Underwood, Ece Gumusel, Mengyi Wei, Abhinav Choudhry, Jinjun Xiong\nabstract\rabstract: Large Language Models (LLMs) are advancing quickly and impacting people\u0026rsquo;slives for better or worse. In higher education, concerns have emerged such asstudents\u0026rsquo; misuse of LLMs and degraded education outcomes. To unpack the ethicalconcerns of LLMs for higher education, we conducted a case study consisting ofstakeholder interviews (n=20) in higher education computer science. We foundthat students use several distinct mental models to interact with LLMs - LLMsserve as a tool for (a) writing, (b) coding, and (c) information retrieval,which differ somewhat in ethical considerations. Students and teachers broughtup ethical issues that directly impact them, such as inaccurate LLM responses,hallucinations, biases, privacy leakage, and academic integrity issues.Participants emphasized the necessity of guidance and rules for the use of LLMsin higher education, including teaching digital literacy, rethinking education,and having cautious and contextual policies. We reflect on the ethicalchallenges and propose solutions.\rRed Teaming Visual Language Models\nMukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu\nabstract\rabstract: VLMs (Vision-Language Models) extend the capabilities of LLMs (Large LanguageModels) to accept multimodal inputs. Since it has been verified that LLMs canbe induced to generate harmful or inaccurate content through specific testcases (termed as Red Teaming), how VLMs perform in similar scenarios,especially with their combination of textual and visual inputs, remains aquestion. To explore this problem, we present a novel red teaming datasetRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modaljail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,privacy, safety, fairness). Our RTVLM is the first red-teaming dataset tobenchmark current VLMs in terms of these 4 different aspects. Detailed analysisshows that 10 prominent open-sourced VLMs struggle with the red teaming indifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning(SFT) using RTVLM, and this bolsters the models\u0026rsquo; performance with 10% in RTVLMtest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,overpassing other LLaVA-based models with regular alignment data. This revealsthat current open-sourced VLMs still lack red teaming alignment. Our code anddatasets will be open-source.\r2024-01-20\nFwdLLM: Efficient FedLLM using Forward Gradient\nMengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, Shangguang Wang\nabstract\rabstract: Large Language Models (LLMs) are transforming the landscape of mobileintelligence. Federated Learning (FL), a method to preserve user data privacy,is often employed in fine-tuning LLMs to downstream mobile tasks, an approachknown as FedLLM. Though recent efforts have addressed the network issue inducedby the vast model size, they have not practically mitigated vital challengesconcerning integration with mobile devices, such as significant memoryconsumption and sluggish model convergence. In response to these challenges, this work introduces FwdLLM, an innovativeFL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLMto employ backpropagation (BP)-free training methods, requiring devices only toexecute ``perturbed inferences\u0026rsquo;\u0026rsquo;. Consequently, FwdLLM delivers way bettermemory efficiency and time efficiency (expedited by mobile NPUs and an expandedarray of participant devices). FwdLLM centers around three key designs: (1) itcombines BP-free training with parameter-efficient training methods, anessential way to scale the approach to the LLM era; (2) it systematically andadaptively allocates computational loads across devices, striking a carefulbalance between convergence speed and accuracy; (3) it discriminatively samplesperturbed predictions that are more valuable to model convergence.Comprehensive experiments with five LLMs and three NLP tasks illustrateFwdLLM\u0026rsquo;s significant advantages over conventional methods, including up tothree orders of magnitude faster convergence and a 14.6x reduction in memoryfootprint. Uniquely, FwdLLM paves the way for federated learning ofbillion-parameter LLMs such as LLaMA on COTS mobile devices \u0026ndash; a featpreviously unattained.\r2024-01-18\nMISS: A Generative Pretraining and Finetuning Approach for Med-VQA\nJiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, Lihua Zhang\nabstract\rabstract: Medical visual question answering (VQA) is a challenging multimodal task,where Vision-Language Pre-training (VLP) models can effectively improve thegeneralization performance. However, most methods in the medical field treatVQA as an answer classification task which is difficult to transfer topractical application scenarios. Additionally, due to the privacy of medicalimages and the expensive annotation process, large-scale medical image-textpairs datasets for pretraining are severely lacking. In this paper, we proposea large-scale MultI-task Self-Supervised learning based framework (MISS) formedical VQA tasks. Unlike existing methods, we treat medical VQA as agenerative task. We unify the text encoder and multimodal encoder and alignimage-text features through multi-task learning. Furthermore, we propose aTransfer-and-Caption method that extends the feature space of single-modalimage datasets using large language models (LLMs), enabling those traditionalmedical vision field task data to be applied to VLP. Experiments show that ourmethod achieves excellent results with fewer multimodal datasets anddemonstrates the advantages of generative VQA models. The code and modelweights will be released upon the paper\u0026rsquo;s acceptance.\rTowards a Responsible AI Metrics Catalogue: A Collection of Metrics for AI Accountability\nBoming Xia, Qinghua Lu, Liming Zhu, Sung Une Lee, Yue Liu, Zhenchang Xing\nabstract\rabstract: Artificial Intelligence (AI), particularly through the advent of large-scalegenerative AI (GenAI) models such as Large Language Models (LLMs), has become atransformative element in contemporary technology. While these models haveunlocked new possibilities, they simultaneously present significant challenges,such as concerns over data privacy and the propensity to generate misleading orfabricated content. Current frameworks for Responsible AI (RAI) often fallshort in providing the granular guidance necessary for tangible application,especially for Accountability-a principle that is pivotal for ensuringtransparent and auditable decision-making, bolstering public trust, and meetingincreasing regulatory expectations. This study bridges the accountability gapby introducing our effort towards a comprehensive metrics catalogue, formulatedthrough a systematic multivocal literature review (MLR) that integratesfindings from both academic and grey literature. Our catalogue delineatesprocess metrics that underpin procedural integrity, resource metrics thatprovide necessary tools and frameworks, and product metrics that reflect theoutputs of AI systems. This tripartite framework is designed to operationalizeAccountability in AI, with a special emphasis on addressing the intricacies ofGenAI.\rSilent Guardian: Protecting Text from Malicious Exploitation by Large Language Models\nJiawei Zhao, Kejiang Chen, Xiaojian Yuan, Yuang Qi, Weiming Zhang, Nenghai Yu\nabstract\rabstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.\r2024-01-17\nMobileAgent: enhancing mobile control via human-machine interaction and SOP integration\nTinghe Ding\nabstract\rabstract: Agents centered around Large Language Models (LLMs) are now capable ofautomating mobile device operations for users. After fine-tuning to learn auser\u0026rsquo;s mobile operations, these agents can adhere to high-level userinstructions online. They execute tasks such as goal decomposition, sequencingof sub-goals, and interactive environmental exploration, until the finalobjective is achieved. However, privacy concerns related to personalized userdata arise during mobile operations, requiring user confirmation. Moreover,users\u0026rsquo; real-world operations are exploratory, with action data being complexand redundant, posing challenges for agent learning. To address these issues,in our practical application, we have designed interactive tasks between agentsand humans to identify sensitive information and align with personalized userneeds. Additionally, we integrated Standard Operating Procedure (SOP)information within the model\u0026rsquo;s in-context learning to enhance the agent\u0026rsquo;scomprehension of complex task execution. Our approach is evaluated on the newdevice control benchmark AitW, which encompasses 30K unique instructions acrossmulti-step tasks, including application operation, web searching, and webshopping. Experimental results show that the SOP-based agent achievesstate-of-the-art performance in LLMs without incurring additional inferencecosts, boasting an overall action success rate of 66.92%. The code and dataexamples are available at https://github.com/alipay/mobile-agent.\r2024-01-12\nGreening Large Language Models of Code\nJieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, David Lo\nabstract\rabstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers\u0026rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\\times$less), carbon footprint (up to 157$\\times$ less), and inference latency (up to76$\\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).\r2024-01-09\nPrivate Fine-tuning of Large Language Models with Zeroth-order Optimization\nXinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal\nabstract\rabstract: Fine-tuning large pretrained models on private datasets may run the risk ofviolating privacy. Differential privacy is a framework for mitigating privacyrisks by enforcing algorithmic stability. DP-SGD enables training models withprivate data in a privacy-preserving manner, but raises new obstacles in theform of performance loss and significant engineering challenges. We introduceDP-ZO, a new method for fine-tuning large language models that preserves theprivacy of training data by privatizing zeroth-order optimization. A keyinsight into the design of our method is that the direction of the gradient inSPSA, the zeroth-order algorithm we use, is always random and the onlyinformation that depends on private data is the step size, i.e., a scalar.Therefore, we only need to privatize the scalar step size, which ismemory-efficient. DP-ZO, which can be instantiated with either Laplace orGaussian noise, provides a strong privacy-utility trade-off across differenttasks, and model sizes, under conservative privacy budgets. One noteworthyresult is that DP-ZO exhibits just $1.86%$ performance degradation due toprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samplesfrom SQuAD.\r2024-01-06\nSecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models\nJinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu\nabstract\rabstract: With the growing use of large language models hosted on cloud platforms tooffer inference services, privacy concerns are escalating, especiallyconcerning sensitive data like investment plans and bank account details.Secure Multi-Party Computing (SMPC) emerges as a promising solution to protectthe privacy of inference data and model parameters. However, the application ofSMPC in Privacy-Preserving Inference (PPI) for large language models,particularly those based on the Transformer architecture, often leads toconsiderable slowdowns or declines in performance. This is largely due to themultitude of nonlinear operations in the Transformer architecture, which arenot well-suited to SMPC and difficult to circumvent or optimize effectively. Toaddress this concern, we introduce an advanced optimization framework calledSecFormer, to achieve fast and accurate PPI for Transformer models. Byimplementing model design optimization, we successfully eliminate the high-costexponential and maximum operations in PPI without sacrificing modelperformance. Additionally, we have developed a suite of efficient SMPCprotocols that utilize segmented polynomials, Fourier series and Goldschmidt\u0026rsquo;smethod to handle other complex nonlinear functions within PPI, such as GeLU,LayerNorm, and Softmax. Our extensive experiments reveal that SecFormeroutperforms MPCFormer in performance, showing improvements of $5.6%$ and$24.2%$ for BERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, respectively. Interms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma forBERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, demonstrating its effectivenessand speed.\r2024-01-03\nPredicting challenge moments from students\u0026rsquo; discourse: A comparison of GPT-4 to two traditional natural language processing approaches\nWannapon Suraworachet, Jennifer Seon, Mutlu Cukurova\nabstract\rabstract: Effective collaboration requires groups to strategically regulate themselvesto overcome challenges. Research has shown that groups may fail to regulate dueto differences in members\u0026rsquo; perceptions of challenges which may benefit fromexternal support. In this study, we investigated the potential of leveragingthree distinct natural language processing models: an expert knowledgerule-based model, a supervised machine learning (ML) model and a Large Languagemodel (LLM), in challenge detection and challenge dimension identification(cognitive, metacognitive, emotional and technical/other challenges) fromstudent discourse, was investigated. The results show that the supervised MLand the LLM approaches performed considerably well in both tasks, in contrastto the rule-based approach, whose efficacy heavily relies on the engineeredfeatures by experts. The paper provides an extensive discussion of the threeapproaches\u0026rsquo; performance for automated detection and support of students\u0026rsquo;challenge moments in collaborative learning activities. It argues that,although LLMs provide many advantages, they are unlikely to be the panacea toissues of the detection and feedback provision of socially shared regulation oflearning due to their lack of reliability, as well as issues of validityevaluation, privacy and confabulation. We conclude the paper with a discussionon additional considerations, including model transparency to explore feasibleand meaningful analytical feedback for students and educators using LLMs.\rDB-GPT: Empowering Database Interactions with Private Large Language Models\nSiqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, Faqiang Chen\nabstract\rabstract: The recent breakthroughs in large language models (LLMs) are positioned totransition many areas of software. Database technologies particularly have animportant entanglement with LLMs as efficient and intuitive databaseinteractions are paramount. In this paper, we present DB-GPT, a revolutionaryand production-ready project that integrates LLMs with traditional databasesystems to enhance user experience and accessibility. DB-GPT is designed tounderstand natural language queries, provide context-aware responses, andgenerate complex SQL queries with high accuracy, making it an indispensabletool for users ranging from novice to expert. The core innovation in DB-GPTlies in its private LLM technology, which is fine-tuned on domain-specificcorpora to maintain user privacy and ensure data security while offering thebenefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, whichincludes a novel retrieval augmented generation (RAG) knowledge system, anadaptive learning mechanism to continuously improve performance based on userfeedback and a service-oriented multi-model framework (SMMF) with powerfuldata-driven agents. Our extensive experiments and user studies confirm thatDB-GPT represents a paradigm shift in database interactions, offering a morenatural, efficient, and secure way to engage with data repositories. The paperconcludes with a discussion of the implications of DB-GPT framework on thefuture of human-database interaction and outlines potential avenues for furtherenhancements and applications in the field. The project code is available athttps://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself byinstalling it with the instructionshttps://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minutevideo at https://www.youtube.com/watch?v=KYs4nTDzEhk.\r2024-01-02\nA Reliable Knowledge Processing Framework for Combustion Science using Foundation Models\nVansh Sharma, Venkat Raman\nabstract\rabstract: This research explores the integration of large language models (LLMs) intoscientific data assimilation, focusing on combustion science as a case study.Leveraging foundational models integrated with Retrieval-Augmented Generation(RAG) framework, the study introduces an approach to process diverse combustionresearch data, spanning experimental studies, simulations, and literature. Themultifaceted nature of combustion research emphasizes the critical role ofknowledge processing in navigating and extracting valuable information from avast and diverse pool of sources. The developed approach minimizescomputational and economic expenses while optimizing data privacy and accuracy.It incorporates prompt engineering and offline open-source LLMs, offering userautonomy in selecting base models. The study provides a thorough examination oftext segmentation strategies, conducts comparative studies between LLMs, andexplores various optimized prompts to demonstrate the effectiveness of theframework. By incorporating an external database, the framework outperforms aconventional LLM in generating accurate responses and constructing robustarguments. Additionally, the study delves into the investigation of optimizedprompt templates for the purpose of efficient extraction of scientificliterature. The research addresses concerns related to hallucinations and falseresearch articles by introducing a custom workflow developed with a detectionalgorithm to filter out inaccuracies. Despite identified areas for improvement,the framework consistently delivers accurate domain-specific responses withminimal human oversight. The prompt-agnostic approach introduced holds promisefor future deliberations. The study underscores the significance of integratingLLMs and knowledge processing techniques in scientific research, providing afoundation for advancements in data assimilation and utilization.\r2023-12-31\nOpening A Pandora\u0026rsquo;s Box: Things You Should Know in the Era of Custom GPTs\nGuanhong Tao, Siyuan Cheng, Zhuo Zhang, Junmin Zhu, Guangyu Shen, Xiangyu Zhang\nabstract\rabstract: The emergence of large language models (LLMs) has significantly acceleratedthe development of a wide range of applications across various fields. There isa growing trend in the construction of specialized platforms based on LLMs,such as the newly introduced custom GPTs by OpenAI. While custom GPTs providevarious functionalities like web browsing and code execution, they alsointroduce significant security threats. In this paper, we conduct acomprehensive analysis of the security and privacy issues arising from thecustom GPT platform. Our systematic examination categorizes potential attackscenarios into three threat models based on the role of the malicious actor,and identifies critical data exchange channels in custom GPTs. Utilizing theSTRIDE threat modeling framework, we identify 26 potential attack vectors, with19 being partially or fully validated in real-world settings. Our findingsemphasize the urgent need for robust security and privacy measures in thecustom GPT ecosystem, especially in light of the forthcoming launch of theofficial GPT store by OpenAI.\r2023-12-30\nSplit-and-Denoise: Protect large language model inference with local differential privacy\nPeihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang\nabstract\rabstract: Large Language Models (LLMs) shows powerful capability in natural languageunderstanding by capturing hidden semantics in vector space. This processenriches the value of the text embeddings for various downstream tasks, therebyfostering the Embedding-as-a-Service (EaaS) business model. However, the directtransmission of text to servers poses a largely unaddressed risk of privacyleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), aninnovative framework that split the model to execute the token embedding layeron the client side at minimal computational cost. This allows the client tointroduce noise prior to transmitting the embeddings to the server, andsubsequently receive and denoise the perturbed output embeddings for downstreamtasks. Our approach is designed for the inference stage of LLMs and requires nomodifications to the model parameters. Extensive experiments demonstrate SnD\u0026rsquo;seffectiveness in optimizing the privacy-utility tradeoff across various LLMarchitectures and diverse downstream tasks. The results reveal a significantperformance improvement under the same privacy budget compared to the baseline,offering clients a privacy-preserving solution for local privacy protection.\rTeach Large Language Models to Forget Privacy\nRan Yan, Yujun Li, Wenqian Li, Peihua Mai, Yan Pang, Yinchuan Li\nabstract\rabstract: Large Language Models (LLMs) have proven powerful, but the risk of privacyleakage remains a significant concern. Traditional privacy-preserving methods,such as Differential Privacy and Homomorphic Encryption, are inadequate forblack-box API-only settings, demanding either model transparency or heavycomputational resources. We propose Prompt2Forget (P2F), the first frameworkdesigned to tackle the LLM local privacy challenge by teaching LLM to forget.The method involves decomposing full questions into smaller segments,generating fabricated answers, and obfuscating the model\u0026rsquo;s memory of theoriginal input. A benchmark dataset was crafted with questions containingprivacy-sensitive information from diverse fields. P2F achieves zero-shotgeneralization, allowing adaptability across a wide range of use cases withoutmanual adjustments. Experimental results indicate P2F\u0026rsquo;s robust capability toobfuscate LLM\u0026rsquo;s memory, attaining a forgetfulness score of around 90% withoutany utility loss. This represents an enhancement of up to 63% when contrastedwith the naive direct instruction technique, highlighting P2F\u0026rsquo;s efficacy inmitigating memory retention of sensitive information within LLMs. Our findingsestablish the first benchmark in the novel field of the LLM forgetting task,representing a meaningful advancement in privacy preservation in the emergingLLM domain.\rEvaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models\nMaximilian Weber, Merle Reichardt\nabstract\rabstract: This paper explores the use of open generative Large Language Models (LLMs)for annotation tasks in the social sciences. The study highlights thechallenges associated with proprietary models, such as limited reproducibilityand privacy concerns, and advocates for the adoption of open (source) modelsthat can be operated on independent devices. Two examples of annotation tasks,sentiment analysis in tweets and identification of leisure activities inchildhood aspirational essays are provided. The study evaluates the performanceof different prompting strategies and models (neural-chat-7b-v3-2,Starling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). Theresults indicate the need for careful validation and tailored promptengineering. The study highlights the advantages of open models for dataprivacy and reproducibility.\r2023-12-29\nDifferentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning\nXiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, Meikang Qiu\nabstract\rabstract: The surge in interest and application of large language models (LLMs) hassparked a drive to fine-tune these models to suit specific applications, suchas finance and medical science. However, concerns regarding data privacy haveemerged, especially when multiple stakeholders aim to collaboratively enhanceLLMs using sensitive data. In this scenario, federated learning becomes anatural choice, allowing decentralized fine-tuning without exposing raw data tocentral servers. Motivated by this, we investigate how data privacy can beensured in LLM fine-tuning through practical federated learning approaches,enabling secure contributions from multiple parties to enhance LLMs. Yet,challenges arise: 1) despite avoiding raw data exposure, there is a risk ofinferring sensitive information from model outputs, and 2) federated learningfor LLMs incurs notable communication overhead. To address these challenges,this article introduces DP-LoRA, a novel federated learning algorithm tailoredfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism thatadds noise in weight updates, maintaining individual data privacy whilefacilitating collaborative model training. Moreover, DP-LoRA optimizescommunication efficiency via low-rank adaptation, minimizing the transmissionof updated weights during distributed training. The experimental results acrossmedical, financial, and general datasets using various LLMs demonstrate thatDP-LoRA effectively ensures strict privacy constraints while minimizingcommunication overhead.\r2023-12-27\nHow to Raise a Robot \u0026ndash; A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots\nNiklas Hemken, Florian Jacob, Fabian Peller-Konrad, Rainer Kartmann, Tamim Asfour, Hannes Hartenstein\nabstract\rabstract: Humanoid robots will be able to assist humans in their daily life, inparticular due to their versatile action capabilities. However, while theserobots need a certain degree of autonomy to learn and explore, they also shouldrespect various constraints, for access control and beyond. We explore thenovel field of incorporating privacy, security, and access control constraintswith robot task planning approaches. We report preliminary results on theclassical symbolic approach, deep-learned neural networks, and modern ideasusing large language models as knowledge base. From analyzing their trade-offs,we conclude that a hybrid approach is necessary, and thereby present a new usecase for the emerging field of neuro-symbolic artificial intelligence.\r2023-12-26\nFedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models\nPanlong Wu, Kangshuo Li, Ting Wang, Fangxin Wang\nabstract\rabstract: Foundation models have shown great success in natural language processing,computer vision, and multimodal tasks. FMs have a large number of modelparameters, thus requiring a substantial amount of data to help optimize themodel during the training. Federated learning has revolutionized machinelearning by enabling collaborative learning from decentralized data while stillpreserving the data privacy of clients. Despite the great benefits foundationmodels can have empowered by federated learning, they face severe computation,communication, and statistical challenges. In this paper, we propose a noveltwo-stage federated learning algorithm called FedMS. A global expert is trainedin the first stage and a local expert is trained in the second stage to providebetter personalization. We construct a Mixture of Foundation Models (MoFM) withthese two experts and design a gate neural network with an inserted gateadapter that joins the aggregation every communication round in the secondstage. To further adapt to edge computing scenarios with limited computationalresources, we design a novel Sparsely Activated LoRA (SAL) algorithm thatfreezes the pre-trained foundation model parameters inserts low-rank adaptationmatrices into transformer blocks and activates them progressively during thetraining. We employ extensive experiments to verify the effectiveness of FedMS,results show that FedMS outperforms other SOTA baselines by up to 55.25% indefault settings.\rCan ChatGPT Read Who You Are?\nErik Derner, Dalibor Kučera, Nuria Oliver, Jan Zahálka\nabstract\rabstract: The interplay between artificial intelligence (AI) and psychology,particularly in personality assessment, represents an important emerging areaof research. Accurate personality trait estimation is crucial not only forenhancing personalization in human-computer interaction but also for a widevariety of applications ranging from mental health to education. This paperanalyzes the capability of a generic chatbot, ChatGPT, to effectively inferpersonality traits from short texts. We report the results of a comprehensiveuser study featuring texts written in Czech by a representative populationsample of 155 participants. Their self-assessments based on the Big FiveInventory (BFI) questionnaire serve as the ground truth. We compare thepersonality trait estimations made by ChatGPT against those by human raters andreport ChatGPT\u0026rsquo;s competitive performance in inferring personality traits fromtext. We also uncover a \u0026lsquo;positivity bias\u0026rsquo; in ChatGPT\u0026rsquo;s assessments across allpersonality dimensions and explore the impact of prompt composition onaccuracy. This work contributes to the understanding of AI capabilities inpsychological assessment, highlighting both the potential and limitations ofusing large language models for personality inference. Our research underscoresthe importance of responsible AI development, considering ethical implicationssuch as privacy, consent, autonomy, and bias in AI applications.\r2023-12-25\nLarge Language Models Empowered Autonomous Edge AI for Connected Intelligence\nYifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang, Khaled B. Letaief\nabstract\rabstract: The evolution of wireless networks gravitates towards connected intelligence,a concept that envisions seamless interconnectivity among humans, objects, andintelligence in a hyper-connected cyber-physical world. Edge artificialintelligence (Edge AI) is a promising solution to achieve connectedintelligence by delivering high-quality, low-latency, and privacy-preserving AIservices at the network edge. This article presents a vision of autonomous edgeAI systems that automatically organize, adapt, and optimize themselves to meetusers\u0026rsquo; diverse requirements, leveraging the power of large language models(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting thepowerful abilities of GPT in language understanding, planning, and codegeneration, as well as incorporating classic wisdom such as task-orientedcommunication and edge federated learning, we present a versatile frameworkthat efficiently coordinates edge AI models to cater to users\u0026rsquo; personal demandswhile automatically generating code to train new models in a privacy-preservingmanner. Experimental results demonstrate the system\u0026rsquo;s remarkable ability toaccurately comprehend user demands, efficiently execute AI models with minimalcost, and effectively create high-performance AI models at edge servers.\r2023-12-21\nExperimenting with Large Language Models and vector embeddings in NASA SciX\nSergi Blanco-Cuaresma, Ioana Ciucă, Alberto Accomazzi, Michael J. Kurtz, Edwin A. Henneken, Kelly E. Lockhart, Felix Grezes, Thomas Allen, Golnaz Shapurian, Carolyn S. Grant, Donna M. Thompson, Timothy W. Hostetler, Matthew R. Templeton, Shinyi Chen, Jennifer Koch, Taylor Jacovich, Daniel Chivvis, Fernanda de Macedo Alves, Jean-Claude Paquin, Jennifer Bartlett, Mugdha Polimera, Stephanie Jarmak\nabstract\rabstract: Open-source Large Language Models enable projects such as NASA SciX (i.e.,NASA ADS) to think out of the box and try alternative approaches forinformation retrieval and data augmentation, while respecting data copyrightand users\u0026rsquo; privacy. However, when large language models are directly promptedwith questions without any context, they are prone to hallucination. At NASASciX we have developed an experiment where we created semantic vectors for ourlarge collection of abstracts and full-text content, and we designed a promptsystem to ask questions using contextual chunks from our system. Based on anon-systematic human evaluation, the experiment shows a lower degree ofhallucination and better responses when using Retrieval Augmented Generation.Further exploration is required to design new features and data augmentationprocesses at NASA SciX that leverages this technology while respecting the highlevel of trust and quality that the project holds.\rFedJudge: Federated Legal Large Language Model\nLinan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, Fangzhou Yao\nabstract\rabstract: Large Language Models (LLMs) have gained prominence in the field of LegalIntelligence, offering potential applications in assisting legal professionalsand laymen. However, the centralized training of these Legal LLMs raises dataprivacy concerns, as legal data is distributed among various institutionscontaining sensitive individual information. This paper addresses thischallenge by exploring the integration of Legal LLMs with Federated Learning(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally ondevices or clients, and their parameters are aggregated and distributed on acentral server, ensuring data privacy without directly sharing raw data.However, computation and communication overheads hinder the full fine-tuning ofLLMs under the FL setting. Moreover, the distribution shift of legal datareduces the effectiveness of FL methods. To this end, in this paper, we proposethe first Federated Legal Large Language Model (FedJudge) framework, whichfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudgeutilizes parameter-efficient fine-tuning methods to update only a fewadditional parameters during the FL training. Besides, we explore the continuallearning methods to preserve the global model\u0026rsquo;s important parameters whentraining local clients to mitigate the problem of data shifts. Extensiveexperimental results on three real-world datasets clearly validate theeffectiveness of FedJudge. Code is released athttps://github.com/yuelinan/FedJudge.\rDeID-GPT: Zero-shot Medical Text De-Identification by GPT-4\nZhengliang Liu, Yue Huang, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Yiwei Li, Peng Shu, Fang Zeng, Lichao Sun, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, Xiang Li\nabstract\rabstract: The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT\u0026quot;) to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.\r2023-12-19\nA Performance Evaluation of a Quantized Large Language Model on Various Smartphones\nTolga Çöplü, Marc Loedi, Arto Bendiken, Mykhailo Makohin, Joshua J. Bouw, Stephen Cobb\nabstract\rabstract: This paper explores the feasibility and performance of on-device largelanguage model (LLM) inference on various Apple iPhone models. Amidst the rapidevolution of generative AI, on-device LLMs offer solutions to privacy,security, and connectivity challenges inherent in cloud-based models.Leveraging existing literature on running multi-billion parameter LLMs onresource-limited devices, our study examines the thermal effects andinteraction speeds of a high-performing LLM across different smartphonegenerations. We present real-world performance results, providing insights intoon-device inference capabilities.\rImage Captioning with Multi-Context Synthetic Data\nFeipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun\nabstract\rabstract: Image captioning requires numerous annotated image-text pairs, resulting insubstantial annotation costs. Recently, large models (e.g. diffusion models andlarge language models) have excelled in producing high-quality images and text.This potential can be harnessed to create synthetic image-text pairs fortraining captioning models. Synthetic data can improve cost and time efficiencyin data collection, allow for customization to specific domains, bootstrapgeneralization capability for zero-shot performance, and circumvent privacyconcerns associated with real-world data. However, existing methods struggle toattain satisfactory performance solely through synthetic data. We identify theissue as generated images from simple descriptions mostly capture a solitaryperspective with limited context, failing to align with the intricate scenesprevalent in real-world imagery. To tackle this, we present an innovativepipeline that introduces multi-context data generation. Beginning with aninitial text corpus, our approach employs a large language model to extractmultiple sentences portraying the same scene from diverse viewpoints. Thesesentences are then condensed into a single sentence with multiple contexts.Subsequently, we generate intricate images using the condensed captions throughdiffusion models. Our model is exclusively trained on synthetic image-textpairs crafted through this process. The effectiveness of our pipeline isvalidated through experimental results in both the in-domain and cross-domainsettings, where it achieves state-of-the-art performance on well-known datasetssuch as MSCOCO, Flickr30k, and NoCaps.\r2023-12-18\nOpportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview\nLiang Zhang, Zhelun Chen\nabstract\rabstract: In recent years, the rapid advancement and impressive capabilities of LargeLanguage Models (LLMs) have been evident across various domains. This paperexplores the application, implications, and potential of LLMs in buildingenergy efficiency and decarbonization studies. The wide-ranging capabilities ofLLMs are examined in the context of the building energy field, includingintelligent control systems, code generation, data infrastructure, knowledgeextraction, and education. Despite the promising potential of LLMs, challengesincluding complex and expensive computation, data privacy, security andcopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.The paper concludes with a call for future research focused on the enhancementof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative researchbetween AI and energy experts.\rAI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs\nYann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny\nabstract\rabstract: Responding to the thousands of student questions on online QA platforms eachsemester has a considerable human cost, particularly in computing courses withrapidly growing enrollments. To address the challenges of scalable andintelligent question-answering (QA), we introduce an innovative solution thatleverages open-source Large Language Models (LLMs) from the LLaMA-2 family toensure data privacy. Our approach combines augmentation techniques such asretrieval augmented generation (RAG), supervised fine-tuning (SFT), andlearning from human preferences data using Direct Preference Optimization(DPO). Through extensive experimentation on a Piazza dataset from anintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs ofpreference data, we demonstrate a significant 30% improvement in the quality ofanswers, with RAG being a particularly impactful addition. Our contributionsinclude the development of a novel architecture for educational QA, extensiveevaluations of LLM performance utilizing both human assessments and LLM-basedmetrics, and insights into the challenges and future directions of educationaldata processing. This work paves the way for the development of AI-TA, anintelligent QA assistant customizable for courses with an online QA platform\r2023-12-17\nFedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion\nWei Tang, Zhiqian Wu, Yixin Cao, Yong Liao, Pengyuan Zhou\nabstract\rabstract: Knowledge graph completion (KGC) aims to predict missing facts in knowledgegraphs (KGs), which is crucial as modern KGs remain largely incomplete. Whiletraining KGC models on multiple aligned KGs can improve performance, previousmethods that rely on transferring raw data among KGs raise privacy concerns. Toaddress this challenge, we propose a new federated learning framework thatimplicitly aggregates knowledge from multiple KGs without demanding raw dataexchange and entity alignment. We treat each KG as a client that trains a locallanguage model through textbased knowledge representation learning. A centralserver then aggregates the model weights from clients. As natural languageprovides a universal representation, the same knowledge thus has similarsemantic representations across KGs. As such, the aggregated language model canleverage complementary knowledge from multilingual KGs without demanding rawuser data sharing. Extensive experiments on a benchmark dataset demonstratethat our method substantially improves KGC on multilingual KGs, achievingcomparable performance to state-of-the-art alignment-based models withoutrequiring any labeled alignments or raw user data sharing. Our codes will bepublicly available.\r2023-12-15\nLLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers\nXuanqi Liu, Zhuotao Liu\nabstract\rabstract: The community explored to build private inference frameworks fortransformer-based large language models (LLMs) in a server-client setting,where the server holds the model parameters and the client inputs its privatedata (or prompt) for inference. However, these frameworks impose significantoverhead when the private inputs are forward propagated through the originalLLMs. In this paper, we show that substituting the computation- andcommunication-heavy operators in the transformer architecture withprivacy-computing friendly approximations can greatly reduce the privateinference costs while incurring very minor impact on model performance.Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computingfriendly model inference pipeline achieves a $5\\times$ acceleration incomputation and an 80% reduction in communication overhead, while retainingnearly identical accuracy.\rPrivacy-Aware Document Visual Question Answering\nRubèn Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas\nabstract\rabstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.\rDistilling Large Language Models for Matching Patients to Clinical Trials\nMauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh\nabstract\rabstract: The recent success of large language models (LLMs) has paved the way fortheir adoption in the high-stakes domain of healthcare. Specifically, theapplication of LLMs in patient-trial matching, which involves assessing patienteligibility against clinical trial\u0026rsquo;s nuanced inclusion and exclusion criteria,has shown promise. Recent research has shown that GPT-3.5, a widely recognizedLLM developed by OpenAI, can outperform existing methods with minimal \u0026lsquo;variableengineering\u0026rsquo; by simply comparing clinical trial information against patientsummaries. However, there are significant challenges associated with usingclosed-source proprietary LLMs like GPT-3.5 in practical healthcareapplications, such as cost, privacy and reproducibility concerns. To addressthese issues, this study presents the first systematic examination of theefficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA7B,13B, and 70B) for the task of patient-trial matching. Employing amultifaceted evaluation framework, we conducted extensive automated andhuman-centric assessments coupled with a detailed error analysis for eachmodel. To enhance the adaptability of open-source LLMs, we have created aspecialized synthetic dataset utilizing GPT-4, enabling effective fine-tuningunder constrained data conditions. Our findings reveal that open-source LLMs,when fine-tuned on this limited and synthetic dataset, demonstrate performanceparity with their proprietary counterparts. This presents a massive opportunityfor their deployment in real-world healthcare applications. To foster furtherresearch and applications in this field, we release both the annotatedevaluation dataset along with the fine-tuned LLM \u0026ndash; Trial-LLAMA \u0026ndash; for publicuse.\rVoCopilot: Voice-Activated Tracking of Everyday Interactions\nSheen An Goh, Manoj Gulati, Ambuj Varshney\nabstract\rabstract: Voice plays an important role in our lives by facilitating communication,conveying emotions, and indicating health. Therefore, tracking vocalinteractions can provide valuable insight into many aspects of our lives. Thispaper presents our ongoing efforts to design a new vocal tracking system wecall VoCopilot. VoCopilot is an end-to-end system centered around anenergy-efficient acoustic hardware and firmware combined with advanced machinelearning models. As a result, VoCopilot is able to continuously trackconversations, record them, transcribe them, and then extract useful insightsfrom them. By utilizing large language models, VoCopilot ensures the user canextract useful insights from recorded interactions without having to learncomplex machine learning techniques. In order to protect the privacy of endusers, VoCopilot uses a novel wake-up mechanism that only records conversationsof end users. Additionally, all the rest of pipeline can be run on a commoditycomputer (Mac Mini M2). In this work, we show the effectiveness of VoCopilot inreal-world environment for two use cases.\r2023-12-14\nRecovering from Privacy-Preserving Masking with Large Language Models\nArpita Vats, Zhe Liu, Peng Su, Debjyoti Paul, Yingyi Ma, Yutong Pang, Zeeshan Ahmed, Ozlem Kalinli\nabstract\rabstract: Model adaptation is crucial to handle the discrepancy between proxy trainingdata and actual users data received. To effectively perform adaptation, textualdata of users is typically stored on servers or their local devices, wheredownstream natural language processing (NLP) models can be directly trainedusing such in-domain data. However, this might raise privacy and securityconcerns due to the extra risks of exposing user information to adversaries.Replacing identifying information in textual data with a generic marker hasbeen recently explored. In this work, we leverage large language models (LLMs)to suggest substitutes of masked tokens and have their effectiveness evaluatedon downstream language modeling tasks. Specifically, we propose multiplepre-trained and fine-tuned LLM-based approaches and perform empirical studieson various datasets for the comparison of these methods. Experimental resultsshow that models trained on the obfuscation corpora are able to achievecomparable performance with the ones trained on the original data withoutprivacy-preserving token masking.\rChatSOS: LLM-based knowledge Q\u0026amp;A system for safety engineering\nHaiyang Tang, Zhenyi Liu, Dongping Chen, Qingzhao Chu\nabstract\rabstract: Recent advancements in large language models (LLMs) have notably propellednatural language processing (NLP) capabilities, demonstrating significantpotential in safety engineering applications. Despite these advancements, LLMsface constraints in processing specialized tasks, attributed to factors such ascorpus size, input processing limitations, and privacy concerns. Obtaininguseful information from reliable sources in a limited time is crucial for LLM.Addressing this, our study introduces an LLM-based Q\u0026amp;A system for safetyengineering, enhancing the comprehension and response accuracy of the model. Weemployed prompt engineering to incorporate external knowledge databases, thusenriching the LLM with up-to-date and reliable information. The system analyzeshistorical incident reports through statistical methods, utilizes vectorembedding to construct a vector database, and offers an efficientsimilarity-based search functionality. Our findings indicate that theintegration of external knowledge significantly augments the capabilities ofLLM for in-depth problem analysis and autonomous task assignment. Iteffectively summarizes accident reports and provides pertinent recommendations.This integration approach not only expands LLM applications in safetyengineering but also sets a precedent for future developments towardsautomation and intelligent systems.\r2023-12-13\nEfficient Representation of the Activation Space in Deep Neural Networks\nTanya Akumu, Celia Cintas, Girmaw Abebe Tadesse, Adebayo Oshingbesan, Skyler Speakman, Edward McFowland III\nabstract\rabstract: The representations of the activation space of deep neural networks (DNNs)are widely utilized for tasks like natural language processing, anomalydetection and speech recognition. Due to the diverse nature of these tasks andthe large size of DNNs, an efficient and task-independent representation ofactivations becomes crucial. Empirical p-values have been used to quantify therelative strength of an observed node activation compared to activationscreated by already-known inputs. Nonetheless, keeping raw data for thesecalculations increases memory resource consumption and raises privacy concerns.To this end, we propose a model-agnostic framework for creating representationsof activations in DNNs using node-specific histograms to compute p-values ofobserved activations without retaining already-known inputs. Our proposedapproach demonstrates promising potential when validated with multiple networkarchitectures across various downstream tasks and compared with the kerneldensity estimates and brute-force empirical baselines. In addition, theframework reduces memory usage by 30% with up to 4 times faster p-valuecomputing time while maintaining state of-the-art detection power in downstreamtasks such as the detection of adversarial attacks and synthesized content.Moreover, as we do not persist raw data at inference time, we could potentiallyreduce susceptibility to attacks and privacy issues.\r2023-12-12\nPractical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration\nWenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang\nabstract\rabstract: Membership Inference Attacks (MIA) aim to infer whether a target data recordhas been utilized for model training or not. Prior attempts have quantified theprivacy risks of language models (LMs) via MIAs, but there is still noconsensus on whether existing MIA algorithms can cause remarkable privacyleakage on practical Large Language Models (LLMs). Existing MIAs designed forLMs can be classified into two categories: reference-free and reference-basedattacks. They are both based on the hypothesis that training recordsconsistently strike a higher probability of being sampled. Nevertheless, thishypothesis heavily relies on the overfitting of target models, which will bemitigated by multiple regularization methods and the generalization of LLMs.The reference-based attack seems to achieve promising effectiveness in LLMs,which measures a more reliable membership signal by comparing the probabilitydiscrepancy between the target model and the reference model. However, theperformance of reference-based attack is highly dependent on a referencedataset that closely resembles the training dataset, which is usuallyinaccessible in the practical scenario. Overall, existing MIAs are unable toeffectively unveil privacy leakage over practical fine-tuned LLMs that areoverfitting-free and private. We propose a Membership Inference Attack based onSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, sincememorization in LLMs is inevitable during the training process and occursbefore overfitting, we introduce a more reliable membership signal,probabilistic variation, which is based on memorization rather thanoverfitting. Furthermore, we introduce a self-prompt approach, which constructsthe dataset to fine-tune the reference model by prompting the target LLMitself. In this manner, the adversary can collect a dataset with a similardistribution from public APIs.\r2023-12-11\nInferDPT: Privacy-Preserving Inference for Black-box Large Language Model\nMeng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang, Nenghai Yu\nabstract\rabstract: Large language models (LLMs), like ChatGPT, have greatly simplified textgeneration tasks. However, they have also raised concerns about privacy riskssuch as data leakage and unauthorized data collection. Existing solutions forprivacy-preserving inference face practical challenges related to computationtime and communication costs. In this paper, we propose InferDPT, the firstpractical framework for the privacy-preserving Inference of black-box LLMs,implementing Differential Privacy in Text generation. InferDPT comprises twokey modules: the \u0026ldquo;perturbation module\u0026rdquo; utilizes the exponential mechanism togenerate a perturbed prompt, facilitating privacy-preserving inference withblack-box LLMs, and the \u0026ldquo;extraction module\u0026rdquo;, inspired by knowledge distillationand retrieval-augmented generation, extracts coherent and consistent text fromthe perturbed generation result, ensuring successful text generationcompletion. To address privacy concerns related to previous exponentialmechanisms\u0026rsquo; susceptibility to embedding revision attacks, we introduce RANTEXT,a novel differential privacy mechanism integrated into the perturbation moduleof InferDPT, which introduces the concept of \u0026ldquo;RANdom adjacency\u0026rdquo; for TEXTperturbation within the prompt. Experimental results across three datasetsdemonstrate that the text generation quality of InferDPT is comparable to thatof non-private GPT-4, and RANTEXT surpasses existing state-of-the-artmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy andutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achievesan average privacy protection rate exceeding 90% against embedding revisionattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higherthan that of CUSTEXT+.\r2023-12-10\nMutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer\nYongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, Yaoxue Zhang\nabstract\rabstract: While large language models (LLMs) are empowered with broad knowledge, theirtask-specific performance is often suboptimal. It necessitates fine-tuning LLMswith task-specific data, but such data may be inaccessible due to privacyconcerns. In this paper, we propose a novel approach to enhance LLMs withsmaller language models (SLMs) that are trained on clients using their privatetask-specific data. To enable mutual enhancement between LLMs and SLMs, wepropose CrossLM, where the SLMs promote the LLM to generate task-specifichigh-quality data, and both the LLM and SLMs are enhanced with the generateddata. We evaluate CrossLM using publicly accessible language models across arange of benchmark tasks. The results demonstrate that CrossLM significantlyenhances the task-specific performance of SLMs on clients and the LLM on thecloud server simultaneously while preserving the LLM\u0026rsquo;s generalizationcapability.\r2023-12-08\nOn the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook\nMingyuan Fan, Chengyu Wang, Cen Chen, Yang Liu, Jun Huang\nabstract\rabstract: Diffusion models and large language models have emerged as leading-edgegenerative models, revolutionizing various aspects of human life. However, thepractical implementations of these models have also exposed inherent risks,bringing to the forefront their evil sides and sparking concerns regardingtheir trustworthiness. Despite the wealth of literature on this subject, acomprehensive survey specifically delving into the intersection of large-scalegenerative models and their trustworthiness remains largely absent. To bridgethis gap, this paper investigates both the long-standing and emerging threatsassociated with these models across four fundamental dimensions: 1) privacy, 2)security, 3) fairness, and 4) responsibility. Based on the investigationresults, we develop an extensive map outlining the trustworthiness of largegenerative models. After that, we provide practical recommendations andpotential research directions for future secure applications equipped withlarge generative models, ultimately promoting the trustworthiness of the modelsand benefiting the society as a whole.\rTypeFly: Flying Drones with Large Language Model\nGuojun Chen, Xiaojing Yu, Lin Zhong\nabstract\rabstract: Commanding a drone with a natural language is not only user-friendly but alsoopens the door for emerging language agents to control the drone. Emerginglarge language models (LLMs) provide a previously impossible opportunity toautomatically translate a task description in a natural language to a programthat can be executed by the drone. However, powerful LLMs and their visioncounterparts are limited in three important ways. First, they are onlyavailable as cloud-based services. Sending images to the cloud raises privacyconcerns. Second, they are expensive, costing proportionally to the requestsize. Finally, without expensive fine-tuning, existing LLMs are quite limitedin their capability of writing a program for specialized systems like drones. In this paper, we present a system called TypeFly that tackles the abovethree problems using a combination of edge-based vision intelligence, novelprogramming language design, and prompt engineering. Instead of the familiarPython, TypeFly gets a cloud-based LLM service to write a program in a small,custom language called MiniSpec, based on task and scene descriptions inEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)but also able to consult the LLM during their execution using a special skillcalled query. Using a set of increasingly challenging drone tasks, we show thatdesign choices made by TypeFly can reduce both the cost of LLM service and thetask execution time by more than 2x. More importantly, query and promptengineering techniques contributed by TypeFly significantly improve the chanceof success of complex tasks.\r2023-12-07\nDomain Private Transformers for Multi-Domain Dialog Systems\nAnmol Kabra, Ethan R. Elenberg\nabstract\rabstract: Large, general purpose language models have demonstrated impressiveperformance across many different conversational domains. While multi-domainlanguage models achieve low overall perplexity, their outputs are notguaranteed to stay within the domain of a given input prompt. This paperproposes domain privacy as a novel way to quantify how likely a conditionallanguage model will leak across domains. We also develop policy functions basedon token-level domain classification, and propose an efficient fine-tuningmethod to improve the trained model\u0026rsquo;s domain privacy. Experiments on membershipinference attacks show that our proposed method has comparable resiliency tomethods adapted from recent literature on differentially private languagemodels.\r2023-12-06\nUnderstanding (Un)Intended Memorization in Text-to-Image Generative Models\nAli Naseh, Jaechul Roh, Amir Houmansadr\nabstract\rabstract: Multimodal machine learning, especially text-to-image models like StableDiffusion and DALL-E 3, has gained significance for transforming text intodetailed images. Despite their growing use and remarkable generative capabilities, there is apressing need for a detailed examination of these models\u0026rsquo; behavior,particularly with respect to memorization. Historically, memorization inmachine learning has been context-dependent, with diverse definitions emergingfrom classification tasks to complex models like Large Language Models (LLMs)and Diffusion models. Yet, a definitive concept of memorization that alignswith the intricacies of text-to-image synthesis remains elusive. Thisunderstanding is vital as memorization poses privacy risks yet is essential formeeting user expectations, especially when generating representations ofunderrepresented entities. In this paper, we introduce a specialized definitionof memorization tailored to text-to-image models, categorizing it into threedistinct types according to user expectations. We closely examine the subtledistinctions between intended and unintended memorization, emphasizing theimportance of balancing user privacy with the generative quality of the modeloutputs. Using the Stable Diffusion model, we offer examples to validate ourmemorization definitions and clarify their application.\r2023-12-05\nDEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models\nXinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong\nabstract\rabstract: Large language models pretrained on a huge amount of data capture richknowledge and information in the training data. The ability of datamemorization and regurgitation in pretrained language models, revealed inprevious studies, brings the risk of data leakage. In order to effectivelyreduce these risks, we propose a framework DEPN to Detect and Edit PrivacyNeurons in pretrained language models, partially inspired by knowledge neuronsand model editing. In DEPN, we introduce a novel method, termed as privacyneuron detector, to locate neurons associated with private information, andthen edit these detected privacy neurons by setting their activations to zero.Furthermore, we propose a privacy neuron aggregator dememorize privateinformation in a batch processing manner. Experimental results show that ourmethod can significantly and efficiently reduce the exposure of private dataleakage without deteriorating the performance of the model. Additionally, weempirically demonstrate the relationship between model memorization and privacyneurons, from multiple perspectives, including model size, training time,prompts, privacy neuron distribution, illustrating the robustness of ourapproach.\r2023-12-04\nResponsible Task Automation: Empowering Large Language Models as Responsible Task Automators\nZhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yan Lu\nabstract\rabstract: The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users\u0026rsquo; privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios.\r2023-12-01\nLinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices\nJunchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, Sangeetha Abdu Jyothi\nabstract\rabstract: Deploying Large Language Models (LLMs) locally on mobile devices presents asignificant challenge due to their extensive memory requirements. In thispaper, we introduce LinguaLinked, a system for decentralized, distributed LLMinference on mobile devices. LinguaLinked enables collaborative execution ofthe inference task across multiple trusted devices. LinguaLinked ensures dataprivacy by processing information locally. LinguaLinked uses three keystrategies. First, an optimized model assignment technique segments LLMs anduses linear optimization to align segments with each device\u0026rsquo;s capabilities.Second, an optimized data transmission mechanism ensures efficient andstructured data flow between model segments while also maintaining theintegrity of the original model structure. Finally, LinguaLinked incorporates aruntime load balancer that actively monitors and redistributes tasks amongmobile devices to prevent bottlenecks, enhancing the system\u0026rsquo;s overallefficiency and responsiveness. We demonstrate that LinguaLinked facilitatesefficient LLM inference while maintaining consistent throughput and minimallatency through extensive testing across various mobile devices, from high-endto low-end Android devices. In our evaluations, compared to the baseline,LinguaLinked achieves an inference performance acceleration of $1.11\\times$ to$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ withmulti-threading. Additionally, runtime load balancing yields an overallinference acceleration of $1.29\\times$ to $1.32\\times$.\r2023-11-30\nLocally Differentially Private Document Generation Using Zero Shot Prompting\nSaiteja Utpala, Sara Hooker, Pin Yu Chen\nabstract\rabstract: Numerous studies have highlighted the privacy risks associated withpretrained large language models. In contrast, our research offers a uniqueperspective by demonstrating that pretrained large language models caneffectively contribute to privacy preservation. We propose a locallydifferentially private mechanism called DP-Prompt, which leverages the power ofpretrained large language models and zero-shot prompting to counter authorde-anonymization attacks while minimizing the impact on downstream utility.When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),we observe a notable reduction in the success rate of de-anonymization attacks,showing that it surpasses existing approaches by a considerable margin despiteits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt(with ChatGPT) perfectly recovers the clean sentiment F1 score while achievinga 46% reduction in author identification F1 score against static attackers anda 26% reduction against adaptive attackers. We conduct extensive experimentsacross six open-source large language models, ranging up to 7 billionparameters, to analyze various effects of the privacy-utility tradeoff.\rSituating the social issues of image generation models in the model life cycle: a sociotechnical approach\nAmelia Katirai, Noa Garcia, Kazuki Ide, Yuta Nakashima, Atsuo Kishimoto\nabstract\rabstract: The race to develop image generation models is intensifying, with a rapidincrease in the number of text-to-image models available. This is coupled withgrowing public awareness of these technologies. Though other generative AImodels\u0026ndash;notably, large language models\u0026ndash;have received recent critical attentionfor the social and other non-technical issues they raise, there has beenrelatively little comparable examination of image generation models. This paperreports on a novel, comprehensive categorization of the social issuesassociated with image generation models. At the intersection of machinelearning and the social sciences, we report the results of a survey of theliterature, identifying seven issue clusters arising from image generationmodels: data issues, intellectual property, bias, privacy, and the impacts onthe informational, cultural, and natural environments. We situate these socialissues in the model life cycle, to aid in considering where potential issuesarise, and mitigation may be needed. We then compare these issue clusters withwhat has been reported for large language models. Ultimately, we argue that therisks posed by image generation models are comparable in severity to the risksposed by large language models, and that the social impact of image generationmodels must be urgently considered.\r2023-11-29\nIdentifying and Mitigating Vulnerabilities in LLM-Integrated Applications\nFengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, Radha Poovendran\nabstract\rabstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users\u0026rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user\u0026rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.\r2023-11-28\nThe Transformative Influence of Large Language Models on Software Development\nSajed Jalil\nabstract\rabstract: The increasing adoption and commercialization of generalized Large LanguageModels (LLMs) have profoundly impacted various aspects of our daily lives.Initially embraced by the computer science community, the versatility of LLMshas found its way into diverse domains. In particular, the software engineeringrealm has witnessed the most transformative changes. With LLMs increasinglyserving as AI Pair Programming Assistants spurred the development ofspecialized models aimed at aiding software engineers. Although this newparadigm offers numerous advantages, it also presents critical challenges andopen problems. To identify the potential and prevailing obstacles, wesystematically reviewed contemporary scholarly publications, emphasizing theperspectives of software developers and usability concerns. Preliminaryfindings underscore pressing concerns about data privacy, bias, andmisinformation. Additionally, we identified several usability challenges,including prompt engineering, increased cognitive demands, and mistrust.Finally, we introduce 12 open problems that we have identified through oursurvey, covering these various domains.\r2023-11-27\nDiffSLVA: Harnessing Diffusion Models for Sign Language Video Anonymization\nZhaoyang Xia, Carol Neidle, Dimitris N. Metaxas\nabstract\rabstract: Since American Sign Language (ASL) has no standard written form, Deaf signersfrequently share videos in order to communicate in their native language.However, since both hands and face convey critical linguistic information insigned languages, sign language videos cannot preserve signer privacy. Whilesigners have expressed interest, for a variety of applications, in signlanguage video anonymization that would effectively preserve linguisticcontent, attempts to develop such technology have had limited success, giventhe complexity of hand movements and facial expressions. Existing approachesrely predominantly on precise pose estimations of the signer in video footageand often require sign language video datasets for training. These requirementsprevent them from processing videos \u0026lsquo;in the wild,\u0026rsquo; in part because of thelimited diversity present in current sign language video datasets. To addressthese limitations, our research introduces DiffSLVA, a novel methodology thatutilizes pre-trained large-scale diffusion models for zero-shot text-guidedsign language video anonymization. We incorporate ControlNet, which leverageslow-level image features such as HED (Holistically-Nested Edge Detection)edges, to circumvent the need for pose estimation. Additionally, we develop aspecialized module dedicated to capturing facial expressions, which arecritical for conveying essential linguistic information in signed languages. Wethen combine the above methods to achieve anonymization that better preservesthe essential linguistic content of the original signer. This innovativemethodology makes possible, for the first time, sign language videoanonymization that could be used for real-world applications, which would offersignificant benefits to the Deaf and Hard-of-Hearing communities. Wedemonstrate the effectiveness of our approach with a series of signeranonymization experiments.\r2023-11-26\nAI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction\nJunsol Kim, Byungkyu Lee\nabstract\rabstract: Large language models (LLMs) that produce human-like responses have begun torevolutionize research practices in the social sciences. This paper shows howwe can integrate LLMs and social surveys to accurately predict individualresponses to survey questions that were not asked before. We develop a novelmethodological framework to personalize LLMs by considering the meaning ofsurvey questions derived from their text, the latent beliefs of individualsinferred from their response patterns, and the temporal contexts acrossdifferent survey periods through fine-tuning LLMs with survey data. Using theGeneral Social Survey from 1972 to 2021, we show that the fine-tuned modelbased on Alpaca-7b can predict individual responses to survey questions thatare partially missing as well as entirely missing. The remarkable predictioncapabilities allow us to fill in missing trends with high confidence andpinpoint when public attitudes changed, such as the rising support for same-sexmarriage. We discuss practical constraints, socio-demographic representation,and ethical concerns regarding individual autonomy and privacy when using LLMsfor opinion prediction. This study demonstrates that LLMs and surveys canmutually enhance each other\u0026rsquo;s capabilities: LLMs broaden survey potential,while surveys improve the alignment of LLMs.\r2023-11-24\nInput Reconstruction Attack against Vertical Federated Large Language Models\nFei Zheng\nabstract\rabstract: Recently, large language models (LLMs) have drawn extensive attention fromacademia and the public, due to the advent of the ChatGPT. While LLMs showtheir astonishing ability in text generation for various tasks, privacyconcerns limit their usage in real-life businesses. More specifically, eitherthe user\u0026rsquo;s inputs (the user sends the query to the model-hosting server) or themodel (the user downloads the complete model) itself will be revealed duringthe usage. Vertical federated learning (VFL) is a promising solution to thiskind of problem. It protects both the user\u0026rsquo;s input and the knowledge of themodel by splitting the model into a bottom part and a top part, which ismaintained by the user and the model provider, respectively. However, in thispaper, we demonstrate that in LLMs, VFL fails to protect the user input sinceit is simple and cheap to reconstruct the input from the intermediateembeddings. Experiments show that even with a commercial GPU, the inputsentence can be reconstructed in only one second. We also discuss severalpossible solutions to enhance the privacy of vertical federated LLMs.\r2023-11-23\nPrivateLoRA For Efficient Privacy Preserving LLM\nYiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang\nabstract\rabstract: End users face a choice between privacy and efficiency in current LargeLanguage Model (LLM) service paradigms. In cloud-based paradigms, users areforced to compromise data locality for generation quality and processing speed.Conversely, edge device paradigms maintain data locality but fail to deliversatisfactory performance. In this work, we propose a novel LLM service paradigmthat distributes privacy-sensitive computation on edge devices and sharedcomputation in the cloud. Only activations are transmitted between the centralcloud and edge devices to ensure data locality. Our core innovation,PrivateLoRA, addresses the challenging communication overhead by exploiting thelow rank of residual activations, achieving over 95% communication reduction.Consequently, PrivateLoRA effectively maintains data locality and is extremelyresource efficient. Under standard 5G networks, PrivateLoRA achieves throughputover 300% of device-only solutions for 7B models and over 80% of an A100 GPUfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRAfor advanced personalization. Our approach democratizes access tostate-of-the-art generative AI for edge devices, paving the way for moretailored LLM experiences for the general public. To our knowledge, our proposedframework is the first efficient and privacy-preserving LLM solution in theliterature.\rChallenges of Large Language Models for Mental Health Counseling\nNeo Christopher Chung, George Dyer, Lennart Brocki\nabstract\rabstract: The global mental health crisis is looming with a rapid increase in mentaldisorders, limited resources, and the social stigma of seeking treatment. Asthe field of artificial intelligence (AI) has witnessed significantadvancements in recent years, large language models (LLMs) capable ofunderstanding and generating human-like text may be used in supporting orproviding psychological counseling. However, the application of LLMs in themental health domain raises concerns regarding the accuracy, effectiveness, andreliability of the information provided. This paper investigates the majorchallenges associated with the development of LLMs for psychologicalcounseling, including model hallucination, interpretability, bias, privacy, andclinical effectiveness. We explore potential solutions to these challenges thatare practical and applicable to the current paradigm of AI. From our experiencein developing and deploying LLMs for mental health, AI holds a great promisefor improving mental health care, if we can carefully navigate and overcomepitfalls of LLMs.\r2023-11-21\nKNVQA: A Benchmark for evaluation knowledge-based VQA\nSirui Cheng, Siyu Zhang, Jiayi Wu, Muchen Lan\nabstract\rabstract: Within the multimodal field, large vision-language models (LVLMs) have madesignificant progress due to their strong perception and reasoning capabilitiesin the visual and language systems. However, LVLMs are still plagued by the twocritical issues of object hallucination and factual accuracy, which limit thepracticality of LVLMs in different scenarios. Furthermore, previous evaluationmethods focus more on the comprehension and reasoning of language content butlack a comprehensive evaluation of multimodal interactions, thereby resultingin potential limitations. To this end, we propose a novel KNVQA-Eval, which isdevoted to knowledge-based VQA task evaluation to reflect the factuality ofmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,we develop a new KNVQA dataset by incorporating human judgment and perception,aiming to evaluate the accuracy of standard answers relative to AI-generatedanswers in knowledge-based VQA. This work not only comprehensively evaluatesthe contextual information of LVLMs using reliable human annotations, but alsofurther analyzes the fine-grained capabilities of current methods to revealpotential avenues for subsequent optimization of LVLMs-based estimators. Ourproposed VQA-Eval and corresponding dataset KNVQA will facilitate thedevelopment of automatic evaluation tools with the advantages of low cost,privacy protection, and reproducibility. Our code will be released uponpublication.\rAdapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications\nSamira Ghodratnama, Mehrdad Zakershahrak\nabstract\rabstract: The advent of Large Language Models (LLMs) heralds a pivotal shift in onlineuser interactions with information. Traditional Information Retrieval (IR)systems primarily relied on query-document matching, whereas LLMs excel incomprehending and generating human-like text, thereby enriching the IRexperience significantly. While LLMs are often associated with chatbotfunctionalities, this paper extends the discussion to their explicitapplication in information retrieval. We explore methodologies to optimize theretrieval process, select optimal models, and effectively scale and orchestrateLLMs, aiming for cost-efficiency and enhanced result accuracy. A notablechallenge, model hallucination-where the model yields inaccurate ormisinterpreted data-is addressed alongside other model-specific hurdles. Ourdiscourse extends to crucial considerations including user privacy, dataoptimization, and the necessity for system clarity and interpretability.Through a comprehensive examination, we unveil not only innovative strategiesfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,but also the consequential considerations that underline the need for abalanced approach aligned with user-centric principles.\rDon\u0026rsquo;t forget private retrieval: distributed private similarity search for large language models\nGuy Zyskind, Tobin South, Alex Pentland\nabstract\rabstract: While the flexible capabilities of large language models (LLMs) allow them toanswer a range of queries based on existing learned knowledge, informationretrieval to augment generation is an important tool to allow LLMs to answerquestions on information not included in pre-training data. Such privateinformation is increasingly being generated in a wide array of distributedcontexts by organizations and individuals. Performing such informationretrieval using neural embeddings of queries and documents always leakedinformation about queries and database content unless both were stored locally.We present Private Retrieval Augmented Generation (PRAG), an approach that usesmulti-party computation (MPC) to securely transmit queries to a distributed setof servers containing a privately constructed database to return top-k andapproximate top-k documents. This is a first-of-its-kind approach to denseinformation retrieval that ensures no server observes a client\u0026rsquo;s query or cansee the database content. The approach introduces a novel MPC friendly protocolfor inverted file approximate search (IVF) that allows for fast document searchover distributed and private data in sublinear communication complexity. Thiswork presents new avenues through which data for use in LLMs can be accessedand used without needing to centralize or forgo privacy.\r2023-11-18\nExperts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q\u0026amp;A\nZahra Kolagar, Anna Katharina Leschanowsky, Birgit Popp\nabstract\rabstract: Privacy policies play a vital role in safeguarding user privacy as legaljurisdictions worldwide emphasize the need for transparent data processing.While the suitability of privacy policies to enhance transparency has beencritically discussed, employing conversational AI systems presents uniquechallenges in informing users effectively. In this position paper, we propose adynamic workflow for transforming privacy policies into privacyquestion-and-answer (Q\u0026amp;A) pairs to make privacy policies easily accessiblethrough conversational AI. Thereby, we facilitate interdisciplinarycollaboration among legal experts and conversation designers, while alsoconsidering the utilization of large language models\u0026rsquo; generative capabilitiesand addressing associated challenges. Our proposed workflow underscorescontinuous improvement and monitoring throughout the construction of privacyQ\u0026amp;As, advocating for comprehensive review and refinement through anexperts-in-the-loop approach.\r2023-11-16\nText Sanitization Beyond Specific Domains: Zero-Shot Redaction \u0026amp; Substitution with Large Language Models\nFederico Albanese, Daniel Ciolek, Nicolas D\u0026rsquo;Ippolito\nabstract\rabstract: In the context of information systems, text sanitization techniques are usedto identify and remove sensitive data to comply with security and regulatoryrequirements. Even though many methods for privacy preservation have beenproposed, most of them are focused on the detection of entities from specificdomains (e.g., credit card numbers, social security numbers), lackinggenerality and requiring customization for each desirable domain. Moreover,removing words is, in general, a drastic measure, as it can degrade textcoherence and contextual information. Less severe measures include substitutinga word for a safe alternative, yet it can be challenging to automatically findmeaningful substitutions. We present a zero-shot text sanitization techniquethat detects and substitutes potentially sensitive information using LargeLanguage Models. Our evaluation shows that our method excels at protectingprivacy while maintaining text coherence and contextual information, preservingdata utility for downstream tasks.\r2023-11-15\nHow Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\nLingbo Mo, Boshi Wang, Muhao Chen, Huan Sun\nabstract\rabstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.\rValue FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values\nJing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, Xing Xie\nabstract\rabstract: The rapid advancement of Large Language Models (LLMs) has attracted muchattention to value alignment for their responsible development. However, how todefine values in this context remains a largely unexplored question. Existingwork mainly follows the Helpful, Honest, Harmless principle and specifiesvalues as risk criteria formulated in the AI community, e.g., fairness andprivacy protection, suffering from poor clarity, adaptability and transparency.Inspired by basic values in humanity and social science across cultures, thiswork proposes a novel basic value alignment paradigm and introduces a valuespace spanned by basic value dimensions. All LLMs\u0026rsquo; behaviors can be mapped intothe space by identifying the underlying values, possessing the potential toaddress the three challenges. To foster future research, we apply therepresentative Schwartz\u0026rsquo;s Theory of Basic Values as an initialized example andconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.Our extensive analysis of FULCRA reveals the underlying relation between basicvalues and LLMs\u0026rsquo; behaviors, demonstrating that our approach not only coversexisting mainstream risks but also anticipates possibly unidentified ones.Additionally, we present an initial implementation of the basic valueevaluation and alignment, paving the way for future research in this line.\r2023-11-14\nSparsity-Preserving Differentially Private Training of Large Embedding Models\nBadih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang\nabstract\rabstract: As the use of large embedding models in recommendation systems and languageapplications increases, concerns over user data privacy have also risen.DP-SGD, a training algorithm that combines differential privacy with stochasticgradient descent, has been the workhorse in protecting user privacy withoutcompromising model accuracy by much. However, applying DP-SGD naively toembedding models can destroy gradient sparsity, leading to reduced trainingefficiency. To address this issue, we present two new algorithms, DP-FEST andDP-AdaFEST, that preserve gradient sparsity during private training of largeembedding models. Our algorithms achieve substantial reductions ($10^6 \\times$)in gradient size, while maintaining comparable levels of accuracy, on benchmarkreal-world datasets.\r2023-11-12\nTunable Soft Prompts are Messengers in Federated Learning\nChenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li\nabstract\rabstract: Federated learning (FL) enables multiple participants to collaborativelytrain machine learning models using decentralized data sources, alleviatingprivacy concerns that arise from directly sharing local data. However, the lackof model privacy protection in FL becomes an unneglectable challenge,especially when people want to federally finetune models based on a proprietarylarge language model. In this study, we propose a novel FL training approachthat accomplishes information exchange among participants via tunable softprompts. These soft prompts, updated and transmitted between the server andclients, assume the role of the global model parameters and serve as messengersto deliver useful knowledge from the local data and global model. As the globalmodel itself is not required to be shared and the local training is conductedbased on an auxiliary model with fewer parameters than the global model, theproposed approach provides protection for the global model while reducingcommunication and computation costs in FL. Extensive experiments show theeffectiveness of the proposed approach compared to several baselines. We havereleased the source code at\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.\rEvaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling\nYujin Cho, Mingeon Kim, Seojin Kim, Oyun Kwon, Ryan Donghan Kwon, Yoonha Lee, Dohyun Lim\nabstract\rabstract: This study investigates the efficacy of Large Language Models (LLMs) ininteractive language therapy for high-functioning autistic adolescents. Withthe rapid advancement of artificial intelligence, particularly in naturallanguage processing, LLMs present a novel opportunity to augment traditionalpsychological counseling methods. This research primarily focuses on evaluatingthe LLM\u0026rsquo;s ability to engage in empathetic, adaptable, and contextuallyappropriate interactions within a therapeutic setting. A comprehensiveevaluation was conducted by a panel of clinical psychologists and psychiatristsusing a specially developed scorecard. The assessment covered various aspectsof the LLM\u0026rsquo;s performance, including empathy, communication skills,adaptability, engagement, and the ability to establish a therapeutic alliance.The study avoided direct testing with patients, prioritizing privacy andethical considerations, and instead relied on simulated scenarios to gauge theLLM\u0026rsquo;s effectiveness. The results indicate that LLMs hold significant promise assupportive tools in therapy, demonstrating strengths in empathetic engagementand adaptability in conversation. However, challenges in achieving the depth ofpersonalization and emotional understanding characteristic of human therapistswere noted. The study also highlights the importance of ethical considerationsin the application of AI in therapeutic contexts. This research providesvaluable insights into the potential and limitations of using LLMs inpsychological counseling for autistic adolescents. It lays the groundwork forfuture explorations into AI\u0026rsquo;s role in mental health care, emphasizing the needfor ongoing development to enhance the capabilities of these models intherapeutic settings.\r2023-11-11\nAn In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing\nLe Peng, Gaoxiang Luo, sicheng zhou, jiandong chen, Rui Zhang, Ziyue Xu, Ju Sun\nabstract\rabstract: Language models (LMs) such as BERT and GPT have revolutionized naturallanguage processing (NLP). However, the medical field faces challenges intraining LMs due to limited data access and privacy constraints imposed byregulations like the Health Insurance Portability and Accountability Act(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning(FL) offers a decentralized solution that enables collaborative learning whileensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasksencompassing 8 corpora using 6 LMs. Our results show that: 1) FL modelsconsistently outperformed models trained on individual clients\u0026rsquo; data andsometimes performed comparably with models trained with polled data; 2) withthe fixed number of total data, FL models training with more clients producedinferior performance but pre-trained transformer-based models exhibited greatresilience. 3) FL models significantly outperformed large language models usingzero-/one-shot learning and offered lightning inference speed.\r2023-11-10\nWatermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service\nYuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu\nabstract\rabstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at https://github.com/Pter61/vlpmarker.\r2023-11-09\nEnhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT\nJingye Yang, Cong Liu, Wendy Deng, Da Wu, Chunhua Weng, Yunyun Zhou, Kai Wang\nabstract\rabstract: We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.\r2023-11-08\nBuilding Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective\nMd Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN\nabstract\rabstract: This paper studies how to effectively build meeting summarization systems forreal-world usage using large language models (LLMs). For this purpose, weconduct an extensive evaluation and comparison of various closed-source andopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findingsreveal that most closed-source LLMs are generally better in terms ofperformance. However, much smaller open-source models like LLaMA- 2 (7B and13B) could still achieve performance comparable to the large closed-sourcemodels even in zero-shot scenarios. Considering the privacy concerns ofclosed-source models for only being accessible via API, alongside the high costassociated with using fine-tuned versions of the closed-source models, theopensource models that can achieve competitive performance are moreadvantageous for industrial use. Balancing performance with associated costsand privacy concerns, the LLaMA-2-7B model looks more promising for industrialusage. In sum, this paper offers practical insights on using LLMs forreal-world business meeting summarization, shedding light on the trade-offsbetween performance and cost.\r2023-11-05\nQuantifying and Analyzing Entity-level Memorization in Large Language Models\nZhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su\nabstract\rabstract: Large language models (LLMs) have been proven capable of memorizing theirtraining data, which can be extracted through specifically designed prompts. Asthe scale of datasets continues to grow, privacy risks arising frommemorization have attracted increasing attention. Quantifying language modelmemorization helps evaluate potential privacy risks. However, prior works onquantifying memorization require access to the precise original data or incursubstantial computational overhead, making it difficult for applications inreal-world language models. To this end, we propose a fine-grained,entity-level definition to quantify memorization with conditions and metricscloser to real-world scenarios. In addition, we also present an approach forefficiently extracting sensitive entities from autoregressive language models.We conduct extensive experiments based on the proposed, probing languagemodels\u0026rsquo; ability to reconstruct sensitive entities under different settings. Wefind that language models have strong memorization at the entity level and areable to reproduce the training data even with partial leakages. The resultsdemonstrate that LLMs not only memorize their training data but also understandassociations between entities. These findings necessitate that trainers of LLMsexercise greater prudence regarding model memorization, adopting memorizationmitigation techniques to preclude privacy violations.\r2023-11-03\nAutomating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models\nJake Chanenson, Madison Pickering, Noah Apthorpe\nabstract\rabstract: Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research.\r2023-11-02\nInclusiveness Matters: A Large-Scale Analysis of User Feedback\nNowshin Nawar Arony, Ze Shi Li, Bowen Xu, Daniela Damian\nabstract\rabstract: In an era of rapidly expanding software usage, catering to the diverse needsof users from various backgrounds has become a critical challenge.Inclusiveness, representing a core human value, is frequently overlooked duringsoftware development, leading to user dissatisfaction. Users often engage indiscourse on online platforms where they indicate their concerns. In thisstudy, we leverage user feedback from three popular online sources, Reddit,Google Play Store, and Twitter, for 50 of the most popular apps in the world toreveal the inclusiveness-related concerns from end users. Using aSocio-Technical Grounded Theory approach, we analyzed 23,107 posts across thethree sources and identified 1,211 inclusiveness related posts. We organize ourempirical results in a taxonomy for inclusiveness comprising 6 majorcategories: Fairness, Technology, Privacy, Demography, Usability, and OtherHuman Values. To explore automated support to identifying inclusiveness-relatedposts, we experimented with five state-of-the-art pre-trained large languagemodels (LLMs) and found that these models\u0026rsquo; effectiveness is high and yet varieddepending on the data source. GPT-2 performed best on Reddit, BERT on theGoogle Play Store, and BART on Twitter. Our study provides an in-depth view ofinclusiveness-related user feedback from most popular apps and online sources.We provide implications and recommendations that can be used to bridge the gapbetween user expectations and software so that software developers can resonatewith the varied and evolving needs of the wide spectrum of users.\r2023-11-01\nMulti-step Jailbreaking Privacy Attacks on ChatGPT\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song\nabstract\rabstract: With the rapid progress of large language models (LLMs), many downstream NLPtasks can be well solved given appropriate prompts. Though model developers andresearchers work hard on dialog safety to avoid generating harmful content fromLLMs, it is still challenging to steer AI-generated content (AIGC) for thehuman good. As powerful LLMs are devouring existing text data from variousdomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whetherthe private information is included in the training data and what privacythreats can these LLMs and their downstream applications bring. In this paper,we study the privacy threats from OpenAI\u0026rsquo;s ChatGPT and the New Bing enhanced byChatGPT and show that application-integrated LLMs may cause new privacythreats. To this end, we conduct extensive experiments to support our claimsand discuss LLMs\u0026rsquo; privacy implications.\rKnowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\nRan Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, Carl Yang\nabstract\rabstract: Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.\r2023-10-31\nUnlearn What You Want to Forget: Efficient Unlearning for LLMs\nJiaao Chen, Diyi Yang\nabstract\rabstract: Large language models (LLMs) have achieved significant progress frompre-training on and memorizing a wide range of textual data, however, thisprocess might suffer from privacy issues and violations of data protectionregulations. As a result, the ability to easily remove data related toindividual users from such models while not deteriorating their predictivequality after the removal becomes increasingly important. To address theseissues, in this work, we propose an efficient unlearning framework that couldefficiently update LLMs without having to retrain the whole model after dataremovals, by introducing lightweight unlearning layers learned with a selectiveteacher-student objective into the transformers. In addition, we introduce afusion mechanism to effectively combine different unlearning layers that learnsto forget different sets of data to handle a sequence of forgetting operations.Experiments on classification and generation tasks demonstrate theeffectiveness of our proposed methods compared to the state-of-the-artbaselines.\rMaking Large Language Models Better Data Creators\nDong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar\nabstract\rabstract: Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world.\r2023-10-30\nKnowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks\nMinki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang\nabstract\rabstract: Large Language Models (LLMs) have shown promising performance inknowledge-intensive reasoning tasks that require a compound understanding ofknowledge. However, deployment of the LLMs in real-world applications can bechallenging due to their high computational requirements and concerns on dataprivacy. Previous studies have focused on building task-specific small LanguageModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,these approaches are ill-suited for knowledge-intensive reasoning tasks due tothe limited capacity of small LMs in memorizing the knowledge required.Motivated by our theoretical analysis on memorization, we proposeKnowledge-Augmented Reasoning Distillation (KARD), a novel method thatfine-tunes small LMs to generate rationales obtained from LLMs with augmentedknowledge retrieved from an external knowledge base. Moreover, we furtherpropose a neural reranker to obtain documents relevant to rationale generation.We empirically show that KARD significantly improves the performance of smallT5 and GPT models on the challenging knowledge-intensive reasoning datasets,namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the250M T5 models achieve superior performance against the fine-tuned 3B models,having 12 times larger parameters, on both MedQA-USMLE and StrategyQAbenchmarks.\r2023-10-27\nCan LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory\nNiloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi\nabstract\rabstract: The interactive use of large language models (LLMs) in AI assistants (atwork, home, etc.) introduces a new set of inference-time privacy risks: LLMsare fed different types of information from multiple sources in their inputsand are expected to reason about what to share in their outputs, for whatpurpose and with whom, within a given context. In this work, we draw attentionto the highly critical yet overlooked notion of contextual privacy by proposingConfAIde, a benchmark designed to identify critical weaknesses in the privacyreasoning capabilities of instruction-tuned LLMs. Our experiments show thateven the most capable models such as GPT-4 and ChatGPT reveal privateinformation in contexts that humans would not, 39% and 57% of the time,respectively. This leakage persists even when we employ privacy-inducingprompts or chain-of-thought reasoning. Our work underscores the immediate needto explore novel inference-time privacy-preserving approaches, based onreasoning and theory of mind.\rSDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes\nAdam D. Lelkes, Eric Loreaux, Tal Schuster, Ming-Jun Chen, Alvin Rajkomar\nabstract\rabstract: Social and behavioral determinants of health (SDOH) play a significant rolein shaping health outcomes, and extracting these determinants from clinicalnotes is a first step to help healthcare providers systematically identifyopportunities to provide appropriate care and address disparities. Progress onusing NLP methods for this task has been hindered by the lack of high-qualitypublicly available labeled data, largely due to the privacy and regulatoryconstraints on the use of real patients\u0026rsquo; information. This paper introduces anew dataset, SDOH-NLI, that is based on publicly available notes and which werelease publicly. We formulate SDOH extraction as a natural language inference(NLI) task, and provide binary textual entailment labels obtained from humanraters for a cross product of a set of social history snippets as premises andSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks inthat our premises and hypotheses are obtained independently. We evaluate both\u0026quot;off-the-shelf\u0026quot; entailment models as well as models fine-tuned on our data, andhighlight the ways in which our dataset appears more challenging than commonlyused NLI datasets.\r2023-10-26\nPockEngine: Sparse and Efficient Fine-tuning in a Pocket\nLigeng Zhu, Lanxiang Hu, Ji Lin, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, Song Han\nabstract\rabstract: On-device learning and efficient fine-tuning enable continuous andprivacy-preserving customization (e.g., locally fine-tuning large languagemodels on personalized data). However, existing training frameworks aredesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) andlack the optimizations for learning on the edge, which faces challenges ofresource limitations and edge hardware diversity. We introduce PockEngine: atiny, sparse and efficient engine to enable fine-tuning on various edgedevices. PockEngine supports sparse backpropagation: it prunes the backwardgraph and sparsely updates the model with measured memory saving and latencyreduction while maintaining the model quality. Secondly, PockEngine iscompilation first: the entire training graph (including forward, backward andoptimization steps) is derived at compile-time, which reduces the runtimeoverhead and brings opportunities for graph transformations. PockEngine alsointegrates a rich set of training graph optimizations, thus can furtheraccelerate the training cost, including operator reordering and backendswitching. PockEngine supports diverse applications, frontends and hardwarebackends: it flexibly compiles and tunes models defined inPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. Weevaluated PockEngine on both vision models and large language models.PockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow(Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin).Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orinat 550 tokens/s, 7.9$\\times$ faster than the PyTorch.\rDocumentNet: Bridging the Data Gap in Document Pre-Training\nLijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G. Hauptmann, Hanjun Dai, Wei Wei\nabstract\rabstract: Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER.\r2023-10-25\nFedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning\nJaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho D. Choi, Sung-Ju Lee\nabstract\rabstract: Psychiatrists diagnose mental disorders via the linguistic use of patients.Still, due to data privacy, existing passive mental health monitoring systemsuse alternative features such as activity, app usage, and location via mobiledevices. We propose FedTherapist, a mobile mental health monitoring system thatutilizes continuous speech and keyboard input in a privacy-preserving way viafederated learning. We explore multiple model designs by comparing theirperformance and overhead for FedTherapist to overcome the complex nature ofon-device language model training on smartphones. We further propose aContext-Aware Language Learning (CALL) methodology to effectively utilizesmartphones\u0026rsquo; large and noisy text for mental health signal sensing. OurIRB-approved evaluation of the prediction of self-reported depression, stress,anxiety, and mood from 46 participants shows higher accuracy of FedTherapistcompared with the performance with non-language features, achieving 0.15 AUROCimprovement and 8.21% MAE reduction.\rPrivately Aligning Language Models with Reinforcement Learning\nFan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim\nabstract\rabstract: Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.\rRCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models\nZefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen\nabstract\rabstract: Large language model (LLM) applications in cloud root cause analysis (RCA)have been actively explored recently. However, current methods are stillreliant on manual workflow settings and do not unleash LLMs\u0026rsquo; decision-makingand environment interaction capabilities. We present RCAgent, a tool-augmentedLLM autonomous agent framework for practical and privacy-aware industrial RCAusage. Running on an internally deployed model rather than GPT families,RCAgent is capable of free-form data collection and comprehensive analysis withtools. Our framework combines a variety of enhancements, including a uniqueSelf-Consistency for action trajectories, and a suite of methods for contextmanagement, stabilization, and importing domain knowledge. Our experiments showRCAgent\u0026rsquo;s evident and consistent superiority over ReAct across all aspects ofRCA \u0026ndash; predicting root causes, solutions, evidence, and responsibilities \u0026ndash; andtasks covered or uncovered by current rules, as validated by both automatedmetrics and human evaluations. Furthermore, RCAgent has already been integratedinto the diagnosis and issue discovery workflow of the Real-time ComputePlatform for Apache Flink of Alibaba Cloud.\r2023-10-24\nThe Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks\nXiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang\nabstract\rabstract: The era post-2018 marked the advent of Large Language Models (LLMs), withinnovations such as OpenAI\u0026rsquo;s ChatGPT showcasing prodigious linguistic prowess.As the industry galloped toward augmenting model parameters and capitalizing onvast swaths of human language data, security and privacy challenges alsoemerged. Foremost among these is the potential inadvertent accrual of PersonalIdentifiable Information (PII) during web-based data acquisition, posing risksof unintended PII disclosure. While strategies like RLHF during training andCatastrophic Forgetting have been marshaled to control the risk of privacyinfringements, recent advancements in LLMs, epitomized by OpenAI\u0026rsquo;s fine-tuninginterface for GPT-3.5, have reignited concerns. One may ask: can thefine-tuning of LLMs precipitate the leakage of personal information embeddedwithin training datasets? This paper reports the first endeavor to seek theanswer to the question, particularly our discovery of a new LLM exploitationavenue, called the Janus attack. In the attack, one can construct a PIIassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,to potentially reinstate and reveal concealed PIIs. Our findings indicate that,with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition frombeing impermeable to PII extraction to a state where they divulge a substantialproportion of concealed PII. This research, through its deep dive into theJanus attack vector, underscores the imperative of navigating the intricateinterplay between LLM utility and privacy preservation.\rSoK: Memorization in General-Purpose Large Language Models\nValentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West\nabstract\rabstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.\rCRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model\nKaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou\nabstract\rabstract: Instruction tuning has recently been recognized as an effective way ofaligning Large Language Models (LLMs) to enhance their generalization abilityacross various tasks. However, when tuning publicly accessible, centralizedLLMs with private instruction data, privacy concerns are inevitable. Whiledirect transfer of parameterized modules between models is a plausible approachto address this, its implications and effectiveness need further exploration.This paper focuses on Offsite-Tuning (OFT), a representative technique thattransfers transformer blocks between centralized LLMs and downstream emulators.Given the limited understanding of the underlying mechanism of OFT, we performan empirical analysis on LLMs from the perspectives of representation andfunctional similarity. Interestingly, our findings reveal a unique modularstructure within the layers of LLMs that appears to emerge as the model sizeexpands. Simultaneously, we note subtle but potentially significant changes inrepresentation and intermediate predictions across the layers. Inspired bythese observations, we propose CRaSh, involving Clustering, Removing, andSharing, a training-free strategy to derive improved emulators from LLMs. CRaShsignificantly boosts performance of OFT with billions of parameters.Furthermore, we investigate the optimal solutions yielded by fine-tuning withand without full model through the lens of loss landscape. Our findingsdemonstrate a linear connectivity among these optima falling over the samebasin, thereby highlighting the effectiveness of CRaSh and OFT. The source codeis publicly available at https://github.com/TsinghuaC3I/CRaSh.\r2023-10-23\n$Λ$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative AI\nShoki Ohta, Takayuki Nishio\nabstract\rabstract: In the wake of the burgeoning expansion of generative artificial intelligence(AI) services, the computational demands inherent to these technologiesfrequently necessitate cloud-powered computational offloading, particularly forresource-constrained mobile devices. These services commonly employ prompts tosteer the generative process, and both the prompts and the resultant content,such as text and images, may harbor privacy-sensitive or confidentialinformation, thereby elevating security and privacy risks. To mitigate theseconcerns, we introduce $\\Lambda$-Split, a split computing framework tofacilitate computational offloading while simultaneously fortifying dataprivacy against risks such as eavesdropping and unauthorized access. In$\\Lambda$-Split, a generative model, usually a deep neural network (DNN), ispartitioned into three sub-models and distributed across the user\u0026rsquo;s localdevice and a cloud server: the input-side and output-side sub-models areallocated to the local, while the intermediate, computationally-intensivesub-model resides on the cloud server. This architecture ensures that only thehidden layer outputs are transmitted, thereby preventing the externaltransmission of privacy-sensitive raw input and output data. Given theblack-box nature of DNNs, estimating the original input or output fromintercepted hidden layer outputs poses a significant challenge for maliciouseavesdroppers. Moreover, $\\Lambda$-Split is orthogonal to traditionalencryption-based security mechanisms, offering enhanced security when deployedin conjunction. We empirically validate the efficacy of the $\\Lambda$-Splitframework using Llama 2 and Stable Diffusion XL, representative large languageand diffusion models developed by Meta and Stability AI, respectively. Our$\\Lambda$-Split implementation is publicly accessible athttps://github.com/nishio-laboratory/lambda_split.\rHealth Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model\nYohn Jairo Parra Bautista, Vinicious Lima, Carlos Theran, Richard Alo\nabstract\rabstract: Health disparities are differences in health outcomes and access tohealthcare between different groups, including racial and ethnic minorities,low-income people, and rural residents. An artificial intelligence (AI) programcalled large language models (LLMs) can understand and generate human language,improving health communication and reducing health disparities. There are manychallenges in using LLMs in human-doctor interaction, including the need fordiverse and representative data, privacy concerns, and collaboration betweenhealthcare providers and technology experts. We introduce the comparativeinvestigation of domain-specific large language models such as SciBERT with amulti-purpose LLMs BERT. We used cosine similarity to analyze text queriesabout health disparities in exam rooms when factors such as race are usedalone. Using text queries, SciBERT fails when it doesn\u0026rsquo;t differentiate betweenqueries text: \u0026ldquo;race\u0026rdquo; alone and \u0026ldquo;perpetuates health disparities.\u0026rdquo; We believeclinicians can use generative AI to create a draft response when communicatingasynchronously with patients. However, careful attention must be paid to ensurethey are developed and implemented ethically and equitably.\rMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems\nJakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan\nabstract\rabstract: While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly.\rTowards LLM-driven Dialogue State Tracking\nYujie Feng, Zexin Lu, Bo Liu, Liming Zhan, Xiao-Ming Wu\nabstract\rabstract: Dialogue State Tracking (DST) is of paramount importance in ensuring accuratetracking of user goals and system actions within task-oriented dialoguesystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPThas sparked considerable interest in assessing their efficacy across diverseapplications. In this study, we conduct an initial examination of ChatGPT\u0026rsquo;scapabilities in DST. Our evaluation uncovers the exceptional performance ofChatGPT in this task, offering valuable insights to researchers regarding itscapabilities and providing useful directions for designing and enhancingdialogue systems. Despite its impressive performance, ChatGPT has significantlimitations including its closed-source nature, request restrictions, raisingdata privacy concerns, and lacking local deployment capabilities. To addressthese concerns, we present LDST, an LLM-driven DST framework based on smaller,open-source foundation models. By utilizing a novel domain-slot instructiontuning method, LDST achieves performance on par with ChatGPT. Comprehensiveevaluations across three distinct experimental settings, we find that LDSTexhibits remarkable performance improvements in both zero-shot and few-shotsetting compared to previous SOTA methods. The source code is provided forreproducibility.\rDid the Neurons Read your Book? Document-level Membership Inference for Large Language Models\nMatthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye\nabstract\rabstract: With large language models (LLMs) poised to become embedded in our dailylives, questions are starting to be raised about the dataset(s) they learnedfrom. These questions range from potential bias or misinformation LLMs couldretain from their training data to questions of copyright and fair use ofhuman-generated text. However, while these questions emerge, developers of therecent state-of-the-art LLMs become increasingly reluctant to disclose detailson their training corpus. We here introduce the task of document-levelmembership inference for real-world LLMs, i.e. inferring whether the LLM hasseen a given document during training or not. First, we propose a procedure forthe development and evaluation of document-level membership inference for LLMsby leveraging commonly used data sources for training and the model releasedate. We then propose a practical, black-box method to predict document-levelmembership and instantiate it on OpenLLaMA-7B with both books and academicpapers. We show our methodology to perform very well, reaching an impressiveAUC of 0.856 for books and 0.678 for papers. We then show our approach tooutperform the sentence-level membership inference attacks used in the privacyliterature for the document-level membership task. We finally evaluate whethersmaller models might be less sensitive to document-level inference and showOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.Taken together, our results show that accurate document-level membership can beinferred for LLMs, increasing the transparency of technology poised to changeour lives.\r2023-10-20\nAssessing Privacy Risks in Language Models: A Case Study on Summarization Tasks\nRuixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan Kulkarni, Xia Hu\nabstract\rabstract: Large language models have revolutionized the field of NLP by achievingstate-of-the-art performance on various tasks. However, there is a concern thatthese models may disclose information in the training data. In this study, wefocus on the summarization task and investigate the membership inference (MI)attack: given a sample and black-box access to a model\u0026rsquo;s API, it is possible todetermine if the sample was part of the training data. We exploit textsimilarity and the model\u0026rsquo;s resistance to document modifications as potential MIsignals and evaluate their effectiveness on widely used datasets. Our resultsdemonstrate that summarization models are at risk of exposing data membership,even in cases where the reference summary is not available. Furthermore, wediscuss several safeguards for training summarization models to protect againstMI attacks and discuss the inherent trade-off between privacy and utility.\rZero-Shot Sharpness-Aware Quantization for Pre-trained Language Models\nMiaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao\nabstract\rabstract: Quantization is a promising approach for reducing memory overhead andaccelerating inference, especially in large pre-trained language model (PLM)scenarios. While having no access to original training data due to security andprivacy concerns has emerged the demand for zero-shot quantization. Most of thecutting-edge zero-shot quantization methods primarily 1) apply to computervision tasks, and 2) neglect of overfitting problem in the generativeadversarial learning process, leading to sub-optimal performance. Motivated bythis, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)framework for the zero-shot quantization of various PLMs. The key algorithm insolving ZSAQ is the SAM-SGA optimization, which aims to improve thequantization accuracy and model generalization via optimizing a minimaxproblem. We theoretically prove the convergence rate for the minimaxoptimization problem and this result can be applied to other nonconvex-PLminimax optimization frameworks. Extensive experiments on 11 tasks demonstratethat our method brings consistent and significant performance gains on bothdiscriminative and generative PLMs, i.e., up to +6.98 average score.Furthermore, we empirically validate that our method can effectively improvethe model generalization.\r2023-10-19\nPrivacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework\nImdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere\nabstract\rabstract: The generative Artificial Intelligence (AI) tools based on Large LanguageModels (LLMs) use billions of parameters to extensively analyse large datasetsand extract critical private information such as, context, specific details,identifying information etc. This have raised serious threats to user privacyand reluctance to use such tools. This article proposes the conceptual modelcalled PrivChatGPT, a privacy-preserving model for LLMs that consists of twomain components i.e., preserving user privacy during the datacuration/pre-processing together with preserving private context and theprivate training process for large-scale data. To demonstrate itsapplicability, we show how a private mechanism could be integrated into theexisting model for training LLMs to protect user privacy; specifically, weemployed differential privacy and private training using Reinforcement Learning(RL). We measure the privacy loss and evaluate the measure of uncertainty orrandomness once differential privacy is applied. It further recursivelyevaluates the level of privacy guarantees and the measure of uncertainty ofpublic database and resources, during each update when new information is addedfor training purposes. To critically evaluate the use of differential privacyfor private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,private information retrieval, randomisation, for various performance measuressuch as the model performance and accuracy, computational complexity, privacyvs. utility etc. We conclude that differential privacy, randomisation, andobfuscation can impact utility and performance of trained models, conversely,the use of ToR, Blockchain, and PIR may introduce additional computationalcomplexity and high training latency. We believe that the proposed model couldbe used as a benchmark for proposing privacy preserving LLMs for generative AItools.\rTabuLa: Harnessing Language Models for Tabular Data Synthesis\nZilong Zhao, Robert Birke, Lydia Chen\nabstract\rabstract: Given the ubiquitous use of tabular data in industries and the growingconcerns in data privacy and security, tabular data synthesis emerges as acritical research area. The recent state-of-the-art methods show that largelanguage models (LLMs) can be adopted to generate realistic tabular data. AsLLMs pre-process tabular data as full text, they have the advantage of avoidingthe curse of dimensionality associated with one-hot encoding high-dimensionaldata. However, their long training time and limited re-usability on new tasksprevent them from replacing exiting tabular generative models. In this paper,we propose Tabula, a tabular data synthesizer based on the language modelstructure. Through Tabula, we demonstrate the inherent limitation of employingpre-trained language models designed for natural language processing (NLP) inthe context of tabular data synthesis. Our investigation delves into thedevelopment of a dedicated foundational model tailored specifically for tabulardata synthesis. Additionally, we propose a token sequence compression strategyto significantly reduce training time while preserving the quality of syntheticdata. Extensive experiments on six datasets demonstrate that using a languagemodel structure without loading the well-trained model weights yields a betterstarting model for tabular data synthesis. Moreover, the Tabula model,previously trained on other tabular data, serves as an excellent foundationmodel for new tabular data synthesis tasks. Additionally, the token sequencecompression method substantially reduces the model\u0026rsquo;s training time. Resultsshow that Tabula averagely reduces 46.2% training time per epoch comparing tocurrent LLMs-based state-of-the-art algorithm and consistently achieves evenhigher synthetic data utility.\r2023-10-17\nLast One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning\nRui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem\nabstract\rabstract: Large Language Models (LLMs) are powerful tools for natural languageprocessing, enabling novel applications and user experiences. However, toachieve optimal performance, LLMs often require adaptation with private data,which poses privacy and security challenges. Several techniques have beenproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparativeprivacy and security properties have not been systematically investigated. Inthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICLagainst three types of well-established attacks: membership inference, whichexposes data leakage (privacy); backdoor, which injects malicious behavior(security); and model stealing, which can violate intellectual property(privacy and security). Our results show that there is no silver bullet forprivacy and security in LLM adaptation and each technique has differentstrengths and weaknesses.\rOpportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health\nShubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C. Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, Zhiyong Lu\nabstract\rabstract: ChatGPT has drawn considerable attention from both the general public anddomain experts with its remarkable text generation capabilities. This hassubsequently led to the emergence of diverse applications in the field ofbiomedicine and health. In this work, we examine the diverse applications oflarge language models (LLMs), such as ChatGPT, in biomedicine and health.Specifically we explore the areas of biomedical information retrieval, questionanswering, medical text summarization, information extraction, and medicaleducation, and investigate whether LLMs possess the transformative power torevolutionize these tasks or whether the distinct complexities of biomedicaldomain presents unique challenges. Following an extensive literature survey, wefind that significant advances have been made in the field of text generationtasks, surpassing the previous state-of-the-art methods. For otherapplications, the advances have been modest. Overall, LLMs have not yetrevolutionized biomedicine, but recent rapid progress indicates that suchmethods hold great potential to provide valuable means for acceleratingdiscovery and improving health. We also find that the use of LLMs, likeChatGPT, in the fields of biomedicine and health entails various risks andchallenges, including fabricated information in its generated responses, aswell as legal and privacy concerns associated with sensitive patient data. Webelieve this survey can provide a comprehensive and timely overview tobiomedical researchers and healthcare practitioners on the opportunities andchallenges associated with using ChatGPT and other LLMs for transformingbiomedicine and health.\r2023-10-16\nPrivacy in Large Language Models: Attacks, Defenses and Future Directions\nHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song\nabstract\rabstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary\u0026rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.\rFATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models\nTao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, haveexhibited remarkable performances across various tasks in recent years.However, LLMs face two main challenges in real-world applications. Onechallenge is that training LLMs consumes vast computing resources, preventingLLMs from being adopted by small and medium-sized enterprises with limitedcomputing resources. Another is that training LLM requires a large amount ofhigh-quality data, which are often scattered among enterprises. To addressthese challenges, we propose FATE-LLM, an industrial-grade federated learningframework for large language models. FATE-LLM (1) facilitates federatedlearning for large language models (coined FedLLM); (2) promotes efficienttraining of FedLLM using parameter-efficient fine-tuning methods; (3) protectsthe intellectual property of LLMs; (4) preserves data privacy during trainingand inference through privacy-preserving mechanisms. We release the code ofFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the researchof FedLLM and enable a broad range of industrial applications.\rSnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds\nYanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren\nabstract\rabstract: Text-to-image diffusion models can create stunning images from naturallanguage descriptions that rival the work of professional artists andphotographers. However, these models are large, with complex networkarchitectures and tens of denoising iterations, making them computationallyexpensive and slow to run. As a result, high-end GPUs and cloud-based inferenceare required to run diffusion models at scale. This is costly and has privacyimplications, especially when user data is sent to a third party. To overcomethese challenges, we present a generic approach that, for the first time,unlocks running text-to-image diffusion models on mobile devices in less than$2$ seconds. We achieve so by introducing efficient network architecture andimproving step distillation. Specifically, we propose an efficient UNet byidentifying the redundancy of the original model and reducing the computationof the image decoder via data distillation. Further, we enhance the stepdistillation by exploring training strategies and introducing regularizationfrom classifier-free guidance. Our extensive experiments on MS-COCO show thatour model with $8$ denoising steps achieves better FID and CLIP scores thanStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creationby bringing powerful text-to-image diffusion models to the hands of users.\r2023-10-12\nAn Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\nShyni Sharaf, V. S. Anoop\nabstract\rabstract: This paper conducts a comprehensive investigation into applying largelanguage models, particularly on BioBERT, in healthcare. It begins withthoroughly examining previous natural language processing (NLP) approaches inhealthcare, shedding light on the limitations and challenges these methodsface. Following that, this research explores the path that led to theincorporation of BioBERT into healthcare applications, highlighting itssuitability for addressing the specific requirements of tasks related tobiomedical text mining. The analysis outlines a systematic methodology forfine-tuning BioBERT to meet the unique needs of the healthcare domain. Thisapproach includes various components, including the gathering of data from awide range of healthcare sources, data annotation for tasks like identifyingmedical entities and categorizing them, and the application of specializedpreprocessing techniques tailored to handle the complexities found inbiomedical texts. Additionally, the paper covers aspects related to modelevaluation, with a focus on healthcare benchmarks and functions like processingof natural language in biomedical, question-answering, clinical documentclassification, and medical entity recognition. It explores techniques toimprove the model\u0026rsquo;s interpretability and validates its performance compared toexisting healthcare-focused language models. The paper thoroughly examinesethical considerations, particularly patient privacy and data security. Ithighlights the benefits of incorporating BioBERT into healthcare contexts,including enhanced clinical decision support and more efficient informationretrieval. Nevertheless, it acknowledges the impediments and complexities ofthis integration, encompassing concerns regarding data privacy, transparency,resource-intensive requirements, and the necessity for model customization toalign with diverse healthcare domains.\r2023-10-11\nBeyond Memorization: Violating Privacy Via Inference with Large Language Models\nRobin Staab, Mark Vero, Mislav Balunović, Martin Vechev\nabstract\rabstract: Current privacy research on large language models (LLMs) primarily focuses onthe issue of extracting memorized training data. At the same time, models\u0026rsquo;inference capabilities have increased drastically. This raises the key questionof whether current LLMs could violate individuals\u0026rsquo; privacy by inferringpersonal attributes from text given at inference time. In this work, we presentthe first comprehensive study on the capabilities of pretrained LLMs to inferpersonal attributes from text. We construct a dataset consisting of real Redditprofiles, and show that current LLMs can infer a wide range of personalattributes (e.g., location, income, sex), achieving up to $85%$ top-1 and$95.8%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time($240\\times$) required by humans. As people increasingly interact withLLM-powered chatbots across all aspects of life, we also explore the emergingthreat of privacy-invasive chatbots trying to extract personal informationthrough seemingly benign questions. Finally, we show that common mitigations,i.e., text anonymization and model alignment, are currently ineffective atprotecting user privacy against LLM inference. Our findings highlight thatcurrent LLMs can infer personal data at a previously unattainable scale. In theabsence of working defenses, we advocate for a broader discussion around LLMprivacy implications beyond memorization, striving for a wider privacyprotection.\r2023-10-10\nMemorization of Named Entities in Fine-tuned BERT Models\nAndor Diera, Nicolas Lell, Aygul Garifullina, Ansgar Scherp\nabstract\rabstract: Privacy preserving deep learning is an emerging field in machine learningthat aims to mitigate the privacy risks in the use of deep neural networks. Onesuch risk is training data extraction from language models that have beentrained on datasets, which contain personal and privacy sensitive information.In our study, we investigate the extent of named entity memorization infine-tuned BERT models. We use single-label text classification asrepresentative downstream task and employ three different fine-tuning setups inour experiments, including one with Differentially Privacy (DP). We create alarge number of text samples from the fine-tuned BERT models utilizing a customsequential sampling strategy with two prompting strategies. We search in thesesamples for named entities and check if they are also present in thefine-tuning datasets. We experiment with two benchmark datasets in the domainsof emails and blogs. We show that the application of DP has a detrimentaleffect on the text generation capabilities of BERT. Furthermore, we show that afine-tuned BERT does not generate more named entities specific to thefine-tuning dataset than a BERT model that is pre-trained only. This suggeststhat BERT is unlikely to emit personal or privacy sensitive named entities.Overall, our results are important to understand to what extent BERT-basedservices are prone to training data extraction attacks.\rEvaluation and Analysis of Hallucination in Large Vision-Language Models\nJunyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang\nabstract\rabstract: Large Vision-Language Models (LVLMs) have recently achieved remarkablesuccess. However, LVLMs are still plagued by the hallucination problem, whichlimits the practicality in many scenarios. Hallucination refers to theinformation of LVLMs\u0026rsquo; responses that does not exist in the visual input, whichposes potential risks of substantial consequences. There has been limited workstudying hallucination evaluation in LVLMs. In this paper, we proposeHallucination Evaluation based on Large Language Models (HaELM), an LLM-basedhallucination evaluation framework. HaELM achieves an approximate 95%performance comparable to ChatGPT and has additional advantages including lowcost, reproducibility, privacy preservation and local deployment. Leveragingthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, weanalyze the factors contributing to hallucination in LVLMs and offer helpfulsuggestions to mitigate the hallucination problem. Our training data and humanannotation hallucination data will be made public soon.\rBC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models\nHaoxiang Luo, Jian Luo, Athanasios V. Vasilakos\nabstract\rabstract: In recent years, artificial intelligence (AI) and machine learning (ML) arereshaping society\u0026rsquo;s production methods and productivity, and also changing theparadigm of scientific research. Among them, the AI language model representedby ChatGPT has made great progress. Such large language models (LLMs) servepeople in the form of AI-generated content (AIGC) and are widely used inconsulting, healthcare, and education. However, it is difficult to guaranteethe authenticity and reliability of AIGC learning data. In addition, there arealso hidden dangers of privacy disclosure in distributed AI training. Moreover,the content generated by LLMs is difficult to identify and trace, and it isdifficult to cross-platform mutual recognition. The above information securityissues in the coming era of AI powered by LLMs will be infinitely amplified andaffect everyone\u0026rsquo;s life. Therefore, we consider empowering LLMs using blockchaintechnology with superior security features to propose a vision for trusted AI.This paper mainly introduces the motivation and technical route of blockchainfor LLM (BC4LLM), including reliable learning corpus, secure training process,and identifiable generated content. Meanwhile, this paper also reviews thepotential applications and future challenges, especially in the frontiercommunication networks field, including network resource allocation, dynamicspectrum sharing, and semantic communication. Based on the above work combinedand the prospect of blockchain and LLMs, it is expected to help the earlyrealization of trusted AI and provide guidance for the academic community.\rWatermarking Classification Dataset for Copyright Protection\nYixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun\nabstract\rabstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals\u0026rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.\r2023-10-09\nDoes Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?\nAdaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao \u0026lsquo;Kenneth\u0026rsquo; Huang, Dongwon Lee\nabstract\rabstract: Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved thegeneration of coherent sentences resembling human writing on a large scale,resulting in the creation of so-called deepfake texts. However, this progressposes security and privacy concerns, necessitating effective solutions fordistinguishing deepfake texts from human-written ones. Although prior worksstudied humans\u0026rsquo; ability to detect deepfake texts, none has examined whether\u0026quot;collaboration\u0026quot; among humans improves the detection of deepfake texts. In thisstudy, to address this gap of understanding on deepfake texts, we conductedexperiments with two groups: (1) nonexpert individuals from the AMT platformand (2) writing experts from the Upwork platform. The results demonstrate thatcollaboration among humans can potentially improve the detection of deepfaketexts for both groups, increasing detection accuracies by 6.36% for non-expertsand 12.76% for experts, respectively, compared to individuals\u0026rsquo; detectionaccuracies. We further analyze the explanations that humans used for detectinga piece of text as deepfake text, and find that the strongest indicator ofdeepfake texts is their lack of coherence and consistency. Our study providesuseful insights for future tools and framework designs to facilitate thecollaborative human detection of deepfake texts. The experiment datasets andAMT implementations are available at:https://github.com/huashen218/llm-deepfake-human-study.git\r2023-10-07\nPrompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\nGabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, Giovanni Trappolini\nabstract\rabstract: In this paper, we present a groundbreaking paradigm for human-computerinteraction that revolutionizes the traditional notion of an operating system. Within this innovative framework, user requests issued to the machine arehandled by an interconnected ecosystem of generative AI models that seamlesslyintegrate with or even replace traditional software applications. At the coreof this paradigm shift are large generative models, such as language anddiffusion models, which serve as the central interface between users andcomputers. This pioneering approach leverages the abilities of advancedlanguage models, empowering users to engage in natural language conversationswith their computing devices. Users can articulate their intentions, tasks, andinquiries directly to the system, eliminating the need for explicit commands orcomplex navigation. The language model comprehends and interprets the user\u0026rsquo;sprompts, generating and displaying contextual and meaningful responses thatfacilitate seamless and intuitive interactions. This paradigm shift not only streamlines user interactions but also opens upnew possibilities for personalized experiences. Generative models can adapt toindividual preferences, learning from user input and continuously improvingtheir understanding and response generation. Furthermore, it enables enhancedaccessibility, as users can interact with the system using speech or text,accommodating diverse communication preferences. However, this visionary concept raises significant challenges, includingprivacy, security, trustability, and the ethical use of generative models.Robust safeguards must be in place to protect user data and prevent potentialmisuse or manipulation of the language model. While the full realization of this paradigm is still far from being achieved,this paper serves as a starting point for envisioning this transformativepotential.\r2023-10-06\nQuantized Transformer Language Model Implementations on Edge Devices\nMohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, Soheil Salehi\nabstract\rabstract: Large-scale transformer-based models like the Bidirectional EncoderRepresentations from Transformers (BERT) are widely used for Natural LanguageProcessing (NLP) applications, wherein these models are initially pre-trainedwith a large corpus with millions of parameters and then fine-tuned for adownstream NLP task. One of the major limitations of these large-scale modelsis that they cannot be deployed on resource-constrained devices due to theirlarge model size and increased inference latency. In order to overcome theselimitations, such large-scale models can be converted to an optimizedFlatBuffer format, tailored for deployment on resource-constrained edgedevices. Herein, we evaluate the performance of such FlatBuffer transformedMobileBERT models on three different edge devices, fine-tuned for Reputationanalysis of English language tweets in the RepLab 2013 dataset. In addition,this study encompassed an evaluation of the deployed models, wherein theirlatency, performance, and resource efficiency were meticulously assessed. Ourexperiment results show that, compared to the original BERT large model, theconverted and quantized MobileBERT models have 160$\\times$ smaller footprintsfor a 4.1% drop in accuracy while analyzing at least one tweet per second onedge devices. Furthermore, our study highlights the privacy-preserving aspectof TinyML systems as all data is processed locally within a serverlessenvironment.\r2023-10-04\nMedAlpaca \u0026ndash; An Open-Source Collection of Medical Conversational AI Models and Training Data\nTianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, Keno K. Bressem\nabstract\rabstract: As large language models (LLMs) like OpenAI\u0026rsquo;s GPT series continue to makestrides, we witness the emergence of artificial intelligence applications in anever-expanding range of fields. In medicine, these LLMs hold considerablepromise for improving medical workflows, diagnostics, patient care, andeducation. Yet, there is an urgent need for open-source models that can bedeployed on-premises to safeguard patient privacy. In our work, we present aninnovative dataset consisting of over 160,000 entries, specifically crafted tofine-tune LLMs for effective medical applications. We investigate the impact offine-tuning these datasets on publicly accessible pre-trained LLMs, andsubsequently, we juxtapose the performance of pre-trained-only models againstthe fine-tuned models concerning the examinations that future medical doctorsmust pass to achieve certification.\rDP-SGD for non-decomposable objective functions\nWilliam Kong, Andrés Muñoz Medina, Mónica Ribero\nabstract\rabstract: Unsupervised pre-training is a common step in developing computer visionmodels and large language models. In this setting, the absence of labelsrequires the use of similarity-based loss functions, such as contrastive loss,that favor minimizing the distance between similar inputs and maximizing thedistance between distinct inputs. As privacy concerns mount, training thesemodels using differential privacy has become more important. However, due tohow inputs are generated for these losses, one of their undesirable propertiesis that their $L_2$ sensitivity can grow with increasing batch size. Thisproperty is particularly disadvantageous for differentially private trainingmethods, such as DP-SGD. To overcome this issue, we develop a new DP-SGDvariant for similarity based loss functions \u0026ndash; in particular the commonly usedcontrastive loss \u0026ndash; that manipulates gradients of the objective function in anovel way to obtain a senstivity of the summed gradient that is $O(1)$ forbatch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, ourmethod\u0026rsquo;s performance comes close to that of a non-private model and generallyoutperforms DP-SGD applied directly to the contrastive loss.\r2023-10-03\nLarge Language Models Can Be Good Privacy Protection Learners\nYijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng\nabstract\rabstract: The proliferation of Large Language Models (LLMs) has driven considerableinterest in fine-tuning them with domain-specific data to create specializedlanguage models. Nevertheless, such domain-specific fine-tuning data oftencontains sensitive personally identifiable information (PII). Directfine-tuning LLMs on this data without privacy protection poses a risk ofleakage. To address this challenge, we introduce Privacy Protection LanguageModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injectsdomain-specific knowledge while safeguarding data privacy. Our work offers atheoretical analysis for model design and delves into various techniques suchas corpus curation, penalty-based unlikelihood in training loss, andinstruction-based tuning, etc. Extensive experiments across diverse datasetsand scenarios demonstrate the effectiveness of our approaches. In particular,instruction tuning with both positive and negative examples, stands out as apromising method, effectively protecting private data while enhancing themodel\u0026rsquo;s knowledge. Our work underscores the potential for Large Language Modelsas robust privacy protection learners.\rCan Language Models be Instructed to Protect Personal Information?\nYang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter\nabstract\rabstract: Large multimodal language models have proven transformative in numerousapplications. However, these models have been shown to memorize and leakpre-training data, raising serious user privacy and information securityconcerns. While data leaks should be prevented, it is also crucial to examinethe trade-off between the privacy protection and model utility of proposedapproaches. In this paper, we introduce PrivQA \u0026ndash; a multimodal benchmark toassess this privacy/utility trade-off when a model is instructed to protectspecific categories of personal information in a simulated scenario. We alsopropose a technique to iteratively self-moderate responses, which significantlyimproves privacy. However, through a series of red-teaming experiments, we findthat adversaries can also easily circumvent these protections with simplejailbreaking methods through textual and/or image inputs. We believe PrivQA hasthe potential to support the development of new models with improved privacyprotections, as well as the adversarial robustness of these protections. Werelease the entire PrivQA dataset at https://llm-access-control.github.io/.\rCan Large Language Models Provide Security \u0026amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions\nYufan Chen, Arjun Arunasalam, Z. Berkay Celik\nabstract\rabstract: Users seek security \u0026amp; privacy (S\u0026amp;P) advice from online resources, includingtrusted websites and content-sharing platforms. These resources help usersunderstand S\u0026amp;P technologies and tools and suggest actionable strategies. LargeLanguage Models (LLMs) have recently emerged as trusted information sources.However, their accuracy and correctness have been called into question. Priorresearch has outlined the shortcomings of LLMs in answering multiple-choicequestions and user ability to inadvertently circumvent model restrictions(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliableS\u0026amp;P advice is not well-explored. In this paper, we measure their ability torefute popular S\u0026amp;P misconceptions that the general public holds. We first studyrecent academic literature to curate a dataset of over a hundred S\u0026amp;P-relatedmisconceptions across six different topics. We then query two popular LLMs(Bard and ChatGPT) and develop a labeling guide to evaluate their responses tothese misconceptions. To comprehensively evaluate their responses, we furtherapply three strategies: query each misconception multiple times, generate andquery their paraphrases, and solicit source URLs of the responses. Both modelsdemonstrate, on average, a 21.3% non-negligible error rate, incorrectlysupporting popular S\u0026amp;P misconceptions. The error rate increases to 32.6% whenwe repeatedly query LLMs with the same or paraphrased misconceptions. We alsoexpose that models may partially support a misconception or remainnoncommittal, refusing a firm stance on misconceptions. Our exploration ofinformation sources for responses revealed that LLMs are susceptible toproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point tounrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).\r2023-10-02\nBTR: Binary Token Representations for Efficient Retrieval Augmented Language Models\nQingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi\nabstract\rabstract: Retrieval augmentation addresses many critical problems in large languagemodels such as hallucination, staleness, and privacy leaks. However, runningretrieval-augmented language models (LMs) is slow and difficult to scale due toprocessing large amounts of retrieved text. We introduce binary tokenrepresentations (BTR), which use 1-bit vectors to precompute every token inpassages, significantly reducing computation during inference. Despite thepotential loss of accuracy, our new calibration techniques and trainingobjectives restore performance. Combined with offline and runtime compression,this only requires 127GB of disk space for encoding 3 billion tokens inWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTRaccelerates state-of-the-art inference by up to 4x and reduces storage by over100x while maintaining over 95% task performance.\rFedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models\nJingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, Holger R. Roth\nabstract\rabstract: Pre-trained language models (PLM) have revolutionized the NLP landscape,achieving stellar performances across diverse tasks. These models, whilebenefiting from vast training data, often require fine-tuning on specific datato cater to distinct downstream tasks. However, this data adaptation processhas inherent security and privacy concerns, primarily when leveraginguser-generated, device-residing data. Federated learning (FL) provides asolution, allowing collaborative model fine-tuning without centralized datacollection. However, applying FL to finetune PLMs is hampered by challenges,including restricted model parameter access, high computational requirements,and communication overheads. This paper introduces Federated Black-box PromptTuning (FedBPT), a framework designed to address these challenges. FedBPT doesnot require the clients to access the model parameters. By focusing on trainingoptimal prompts and utilizing gradient-free optimization methods, FedBPTreduces the number of exchanged variables, boosts communication efficiency, andminimizes computational and storage costs. Experiments highlight theframework\u0026rsquo;s ability to drastically cut communication and memory costs whilemaintaining competitive performance. Ultimately, FedBPT presents a promisingsolution for efficient, privacy-preserving fine-tuning of PLM in the age oflarge language models.\rCoupling public and private gradient provably helps optimization\nRuixuan Liu, Zhiqi Bu, Yu-xiang Wang, Sheng Zha, George Karypis\nabstract\rabstract: The success of large neural networks is crucially determined by theavailability of data. It has been observed that training only on a small amountof public data, or privately on the abundant private data can lead toundesirable degradation of accuracy. In this work, we leverage both private andpublic data to improve the optimization, by coupling their gradients via aweighted linear combination. We formulate an optimal solution for the optimalweight in the convex setting to indicate that the weighting coefficient shouldbe hyperparameter-dependent. Then, we prove the acceleration in the convergenceof non-convex loss and the effects of hyper-parameters such as privacy budget,number of iterations, batch size, and model size on the choice of the weightingcoefficient. We support our analysis with empirical experiments across languageand vision benchmarks, and provide a guideline for choosing the optimal weightof the gradient coupling.\rGotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models\nZhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsum Kim, Donggyun Han, David Lo\nabstract\rabstract: Given large-scale source code datasets available in open-source projects andadvanced large language models, recent code models have been proposed toaddress a series of critical software engineering tasks, such as program repairand code completion. The training data of the code models come from varioussources, not only the publicly available source code, e.g., open-sourceprojects on GitHub but also the private data such as the confidential sourcecode from companies, which may contain sensitive information (for example, SSHkeys and personal information). As a result, the use of these code models mayraise new privacy concerns. In this paper, we focus on a critical yet not well-explored question on usingcode models: what is the risk of membership information leakage in code models?Membership information leakage refers to the risk that an attacker can inferwhether a given data point is included in (i.e., a member of) the trainingdata. To answer this question, we propose Gotcha, a novel membership inferenceattack method specifically for code models. We investigate the membershipleakage risk of code models. Our results reveal a worrying fact that the riskof membership leakage is high: although the previous attack methods are closeto random guessing, Gotcha can predict the data membership with a high truepositive rate of 0.95 and a low false positive rate of 0.10. We also show thatthe attacker\u0026rsquo;s knowledge of the victim model (e.g., the model architecture andthe pre-training data) impacts the success rate of attacks. Further analysisdemonstrates that changing the decoding strategy can mitigate the risk ofmembership leakage. This study calls for more attention to understanding theprivacy of code models and developing more effective countermeasures againstsuch attacks.\r2023-09-30\nPrivacy-Preserving In-Context Learning for Large Language Models\nTong Wu, Ashwinee Panda, Jiachen T. Wang, Prateek Mittal\nabstract\rabstract: In-context learning (ICL) is an important capability of Large Language Models(LLMs), enabling these models to dynamically adapt based on specific,in-context exemplars, thereby improving accuracy and relevance. However, LLM\u0026rsquo;sresponses may leak the sensitive private information contained in in-contextexemplars. To address this challenge, we propose Differentially PrivateIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. Thekey idea for DP-ICL paradigm is generating differentially private responsesthrough a noisy consensus among an ensemble of LLM\u0026rsquo;s responses based ondisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiateseveral techniques showing how to privatize ICL for text classification andlanguage generation. We evaluate DP-ICL on four text classification benchmarksand two language generation tasks, and our empirical results show that DP-ICLachieves a strong utility-privacy tradeoff.\rInvestigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting\nBaphumelele Masikisiki, Vukosi Marivate, Yvette Hlope\nabstract\rabstract: Large Language Models, such as Generative Pre-trained Transformer 3 (aka.GPT-3), have been developed to understand language through the analysis ofextensive text data, allowing them to identify patterns and connections betweenwords. While LLMs have demonstrated impressive performance across varioustext-related tasks, they encounter challenges in tasks associated withreasoning. To address this challenge, Chain of Thought(CoT) prompting methodhas been proposed as a means to enhance LLMs\u0026rsquo; proficiency in complex reasoningtasks like solving math word problems and answering questions based on logicalargumentative reasoning. The primary aim of this research is to assess how wellfour language models can grade reflective essays of third-year medicalstudents. The assessment will specifically target the evaluation of criticalthinking skills using CoT prompting. The research will provide the following contributions; to introduce andeducate on the process of instructing models to evaluate reflective essays froma dataset they have not been previously trained on; to illustrate the use ofCoT prompting as an instructional approach for training large models to carryout particular tasks. Our results suggest that among all the models, Llama-7bperforms the least effectively, displaying the highest mean squared error.Conversely, ChatGPT emerges as the superior model, boasting a higher Cohenkappa score value of 0.53. Lastly, it\u0026rsquo;s important to note that the selectedmodels do prioritise user privacy by allowing users to delete their ownconducted conversations.\r2023-09-29\nRevolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\nSamuel Carreira, Tomás Marques, José Ribeiro, Carlos Grilo\nabstract\rabstract: The field of Artificial Intelligence has witnessed remarkable progress inrecent years, especially with the emergence of powerful large language models(LLMs) based on the transformer architecture. Cloud-based LLMs, such asOpenAI\u0026rsquo;s ChatGPT, offer impressive capabilities but come with concernsregarding latency and privacy due to network dependencies. This articlepresents an innovative approach to LLM inference, envisioning a future whereLLMs with billions of parameters can be executed directly on mobile deviceswithout network connectivity. The article showcases a fine-tuned GPT LLM with 3billion parameters that can operate smoothly on devices with as low as 4GB ofmemory. Through the integration of native code and model quantizationtechniques, the application not only serves as a general-purpose assistant butalso facilitates seamless mobile interactions with text-to-actions features.The article provides insights into the training pipeline, implementationdetails, test results, and future directions of on-device LLM inference. Thisbreakthrough technology opens up possibilities for empowering users withsophisticated AI capabilities while preserving their privacy and eliminatinglatency concerns.\r2023-09-26\nPLMM: Personal Large Models on Mobile Devices\nYuanhao Gong\nabstract\rabstract: Inspired by Federated Learning, in this paper, we propose personal largemodels that are distilled from traditional large language models but moreadaptive to local users\u0026rsquo; personal information such as education background andhobbies. We classify the large language models into three levels: the personallevel, expert level and traditional level. The personal level models areadaptive to users\u0026rsquo; personal information. They encrypt the users\u0026rsquo; input andprotect their privacy. The expert level models focus on merging specificknowledge such as finance, IT and art. The traditional models focus on theuniversal knowledge discovery and upgrading the expert models. In suchclassifications, the personal models directly interact with the user. For thewhole system, the personal models have users\u0026rsquo; (encrypted) personal information.Moreover, such models must be small enough to be performed on personalcomputers or mobile devices. Finally, they also have to response in real-timefor better user experience and produce high quality results. The proposedpersonal large models can be applied in a wide range of applications such aslanguage and vision tasks.\r2023-09-25\nAn Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox\nChaoran Chen, Weijun Li, Wenxin Song, Yanfang Ye, Yaxing Yao, Toby Jia-jun Li\nabstract\rabstract: The \u0026ldquo;privacy paradox\u0026rdquo; describes the discrepancy between users\u0026rsquo; privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users\u0026rsquo; hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes.\r2023-09-22\nRight to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions\nDawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu\nabstract\rabstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.\r2023-09-20\n\u0026ldquo;It\u0026rsquo;s a Fair Game\u0026rsquo;\u0026rsquo;, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents\nZhiping Zhang, Michelle Jia, Hao-Ping, Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li\nabstract\rabstract: The widespread use of Large Language Model (LLM)-based conversational agents(CAs), especially in high-stakes domains, raises many privacy concerns.Building ethical LLM-based CAs that respect user privacy requires an in-depthunderstanding of the privacy risks that concern users the most. However,existing research, primarily model-centered, does not provide insight intousers\u0026rsquo; perspectives. To bridge this gap, we analyzed sensitive disclosures inreal-world ChatGPT conversations and conducted semi-structured interviews with19 LLM-based CA users. We found that users are constantly faced with trade-offsbetween privacy, utility, and convenience when using LLM-based CAs. However,users\u0026rsquo; erroneous mental models and the dark patterns in system design limitedtheir awareness and comprehension of the privacy risks. Additionally, thehuman-like interactions encouraged more sensitive disclosures, whichcomplicated users\u0026rsquo; ability to navigate the trade-offs. We discuss practicaldesign guidelines and the needs for paradigmatic shifts to protect the privacyof LLM-based CA users.\r2023-09-19\nPolicyGPT: Automated Analysis of Privacy Policies with Large Language Models\nChenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan\nabstract\rabstract: Privacy policies serve as the primary conduit through which online serviceproviders inform users about their data collection and usage procedures.However, in a bid to be comprehensive and mitigate legal risks, these policydocuments are often quite verbose. In practical use, users tend to click theAgree button directly rather than reading them carefully. This practice exposesusers to risks of privacy leakage and legal issues. Recently, the advent ofLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened newpossibilities for text analysis, especially for lengthy documents like privacypolicies. In this study, we investigate a privacy policy text analysisframework PolicyGPT based on the LLM. This framework was tested using twodatasets. The first dataset comprises of privacy policies from 115 websites,which were meticulously annotated by legal experts, categorizing each segmentinto one of 10 classes. The second dataset consists of privacy policies from304 popular mobile applications, with each sentence manually annotated andclassified into one of another 10 categories. Under zero-shot learningconditions, PolicyGPT demonstrated robust performance. For the first dataset,it achieved an accuracy rate of 97%, while for the second dataset, it attainedan 87% accuracy rate, surpassing that of the baseline machine learning andneural network models.\rSpecializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training\nRuiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang\nabstract\rabstract: In this work, we introduce the concept of complex text style transfer tasks,and constructed complex text datasets based on two widely applicable scenarios.Our dataset is the first large-scale data set of its kind, with 700 rephrasedsentences and 1,000 sentences from the game Genshin Impact. While largelanguage models (LLM) have shown promise in complex text style transfer, theyhave drawbacks such as data privacy concerns, network instability, and highdeployment costs. To address these issues, we explore the effectiveness ofsmall models (less than T5-3B) with implicit style pre-training throughcontrastive learning. We also propose a method for automated evaluation of textgeneration quality based on alignment with human evaluations using ChatGPT.Finally, we compare our approach with existing methods and show that our modelachieves state-of-art performances of few-shot text style transfer models.\rLLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI\u0026rsquo;s ChatGPT Plugins\nUmar Iqbal, Tadayoshi Kohno, Franziska Roesner\nabstract\rabstract: Large language model (LLM) platforms, such as ChatGPT, have recently begunoffering a plugin ecosystem to interface with third-party services on theinternet. While these plugins extend the capabilities of LLM platforms, theyare developed by arbitrary third parties and thus cannot be implicitly trusted.Plugins also interface with LLM platforms and users using natural language,which can have imprecise interpretations. In this paper, we propose a frameworkthat lays a foundation for LLM platform designers to analyze and improve thesecurity, privacy, and safety of current and future plugin-integrated LLMplatforms. Our framework is a formulation of an attack taxonomy that isdeveloped by iteratively exploring how LLM platform stakeholders could leveragetheir capabilities and responsibilities to mount attacks against each other. Aspart of our iterative process, we apply our framework in the context ofOpenAI\u0026rsquo;s plugin ecosystem. We uncover plugins that concretely demonstrate thepotential for the types of issues that we outline in our attack taxonomy. Weconclude by discussing novel challenges and by providing recommendations toimprove the security, privacy, and safety of present and future LLM-basedcomputing platforms.\rDifferentially Private Optimization on Large Model at Small Cost\nZhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis\nabstract\rabstract: Differentially private (DP) optimization is the standard paradigm to learnlarge neural networks that are accurate and privacy-preserving. Thecomputational cost for DP deep learning, however, is notoriously heavy due tothe per-sample gradient clipping. Existing DP implementations are 2-1000X morecostly in time and space complexity than the standard (non-private) training.In this work, we develop a novel Book-Keeping (BK) technique that implementsexisting DP optimizers (thus achieving the same accuracy), with a substantialimprovement on the computational cost. Specifically, BK enables DP training onlarge models and high dimensional data to be roughly as fast and memory-savingas the standard training, whereas previous DP algorithms can be inefficient orincapable of training due to memory error. The computational advantage of BK issupported by the complexity analysis as well as extensive experiments on visionand language tasks. Our implementation achieves state-of-the-art (SOTA)accuracy with very small extra cost: on GPT2 and at almost the same memory cost(\u0026lt;1% overhead), BK has 1.03X the time complexity of the standard training(0.83X training speed in practice), and 0.61X the time complexity of the mostefficient DP implementation (1.36X training speed in practice). We open-sourcethe codebase for the BK algorithm at the FastDP library(https://github.com/awslabs/fast-differential-privacy).\rDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass\nMinxin Du, Xiang Yue, Sherman S. M. Chow, Tianhao Wang, Chenyu Huang, Huan Sun\nabstract\rabstract: Differentially private stochastic gradient descent (DP-SGD) adds noise togradients in back-propagation, safeguarding training data from privacy leakage,particularly membership inference. It fails to cover (inference-time) threatslike embedding inversion and sensitive attribute inference. It is also costlyin storage and computation when used to fine-tune large pre-trained languagemodels (LMs). We propose DP-Forward, which directly perturbs embedding matrices in theforward pass of LMs. It satisfies stringent local DP requirements for trainingand inference data. To instantiate it using the smallest matrix-valued noise,we devise an analytic matrix Gaussian~mechanism (aMGM) by drawing possiblynon-i.i.d. noise from a matrix Gaussian distribution. We then investigateperturbing outputs from different hidden (sub-)layers of LMs with aMGM noises.Its utility on three typical tasks almost hits the non-private baseline andoutperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves3$\\times$ time and memory costs compared to DP-SGD with the latest high-speedlibrary. It also reduces the average success rates of embedding inversion andsensitive attribute inference by up to 88pp and 41pp, respectively, whereasDP-SGD fails.\r2023-09-18\nInstruction-Following Speech Recognition\nCheng-I Jeff Lai, Zhiyun Lu, Liangliang Cao, Ruoming Pang\nabstract\rabstract: Conventional end-to-end Automatic Speech Recognition (ASR) models primarilyfocus on exact transcription tasks, lacking flexibility for nuanced userinteractions. With the advent of Large Language Models (LLMs) in speechprocessing, more organic, text-prompt-based interactions have become possible.However, the mechanisms behind these models\u0026rsquo; speech understanding and\u0026quot;reasoning\u0026quot; capabilities remain underexplored. To study this question from thedata perspective, we introduce instruction-following speech recognition,training a Listen-Attend-Spell model to understand and execute a diverse set offree-form text instructions. This enables a multitude of speech recognitiontasks \u0026ndash; ranging from transcript manipulation to summarization \u0026ndash; withoutrelying on predefined command sets. Remarkably, our model, trained from scratchon Librispeech, interprets and executes simple instructions without requiringLLMs or pre-trained speech modules. It also offers selective transcriptionoptions based on instructions like \u0026ldquo;transcribe first half and then turn offlistening,\u0026rdquo; providing an additional layer of privacy and safety compared toexisting LLMs. Our findings highlight the significant potential ofinstruction-following training to advance speech foundation models.\r2023-09-17\nYour Room is not Private: Gradient Inversion Attack on Reinforcement Learning\nMiao Li, Wenhao Ding, Ding Zhao\nabstract\rabstract: The prominence of embodied Artificial Intelligence (AI), which empowersrobots to navigate, perceive, and engage within virtual environments, hasattracted significant attention, owing to the remarkable advancements incomputer vision and large language models. Privacy emerges as a pivotal concernwithin the realm of embodied AI, as the robot accesses substantial personalinformation. However, the issue of privacy leakage in embodied AI tasks,particularly in relation to reinforcement learning algorithms, has not receivedadequate consideration in research. This paper aims to address this gap byproposing an attack on the value-based algorithm and the gradient-basedalgorithm, utilizing gradient inversion to reconstruct states, actions, andsupervision signals. The choice of using gradients for the attack is motivatedby the fact that commonly employed federated learning techniques solely utilizegradients computed based on private user data to optimize models, withoutstoring or transmitting the data to public servers. Nevertheless, thesegradients contain sufficient information to potentially expose private data. Tovalidate our approach, we conduct experiments on the AI2THOR simulator andevaluate our algorithm on active perception, a prevalent task in embodied AI.The experimental results demonstrate the effectiveness of our method insuccessfully reconstructing all information from the data across 120 roomlayouts.\r2023-09-13\neDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models\nMinsik Cho, Keivan A. Vahid, Qichen Fu, Saurabh Adya, Carlo C Del Mundo, Mohammad Rastegari, Devang Naik, Peter Zatloukal\nabstract\rabstract: Since Large Language Models or LLMs have demonstrated high-qualityperformance on many complex language tasks, there is a great interest inbringing these LLMs to mobile devices for faster responses and better privacyprotection. However, the size of LLMs (i.e., billions of parameters) requireshighly effective compression to fit into storage-limited devices. Among manycompression techniques, weight-clustering, a form of non-linear quantization,is one of the leading candidates for LLM compression, and supported by modernsmartphones. Yet, its training overhead is prohibitively significant for LLMfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shownthe state-of-the-art trade-off between compression ratio and accuracyregression, but its large memory complexity makes it nearly impossible to applyto train-time LLM compression. In this paper, we propose a memory-efficient DKMimplementation, eDKM powered by novel techniques to reduce the memory footprintof DKM by orders of magnitudes. For a given tensor to be saved on CPU for thebackward pass of DKM, we compressed the tensor by applying uniquification andsharding after checking if there is no duplicated tensor previously copied toCPU. Our experimental results demonstrate that \\prjname can fine-tune andcompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) withthe Alpaca dataset by reducing the train-time memory footprint of a decoderlayer by 130$\\times$, while delivering good accuracy on broader LLM benchmarks(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).\r2023-09-08\nLLMCad: Fast and Scalable On-device Large Language Model Inference\nDaliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, Xuanzhe Liu\nabstract\rabstract: Generative tasks, such as text generation and question answering, hold acrucial position in the realm of mobile applications. Due to their sensitivityto privacy concerns, there is a growing demand for their execution directly onmobile devices. Currently, the execution of these generative tasks heavilydepends on Large Language Models (LLMs). Nevertheless, the limited memorycapacity of these devices presents a formidable challenge to the scalability ofsuch models. In our research, we introduce LLMCad, an innovative on-device inferenceengine specifically designed for efficient generative Natural LanguageProcessing (NLP) tasks. The core idea behind LLMCad revolves around modelcollaboration: a compact LLM, residing in memory, takes charge of generatingthe most straightforward tokens, while a high-precision LLM steps in tovalidate these tokens and rectify any identified errors. LLMCad incorporatesthree novel techniques: (1) Instead of generating candidate tokens in asequential manner, LLMCad employs the smaller LLM to construct a token tree,encompassing a wider range of plausible token pathways. Subsequently, thelarger LLM can efficiently validate all of these pathways simultaneously. (2)It employs a self-adjusting fallback strategy, swiftly initiating theverification process whenever the smaller LLM generates an erroneous token. (3)To ensure a continuous flow of token generation, LLMCad speculatively generatestokens during the verification process by implementing a compute-IO pipeline.Through an extensive series of experiments, LLMCad showcases an impressivetoken generation speed, achieving rates up to 9.3x faster than existinginference engines.\r2023-09-07\nEnhancing Pipeline-Based Conversational Agents with Large Language Models\nMina Foosherian, Hendrik Purwins, Purna Rathnayake, Touhidul Alam, Rui Teimao, Klaus-Dieter Thoben\nabstract\rabstract: The latest advancements in AI and deep learning have led to a breakthrough inlarge language model (LLM)-based agents such as GPT-4. However, many commercialconversational agent development tools are pipeline-based and have limitationsin holding a human-like conversation. This paper investigates the capabilitiesof LLMs to enhance pipeline-based conversational agents during two phases: 1)in the design and development phase and 2) during operations. In 1) LLMs canaid in generating training data, extracting entities and synonyms,localization, and persona design. In 2) LLMs can assist in contextualization,intent classification to prevent conversational breakdown and handleout-of-scope questions, auto-correcting utterances, rephrasing responses,formulating disambiguation questions, summarization, and enabling closedquestion-answering capabilities. We conducted informal experiments with GPT-4in the private banking domain to demonstrate the scenarios above with apractical example. Companies may be hesitant to replace their pipeline-basedagents with LLMs entirely due to privacy concerns and the need for deepintegration within their existing ecosystems. A hybrid approach in which LLMs\u0026rsquo;are integrated into the pipeline-based agents allows them to save time andcosts of building and running agents by capitalizing on the capabilities ofLLMs while retaining the integration and privacy safeguards of their existingsystems.\r2023-09-06\nPublicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes\nSunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi\nabstract\rabstract: The development of large language models tailored for handling patients\u0026rsquo;clinical notes is often hindered by the limited accessibility and usability ofthese notes due to strict privacy regulations. To address these challenges, wefirst create synthetic large-scale clinical notes using publicly available casereports extracted from biomedical literature. We then use these synthetic notesto train our specialized clinical large language model, Asclepius. WhileAsclepius is trained on synthetic data, we assess its potential performance inreal-world applications by evaluating it using real clinical notes. Webenchmark Asclepius against several other large language models, includingGPT-3.5-turbo and other open-source alternatives. To further validate ourapproach using synthetic notes, we also compare Asclepius with its variantstrained on real clinical notes. Our findings convincingly demonstrate thatsynthetic clinical notes can serve as viable substitutes for real ones whenconstructing high-performing clinical language models. This conclusion issupported by detailed evaluations conducted by both GPT-4 and medicalprofessionals. All resources including weights, codes, and data used in thedevelopment of Asclepius are made publicly accessible for future research.\rHide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection\nYu Chen, Tingxin Li, Huiming Liu, Yang Yu\nabstract\rabstract: Numerous companies have started offering services based on large languagemodels (LLM), such as ChatGPT, which inevitably raises privacy concerns asusers\u0026rsquo; prompts are exposed to the model provider. Previous research on securereasoning using multi-party computation (MPC) has proven to be impractical forLLM applications due to its time-consuming and communication-intensive nature.While lightweight anonymization techniques can protect private information inprompts through substitution or masking, they fail to recover sensitive datareplaced in the LLM-generated results. In this paper, we expand the applicationscenarios of anonymization techniques by training a small local model tode-anonymize the LLM\u0026rsquo;s returned results with minimal computational overhead. Weintroduce the HaS framework, where \u0026ldquo;H(ide)\u0026rdquo; and \u0026ldquo;S(eek)\u0026rdquo; represent its two coreprocesses: hiding private entities for anonymization and seeking privateentities for de-anonymization, respectively. To quantitatively assess HaS\u0026rsquo;sprivacy protection performance, we propose both black-box and white-boxadversarial models. Furthermore, we conduct experiments to evaluate HaS\u0026rsquo;susability in translation and classification tasks. The experimental findingsdemonstrate that the HaS framework achieves an optimal balance between privacyprotection and utility.\rAutomated Bioinformatics Analysis via AutoBA\nJuexiao Zhou, Bin Zhang, Xiuying Chen, Haoyang Li, Xiaopeng Xu, Siyuan Chen, Xin Gao\nabstract\rabstract: With the fast-growing and evolving omics data, the demand for streamlined andadaptable tools to handle the analysis continues to grow. In response to thisneed, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AIagent based on a large language model designed explicitly for conventionalomics data analysis. AutoBA simplifies the analytical process by requiringminimal user input while delivering detailed step-by-step plans for variousbioinformatics tasks. Through rigorous validation by expert bioinformaticians,AutoBA\u0026rsquo;s robustness and adaptability are affirmed across a diverse range ofomics analysis cases, including whole genome sequencing (WGS), RNA sequencing(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA\u0026rsquo;sunique capacity to self-design analysis processes based on input datavariations further underscores its versatility. Compared with onlinebioinformatic services, AutoBA deploys the analysis locally, preserving dataprivacy. Moreover, different from the predefined pipeline, AutoBA hasadaptability in sync with emerging bioinformatics tools. Overall, AutoBArepresents a convenient tool, offering robustness and adaptability for complexomics data analysis.\rAI for Investment: A Platform Disruption\nMohammad Rasouli, Ravi Chiruvolu, Ali Risheh\nabstract\rabstract: With the investment landscape becoming more competitive, efficiently scalingdeal sourcing and improving deal insights have become a dominant strategy forfunds. While funds are already spending significant efforts on these two tasks,they cannot be scaled with traditional approaches; hence, there is a surge inautomating them. Many third party software providers have emerged recently toaddress this need with productivity solutions, but they fail due to a lack ofpersonalization for the fund, privacy constraints, and natural limits ofsoftware use cases. Therefore, most major funds and many smaller funds havestarted developing their in-house AI platforms: a game changer for theindustry. These platforms grow smarter by direct interactions with the fund andcan be used to provide personalized use cases. Recent developments in largelanguage models, e.g. ChatGPT, have provided an opportunity for other funds toalso develop their own AI platforms. While not having an AI platform now is nota competitive disadvantage, it will be in two years. Funds require a practicalplan and corresponding risk assessments for such AI platforms.\r2023-09-03\nFusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs\nZhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu\nabstract\rabstract: The rapid growth of memory and computation requirements of large languagemodels (LLMs) has outpaced the development of hardware, hindering people wholack large-scale high-end GPUs from training or deploying LLMs. However,consumer-level GPUs, which constitute a larger market share, are typicallyoverlooked in LLM due to their weaker computing performance, smaller storagecapacity, and lower communication bandwidth. Additionally, users may haveprivacy concerns when interacting with remote LLMs. In this paper, we envisiona decentralized system unlocking the potential vast untapped consumer-levelGPUs in pre-training, inference and fine-tuning of LLMs with privacyprotection. However, this system faces critical challenges, including limitedCPU and GPU memory, low network bandwidth, the variability of peer and deviceheterogeneity. To address these challenges, our system design incorporates: 1)a broker with backup pool to implement dynamic join and quit of computingproviders; 2) task scheduling with hardware performance to improve systemefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) toachieve model and task universality; 4) abstracting intermediate representionand execution planes to ensure compatibility of various devices and deeplearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which aresignificantly more expensive.\r2023-09-02\nCombing for Credentials: Active Pattern Extraction from Smart Reply\nBargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, David Evans\nabstract\rabstract: Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply\u0026rsquo;\u0026rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.\r2023-09-01\nDesigning a realistic peer-like embodied conversational agent for supporting children\u0026rsquo;s storytelling\nZhixin Li, Ying Xu\nabstract\rabstract: Advances in artificial intelligence have facilitated the use of largelanguage models (LLMs) and AI-generated synthetic media in education, which mayinspire HCI researchers to develop technologies, in particular, embodiedconversational agents (ECAs) to simulate the kind of scaffolding children mightreceive from a human partner. In this paper, we will propose a design prototypeof a peer-like ECA named STARie that integrates multiple AI models - GPT-3,Speech Synthesis (Real-time Voice Cloning), VOCA (Voice Operated CharacterAnimation), and FLAME (Faces Learned with an Articulated Model and Expressions)that aims to support narrative production in collaborative storytelling,specifically for children aged 4-8. However, designing a child-centered ECAraises concerns about age appropriateness, children privacy, gender choices ofECAs, and the uncanny valley effect. Thus, this paper will also discussconsiderations and ethical concerns that must be taken into account whendesigning such an ECA. This proposal offers insights into the potential use ofAI-generated synthetic media in child-centered AI design and how peer-like AIembodiment may support children\\textquotesingle s storytelling.\r2023-08-30\nIntroducing Language Guidance in Prompt-based Continual Learning\nMuhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier Stricker, Federico Tombari, Muhammad Zeshan Afzal\nabstract\rabstract: Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters.\r2023-08-29\nCEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction\nUmar Khalid, Hasan Iqbal, Saeed Vahidian, Jing Hua, Chen Chen\nabstract\rabstract: Human-robot interaction (HRI) is a rapidly growing field that encompassessocial and industrial applications. Machine learning plays a vital role inindustrial HRI by enhancing the adaptability and autonomy of robots in complexenvironments. However, data privacy is a crucial concern in the interactionbetween humans and robots, as companies need to protect sensitive data whilemachine learning algorithms require access to large datasets. FederatedLearning (FL) offers a solution by enabling the distributed training of modelswithout sharing raw data. Despite extensive research on Federated learning (FL)for tasks such as natural language processing (NLP) and image classification,the question of how to use FL for HRI remains an open research problem. Thetraditional FL approach involves transmitting large neural network parametermatrices between the server and clients, which can lead to high communicationcosts and often becomes a bottleneck in FL. This paper proposes acommunication-efficient FL framework for human-robot interaction (CEFHRI) toaddress the challenges of data heterogeneity and communication costs. Theframework leverages pre-trained models and introduces a trainablespatiotemporal adapter for video understanding tasks in HRI. Experimentalresults on three human-robot interaction benchmark datasets: HRI30, InHARD, andCOIN demonstrate the superiority of CEFHRI over full fine-tuning in terms ofcommunication costs. The proposed methodology provides a secure and efficientapproach to HRI federated learning, particularly in industrial environmentswith data privacy concerns and limited communication bandwidth. Our code isavailable athttps://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.\rTransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification\nJianing Wang, Chengyu Wang, Cen Chen, Ming Gao, Jun Huang, Aoying Zhou\nabstract\rabstract: Text classification is one of the most imperative tasks in natural languageprocessing (NLP). Recent advances with pre-trained language models (PLMs) haveshown remarkable success on this task. However, the satisfying results obtainedby PLMs heavily depend on the large amounts of task-specific labeled data,which may not be feasible in many application scenarios due to data access andprivacy constraints. The recently-proposed prompt-based fine-tuning paradigmimproves the performance of PLMs for few-shot text classification withtask-specific templates. Yet, it is unclear how the prompting knowledge can betransferred across tasks, for the purpose of mutual reinforcement. We proposeTransPrompt v2, a novel transferable prompting framework for few-shot learningacross similar or distant text classification tasks. For learning acrosssimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)procedure to train a meta-learner that captures the cross-task transferableknowledge. For learning across distant tasks, we further inject the task typedescriptions into the prompt, and capture the intra-type and inter-type promptembeddings among multiple distant tasks. Additionally, two de-biasingtechniques are further designed to make the trained meta-learner moretask-agnostic and unbiased towards any tasks. After that, the meta-learner canbe adapted to each specific task with better parameters initialization.Extensive experiments show that TransPrompt v2 outperforms single-task andcross-task strong baselines over multiple NLP tasks and datasets. We furthershow that the meta-learner can effectively improve the performance of PLMs onpreviously unseen tasks. In addition, TransPrompt v2 also outperforms strongfine-tuning baselines when learning with full training sets.\r2023-08-28\nEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models\nRongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei Xu\nabstract\rabstract: Large Language Models (LLMs) such as GPTs and LLaMa have ushered in arevolution in machine intelligence, owing to their exceptional capabilities ina wide range of machine learning tasks. However, the transition of LLMs fromdata centers to edge devices presents a set of challenges and opportunities.While this shift can enhance privacy and availability, it is hampered by theenormous parameter sizes of these models, leading to impractical runtime costs.In light of these considerations, we introduce EdgeMoE, the first on-deviceinference engine tailored for mixture-of-expert (MoE) LLMs, a popular variantof sparse LLMs that exhibit nearly constant computational complexity as theirparameter size scales. EdgeMoE achieves both memory and computationalefficiency by strategically partitioning the model across the storagehierarchy. Specifically, non-expert weights are stored in the device\u0026rsquo;s memory,while expert weights are kept in external storage and are fetched into memoryonly when they are activated. This design is underpinned by a crucial insightthat expert weights, though voluminous, are infrequently accessed due to sparseactivation patterns. To further mitigate the overhead associated with expertI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wisebitwidth adaptation: This method reduces the size of expert weights with anacceptable level of accuracy loss. (2) Expert management: It predicts theexperts that will be activated in advance and preloads them into thecompute-I/O pipeline, thus further optimizing the process. In empiricalevaluations conducted on well-established MoE LLMs and various edge devices,EdgeMoE demonstrates substantial memory savings and performance improvementswhen compared to competitive baseline solutions.\r2023-08-22\nTowards an On-device Agent for Text Rewriting\nYun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu-hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, Lei Meng\nabstract\rabstract: Large Language Models (LLMs) have demonstrated impressive capabilities fortext rewriting. Nonetheless, the large sizes of these models make themimpractical for on-device inference, which would otherwise allow for enhancedprivacy and economical inference. Creating a smaller yet potent language modelfor text rewriting presents a formidable challenge because it requiresbalancing the need for a small size with the need to retain the emergentcapabilities of the LLM, that requires costly data collection. To address theabove challenge, we introduce a new instruction tuning approach for building amobile-centric text rewriting model. Our strategies enable the generation ofhigh quality training data without any human labeling. In addition, we proposea heuristic reinforcement learning framework which substantially enhancesperformance without requiring preference data. To further bridge theperformance gap with the larger server-side model, we propose an effectiveapproach that combines the mobile rewrite agent with the server model using acascade. To tailor the text rewriting tasks to mobile scenarios, we introduceMessageRewriteEval, a benchmark that focuses on text rewriting for messagesthrough natural language instructions. Through empirical experiments, wedemonstrate that our on-device model surpasses the current state-of-the-artLLMs in text rewriting while maintaining a significantly reduced model size.Notably, we show that our proposed cascading approach improves modelperformance.\r2023-08-21\nFederated learning for secure development of AI models for Parkinson\u0026rsquo;s disease detection using speech from different languages\nSoroosh Tayebi Arasteh, Cristian David Rios-Urrego, Elmar Noeth, Andreas Maier, Seung Hee Yang, Jan Rusz, Juan Rafael Orozco-Arroyave\nabstract\rabstract: Parkinson\u0026rsquo;s disease (PD) is a neurological disorder impacting a person\u0026rsquo;sspeech. Among automatic PD assessment methods, deep learning models have gainedparticular interest. Recently, the community has explored cross-pathology andcross-language models which can improve diagnostic accuracy even further.However, strict patient data privacy regulations largely prevent institutionsfrom sharing patient speech data with each other. In this paper, we employfederated learning (FL) for PD detection using speech signals from 3 real-worldlanguage corpora of German, Spanish, and Czech, each from a separateinstitution. Our results indicate that the FL model outperforms all the localmodels in terms of diagnostic accuracy, while not performing very differentlyfrom the model based on centrally combined training sets, with the advantage ofnot requiring any data sharing among collaborators. This will simplifyinter-institutional collaborations, resulting in enhancement of patientoutcomes.\r2023-08-18\nLeveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks\nAbubakar S. Ali, Dimitrios Michael Manias, Abdallah Shami, Sami Muhaidat\nabstract\rabstract: As the dawn of sixth-generation (6G) networking approaches, it promisesunprecedented advancements in communication and automation. Among the leadinginnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming toachieve fully automated, self-optimizing networks with minimal humanintervention. Despite the advantages ZTNs offer in terms of efficiency andscalability, challenges surrounding transparency, adaptability, and human trustremain prevalent. Concurrently, the advent of Large Language Models (LLMs)presents an opportunity to elevate the ZTN framework by bridging the gapbetween automated processes and human-centric interfaces. This paper exploresthe integration of LLMs into ZTNs, highlighting their potential to enhancenetwork transparency and improve user interactions. Through a comprehensivecase study on deep reinforcement learning (DRL)-based anti-jamming technique,we demonstrate how LLMs can distill intricate network operations intointuitive, human-readable reports. Additionally, we address the technical andethical intricacies of melding LLMs with ZTNs, with an emphasis on dataprivacy, transparency, and bias reduction. Looking ahead, we identify emergingresearch avenues at the nexus of LLMs and ZTNs, advocating for sustainedinnovation and interdisciplinary synergy in the domain of automated networks.\r2023-08-17\nDifferential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models\nPhillip Rust, Anders Søgaard\nabstract\rabstract: Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingualgeneralization or compression to facilitate transfer to a large number of(potentially unseen) languages. However, these models should ideally also beprivate, linguistically fair, and transparent, by relating their predictions totraining data. Can these requirements be simultaneously satisfied? We show thatmultilingual compression and linguistic fairness are compatible withdifferential privacy, but that differential privacy is at odds with trainingdata influence sparsity, an objective for transparency. We further present aseries of experiments on two common NLP tasks and evaluate multilingualcompression and training data influence sparsity under different privacyguarantees, exploring these trade-offs in more detail. Our results suggest thatwe need to develop ways to jointly optimize for these objectives in order tofind practical trade-offs.\r2023-08-15\nRobustness Over Time: Understanding Adversarial Examples\u0026rsquo; Effectiveness on Longitudinal Versions of Large Language Models\nYugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang\nabstract\rabstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.\r2023-08-14\nTowards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark\nShuyu Yang, Yinan Zhou, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng\nabstract\rabstract: In this paper, we introduce a large Multi-Attribute and Language Searchdataset for text-based person retrieval, called MALS, and explore thefeasibility of performing pre-training on both attribute recognition andimage-text matching tasks in one stone. In particular, MALS contains 1,510,330image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,and all images are annotated with 27 attributes. Considering the privacyconcerns and annotation costs, we leverage the off-the-shelf diffusion modelsto generate the dataset. To verify the feasibility of learning from thegenerated data, we develop a new joint Attribute Prompt Learning and TextMatching Learning (APTM) framework, considering the shared knowledge betweenattribute and text. As the name implies, APTM contains an attribute promptlearning stream and a text matching learning stream. (1) The attribute promptlearning leverages the attribute prompts for image-attribute alignment, whichenhances the text matching learning. (2) The text matching learning facilitatesthe representation learning on fine-grained details, and in turn, boosts theattribute prompt learning. Extensive experiments validate the effectiveness ofthe pre-training on MALS, achieving state-of-the-art retrieval performance viaAPTM on three challenging real-world benchmarks. In particular, APTM achieves aconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy onCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.\r2023-08-13\nFree-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks\nDavid Junhao Zhang, Mutian Xu, Chuhui Xue, Wenqing Zhang, Xiaoguang Han, Song Bai, Mike Zheng Shou\nabstract\rabstract: Despite the rapid advancement of unsupervised learning in visualrepresentation, it requires training on large-scale datasets that demand costlydata collection, and pose additional challenges due to concerns regarding dataprivacy. Recently, synthetic images generated by text-to-image diffusionmodels, have shown great potential for benefiting image recognition. Althoughpromising, there has been inadequate exploration dedicated to unsupervisedlearning on diffusion-generated images. To address this, we start by uncoveringthat diffusion models\u0026rsquo; cross-attention layers inherently provideannotation-free attention masks aligned with corresponding text inputs ongenerated images. We then investigate the problems of three prevalentunsupervised learning techniques ( i.e., contrastive learning, masked modeling,and vision-language pretraining) and introduce customized solutions by fullyexploiting the aforementioned free attention masks. Our approach is validatedthrough extensive experiments that show consistent improvements in baselinemodels across various downstream tasks, including image classification,detection, segmentation, and image-text retrieval. By utilizing our method, itis possible to close the performance gap between unsupervised pretraining onsynthetic data and real-world scenarios.\r2023-08-11\nEnhancing Network Management Using Code Generated by Large Language Models\nSathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Ranveer Chandra, Srikanth Kandula\nabstract\rabstract: Analyzing network topologies and communication graphs plays a crucial role incontemporary network management. However, the absence of a cohesive approachleads to a challenging learning curve, heightened errors, and inefficiencies.In this paper, we introduce a novel approach to facilitate anatural-language-based network management experience, utilizing large languagemodels (LLMs) to generate task-specific code from natural language queries.This method tackles the challenges of explainability, scalability, and privacyby allowing network operators to inspect the generated code, eliminating theneed to share network data with LLMs, and concentrating on application-specificrequests combined with general program synthesis techniques. We design andevaluate a prototype system using benchmark applications, showcasing highaccuracy, cost-effectiveness, and the potential for further enhancements usingcomplementary program synthesis techniques.\r2023-08-09\nLLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following\nKaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, Qing Li\nabstract\rabstract: E-commerce authoring involves creating attractive, abundant, and targetedpromotional content to drive product sales. The emergence of large languagemodels (LLMs) introduces an innovative paradigm, offering a unified solution toaddress various authoring tasks within this scenario. However, mainstream LLMstrained on general corpora with common sense knowledge reveal limitations infitting complex and personalized features unique to e-commerce products andcustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,raising concerns about safeguarding voluminous customer privacy data duringtransmission. This paper proposes the LLaMA-E, the unified and customizedinstruction-following language models focusing on diverse e-commerce authoringtasks. Specifically, the domain experts create the seed instruction set fromthe tasks of ads generation, query-enhanced product title rewriting, productclassification, purchase intent speculation, and general Q\u0026amp;A. These tasksenable the models to comprehensively understand precise e-commerce authoringknowledge by interleaving features covering typical service aspects ofcustomers, sellers, and platforms. The GPT-3.5 is introduced as a teachermodel, which expands the seed instructions to form a training set for theLLaMA-E models with various scales. The experimental results show that theproposed LLaMA-E models achieve state-of-the-art results in quantitative andqualitative evaluations, also exhibiting the advantage in zero-shot scenes. Tothe best of our knowledge, this study is the first to serve the LLMs tospecific e-commerce authoring scenarios.\r2023-08-08\nSimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\nYouyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka, Osamu Torii, Tomoya Kodama, Jun Deguchi\nabstract\rabstract: Large Language Model (LLM) based Generative AI systems have seen significantprogress in recent years. Integrating a knowledge retrieval architecture allowsfor seamless integration of private data into publicly available Generative AIsystems using pre-trained LLM without requiring additional model fine-tuning.Moreover, Retrieval-Centric Generation (RCG) approach, a promising futureresearch direction that explicitly separates roles of LLMs and retrievers incontext interpretation and knowledge memorization, potentially leads to moreefficient implementation. SimplyRetrieve is an open-source tool with the goalof providing a localized, lightweight, and user-friendly interface to thesesophisticated advancements to the machine learning community. SimplyRetrievefeatures a GUI and API based RCG platform, assisted by a Private Knowledge BaseConstructor and a Retrieval Tuning Module. By leveraging these capabilities,users can explore the potential of RCG for improving generative AI performancewhile maintaining privacy standards. The tool is available athttps://github.com/RCGAI/SimplyRetrieve with an MIT license.\r2023-08-07\nTableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao\nabstract\rabstract: Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework\u0026rsquo;s adaptability tospecific use cases.\r2023-08-05\nLarge Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching\nJiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, Xia Hu\nabstract\rabstract: The process of matching patients with suitable clinical trials is essentialfor advancing medical research and providing optimal care. However, currentapproaches face challenges such as data standardization, ethicalconsiderations, and a lack of interoperability between Electronic HealthRecords (EHRs) and clinical trial criteria. In this paper, we explore thepotential of large language models (LLMs) to address these challenges byleveraging their advanced natural language generation capabilities to improvecompatibility between EHRs and clinical trial descriptions. We propose aninnovative privacy-aware data augmentation approach for LLM-based patient-trialmatching (LLM-PTM), which balances the benefits of LLMs while ensuring thesecurity and confidentiality of sensitive patient data. Our experimentsdemonstrate a 7.32% average improvement in performance using the proposedLLM-PTM method, and the generalizability to new data is improved by 12.12%.Additionally, we present case studies to further illustrate the effectivenessof our approach and provide a deeper understanding of its underlyingprinciples.\r2023-07-31\nChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model\nHanyao Huang, Ou Zheng, Dongdong Wang, Jiayi Yin, Zijin Wang, Shengxuan Ding, Heng Yin, Chuan Xu, Renjie Yang, Qian Zheng, Bing Shi\nabstract\rabstract: The ChatGPT, a lite and conversational variant of Generative PretrainedTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone LargeLanguage Models (LLMs) with billions of parameters. LLMs have stirred up muchinterest among researchers and practitioners in their impressive skills innatural language processing tasks, which profoundly impact various fields. Thispaper mainly discusses the future applications of LLMs in dentistry. Weintroduce two primary LLM deployment methods in dentistry, including automateddental diagnosis and cross-modal dental diagnosis, and examine their potentialapplications. Especially, equipped with a cross-modal encoder, a single LLM canmanage multi-source data and conduct advanced natural language reasoning toperform complex clinical operations. We also present cases to demonstrate thepotential of a fully automatic Multi-Modal LLM AI system for dentistry clinicalapplication. While LLMs offer significant potential benefits, the challenges,such as data privacy, data quality, and model bias, need further study.Overall, LLMs have the potential to revolutionize dental diagnosis andtreatment, which indicates a promising avenue for clinical application andresearch in dentistry.\r2023-07-28\nHolistic Survey of Privacy and Fairness in Machine Learning\nSina Shaham, Arash Hajisafi, Minh K Quan, Dinh C Nguyen, Bhaskar Krishnamachari, Charith Peris, Gabriel Ghinita, Cyrus Shahabi, Pubudu N. Pathirana\nabstract\rabstract: Privacy and fairness are two crucial pillars of responsible ArtificialIntelligence (AI) and trustworthy Machine Learning (ML). Each objective hasbeen independently studied in the literature with the aim of reducing utilityloss in achieving them. Despite the significant interest attracted from bothacademia and industry, there remains an immediate demand for more in-depthresearch to unravel how these two objectives can be simultaneously integratedinto ML models. As opposed to well-accepted trade-offs, i.e., privacy-utilityand fairness-utility, the interrelation between privacy and fairness is notwell-understood. While some works suggest a trade-off between the two objectivefunctions, there are others that demonstrate the alignment of these functionsin certain scenarios. To fill this research gap, we provide a thorough reviewof privacy and fairness in ML, including supervised, unsupervised,semi-supervised, and reinforcement learning. After examining and consolidatingthe literature on both objectives, we present a holistic survey on the impactof privacy on fairness, the impact of fairness on privacy, existingarchitectures, their interaction in application domains, and algorithms thataim to achieve both objectives while minimizing the utility sacrificed.Finally, we identify research challenges in achieving privacy and fairnessconcurrently in ML, particularly focusing on large language models.\r2023-07-26\nUnveiling Security, Privacy, and Ethical Concerns of ChatGPT\nXiaodong Wu, Ran Duan, Jianbing Ni\nabstract\rabstract: This paper delves into the realm of ChatGPT, an AI-powered chatbot thatutilizes topic modeling and reinforcement learning to generate naturalresponses. Although ChatGPT holds immense promise across various industries,such as customer service, education, mental health treatment, personalproductivity, and content creation, it is essential to address its security,privacy, and ethical implications. By exploring the upgrade path from GPT-1 toGPT-4, discussing the model\u0026rsquo;s features, limitations, and potentialapplications, this study aims to shed light on the potential risks ofintegrating ChatGPT into our daily lives. Focusing on security, privacy, andethics issues, we highlight the challenges these concerns pose for widespreadadoption. Finally, we analyze the open problems in these areas, calling forconcerted efforts to ensure the development of secure and ethically sound largelanguage models.\r2023-07-25\nMultilevel Large Language Models for Everyone\nYuanhao Gong\nabstract\rabstract: Large language models have made significant progress in the past few years.However, they are either generic {\\it or} field specific, splitting thecommunity into different groups. In this paper, we unify these large languagemodels into a larger map, where the generic {\\it and} specific models arelinked together and can improve each other, based on the user personal inputand information from the internet. The idea of linking several large languagemodels together is inspired by the functionality of human brain. The specificregions on the brain cortex are specific for certain low level functionality.And these regions can jointly work together to achieve more complex high levelfunctionality. Such behavior on human brain cortex sheds the light to designthe multilevel large language models that contain global level, field level anduser level models. The user level models run on local machines to achieveefficient response and protect the user\u0026rsquo;s privacy. Such multilevel modelsreduce some redundancy and perform better than the single level models. Theproposed multilevel idea can be applied in various applications, such asnatural language processing, computer vision tasks, professional assistant,business and healthcare.\r2023-07-22\nSecurity and Privacy Issues of Federated Learning\nJahid Hasan\nabstract\rabstract: Federated Learning (FL) has emerged as a promising approach to address dataprivacy and confidentiality concerns by allowing multiple participants toconstruct a shared model without centralizing sensitive data. However, thisdecentralized paradigm introduces new security challenges, necessitating acomprehensive identification and classification of potential risks to ensureFL\u0026rsquo;s security guarantees. This paper presents a comprehensive taxonomy ofsecurity and privacy challenges in Federated Learning (FL) across variousmachine learning models, including large language models. We specificallycategorize attacks performed by the aggregator and participants, focusing onpoisoning attacks, backdoor attacks, membership inference attacks, generativeadversarial network (GAN) based attacks, and differential privacy attacks.Additionally, we propose new directions for future research, seeking innovativesolutions to fortify FL systems against emerging security risks and upholdsensitive data confidentiality in distributed learning environments.\rPractical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review\nLixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Gašević\nabstract\rabstract: Educational technology innovations leveraging large language models (LLMs)have shown the potential to automate the laborious process of generating andanalysing textual content. While various innovations have been developed toautomate a range of educational tasks (e.g., question generation, feedbackprovision, and essay grading), there are concerns regarding the practicalityand ethicality of these innovations. Such concerns may hinder future researchand the adoption of LLMs-based innovations in authentic educational contexts.To address this, we conducted a systematic scoping review of 118 peer-reviewedpapers published since 2017 to pinpoint the current state of research on usingLLMs to automate and support educational tasks. The findings revealed 53 usecases for LLMs in automating education tasks, categorised into nine maincategories: profiling/labelling, detection, grading, teaching support,prediction, knowledge representation, feedback, content generation, andrecommendation. Additionally, we also identified several practical and ethicalchallenges, including low technological readiness, lack of replicability andtransparency, and insufficient privacy and beneficence considerations. Thefindings were summarised into three recommendations for future studies,including updating existing innovations with state-of-the-art models (e.g.,GPT-3/4), embracing the initiative of open-sourcing models/systems, andadopting a human-centred approach throughout the developmental process. As theintersection of AI and education is continuously evolving, the findings of thisstudy can serve as an essential reference point for researchers, allowing themto leverage the strengths, learn from the limitations, and uncover potentialresearch opportunities enabled by ChatGPT and other generative AI models.\r2023-07-21\nProject Florida: Federated Learning Made Easy\nDaniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim\nabstract\rabstract: We present Project Florida, a system architecture and software developmentkit (SDK) enabling deployment of large-scale Federated Learning (FL) solutionsacross a heterogeneous device ecosystem. Federated learning is an approach tomachine learning based on a strong data sovereignty principle, i.e., thatprivacy and security of data is best enabled by storing it at its origin,whether on end-user devices or in segregated cloud storage silos. Federatedlearning enables model training across devices and silos while the trainingdata remains within its security boundary, by distributing a model snapshot toa client running inside the boundary, running client code to update the model,and then aggregating updated snapshots across many clients in a centralorchestrator. Deploying a FL solution requires implementation of complexprivacy and security mechanisms as well as scalable orchestrationinfrastructure. Scale and performance is a paramount concern, as the modeltraining process benefits from full participation of many client devices, whichmay have a wide variety of performance characteristics. Project Florida aims tosimplify the task of deploying cross-device FL solutions by providingcloud-hosted infrastructure and accompanying task management interfaces, aswell as a multi-platform SDK supporting most major programming languagesincluding C++, Java, and Python, enabling FL training across a wide range ofoperating system (OS) and hardware specifications. The architecture decouplesservice management from the FL workflow, enabling a cloud service provider todeliver FL-as-a-service (FLaaS) to ML engineers and application developers. Wepresent an overview of Florida, including a description of the architecture,sample code, and illustrative experiments demonstrating system capabilities.\r2023-07-20\nPatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation\nLe Xiao, Xin Shan\nabstract\rabstract: Large language models(LLMS)have shown excellent text generation capabilities,capable of generating fluent human-like responses for many downstream tasks.However, applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To cope with the above challenges, this paperproposes PatternGPT, a pattern-driven text generation framework for LargeLanguage Models. Firstly, the framework utilizes the extraction capability ofLarge Language Models to generate rich and diversified structured andformalized patterns, which facilitates the introduction of external knowledgeto do the computation, and then draws on the idea of federated learning to usemultiple agents to achieve the sharing in order to obtain more diversifiedpatterns, and finally uses judgment criteria and optimization algorithm tosearch for high-quality patterns to guide the generation of models. Finally,external knowledge such as judgment criteria and optimization algorithms areused to search for high-quality patterns, and the searched patterns are used toguide model generation. This framework has the advantages of generatingdiversified patterns, protecting data privacy, combining external knowledge,and improving the quality of generation, which provides an effective method tooptimize the text generation capability of large language models, and make itbetter applied to the field of intelligent dialogue and content generation.\r2023-07-19\nWhat can we learn from Data Leakage and Unlearning for Law?\nJaydeep Borkar\nabstract\rabstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users\u0026rsquo; PII.In order to comply with privacy laws such as the \u0026ldquo;right to be forgotten\u0026rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.\r2023-07-18\nFederated Large Language Model: A Position Paper\nChaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng\nabstract\rabstract: Large scale language models (LLM) have received significant attention andfound diverse applications across various domains, but their developmentencounters challenges in real-world scenarios. These challenges arise due tothe scarcity of public domain data availability and the need to maintainprivacy with respect to private domain data. To address these issues, federatedlearning (FL) has emerged as a promising technology that enables collaborativetraining of shared models while preserving decentralized data. We propose theconcept of federated LLM, which comprises three key components, i.e., federatedLLM pre-training, federated LLM fine-tuning, and federated LLM promptengineering. For each component, we discuss its advantage over traditional LLMtraining methods and propose specific engineering strategies forimplementation. Furthermore, we explore the novel challenges introduced by theintegration of FL and LLM. We analyze existing solutions and identify potentialobstacles faced by these solutions within the context of federated LLM.\r2023-07-17\nFederated Learning of Gboard Language Models with Differential Privacy\nZheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, Yuanbo Zhang\nabstract\rabstract: We train language models (LMs) with federated learning (FL) and differentialprivacy (DP) in the Google Keyboard (Gboard). We apply theDP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm toachieve meaningfully formal DP guarantees without requiring uniform sampling ofclient devices. To provide favorable privacy-utility trade-offs, we introduce anew client participation criterion and discuss the implication of itsconfiguration in large scale systems. We show how quantile-based clipestimation~\\citep{andrew2019differentially} can be combined with DP-FTRL toadaptively choose the clip norm during training or reduce the hyperparametertuning in preparation for training. With the help of pretraining on publicdata, we train and deploy more than twenty Gboard LMs that achieve high utilityand $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two modelsadditionally trained with secure aggregation~\\citep{bonawitz2017practical}. Weare happy to announce that all the next word prediction neural network LMs inGboard now have DP guarantees, and all future launches of Gboard neural networkLMs will require DP guarantees. We summarize our experience and provideconcrete suggestions on DP training for practitioners.\r2023-07-14\nPopulation Expansion for Training Language Models with Private Federated Learning\nTatsuki Koga, Congzheng Song, Martin Pelikan, Mona Chitnis\nabstract\rabstract: Federated learning (FL) combined with differential privacy (DP) offersmachine learning (ML) training with distributed devices and with a formalprivacy guarantee. With a large population of devices, FL with DP produces aperformant model in a timely manner. However, for applications with a smallerpopulation, not only does the model utility degrade as the DP noise isinversely proportional to population, but also the training latency increasessince waiting for enough clients to become available from a smaller pool isslower. In this work, we thus propose expanding the population based on domainadaptation techniques to speed up the training and improves the final modelquality when training with small populations. We empirically demonstrate thatour techniques can improve the utility by 13% to 30% on real-world languagemodeling datasets.\r2023-07-13\nLarge Language Models for Supply Chain Optimization\nBeibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache\nabstract\rabstract: Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide \u0026ndash; a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft\u0026rsquo;s cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.\r2023-07-12\nUnified Medical Image-Text-Label Contrastive Learning With Continuous Prompt\nYuhao Wang\nabstract\rabstract: Contrastive language-image Pre-training (CLIP) [13] can leverage largedatasets of unlabeled Image-Text pairs, which have demonstrated impressiveperformance in various downstream tasks. Given that annotating medical data istime-consuming and laborious, Image-Text Pre-training has promisingapplications in exploiting large-scale medical image and radiology reportdatasets. However, medical Image-Text Pre-training faces several challenges, asfollows: (1) Due to privacy concerns, the amount of available medical data isrelatively small compared to natural data, leading to weaker generalizationability of the model. (2) Medical images are highly similar with onlyfine-grained differences in subtleties, resulting in a large number offalse-negative sample pairs in comparison learning. (3) The hand-crafted Promptusually differs from the natural medical image report, Subtle changes inwording can lead to significant differences in performance. In this paper, wepropose a unified Image-Text-Label contrastive learning framework based oncontinuous prompts, with three main contributions. First, We unified the dataof images, text, and labels, which greatly expanded the training data that themodel could utilize. Second, we address the issue of data diversity and theimpact of hand-crafted prompts on model performance by introducing continuousimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training tomitigate the problem of too many false-negative samples. We demonstrate throughsufficient experiments that the Unified Medical Contrastive Learning (UMCL)framework exhibits excellent performance on several downstream tasks.\r2023-07-10\nEthicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation\nZhexin Zhang, Jiaxin Wen, Minlie Huang\nabstract\rabstract: Large pre-trained language models achieve impressive results across manytasks. However, recent works point out that pre-trained language models maymemorize a considerable fraction of their training data, leading to the privacyrisk of information leakage. In this paper, we propose a method named Ethicistfor targeted training data extraction through loss smoothed soft prompting andcalibrated confidence estimation, investigating how to recover the suffix inthe training data when given a prefix. To elicit memorization in the attackedmodel, we tune soft prompt embeddings while keeping the model fixed. We furtherpropose a smoothing loss that smooths the loss distribution of the suffixtokens to make it easier to sample the correct suffix. In order to select themost probable suffix from a collection of sampled suffixes and estimate theprediction confidence, we propose a calibrated confidence estimation method,which normalizes the confidence of the generated suffixes with a localestimation. We show that Ethicist significantly improves the extractionperformance on a recently proposed public benchmark. We also investigateseveral factors influencing the data extraction performance, including decodingstrategy, model scale, prefix length, and suffix length. Our code is availableat https://github.com/thu-coai/Targeted-Data-Extraction.\r2023-07-09\nShaping the Emerging Norms of Using Large Language Models in Social Computing Research\nHong Shen, Tianshi Li, Toby Jia-Jun Li, Joon Sung Park, Diyi Yang\nabstract\rabstract: The emergence of Large Language Models (LLMs) has brought both excitement andconcerns to social computing research. On the one hand, LLMs offerunprecedented capabilities in analyzing vast amounts of textual data andgenerating human-like responses, enabling researchers to delve into complexsocial phenomena. On the other hand, concerns are emerging regarding thevalidity, privacy, and ethics of the research when LLMs are involved. This SIGaims at offering an open space for social computing researchers who areinterested in understanding the impacts of LLMs to discuss their currentpractices, perspectives, challenges when engaging with LLMs in their everydaywork and collectively shaping the emerging norms of using LLMs in socialcomputing research.\r2023-07-04\nProPILE: Probing Privacy Leakage in Large Language Models\nSiwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh\nabstract\rabstract: The rapid advancement and widespread use of large language models (LLMs) haveraised significant concerns regarding the potential leakage of personallyidentifiable information (PII). These models are often trained on vastquantities of web-collected data, which may inadvertently include sensitivepersonal data. This paper presents ProPILE, a novel probing tool designed toempower data subjects, or the owners of the PII, with awareness of potentialPII leakage in LLM-based services. ProPILE lets data subjects formulate promptsbased on their own PII to evaluate the level of privacy intrusion in LLMs. Wedemonstrate its application on the OPT-1.3B model trained on the publiclyavailable Pile dataset. We show how hypothetical data subjects may assess thelikelihood of their PII being included in the Pile dataset being revealed.ProPILE can also be leveraged by LLM service providers to effectively evaluatetheir own levels of PII leakage with more powerful prompts specifically tunedfor their in-house models. This tool represents a pioneering step towardsempowering the data subjects for their awareness and control over their owndata on the web.\r2023-06-26\nChatIDS: Explainable Cybersecurity Using Generative AI\nVictor Jüttner, Martin Grimmer, Erik Buchmann\nabstract\rabstract: Intrusion Detection Systems (IDS) are a proven approach to secure networks.However, in a privately used network, it is difficult for users withoutcybersecurity expertise to understand IDS alerts, and to respond in time withadequate measures. This puts the security of home networks, smart homeinstallations, home-office workers, etc. at risk, even if an IDS is correctlyinstalled and configured. In this work, we propose ChatIDS, our approach toexplain IDS alerts to non-experts by using large language models. We evaluatethe feasibility of ChatIDS by using ChatGPT, and we identify open researchissues with the help of interdisciplinary experts in artificial intelligence.Our results show that ChatIDS has the potential to increase network security byproposing meaningful security measures in an intuitive language from IDSalerts. Nevertheless, some potential issues in areas such as trust, privacy,ethics, etc. need to be resolved, before ChatIDS might be put into practice.\r2023-06-24\nChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang\nabstract\rabstract: The primary aim of this research was to address the limitations observed inthe medical knowledge of prevalent large language models (LLMs) such asChatGPT, by creating a specialized language model with enhanced accuracy inmedical advice. We achieved this by adapting and refining the large languagemodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialoguessourced from a widely used online medical consultation platform. Theseconversations were cleaned and anonymized to respect privacy concerns. Inaddition to the model refinement, we incorporated a self-directed informationretrieval mechanism, allowing the model to access and utilize real-timeinformation from online sources like Wikipedia and data from curated offlinemedical databases. The fine-tuning of the model with real-world patient-doctorinteractions significantly improved the model\u0026rsquo;s ability to understand patientneeds and provide informed advice. By equipping the model with self-directedinformation retrieval from reliable online and offline sources, we observedsubstantial improvements in the accuracy of its responses. Our proposedChatDoctor, represents a significant advancement in medical LLMs, demonstratinga significant improvement in understanding patient inquiries and providingaccurate advice. Given the high stakes and low error tolerance in the medicalfield, such enhancements in providing accurate and reliable information are notonly beneficial but essential.\r2023-06-23\nExploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT\nRyan Lingo\nabstract\rabstract: This research delves into the construction and utilization of syntheticdatasets, specifically within the telematics sphere, leveraging OpenAI\u0026rsquo;spowerful language model, ChatGPT. Synthetic datasets present an effectivesolution to challenges pertaining to data privacy, scarcity, and control overvariables - characteristics that make them particularly valuable for researchpursuits. The utility of these datasets, however, largely depends on theirquality, measured through the lenses of diversity, relevance, and coherence. Toillustrate this data creation process, a hands-on case study is conducted,focusing on the generation of a synthetic telematics dataset. The experimentinvolved an iterative guidance of ChatGPT, progressively refining prompts andculminating in the creation of a comprehensive dataset for a hypothetical urbanplanning scenario in Columbus, Ohio. Upon generation, the synthetic dataset wassubjected to an evaluation, focusing on the previously identified qualityparameters and employing descriptive statistics and visualization techniquesfor a thorough analysis. Despite synthetic datasets not serving as perfectreplacements for actual world data, their potential in specific use-cases, whenexecuted with precision, is significant. This research underscores thepotential of AI models like ChatGPT in enhancing data availability for complexsectors like telematics, thus paving the way for a myriad of new researchopportunities.\r2023-06-20\nFedMultimodal: A Benchmark For Multimodal Federated Learning\nTiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, Mi Zhang, Salman Avestimehr, Shrikanth Narayanan\nabstract\rabstract: Over the past few years, Federated Learning (FL) has become an emergingmachine learning technique to tackle data privacy challenges throughcollaborative training. In the Federated Learning algorithm, the clients submita locally trained model, and the server aggregates these parameters untilconvergence. Despite significant efforts that have been made to FL in fieldslike computer vision, audio, and natural language processing, the FLapplications utilizing multimodal data streams remain largely unexplored. It isknown that multimodal learning has broad real-world applications in emotionrecognition, healthcare, multimedia, and social media, while user privacypersists as a critical concern. Specifically, there are no existing FLbenchmarks targeting multimodal applications or related tasks. In order tofacilitate the research in multimodal FL, we introduce FedMultimodal, the firstFL benchmark for multimodal learning covering five representative multimodalapplications from ten commonly used datasets with a total of eight uniquemodalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-endmodeling framework ranging from data partition and feature extraction to FLbenchmark algorithms and model evaluation. Unlike existing FL benchmarks,FedMultimodal provides a standardized approach to assess the robustness of FLagainst three common data corruptions in real-life multimodal applications:missing modalities, missing labels, and erroneous labels. We hope thatFedMultimodal can accelerate numerous future research directions, includingdesigning multimodal FL algorithms toward extreme data heterogeneity,robustness multimodal FL, and efficient multimodal FL. The datasets andbenchmark results can be accessed at:https://github.com/usc-sail/fed-multimodal.\r2023-06-19\nPath to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost\nJuexiao Zhou, Xiuying Chen, Xin Gao\nabstract\rabstract: Medical artificial general intelligence (AGI) is an emerging field that aimsto develop systems specifically designed for medical applications that possessthe ability to understand, learn, and apply knowledge across a wide range oftasks and domains. Large language models (LLMs) represent a significant steptowards AGI. However, training cross-domain LLMs in the medical field posessignificant challenges primarily attributed to the requirement of collectingdata from diverse domains. This task becomes particularly difficult due toprivacy restrictions and the scarcity of publicly available medical datasets.Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specificmedical LLMs with the lowest cost, and suggest a possible path to achievemedical AGI. With an increasing number of domain-specific professionalmultimodal LLMs in the medical field being developed, MedAGI is designed toautomatically select appropriate medical models by analyzing users\u0026rsquo; questionswith our novel adaptive expert selection algorithm. It offers a unifiedapproach to existing LLMs in the medical field, eliminating the need forretraining regardless of the introduction of new models. This characteristicrenders it a future-proof solution in the dynamically advancing medical domain.To showcase the resilience of MedAGI, we conducted an evaluation across threedistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysisof pathology pictures. The results demonstrated that MedAGI exhibitedremarkable versatility and scalability, delivering exceptional performanceacross diverse domains. Our code is publicly available to facilitate furtherresearch at https://github.com/JoshuaChou2018/MedAGI.\r2023-06-14\nProtecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization\nZhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li\nabstract\rabstract: Large Language Models (LLMs) are gaining increasing attention due to theirexceptional performance across numerous tasks. As a result, the general publicutilize them as an influential tool for boosting their productivity whilenatural language processing researchers endeavor to employ them in solvingexisting or new research problems. Unfortunately, individuals can only accesssuch powerful AIs through APIs, which ultimately leads to the transmission ofraw data to the models\u0026rsquo; providers and increases the possibility of privacy dataleakage. Current privacy-preserving methods for cloud-deployed language modelsaim to protect privacy information in the pre-training dataset or during themodel training phase. However, they do not meet the specific challengespresented by the remote access approach of new large-scale language models. This paper introduces a novel task, \u0026ldquo;User Privacy Protection for DialogueModels,\u0026rdquo; which aims to safeguard sensitive user information from any possibledisclosure while conversing with chatbots. We also present an evaluation schemefor this task, which covers evaluation metrics for privacy protection, dataavailability, and resistance to simulation attacks. Moreover, we propose thefirst framework for this task, namely privacy protection through textsanitization. Before sending the input to remote large models, it filters outthe sensitive information, using several rounds of text sanitization based onprivacy types that users define. Upon receiving responses from the largermodel, our framework automatically restores privacy to ensure that theconversation goes smoothly, without intervention from the privacy filter.Experiments based on real-world datasets demonstrate the efficacy of ourprivacy-preserving approach against eavesdropping from potential attackers.\r2023-06-13\nPersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer\nXu Han, Bin Guo, Yoon Jung, Benjamin Yao, Yu Zhang, Xiaohu Liu, Chenlei Guo\nabstract\rabstract: Personalized dialogue agents (DAs) powered by large pre-trained languagemodels (PLMs) often rely on explicit persona descriptions to maintainpersonality consistency. However, such descriptions may not always be availableor may pose privacy concerns. To tackle this bottleneck, we introducePersonaPKT, a lightweight transfer learning approach that can buildpersona-consistent dialogue models without explicit persona descriptions. Byrepresenting each persona as a continuous vector, PersonaPKT learns implicitpersona-specific features directly from a small number of dialogue samplesproduced by the same persona, adding less than 0.1% trainable parameters foreach persona on top of the PLM backbone. Empirical results demonstrate thatPersonaPKT effectively builds personalized DAs with high storage efficiency,outperforming various baselines in terms of persona consistency whilemaintaining good response generation quality. In addition, it enhances privacyprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is aneffective solution for creating personalized DAs that respect user privacy.\r2023-06-09\nPrivacy Aware Question-Answering System for Online Mental Health Risk Assessment\nPrateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari\nabstract\rabstract: Social media platforms have enabled individuals suffering from mentalillnesses to share their lived experiences and find the online supportnecessary to cope. However, many users fail to receive genuine clinicalsupport, thus exacerbating their symptoms. Screening users based on what theypost online can aid providers in administering targeted healthcare and minimizefalse positives. Pre-trained Language Models (LMs) can assess users\u0026rsquo; socialmedia data and classify them in terms of their mental health risk. We propose aQuestion-Answering (QA) approach to assess mental health risk using theUnified-QA model on two large mental health datasets. To protect user data, weextend Unified-QA by anonymizing the model training process using differentialprivacy. Our results demonstrate the effectiveness of modeling risk assessmentas a QA task, specifically for mental health use cases. Furthermore, themodel\u0026rsquo;s performance decreases by less than 1% with the inclusion ofdifferential privacy. The proposed system\u0026rsquo;s performance is indicative of apromising research direction that will lead to the development of privacy-awarediagnostic systems.\r2023-06-08\nPandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang\nabstract\rabstract: Instruction tuning large language models (LLMs) remains a challenging task,owing to the complexity of hyperparameter selection and the difficulty involvedin evaluating the tuned models. To determine the optimal hyperparameters, anautomatic, robust, and reliable evaluation benchmark is essential. However,establishing such a benchmark is not a trivial task due to the challengesassociated with evaluation accuracy and privacy protection. In response tothese challenges, we introduce a judge large language model, named PandaLM,which is trained to distinguish the superior model given several LLMs.PandaLM\u0026rsquo;s focus extends beyond just the objective correctness of responses,which is the main focus of traditional evaluation datasets. It addresses vitalsubjective factors such as relative conciseness, clarity, adherence toinstructions, comprehensiveness, and formality. To ensure the reliability ofPandaLM, we collect a diverse human-annotated test dataset, where all contextsare generated by humans and labels are aligned with human preferences. Ourresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5\u0026rsquo;s evaluationability and 88.28% of GPT-4\u0026rsquo;s in terms of F1-score on our test dataset. PandaLMenables the evaluation of LLM to be fairer but with less cost, evidenced bysignificant improvements achieved by models tuned through PandaLM compared totheir counterparts trained with default Alpaca\u0026rsquo;s hyperparameters. In addition,PandaLM does not depend on API-based evaluations, thus avoiding potential dataleakage. All resources of PandaLM are released athttps://github.com/WeOpenML/PandaLM.\rSkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model\nJuexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, Xin Gao\nabstract\rabstract: Skin and subcutaneous diseases rank high among the leading contributors tothe global burden of nonfatal diseases, impacting a considerable portion of thepopulation. Nonetheless, the field of dermatology diagnosis faces threesignificant hurdles. Firstly, there is a shortage of dermatologists accessibleto diagnose patients, particularly in rural regions. Secondly, accuratelyinterpreting skin disease images poses a considerable challenge. Lastly,generating patient-friendly diagnostic reports is usually a time-consuming andlabor-intensive task for dermatologists. To tackle these challenges, we presentSkinGPT-4, which is the world\u0026rsquo;s first interactive dermatology diagnostic systempowered by an advanced visual large language model. SkinGPT-4 leverages afine-tuned version of MiniGPT-4, trained on an extensive collection of skindisease images (comprising 52,929 publicly available and proprietary images)along with clinical concepts and doctors\u0026rsquo; notes. We designed a two-steptraining process to allow SkinGPT to express medical features in skin diseaseimages with natural language and make accurate diagnoses of the types of skindiseases. With SkinGPT-4, users could upload their own skin photos fordiagnosis, and the system could autonomously evaluate the images, identifiesthe characteristics and categories of the skin conditions, performs in-depthanalysis, and provides interactive treatment recommendations. Meanwhile,SkinGPT-4\u0026rsquo;s local deployment capability and commitment to user privacy alsorender it an appealing choice for patients in search of a dependable andprecise diagnosis of their skin ailments. To demonstrate the robustness ofSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, whichwere independently reviewed by certified dermatologists, and showed thatSkinGPT-4 could provide accurate diagnoses of skin diseases.\r2023-06-02\nWhen Federated Learning Meets Pre-trained Language Models\u0026rsquo; Parameter-Efficient Tuning Methods\nZhuo Zhang, Yuanhang Yang, Yong Dai, Lizhen Qu, Zenglin Xu\nabstract\rabstract: With increasing privacy concerns on data, recent studies have madesignificant progress using federated learning (FL) on privacy-sensitive naturallanguage processing (NLP) tasks. Much literature suggests fully fine-tuningpre-trained language models (PLMs) in the FL paradigm can mitigate the dataheterogeneity problem and close the performance gap with centralized training.However, large PLMs bring the curse of prohibitive communication overhead andlocal model adaptation costs for the FL system. To this end, we introducevarious parameter-efficient tuning (PETuning) methods into federated learning.Specifically, we provide a holistic empirical study of representative PLMstuning methods in FL. The experimental results cover the analysis of dataheterogeneity levels, data scales, and different FL scenarios. Overallcommunication overhead can be significantly reduced by locally tuning andglobally aggregating lightweight model parameters while maintaining acceptableperformance in various FL settings. To facilitate the research of PETuning inFL, we also develop a federated tuning framework FedPETuning, which allowspractitioners to exploit different PETuning methods under the FL trainingparadigm conveniently. The source code is available at\\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.\r2023-06-01\nBag of Tricks for Training Data Extraction from Language Models\nWeichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, Shuicheng Yan\nabstract\rabstract: With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available athttps://github.com/weichen-yu/LM-Extraction.\r2023-05-31\nDeep Regression Unlearning\nAyush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli\nabstract\rabstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: https://github.com/ayu987/deep-regression-unlearning\rSynthetic Pre-Training Tasks for Neural Machine Translation\nZexue He, Graeme Blackwood, Rameswar Panda, Julian McAuley, Rogerio Feris\nabstract\rabstract: Pre-training models with large crawled corpora can lead to issues such astoxicity and bias, as well as copyright and privacy concerns. A promising wayof alleviating such concerns is to conduct pre-training with synthetic tasksand data, since no real-world information is ingested by the model. Our goal inthis paper is to understand the factors that contribute to the effectiveness ofpre-training models when using synthetic resources, particularly in the contextof neural machine translation. We propose several novel approaches topre-training translation models that involve different levels of lexical andstructural knowledge, including: 1) generating obfuscated data from a largeparallel corpus 2) concatenating phrase pairs extracted from a smallword-aligned corpus, and 3) generating synthetic parallel data without realhuman language corpora. Our experiments on multiple language pairs reveal thatpre-training benefits can be realized even with high levels of obfuscation orpurely synthetic parallel data. We hope the findings from our comprehensiveempirical analysis will shed light on understanding what matters for NMTpre-training, as well as pave the way for the development of more efficient andless toxic models.\r2023-05-30\nDoes CLIP Know My Face?\nDominik Hintersdorf, Lukas Struppek, Manuel Brack, Felix Friedrich, Patrick Schramowski, Kristian Kersting\nabstract\rabstract: With the rise of deep learning in various applications, privacy concernsaround the protection of training data has become a critical area of research.Whereas prior studies have focused on privacy risks in single-modal models, weintroduce a novel method to assess privacy for multi-modal models, specificallyvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)reveals whether an individual was included in the training data by querying themodel with images of the same person. Letting the model choose from a widevariety of possible text labels, the model reveals whether it recognizes theperson and, therefore, was used for training. Our large-scale experiments onCLIP demonstrate that individuals used for training can be identified with veryhigh accuracy. We confirm that the model has learned to associate names withdepicted individuals, implying the existence of sensitive information that canbe extracted by adversaries. Our results highlight the need for strongerprivacy protection in large-scale models and suggest that IDIAs can be used toprove the unauthorized use of data for training and to enforce privacy laws.\r2023-05-24\nMachine Unlearning: its nature, scope, and importance for a \u0026ldquo;delete culture\u0026rdquo;\nLuciano Floridi\nabstract\rabstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them \u0026lsquo;forget\u0026rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.\rFlocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\nHaonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch\nabstract\rabstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock\u0026rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.\rMGeo: Multi-Modal Geographic Pre-Training Method\nRuixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, Yao Xu\nabstract\rabstract: As a core task in location-based services (LBS) (e.g., navigation maps),query and point of interest (POI) matching connects users\u0026rsquo; intent withreal-world geographic information. Recently, pre-trained models (PTMs) havemade advancements in many natural language processing (NLP) tasks. Generictext-based PTMs do not have enough geographic knowledge for query-POI matching.To overcome this limitation, related literature attempts to employdomain-adaptive pre-training based on geo-related corpus. However, a querygenerally contains mentions of multiple geographic objects, such as nearbyroads and regions of interest (ROIs). The geographic context (GC), i.e., thesediverse geographic objects and their relationships, is therefore pivotal toretrieving the most relevant POI. Single-modal PTMs can barely make use of theimportant GC and therefore have limited performance. In this work, we propose anovel query-POI matching method Multi-modal Geographic language model (MGeo),which comprises a geographic encoder and a multi-modal interaction module. MGeorepresents GC as a new modality and is able to fully extract multi-modalcorrelations for accurate query-POI matching. Besides, there is no publiclyavailable benchmark for this topic. In order to facilitate further research, webuild a new open-source large-scale benchmark Geographic TExtual Similarity(GeoTES). The POIs come from an open-source geographic information system(GIS). The queries are manually generated by annotators to prevent privacyissues. Compared with several strong baselines, the extensive experimentresults and detailed ablation analyses on GeoTES demonstrate that our proposedmulti-modal pre-training method can significantly improve the query-POImatching capability of generic PTMs, even when the queries\u0026rsquo; GC is not provided.Our code and dataset are publicly available athttps://github.com/PhantomGrapes/MGeo.\r2023-05-22\nEnhancing Small Medical Learners with Privacy-preserving Contextual Prompting\nXinlu Zhang, Shiyang Li, Xianjun Yang, Chenxin Tian, Yao Qin, Linda Ruth Petzold\nabstract\rabstract: Large language models (LLMs) demonstrate remarkable medical expertise, butdata privacy concerns impede their direct use in healthcare environments.Although offering improved data privacy protection, domain-specific smalllanguage models (SLMs) often underperform LLMs, emphasizing the need formethods that reduce this performance gap while alleviating privacy concerns. Inthis paper, we present a simple yet effective method that harnesses LLMs\u0026rsquo;medical proficiency to boost SLM performance in medical tasks underprivacy-restricted scenarios. Specifically, we mitigate patient privacy issuesby extracting keywords from medical data and prompting the LLM to generate amedical knowledge-intensive context by simulating clinicians\u0026rsquo; thoughtprocesses. This context serves as additional input for SLMs, augmenting theirdecision-making capabilities. Our method significantly enhances performance inboth few-shot and full training settings across three medicalknowledge-intensive tasks, achieving up to a 22.57% increase in absoluteaccuracy compared to SLM fine-tuning without context, and sets newstate-of-the-art results in two medical tasks within privacy-restrictedscenarios. Further out-of-domain testing and experiments in two general domaindatasets showcase its generalizability and broad applicability.\r2023-05-20\nCan Public Large Language Models Help Private Cross-device Federated Learning?\nBoxin Wang, Yibo Jacky Zhang, Yuan Cao, Bo Li, H. Brendan McMahan, Sewoong Oh, Zheng Xu, Manzil Zaheer\nabstract\rabstract: We study (differentially) private federated learning (FL) of language models.The language models in cross-device FL are relatively small, which can betrained with meaningful formal user-level differential privacy (DP) guaranteeswhen massive parallelism in training is enabled by the participation of amoderate size of users. Recently, public data has been used to improveprivacy-utility trade-offs for both large and small language models. In thiswork, we provide a systematic study of using large-scale public data and LLMsto help differentially private training of on-device FL models, and furtherimprove the privacy-utility tradeoff by techniques of distillation. Moreover,we propose a novel distribution matching algorithm with theoretical groundingto sample public data close to private data distribution, which significantlyimproves the sample efficiency of (pre-)training on public data. The proposedmethod is efficient and effective for training private model by takingadvantage of public data, especially for customized on-device architecturesthat do not have ready-to-use pre-trained models.\r2023-05-19\nChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery\nAnaelia Ovalle, Mehrab Beikzadeh, Parshan Teimouri, Kai-Wei Chang, Majid Sarrafzadeh\nabstract\rabstract: Large language models have been useful in expanding mental health caredelivery. ChatGPT, in particular, has gained popularity for its ability togenerate human-like dialogue. However, data-sensitive domains \u0026ndash; including butnot limited to healthcare \u0026ndash; face challenges in using ChatGPT due to privacyand data-ownership concerns. To enable its utilization, we propose a textambiguation framework that preserves user privacy. We ground this in the taskof addressing stress prompted by user-provided texts to demonstrate theviability and helpfulness of privacy-preserved generations. Our results suggestthat chatGPT recommendations are still able to be moderately helpful andrelevant, even when the original user text is not provided.\rControlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning\nMustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta\nabstract\rabstract: Large Language Models (LLMs) are known to memorize significant portions oftheir training data. Parts of this memorized content have been shown to beextractable by simply querying the model, which poses a privacy risk. Wepresent a novel approach which uses prompt-tuning to control the extractionrates of memorized content in LLMs. We present two prompt training strategiesto increase and decrease extraction rates, which correspond to an attack and adefense, respectively. We demonstrate the effectiveness of our techniques byusing models from the GPT-Neo family on a public benchmark. For the 1.3Bparameter GPT-Neo model, our attack yields a 9.3 percentage point increase inextraction rate compared to our baseline. Our defense can be tuned to achievedifferent privacy-utility trade-offs by a user-specified hyperparameter. Weachieve an extraction rate reduction of up to 97.7% relative to our baseline,with a perplexity increase of 16.9%.\rTowards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models\nJiayi Fu, Haoying Han, Xing Su, Chao Fan\nabstract\rabstract: Pre-trained large language models (PLMs) have the potential to support urbanscience research through content creation, information extraction, assistedprogramming, text classification, and other technical advances. In thisresearch, we explored the opportunities, challenges, and prospects of PLMs inurban science research. Specifically, we discussed potential applications ofPLMs to urban institution, urban space, urban information, and citizenbehaviors research through seven examples using ChatGPT. We also examined thechallenges of PLMs in urban science research from both technical and socialperspectives. The prospects of the application of PLMs in urban scienceresearch were then proposed. We found that PLMs can effectively aid inunderstanding complex concepts in urban science, facilitate urban spatial formidentification, assist in disaster monitoring, and sense public sentiment. Atthe same time, however, the applications of PLMs in urban science research faceevident threats, such as technical limitations, security, privacy, and socialbias. The development of fundamental models based on domain knowledge andhuman-AI collaboration may help improve PLMs to support urban science researchin future.\r2023-05-18\nEthical ChatGPT: Concerns, Challenges, and Commandments\nJianlong Zhou, Heimo Müller, Andreas Holzinger, Fang Chen\nabstract\rabstract: Large language models, e.g. ChatGPT are currently contributing enormously tomake artificial intelligence even more popular, especially among the generalpopulation. However, such chatbot models were developed as tools to supportnatural language communication between humans. Problematically, it is very mucha ``statistical correlation machine\u0026quot; (correlation instead of causality) andthere are indeed ethical concerns associated with the use of AI language modelssuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlightsspecific ethical concerns on ChatGPT and articulates key challenges whenChatGPT is used in various applications. Practical commandments for differentstakeholders of ChatGPT are also proposed that can serve as checklistguidelines for those applying ChatGPT in their applications. These commandmentexamples are expected to motivate the ethical use of ChatGPT.\rAugmented Large Language Models with Parametric Knowledge Guiding\nZiyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang\nabstract\rabstract: Large Language Models (LLMs) have significantly advanced natural languageprocessing (NLP) with their impressive language understanding and generationcapabilities. However, their performance may be suboptimal for domain-specifictasks that require specialized knowledge due to limited exposure to the relateddata. Additionally, the lack of transparency of most state-of-the-art (SOTA)LLMs, which can only be accessed via APIs, impedes further fine-tuning withdomain custom data. Moreover, providing private data to the LLMs\u0026rsquo; owner leadsto data privacy problems. To address these challenges, we propose the novelParametric Knowledge Guiding (PKG) framework, which equips LLMs with aknowledge-guiding module to access relevant knowledge without altering theLLMs\u0026rsquo; parameters. Our PKG is based on open-source \u0026ldquo;white-box\u0026rdquo; language models,allowing offline memory of any knowledge that LLMs require. We demonstrate thatour PKG framework can enhance the performance of \u0026ldquo;black-box\u0026rdquo; LLMs on a range ofdomain knowledge-intensive tasks that require factual (+7.9%), tabular(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.\r2023-05-17\nLife of PII \u0026ndash; A PII Obfuscation Transformer\nAjinkya Deshmukh, Saumya Banthia, Anantha Sharma\nabstract\rabstract: Protecting sensitive information is crucial in today\u0026rsquo;s world of LargeLanguage Models (LLMs) and data-driven services. One common method used topreserve privacy is by using data perturbation techniques to reduceoverreaching utility of (sensitive) Personal Identifiable Information (PII)data while maintaining its statistical and semantic properties. Dataperturbation methods often result in significant information loss, making themimpractical for use. In this paper, we propose \u0026lsquo;Life of PII\u0026rsquo;, a novelObfuscation Transformer framework for transforming PII into faux-PII whilepreserving the original information, intent, and context as much as possible.Our approach includes an API to interface with the given document, aconfiguration-based obfuscator, and a model based on the Transformerarchitecture, which has shown high context preservation and performance innatural language processing tasks and LLMs. Our Transformer-based approach learns mapping between the original PII andits transformed faux-PII representation, which we call \u0026ldquo;obfuscated\u0026rdquo; data. Ourexperiments demonstrate that our method, called Life of PII, outperformstraditional data perturbation techniques in terms of both utility preservationand privacy protection. We show that our approach can effectively reduceutility loss while preserving the original information, offering greaterflexibility in the trade-off between privacy protection and data utility. Ourwork provides a solution for protecting PII in various real-world applications.\rNetGPT: Generative Pretrained Transformer for Network Traffic\nXuying Meng, Chungang Lin, Yequan Wang, Yujun Zhang\nabstract\rabstract: All data on the Internet are transferred by network traffic, thus accuratelymodeling network traffic can help improve network services quality and protectdata privacy. Pretrained models for network traffic can utilize large-scale rawdata to learn the essential characteristics of network traffic, and generatedistinguishable results for input traffic without considering specificdownstream tasks. Effective pretrained models can significantly optimize thetraining efficiency and effectiveness of downstream tasks, such as applicationclassification, attack detection and traffic generation. Despite the greatsuccess of pretraining in natural language processing, there is no work in thenetwork field. Considering the diverse demands and characteristics of networktraffic and network tasks, it is non-trivial to build a pretrained model fornetwork traffic and we face various challenges, especially the heterogeneousheaders and payloads in the multi-pattern network traffic and the differentdependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt toprovide a generative pretrained model NetGPT for both traffic understanding andgeneration tasks. We propose the multi-pattern network traffic modeling toconstruct unified text inputs and support both traffic understanding andgeneration tasks. We further optimize the adaptation effect of the pretrainedmodel to diversified tasks by shuffling header fields, segmenting packets inflows, and incorporating diverse task labels with prompts. With diverse trafficdatasets from encrypted software, DNS, private industrial protocols andcryptocurrency mining, expensive experiments demonstrate the effectiveness ofour NetGPT in a range of traffic understanding and generation tasks on trafficdatasets, and outperform state-of-the-art baselines by a wide margin.\r2023-05-12\nPLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English\nJianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, Kai-Wei Chang\nabstract\rabstract: Privacy policies provide individuals with information about their rights andhow their personal information is handled. Natural language understanding (NLU)technologies can support individuals and practitioners to understand betterprivacy practices described in lengthy and complex documents. However, existingefforts that use NLU technologies are limited by processing the language in away exclusive to a single task focusing on certain privacy practices. To thisend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)benchmark, a multi-task benchmark for evaluating the privacy policy languageunderstanding across various tasks. We also collect a large corpus of privacypolicies to enable privacy policy domain-specific language model pre-training.We evaluate several generic pre-trained language models and continuepre-training them on the collected corpus. We demonstrate that domain-specificcontinual pre-training offers performance improvements across all tasks.\r2023-05-10\nPrivacy-Preserving Prompt Tuning for Large Language Model Services\nYansong Li, Zhixing Tan, Yang Liu\nabstract\rabstract: Prompt tuning provides an efficient way for users to customize Large LanguageModels (LLMs) with their private data in the emerging LLM service scenario.However, the sensitive nature of private data brings the need for privacypreservation in LLM service customization. Based on prompt tuning, we proposePrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacyguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,allowing users to privatize their data locally with local differential privacy.As prompt tuning performs poorly when directly trained on privatized data, weintroduce a novel privatized token reconstruction task that is trained jointlywith the downstream task, allowing LLMs to learn better task-dependentrepresentations. Despite the simplicity of our framework, experiments show thatRAPT achieves competitive performance across tasks while providing privacyguarantees against adversaries.\r2023-05-09\nMeasuring Forgetting of Memorized Training Examples\nMatthew Jagielski, Om Thakkar, Florian Tramèr, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang\nabstract\rabstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models \u0026ldquo;forget\u0026rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.\r2023-05-08\nDifferentially Private Attention Computation\nYeqi Gao, Zhao Song, Xin Yang\nabstract\rabstract: Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].\r2023-05-02\nMitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy\nAly M. Kassem\nabstract\rabstract: Large Language models (LLMs) are trained on large amounts of data, which caninclude sensitive information that may compromise personal privacy. LLMs showedto memorize parts of the training data and emit those data verbatim when anadversary prompts appropriately. Previous research has primarily focused ondata preprocessing and differential privacy techniques to address memorizationor prevent verbatim memorization exclusively, which can give a false sense ofprivacy. However, these methods rely on explicit and implicit assumptions aboutthe structure of the data to be protected, which often results in an incompletesolution to the problem. To address this, we propose a novel framework thatutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigateapproximate memorization. Our approach utilizes a negative similarity score,such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilaritypolicy. Our results demonstrate that this framework effectively mitigatesapproximate memorization while maintaining high levels of coherence and fluencyin the generated samples. Furthermore, our framework is robust in mitigatingapproximate memorization across various circumstances, including longercontext, which is known to increase memorization in LLMs.\r2023-05-01\nA Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, Lichao Sun\nabstract\rabstract: Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence.\r2023-04-30\nReliable Gradient-free and Likelihood-free Prompt Tuning\nMaohao Shen, Soumya Ghosh, Prasanna Sattigeri, Subhro Das, Yuheng Bu, Gregory Wornell\nabstract\rabstract: Due to privacy or commercial constraints, large pre-trained language models(PLMs) are often offered as black-box APIs. Fine-tuning such models todownstream tasks is challenging because one can neither access the model\u0026rsquo;sinternal representations nor propagate gradients through it. This paperaddresses these challenges by developing techniques for adapting PLMs with onlyAPI access. Building on recent work on soft prompt tuning, we develop methodsto tune the soft prompts without requiring gradient computation. Further, wedevelop extensions that in addition to not requiring gradients also do not needto access any internal representation of the PLM beyond the input embeddings.Moreover, instead of learning a single prompt, our methods learn a distributionover prompts allowing us to quantify predictive uncertainty. Ours is the firstwork to consider uncertainty in prompts when only having API access to the PLM.Finally, through extensive experiments, we carefully vet the proposed methodsand find them competitive with (and sometimes even improving on) gradient-basedapproaches with full access to the PLM.\r2023-04-25\nTABLET: Learning From Instructions For Tabular Data\nDylan Slack, Sameer Singh\nabstract\rabstract: Acquiring high-quality data is often a significant challenge in trainingmachine learning (ML) models for tabular prediction, particularly inprivacy-sensitive and costly domains like medicine and finance. Providingnatural language instructions to large language models (LLMs) offers analternative solution. However, it is unclear how effectively instructionsleverage the knowledge in LLMs for solving tabular prediction problems. Toaddress this gap, we introduce TABLET, a benchmark of 20 diverse tabulardatasets annotated with instructions that vary in their phrasing, granularity,and technicality. Additionally, TABLET includes the instructions\u0026rsquo; logic andstructured modifications to the instructions. We find in-context instructionsincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% forChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabularprediction in our benchmark by evaluating instruction faithfulness. We findLLMs often ignore instructions and fail to predict specific instancescorrectly, even with examples. Our analysis on TABLET shows that, whileinstructions help LLM performance, learning from instructions for tabular datarequires new capabilities.\r2023-04-22\nRetrieval Enhanced Data Augmentation for Question Answering on Privacy Policies\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, Kai-Wei Chang\nabstract\rabstract: Prior studies in privacy policies frame the question answering (QA) task asidentifying the most relevant text segment or a list of sentences from a policydocument given a user query. Existing labeled datasets are heavily imbalanced(only a few relevant segments), limiting the QA performance in this domain. Inthis paper, we develop a data augmentation framework based on ensemblingretriever models that captures the relevant text segments from unlabeled policydocuments and expand the positive examples in the training set. In addition, toimprove the diversity and quality of the augmented data, we leverage multiplepre-trained language models (LMs) and cascade them with noise reduction filtermodels. Using our augmented data on the PrivacyQA benchmark, we elevate theexisting baseline by a large margin (10% F1) and achieve a newstate-of-the-art F1 score of 50%. Our ablation studies provide furtherinsights into the effectiveness of our approach.\r2023-04-15\nDoes Prompt-Tuning Language Model Ensure Privacy?\nShangyu Xie, Wei Dai, Esha Ghosh, Sambuddha Roy, Dan Schwartz, Kim Laine\nabstract\rabstract: Prompt-tuning has received attention as an efficient tuning method in thelanguage domain, i.e., tuning a prompt that is a few tokens long, while keepingthe large language model frozen, yet achieving comparable performance withconventional fine-tuning. Considering the emerging privacy concerns withlanguage models, we initiate the study of privacy leakage in the setting ofprompt-tuning. We first describe a real-world email service pipeline to providecustomized output for various users via prompt-tuning. Then we propose a novelprivacy attack framework to infer users\u0026rsquo; private information by exploiting theprompt module with user-specific signals. We conduct a comprehensive privacyevaluation on the target pipeline to demonstrate the potential leakage fromprompt-tuning. The results also demonstrate the effectiveness of the proposedattack.\r2023-04-13\nChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review\nSunder Ali Khowaja, Parus Khuwaja, Kapal Dev\nabstract\rabstract: ChatGPT is another large language model (LLM) inline but due to itsperformance and ability to converse effectively, it has gained a hugepopularity amongst research as well as industrial community. Recently, manystudies have been published to show the effectiveness, efficiency, integration,and sentiments of chatGPT and other LLMs. In contrast, this study focuses onthe important aspects that are mostly overlooked, i.e. sustainability, privacy,digital divide, and ethics and suggests that not only chatGPT but everysubsequent entry in the category of conversational bots should undergoSustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. Thispaper discusses in detail about the issues and concerns raised over chatGPT inline with aforementioned characteristics. We support our hypothesis by somepreliminary data collection and visualizations along with hypothesized facts.We also suggest mitigations and recommendations for each of the concerns.Furthermore, we also suggest some policies and recommendations for AI policyact, if designed by the governments.\r2023-04-12\nUnleashing ChatGPT on the Metaverse: Savior or Destroyer?\nPengyuan Zhou\nabstract\rabstract: The incorporation of artificial intelligence (AI) technology, and inparticular natural language processing (NLP), is becoming increasingly vitalfor the development of immersive and interactive metaverse experiences. Onesuch artificial intelligence tool that is gaining traction in the metaverse isChatGPT, a large language model trained by OpenAI. The article delves into thepros and cons of utilizing ChatGPT for metaverse-based education,entertainment, personalization, and support. Dynamic and personalizedexperiences are possible with this technology, but there are also legitimateprivacy, bias, and ethical issues to consider. This article aims to helpreaders understand the possible influence of ChatGPT on the metaverse and howit may be used to effectively create a more immersive and engaging virtualenvironment by evaluating these opportunities and obstacles.\r2023-04-10\nDoes Synthetic Data Generation of LLMs Help Clinical Text Mining?\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu\nabstract\rabstract: Recent advancements in large language models (LLMs) have led to thedevelopment of highly potent models like OpenAI\u0026rsquo;s ChatGPT. These models haveexhibited exceptional performance in a variety of tasks, such as questionanswering, essay composition, and code generation. However, their effectivenessin the healthcare sector remains uncertain. In this study, we seek toinvestigate the potential of ChatGPT to aid in clinical text mining byexamining its ability to extract structured information from unstructuredhealthcare texts, with a focus on biological named entity recognition andrelation extraction. However, our preliminary results indicate that employingChatGPT directly for these tasks resulted in poor performance and raisedprivacy concerns associated with uploading patients\u0026rsquo; information to the ChatGPTAPI. To overcome these limitations, we propose a new training paradigm thatinvolves generating a vast quantity of high-quality synthetic data with labelsutilizing ChatGPT and fine-tuning a local model for the downstream task. Ourmethod has resulted in significant improvements in the performance ofdownstream tasks, improving the F1-score from 23.37% to 63.99% for the namedentity recognition task and from 75.86% to 83.59% for the relation extractiontask. Furthermore, generating data using ChatGPT can significantly reduce thetime and effort required for data collection and labeling, as well as mitigatedata privacy concerns. In summary, the proposed framework presents a promisingsolution to enhance the applicability of LLM models to clinical text mining.\r2023-03-30\nConStruct-VL: Data-Free Continual Structured VL Concepts Learning\nJames Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim, Rameswar Panda, David Cox, Diyi Yang, Zsolt Kira, Rogerio Feris, Leonid Karlinsky\nabstract\rabstract: Recently, large-scale pre-trained Vision-and-Language (VL) foundation modelshave demonstrated remarkable capabilities in many zero-shot downstream tasks,achieving competitive results for recognizing objects defined by as little asshort text prompts. However, it has also been shown that VL models are stillbrittle in Structured VL Concept (SVLC) reasoning, such as the ability torecognize object attributes, states, and inter-object relations. This leads toreasoning mistakes, which need to be corrected as they occur by teaching VLmodels the missing SVLC skills; often this must be done using private datawhere the issue was found, which naturally leads to a data-free continual (notask-id) VL learning setting. In this work, we introduce the first ContinualData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show itis challenging for many existing data-free CL strategies. We, therefore,propose a data-free method comprised of a new approach of AdversarialPseudo-Replay (APR) which generates adversarial reminders of past tasks frompast task models. To use this method efficiently, we also propose a continualparameter-efficient Layered-LoRA (LaLo) neural architecture allowingno-memory-cost access to all past models at train time. We show this approachoutperforms all data-free methods by as much as ~7% while even matching somelevels of experience-replay (prohibitive for applications where data-privacymust be preserved). Our code is publicly available athttps://github.com/jamessealesmith/ConStruct-VL\r2023-03-28\nSynthetically generated text for supervised text analysis\nAndrew Halterman\nabstract\rabstract: Supervised text models are a valuable tool for political scientists butpresent several obstacles to their use, including the expense of hand-labelingdocuments, the difficulty of retrieving rare relevant documents for annotation,and copyright and privacy concerns involved in sharing annotated documents.This article proposes a partial solution to these three issues, in the form ofcontrolled generation of synthetic text with large language models. I provide aconceptual overview of text generation, guidance on when researchers shouldprefer different techniques for generating synthetic text, a discussion ofethics, and a simple technique for improving the quality of synthetic text. Idemonstrate the usefulness of synthetic text with three applications:generating synthetic tweets describing the fighting in Ukraine, synthetic newsarticles describing specified political events for training an event detectionsystem, and a multilingual corpus of populist manifesto statements for traininga sentence-level populism classifier.\r2023-03-22\nMan vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models\nConstantinos Patsakis, Nikolaos Lykousas\nabstract\rabstract: The collection and use of personal data are becoming more common in today\u0026rsquo;sdata-driven culture. While there are many advantages to this, including betterdecision-making and service delivery, it also poses significant ethical issuesaround confidentiality and privacy. Text anonymisation tries to prune and/ormask identifiable information from a text while keeping the remaining contentintact to alleviate privacy concerns. Text anonymisation is especiallyimportant in industries like healthcare, law, as well as research, wheresensitive and personal information is collected, processed, and exchanged underhigh legal and ethical standards. Although text anonymization is widely adopted in practice, it continues toface considerable challenges. The most significant challenge is striking abalance between removing information to protect individuals\u0026rsquo; privacy whilemaintaining the text\u0026rsquo;s usability for future purposes. The question is whetherthese anonymisation methods sufficiently reduce the risk of re-identification,in which an individual can be identified based on the remaining information inthe text. In this work, we challenge the effectiveness of these methods and how weperceive identifiers. We assess the efficacy of these methods against theelephant in the room, the use of AI over big data. While most of the researchis focused on identifying and removing personal information, there is limiteddiscussion on whether the remaining information is sufficient to deanonymiseindividuals and, more precisely, who can do it. To this end, we conduct anexperiment using GPT over anonymised texts of famous people to determinewhether such trained networks can deanonymise them. The latter allows us torevise these methods and introduce a novel methodology that employs LargeLanguage Models to improve the anonymity of texts.\r2023-03-20\nPrivately Fine-Tuning Large Language Models with Differential Privacy\nRouzbeh Behnia, Mohamamdreza Ebrahimi, Jason Pacheco, Balaji Padmanabhan\nabstract\rabstract: Pre-trained Large Language Models (LLMs) are an integral part of modern AIthat have led to breakthrough performances in complex AI tasks. Major AIcompanies with expensive infrastructures are able to develop and train theselarge models with billions and millions of parameters from scratch. Thirdparties, researchers, and practitioners are increasingly adopting thesepre-trained models and fine-tuning them on their private data to accomplishtheir downstream AI tasks. However, it has been shown that an adversary canextract/reconstruct the exact training samples from these LLMs, which can leadto revealing personally identifiable information. The issue has raised deepconcerns about the privacy of LLMs. Differential privacy (DP) provides arigorous framework that allows adding noise in the process of training orfine-tuning LLMs such that extracting the training data becomes infeasible(i.e., with a cryptographically small success probability). While thetheoretical privacy guarantees offered in most extant studies assume learningmodels from scratch through many training iterations in an asymptotic setting,this assumption does not hold in fine-tuning scenarios in which the number oftraining iterations is significantly smaller. To address the gap, we present\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant withfinite-sample privacy guarantees. Our results across four well-establishednatural language understanding (NLU) tasks show that while \\ewtune~adds privacyguarantees to LLM fine-tuning process, it directly contributes to decreasingthe induced noise to up to 5.6% and improves the state-of-the-art LLMsperformance by up to 1.1% across all NLU tasks. We have open-sourced ourimplementations for wide adoption and public testing purposes.\r2023-03-16\nA Short Survey of Viewing Large Language Models in Legal Aspect\nZhongxiang Sun\nabstract\rabstract: Large language models (LLMs) have transformed many fields, including naturallanguage processing, computer vision, and reinforcement learning. These modelshave also made a significant impact in the field of law, where they are beingincreasingly utilized to automate various legal tasks, such as legal judgementprediction, legal document analysis, and legal document writing. However, theintegration of LLMs into the legal field has also raised several legalproblems, including privacy concerns, bias, and explainability. In this survey,we explore the integration of LLMs into the field of law. We discuss thevarious applications of LLMs in legal tasks, examine the legal challenges thatarise from their use, and explore the data resources that can be used tospecialize LLMs in the legal domain. Finally, we discuss several promisingdirections and conclude this paper. By doing so, we hope to provide an overviewof the current state of LLMs in law and highlight the potential benefits andchallenges of their integration.\r2023-03-06\nQuantifying Memorization Across Neural Language Models\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang\nabstract\rabstract: Large language models (LMs) have been shown to memorize parts of theirtraining data, and when prompted appropriately, they will emit the memorizedtraining data verbatim. This is undesirable because memorization violatesprivacy (exposing user data), degrades utility (repeated easy-to-memorize textis often low quality), and hurts fairness (some texts are memorized overothers). We describe three log-linear relationships that quantify the degree to whichLMs emit memorized training data. Memorization significantly grows as weincrease (1) the capacity of a model, (2) the number of times an example hasbeen duplicated, and (3) the number of tokens of context used to prompt themodel. Surprisingly, we find the situation becomes more complicated whengeneralizing these results across model families. On the whole, we find thatmemorization in LMs is more prevalent than previously believed and will likelyget worse as models continues to scale, at least without active mitigations.\r2023-03-01\nDTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction\nRaviteja Anantha, Kriti Bhasin, Daniela de la Parra Aguilar, Prabal Vashisht, Becci Williamson, Srinivas Chappidi\nabstract\rabstract: Personal Digital Assistants (PDAs) - such as Siri, Alexa and GoogleAssistant, to name a few - play an increasingly important role to accessinformation and complete tasks spanning multiple domains, and by diverse groupsof users. A text-to-speech (TTS) module allows PDAs to interact in a natural,human-like manner, and play a vital role when the interaction involves peoplewith visual impairments or other disabilities. To cater to the needs of adiverse set of users, inclusive TTS is important to recognize and pronouncecorrectly text in different languages and dialects. Despite great progress inspeech synthesis, the pronunciation accuracy of named entities in amulti-lingual setting still has a large room for improvement. Existingapproaches to correct named entity (NE) mispronunciations, like retrainingGrapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciationdictionary, require expensive annotation of the ground truth pronunciation,which is also time consuming. In this work, we present a highly-precise,PDA-compatible pronunciation learning framework for the task of TTSmispronunciation detection and correction. In addition, we also propose a novelmispronunciation detection model called DTW-SiameseNet, which employs metriclearning with a Siamese architecture for Dynamic Time Warping (DTW) withtriplet loss. We demonstrate that a locale-agnostic, privacy-preservingsolution to the problem of TTS mispronunciation detection is feasible. Weevaluate our approach on a real-world dataset, and a corpus of NEpronunciations of an anonymized audio dataset of person names recorded byparticipants from 10 different locales. Human evaluation shows our proposedapproach improves pronunciation accuracy on average by ~6% compared to strongphoneme-based and audio-based baselines.\r2023-02-28\nThe (ab)use of Open Source Code to Train Large Language Models\nAli Al-Kaswan, Maliheh Izadi\nabstract\rabstract: In recent years, Large Language Models (LLMs) have gained significantpopularity due to their ability to generate human-like text and their potentialapplications in various fields, such as Software Engineering. LLMs for Code arecommonly trained on large unsanitized corpora of source code scraped from theInternet. The content of these datasets is memorized and emitted by the models,often in a verbatim manner. In this work, we will discuss the security,privacy, and licensing implications of memorization. We argue why the use ofcopyleft code to train LLMs is a legal and ethical dilemma. Finally, we providefour actionable recommendations to address this issue.\r2023-02-25\nOn pitfalls (and advantages) of sophisticated large language models\nAnna Strasser\nabstract\rabstract: Natural language processing based on large language models (LLMs) is abooming field of AI research. After neural networks have proven to outperformhumans in games and practical domains based on pattern recognition, we mightstand now at a road junction where artificial entities might eventually enterthe realm of human communication. However, this comes with serious risks. Dueto the inherent limitations regarding the reliability of neural networks,overreliance on LLMs can have disruptive consequences. Since it will beincreasingly difficult to distinguish between human-written andmachine-generated text, one is confronted with new ethical challenges. Thisbegins with the no longer undoubtedly verifiable human authorship and continueswith various types of fraud, such as a new form of plagiarism. This alsoconcerns the violation of privacy rights, the possibility of circulatingcounterfeits of humans, and, last but not least, it makes a massive spread ofmisinformation possible.\r2023-02-23\nPrivately Customizing Prefinetuning to Better Match User Data in Federated Learning\nCharlie Hou, Hongyuan Zhan, Akshat Shrivastava, Sid Wang, Aleksandr Livshits, Giulia Fanti, Daniel Lazar\nabstract\rabstract: In Federated Learning (FL), accessing private client data incurscommunication and privacy costs. As a result, FL deployments commonlyprefinetune pretrained foundation models on a (large, possibly public) datasetthat is held by the central server; they then FL-finetune the model on aprivate, federated dataset held by clients. Evaluating prefinetuning datasetquality reliably and privately is therefore of high importance. To this end, wepropose FreD (Federated Private Fr'echet Distance) \u0026ndash; a privately computeddistance between a prefinetuning dataset and federated datasets. Intuitively,it privately computes and compares a Fr'echet distance between embeddingsgenerated by a large language model on both the central (public) dataset andthe federated private client data. To make this computation privacy-preserving,we use distributed, differentially-private mean and covariance estimators. Weshow empirically that FreD accurately predicts the best prefinetuning datasetat minimal privacy cost. Altogether, using FreD we demonstrate aproof-of-concept for a new approach in private FL training: (1) customize aprefinetuning dataset to better match user data (2) prefinetune (3) performFL-finetuning.\r2023-02-17\nPLACES: Prompting Language Models for Social Conversation Synthesis\nMaximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, Dilek Hakkani-Tur\nabstract\rabstract: Collecting high quality conversational data can be very expensive for mostapplications and infeasible for others due to privacy, ethical, or similarconcerns. A promising direction to tackle this problem is to generate syntheticdialogues by prompting large language models. In this work, we use a small setof expert-written conversations as in-context examples to synthesize a socialconversation dataset using prompting. We perform several thorough evaluationsof our synthetic conversations compared to human-collected conversations. Thisincludes various dimensions of conversation quality with human evaluationdirectly on the synthesized conversations, and interactive human evaluation ofchatbots fine-tuned on the synthetically generated dataset. We additionallydemonstrate that this prompting approach is generalizable to multi-partyconversations, providing potential to create new synthetic data for multi-partytasks. Our synthetic multi-party conversations were rated more favorably acrossall measured dimensions compared to conversation excerpts sampled from ahuman-collected multi-party dataset.\rUncertainty-aware Self-training for Low-resource Neural Sequence Labeling\nJianing Wang, Chengyu Wang, Jun Huang, Ming Gao, Aoying Zhou\nabstract\rabstract: Neural sequence labeling (NSL) aims at assigning labels for input languagetokens, which covers a broad range of applications, such as named entityrecognition (NER) and slot filling, etc. However, the satisfying resultsachieved by traditional supervised-based approaches heavily depend on the largeamounts of human annotation data, which may not be feasible in real-worldscenarios due to data privacy and computation efficiency issues. This paperpresents SeqUST, a novel uncertain-aware self-training framework for NSL toaddress the labeled data scarcity issue and to effectively utilize unlabeleddata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neuralnetwork (BNN) to perform uncertainty estimation at the token level and thenselect reliable language tokens from unlabeled data based on the modelconfidence and certainty. A well-designed masked sequence labeling task with anoise-robust loss supports robust training, which aims to suppress the problemof noisy pseudo labels. In addition, we develop a Gaussian-based consistencyregularization technique to further improve the model robustness onGaussian-distributed perturbed representations. This effectively alleviates theover-fitting dilemma originating from pseudo-labeled augmented data. Extensiveexperiments over six benchmarks demonstrate that our SeqUST frameworkeffectively improves the performance of self-training, and consistentlyoutperforms strong baselines by a large margin in low-resource scenarios\r2023-02-13\nTargeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge\nAli Al-Kaswan, Maliheh Izadi, Arie van Deursen\nabstract\rabstract: Previous work has shown that Large Language Models are susceptible toso-called data extraction attacks. This allows an attacker to extract a samplethat was contained in the training data, which has massive privacyimplications. The construction of data extraction attacks is challenging,current attacks are quite inefficient, and there exists a significant gap inthe extraction capabilities of untargeted attacks and memorization. Thus,targeted attacks are proposed, which identify if a given sample from thetraining data, is extractable from a model. In this work, we apply a targeteddata extraction attack to the SATML2023 Language Model Training Data ExtractionChallenge. We apply a two-step approach. In the first step, we maximise therecall of the model and are able to extract the suffix for 69% of the samples.In the second step, we use a classifier-based Membership Inference Attack onthe generations. Our AutoSklearn classifier achieves a precision of 0.841. Thefull approach reaches a score of 0.405 recall at a 10% false positive rate,which is an improvement of 34% over the baseline of 0.301.\r2023-02-12\nChat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models\nPaula Maddigan, Teo Susnjak\nabstract\rabstract: The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies.\r2023-02-09\nOffsite-Tuning: Transfer Learning without Full Model\nGuangxuan Xiao, Ji Lin, Song Han\nabstract\rabstract: Transfer learning is important for foundation models to adapt to downstreamtasks. However, many foundation models are proprietary, so users must sharetheir data with model owners to fine-tune the models, which is costly and raiseprivacy concerns. Moreover, fine-tuning large foundation models iscomputation-intensive and impractical for most downstream users. In this paper,we propose Offsite-Tuning, a privacy-preserving and efficient transfer learningframework that can adapt billion-parameter foundation models to downstream datawithout access to the full model. In offsite-tuning, the model owner sends alight-weight adapter and a lossy compressed emulator to the data owner, whothen fine-tunes the adapter on the downstream data with the emulator\u0026rsquo;sassistance. The fine-tuned adapter is then returned to the model owner, whoplugs it into the full model to create an adapted foundation model.Offsite-tuning preserves both parties\u0026rsquo; privacy and is computationally moreefficient than the existing fine-tuning methods that require access to the fullmodel weights. We demonstrate the effectiveness of offsite-tuning on variouslarge language and vision foundation models. Offsite-tuning can achievecomparable accuracy as full model fine-tuning while being privacy-preservingand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code isavailable at https://github.com/mit-han-lab/offsite-tuning.\r2023-01-26\nCommunication-Efficient Learning of Deep Networks from Decentralized Data\nH. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Agüera y Arcas\nabstract\rabstract: Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksbased on iterative model averaging, and conduct an extensive empiricalevaluation, considering five different model architectures and four datasets.These experiments demonstrate the approach is robust to the unbalanced andnon-IID data distributions that are a defining characteristic of this setting.Communication costs are the principal constraint, and we show a reduction inrequired communication rounds by 10-100x as compared to synchronized stochasticgradient descent.\r2023-01-24\nFedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning\nHaodong Zhao, Wei Du, Fangqi Li, Peixuan Li, Gongshen Liu\nabstract\rabstract: Federated learning (FL) has enabled global model training on decentralizeddata in a privacy-preserving way by aggregating model updates. However, formany natural language processing (NLP) tasks that utilize pre-trained languagemodels (PLMs) with large numbers of parameters, there are considerablecommunication costs associated with FL. Recently, prompt tuning, which tunessome soft prompts without modifying PLMs, has achieved excellent performance asa new learning paradigm. Therefore we want to combine the two methods andexplore the effect of prompt tuning under FL. In this paper, we propose\u0026quot;FedPrompt\u0026quot; to study prompt tuning in a model split aggregation way using FL,and prove that split aggregation greatly reduces the communication cost, only0.01% of the PLMs\u0026rsquo; parameters, with little decrease on accuracy both on IID andNon-IID data distribution. This improves the efficiency of FL method while alsoprotecting the data privacy in prompt tuning. In addition, like PLMs, promptsare uploaded and downloaded between public platforms and personal users, so wetry to figure out whether there is still a backdoor threat using only softprompts in FL scenarios. We further conduct backdoor attacks by data poisoningon FedPrompt. Our experiments show that normal backdoor attack can not achievea high attack success rate, proving the robustness of FedPrompt. We hope thiswork can promote the application of prompt in FL and raise the awareness of thepossible security threats.\r2022-12-20\nDeduplicating Training Data Mitigates Privacy Risks in Language Models\nNikhil Kandpal, Eric Wallace, Colin Raffel\nabstract\rabstract: Past work has shown that large language models are susceptible to privacyattacks, where adversaries generate sequences from a trained model and detectwhich sequences are memorized from the training set. In this work, we show thatthe success of these attacks is largely due to duplication in commonly usedweb-scraped training sets. We first show that the rate at which language modelsregenerate training sequences is superlinearly related to a sequence\u0026rsquo;s count inthe training set. For instance, a sequence that is present 10 times in thetraining data is on average generated ~1000 times more often than a sequencethat is present only once. We next show that existing methods for detectingmemorized sequences have near-chance accuracy on non-duplicated trainingsequences. Finally, we find that after applying methods to deduplicate trainingdata, language models are considerably more secure against these types ofprivacy attacks. Taken together, our results motivate an increased focus ondeduplication in privacy-sensitive applications and a reevaluation of thepracticality of existing privacy attacks.\r2022-12-16\nPlanting and Mitigating Memorized Content in Predictive-Text Language Models\nC. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa\nabstract\rabstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both \u0026ldquo;heuristic\u0026rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define\u0026quot;sensitive\u0026quot; or \u0026ldquo;private\u0026rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.\rFewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks\nWeilong Dong, Xinwei Wu, Junzhuo Li, Shuangzhi Wu, Chao Bian, Deyi Xiong\nabstract\rabstract: Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods.\r2022-12-02\nSemantics-Preserved Distortion for Personal Privacy Protection in Information Management\nJiajia Li, Letian Peng, Ping Wang, Zuchao Li, Xueyi Li, Hai Zhao\nabstract\rabstract: Although machine learning and especially deep learning methods have played animportant role in the field of information management, privacy protection is animportant and concerning topic in current machine learning models. Ininformation management field, a large number of texts containing personalinformation are produced by users every day. As the model training oninformation from users is likely to invade personal privacy, many methods havebeen proposed to block the learning and memorizing of the sensitive data in rawtexts. In this paper, we try to do this more linguistically via distorting thetext while preserving the semantics. In practice, we leverage a recently ourproposed metric, Neighboring Distribution Divergence, to evaluate the semanticpreservation during the distortion. Based on the metric, we propose twoframeworks for semantics-preserved distortion, a generative one and asubstitutive one. We conduct experiments on named entity recognition,constituency parsing, and machine reading comprehension tasks. Results from ourexperiments show the plausibility and efficiency of our distortion as a methodfor personal privacy protection. Moreover, we also evaluate the attributeattack on three privacy-related tasks in the current natural languageprocessing field, and the results show the simplicity and effectiveness of ourdata-based improvement approach compared to the structural improvementapproach. Further, we also investigate the effects of privacy protection inspecific medical information management in this work and show that the medicalinformation pre-training model using our approach can effectively reduce thememory of patients and symptoms, which fully demonstrates the practicality ofour approach.\r2022-11-30\nA Case for Business Process-Specific Foundation Models\nYara Rizk, Praveen Venkateswaran, Vatche Isahagian, Vinod Muthusamy\nabstract\rabstract: The inception of large language models has helped advance state-of-the-artperformance on numerous natural language tasks. This has also opened the doorfor the development of foundation models for other domains and data modalitiessuch as images, code, and music. In this paper, we argue that business processdata representations have unique characteristics that warrant the developmentof a new class of foundation models to handle tasks like process mining,optimization, and decision making. These models should also tackle the uniquechallenges of applying AI to business processes which include data scarcity,multi-modal representations, domain specific terminology, and privacy concerns.\r2022-11-22\nGDPR Compliant Collection of Therapist-Patient-Dialogues\nTobias Mayer, Neha Warikoo, Oliver Grimm, Andreas Reif, Iryna Gurevych\nabstract\rabstract: According to the Global Burden of Disease list provided by the World HealthOrganization (WHO), mental disorders are among the most debilitatingdisorders.To improve the diagnosis and the therapy effectiveness in recentyears, researchers have tried to identify individual biomarkers. Gatheringneurobiological data however, is costly and time-consuming. Another potentialsource of information, which is already part of the clinical routine, aretherapist-patient dialogues. While there are some pioneering worksinvestigating the role of language as predictors for various therapeuticparameters, for example patient-therapist alliance, there are no large-scalestudies. A major obstacle to conduct these studies is the availability ofsizeable datasets, which are needed to train machine learning models. Whilethese conversations are part of the daily routine of clinicians, gathering themis usually hindered by various ethical (purpose of data usage), legal (dataprivacy) and technical (data formatting) limitations. Some of these limitationsare particular to the domain of therapy dialogues, like the increaseddifficulty in anonymisation, or the transcription of the recordings. In thispaper, we elaborate on the challenges we faced in starting our collection oftherapist-patient dialogues in a psychiatry clinic under the General DataPrivacy Regulation of the European Union with the goal to use the data forNatural Language Processing (NLP) research. We give an overview of each step inour procedure and point out the potential pitfalls to motivate further researchin this field.\r2022-11-15\nA Closer Look at the Calibration of Differentially Private Learners\nHanlin Zhang, Xuechen Li, Prithviraj Sen, Salim Roukos, Tatsunori Hashimoto\nabstract\rabstract: We systematically study the calibration of classifiers trained withdifferentially private stochastic gradient descent (DP-SGD) and observemiscalibration across a wide range of vision and language tasks. Our analysisidentifies per-example gradient clipping in DP-SGD as a major cause ofmiscalibration, and we show that existing approaches for improving calibrationwith differential privacy only provide marginal improvements in calibrationerror while occasionally causing large degradations in accuracy. As a solution,we show that differentially private variants of post-processing calibrationmethods such as temperature scaling and Platt scaling are surprisinglyeffective and have negligible utility cost to the overall model. Across 7tasks, temperature scaling and Platt scaling with DP-SGD result in an average3.1-fold reduction in the in-domain expected calibration error and only incurat most a minor percent drop in accuracy.\r2022-11-10\nLarge Language Models Can Be Strong Differentially Private Learners\nXuechen Li, Florian Tramèr, Percy Liang, Tatsunori Hashimoto\nabstract\rabstract: Differentially Private (DP) learning has seen limited success for buildinglarge deep learning models of text, and straightforward attempts at applyingDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks haveresulted in large performance drops and high computational overhead. We showthat this performance drop can be mitigated with (1) the use of largepretrained language models; (2) non-standard hyperparameters that suit DPoptimization; and (3) fine-tuning objectives which are aligned with thepretraining procedure. With the above, we obtain NLP models that outperformstate-of-the-art DP-trained models under the same privacy budget and strongnon-private baselines \u0026ndash; by directly fine-tuning pretrained models with DPoptimization on moderately-sized corpora. To address the computationalchallenge of running DP-SGD with large Transformers, we propose a memory savingtechnique that allows clipping in DP-SGD to run without instantiatingper-example gradients for any linear layer in the model. The technique enablesprivately training Transformers with almost the same memory cost as non-privatetraining at a modest run-time overhead. Contrary to conventional wisdom that DPoptimization fails at learning high-dimensional models (due to noise thatscales with dimension) empirical results reveal that private learning withpretrained language models doesn\u0026rsquo;t tend to suffer from dimension-dependentperformance degradation. Code to reproduce results can be found athttps://github.com/lxuechen/private-transformers.\r2022-11-05\nPrivacy-Preserving Models for Legal Natural Language Processing\nYing Yin, Ivan Habernal\nabstract\rabstract: Pre-training large transformer models with in-domain data improves domainadaptation and helps gain performance on the domain-specific downstream tasks.However, sharing models pre-trained on potentially sensitive data is prone toadversarial privacy attacks. In this paper, we asked to which extent we canguarantee privacy of pre-training data and, at the same time, achieve betterdownstream performance on legal tasks without the need of additional labeleddata. We extensively experiment with scalable self-supervised learning oftransformer models under the formal paradigm of differential privacy and showthat under specific training configurations we can improve downstreamperformance without sacrifying privacy protection for the in-domain data. Ourmain contribution is utilizing differential privacy for large-scalepre-training of transformer language models in the legal NLP domain, which, tothe best of our knowledge, has not been addressed before.\r2022-11-04\nMemorization in NLP Fine-tuning Methods\nFatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, Taylor Berg-Kirkpatrick\nabstract\rabstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the\u0026quot;pre-train and fine-tune\u0026quot; paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.\r2022-10-27\nJust Fine-tune Twice: Selective Differential Privacy for Large Language Models\nWeiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, Zhou Yu\nabstract\rabstract: Protecting large language models from privacy leakage is becomingincreasingly crucial with their wide adoption in real-world products. Yetapplying differential privacy (DP), a canonical notion with provable privacyguarantees for machine learning models, to those models remains challenging dueto the trade-off between model utility and privacy loss. Utilizing the factthat sensitive information in language data tends to be sparse, Shi et al.(2021) formalized a DP notion extension called Selective Differential Privacy(SDP) to protect only the sensitive tokens defined by a policy function.However, their algorithm only works for RNN-based models. In this paper, wedevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP forstate-of-the-art large transformer-based models. Our method is easy toimplement: it first fine-tunes the model with redacted in-domain data, and thenfine-tunes it again with the original in-domain data using a private trainingmechanism. Furthermore, we study the scenario of imperfect implementation ofpolicy functions that misses sensitive tokens and develop systematic methods tohandle it. Experiments show that our method achieves strong utility compared toprevious baselines. We also analyze the SDP privacy guarantee empirically withthe canary insertion attack.\r2022-10-26\nWhen Does Differentially Private Learning Not Suffer in High Dimensions?\nXuechen Li, Daogao Liu, Tatsunori Hashimoto, Huseyin A. Inan, Janardhan Kulkarni, Yin Tat Lee, Abhradeep Guha Thakurta\nabstract\rabstract: Large pretrained models can be privately fine-tuned to achieve performanceapproaching that of non-private models. A common theme in these results is thesurprising observation that high-dimensional models can achieve favorableprivacy-utility trade-offs. This seemingly contradicts known results on themodel-size dependence of differentially private convex learning and raises thefollowing research question: When does the performance of differentiallyprivate learning not degrade with increasing model size? We identify that themagnitudes of gradients projected onto subspaces is a key factor thatdetermines performance. To precisely characterize this for private convexlearning, we introduce a condition on the objective that we term\\emph{restricted Lipschitz continuity} and derive improved bounds for theexcess empirical and population risks that are dimension-independent underadditional conditions. We empirically show that in private fine-tuning of largelanguage models, gradients obtained during fine-tuning are mostly controlled bya few principal components. This behavior is similar to conditions under whichwe obtain dimension-independent bounds in convex settings. Our theoretical andempirical results together provide a possible explanation for recent successesin large-scale private fine-tuning. Code to reproduce our results can be foundat\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.\r2022-10-25\nLeveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios\nZhuohao Chen, Nikolaos Flemotomos, Zac E. Imel, David C. Atkins, Shrikanth Narayanan\nabstract\rabstract: In psychotherapy interactions, the quality of a session is assessed bycodifying the communicative behaviors of participants during the conversationthrough manual observation and annotation. Developing computational approachesfor automated behavioral coding can reduce the burden on human coders andfacilitate the objective evaluation of the intervention. In the real world,however, implementing such algorithms is associated with data sparsitychallenges since privacy concerns lead to limited available in-domain data. Inthis paper, we leverage a publicly available conversation-based dataset andtransfer knowledge to the low-resource behavioral coding task by performing anintermediate language model training via meta-learning. We introduce a taskaugmentation method to produce a large number of \u0026ldquo;analogy tasks\u0026rdquo; - taskssimilar to the target one - and demonstrate that the proposed frameworkpredicts target behaviors more accurately than all the other baseline models.\r2022-10-20\nAre Large Pre-Trained Language Models Leaking Your Personal Information?\nJie Huang, Hanyin Shao, Kevin Chen-Chuan Chang\nabstract\rabstract: Are Large Pre-Trained Language Models Leaking Your Personal Information? Inthis paper, we analyze whether Pre-Trained Language Models (PLMs) are prone toleaking personal information. Specifically, we query PLMs for email addresseswith contexts of the email address or prompts containing the owner\u0026rsquo;s name. Wefind that PLMs do leak personal information due to memorization. However, sincethe models are weak at association, the risk of specific personal informationbeing extracted by attackers is low. We hope this work could help the communityto better understand the privacy risk of PLMs and bring new insights to makePLMs safe.\r2022-10-18\nRecovering Private Text in Federated Learning of Language Models\nSamyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, Danqi Chen\nabstract\rabstract: Federated learning allows distributed users to collaboratively train a modelwhile keeping each user\u0026rsquo;s data private. Recently, a growing body of work hasdemonstrated that an eavesdropping attacker can effectively recover image datafrom gradients transmitted during federated learning. However, little progresshas been made in recovering text data. In this paper, we present a novel attackmethod FILM for federated learning of language models (LMs). For the firsttime, we show the feasibility of recovering text from large batch sizes of upto 128 sentences. Unlike image-recovery methods that are optimized to matchgradients, we take a distinct approach that first identifies a set of wordsfrom gradients and then directly reconstructs sentences based on beam searchand a prior-based reordering strategy. We conduct the FILM attack on severallarge-scale datasets and show that it can successfully reconstruct singlesentences with high fidelity for large batch sizes and even multiple sentencesif applied iteratively. We evaluate three defense methods: gradient pruning,DPSGD, and a simple approach to freeze word embeddings that we propose. We showthat both gradient pruning and DPSGD lead to a significant drop in utility.However, if we fine-tune a public pre-trained LM on private text withoutupdating word embeddings, it can effectively defend the attack with minimaldata utility loss. Together, we hope that our results can encourage thecommunity to rethink the privacy concerns of LM training and its standardpractices in the future.\r2022-10-04\nDifferentially Private Bias-Term only Fine-tuning of Foundation Models\nZhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis\nabstract\rabstract: We study the problem of differentially private (DP) fine-tuning of largepre-trained models \u0026ndash; a recent privacy-preserving approach suitable for solvingdownstream tasks with sensitive data. Existing work has demonstrated that highaccuracy is possible under strong privacy constraint, yet requires significantcomputational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), whichmatches the state-of-the-art accuracy for DP algorithms and the efficiency ofthe standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the networkarchitecture), parameter efficient (only training about $0.1%$ of theparameters), and computation efficient (almost removing the overhead caused byDP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiTis $2\\sim 30\\times$ faster and uses $2\\sim 8\\times$ less memory than DP fullfine-tuning, even faster than the standard full fine-tuning. This amazingefficiency enables us to conduct DP fine-tuning on language and vision taskswith long-sequence texts and high-resolution images, which were computationallydifficult using existing methods.\r2022-09-08\nDifferentially Private Decoding in Large Language Models\nJimit Majmudar, Christophe Dupuy, Charith Peris, Sami Smaili, Rahul Gupta, Richard Zemel\nabstract\rabstract: Recent large-scale natural language processing (NLP) systems use apre-trained Large Language Model (LLM) on massive and diverse corpora as aheadstart. In practice, the pre-trained model is adapted to a wide array oftasks via fine-tuning on task-specific datasets. LLMs, while effective, havebeen shown to memorize instances of training data thereby potentially revealingprivate information processed during pre-training. The potential leakage mightfurther propagate to the downstream tasks for which LLMs are fine-tuned. On theother hand, privacy-preserving algorithms usually involve retraining fromscratch, which is prohibitively expensive for LLMs. In this work, we propose asimple, easy to interpret, and computationally lightweight perturbationmechanism to be applied to an already trained model at the decoding stage. Ourperturbation mechanism is model-agnostic and can be used in conjunction withany LLM. We provide theoretical analysis showing that the proposed mechanism isdifferentially private, and experimental results showing a privacy-utilitytrade-off.\r2022-07-18\nTraining Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices\nMingbin Xu, Congzheng Song, Ye Tian, Neha Agrawal, Filip Granqvist, Rogier van Dalen, Xiao Zhang, Arturo Argueta, Shiyi Han, Yaqiao Deng, Leo Liu, Anmol Walia, Alex Jin\nabstract\rabstract: Federated Learning (FL) is a technique to train models using data distributedacross devices. Differential Privacy (DP) provides a formal privacy guaranteefor sensitive data. Our goal is to train a large neural network language model(NNLM) on compute-constrained devices while preserving privacy using FL and DP.However, the DP-noise introduced to the model increases as the model sizegrows, which often prevents convergence. We propose Partial Embedding Updates(PEU), a novel technique to decrease noise by decreasing payload size.Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise ContrastiveEstimation (NCE) to reduce the memory demands of large models oncompute-constrained devices. This combination of techniques makes it possibleto train large-vocabulary language models while preserving accuracy andprivacy.\r2022-07-14\nDifferentially Private Fine-tuning of Language Models\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang\nabstract\rabstract: We give simpler, sparser, and faster algorithms for differentially privatefine-tuning of large-scale pre-trained language models, which achieve thestate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.We propose a meta-framework for this problem, inspired by the recent success ofhighly parameter-efficient methods for fine-tuning. Our experiments show thatdifferentially private adaptations of these approaches outperform previousprivate algorithms in three important dimensions: utility, privacy, and thecomputational and memory cost of private training. On many commonly studieddatasets, the utility of private models approaches that of non-private models.For example, on the MNLI dataset we achieve an accuracy of $87.8%$ usingRoBERTa-Large and $83.5%$ using RoBERTa-Base with a privacy budget of$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Largeachieves an accuracy of $90.2%$. Our findings are similar for natural languagegeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8respectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas thenon-private baseline is $48.1$. All our experiments suggest that larger modelsare better suited for private fine-tuning: while they are well known to achievesuperior accuracy non-privately, we find that they also better maintain theiraccuracy when privacy is introduced.\r2022-06-28\nNegDL: Privacy-Preserving Deep Learning Based on Negative Database\nDongdong Zhao, Pingchuan Zhang, Jianwen Xiang, Jing Tian\nabstract\rabstract: In the era of big data, deep learning has become an increasingly populartopic. It has outstanding achievements in the fields of image recognition,object detection, and natural language processing et al. The first priority ofdeep learning is exploiting valuable information from a large amount of data,which will inevitably induce privacy issues that are worthy of attention.Presently, several privacy-preserving deep learning methods have been proposed,but most of them suffer from a non-negligible degradation of either efficiencyor accuracy. Negative database (\\textit{NDB}) is a new type of datarepresentation which can protect data privacy by storing and utilizing thecomplementary form of original data. In this paper, we propose aprivacy-preserving deep learning method named NegDL based on \\textit{NDB}.Specifically, private data are first converted to \\textit{NDB} as the input ofdeep learning models by a generation algorithm called \\textit{QK}-hiddenalgorithm, and then the sketches of \\textit{NDB} are extracted for training andinference. We demonstrate that the computational complexity of NegDL is thesame as the original deep learning model without privacy protection.Experimental results on Breast Cancer, MNIST, and CIFAR-10 benchmark datasetsdemonstrate that the accuracy of NegDL could be comparable to the original deeplearning model in most cases, and it performs better than the method based ondifferential privacy.\r2022-06-23\nProvably Confidential Language Modelling\nXuandong Zhao, Lei Li, Yu-Xiang Wang\nabstract\rabstract: Large language models are shown to memorize privacy information such associal security numbers in training data. Given the sheer scale of the trainingcorpus, it is challenging to screen and filter these privacy data, eithermanually or automatically. In this paper, we propose Confidentially RedactedTraining (CRT), a method to train language generation models while protectingthe confidential segments. We borrow ideas from differential privacy (whichsolves a related but distinct problem) and show that our method is able toprovably prevent unintended memorization by randomizing parts of the trainingprocess. Moreover, we show that redaction with an approximately correctscreening policy amplifies the confidentiality guarantee. We implement themethod for both LSTM and GPT language models. Our experimental results showthat the models trained by CRT obtain almost the same perplexity whilepreserving strong confidentiality.\r2022-06-03\nDifferentially Private Model Compression\nFatemehsadat Mireshghallah, Arturs Backurs, Huseyin A Inan, Lukas Wutschitz, Janardhan Kulkarni\nabstract\rabstract: Recent papers have shown that large pre-trained language models (LLMs) suchas BERT, GPT-2 can be fine-tuned on private data to achieve performancecomparable to non-private models for many downstream Natural LanguageProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.The inference cost of these models \u0026ndash; which consist of hundreds of millions ofparameters \u0026ndash; however, can be prohibitively large. Hence, often in practice,LLMs are compressed before they are deployed in specific applications. In thispaper, we initiate the study of differentially private model compression andpropose frameworks for achieving 50% sparsity levels while maintaining nearlyfull performance. We demonstrate these ideas on standard GLUE benchmarks usingBERT models, setting benchmarks for future research on this topic.\r2022-05-29\nCPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\nYirong Chen, Weiquan Fan, Xiaofen Xing, Jianxin Pang, Minlie Huang, Wenjing Han, Qianfeng Tie, Xiangmin Xu\nabstract\rabstract: Human language expression is based on the subjective construal of thesituation instead of the objective truth conditions, which means that speakers\u0026rsquo;personalities and emotions after cognitive processing have an importantinfluence on conversation. However, most existing datasets for conversationalAI ignore human personalities and emotions, or only consider part of them. It\u0026rsquo;sdifficult for dialogue systems to understand speakers\u0026rsquo; personalities andemotions although large-scale pre-training language models have been widelyused. In order to consider both personalities and emotions in the process ofconversation generation, we propose CPED, a large-scale Chinese personalizedand emotional dialogue dataset, which consists of multi-source knowledgerelated to empathy and personal characteristic. These knowledge covers gender,Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPEDcontains more than 12K dialogues of 392 speakers from 40 TV shows. We releasethe textual dataset with audio features and video features according to thecopyright claims, privacy issues, terms of service of video platforms. Weprovide detailed description of the CPED construction process and introducethree tasks for conversational AI, including personality recognition, emotionrecognition in conversations as well as personalized and emotional conversationgeneration. Finally, we provide baseline systems for these tasks and considerthe function of speakers\u0026rsquo; personalities and emotions on conversation. Ourmotivation is to propose a dataset to be widely adopted by the NLP community asa new open benchmark for conversational AI research. The full dataset isavailable at https://github.com/scutcyr/CPED.\r2022-05-19\nNebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\nYang Xiang, Zhihua Wu, Weibao Gong, Siyu Ding, Xianjie Mo, Yuang Liu, Shuohuan Wang, Peng Liu, Yongshuai Hou, Long Li, Bin Wang, Shaohuai Shi, Yaqian Han, Yue Yu, Ge Li, Yu Sun, Yanjun Ma, Dianhai Yu\nabstract\rabstract: The ever-growing model size and scale of compute have attracted increasinginterests in training deep learning models over multiple nodes. However, whenit comes to training on cloud clusters, especially across remote clusters, hugechallenges are faced. In this work, we introduce a general framework, Nebula-I,for collaboratively training deep learning models over remote heterogeneousclusters, the connections between which are low-bandwidth wide area networks(WANs). We took natural language processing (NLP) as an example to show howNebula-I works in different training phases that include: a) pre-training amultilingual language model using two remote clusters; and b) fine-tuning amachine translation model using knowledge distilled from pre-trained models,which run through the most popular paradigm of recent deep learning. To balancethe accuracy and communication efficiency, in Nebula-I, parameter-efficienttraining strategies, hybrid parallel computing methods and adaptivecommunication acceleration techniques are jointly applied. Meanwhile, securitystrategies are employed to guarantee the safety, reliability and privacy inintra-cluster computation and inter-cluster communication. Nebula-I isimplemented with the PaddlePaddle deep learning framework, which can supportcollaborative training over heterogeneous hardware, e.g. GPU and NPU.Experiments demonstrate that the proposed framework could substantiallymaximize the training efficiency while preserving satisfactory NLP performance.By using Nebula-I, users can run large-scale training tasks over cloud clusterswith minimum developments, and the utility of existed large pre-trained modelscould be further promoted. We also introduced new state-of-the-art results oncross-lingual natural language inference tasks, which are generated based upona novel learning framework and Nebula-I.\r2022-05-17\nTranslatotron 2: High-quality direct speech-to-speech translation with voice preservation\nYe Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz\nabstract\rabstract: We present Translatotron 2, a neural direct speech-to-speech translationmodel that can be trained end-to-end. Translatotron 2 consists of a speechencoder, a linguistic decoder, an acoustic synthesizer, and a single attentionmodule that connects them together. Experimental results on three datasetsconsistently show that Translatotron 2 outperforms the original Translatotronby a large margin on both translation quality (up to +15.5 BLEU) and speechgeneration quality, and approaches the same of cascade systems. In addition, wepropose a simple method for preserving speakers\u0026rsquo; voices from the source speechto the translation speech in a different language. Unlike existing approaches,the proposed method is able to preserve each speaker\u0026rsquo;s voice on speaker turnswithout requiring for speaker segmentation. Furthermore, compared to existingapproaches, it better preserves speaker\u0026rsquo;s privacy and mitigates potentialmisuse of voice cloning for creating spoofing audio artifacts.\r2022-05-08\nA Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges\nZhenghua Chen, Min Wu, Alvin Chan, Xiaoli Li, Yew-Soon Ong\nabstract\rabstract: Artificial Intelligence (AI) is a fast-growing research and development (R\u0026amp;D)discipline which is attracting increasing attention because of its promises tobring vast benefits for consumers and businesses, with considerable benefitspromised in productivity growth and innovation. To date it has reportedsignificant accomplishments in many areas that have been deemed as challengingfor machines, ranging from computer vision, natural language processing, audioanalysis to smart sensing and many others. The technical trend in realizing thesuccesses has been towards increasing complex and large size AI models so as tosolve more complex problems at superior performance and robustness. This rapidprogress, however, has taken place at the expense of substantial environmentalcosts and resources. Besides, debates on the societal impacts of AI, such asfairness, safety and privacy, have continued to grow in intensity. These issueshave presented major concerns pertaining to the sustainable development of AI.In this work, we review major trends in machine learning approaches that canaddress the sustainability problem of AI. Specifically, we examine emerging AImethodologies and algorithms for addressing the sustainability issue of AI intwo major aspects, i.e., environmental sustainability and social sustainabilityof AI. We will also highlight the major limitations of existing studies andpropose potential research challenges and directions for the development ofnext generation of sustainable AI techniques. We believe that this technicalreview can help to promote a sustainable development of AI R\u0026amp;D activities forthe research community.\r2022-05-06\nFedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks\nBill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr\nabstract\rabstract: Increasing concerns and regulations about data privacy and sparsitynecessitate the study of privacy-preserving, decentralized learning methods fornatural language processing (NLP) tasks. Federated learning (FL) providespromising approaches for a large number of clients (e.g., personal devices ororganizations) to collaboratively learn a shared global model to benefit allclients while allowing users to keep their data locally. Despite interest instudying FL methods for NLP tasks, a systematic comparison and analysis islacking in the literature. Herein, we present the FedNLP, a benchmarkingframework for evaluating federated learning methods on four different taskformulations: text classification, sequence tagging, question answering, andseq2seq. We propose a universal interface between Transformer-based languagemodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) undervarious non-IID partitioning strategies. Our extensive experiments with FedNLPprovide empirical comparisons between FL methods and helps us better understandthe inherent challenges of this direction. The comprehensive analysis points tointriguing and exciting future research aimed at developing FL methods for NLPtasks.\r2022-04-26\nYou Don\u0026rsquo;t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers\u0026rsquo; Private Personas\nHaoran Li, Yangqiu Song, Lixin Fan\nabstract\rabstract: Social chatbots, also known as chit-chat chatbots, evolve rapidly with largepretrained language models. Despite the huge progress, privacy concerns havearisen recently: training data of large language models can be extracted viamodel inversion attacks. On the other hand, the datasets used for trainingchatbots contain many private conversations between two individuals. In thiswork, we further investigate the privacy leakage of the hidden states ofchatbots trained by language modeling which has not been well studied yet. Weshow that speakers\u0026rsquo; personas can be inferred through a simple neural networkwith high accuracy. To this end, we propose effective defense objectives toprotect persona leakage from hidden states. We conduct extensive experiments todemonstrate that our proposed defense objectives can greatly reduce the attackaccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preservelanguage models\u0026rsquo; powerful generation ability.\r2022-04-20\nYou Are What You Write: Preserving Privacy in the Era of Large Language Models\nRichard Plant, Valerio Giuffrida, Dimitra Gkatzia\nabstract\rabstract: Large scale adoption of large language models has introduced a new era ofconvenient knowledge transfer for a slew of natural language processing tasks.However, these models also run the risk of undermining user trust by exposingunwanted information about the data subjects, which may be extracted by amalicious party, e.g. through adversarial attacks. We present an empiricalinvestigation into the extent of the personal information encoded intopre-trained representations by a range of popular models, and we show apositive correlation between the complexity of a model, the amount of data usedin pre-training, and data leakage. In this paper, we present the first widecoverage evaluation and comparison of some of the most popularprivacy-preserving algorithms, on a large, multi-lingual dataset on sentimentanalysis annotated with demographic information (location, age and gender). Theresults show since larger and more complex models are more prone to leakingprivate information, use of privacy-preserving methods is highly desirable. Wealso find that highly privacy-preserving technologies like differential privacy(DP) can have serious model utility effects, which can be ameliorated usinghybrid or metric-DP techniques.\r2022-04-06\nDifferentially Private Set Union\nSivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, Sergey Yekhanin\nabstract\rabstract: We study the basic operation of set union in the global model of differentialprivacy. In this problem, we are given a universe $U$ of items, possibly ofinfinite size, and a database $D$ of users. Each user $i$ contributes a subset$W_i \\subseteq U$ of items. We want an ($\\epsilon$,$\\delta$)-differentiallyprivate algorithm which outputs a subset $S \\subset \\cup_i W_i$ such that thesize of $S$ is as large as possible. The problem arises in countless real worldapplications; it is particularly ubiquitous in natural language processing(NLP) applications as vocabulary extraction. For example, discovering words,sentences, $n$-grams etc., from private text data belonging to users is aninstance of the set union problem. Known algorithms for this problem proceed by collecting a subset of itemsfrom each user, taking the union of such subsets, and disclosing the itemswhose noisy counts fall above a certain threshold. Crucially, in the aboveprocess, the contribution of each individual user is always independent of theitems held by other users, resulting in a wasteful aggregation process, wheresome item counts happen to be way above the threshold. We deviate from theabove paradigm by allowing users to contribute their items in a$\\textit{dependent fashion}$, guided by a $\\textit{policy}$. In this newsetting ensuring privacy is significantly delicate. We prove that any policywhich has certain $\\textit{contractive}$ properties would result in adifferentially private algorithm. We design two new algorithms, one usingLaplace noise and other Gaussian noise, as specific instances of policiessatisfying the contractive properties. Our experiments show that the newalgorithms significantly outperform previously known mechanisms for theproblem.\r2022-03-28\nMixed Differential Privacy in Computer Vision\nAditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, Stefano Soatto\nabstract\rabstract: We introduce AdaMix, an adaptive differentially private algorithm fortraining deep neural network classifiers using both private and public imagedata. While pre-training language models on large public datasets has enabledstrong differential privacy (DP) guarantees with minor loss of accuracy, asimilar practice yields punishing trade-offs in vision tasks. A few-shot oreven zero-shot learning baseline that ignores private data can outperformfine-tuning on a large private dataset. AdaMix incorporates few-shot training,or cross-modal zero-shot learning, on public data prior to private fine-tuning,to improve the trade-off. AdaMix reduces the error increase from thenon-private upper bound from the 167-311% of the baseline, on average across 6datasets, to 68-92% depending on the desired privacy level selected by theuser. AdaMix tackles the trade-off arising in visual classification, wherebythe most privacy sensitive data, corresponding to isolated points inrepresentation space, are also critical for high classification accuracy. Inaddition, AdaMix comes with strong theoretical privacy guarantees andconvergence analysis.\r2022-03-01\nCompliance Checking with NLI: Privacy Policies vs. Regulations\nAmin Rabinia, Zane Nygaard\nabstract\rabstract: A privacy policy is a document that states how a company intends to handleand manage their customers\u0026rsquo; personal data. One of the problems that arises withthese privacy policies is that their content might violate data privacyregulations. Because of the enormous number of privacy policies that exist, theonly realistic way to check for legal inconsistencies in all of them is throughan automated method. In this work, we use Natural Language Inference (NLI)techniques to compare privacy regulations against sections of privacy policiesfrom a selection of large companies. Our NLI model uses pre-trained embeddings,along with BiLSTM in its attention mechanism. We tried two versions of ourmodel: one that was trained on the Stanford Natural Language Inference (SNLI)and the second on the Multi-Genre Natural Language Inference (MNLI) dataset. Wefound that our test accuracy was higher on our model trained on the SNLI, butwhen actually doing NLI tasks on real world privacy policies, the model trainedon MNLI generalized and performed much better.\r2022-02-15\nDefending against Reconstruction Attacks with Rényi Differential Privacy\nPierre Stock, Igor Shilov, Ilya Mironov, Alexandre Sablayrolles\nabstract\rabstract: Reconstruction attacks allow an adversary to regenerate data samples of thetraining set using access to only a trained model. It has been recently shownthat simple heuristics can reconstruct data samples from language models,making this threat scenario an important aspect of model release. Differentialprivacy is a known solution to such attacks, but is often used with arelatively large privacy budget (epsilon \u0026gt; 8) which does not translate tomeaningful guarantees. In this paper we show that, for a same mechanism, we canderive privacy guarantees for reconstruction attacks that are better than thetraditional ones from the literature. In particular, we show that largerprivacy budgets do not protect against membership inference, but can stillprotect extraction of rare secrets. We show experimentally that our guaranteeshold against various language models, including GPT-2 finetuned onWikitext-103.\r2022-02-12\nWav2Vec2.0 on the Edge: Performance Evaluation\nSantosh Gondi\nabstract\rabstract: Wav2Vec2.0 is a state-of-the-art model which learns speech representationsthrough unlabeled speech data, aka, self supervised learning. The pretrainedmodel is then fine tuned on small amounts of labeled data to use it forspeech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformativesolution for low resource languages as it is mainly developed using unlabeledaudio data. Getting large amounts of labeled data is resource intensive andespecially challenging to do for low resource languages such as Swahilli,Tatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpassesthe very recent supervised learning algorithms while using 100x less labeleddata. Given its importance and enormous potential in enabling speech basedtasks on world\u0026rsquo;s 7000 languages, it is key to evaluate the accuracy, latencyand efficiency of this model on low resource and low power edge devices andinvestigate the feasibility of using it in such devices for private, secure andreliable speech based tasks. On-device speech tasks preclude sending audio datato the server hence inherently providing privacy, reduced latency and enhancedreliability. In this paper, Wav2Vec2.0 model\u0026rsquo;s accuracy and latency has beenevaluated on Raspberry Pi along with the KenLM language model for speechrecognition tasks. How to tune certain parameters to achieve desired level ofWER rate and latency while meeting the CPU, memory and energy budgets of theproduct has been discussed.\r2022-02-09\nFedQAS: Privacy-aware machine reading comprehension with federated learning\nAddi Ait-Mlouk, Sadi Alawadi, Salman Toor, Andreas Hellander\nabstract\rabstract: Machine reading comprehension (MRC) of text data is one important task inNatural Language Understanding. It is a complex NLP problem with a lot ofongoing research fueled by the release of the Stanford Question AnsweringDataset (SQuAD) and Conversational Question Answering (CoQA). It is consideredto be an effort to teach computers how to \u0026ldquo;understand\u0026rdquo; a text, and then to beable to answer questions about it using deep learning. However, until nowlarge-scale training on private text data and knowledge sharing has beenmissing for this NLP task. Hence, we present FedQAS, a privacy-preservingmachine reading system capable of leveraging large-scale private data withoutthe need to pool those datasets in a central location. The proposed approachcombines transformer models and federated learning technologies. The system isdeveloped using the FEDn framework and deployed as a proof-of-concept allianceinitiative. FedQAS is flexible, language-agnostic, and allows intuitiveparticipation and execution of local model training. In addition, we presentthe architecture and implementation of the system, as well as provide areference evaluation based on the SQUAD dataset, to showcase how it overcomesdata privacy issues and enables knowledge sharing between alliance members in aFederated learning setting.\r2022-01-04\nSubmix: Practical Private Prediction for Large-Scale Language Models\nAntonio Ginart, Laurens van der Maaten, James Zou, Chuan Guo\nabstract\rabstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model\u0026rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.\r2021-11-30\nIdentifying Terms and Conditions Important to Consumers using Crowdsourcing\nXingyu Liu, Annabel Sun, Jason I. Hong\nabstract\rabstract: Terms and conditions (T\u0026amp;Cs) are pervasive on the web and often containimportant information for consumers, but are rarely read. Previous research hasexplored methods to surface alarming privacy policies using manual labelers,natural language processing, and deep learning techniques. However, this priorwork used pre-determined categories for annotations, and did not investigatewhat consumers really deem as important from their perspective. In this paper,we instead combine crowdsourcing with an open definition of \u0026ldquo;what is important\u0026quot;in T\u0026amp;Cs. We present a workflow consisting of pairwise comparisons, agreementvalidation, and Bradley-Terry rank modeling, to effectively establish rankingsof T\u0026amp;C statements from non-expert crowdworkers on this open definition, andfurther analyzed consumers\u0026rsquo; preferences. We applied this workflow to 1,551 T\u0026amp;Cstatements from 27 e-commerce websites, contributed by 3,462 unique crowdworkers doing 203,068 pairwise comparisons, and conducted thematic andreadability analysis on the statements considered as important/unimportant. Wefound that consumers especially cared about policies related to after-sales andmoney, and tended to regard harder-to-understand statements as more important.We also present machine learning models to identify T\u0026amp;C clauses that consumersconsidered important, achieving at best a 92.7% balanced accuracy, 91.6%recall, and 89.2% precision. We foresee using our workflow and model toefficiently and reliably highlight important T\u0026amp;Cs on websites at a large scale,improving consumers\u0026rsquo; awareness\r2021-11-02\nKLUE: Korean Language Understanding Evaluation\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, Kyunghyun Cho\nabstract\rabstract: We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUEis a collection of 8 Korean natural language understanding (NLU) tasks,including Topic Classification, SemanticTextual Similarity, Natural LanguageInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,Machine Reading Comprehension, and Dialogue State Tracking. We build all of thetasks from scratch from diverse source corpora while respecting copyrights, toensure accessibility for anyone without any restrictions. With ethicalconsiderations in mind, we carefully design annotation protocols. Along withthe benchmark tasks and data, we provide suitable evaluation metrics andfine-tuning recipes for pretrained language models for each task. Wefurthermore release the pretrained language models (PLM), KLUE-BERT andKLUE-RoBERTa, to help reproducing baseline models on KLUE and therebyfacilitate future research. We make a few interesting observations from thepreliminary experiments using the proposed KLUE benchmark suite, alreadydemonstrating the usefulness of this new benchmark suite. First, we findKLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs andexisting open-source Korean PLMs. Second, we see minimal degradation inperformance even when we replace personally identifiable information from thepretraining corpus, suggesting that privacy and NLU capability are not at oddswith each other. Lastly, we find that using BPE tokenization in combinationwith morpheme-level pre-tokenization is effective in tasks involvingmorpheme-level tagging, detection and generation. In addition to acceleratingKorean NLP research, our comprehensive documentation on creating KLUE willfacilitate creating similar resources for other languages in the future. KLUEis available at https://klue-benchmark.com.\r2021-10-13\nFederated Natural Language Generation for Personalized Dialogue System\nYujie Lu, Chao Huang, Huanli Zhan, Yong Zhuang\nabstract\rabstract: Neural conversational models have long suffered from the problem ofinconsistency and lacking coherent personality. To address the issue,persona-based models capturing individual characteristics have been proposed,but they still face the dilemma of model adaption and data privacy. To breakthis dilemma, we propose a novel Federated Natural Language Generation (FedNLG)framework, which learns personalized representations from various dataset ondistributed devices, and thus implements the personalized dialogue systemefficiently and safely. FedNLG first pre-trains parameters of standard neuralconversational model over a large dialogue corpus, and then fine-tune the modelparameters and persona embeddings on specific datasets, in a federated manner.Thus, the model could simultaneously learn the persona embeddings in localclients and learn shared model parameters by federated aggregation, whichachieves accuracyprivacy balance. By conducting extensive experiments, wedemonstrate the effectiveness of our model by pre-training model over CornellMovie-Dialogs Corpus and fine-tuning the model over two TV series dataset.\r2021-10-05\nAI-enabled Automation for Completeness Checking of Privacy Policies\nOrlando Amaral, Sallam Abualhaija, Damiano Torre, Mehrdad Sabetzadeh, Lionel C. Briand\nabstract\rabstract: Technological advances in information sharing have raised concerns about dataprotection. Privacy policies contain privacy-related requirements about how thepersonal data of individuals will be handled by an organization or a softwaresystem (e.g., a web service or an app). In Europe, privacy policies are subjectto compliance with the General Data Protection Regulation (GDPR). Aprerequisite for GDPR compliance checking is to verify whether the content of aprivacy policy is complete according to the provisions of GDPR. Incompleteprivacy policies might result in large fines on violating organization as wellas incomplete privacy-related software specifications. Manual completenesschecking is both time-consuming and error-prone. In this paper, we proposeAI-based automation for the completeness checking of privacy policies. Throughsystematic qualitative methods, we first build two artifacts to characterizethe privacy-related provisions of GDPR, namely a conceptual model and a set ofcompleteness criteria. Then, we develop an automated solution on top of theseartifacts by leveraging a combination of natural language processing andsupervised machine learning. Specifically, we identify the GDPR-relevantinformation content in privacy policies and subsequently check them against thecompleteness criteria. To evaluate our approach, we collected 234 real privacypolicies from the fund industry. Over a set of 48 unseen privacy policies, ourapproach detected 300 of the total of 334 violations of some completenesscriteria correctly, while producing 23 false positives. The approach thus has aprecision of 92.9% and recall of 89.8%. Compared to a baseline that applieskeyword search only, our approach results in an improvement of 24.5% inprecision and 38% in recall.\r2021-10-04\nPrivacy enabled Financial Text Classification using Differential Privacy and Federated Learning\nPriyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu\nabstract\rabstract: Privacy is important considering the financial Domain as such data is highlyconfidential and sensitive. Natural Language Processing (NLP) techniques can beapplied for text classification and entity detection purposes in financialdomains such as customer feedback sentiment analysis, invoice entity detection,categorisation of financial documents by type etc. Due to the sensitive natureof such data, privacy measures need to be taken for handling and training largemodels with such data. In this work, we propose a contextualized transformer(BERT and RoBERTa) based text classification model integrated with privacyfeatures such as Differential Privacy (DP) and Federated Learning (FL). Wepresent how to privately train NLP models and desirable privacy-utilitytradeoffs and evaluate them on the Financial Phrase Bank dataset.\r2021-09-21\nTAG: Gradient Attack on Transformer-based Language Models\nJieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, Caiwen Ding\nabstract\rabstract: Although federated learning has increasingly gained attention in terms ofeffectively utilizing local devices for data privacy enhancement, recentstudies show that publicly shared gradients in the training process can revealthe private training images (gradient leakage) to a third-party in computervision. We have, however, no systematic understanding of the gradient leakagemechanism on the Transformer based language models. In this paper, as the firstattempt, we formulate the gradient attack problem on the Transformer-basedlanguage models and propose a gradient attack algorithm, TAG, to reconstructthe local training data. We develop a set of metrics to evaluate theeffectiveness of the proposed attack algorithm quantitatively. Experimentalresults on Transformer, TinyBERT${4}$, TinyBERT${6}$, BERT${BASE}$, andBERT${LARGE}$ using GLUE benchmark show that TAG works well on more weightdistributions in reconstructing training data and achieves 1.5$\\times$ recoverrate and 2.5$\\times$ ROUGE-2 over prior methods without the need of groundtruth label. TAG can obtain up to 90$%$ data by attacking gradients in CoLAdataset. In addition, TAG has a stronger adversary on large models, smalldictionary size, and small input length. We hope the proposed TAG will shedsome light on the privacy leakage problem in Transformer-based NLP models.\r2021-09-10\nCross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph\nNuttapong Chairatanakul, Noppayut Sriwatanasakdi, Nontawat Charoenphakdee, Xin Liu, Tsuyoshi Murata\nabstract\rabstract: In cross-lingual text classification, it is required that task-specifictraining data in high-resource source languages are available, where the taskis identical to that of a low-resource target language. However, collectingsuch training data can be infeasible because of the labeling cost, taskcharacteristics, and privacy concerns. This paper proposes an alternativesolution that uses only task-independent word embeddings of high-resourcelanguages and bilingual dictionaries. First, we construct a dictionary-basedheterogeneous graph (DHG) from bilingual dictionaries. This opens thepossibility to use graph neural networks for cross-lingual transfer. Theremaining challenge is the heterogeneity of DHG because multiple languages areconsidered. To address this challenge, we propose dictionary-basedheterogeneous graph neural network (DHGNet) that effectively handles theheterogeneity of DHG by two-step aggregations, which are word-level andlanguage-level aggregations. Experimental results demonstrate that our methodoutperforms pretrained models even though it does not access to large corpora.Furthermore, it can perform well even though dictionaries contain manyincorrect translations. Its robustness allows the usage of a wider range ofdictionaries such as an automatically constructed dictionary and crowdsourceddictionary, which are convenient for real-world applications.\r2021-09-08\nFedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks\nChaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao Sun, Lifang He, Liangwei Yang, Philip S. Yu, Yu Rong, Peilin Zhao, Junzhou Huang, Murali Annavaram, Salman Avestimehr\nabstract\rabstract: Graph Neural Network (GNN) research is rapidly growing thanks to the capacityof GNNs in learning distributed representations from graph-structured data.However, centralizing a massive amount of real-world graph data for GNNtraining is prohibitive due to privacy concerns, regulation restrictions, andcommercial competitions. Federated learning (FL), a trending distributedlearning paradigm, provides possibilities to solve this challenge whilepreserving data privacy. Despite recent advances in vision and languagedomains, there is no suitable platform for the FL of GNNs. To this end, weintroduce FedGraphNN, an open FL benchmark system that can facilitate researchon federated GNNs. FedGraphNN is built on a unified formulation of graph FL andcontains a wide range of datasets from different domains, popular GNN models,and FL algorithms, with secure and efficient system support. Particularly forthe datasets, we collect, preprocess, and partition 36 datasets from 7 domains,including both publicly available ones and specifically obtained ones such ashERG and Tencent. Our empirical analysis showcases the utility of our benchmarksystem, while exposing significant challenges in graph FL: federated GNNsperform worse in most datasets with a non-IID split than centralized GNNs; theGNN model that attains the best result in the centralized setting may notmaintain its advantage in the FL setting. These results imply that moreresearch efforts are needed to unravel the mystery behind federated GNNs.Moreover, our system performance analysis demonstrates that the FedGraphNNsystem is computationally efficient and secure to large-scale graphs datasets.We maintain the source code at https://github.com/FedML-AI/FedGraphNN.\r2021-07-27\nFederated Learning Meets Natural Language Processing: A Survey\nMing Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang\nabstract\rabstract: Federated Learning aims to learn machine learning models from multipledecentralized edge devices (e.g. mobiles) or servers without sacrificing localdata privacy. Recent Natural Language Processing techniques rely on deeplearning and large pre-trained language models. However, both big deep neuraland language models are trained with huge amounts of data which often lies onthe server side. Since text data is widely originated from end users, in thiswork, we look into recent NLP models and techniques which use federatedlearning as the learning framework. Our survey discusses major challenges infederated natural language processing, including the algorithm challenges,system challenges as well as the privacy issues. We also provide a criticalreview of the existing Federated NLP evaluation methods and tools. Finally, wehighlight the current research gaps and future directions.\r2021-07-22\nConfronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective\nSvetlana Kiritchenko, Isar Nejadgholi, Kathleen C. Fraser\nabstract\rabstract: The pervasiveness of abusive content on the internet can lead to severepsychological and physical harm. Significant effort in Natural LanguageProcessing (NLP) research has been devoted to addressing this problem throughabusive content detection and related sub-areas, such as the detection of hatespeech, toxicity, cyberbullying, etc. Although current technologies achievehigh classification performance in research studies, it has been observed thatthe real-life application of this technology can cause unintended harms, suchas the silencing of under-represented groups. We review a large body of NLPresearch on automatic abuse detection with a new focus on ethical challenges,organized around eight established ethical principles: privacy, accountability,safety and security, transparency and explainability, fairness andnon-discrimination, human control of technology, professional responsibility,and promotion of human values. In many cases, these principles relate not onlyto situational ethical codes, which may be context-dependent, but are in factconnected to universal human rights, such as the right to privacy, freedom fromdiscrimination, and freedom of expression. We highlight the need to examine thebroad social impacts of this technology, and to bring ethical and human rightsconsiderations to every stage of the application life-cycle, from taskformulation and dataset design, to model training and evaluation, toapplication deployment. Guided by these principles, we identify severalopportunities for rights-respecting, socio-technical solutions to detect andconfront online abuse, including nudging', quarantining\u0026rsquo;, value sensitivedesign, counter-narratives, style transfer, and AI-driven public educationapplications.\r2021-06-08\nLabeled Data Generation with Inexact Supervision\nEnyan Dai, Kai Shu, Yiwei Sun, Suhang Wang\nabstract\rabstract: The recent advanced deep learning techniques have shown the promising resultsin various domains such as computer vision and natural language processing. Thesuccess of deep neural networks in supervised learning heavily relies on alarge amount of labeled data. However, obtaining labeled data with targetlabels is often challenging due to various reasons such as cost of labeling andprivacy issues, which challenges existing deep models. In spite of that, it isrelatively easy to obtain data with \\textit{inexact supervision}, i.e., havinglabels/tags related to the target task. For example, social media platforms areoverwhelmed with billions of posts and images with self-customized tags, whichare not the exact labels for target classification tasks but are usuallyrelated to the target labels. It is promising to leverage these tags (inexactsupervision) and their relations with target classes to generate labeled datato facilitate the downstream classification tasks. However, the work on this israther limited. Therefore, we study a novel problem of labeled data generationwith inexact supervision. We propose a novel generative framework named asADDES which can synthesize high-quality labeled data for target classificationtasks by learning from data with inexact supervision and the relations betweeninexact supervision and target classes. Experimental results on image and textdatasets demonstrate the effectiveness of the proposed ADDES for generatingrealistic labeled data from inexact supervision to facilitate the targetclassification task.\r2021-06-02\nPersonalizing Pre-trained Models\nMina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali Hazariwala, Pattie Maes\nabstract\rabstract: Self-supervised or weakly supervised models trained on large-scale datasetshave shown sample-efficient transfer to diverse datasets in few-shot settings.We consider how upstream pretrained models can be leveraged for downstreamfew-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIPPERsonalized) uses image representations from CLIP, a large-scale imagerepresentation learning model trained using weak natural language supervision.We developed a technique, called Multi-label Weight Imprinting (MWI), formulti-label, continual, and few-shot learning, and CLIPPER uses MWI with imagerepresentations from CLIP. We evaluated CLIPPER on 10 single-label and 5multi-label datasets. Our model shows robust and competitive performance, andwe set new benchmarks for few-shot, multi-label, and continual learning. Ourlightweight technique is also compute-efficient and enables privacy-preservingapplications as the data is not sent to the upstream model for fine-tuning.\rMultilingual Medical Question Answering and Information Retrieval for Rural Health Intelligence Access\nVishal Vinod, Susmit Agrawal, Vipul Gaurav, Pallavi R, Savita Choudhary\nabstract\rabstract: In rural regions of several developing countries, access to qualityhealthcare, medical infrastructure, and professional diagnosis is largelyunavailable. Many of these regions are gradually gaining access to internetinfrastructure, although not with a strong enough connection to allow forsustained communication with a medical practitioner. Several deaths resultingfrom this lack of medical access, absence of patient\u0026rsquo;s previous health records,and the unavailability of information in indigenous languages can be easilyprevented. In this paper, we describe an approach leveraging the phenomenalprogress in Machine Learning and NLP (Natural Language Processing) techniquesto design a model that is low-resource, multilingual, and a preliminaryfirst-point-of-contact medical assistant. Our contribution includes definingthe NLP pipeline required for named-entity-recognition, language-agnosticsentence embedding, natural language translation, information retrieval,question answering, and generative pre-training for final query processing. Weobtain promising results for this pipeline and preliminary results for EHR(Electronic Health Record) analysis with text summarization for medicalpractitioners to peruse for their diagnosis. Through this NLP pipeline, we aimto provide preliminary medical information to the user and do not claim tosupplant diagnosis from qualified medical practitioners. Using the input fromsubject matter experts, we have compiled a large corpus to pre-train andfine-tune our BioBERT based NLP model for the specific tasks. We expect recentadvances in NLP architectures, several of which are efficient andprivacy-preserving models, to further the impact of our solution and improve onindividual task performance.\r2021-02-01\nScaling Federated Learning for Fine-tuning of Large Language Models\nAgrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon René Sütfeld, Edvin Listo Zec, Olof Mogren\nabstract\rabstract: Federated learning (FL) is a promising approach to distributed compute, aswell as distributed data, and provides a level of privacy and compliance tolegal frameworks. This makes FL attractive for both consumer and healthcareapplications. While the area is actively being explored, few studies haveexamined FL in the context of larger language models and there is a lack ofcomprehensive reviews of robustness across tasks, architectures, numbers ofclients, and other relevant factors. In this paper, we explore the fine-tuningof Transformer-based language models in a federated learning setting. Weevaluate three popular BERT-variants of different sizes (BERT, ALBERT, andDistilBERT) on a number of text classification tasks such as sentiment analysisand author identification. We perform an extensive sweep over the number ofclients, ranging up to 32, to evaluate the impact of distributed compute ontask performance in the federated averaging setting. While our findings suggestthat the large sizes of the evaluated models are not generally prohibitive tofederated training, we found that the different models handle federatedaveraging to a varying degree. Most notably, DistilBERT converges significantlyslower with larger numbers of clients, and under some circumstances, evencollapses to chance level performance. Investigating this issue presents aninteresting perspective for future research.\r2021-01-29\nN-grams Bayesian Differential Privacy\nOsman Ramadan, James Withers, Douglas Orr\nabstract\rabstract: Differential privacy has gained popularity in machine learning as a strongprivacy guarantee, in contrast to privacy mitigation techniques such ask-anonymity. However, applying differential privacy to n-gram countssignificantly degrades the utility of derived language models due to theirlarge vocabularies. We propose a differential privacy mechanism that usespublic data as a prior in a Bayesian setup to provide tighter bounds on theprivacy loss metric epsilon, and thus better privacy-utility trade-offs. Itfirst transforms the counts to log space, approximating the distribution of thepublic and private data as Gaussian. The posterior distribution is thenevaluated and softmax is applied to produce a probability distribution. Thistechnique achieves up to 85% reduction in KL divergence compared to previouslyknown mechanisms at epsilon equals 0.1. We compare our mechanism to k-anonymityin a n-gram language modelling task and show that it offers competitiveperformance at large vocabulary sizes, while also providing superior privacyprotection.\r2020-12-11\nAdaptive Self-training for Few-shot Neural Sequence Labeling\nYaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu, Jing Gao, Ahmed Hassan Awadallah\nabstract\rabstract: Sequence labeling is an important technique employed for many NaturalLanguage Processing (NLP) tasks, such as Named Entity Recognition (NER), slottagging for dialog systems and semantic parsing. Large-scale pre-trainedlanguage models obtain very good performance on these tasks when fine-tuned onlarge amounts of task-specific labeled data. However, such large-scale labeleddatasets are difficult to obtain for several tasks and domains due to the highcost of human annotation as well as privacy and data access constraints forsensitive user applications. This is exacerbated for sequence labeling tasksrequiring such annotations at token-level. In this work, we develop techniquesto address the label scarcity challenge for neural sequence labeling models.Specifically, we develop self-training and meta-learning techniques fortraining neural sequence taggers with few labels. While self-training serves asan effective mechanism to learn from large amounts of unlabeled data \u0026ndash;meta-learning helps in adaptive sample re-weighting to mitigate errorpropagation from noisy pseudo-labels. Extensive experiments on six benchmarkdatasets including two for massive multilingual NER and four slot taggingdatasets for task-oriented dialog systems demonstrate the effectiveness of ourmethod. With only 10 labeled examples for each class for each task, our methodobtains 10% improvement over state-of-the-art systems demonstrating itseffectiveness for the low-resource setting.\r2020-12-01\nModifying Memories in Transformer Models\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar\nabstract\rabstract: Large Transformer models have achieved impressive performance in many naturallanguage tasks. In particular, Transformer based language models have beenshown to have great capabilities in encoding factual knowledge in their vastamount of parameters. While the tasks of improving the memorization andgeneralization of Transformers have been widely studied, it is not well knownhow to make transformers forget specific old facts and memorize new ones. Inthis paper, we propose a new task of \\emph{explicitly modifying specificfactual knowledge in Transformer models while ensuring the model performancedoes not degrade on the unmodified facts}. This task is useful in manyscenarios, such as updating stale knowledge, protecting privacy, andeliminating unintended biases stored in the models. We benchmarked severalapproaches that provide natural baseline performances on this task. This leadsto the discovery of key components of a Transformer model that are especiallyeffective for knowledge modifications. The work also provides insights into therole that different training phases (such as pretraining and fine-tuning) playtowards memorization and knowledge modification.\r2020-11-07\nPrivacy in Deep Learning: A Survey\nFatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh\nabstract\rabstract: The ever-growing advances of deep learning in many areas including vision,recommendation systems, natural language processing, etc., have led to theadoption of Deep Neural Networks (DNNs) in production systems. The availabilityof large datasets and high computational power are the main contributors tothese advances. The datasets are usually crowdsourced and may contain sensitiveinformation. This poses serious privacy concerns as this data can be misused orleaked through various vulnerabilities. Even if the cloud provider and thecommunication link is trusted, there are still threats of inference attackswhere an attacker could speculate properties of the data used for training, orfind the underlying model architecture and parameters. In this survey, wereview the privacy concerns brought by deep learning, and the mitigatingtechniques introduced to tackle these issues. We also show that there is a gapin the literature regarding test-time inference privacy, and propose possiblefuture research directions.\r2020-10-14\nCommunication-Efficient Federated Learning via Optimal Client Sampling\nMonica Ribero, Haris Vikalo\nabstract\rabstract: Federated learning (FL) ameliorates privacy concerns in settings where acentral server coordinates learning from data distributed across many clients.The clients train locally and communicate the models they learn to the server;aggregation of local models requires frequent communication of large amounts ofinformation between the clients and the central server. We propose a novel,simple and efficient way of updating the central model incommunication-constrained settings based on collecting models from clients withinformative updates and estimating local updates that were not communicated. Inparticular, modeling the progression of model\u0026rsquo;s weights by anOrnstein-Uhlenbeck process allows us to derive an optimal sampling strategy forselecting a subset of clients with significant weight updates. The centralserver collects updated local models from only the selected clients andcombines them with estimated model updates of the clients that were notselected for communication. We test this policy on a synthetic dataset forlogistic regression and two FL benchmarks, namely, a classification task onEMNIST and a realistic language modeling task using the Shakespeare dataset.The results demonstrate that the proposed framework provides significantreduction in communication while maintaining competitive or achieving superiorperformance compared to a baseline. Our method represents a new line ofstrategies for communication-efficient FL that is orthogonal to the existinguser-local methods such as quantization or sparsification, thus complementingrather than aiming to replace those existing methods.\r2020-10-10\nAdversarial Self-Supervised Data-Free Distillation for Text Classification\nXinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu\nabstract\rabstract: Large pre-trained transformer-based language models have achieved impressiveresults on a wide range of NLP tasks. In the past few years, KnowledgeDistillation(KD) has become a popular paradigm to compress a computationallyexpensive model to a resource-efficient lightweight model. However, most KDalgorithms, especially in NLP, rely on the accessibility of the originaltraining dataset, which may be unavailable due to privacy issues. To tacklethis problem, we propose a novel two-stage data-free distillation method, namedAdversarial self-Supervised Data-Free Distillation (AS-DFD), which is designedfor compressing large-scale transformer-based models (e.g., BERT). To avoidtext generation in discrete space, we introduce a Plug \u0026amp; Play EmbeddingGuessing method to craft pseudo embeddings from the teacher\u0026rsquo;s hidden knowledge.Meanwhile, with a self-supervised module to quantify the student\u0026rsquo;s ability, weadapt the difficulty of pseudo embeddings in an adversarial training manner. Tothe best of our knowledge, our framework is the first data-free distillationframework designed for NLP tasks. We verify the effectiveness of our method onseveral text classification datasets.\r2020-06-12\nUnderstanding Unintended Memorization in Federated Learning\nOm Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Françoise Beaufays\nabstract\rabstract: Recent works have shown that generative sequence models (e.g., languagemodels) have a tendency to memorize rare or unique sequences in the trainingdata. Since useful models are often trained on sensitive data, to ensure theprivacy of the training data it is critical to identify and mitigate suchunintended memorization. Federated Learning (FL) has emerged as a novelframework for large-scale distributed learning tasks. However, it differs inmany aspects from the well-studied central learning setting where all the datais stored at the central server. In this paper, we initiate a formal study tounderstand the effect of different components of canonical FL on unintendedmemorization in trained models, comparing with the central learning setting.Our results show that several differing components of FL play an important rolein reducing unintended memorization. Specifically, we observe that theclustering of data according to users\u0026mdash;which happens by design in FL\u0026mdash;has asignificant effect in reducing such memorization, and using the method ofFederated Averaging for training causes a further reduction. We also show thattraining with a strong user-level differential privacy guarantee results inmodels that exhibit the least amount of unintended memorization.\r2020-04-23\nPrivacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies\nMukund Srinath, Shomir Wilson, C. Lee Giles\nabstract\rabstract: Organisations disclose their privacy practices by posting privacy policies ontheir website. Even though users often care about their digital privacy, theyoften don\u0026rsquo;t read privacy policies since they require a significant investmentin time and effort. Although natural language processing can help in privacypolicy understanding, there has been a lack of large scale privacy policycorpora that could be used to analyse, understand, and simplify privacypolicies. Thus, we create PrivaSeer, a corpus of over one million Englishlanguage website privacy policies, which is significantly larger than anypreviously available corpus. We design a corpus creation pipeline whichconsists of crawling the web followed by filtering documents using languagedetection, document classification, duplicate and near-duplication removal, andcontent extraction. We investigate the composition of the corpus and showresults from readability tests, document similarity, keyphrase extraction, andexplored the corpus through topic modeling.\r2020-03-17\nSensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT\nAitor García-Pablos, Naiara Perez, Montse Cuadros\nabstract\rabstract: Massive digital data processing provides a wide range of opportunities andbenefits, but at the cost of endangering personal data privacy. Anonymisationconsists in removing or replacing sensitive information from data, enabling itsexploitation for different purposes while preserving the privacy ofindividuals. Over the years, a lot of automatic anonymisation systems have beenproposed; however, depending on the type of data, the target language or theavailability of training documents, the task remains challenging still. Theemergence of novel deep-learning models during the last two years has broughtlarge improvements to the state of the art in the field of Natural LanguageProcessing. These advancements have been most noticeably led by BERT, a modelproposed by Google in 2018, and the shared language models pre-trained onmillions of documents. In this paper, we use a BERT-based sequence labellingmodel to conduct a series of anonymisation experiments on several clinicaldatasets in Spanish. We also compare BERT to other algorithms. The experimentsshow that a simple BERT-based model with general-domain pre-training obtainshighly competitive results without any domain specific feature engineering.\r2020-02-20\nFederated pretraining and fine tuning of BERT using clinical notes from multiple silos\nDianbo Liu, Tim Miller\nabstract\rabstract: Large scale contextual representation models, such as BERT, havesignificantly advanced natural language processing (NLP) in recently years.However, in certain area like healthcare, accessing diverse large scale textdata from multiple institutions is extremely challenging due to privacy andregulatory reasons. In this article, we show that it is possible to bothpretrain and fine tune BERT models in a federated manner using clinical textsfrom different silos without moving the data.\r2020-01-02\nPersonalized Dialogue Generation with Diversified Traits\nYinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu, Xuan Zhu\nabstract\rabstract: Endowing a dialogue system with particular personality traits is essential todeliver more human-like conversations. However, due to the challenge ofembodying personality via language expression and the lack of large-scalepersona-labeled dialogue data, this research problem is still far fromwell-studied. In this paper, we investigate the problem of incorporatingexplicit personality traits in dialogue generation to deliver personalizeddialogues. To this end, firstly, we construct PersonalDialog, a large-scale multi-turndialogue dataset containing various traits from a large number of speakers. Thedataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.Each utterance is associated with a speaker who is marked with traits like Age,Gender, Location, Interest Tags, etc. Several anonymization schemes aredesigned to protect the privacy of each speaker. This large-scale dataset willfacilitate not only the study of personalized dialogue generation, but alsoother researches on sociolinguistics or social science. Secondly, to study how personality traits can be captured and addressed indialogue generation, we propose persona-aware dialogue generation models withinthe sequence to sequence learning framework. Explicit personality traits(structured by key-value pairs) are embedded using a trait fusion module.During the decoding process, two techniques, namely persona-aware attention andpersona-aware bias, are devised to capture and address trait-relatedinformation. Experiments demonstrate that our model is able to address propertraits in different contexts. Case studies also show interesting results forthis challenging research problem.\r2019-10-08\nFederated Learning of N-gram Language Models\nMingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril Allauzen, Françoise Beaufays, Michael Riley\nabstract\rabstract: We propose algorithms to train production-quality n-gram language modelsusing federated learning. Federated learning is a distributed computationplatform that can be used to train global models for portable devices such assmart phones. Federated learning is especially relevant for applicationshandling privacy-sensitive data, such as virtual keyboards, because training isperformed without the users\u0026rsquo; data ever leaving their devices. While theprinciples of federated learning are fairly generic, its methodology assumesthat the underlying models are neural networks. However, virtual keyboards aretypically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using thedecentralized FederatedAveraging algorithm and to approximate this federatedmodel server-side with an n-gram model that can be deployed to devices for fastinference. Our technical contributions include ways of handling largevocabularies, algorithms to correct capitalization errors in user data, andefficient finite state transducer algorithms to convert word language models toword-piece language models and vice versa. The n-gram language models trainedwith federated learning are compared to n-grams trained with traditionalserver-based algorithms using A/B tests on tens of millions of users of virtualkeyboard. Results are presented for two languages, American English andBrazilian Portuguese. This work demonstrates that high-quality n-gram languagemodels can be trained directly on client mobile devices without sensitivetraining data ever leaving the devices.\r2019-10-04\nSynthesizing Credit Card Transactions\nErik R. Altman\nabstract\rabstract: Two elements have been essential to AI\u0026rsquo;s recent boom: (1) deep neural netsand the theory and practice behind them; and (2) cloud computing with itsabundant labeled data and large computing resources. Abundant labeled data is available for key domains such as images, speech,natural language processing, and recommendation engines. However, there aremany other domains where such data is not available, or access to it is highlyrestricted for privacy reasons, as with health and financial data. Even whenabundant data is available, it is often not labeled. Doing such labeling islabor-intensive and non-scalable. As a result, to the best of our knowledge, key domains still lack labeleddata or have at most toy data; or the synthetic data must have access to realdata from which it can mimic new data. This paper outlines work to generaterealistic synthetic data for an important domain: credit card transactions. Some challenges: there are many patterns and correlations in real purchases.There are millions of merchants and innumerable locations. Those merchantsoffer a wide variety of goods. Who shops where and when? How much do peoplepay? What is a realistic fraudulent transaction? We use a mixture of technical approaches and domain knowledge includingmechanics of credit card processing, a broad set of consumer domains:electronics, clothing, hair styling, etc. Connecting everything is a virtualworld. This paper outlines some of our key techniques and provides evidencethat the data generated is indeed realistic. Beyond the scope of this paper: (1) use of our data to develop and trainmodels to predict fraud; (2) coupling models and the synthetic dataset toassess performance in designing accelerators such as GPUs and TPUs.\r2019-06-03\nProtection Against Reconstruction and Its Applications in Private Federated Learning\nAbhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, Ryan Rogers\nabstract\rabstract: In large-scale statistical learning, data collection and model fitting aremoving increasingly toward peripheral devices\u0026mdash;phones, watches, fitnesstrackers\u0026mdash;away from centralized data collection. Concomitant with this rise indecentralized data are increasing challenges of maintaining privacy whileallowing enough information to fit accurate, useful statistical models. Thismotivates local notions of privacy\u0026mdash;most significantly, local differentialprivacy, which provides strong protections against sensitive datadisclosures\u0026mdash;where data is obfuscated before a statistician or learner caneven observe it, providing strong protections to individuals\u0026rsquo; data. Yet localprivacy as traditionally employed may prove too stringent for practical use,especially in modern high-dimensional statistical and machine learningproblems. Consequently, we revisit the types of disclosures and adversariesagainst which we provide protections, considering adversaries with limitedprior information and ensuring that with high probability, ensuring they cannotreconstruct an individual\u0026rsquo;s data within useful tolerances. By reconceptualizingthese protections, we allow more useful data release\u0026mdash;large privacy parametersin local differential privacy\u0026mdash;and we design new (minimax) optimal locallydifferentially private mechanisms for statistical learning problems for\\emph{all} privacy levels. We thus present practicable approaches tolarge-scale locally private model training that were previously impossible,showing theoretically and empirically that we can fit large-scale imageclassification and language models with little degradation in utility.\r2019-05-21\nTowards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models\nOren Melamud, Chaitanya Shivade\nabstract\rabstract: Large-scale clinical data is invaluable to driving many computationalscientific advances today. However, understandable concerns regarding patientprivacy hinder the open dissemination of such data and give rise to suboptimalsiloed research. De-identification methods attempt to address these concernsbut were shown to be susceptible to adversarial attacks. In this work, we focuson the vast amounts of unstructured natural language data stored in clinicalnotes and propose to automatically generate synthetic clinical notes that aremore amenable to sharing using generative models trained on real de-identifiedrecords. To evaluate the merit of such notes, we measure both their privacypreservation properties as well as utility in training clinical NLP models.Experiments using neural language models yield notes whose utility is close tothat of the real ones in some clinical NLP tasks, yet leave ample room forfuture improvements.\r2019-02-27\nScalable Privacy-Compliant Virality Prediction on Twitter\nDamian Konrad Kowalczyk, Jan Larsen\nabstract\rabstract: The digital town hall of Twitter becomes a preferred medium of communicationfor individuals and organizations across the globe. Some of them reachaudiences of millions, while others struggle to get noticed. Given the impactof social media, the question remains more relevant than ever: how to model thedynamics of attention in Twitter. Researchers around the world turn to machinelearning to predict the most influential tweets and authors, navigating thevolume, velocity, and variety of social big data, with many compromises. Inthis paper, we revisit content popularity prediction on Twitter. We argue thatstrict alignment of data acquisition, storage and analysis algorithms isnecessary to avoid the common trade-offs between scalability, accuracy andprivacy compliance. We propose a new framework for the rapid acquisition oflarge-scale datasets, high accuracy supervisory signal and multilanguagesentiment prediction while respecting every privacy request applicable. We thenapply a novel gradient boosting framework to achieve state-of-the-art resultsin virality ranking, already before including tweet\u0026rsquo;s visual or propagationfeatures. Our Gradient Boosted Regression Tree is the first to offerexplainable, strong ranking performance on benchmark datasets. Since theanalysis focused on features available early, the model is immediatelyapplicable to incoming tweets in 18 languages.\r2018-11-12\nAutomatically Generate Steganographic Text Based on Markov Model and Huffman Coding\nZhongliang Yang, Shuyu Jin, Yongfeng Huang, Yujin Zhang, Hui Li\nabstract\rabstract: Steganography, as one of the three basic information security systems, haslong played an important role in safeguarding the privacy and confidentialityof data in cyberspace. The text is the most widely used information carrier inpeople\u0026rsquo;s daily life, using text as a carrier for information hiding has broadresearch prospects. However, due to the high coding degree and less informationredundancy in the text, it has been an extremely challenging problem to hideinformation in it for a long time. In this paper, we propose a steganographymethod which can automatically generate steganographic text based on the Markovchain model and Huffman coding. It can automatically generate fluent textcarrier in terms of secret information which need to be embedded. The proposedmodel can learn from a large number of samples written by people and obtain agood estimate of the statistical language model. We evaluated the proposedmodel from several perspectives. Experimental results show that the performanceof the proposed model is superior to all the previous related methods in termsof information imperceptibility and information hidden capacity.\r2018-02-24\nLearning Differentially Private Recurrent Language Models\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang\nabstract\rabstract: We demonstrate that it is possible to train large recurrent language modelswith user-level differential privacy guarantees with only a negligible cost inpredictive accuracy. Our work builds on recent advances in the training of deepnetworks on user-partitioned data and privacy accounting for stochasticgradient descent. In particular, we add user-level privacy protection to thefederated averaging algorithm, which makes \u0026ldquo;large step\u0026rdquo; updates from user-leveldata. Our work demonstrates that given a dataset with a sufficiently largenumber of users (a requirement easily met by even small internet-scaledatasets), achieving differential privacy comes at the cost of increasedcomputation, rather than in decreased utility as in most prior work. We findthat our private LSTM language models are quantitatively and qualitativelysimilar to un-noised models when trained on a large dataset.\r2018-02-14\nCognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner\nEmmanuel Dupoux\nabstract\rabstract: During their first years of life, infants learn the language(s) of theirenvironment at an amazing speed despite large cross cultural variations inamount and complexity of the available language input. Understanding thissimple fact still escapes current cognitive and linguistic theories. Recently,spectacular progress in the engineering science, notably, machine learning andwearable technology, offer the promise of revolutionizing the study ofcognitive development. Machine learning offers powerful learning algorithmsthat can achieve human-like performance on many linguistic tasks. Wearablesensors can capture vast amounts of data, which enable the reconstruction ofthe sensory experience of infants in their natural environment. The project of\u0026rsquo;reverse engineering\u0026rsquo; language development, i.e., of building an effectivesystem that mimics infant\u0026rsquo;s achievements appears therefore to be within reach.Here, we analyze the conditions under which such a project can contribute toour scientific understanding of early language development. We argue thatinstead of defining a sub-problem or simplifying the data, computational modelsshould address the full complexity of the learning situation, and take as inputthe raw sensory signals available to infants. This implies that (1) accessiblebut privacy-preserving repositories of home data be setup and widely shared,and (2) models be evaluated at different linguistic levels through a benchmarkof psycholinguist tests that can be passed by machines and humans alike, (3)linguistically and psychologically plausible learning architectures be scaledup to real data using probabilistic/optimization principles from machinelearning. We discuss the feasibility of this approach and present preliminaryresults.\r2014-10-21\nOHMF: A Query Based Optimal Healthcare Medication Framework\nSantosh Kumar Majhi, Padmalochan Bera\nabstract\rabstract: Today cloud computing infrastructure is largely being deployed in healthcareto access various healthcare services easily over the Internet on an as neededbasis. The main advantage of healthcare cloud is that it can be used as a toolfor patients, medical professionals and insurance providers, to query andcoordinate among medical departments, organizations and other healthcarerelated hubs. Although healthcare cloud services can enable better medicationprocess with high responsiveness, but the privacy and other requirements of thepatients need to be ensured in the process. Patients medical data may berequired by the medical professionals, hospitals, diagnostic centers foranalysis and diagnosis. However, data privacy and service quality cannot becompromised. In other words, there may exist various service providerscorresponding to a specific healthcare service. The main challenge is to findthe appropriate providers that comply best with patients requirement. In thispaper, we propose a query based optimal medication framework to support thepatients healthcare service accessibility comprehensively with considerableresponse time. The framework accepts related healthcare queries in naturallanguage through a comprehensive user-interface and then processes the inputquery through a first order logic based evaluation engine and finds allpossible services satisfying the requirements. First order logic is used formodeling of user requirements and queries. The query evaluation engine is builtusing zChaff, a Boolean logic satisfiability solver. The efficacy and usabilityof the framework is evaluated with initial case studies on synthetic and reallife healthcare cloud.\r"},{"id":3,"href":"/docs/arxiv_papers/machine_unlearning/","title":"Machine Unlearning","section":"Arxiv Papers","content":"\rArxiv Papers: Machine Unlearning\r#\r2024-03-20\nThreats, Attacks, and Defenses in Machine Unlearning: A Survey\nZiyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam\nabstract\rabstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF).Therefore, strategic knowledge removal mitigates the risk of harmful outcomes,safeguarding against biases, misinformation, and unauthorized dataexploitation, thereby enhancing the ethical use and reliability of AI systems.Efforts have been made to design efficient unlearning approaches, with MUservices being examined for integration with existing machine learning as aservice (MLaaS), allowing users to submit requests to erase data. However,recent research highlights vulnerabilities in machine unlearning systems, suchas information leakage and malicious unlearning requests, that can lead tosignificant security and privacy concerns. Moreover, extensive researchindicates that unlearning methods and prevalent attacks fulfill diverse roleswithin MU systems. For instance, unlearning can act as a mechanism to recovermodels from backdoor attacks, while backdoor attacks themselves can serve as anevaluation metric for unlearning effectiveness. This underscores the intricaterelationship and complex interplay between these elements in maintaining systemfunctionality and safety. Therefore, this survey seeks to bridge the gapbetween the extensive number of studies on threats, attacks, and defenses inmachine unlearning and the absence of a comprehensive review that categorizestheir taxonomy, methods, and solutions, thus offering valuable insights forfuture research directions and practical implementations.\r2024-03-19\nHas Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects\nCheng-Long Wang, Qi Li, Zihang Xiang, Di Wang\nabstract\rabstract: The growing concerns surrounding data privacy and security have underscoredthe critical necessity for machine unlearning\u0026ndash;aimed at fully removing datalineage from machine learning models. MLaaS providers expect this to be theirultimate safeguard for regulatory compliance. Despite its critical importance,the pace at which privacy communities have been developing and implementingstrong methods to verify the effectiveness of machine unlearning has beendisappointingly slow, with this vital area often receiving insufficient focus.This paper seeks to address this shortfall by introducing well-defined andeffective metrics for black-box unlearning auditing tasks. We transform theauditing challenge into a question of non-membership inference and developefficient metrics for auditing. By relying exclusively on the original andunlearned models\u0026ndash;eliminating the need to train additional shadow models\u0026ndash;ourapproach simplifies the evaluation of unlearning at the individual data pointlevel. Utilizing these metrics, we conduct an in-depth analysis of currentapproximate machine unlearning algorithms, identifying three key directionswhere these approaches fall short: utility, resilience, and equity. Our aim isthat this work will greatly improve our understanding of approximate machineunlearning methods, taking a significant stride towards converting thetheoretical right to data erasure into a auditable reality.\rSelf-generated Replay Memories for Continual Neural Machine Translation\nMichele Resta, Davide Bacciu\nabstract\rabstract: Modern Neural Machine Translation systems exhibit strong performance inseveral different languages and are constantly improving. Their ability tolearn continuously is, however, still severely limited by the catastrophicforgetting issue. In this work, we leverage a key property of encoder-decoderTransformers, i.e. their generative ability, to propose a novel approach tocontinually learning Neural Machine Translation systems. We show how this caneffectively learn on a stream of experiences comprising different languages, byleveraging a replay memory populated by using the model itself as a generatorof parallel sentences. We empirically demonstrate that our approach cancounteract catastrophic forgetting without requiring explicit memorization oftraining data. Code will be publicly available upon publication. Code:https://github.com/m-resta/sg-rep\rInteractive Continual Learning: Fast and Slow Thinking\nBiqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou\nabstract\rabstract: Advanced life forms, sustained by the synergistic interaction of neuralcognitive mechanisms, continually acquire and transfer knowledge throughouttheir lifespan. In contrast, contemporary machine learning paradigms exhibitlimitations in emulating the facets of continual learning (CL). Nonetheless,the emergence of large language models (LLMs) presents promising avenues forrealizing CL via interactions with these models. Drawing on ComplementaryLearning System theory, this paper presents a novel Interactive ContinualLearning (ICL) framework, enabled by collaborative interactions among models ofvarious sizes. Specifically, we assign the ViT model as System1 and multimodalLLM as System2. To enable the memory module to deduce tasks from classinformation and enhance Set2Set retrieval, we propose the Class-Knowledge-TaskMulti-Head Attention (CKT-MHA). Additionally, to improve memory retrieval inSystem1 through enhanced geometric representation, we introduce the CL-vMFmechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, weintroduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI)strategy to identify hard examples, thus enhancing collaboration betweenSystem1 and System2 for complex reasoning realization. Comprehensive evaluationof our proposed ICL demonstrates significant resistance to forgetting andsuperior performance relative to existing methods. Code is available atgithub.com/ICL.\r2024-03-17\nBenefits of non-adiabatic quantum control in quantum computation through spin qubit systems\nNirupam Dutta\nabstract\rabstract: This is evident that the controllable quantum systems can be the reliablebuilding blocks for Quantum computation. In reality we are witnessing theprogress towards making the idea tractable enough, though optimistic but thethreshold is not very near to us. The dawn of quantum computation has begun. Inthe future, we hope to see a full fledged operationally stable quantum computerwhich can solve the problems beyond the scope of classical digital computers.We may call it quantum supremacy. Nevertheless, we should not forget that thereare problems which demand classical computers to be in the game for a betterperformance in comparison to the same through quantum devices. In the currentstage of computing technology, the most beneficial area is nothing but anhybrid approach and that is for no doubt will reign the market for the nextfive to ten years. This hybrid aspect has several directions such as simulatingquantum computation on a classical computer. Keeping both the aspect,computation through real physical devices and simulation on a classicalcomputer by accessing available quantum computers for cloud computing, someadvantages have been discussed in this article which will be elaborated as wellin future articles. These advantages are inherent if we can achieve propernon-adiabatic control over the spin system in the laboratory. Otherwise theseaspects can always be simulated by using quantum algorithms to see whether theycan be useful in comparison to a purely classical computing machine. This is nodoubt a new window for progress in the direction of quantum computation.\r2024-03-16\nSIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization\nYann Fraboni, Martin Van Waerebeke, Kevin Scaman, Richard Vidal, Laetitia Kameni, Marco Lorenzi\nabstract\rabstract: Machine Unlearning (MU) is an increasingly important topic in machinelearning safety, aiming at removing the contribution of a given data point froma training procedure. Federated Unlearning (FU) consists in extending MU tounlearn a given client\u0026rsquo;s contribution from a federated training routine. Whileseveral FU methods have been proposed, we currently lack a general approachproviding formal unlearning guarantees to the FedAvg routine, while ensuringscalability and generalization beyond the convex assumption on the clients\u0026rsquo;loss functions. We aim at filling this gap by proposing SIFU (SequentialInformed Federated Unlearning), a new FU method applying to both convex andnon-convex optimization regimes. SIFU naturally applies to FedAvg withoutadditional computational cost for the clients and provides formal guarantees onthe quality of the unlearning task. We provide a theoretical analysis of theunlearning properties of SIFU, and practically demonstrate its effectiveness ascompared to a panel of unlearning methods from the state-of-the-art.\r2024-03-15\nMachine Unlearning: Solutions and Challenges\nJie Xu, Zihan Wu, Cong Wang, Xiaohua Jia\nabstract\rabstract: Machine learning models may inadvertently memorize sensitive, unauthorized,or malicious data, posing risks of privacy breaches, security vulnerabilities,and performance degradation. To address these issues, machine unlearning hasemerged as a critical technique to selectively remove specific training datapoints\u0026rsquo; influence on trained models. This paper provides a comprehensivetaxonomy and analysis of the solutions in machine unlearning. We categorizeexisting solutions into exact unlearning approaches that remove data influencethoroughly and approximate unlearning approaches that efficiently minimize datainfluence. By comprehensively reviewing solutions, we identify and discusstheir strengths and limitations. Furthermore, we propose future directions toadvance machine unlearning and establish it as an essential capability fortrustworthy and adaptive machine learning models. This paper providesresearchers with a roadmap of open problems, encouraging impactfulcontributions to address real-world needs for selective data removal.\rIntroducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness\nMohamed elShehaby, Aditya Kotha, Ashraf Matrawy\nabstract\rabstract: Machine Learning (ML) is susceptible to adversarial attacks that aim to trickML models, making them produce faulty predictions. Adversarial training wasfound to increase the robustness of ML models against these attacks. However,in network and cybersecurity, obtaining labeled training and adversarialtraining data is challenging and costly. Furthermore, concept drift deepens thechallenge, particularly in dynamic domains like network and cybersecurity, andrequires various models to conduct periodic retraining. This letter introducesAdaptive Continuous Adversarial Training (ACAT) to continuously integrateadversarial training samples into the model during ongoing learning sessions,using real-world detected adversarial data, to enhance model resilience againstevolving adversarial threats. ACAT is an adaptive defense mechanism thatutilizes periodic retraining to effectively counter adversarial attacks whilemitigating catastrophic forgetting. Our approach also reduces the total timerequired for adversarial sample detection, especially in environments such asnetwork security where the rate of attacks could be very high. Traditionaldetection processes that involve two stages may result in lengthy procedures.Experimental results using a SPAM detection dataset demonstrate that with ACAT,the accuracy of the SPAM filter increased from 69% to over 88% after just threeretraining sessions. Furthermore, ACAT outperforms conventional adversarialsample detectors, providing faster decision times, up to four times faster insome cases.\r2024-03-13\nFederated Knowledge Graph Unlearning via Diffusion Model\nBingchen Liu, Yuanyuan Fang\nabstract\rabstract: Federated learning (FL) promotes the development and application ofartificial intelligence technologies by enabling model sharing andcollaboration while safeguarding data privacy. Knowledge graph (KG) embeddingrepresentation provides a foundation for knowledge reasoning and applicationsby mapping entities and relations into vector space. Federated KG embeddingenables the utilization of knowledge from diverse client sources whilesafeguarding the privacy of local data. However, due to demands such as privacyprotection and the need to adapt to dynamic data changes, investigations intomachine unlearning (MU) have been sparked. However, it is challenging tomaintain the performance of KG embedding models while forgetting the influenceof specific forgotten data on the model. In this paper, we propose FedDM, anovel framework tailored for machine unlearning in federated knowledge graphs.Leveraging diffusion models, we generate noisy data to sensibly mitigate theinfluence of specific knowledge on FL models while preserving the overallperformance concerning the remaining data. We conduct experimental evaluationson benchmark datasets to assess the efficacy of the proposed model. Extensiveexperiments demonstrate that FedDM yields promising results in knowledgeforgetting.\rMachine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects\nNa Li, Chunyi Zhou, Yansong Gao, Hui Chen, Anmin Fu, Zhi Zhang, Yu Shui\nabstract\rabstract: Personal digital data is a critical asset, and governments worldwide haveenforced laws and regulations to protect data privacy. Data users have beenendowed with the right to be forgotten of their data. In the course of machinelearning (ML), the forgotten right requires a model provider to delete userdata and its subsequent impact on ML models upon user requests. Machineunlearning emerges to address this, which has garnered ever-increasingattention from both industry and academia. While the area has developedrapidly, there is a lack of comprehensive surveys to capture the latestadvancements. Recognizing this shortage, we conduct an extensive exploration tomap the landscape of machine unlearning including the (fine-grained) taxonomyof unlearning algorithms under centralized and distributed settings, debate onapproximate unlearning, verification and evaluation metrics, challenges andsolutions for unlearning under different applications, as well as attackstargeting machine unlearning. The survey concludes by outlining potentialdirections for future research, hoping to serve as a guide for interestedscholars.\r2024-03-12\nEfficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning\nVinay Chakravarthi Gogineni, Esmaeil S. Nadimi\nabstract\rabstract: Machine unlearning has garnered significant attention due to its ability toselectively erase knowledge obtained from specific training data samples in analready trained machine learning model. This capability enables data holders toadhere strictly to data protection regulations. However, existing unlearningtechniques face practical constraints, often causing performance degradation,demanding brief fine-tuning post unlearning, and requiring significant storage.In response, this paper introduces a novel class of machine unlearningalgorithms. First method is partial amnesiac unlearning, integration oflayer-wise pruning with amnesiac unlearning. In this method, updates made tothe model during training are pruned and stored, subsequently used to forgetspecific data from trained model. The second method assimilates layer-wisepartial-updates into label-flipping and optimization-based unlearning tomitigate the adverse effects of data deletion on model efficacy. Through adetailed experimental evaluation, we showcase the effectiveness of proposedunlearning methods. Experimental results highlight that the partial amnesiacunlearning not only preserves model efficacy but also eliminates the necessityfor brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,employing layer-wise partial updates in label-flipping and optimization-basedunlearning techniques demonstrates superiority in preserving model efficacycompared to their naive counterparts.\rChallenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning\nChongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu\nabstract\rabstract: The trustworthy machine learning (ML) community is increasingly recognizingthe crucial need for models capable of selectively \u0026lsquo;unlearning\u0026rsquo; data pointsafter training. This leads to the problem of machine unlearning (MU), aiming toeliminate the influence of chosen data points on model performance, while stillmaintaining the model\u0026rsquo;s utility post-unlearning. Despite various MU methods fordata influence erasure, evaluations have largely focused on random dataforgetting, ignoring the vital inquiry into which subset should be chosen totruly gauge the authenticity of unlearning performance. To tackle this issue,we introduce a new evaluative angle for MU from an adversarial viewpoint. Wepropose identifying the data subset that presents the most significantchallenge for influence erasure, i.e., pinpointing the worst-case forget set.Utilizing a bi-level optimization principle, we amplify unlearning challengesat the upper optimization level to emulate worst-case scenarios, whilesimultaneously engaging in standard training and unlearning at the lower level,achieving a balance between data influence erasure and model utility. Ourproposal offers a worst-case evaluation of MU\u0026rsquo;s resilience and effectiveness.Through extensive experiments across different datasets (including CIFAR-10,100, CelebA, Tiny ImageNet, and ImageNet) and models (including both imageclassifiers and generative models), we expose critical pros and cons inexisting (approximate) unlearning strategies. Our results illuminate thecomplex challenges of MU in practice, guiding the future development of moreaccurate and robust unlearning algorithms. The code is available athttps://github.com/OPTML-Group/Unlearn-WorstCase.\rScissorhands: Scrub Data Influence via Connection Sensitivity in Networks\nJing Wu, Mehrtash Harandi\nabstract\rabstract: Machine unlearning has become a pivotal task to erase the influence of datafrom a trained model. It adheres to recent data regulation standards andenhances the privacy and security of machine learning applications. In thiswork, we present a new machine unlearning approach Scissorhands. Initially,Scissorhands identifies the most pertinent parameters in the given modelrelative to the forgetting data via connection sensitivity. By reinitializingthe most influential top-k percent of these parameters, a trimmed model forerasing the influence of the forgetting data is obtained. Subsequently,Scissorhands fine-tunes the trimmed model with a gradient projection-basedapproach, seeking parameters that preserve information on the remaining datawhile discarding information related to the forgetting data. Our experimentalresults, conducted across image classification and image generation tasks,demonstrate that Scissorhands, showcases competitive performance when comparedto existing methods.\rTowards Independence Criterion in Machine Unlearning of Features and Labels\nLing Han, Nanqing Luo, Hao Huang, Jing Chen, Mary-Anne Hartley\nabstract\rabstract: This work delves into the complexities of machine unlearning in the face ofdistributional shifts, particularly focusing on the challenges posed bynon-uniform feature and label removal. With the advent of regulations like theGDPR emphasizing data privacy and the right to be forgotten, machine learningmodels face the daunting task of unlearning sensitive information withoutcompromising their integrity or performance. Our research introduces a novelapproach that leverages influence functions and principles of distributionalindependence to address these challenges. By proposing a comprehensiveframework for machine unlearning, we aim to ensure privacy protection whilemaintaining model performance and adaptability across varying distributions.Our method not only facilitates efficient data removal but also dynamicallyadjusts the model to preserve its generalization capabilities. Throughextensive experimentation, we demonstrate the efficacy of our approach inscenarios characterized by significant distributional shifts, makingsubstantial contributions to the field of machine unlearning. This researchpaves the way for developing more resilient and adaptable unlearningtechniques, ensuring models remain robust and accurate in the dynamic landscapeof data privacy and machine learning.\r12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning\nYoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael Hersche, Leo Zhao, Abbas Rahimi, Luca Benini\nabstract\rabstract: Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systemsto expand their inference capabilities to new classes using only a few labeledexamples, without forgetting the previously learned classes. Classicalbackpropagation-based learning and its variants are often unsuitable forbattery-powered, memory-constrained systems at the extreme edge. In this work,we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on alightweight model consisting of a pretrained and metalearned feature extractorand an expandable explicit memory storing the class prototypes. Thearchitecture is pretrained with a novel feature orthogonality regularizationand metalearned with a multi-margin loss. For learning a new class, ourapproach extends the explicit memory with novel class prototypes, while theremaining architecture is kept frozen. This allows learning previously unseenclasses based on only a few examples with one single pass (hence online).O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,achieving state-of-the-art results. Tailored for ultra-low-power platforms, weimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating onlinelearning capabilities within just 12 mJ per new class.\r2024-03-11\nTask-Aware Machine Unlearning and Its Application in Load Forecasting\nWangkun Xu, Fei Teng\nabstract\rabstract: Data privacy and security have become a non-negligible factor in loadforecasting. Previous researches mainly focus on training stage enhancement.However, once the model is trained and deployed, it may need to `forget\u0026rsquo; (i.e.,remove the impact of) part of training data if the these data are found to bemalicious or as requested by the data owner. This paper introduces the conceptof machine unlearning which is specifically designed to remove the influence ofpart of the dataset on an already trained forecaster. However, directunlearning inevitably degrades the model generalization ability. To balancebetween unlearning completeness and model performance, a performance-awarealgorithm is proposed by evaluating the sensitivity of local model parameterchange using influence function and sample re-weighting. Furthermore, weobserve that the statistical criterion such as mean squared error, cannot fullyreflect the operation cost of the downstream tasks in power system. Therefore,a task-aware machine unlearning is proposed whose objective is a trileveloptimization with dispatch and redispatch problems considered. We theoreticallyprove the existence of the gradient of such an objective, which is key tore-weighting the remaining samples. We tested the unlearning algorithms onlinear, CNN, and MLP-Mixer based load forecasters with a realistic loaddataset. The simulation demonstrates the balance between unlearningcompleteness and operational cost. All codes can be found athttps://github.com/xuwkk/task_aware_machine_unlearning.\r2024-03-10\nProbing Image Compression For Class-Incremental Learning\nJustin Yang, Zhihao Duan, Andrew Peng, Yuning Huang, Jiangpeng He, Fengqing Zhu\nabstract\rabstract: Image compression emerges as a pivotal tool in the efficient handling andtransmission of digital images. Its ability to substantially reduce file sizenot only facilitates enhanced data storage capacity but also potentially bringsadvantages to the development of continual machine learning (ML) systems, whichlearn new knowledge incrementally from sequential data. Continual ML systemsoften rely on storing representative samples, also known as exemplars, within alimited memory constraint to maintain the performance on previously learneddata. These methods are known as memory replay-based algorithms and have proveneffective at mitigating the detrimental effects of catastrophic forgetting.Nonetheless, the limited memory buffer size often falls short of adequatelyrepresenting the entire data distribution. In this paper, we explore the use ofimage compression as a strategy to enhance the buffer\u0026rsquo;s capacity, therebyincreasing exemplar diversity. However, directly using compressed exemplarsintroduces domain shift during continual ML, marked by a discrepancy betweencompressed training data and uncompressed testing data. Additionally, it isessential to determine the appropriate compression algorithm and select themost effective rate for continual ML systems to balance the trade-off betweenexemplar quality and quantity. To this end, we introduce a new framework toincorporate image compression for continual ML including a pre-processing datacompression step and an efficient compression rate/algorithm selection method.We conduct extensive experiments on CIFAR-100 and ImageNet datasets and showthat our method significantly improves image classification accuracy incontinual ML settings.\r2024-03-09\nDetecting Pretraining Data from Large Language Models\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\r2024-03-08\nCooperative data-driven modeling\nAleksandr Dekhovich, O. Taylan Turan, Jiaxiang Yi, Miguel A. Bessa\nabstract\rabstract: Data-driven modeling in mechanics is evolving rapidly based on recent machinelearning advances, especially on artificial neural networks. As the fieldmatures, new data and models created by different groups become available,opening possibilities for cooperative modeling. However, artificial neuralnetworks suffer from catastrophic forgetting, i.e. they forget how to performan old task when trained on a new one. This hinders cooperation becauseadapting an existing model for a new task affects the performance on a previoustask trained by someone else. The authors developed a continual learning methodthat addresses this issue, applying it here for the first time to solidmechanics. In particular, the method is applied to recurrent neural networks topredict history-dependent plasticity behavior, although it can be used on anyother architecture (feedforward, convolutional, etc.) and to predict otherphenomena. This work intends to spawn future developments on continual learningthat will foster cooperative strategies among the mechanics community to solveincreasingly challenging problems. We show that the chosen continual learningstrategy can sequentially learn several constitutive laws without forgettingthem, using less data to achieve the same error as standard (non-cooperative)training of one law per model.\r2024-03-06\nEternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten\nMeem Arafat Manab\nabstract\rabstract: As we keep rapidly advancing toward an era where artificial intelligence is aconstant and normative experience for most of us, we must also be aware of whatthis vision and this progress entail. By first approximating neural connectionsand activities in computer circuits and then creating more and moresophisticated versions of this crude approximation, we are now facing an age tocome where modern deep learning-based artificial intelligence systems canrightly be called thinking machines, and they are sometimes even lauded fortheir emergent behavior and black-box approaches. But as we create morepowerful electronic brains, with billions of neural connections and parameters,can we guarantee that these mammoths built of artificial neurons will be ableto forget the data that we store in them? If they are at some level like abrain, can the right to be forgotten still be protected while dealing withthese AIs? The essential gap between machine learning and the RTBF is exploredin this article, with a premonition of far-reaching conclusions if the gap isnot bridged or reconciled any time soon. The core argument is that deeplearning models, due to their structure and size, cannot be expected to forgetor delete a data as it would be expected from a tabular database, and theyshould be treated more like a mechanical brain, albeit still in development.\rDOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors\nChia-Hao Li, Niraj K. Jha\nabstract\rabstract: Modern advances in machine learning (ML) and wearable medical sensors (WMSs)in edge devices have enabled ML-driven disease detection for smart healthcare.Conventional ML-driven methods for disease detection rely on customizingindividual models for each disease and its corresponding WMS data. However,such methods lack adaptability to distribution shifts and new taskclassification classes. In addition, they need to be rearchitected andretrained from scratch for each new disease. Moreover, installing multiple MLmodels in an edge device consumes excessive memory, drains the battery faster,and complicates the detection process. To address these challenges, we proposeDOCTOR, a multi-disease detection continual learning (CL) framework based onWMSs. It employs a multi-headed deep neural network (DNN) and a replay-style CLalgorithm. The CL algorithm enables the framework to continually learn newmissions where different data distributions, classification classes, anddisease detection tasks are introduced sequentially. It counteractscatastrophic forgetting with a data preservation method and a synthetic datageneration (SDG) module. The data preservation method preserves the mostinformative subset of real training data from previous missions for exemplarreplay. The SDG module models the probability distribution of the real trainingdata and generates synthetic data for generative replay while retaining dataprivacy. The multi-headed DNN enables DOCTOR to detect multiple diseasessimultaneously based on user WMS data. We demonstrate DOCTOR\u0026rsquo;s efficacy inmaintaining high disease classification accuracy with a single DNN model invarious CL experiments. In complex scenarios, DOCTOR achieves 1.43 times betteraverage test accuracy, 1.25 times better F1-score, and 0.41 higher backwardtransfer than the naive fine-tuning framework with a small model size of lessthan 350KB.\r2024-03-04\nDemolition and Reinforcement of Memories in Spin-Glass-like Neural Networks\nEnrico Ventura\nabstract\rabstract: Statistical mechanics has made significant contributions to the study ofbiological neural systems by modeling them as recurrent networks ofinterconnected units with adjustable interactions. Several algorithms have beenproposed to optimize the neural connections to enable network tasks such asinformation storage (i.e. associative memory) and learning probabilitydistributions from data (i.e. generative modeling). Among these methods, theUnlearning algorithm, aligned with emerging theories of synaptic plasticity,was introduced by John Hopfield and collaborators. The primary objective ofthis thesis is to understand the effectiveness of Unlearning in bothassociative memory models and generative models. Initially, we demonstrate thatthe Unlearning algorithm can be simplified to a linear perceptron model whichlearns from noisy examples featuring specific internal correlations. Theselection of structured training data enables an associative memory model toretrieve concepts as attractors of a neural dynamics with considerable basinsof attraction. Subsequently, a novel regularization technique for BoltzmannMachines is presented, proving to outperform previously developed methods inlearning hidden probability distributions from data-sets. The Unlearning ruleis derived from this new regularized algorithm and is showed to be comparable,in terms of inferential performance, to traditional Boltzmann-Machine learning.\r2024-03-02\nDissecting Language Models: Machine Unlearning via Selective Pruning\nNicholas Pochinkov, Nandi Schoots\nabstract\rabstract: Understanding and shaping the behaviour of Large Language Models (LLMs) isincreasingly important as applications become more powerful and more frequentlyadopted. This paper introduces a machine unlearning method specificallydesigned for LLMs. We introduce a selective pruning method for LLMs thatremoves neurons based on their relative importance on a targeted capabilitycompared to overall network performance. This approach is a compute- anddata-efficient method for identifying and removing neurons that enable specificbehaviours. Our findings reveal that both feed-forward and attention neurons inLLMs are specialized; that is, for specific tasks, certain neurons are morecrucial than others.\r2024-03-01\nSalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation\nChongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia Liu\nabstract\rabstract: With evolving data regulations, machine unlearning (MU) has become animportant tool for fostering trust and safety in today\u0026rsquo;s AI models. However,existing MU methods focusing on data and/or weight perspectives often sufferlimitations in unlearning accuracy, stability, and cross-domain applicability.To address these challenges, we introduce the concept of \u0026lsquo;weight saliency\u0026rsquo; forMU, drawing parallels with input saliency in model explanation. This innovationdirects MU\u0026rsquo;s attention toward specific model weights rather than the entiremodel, improving effectiveness and efficiency. The resultant method that wecall saliency unlearning (SalUn) narrows the performance gap with \u0026rsquo;exact\u0026rsquo;unlearning (model retraining from scratch after removing the forgetting datapoints). To the best of our knowledge, SalUn is the first principled MUapproach that can effectively erase the influence of forgetting data, classes,or concepts in both image classification and generation tasks. As highlightedbelow, For example, SalUn yields a stability advantage in high-variance randomdata forgetting, e.g., with a 0.2% gap compared to exact unlearning on theCIFAR-10 dataset. Moreover, in preventing conditional diffusion models fromgenerating harmful images, SalUn achieves nearly 100% unlearning accuracy,outperforming current state-of-the-art baselines like Erased Stable Diffusionand Forget-Me-Not. Codes are available athttps://github.com/OPTML-Group/Unlearn-Saliency. (WARNING: This paper containsmodel outputs that may be offensive in nature.)\r2024-02-29\nLoss-Free Machine Unlearning\nJack Foster, Stefan Schoepf, Alexandra Brintrup\nabstract\rabstract: We present a machine unlearning approach that is both retraining- andlabel-free. Most existing machine unlearning approaches require a model to befine-tuned to remove information while preserving performance. This iscomputationally expensive and necessitates the storage of the whole dataset forthe lifetime of the model. Retraining-free approaches often utilise Fisherinformation, which is derived from the loss and requires labelled data whichmay not be available. Thus, we present an extension to the Selective SynapticDampening algorithm, substituting the diagonal of the Fisher information matrixfor the gradient of the l2 norm of the model output to approximate sensitivity.We evaluate our method in a range of experiments using ResNet18 and VisionTransformer. Results show our label-free method is competitive with existingstate-of-the-art approaches.\rInfinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization\nSebastian Dziadzio, Çağatay Yıldız, Gido M. van de Ven, Tomasz Trzciński, Tinne Tuytelaars, Matthias Bethge\nabstract\rabstract: The ability of machine learning systems to learn continually is hindered bycatastrophic forgetting, the tendency of neural networks to overwrite existingknowledge when learning a new task. Continual learning methods alleviate thisproblem through regularization, parameter isolation, or rehearsal, but they aretypically evaluated on benchmarks comprising only a handful of tasks. Incontrast, humans are able to learn continually in dynamic, open-worldenvironments, effortlessly achieving one-shot memorization of unfamiliarobjects and reliably recognizing them under various transformations. To makeprogress towards closing this gap, we introduce Infinite dSprites, aparsimonious tool for creating continual classification and disentanglementbenchmarks of arbitrary length and with full control over generative factors.We show that over a sufficiently long time horizon, the performance of allmajor types of continual learning methods deteriorates on this simplebenchmark. Thus, Infinite dSprites highlights an important aspect of continuallearning that has not received enough attention so far: given a finitemodelling capacity and an arbitrarily long learning horizon, efficient learningrequires memorizing class-specific information and accumulating knowledge aboutgeneral mechanisms. In a simple setting with direct supervision on thegenerative factors, we show how learning class-agnostic transformations offersa way to circumvent catastrophic forgetting and improve classification accuracyover time. Our approach sets the stage for continual learning over hundreds oftasks with explicit control over memorization and forgetting, emphasizingopen-set classification and one-shot generalization.\r2024-02-27\nMachine Unlearning of Pre-trained Large Language Models\nJin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue\nabstract\rabstract: This study investigates the concept of the `right to be forgotten\u0026rsquo; within thecontext of large language models (LLMs). We explore machine unlearning as apivotal solution, with a focus on pre-trained models\u0026ndash;a notablyunder-researched area. Our research delineates a comprehensive framework formachine unlearning in pre-trained LLMs, encompassing a critical analysis ofseven diverse unlearning methods. Through rigorous evaluation using curateddatasets from arXiv, books, and GitHub, we establish a robust benchmark forunlearning performance, demonstrating that these methods are over $10^5$ timesmore computationally efficient than retraining. Our results show thatintegrating gradient ascent with gradient descent on in-distribution dataimproves hyperparameter robustness. We also provide detailed guidelines forefficient hyperparameter tuning in the unlearning process. Our findings advancethe discourse on ethical AI practices, offering substantive insights into themechanics of machine unlearning for pre-trained LLMs and underscoring thepotential for responsible AI development.\r2024-02-26\nEight Methods to Evaluate Robust Unlearning in LLMs\nAengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan Hadfield-Menell\nabstract\rabstract: Machine unlearning can be useful for removing harmful capabilities andmemorized text from large language models (LLMs), but there are not yetstandardized methods for rigorously evaluating it. In this paper, we firstsurvey techniques and limitations of existing unlearning evaluations. Second,we apply a comprehensive set of tests for the robustness and competitiveness ofunlearning in the \u0026ldquo;Who\u0026rsquo;s Harry Potter\u0026rdquo; (WHP) model from Eldan and Russinovich(2023). While WHP\u0026rsquo;s unlearning generalizes well when evaluated with the\u0026quot;Familiarity\u0026quot; metric from Eldan and Russinovich, we find i)higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHPperforms on par with the original model on Harry Potter Q\u0026amp;A tasks, iii) itrepresents latent knowledge comparably to the original model, and iv) there iscollateral unlearning in related domains. Overall, our results highlight theimportance of comprehensive unlearning evaluation that avoids ad-hoc metrics.\rUnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models\nYihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu\nabstract\rabstract: The rapid advancement of diffusion models (DMs) has not only transformedvarious real-world industries but has also introduced negative societalconcerns, including the generation of harmful content, copyright disputes, andthe rise of stereotypes and biases. To mitigate these issues, machineunlearning (MU) has emerged as a potential solution, demonstrating its abilityto remove undesired generative capabilities of DMs in various applications.However, by examining existing MU evaluation methods, we uncover several keychallenges that can result in incomplete, inaccurate, or biased evaluations forMU in DMs. To address them, we enhance the evaluation metrics for MU, includingthe introduction of an often-overlooked retainability measurement for DMspost-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensivehigh-resolution stylized image dataset that facilitates us to evaluate theunlearning of artistic painting styles in conjunction with associated imageobjects. We show that this dataset plays a pivotal role in establishing astandardized and automated evaluation framework for MU techniques on DMs,featuring 7 quantitative metrics to address various aspects of unlearningeffectiveness. Through extensive experiments, we benchmark 5 state-of-the-artMU methods, revealing novel insights into their pros and cons, and theunderlying unlearning mechanisms. Furthermore, we demonstrate the potential ofUnlearnCanvas to benchmark other generative modeling tasks, such as styletransfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce allthe results in this work can be found athttps://github.com/OPTML-Group/UnlearnCanvas.\rAvoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation\nNicki Barari, Xin Lian, Christopher J. MacLellan\nabstract\rabstract: Deep neural networks have excelled in machine learning, particularly invision tasks, however, they often suffer from catastrophic forgetting whenlearning new tasks sequentially. In this work, we propose Cobweb4V, a novelvisual classification approach that builds on Cobweb, a human like learningsystem that is inspired by the way humans incrementally learn new concepts overtime. In this research, we conduct a comprehensive evaluation, showcasing theproficiency of Cobweb4V in learning visual concepts, requiring less data toachieve effective learning outcomes compared to traditional methods,maintaining stable performance over time, and achieving commendable asymptoticbehavior, without catastrophic forgetting effects. These characteristics alignwith learning strategies in human cognition, positioning Cobweb4V as apromising alternative to neural network approaches.\r2024-02-23\nMachine unlearning through fine-grained model parameters perturbation\nZhiwei Zuo, Zhuo Tang, Kenli Li, Anwitaman Datta\nabstract\rabstract: Machine unlearning techniques, which involve retracting data records andreducing influence of said data on trained models, help with the user privacyprotection objective but incur significant computational costs. Weightperturbation-based unlearning is a general approach, but it typically involvesglobally modifying the parameters. We propose fine-grained Top-K and Random-kparameters perturbed inexact machine unlearning strategies that address theprivacy needs while keeping the computational costs tractable. In order to demonstrate the efficacy of our strategies we also tackle thechallenge of evaluating the effectiveness of machine unlearning by consideringthe model\u0026rsquo;s generalization performance across both unlearning and remainingdata. To better assess the unlearning effect and model generalization, wepropose novel metrics, namely, the forgetting rate and memory retention rate.However, for inexact machine unlearning, current metrics are inadequate inquantifying the degree of forgetting that occurs after unlearning strategiesare applied. To address this, we introduce SPD-GAN, which subtly perturbs thedistribution of data targeted for unlearning. Then, we evaluate the degree ofunlearning by measuring the performance difference of the models on theperturbed unlearning data before and after the unlearning process. Byimplementing these innovative techniques and metrics, we achievecomputationally efficacious privacy protection in machine learning applicationswithout significant sacrifice of model performance. Furthermore, this approachprovides a novel method for evaluating the degree of unlearning.\rMachine Unlearning by Suppressing Sample Contribution\nXinwen Cheng, Zhehao Huang, Xiaolin Huang\nabstract\rabstract: Machine Unlearning (MU) is to forget data from a well-trained model, which ispractically important due to the \u0026ldquo;right to be forgotten\u0026rdquo;. In this paper, westart from the fundamental distinction between training data and unseen data ontheir contribution to the model: the training data contributes to the finalmodel while the unseen data does not. We theoretically discover that the inputsensitivity can approximately measure the contribution and practically designan algorithm, called MU-Mis (machine unlearning via minimizing inputsensitivity), to suppress the contribution of the forgetting data. Experimentalresults demonstrate that MU-Mis outperforms state-of-the-art MU methodssignificantly. Additionally, MU-Mis aligns more closely with the application ofMU as it does not require the use of remaining data.\r2024-02-22\nBreaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning\nZheyuan Liu, Guangyao Dou, Yijun Tian, Chunhui Zhang, Eli Chien, Ziwei Zhu\nabstract\rabstract: Machine Unlearning (MU) algorithms have become increasingly critical due tothe imperative adherence to data privacy regulations. The primary objective ofMU is to erase the influence of specific data samples on a given model withoutthe need to retrain it from scratch. Accordingly, existing methods focus onmaximizing user privacy protection. However, there are different degrees ofprivacy regulations for each real-world web-based application. Exploring thefull spectrum of trade-offs between privacy, model utility, and runtimeefficiency is critical for practical unlearning scenarios. Furthermore,designing the MU algorithm with simple control of the aforementioned trade-offis desirable but challenging due to the inherent complex interaction. Toaddress the challenges, we present Controllable Machine Unlearning (ConMU), anovel framework designed to facilitate the calibration of MU. The ConMUframework contains three integral modules: an important data selection modulethat reconciles the runtime efficiency and model generalization, a progressiveGaussian mechanism module that balances privacy and model generalization, andan unlearning proxy that controls the trade-offs between privacy and runtimeefficiency. Comprehensive experiments on various benchmark datasets havedemonstrated the robust adaptability of our control mechanism and itssuperiority over established unlearning methods. ConMU explores the fullspectrum of the Privacy-Utility-Efficiency trade-off and allows practitionersto account for different real-world regulations. Source code available at:https://github.com/guangyaodou/ConMU.\rRefuteBench: Evaluating Refuting Instruction-Following for Large Language Models\nJianhao Yan, Yun Luo, Yue Zhang\nabstract\rabstract: The application scope of large language models (LLMs) is increasinglyexpanding. In practical use, users might provide feedback based on the model\u0026rsquo;soutput, hoping for a responsive model that can complete responses according totheir feedback. Whether the model can appropriately respond to users\u0026rsquo; refutingfeedback and consistently follow through with execution has not been thoroughlyanalyzed. In light of this, this paper proposes a comprehensive benchmark,RefuteBench, covering tasks such as question answering, machine translation,and email writing. The evaluation aims to assess whether models can positivelyaccept feedback in form of refuting instructions and whether they canconsistently adhere to user demands throughout the conversation. We conductevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibitinclination to their internal knowledge, often failing to comply with userfeedback. Additionally, as the length of the conversation increases, modelsgradually forget the user\u0026rsquo;s stated feedback and roll back to their ownresponses. We further propose a recall-and-repeat prompts as a simple andeffective way to enhance the model\u0026rsquo;s responsiveness to feedback.\r2024-02-21\nCorrective Machine Unlearning\nShashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal\nabstract\rabstract: Machine Learning models increasingly face data integrity challenges due tothe use of large-scale training datasets drawn from the internet. We study whatmodel developers can do if they detect that some data was manipulated orincorrect. Such manipulated data can cause adverse effects like vulnerabilityto backdoored samples, systematic biases, and in general, reduced accuracy oncertain input domains. Often, all manipulated training samples are not known,and only a small, representative subset of the affected data is flagged. We formalize \u0026ldquo;Corrective Machine Unlearning\u0026rdquo; as the problem of mitigating theimpact of data affected by unknown manipulations on a trained model, possiblyknowing only a subset of impacted samples. We demonstrate that the problem ofcorrective unlearning has significantly different requirements from traditionalprivacy-oriented unlearning. We find most existing unlearning methods,including the gold-standard retraining-from-scratch, require most of themanipulated data to be identified for effective corrective unlearning. However,one approach, SSD, achieves limited success in unlearning adverse effects withjust a small portion of the manipulated samples, showing the tractability ofthis setting. We hope our work spurs research towards developing better methodsfor corrective unlearning and offers practitioners a new strategy to handledata integrity challenges arising from web-scale training.\r2024-02-20\nRegularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting\nElena Agliari, Francesco Alemanno, Miriam Aquaro, Alberto Fachechi\nabstract\rabstract: In this work we approach attractor neural networks from a machine learningperspective: we look for optimal network parameters by applying a gradientdescent over a regularized loss function. Within this framework, the optimalneuron-interaction matrices turn out to be a class of matrices which correspondto Hebbian kernels revised by a reiterated unlearning protocol. Remarkably, theextent of such unlearning is proved to be related to the regularizationhyperparameter of the loss function and to the training time. Thus, we candesign strategies to avoid overfitting that are formulated in terms ofregularization and early-stopping tuning. The generalization capabilities ofthese attractor networks are also investigated: analytical results are obtainedfor random synthetic datasets, next, the emerging picture is corroborated bynumerical experiments that highlight the existence of several regimes (i.e.,overfitting, failure and success) as the dataset parameters are varied.\rTowards Robust Graph Incremental Learning on Evolving Graphs\nJunwei Su, Difan Zou, Zijun Zhang, Chuan Wu\nabstract\rabstract: Incremental learning is a machine learning approach that involves training amodel on a sequence of tasks, rather than all tasks at once. This ability tolearn incrementally from a stream of tasks is crucial for many real-worldapplications. However, incremental learning is a challenging problem ongraph-structured data, as many graph-related problems involve prediction tasksfor each individual node, known as Node-wise Graph Incremental Learning (NGIL).This introduces non-independent and non-identically distributed characteristicsin the sample data generation process, making it difficult to maintain theperformance of the model as new tasks are added. In this paper, we focus on theinductive NGIL problem, which accounts for the evolution of graph structure(structural shift) induced by emerging tasks. We provide a formal formulationand analysis of the problem, and propose a novel regularization-based techniquecalled Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of thestructural shift on catastrophic forgetting of the inductive NGIL problem. Weshow that the structural shift can lead to a shift in the input distributionfor the existing tasks, and further lead to an increased risk of catastrophicforgetting. Through comprehensive empirical studies with several benchmarkdatasets, we demonstrate that our proposed method,Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt toimprove the performance of state-of-the-art GNN incremental learning frameworksin the inductive setting.\r2024-02-19\nReinforcement Unlearning\nDayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi, Sheng Shen, Wanlei Zhou, Minhui Xue\nabstract\rabstract: Machine unlearning refers to the process of mitigating the influence ofspecific training data on machine learning models based on removal requestsfrom data owners. However, one important area that has been largely overlookedin the research of unlearning is reinforcement learning. Reinforcement learningfocuses on training an agent to make optimal decisions within an environment tomaximize its cumulative rewards. During the training, the agent tends tomemorize the features of the environment, which raises a significant concernabout privacy. As per data protection regulations, the owner of the environmentholds the right to revoke access to the agent\u0026rsquo;s training data, thusnecessitating the development of a novel and pressing research field, known as\\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revokingentire environments rather than individual data samples. This uniquecharacteristic presents three distinct challenges: 1) how to propose unlearningschemes for environments; 2) how to avoid degrading the agent\u0026rsquo;s performance inremaining environments; and 3) how to evaluate the effectiveness of unlearning.To tackle these challenges, we propose two reinforcement unlearning methods.The first method is based on decremental reinforcement learning, which aims toerase the agent\u0026rsquo;s previously acquired knowledge gradually. The second methodleverages environment poisoning attacks, which encourage the agent to learnnew, albeit incorrect, knowledge to remove the unlearning environment.Particularly, to tackle the third challenge, we introduce the concept of``environment inference attack\u0026rsquo;\u0026rsquo; to evaluate the unlearning outcomes.\rHebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks\nMingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin\nabstract\rabstract: Neuromorphic computing with spiking neural networks is promising forenergy-efficient artificial intelligence (AI) applications. However, differentfrom humans who continually learn different tasks in a lifetime, neural networkmodels suffer from catastrophic forgetting. How could neuronal operations solvethis problem is an important question for AI and neuroscience. Many previousstudies draw inspiration from observed neuroscience phenomena and proposeepisodic replay or synaptic metaplasticity, but they are not guaranteed toexplicitly preserve knowledge for neuron populations. Other works focus onmachine learning methods with more mathematical grounding, e.g., orthogonalprojection on high dimensional spaces, but there is no neural correspondencefor neuromorphic computing. In this work, we develop a new method with neuronaloperations based on lateral connections and Hebbian learning, which can protectknowledge by projecting activity traces of neurons into an orthogonal subspaceso that synaptic weight update will not interfere with old tasks. We show thatHebbian and anti-Hebbian learning on recurrent lateral connections caneffectively extract the principal subspace of neural activities and enableorthogonal projection. This provides new insights into how neural circuits andHebbian learning can help continual learning, and also how the concept oforthogonal projection can be realized in neuronal systems. Our method is alsoflexible to utilize arbitrary training methods based on presynapticactivities/traces. Experiments show that our method consistently solvesforgetting for spiking neural networks with nearly zero forgetting undervarious supervised training methods with different error propagationapproaches, and outperforms previous approaches under various settings. Ourmethod can pave a solid path for building continual neuromorphic computingsystems.\r2024-02-16\nFair Machine Unlearning: Data Removal while Mitigating Disparities\nAlex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju\nabstract\rabstract: The Right to be Forgotten is a core principle outlined by regulatoryframeworks such as the EU\u0026rsquo;s General Data Protection Regulation (GDPR). Thisprinciple allows individuals to request that their personal data be deletedfrom deployed machine learning models. While \u0026ldquo;forgetting\u0026rdquo; can be naivelyachieved by retraining on the remaining dataset, it is computationallyexpensive to do to so with each new request. As such, several machineunlearning methods have been proposed as efficient alternatives to retraining.These methods aim to approximate the predictive performance of retraining, butfail to consider how unlearning impacts other properties critical to real-worldapplications such as fairness. In this work, we demonstrate that most efficientunlearning methods cannot accommodate popular fairness interventions, and wepropose the first fair machine unlearning method that can efficiently unlearndata instances from a fair objective. We derive theoretical results whichdemonstrate that our method can provably unlearn data and provably maintainfairness performance. Extensive experimentation with real-world datasetshighlight the efficacy of our method at unlearning data instances whilepreserving fairness.\rFederated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics\nNicolò Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista\nabstract\rabstract: Federated Learning (FL) enables collaborative training of a Machine Learning(ML) model across multiple parties, facilitating the preservation of users\u0026rsquo; andinstitutions\u0026rsquo; privacy by keeping data stored locally. Instead of centralizingraw data, FL exchanges locally refined model parameters to build a global modelincrementally. While FL is more compliant with emerging regulations such as theEuropean General Data Protection Regulation (GDPR), ensuring the right to beforgotten in this context - allowing FL participants to remove their datacontributions from the learned model - remains unclear. In addition, it isrecognized that malicious clients may inject backdoors into the global modelthrough updates, e.g. to generate mispredictions on specially crafted dataexamples. Consequently, there is the need for mechanisms that can guaranteeindividuals the possibility to remove their data and erase maliciouscontributions even after aggregation, without compromising the already acquired\u0026quot;good\u0026quot; knowledge. This highlights the necessity for novel Federated Unlearning(FU) algorithms, which can efficiently remove specific clients\u0026rsquo; contributionswithout full model retraining. This survey provides background concepts,empirical evidence, and practical guidelines to design/implement efficient FUschemes. Our study includes a detailed analysis of the metrics for evaluatingunlearning in FL and presents an in-depth literature review categorizingstate-of-the-art FU contributions under a novel taxonomy. Finally, we outlinethe most relevant and still open technical challenges, by identifying the mostpromising research directions in the field.\r2024-02-15\nRethinking Machine Unlearning for Large Language Models\nSijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\r2024-02-13\nDiscriminative Adversarial Unlearning\nRohan Sharma, Shijie Zhou, Kaiyi Ji, Changyou Chen\nabstract\rabstract: We introduce a novel machine unlearning framework founded upon theestablished principles of the min-max optimization paradigm. We capitalize onthe capabilities of strong Membership Inference Attacks (MIA) to facilitate theunlearning of specific samples from a trained model. We consider the scenarioof two networks, the attacker $\\mathbf{A}$ and the trained defender$\\mathbf{D}$ pitted against each other in an adversarial objective, wherein theattacker aims at teasing out the information of the data to be unlearned inorder to infer membership, and the defender unlearns to defend the networkagainst the attack, whilst preserving its general performance. The algorithmcan be trained end-to-end using backpropagation, following the well knowniterative min-max approach in updating the attacker and the defender. Weadditionally incorporate a self-supervised objective effectively addressing thefeature space discrepancies between the forget set and the validation set,enhancing unlearning performance. Our proposed algorithm closely approximatesthe ideal benchmark of retraining from scratch for both random sampleforgetting and class-wise forgetting schemes on standard machine-unlearningdatasets. Specifically, on the class unlearning scheme, the method demonstratesnear-optimal performance and comprehensively overcomes known methods over therandom sample forgetting scheme across all metrics and multiple network pruningstrategies.\rDistal Interference: Exploring the Limits of Model-Based Continual Learning\nHeinrich van Deventer, Anna Sergeevna Bosman\nabstract\rabstract: Continual learning is the sequential learning of different tasks by a machinelearning model. Continual learning is known to be hindered by catastrophicinterference or forgetting, i.e. rapid unlearning of earlier learned tasks whennew tasks are learned. Despite their practical success, artificial neuralnetworks (ANNs) are prone to catastrophic interference. This study analyses howgradient descent and overlapping representations between distant input pointslead to distal interference and catastrophic interference. Distal interferencerefers to the phenomenon where training a model on a subset of the domain leadsto non-local changes on other subsets of the domain. This study shows thatuniformly trainable models without distal interference must be exponentiallylarge. A novel antisymmetric bounded exponential layer B-spline ANNarchitecture named ABEL-Spline is proposed that can approximate any continuousfunction, is uniformly trainable, has polynomial computational complexity, andprovides some guarantees for distal interference. Experiments are presented todemonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are alsoevaluated on benchmark regression problems. It is concluded that the weakerdistal interference guarantees in ABEL-Splines are insufficient for model-onlycontinual learning. It is conjectured that continual learning with polynomialcomplexity models requires augmentation of the training data or algorithm.\r2024-02-12\nOn the Costs and Benefits of Adopting Lifelong Learning for Software Analytics \u0026ndash; Empirical Study on Brown Build and Risk Prediction\nDoriane Olewicki, Sarra Habchi, Mathieu Nayrolles, Mojtaba Faramarzi, Sarath Chandar, Bram Adams\nabstract\rabstract: Nowadays, software analytics tools using machine learning (ML) models to, forexample, predict the risk of a code change are well established. However, asthe goals of a project shift over time, and developers and their habits change,the performance of said models tends to degrade (drift) over time. Currentretraining practices typically require retraining a new model from scratch on alarge updated dataset when performance decay is observed, thus incurring acomputational cost; also there is no continuity between the models as the pastmodel is discarded and ignored during the new model training. Even though theliterature has taken interest in online learning approaches, those have rarelybeen integrated and evaluated in industrial environments. This paper evaluatesthe use of lifelong learning (LL) for industrial use cases at Ubisoft,evaluating both the performance and the required computational effort incomparison to the retraining-from-scratch approaches commonly used by theindustry. LL is used to continuously build and maintain ML-based softwareanalytics tools using an incremental learner that progressively updates the oldmodel using new data. To avoid so-called \u0026ldquo;catastrophic forgetting\u0026rdquo; of importantolder data points, we adopt a replay buffer of older data, which still allowsus to drastically reduce the size of the overall training dataset, and hencemodel training time.\r2024-02-09\nUnlearning regularization for Boltzmann Machines\nEnrico Ventura, Simona Cocco, Rémi Monasson, Francesco Zamponi\nabstract\rabstract: Boltzmann Machines (BMs) are graphical models with interconnected binaryunits, employed for the unsupervised modeling of data distributions. Whentrained on real data, BMs show the tendency to behave like critical systems,displaying a high susceptibility of the model under a small rescaling of theinferred parameters. This behaviour is not convenient for the purpose ofgenerating data, because it slows down the sampling process, and induces themodel to overfit the training-data. In this study, we introduce aregularization method for BMs to improve the robustness of the model underrescaling of the parameters. The new technique shares formal similarities withthe unlearning algorithm, an iterative procedure used to improve memoryassociativity in Hopfield-like neural networks. We test our unlearningregularization on synthetic data generated by two simple models, theCurie-Weiss ferromagnetic model and the Sherrington-Kirkpatrick spin glassmodel, and we show that it outperforms $L_p$-norm schemes. Finally, we discussthe role of parameter initialization.\r2024-02-08\nSelective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\nLingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob\nabstract\rabstract: The aim of this study is to investigate Machine Unlearning (MU), a burgeoningfield focused on addressing concerns related to neural models inadvertentlyretaining personal or sensitive data. Here, a novel approach is introduced toachieve precise and selective forgetting within language models. Unlikeprevious methodologies that adopt completely opposing training objectives, thisapproach aims to mitigate adverse effects on language model performance,particularly in generation tasks. Furthermore, two innovative evaluationmetrics are proposed: Sensitive Information Extraction Likelihood (S-EL) andSensitive Information Memory Accuracy (S-MA), designed to gauge theeffectiveness of sensitive information elimination. To reinforce the forgettingframework, an effective method for annotating sensitive scopes is presented,involving both online and offline strategies. The online selection mechanismleverages language probability scores to ensure computational efficiency, whilethe offline annotation entails a robust two-stage process based on LargeLanguage Models (LLMs).\rText2Data: Low-Resource Data Generation with Textual Control\nShiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese\nabstract\rabstract: Natural language serves as a common and straightforward control signal forhumans to interact seamlessly with machines. Recognizing the importance of thisinterface, the machine learning community is investing considerable effort ingenerating data that is semantically coherent with textual instructions. Whilestrides have been made in text-to-data generation spanning image editing, audiosynthesis, video creation, and beyond, low-resource areas characterized byexpensive annotations or complex data structures, such as molecules, motiondynamics, and time series, often lack textual labels. This deficiency impedessupervised learning, thereby constraining the application of advancedgenerative models for text-to-data tasks. In response to these challenges inthe low-resource scenario, we propose Text2Data, a novel approach that utilizesunlabeled data to understand the underlying data distribution through anunsupervised diffusion model. Subsequently, it undergoes controllablefinetuning via a novel constraint optimization-based learning objective thatensures controllability and effectively counteracts catastrophic forgetting.Comprehensive experiments demonstrate that Text2Data is able to achieveenhanced performance regarding controllability across various modalities,including molecules, motions and time series, when compared to existingbaselines.\r2024-02-07\nLangevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning\nEli Chien, Haoyu Wang, Ziang Chen, Pan Li\nabstract\rabstract: Machine unlearning has raised significant interest with the adoption of lawsensuring the ``right to be forgotten\u0026rsquo;\u0026rsquo;. Researchers have provided aprobabilistic notion of approximate unlearning under a similar definition ofDifferential Privacy (DP), where privacy is defined as statisticalindistinguishability to retraining from scratch. We propose Langevinunlearning, an unlearning framework based on noisy gradient descent withprivacy guarantees for approximate unlearning problems. Langevin unlearningunifies the DP learning process and the privacy-certified unlearning processwith many algorithmic benefits. These include approximate certified unlearningfor non-convex problems, complexity saving compared to retraining, sequentialand batch unlearning for multiple unlearning requests. We verify thepracticality of Langevin unlearning by studying its privacy-utility-complexitytrade-off via experiments on benchmark datasets, and also demonstrate itssuperiority against gradient-decent-plus-output-perturbation based approximateunlearning.\rViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied to Vision Transformers\nIkhyun Cho, Changyeon Park, Julia Hockenmaier\nabstract\rabstract: Machine unlearning (MUL) is an arising field in machine learning that seeksto erase the learned information of specific training data points from atrained model. Despite the recent active research in MUL within computervision, the majority of work has focused on ResNet-based models. Given thatVision Transformers (ViT) have become the predominant model architecture, adetailed study of MUL specifically tailored to ViT is essential. In this paper,we present comprehensive experiments on ViTs using recent MUL algorithms anddatasets. We anticipate that our experiments, ablation studies, and findingscould provide valuable insights and inspire further research in this field.\rExample-based Explanations for Random Forests using Machine Unlearning\nTanmay Surve, Romila Pradhan\nabstract\rabstract: Tree-based machine learning models, such as decision trees and randomforests, have been hugely successful in classification tasks primarily becauseof their predictive power in supervised learning tasks and ease ofinterpretation. Despite their popularity and power, these models have beenfound to produce unexpected or discriminatory outcomes. Given theiroverwhelming success for most tasks, it is of interest to identify sources oftheir unexpected and discriminatory behavior. However, there has not been muchwork on understanding and debugging tree-based classifiers in the context offairness. We introduce FairDebugger, a system that utilizes recent advances in machineunlearning research to identify training data subsets responsible for instancesof fairness violations in the outcomes of a random forest classifier.FairDebugger generates top-$k$ explanations (in the form of coherent trainingdata subsets) for model unfairness. Toward this goal, FairDebugger firstutilizes machine unlearning to estimate the change in the tree structures ofthe random forest when parts of the underlying training data are removed, andthen leverages the Apriori algorithm from frequent itemset mining to reduce thesubset search space. We empirically evaluate our approach on three real-worlddatasets, and demonstrate that the explanations generated by FairDebugger areconsistent with insights from prior studies on these datasets.\r2024-02-06\nParameter-tuning-free data entry error unlearning with adaptive selective synaptic dampening\nStefan Schoepf, Jack Foster, Alexandra Brintrup\nabstract\rabstract: Data entry constitutes a fundamental component of the machine learningpipeline, yet it frequently results in the introduction of labelling errors.When a model has been trained on a dataset containing such errors itsperformance is reduced. This leads to the challenge of efficiently unlearningthe influence of the erroneous data to improve the model performance withoutneeding to completely retrain the model. While model editing methods exist forcases in which the correct label for a wrong entry is known, we focus on thecase of data entry errors where we do not know the correct labels for theerroneous data. Our contribution is twofold. First, we introduce an extensionto the selective synaptic dampening unlearning method that removes the need forparameter tuning, making unlearning accessible to practitioners. We demonstratethe performance of this extension, adaptive selective synaptic dampening(ASSD), on various ResNet18 and Vision Transformer unlearning tasks. Second, wedemonstrate the performance of ASSD in a supply chain delay prediction problemwith labelling errors using real-world data where we randomly introduce variouslevels of labelling errors. The application of this approach is particularlycompelling in industrial settings, such as supply chain management, where asignificant portion of data entry occurs manually through Excel sheets,rendering it error-prone. ASSD shows strong performance on general unlearningbenchmarks and on the error correction problem where it outperforms fine-tuningfor error correction.\r2024-02-05\nZero-Shot Machine Unlearning at Scale via Lipschitz Regularization\nJack Foster, Kyle Fogarty, Stefan Schoepf, Cengiz Öztireli, Alexandra Brintrup\nabstract\rabstract: To comply with AI and data regulations, the need to forget private orcopyrighted information from trained machine learning models is increasinglyimportant. The key challenge in unlearning is forgetting the necessary data ina timely manner, while preserving model performance. In this work, we addressthe zero-shot unlearning scenario, whereby an unlearning algorithm must be ableto remove data given only a trained model and the data to be forgotten. Undersuch a definition, existing state-of-the-art methods are insufficient. Buildingon the concepts of Lipschitz continuity, we present a method that inducessmoothing of the forget sample\u0026rsquo;s output, with respect to perturbations of thatsample. We show this smoothing successfully results in forgetting whilepreserving general model performance. We perform extensive empirical evaluationof our method over a range of contemporary benchmarks, verifying that ourmethod achieves state-of-the-art performance under the strict constraints ofzero-shot unlearning.\r2024-02-03\nUnlearnable Examples For Time Series\nYujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey\nabstract\rabstract: Unlearnable examples (UEs) refer to training samples modified to beunlearnable to Deep Neural Networks (DNNs). These examples are usuallygenerated by adding error-minimizing noises that can fool a DNN model intobelieving that there is nothing (no error) to learn from the data. The conceptof UE has been proposed as a countermeasure against unauthorized dataexploitation on personal data. While UE has been extensively studied on images,it is unclear how to craft effective UEs for time series data. In this work, weintroduce the first UE generation method to protect time series data fromunauthorized training by deep learning models. To this end, we propose a newform of error-minimizing noise that can be \\emph{selectively} applied tospecific segments of time series, rendering them unlearnable to DNN modelswhile remaining imperceptible to human observers. Through extensive experimentson a wide range of time series datasets, we demonstrate that the proposed UEgeneration method is effective in both classification and generation tasks. Itcan protect time series data against unauthorized exploitation, whilepreserving their utility for legitimate usage, thereby contributing to thedevelopment of secure and trustworthy machine learning systems.\rSeparable Multi-Concept Erasure from Diffusion Models\nMengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong, Baocai Yin\nabstract\rabstract: Large-scale diffusion models, known for their impressive image generationcapabilities, have raised concerns among researchers regarding social impacts,such as the imitation of copyrighted artistic styles. In response, existingapproaches turn to machine unlearning techniques to eliminate unsafe conceptsfrom pre-trained models. However, these methods compromise the generativeperformance and neglect the coupling among multi-concept erasures, as well asthe concept restoration problem. To address these issues, we propose aSeparable Multi-concept Eraser (SepME), which mainly includes two parts: thegeneration of concept-irrelevant representations and the weight decoupling. Theformer aims to avoid unlearning substantial information that is irrelevant toforgotten concepts. The latter separates optimizable model weights, making eachweight increment correspond to a specific concept erasure without affectinggenerative performance on other concepts. Specifically, the weight incrementfor erasing a specified concept is formulated as a linear combination ofsolutions calculated based on other known undesirable concepts. Extensiveexperiments indicate the efficacy of our approach in eliminating concepts,preserving model performance, and offering flexibility in the erasure orrecovery of various concepts.\rUnderstanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis\nOscar Chew, Hsuan-Tien Lin, Kai-Wei Chang, Kuan-Hao Huang\nabstract\rabstract: Recent research has revealed that machine learning models have a tendency toleverage spurious correlations that exist in the training set but may not holdtrue in general circumstances. For instance, a sentiment classifier mayerroneously learn that the token \u0026ldquo;performances\u0026rdquo; is commonly associated withpositive movie reviews. Relying on these spurious correlations degrades theclassifiers performance when it deploys on out-of-distribution data. In thispaper, we examine the implications of spurious correlations through a novelperspective called neighborhood analysis. The analysis uncovers how spuriouscorrelations lead unrelated words to erroneously cluster together in theembedding space. Driven by the analysis, we design a metric to detect spurioustokens and also propose a family of regularization methods, NFL (doN\u0026rsquo;t Forgetyour Language) to mitigate spurious correlations in text classification.Experiments show that NFL can effectively prevent erroneous clusters andsignificantly improve the robustness of classifiers without auxiliary data. Thecode is publicly available athttps://github.com/oscarchew/doNt-Forget-your-Language.\r2024-02-02\nMachine Unlearning for Image-to-Image Generative Models\nGuihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu\nabstract\rabstract: Machine unlearning has emerged as a new paradigm to deliberately forget datasamples from a given model in order to adhere to stringent regulations.However, existing machine unlearning methods have been primarily focused onclassification models, leaving the landscape of unlearning for generativemodels relatively unexplored. This paper serves as a bridge, addressing the gapby providing a unifying framework of machine unlearning for image-to-imagegenerative models. Within this framework, we propose acomputationally-efficient algorithm, underpinned by rigorous theoreticalanalysis, that demonstrates negligible performance degradation on the retainsamples, while effectively removing the information from the forget samples.Empirical studies on two large-scale datasets, ImageNet-1K and Places-365,further show that our algorithm does not rely on the availability of the retainsamples, which further complies with data retention policy. To our bestknowledge, this work is the first that represents systemic, theoretical,empirical explorations of machine unlearning specifically tailored forimage-to-image generative models. Our code is available athttps://github.com/jpmorganchase/l2l-generator-unlearning.\rFRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning\nThanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Xiaofeng Zhu, Qing Li\nabstract\rabstract: Machine Unlearning is an emerging field that addresses data privacy issues byenabling the removal of private or irrelevant data from the Machine Learningprocess. Challenges related to privacy and model efficiency arise from the useof outdated, private, and irrelevant data. These issues compromise both theaccuracy and the computational efficiency of models in both Machine Learningand Unlearning. To mitigate these challenges, we introduce a novel framework,Attention-based Machine Unlearning using Federated Reinforcement Learning(FRAMU). This framework incorporates adaptive learning mechanisms, privacypreservation techniques, and optimization strategies, making it a well-roundedsolution for handling various data sources, either single-modality ormulti-modality, while maintaining accuracy and privacy. FRAMU\u0026rsquo;s strength liesin its adaptability to fluctuating data landscapes, its ability to unlearnoutdated, private, or irrelevant data, and its support for continual modelevolution without compromising privacy. Our experiments, conducted on bothsingle-modality and multi-modality datasets, revealed that FRAMU significantlyoutperformed baseline models. Additional assessments of convergence behaviorand optimization strategies further validate the framework\u0026rsquo;s utility infederated learning applications. Overall, FRAMU advances Machine Unlearning byoffering a robust, privacy-preserving solution that optimizes model performancewhile also addressing key challenges in dynamic data environments.\r2024-02-01\nERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach\nYuke Hu, Jian Lou, Jiaqi Liu, Wangze Ni, Feng Lin, Zhan Qin, Kui Ren\nabstract\rabstract: Over the past years, Machine Learning-as-a-Service (MLaaS) has received asurging demand for supporting Machine Learning-driven services to offerrevolutionized user experience across diverse application areas. MLaaS providesinference service with low inference latency based on an ML model trained usinga dataset collected from numerous individual data owners. Recently, for thesake of data owners\u0026rsquo; privacy and to comply with the \u0026ldquo;right to be forgotten(RTBF)\u0026rdquo; as enacted by data protection legislation, many machine unlearningmethods have been proposed to remove data owners\u0026rsquo; data from trained models upontheir unlearning requests. However, despite their promising efficiency, almostall existing machine unlearning methods handle unlearning requestsindependently from inference requests, which unfortunately introduces a newsecurity issue of inference service obsolescence and a privacy vulnerability ofundesirable exposure for machine unlearning in MLaaS. In this paper, we propose the ERASER framework for machinE unleaRning inMLaAS via an inferencE seRving-aware approach. ERASER strategically chooseappropriate unlearning execution timing to address the inference serviceobsolescence issue. A novel inference consistency certification mechanism isproposed to avoid the violation of RTBF principle caused by postponedunlearning executions, thereby mitigating the undesirable exposurevulnerability. ERASER offers three groups of design choices to allow fortailor-made variants that best suit the specific environments and preferencesof various MLaaS systems. Extensive empirical evaluations across varioussettings confirm ERASER\u0026rsquo;s effectiveness, e.g., it can effectively save up to99% of inference latency and 31% of computation overhead over theinference-oblivion baseline.\rUnlearnable Algorithms for In-context Learning\nAndrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot\nabstract\rabstract: Machine unlearning is a desirable operation as models get increasinglydeployed on data with unknown provenance. However, achieving exact unlearning\u0026ndash; obtaining a model that matches the model distribution when the data to beforgotten was never used \u0026ndash; is challenging or inefficient, often requiringsignificant retraining. In this paper, we focus on efficient unlearning methodsfor the task adaptation phase of a pretrained large language model (LLM). Weobserve that an LLM\u0026rsquo;s ability to do in-context learning for task adaptationallows for efficient exact unlearning of task adaptation training data. Weprovide an algorithm for selecting few-shot training examples to prepend to theprompt given to an LLM (for task adaptation), ERASE, whose unlearning operationcost is independent of model and dataset size, meaning it scales to largemodels and datasets. We additionally compare our approach to fine-tuningapproaches and discuss the trade-offs between the two approaches. This leads usto propose a new holistic measure of unlearning cost which accounts for varyinginference costs, and conclude that in-context learning can often be morefavourable than fine-tuning for deployments involving unlearning requests.\rExploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy\nThanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Xiaofeng Zhu, Qing Li\nabstract\rabstract: Machine unlearning (MU) is gaining increasing attention due to the need toremove or modify predictions made by machine learning (ML) models. Whiletraining models have become more efficient and accurate, the importance ofunlearning previously learned information has become increasingly significantin fields such as privacy, security, and fairness. This paper presents acomprehensive survey of MU, covering current state-of-the-art techniques andapproaches, including data deletion, perturbation, and model updates. Inaddition, commonly used metrics and datasets are also presented. The paper alsohighlights the challenges that need to be addressed, including attacksophistication, standardization, transferability, interpretability, trainingdata, and resource constraints. The contributions of this paper includediscussions about the potential benefits of MU and its future directions.Additionally, the paper emphasizes the need for researchers and practitionersto continue exploring and refining unlearning techniques to ensure that MLmodels can adapt to changing circumstances while maintaining user trust. Theimportance of unlearning is further highlighted in making ArtificialIntelligence (AI) more trustworthy and transparent, especially with theincreasing importance of AI in various domains that involve large amounts ofpersonal user data.\r2024-01-31\nDataset Condensation Driven Machine Unlearning\nJunaid Iqbal Khan\nabstract\rabstract: The current trend in data regulation requirements and privacy-preservingmachine learning has emphasized the importance of machine unlearning. The naiveapproach to unlearning training data by retraining over the complement of theforget samples is susceptible to computational challenges. These challengeshave been effectively addressed through a collection of techniques fallingunder the umbrella of machine unlearning. However, there still exists a lack ofsufficiency in handling persistent computational challenges in harmony with theutility and privacy of unlearned model. We attribute this to the lack of workon improving the computational complexity of approximate unlearning from theperspective of the training dataset. In this paper, we aim to fill this gap byintroducing dataset condensation as an essential component of machineunlearning in the context of image classification. To achieve this goal, wepropose new dataset condensation techniques and an innovative unlearning schemethat strikes a balance between machine unlearning privacy, utility, andefficiency. Furthermore, we present a novel and effective approach toinstrumenting machine unlearning and propose its application in defendingagainst membership inference and model inversion attacks. Additionally, weexplore a new application of our approach, which involves removing data from`condensed model\u0026rsquo;, which can be employed to quickly train any arbitrary modelwithout being influenced by unlearning samples.\rLearning to Predict Gradients for Semi-Supervised Continual Learning\nYan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao\nabstract\rabstract: A key challenge for machine intelligence is to learn new visual conceptswithout forgetting the previously acquired knowledge. Continual learning isaimed towards addressing this challenge. However, there is a gap betweenexisting supervised continual learning and human-like intelligence, where humanis able to learn from both labeled and unlabeled data. How unlabeled dataaffects learning and catastrophic forgetting in the continual learning processremains unknown. To explore these issues, we formulate a new semi-supervisedcontinual learning method, which can be generically applied to existingcontinual learning models. Specifically, a novel gradient learner learns fromlabeled data to predict gradients on unlabeled data. Hence, the unlabeled datacould fit into the supervised continual learning method. Different fromconventional semi-supervised settings, we do not hypothesize that theunderlying classes, which are associated to the unlabeled data, are known tothe learning process. In other words, the unlabeled data could be very distinctfrom the labeled data. We evaluate the proposed method on mainstream continuallearning, adversarial continual learning, and semi-supervised learning tasks.The proposed method achieves state-of-the-art performance on classificationaccuracy and backward transfer in the continual learning setting whileachieving desired performance on classification accuracy in the semi-supervisedlearning setting. This implies that the unlabeled images can enhance thegeneralizability of continual learning models on the predictive ability onunseen data and significantly alleviate catastrophic forgetting. The code isavailable at \\url{https://github.com/luoyan407/grad_prediction.git}.\r2024-01-30\nCaMU: Disentangling Causal Effects in Deep Model Unlearning\nShaofei Shen, Chenhao Zhang, Alina Bialkowski, Weitong Chen, Miao Xu\nabstract\rabstract: Machine unlearning requires removing the information of forgetting data whilekeeping the necessary information of remaining data. Despite recentadvancements in this area, existing methodologies mainly focus on the effect ofremoving forgetting data without considering the negative impact this can haveon the information of the remaining data, resulting in significant performancedegradation after data removal. Although some methods try to repair theperformance of remaining data after removal, the forgotten information can alsoreturn after repair. Such an issue is due to the intricate intertwining of theforgetting and remaining data. Without adequately differentiating the influenceof these two kinds of data on the model, existing algorithms take the risk ofeither inadequate removal of the forgetting data or unnecessary loss ofvaluable information from the remaining data. To address this shortcoming, thepresent study undertakes a causal analysis of the unlearning and introduces anovel framework termed Causal Machine Unlearning (CaMU). This framework addsintervention on the information of remaining data to disentangle the causaleffects between forgetting data and remaining data. Then CaMU eliminates thecausal impact associated with forgetting data while concurrently preserving thecausal relevance of the remaining data. Comprehensive empirical results onvarious datasets and models suggest that CaMU enhances performance on theremaining data and effectively minimizes the influences of forgetting data.Notably, this work is the first to interpret deep model unlearning tasks from anew perspective of causality and provide a solution based on causal analysis,which opens up new possibilities for future research in deep model unlearning.\r2024-01-29\nA Survey on Federated Unlearning: Challenges, Methods, and Future Directions\nZiyao Liu, Yu Jiang, Jiyuan Shen, Minyi Peng, Kwok-Yan Lam, Xingliang Yuan, Xiaoning Liu\nabstract\rabstract: In recent years, the notion of \u0026ldquo;the right to be forgotten\u0026rdquo; (RTBF) has evolvedinto a fundamental element of data privacy regulations, affording individualsthe ability to request the removal of their personal data from digital records.Consequently, given the extensive adoption of data-intensive machine learning(ML) algorithms and increasing concerns for personal data privacy protection,the concept of machine unlearning (MU) has gained considerable attention. MUempowers an ML model to selectively eliminate sensitive or personallyidentifiable information it acquired during the training process. Evolving fromthe foundational principles of MU, federated unlearning (FU) has emerged toconfront the challenge of data erasure within the domain of federated learning(FL) settings. This empowers the FL model to unlearn an FL client oridentifiable information pertaining to the client while preserving theintegrity of the decentralized learning process. Nevertheless, unliketraditional MU, the distinctive attributes of federated learning introducespecific challenges for FU techniques. These challenges lead to the need fortailored design when designing FU algorithms. Therefore, this comprehensivesurvey delves into the techniques, methodologies, and recent advancements infederated unlearning. It provides an overview of fundamental concepts andprinciples, evaluates existing federated unlearning algorithms, reviewsoptimizations tailored to federated learning, engages in discussions regardingpractical applications, along with an assessment of their limitations, andoutlines promising directions for future research.\rBlockchain-enabled Trustworthy Federated Unlearning\nYijing Lin, Zhipeng Gao, Hongyang Du, Jinke Ren, Zhiqiang Xie, Dusit Niyato\nabstract\rabstract: Federated unlearning is a promising paradigm for protecting the dataownership of distributed clients. It allows central servers to removehistorical data effects within the machine learning model as well as addressthe \u0026ldquo;right to be forgotten\u0026rdquo; issue in federated learning. However, existingworks require central servers to retain the historical model parameters fromdistributed clients, such that allows the central server to utilize theseparameters for further training even, after the clients exit the trainingprocess. To address this issue, this paper proposes a new blockchain-enabledtrustworthy federated unlearning framework. We first design a proof offederated unlearning protocol, which utilizes the Chameleon hash function toverify data removal and eliminate the data contributions stored in otherclients\u0026rsquo; models. Then, an adaptive contribution-based retraining mechanism isdeveloped to reduce the computational overhead and significantly improve thetraining efficiency. Extensive experiments demonstrate that the proposedframework can achieve a better data removal effect than the state-of-the-artframeworks, marking a significant stride towards trustworthy federatedunlearning.\r2024-01-27\nModel Sparsity Can Simplify Machine Unlearning\nJinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu\nabstract\rabstract: In response to recent data regulation requirements, machine unlearning (MU)has emerged as a critical process to remove the influence of specific examplesfrom a given model. Although exact unlearning can be achieved through completemodel retraining using the remaining dataset, the associated computationalcosts have driven the development of efficient, approximate unlearningtechniques. Moving beyond data-centric MU approaches, our study introduces anovel model-based perspective: model sparsification via weight pruning, whichis capable of reducing the gap between exact unlearning and approximateunlearning. We show in both theory and practice that model sparsity can boostthe multi-criteria unlearning performance of an approximate unlearner, closingthe approximation gap, while continuing to be efficient. This leads to a new MUparadigm, termed prune first, then unlearn, which infuses a sparse model priorinto the unlearning process. Building on this insight, we also develop asparsity-aware unlearning method that utilizes sparsity regularization toenhance the training process of approximate unlearning. Extensive experimentsshow that our proposals consistently benefit MU in various unlearningscenarios. A notable highlight is the 77% unlearning efficacy gain offine-tuning (one of the simplest unlearning methods) when using sparsity-awareunlearning. Furthermore, we demonstrate the practical impact of our proposed MUmethods in addressing other machine learning challenges, such as defendingagainst backdoor attacks and enhancing transfer learning. Codes are availableat https://github.com/OPTML-Group/Unlearn-Sparse.\r2024-01-23\nSimultaneous exercise recognition and evaluation in prescribed routines: Approach to virtual coaches\nSara García-de-Villa, David Casillas-Pérez, Ana Jiménez-Martín, Juan Jesús García-Domínguez\nabstract\rabstract: Home-based physical therapies are effective if the prescribed exercises arecorrectly executed and patients adhere to these routines. This is speciallyimportant for older adults who can easily forget the guidelines fromtherapists. Inertial Measurement Units (IMUs) are commonly used for trackingexercise execution giving information of patients\u0026rsquo; motion data. In this work,we propose the use of Machine Learning techniques to recognize which exerciseis being carried out and to assess if the recognized exercise is properlyexecuted by using data from four IMUs placed on the person limbs. To the bestof our knowledge, both tasks have never been addressed together as a uniquecomplex task before. However, their combination is needed for the completecharacterization of the performance of physical therapies. We evaluate theperformance of six machine learning classifiers in three contexts: recognitionand evaluation in a single classifier, recognition of correct exercises,excluding the wrongly performed exercises, and a two-stage approach that firstrecognizes the exercise and then evaluates it. We apply our proposal to a setof 8 exercises of the upper-and lower-limbs designed for maintaining elderlypeople health status. To do so, the motion of volunteers were monitored with 4IMUs. We obtain accuracies of 88.4 % and the 91.4 % in the two initialscenarios. In the third one, the recognition provides an accuracy of 96.2 %,whereas the exercise evaluation varies between 93.6 % and 100.0 %. This workproves the feasibility of IMUs for a complete monitoring of physical therapiesin which we can get information of which exercise is being performed and itsquality, as a basis for designing virtual coaches.\r2024-01-22\nTowards Effective and General Graph Unlearning via Mutual Evolution\nXunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang\nabstract\rabstract: With the rapid advancement of AI applications, the growing needs for dataprivacy and model robustness have highlighted the importance of machineunlearning, especially in thriving graph-based scenarios. However, mostexisting graph unlearning strategies primarily rely on well-designedarchitectures or manual process, rendering them less user-friendly and posingchallenges in terms of deployment efficiency. Furthermore, striking a balancebetween unlearning performance and framework generalization is also a pivotalconcern. To address the above issues, we propose \\underline{\\textbf{M}}utual\\underline{\\textbf{E}}volution \\underline{\\textbf{G}}raph\\underline{\\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm thatsimultaneously evolves the predictive and unlearning capacities of graphunlearning. By incorporating aforementioned two components, MEGU ensurescomplementary optimization in a unified training framework that aligns with theprediction and unlearning requirements. Extensive experiments on 9 graphbenchmark datasets demonstrate the superior performance of MEGU in addressingunlearning requirements at the feature, node, and edge levels. Specifically,MEGU achieves average performance improvements of 2.7%, 2.5%, and 3.2%across these three levels of unlearning tasks when compared to state-of-the-artbaselines. Furthermore, MEGU exhibits satisfactory training efficiency,reducing time and space overhead by an average of 159.8x and 9.6x,respectively, in comparison to retraining GNN from scratch.\r2024-01-19\nContrastive Unlearning: A Contrastive Approach to Machine Unlearning\nHong kyu Lee, Qiuchen Zhang, Carl Yang, Jian Lou, Li Xiong\nabstract\rabstract: Machine unlearning aims to eliminate the influence of a subset of trainingsamples (i.e., unlearning samples) from a trained model. Effectively andefficiently removing the unlearning samples without negatively impacting theoverall model performance is still challenging. In this paper, we propose acontrastive unlearning framework, leveraging the concept of representationlearning for more effective unlearning. It removes the influence of unlearningsamples by contrasting their embeddings against the remaining samples so thatthey are pushed away from their original classes and pulled toward otherclasses. By directly optimizing the representation space, it effectivelyremoves the influence of unlearning samples while maintaining therepresentations learned from the remaining samples. Experiments on a variety ofdatasets and models on both class unlearning and sample unlearning showed thatcontrastive unlearning achieves the best unlearning effects and efficiency withthe lowest performance loss compared with the state-of-the-art algorithms.\r2024-01-18\nCompositional Program Generation for Few-Shot Systematic Generalization\nTim Klinger, Luke Liu, Soham Dan, Maxwell Crouse, Parikshit Ram, Alexander Gray\nabstract\rabstract: Compositional generalization is a key ability of humans that enables us tolearn new concepts from only a handful examples. Neural machine learningmodels, including the now ubiquitous Transformers, struggle to generalize inthis way, and typically require thousands of examples of a concept duringtraining in order to generalize meaningfully. This difference in abilitybetween humans and artificial neural architectures, motivates this study on aneuro-symbolic architecture called the Compositional Program Generator (CPG).CPG has three key features: \\textit{modularity}, \\textit{composition}, and\\textit{abstraction}, in the form of grammar rules, that enable it togeneralize both systematically to new concepts in a few-shot manner, as well asproductively by length on various sequence-to-sequence language tasks. For eachinput, CPG uses a grammar of the input language and a parser to generate aparse in which each grammar rule is assigned its own unique semantic module, aprobabilistic copy or substitution program. Instances with the same parse arealways processed with the same composed modules, while those with differentparses may be processed with different modules. CPG learns parameters for themodules and is able to learn the semantics for new rules and typesincrementally, without forgetting or retraining on rules it\u0026rsquo;s already seen. Itachieves perfect generalization on both the SCAN and COGS benchmarks using just14 examples for SCAN and 22 examples for COGS \u0026ndash; state-of-the-art accuracy witha 1000x improvement in sample efficiency.\r2024-01-17\nAttack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization\nYoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier\nabstract\rabstract: With growing concerns surrounding privacy and regulatory compliance, theconcept of machine unlearning has gained prominence, aiming to selectivelyforget or erase specific learned information from a trained model. In responseto this critical need, we introduce a novel approach called Attack-and-Resetfor Unlearning (ARU). This algorithm leverages meticulously crafted adversarialnoise to generate a parameter mask, effectively resetting certain parametersand rendering them unlearnable. ARU outperforms current state-of-the-artresults on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC.In particular, we present the steps involved in attacking and masking thatstrategically filter and re-initialize network parameters biased towards theforget set. Our work represents a significant advancement in rendering dataunexploitable to deep learning models through parameter re-initialization,achieved by harnessing adversarial noise to craft a mask.\rMachine Unlearning for Recommendation Systems: An Insight\nBhavika Sachdeva, Harshita Rathee, Sristi, Arun Sharma, Witold Wydmański\nabstract\rabstract: This review explores machine unlearning (MUL) in recommendation systems,addressing adaptability, personalization, privacy, and bias challenges. Unliketraditional models, MUL dynamically adjusts system knowledge based on shifts inuser preferences and ethical considerations. The paper critically examinesMUL\u0026rsquo;s basics, real-world applications, and challenges like algorithmictransparency. It sifts through literature, offering insights into how MUL couldtransform recommendations, discussing user trust, and suggesting paths forfuture research in responsible and user-focused artificial intelligence (AI).The document guides researchers through challenges involving the trade-offbetween personalization and privacy, encouraging contributions to meetpractical demands for targeted data removal. Emphasizing MUL\u0026rsquo;s role in secureand adaptive machine learning, the paper proposes ways to push its boundaries.The novelty of this paper lies in its exploration of the limitations of themethods, which highlights exciting prospects for advancing the field.\r2024-01-16\nContinual learning under domain transfer with sparse synaptic bursting\nShawn L. Beaulieu, Jeff Clune, Nick Cheney\nabstract\rabstract: Existing machines are functionally specific tools that were made for easyprediction and control. Tomorrow\u0026rsquo;s machines may be closer to biological systemsin their mutability, resilience, and autonomy. But first they must be capableof learning and retaining new information without being exposed to itarbitrarily often. Past efforts to engineer such systems have sought to buildor regulate artificial neural networks using disjoint sets of weights that areuniquely sensitive to specific tasks or inputs. This has not yet enabledcontinual learning over long sequences of previously unseen data withoutcorrupting existing knowledge: a problem known as catastrophic forgetting. Inthis paper, we introduce a system that can learn sequentially over previouslyunseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This isdone by controlling the activity of weights in a convolutional neural networkon the basis of inputs using top-down regulation generated by a secondfeed-forward neural network. We find that our method learns continually underdomain transfer with sparse bursts of activity in weights that are recycledacross tasks, rather than by maintaining task-specific modules. Sparse synapticbursting is found to balance activity and suppression such that new functionscan be learned without corrupting extant knowledge, thus mirroring the balanceof order and disorder in systems at the edge of chaos. This behavior emergesduring a prior pre-training (or \u0026lsquo;meta-learning\u0026rsquo;) phase in which regulatedsynapses are selectively disinhibited, or grown, from an initial state ofuniform suppression through prediction error minimization.\r2024-01-15\nA Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services\nHongsheng Hu, Shuo Wang, Jiamin Chang, Haonan Zhong, Ruoxi Sun, Shuang Hao, Haojin Zhu, Minhui Xue\nabstract\rabstract: The right to be forgotten requires the removal or \u0026ldquo;unlearning\u0026rdquo; of a user\u0026rsquo;sdata from machine learning models. However, in the context of Machine Learningas a Service (MLaaS), retraining a model from scratch to fulfill the unlearningrequest is impractical due to the lack of training data on the serviceprovider\u0026rsquo;s side (the server). Furthermore, approximate unlearning furtherembraces a complex trade-off between utility (model performance) and privacy(unlearning performance). In this paper, we try to explore the potentialthreats posed by unlearning services in MLaaS, specifically over-unlearning,where more information is unlearned than expected. We propose two strategiesthat leverage over-unlearning to measure the impact on the trade-off balancing,under black-box access settings, in which the existing machine unlearningattacks are not applicable. The effectiveness of these strategies is evaluatedthrough extensive experiments on benchmark datasets, across various modelarchitectures and representative unlearning approaches. Results indicatesignificant potential for both strategies to undermine model efficacy inunlearning scenarios. This study uncovers an underexplored gap betweenunlearning and contemporary MLaaS, highlighting the need for carefulconsiderations in balancing data unlearning, model utility, and security.\r2024-01-11\nNavigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI\nDawen Zhang, Boming Xia, Yue Liu, Xiwei Xu, Thong Hoang, Zhenchang Xing, Mark Staples, Qinghua Lu, Liming Zhu\nabstract\rabstract: The advent of Generative AI has marked a significant milestone in artificialintelligence, demonstrating remarkable capabilities in generating realisticimages, texts, and data patterns. However, these advancements come withheightened concerns over data privacy and copyright infringement, primarily dueto the reliance on vast datasets for model training. Traditional approacheslike differential privacy, machine unlearning, and data poisoning only offerfragmented solutions to these complex issues. Our paper delves into themultifaceted challenges of privacy and copyright protection within the datalifecycle. We advocate for integrated approaches that combines technicalinnovation with ethical foresight, holistically addressing these concerns byinvestigating and devising solutions that are informed by the lifecycleperspective. This work aims to catalyze a broader discussion and inspireconcerted efforts towards data privacy and copyright integrity in GenerativeAI.\r2024-01-10\nTo Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods\nDawen Zhang, Shidong Pan, Thong Hoang, Zhenchang Xing, Mark Staples, Xiwei Xu, Lina Yao, Qinghua Lu, Liming Zhu\nabstract\rabstract: The right to be forgotten (RTBF) is motivated by the desire of people not tobe perpetually disadvantaged by their past deeds. For this, data deletion needsto be deep and permanent, and should be removed from machine learning models.Researchers have proposed machine unlearning algorithms which aim to erasespecific data from trained models more efficiently. However, these methodsmodify how data is fed into the model and how training is done, which maysubsequently compromise AI ethics from the fairness perspective. To helpsoftware engineers make responsible decisions when adopting these unlearningmethods, we present the first study on machine unlearning methods to revealtheir fairness implications. We designed and conducted experiments on twotypical machine unlearning methods (SISA and AmnesiacML) along with aretraining method (ORTR) as baseline using three fairness datasets under threedifferent deletion strategies. Experimental results show that under non-uniformdata deletion, SISA leads to better fairness compared with ORTR and AmnesiacML,while initial training and uniform data deletion do not necessarily affect thefairness of all three methods. These findings have exposed an importantresearch problem in software engineering, and can help practitioners betterunderstand the potential trade-offs on fairness when considering solutions forRTBF.\r2024-01-04\neCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning\nZhiwei Zuo, Zhuo Tang, Bin Wang, Kenli Li, Anwitaman Datta\nabstract\rabstract: New categories may be introduced over time, or existing categories may needto be reclassified. Class incremental learning (CIL) is employed for thegradual acquisition of knowledge about new categories while preservinginformation about previously learned ones in such dynamic environments. Itmight also be necessary to also eliminate the influence of related categorieson the model to adapt to reclassification. We thus introduce class-levelmachine unlearning (MU) within CIL. Typically, MU methods tend to betime-consuming and can potentially harm the model\u0026rsquo;s performance. A continuousstream of unlearning requests could lead to catastrophic forgetting. To addressthese issues, we propose a non-destructive eCIL-MU framework based on embeddingtechniques to map data into vectors and then be stored in vector databases. Ourapproach exploits the overlap between CIL and MU tasks for acceleration.Experiments demonstrate the capability of achieving unlearning effectivenessand orders of magnitude (upto $\\sim 278\\times$) of acceleration.\r2023-12-28\nLayer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation\nHyunjune Kim, Sangyong Lee, Simon S. Woo\nabstract\rabstract: Recently, serious concerns have been raised about the privacy issues relatedto training datasets in machine learning algorithms when including personaldata. Various regulations in different countries, including the GDPR grantindividuals to have personal data erased, known as \u0026rsquo;the right to be forgotten\u0026rsquo;or \u0026rsquo;the right to erasure\u0026rsquo;. However, there has been less research on effectivelyand practically deleting the requested personal data from the training setwhile not jeopardizing the overall machine learning performance. In this work,we propose a fast and novel machine unlearning paradigm at the layer levelcalled layer attack unlearning, which is highly accurate and fast compared toexisting machine unlearning algorithms. We introduce the Partial-PGD algorithmto locate the samples to forget efficiently. In addition, we only use the lastlayer of the model inspired by the Forward-Forward algorithm for unlearningprocess. Lastly, we use Knowledge Distillation (KD) to reliably learn thedecision boundaries from the teacher using soft label information to improveaccuracy performance. We conducted extensive experiments with SOTA machineunlearning models and demonstrated the effectiveness of our approach foraccuracy and end-to-end unlearning performance.\r2023-12-24\nTowards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems\nDasol Choi, Dongbin Na\nabstract\rabstract: Machine unlearning is a crucial tool for enabling a classification model toforget specific data that are used in the training time. Recently, variousstudies have presented machine unlearning algorithms and evaluated theirmethods on several datasets. However, most of the current machine unlearningalgorithms have been evaluated solely on traditional computer vision datasetssuch as CIFAR-10, MNIST, and SVHN. Furthermore, previous studies generallyevaluate the unlearning methods in the class-unlearning setup. Most previouswork first trains the classification models and then evaluates the machineunlearning performance of machine unlearning algorithms by forgetting selectedimage classes (categories) in the experiments. Unfortunately, theseclass-unlearning settings might not generalize to real-world scenarios. In thiswork, we propose a machine unlearning setting that aims to unlearn specificinstance that contains personal privacy (identity) while maintaining theoriginal task of a given model. Specifically, we propose two machine unlearningbenchmark datasets, MUFAC and MUCAC, that are greatly useful to evaluate theperformance and robustness of a machine unlearning algorithm. In our benchmarkdatasets, the original model performs facial feature recognition tasks: faceage estimation (multi-class classification) and facial attribute classification(binary class classification), where a class does not depend on any singletarget subject (personal identity), which can be a realistic setting. Moreover,we also report the performance of the state-of-the-art machine unlearningmethods on our proposed benchmark datasets. All the datasets, source codes, andtrained models are publicly available athttps://github.com/ndb796/MachineUnlearning.\r2023-12-22\nFAST: Feature Aware Similarity Thresholding for Weak Unlearning in Black-Box Generative Models\nSubhodip Panda, Prathosh AP\nabstract\rabstract: The heightened emphasis on the regulation of deep generative models,propelled by escalating concerns pertaining to privacy and compliance withregulatory frameworks, underscores the imperative need for precise controlmechanisms over these models. This urgency is particularly underscored byinstances in which generative models generate outputs that encompassobjectionable, offensive, or potentially injurious content. In response,machine unlearning has emerged to selectively forget specific knowledge orremove the influence of undesirable data subsets from pre-trained models.However, modern machine unlearning approaches typically assume access to modelparameters and architectural details during unlearning, which is not alwaysfeasible. In multitude of downstream tasks, these models function as black-boxsystems, with inaccessible pre-trained parameters, architectures, and trainingdata. In such scenarios, the possibility of filtering undesired outputs becomesa practical alternative. The primary goal of this study is twofold: first, toelucidate the relationship between filtering and unlearning processes, andsecond, to formulate a methodology aimed at mitigating the display ofundesirable outputs generated from models characterized as black-box systems.Theoretical analysis in this study demonstrates that, in the context ofblack-box models, filtering can be seen as a form of weak unlearning. Ourproposed \\textbf{\\textit{Feature Aware Similarity Thresholding(FAST)}} methodeffectively suppresses undesired outputs by systematically encoding therepresentation of unwanted features in the latent space.\rFast-NTK: Parameter-Efficient Unlearning for Large-Scale Models\nGuihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu\nabstract\rabstract: The rapid growth of machine learning has spurred legislative initiatives suchas the Right to be Forgotten,'' allowing users to request data removal. Inresponse, machine unlearning\u0026rsquo;\u0026rsquo; proposes the selective removal of unwanteddata without the need for retraining from scratch. While theNeural-Tangent-Kernel-based (NTK-based) unlearning method excels inperformance, it suffers from significant computational complexity, especiallyfor large-scale models and datasets. Our work introduces ``Fast-NTK,\u0026rsquo;\u0026rsquo; a novelNTK-based unlearning algorithm that significantly reduces the computationalcomplexity by incorporating parameter-efficient fine-tuning methods, such asfine-tuning batch normalization layers in a CNN or visual prompts in a visiontransformer. Our experimental results demonstrate scalability to much largerneural networks and datasets (e.g., 88M parameters; 5k images), surpassing thelimitations of previous full-model NTK-based approaches designed for smallercases (e.g., 8M parameters; 500 images). Notably, our approach maintains aperformance comparable to the traditional method of retraining on the retainset alone. Fast-NTK can thus enable for practical and scalable NTK-basedunlearning in deep neural networks.\r2023-12-18\nPredicting depinning dynamics of elastic interfaces by machine learning\nValtteri Haavisto, Marcin Mińkowski, Lasse Laurson\nabstract\rabstract: Predicting the future behaviour of complex systems exhibiting critical-likedynamics is often considered to be an intrinsically hard task. Here, we studythe predictability of the depinning dynamics of elastic interfaces in randommedia driven by a slowly increasing external force, a paradigmatic complexsystem exhibiting critical avalanche dynamics linked to a continuousnon-equilibrium depinning phase transition. To this end, we train a variety ofmachine learning models to infer the mapping from features of the initialrelaxed line shape and the random pinning landscape to predict thesample-dependent staircase-like force-displacement curve that emerges from thedepinning process. Even if for a given realization of the quenched randommedium the dynamics are in principle deterministic, we find that there is anexponential decay of the predictability with the displacement of the line,quantifying how the system forgets its initial state as it nears the depinningtransition from below. Our analysis on how the related displacement scaledepends on the system size and the dimensionality of the input descriptorreveals that the onset of the depinning phase transition gives rise tofundamental limits to predictability.\r2023-12-16\nCertified Minimax Unlearning with Generalization Rates and Deletion Capacity\nJiaqi Liu, Jian Lou, Zhan Qin, Kui Ren\nabstract\rabstract: We study the problem of $(\\epsilon,\\delta)$-certified machine unlearning forminimax models. Most of the existing works focus on unlearning from standardstatistical learning models that have a single variable and their unlearningsteps hinge on the direct Hessian-based conventional Newton update. We developa new $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimaxmodels. It proposes a minimax unlearning step consisting of atotal-Hessian-based complete Newton update and the Gaussian mechanism borrowedfrom differential privacy. To obtain the unlearning certification, our methodinjects calibrated Gaussian noises by carefully analyzing the \u0026ldquo;sensitivity\u0026rdquo; ofthe minimax unlearning step (i.e., the closeness between the minimax unlearningvariables and the retraining-from-scratch variables). We derive thegeneralization rates in terms of population strong and weak primal-dual riskfor three different cases of loss functions, i.e.,(strongly-)convex-(strongly-)concave losses. We also provide the deletioncapacity to guarantee that a desired population risk can be maintained as longas the number of deleted samples does not exceed the derived amount. Withtraining samples $n$ and model dimension $d$, it yields the order $\\mathcalO(n/d^{1/4})$, which shows a strict gap over the baseline method ofdifferentially private minimax learning that has $\\mathcal O(n/d^{1/2})$. Inaddition, our rates of generalization and deletion capacity match thestate-of-the-art rates derived previously for standard statistical learningmodels.\rFew-shot Class-incremental Learning: A Survey\nJinghua Zhang, Li Liu, Olli Silvén, Matti Pietikäinen, Dewen Hu\nabstract\rabstract: Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge inMachine Learning (ML), as it necessitates the Incremental Learning (IL) of newclasses from sparsely labeled training samples without forgetting previousknowledge. While this field has seen recent progress, it remains an activeexploration area. This paper aims to provide a comprehensive and systematicreview of FSCIL. In our in-depth examination, we delve into various facets ofFSCIL, encompassing the problem definition, the discussion of the primarychallenges of unreliable empirical risk minimization and thestability-plasticity dilemma, general schemes, and relevant problems of IL andFew-shot Learning (FSL). Besides, we offer an overview of benchmark datasetsand evaluation metrics. Furthermore, we introduce the Few-shotClass-incremental Classification (FSCIC) methods from data-based,structure-based, and optimization-based approaches and the Few-shotClass-incremental Object Detection (FSCIOD) methods from anchor-free andanchor-based approaches. Beyond these, we present several promising researchdirections within FSCIL that merit further investigation.\r2023-12-15\nWhat to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection\nXiaohui Zhang, Jiangyan Yi, Chenglong Wang, Chuyuan Zhang, Siding Zeng, Jianhua Tao\nabstract\rabstract: The rapid evolution of speech synthesis and voice conversion has raisedsubstantial concerns due to the potential misuse of such technology, promptinga pressing need for effective audio deepfake detection mechanisms. Existingdetection models have shown remarkable success in discriminating known deepfakeaudio, but struggle when encountering new attack types. To address thischallenge, one of the emergent effective approaches is continual learning. Inthis paper, we propose a continual learning approach called Radian WeightModification (RWM) for audio deepfake detection. The fundamental conceptunderlying RWM involves categorizing all classes into two groups: those withcompact feature distributions across tasks, such as genuine audio, and thosewith more spread-out distributions, like various types of fake audio. Thesedistinctions are quantified by means of the in-class cosine distance, whichsubsequently serves as the basis for RWM to introduce a trainable gradientmodification direction for distinct data types. Experimental evaluationsagainst mainstream continual learning methods reveal the superiority of RWM interms of knowledge acquisition and mitigating forgetting in audio deepfakedetection. Furthermore, RWM\u0026rsquo;s applicability extends beyond audio deepfakedetection, demonstrating its potential significance in diverse machine learningdomains such as image recognition.\r2023-12-14\nUnifilar Machines and the Adjoint Structure of Bayesian Filtering\nNathaniel Virgo\nabstract\rabstract: We elucidate the mathematical structure of Bayesian filtering, and Bayesianinference more broadly, by applying recent work on category theoreticalprobability, specifically the concept of a strongly representable Markovcategory. We show that filtering, along with related concepts such as conjugatepriors, arise from an adjunction: the process of taking a hidden Markov processis right adjoint to a forgetful functor. This has an interesting consequence.In practice, filtering is usually implemented using parametrised families ofdistributions. The Kalman filter is a particularly important example, whichuses Gaussians. Rather than calculating a new posterior each time, theimplementation only needs to udpate the parameters. This structure arisesnaturally from our adjunction; the correctness of such a model is witnessed bya map from the model into the system being modelled. Conjugate priors arisefrom this construction as a special case. In showing this we define a notion of unifilar machine, which has its originsin the literature on epsilon-machines. Unifilar machines are useful as modelsof the \u0026ldquo;observable behaviour\u0026rdquo; of stochastic systems; we show additionally thatin the Kleisli category of the distribution monad there is a terminal unifilarmachine, and its elements are controlled stochastic processes, mappingsequences of the input alphabet probabilistically to sequences of the outputalphabet.\r2023-12-13\nFast Machine Unlearning Without Retraining Through Selective Synaptic Dampening\nJack Foster, Stefan Schoepf, Alexandra Brintrup\nabstract\rabstract: Machine unlearning, the ability for a machine learning model to forget, isbecoming increasingly important to comply with data privacy regulations, aswell as to remove harmful, manipulated, or outdated information. The keychallenge lies in forgetting specific information while protecting modelperformance on the remaining data. While current state-of-the-art methodsperform well, they typically require some level of retraining over the retaineddata, in order to protect or restore model performance. This adds computationaloverhead and mandates that the training data remain available and accessible,which may not be feasible. In contrast, other methods employ a retrain-freeparadigm, however, these approaches are prohibitively computationally expensiveand do not perform on par with their retrain-based counterparts. We presentSelective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-freeapproach to machine unlearning which is fast, performant, and does not requirelong-term storage of the training data. First, SSD uses the Fisher informationmatrix of the training and forgetting data to select parameters that aredisproportionately important to the forget set. Second, SSD induces forgettingby dampening these parameters proportional to their relative importance to theforget set with respect to the wider training data. We evaluate our methodagainst several existing unlearning methods in a range of experiments usingResNet18 and Vision Transformer. Results show that the performance of SSD iscompetitive with retrain-based post hoc methods, demonstrating the viability ofretrain-free post hoc unlearning approaches.\rExploiting Machine Unlearning for Backdoor Attacks in Deep Learning System\nPeixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang\nabstract\rabstract: In recent years, the security issues of artificial intelligence have becomeincreasingly prominent due to the rapid development of deep learning researchand applications. Backdoor attack is an attack targeting the vulnerability ofdeep learning models, where hidden backdoors are activated by triggers embeddedby the attacker, thereby outputting malicious predictions that may not alignwith the intended output for a given input. In this work, we propose a novelblack-box backdoor attack based on machine unlearning. The attacker firstaugments the training set with carefully designed samples, including poison andmitigation data, to train a `benign\u0026rsquo; model. Then, the attacker posts unlearningrequests for the mitigation samples to remove the impact of relevant data onthe model, gradually activating the hidden backdoor. Since backdoors areimplanted during the iterative unlearning process, it significantly increasesthe computational overhead of existing defense methods for backdoor detectionor mitigation. To address this new security threat, we proposes two methods fordetecting or mitigating such malicious unlearning requests. We conduct theexperiment in both exact unlearning and approximate unlearning (i.e., SISA)settings. Experimental results indicate that: 1) our attack approach cansuccessfully implant backdoor into the model, and sharding increases thedifficult of attack; 2) our detection algorithms are effective in identifyingthe mitigation samples, while sharding reduces the effectiveness of ourdetection algorithms.\rGraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks\nBang Wu, He Zhang, Xiangwen Yang, Shuo Wang, Minhui Xue, Shirui Pan, Xingliang Yuan\nabstract\rabstract: The emergence of Graph Neural Networks (GNNs) in graph data analysis andtheir deployment on Machine Learning as a Service platforms have raisedcritical concerns about data misuse during model training. This situation isfurther exacerbated due to the lack of transparency in local trainingprocesses, potentially leading to the unauthorized accumulation of largevolumes of graph data, thereby infringing on the intellectual property rightsof data owners. Existing methodologies often address either data misusedetection or mitigation, and are primarily designed for local GNN models ratherthan cloud-based MLaaS platforms. These limitations call for an effective andcomprehensive solution that detects and mitigates data misuse without requiringexact training data while respecting the proprietary nature of such data. Thispaper introduces a pioneering approach called GraphGuard, to tackle thesechallenges. We propose a training-data-free method that not only detects graphdata misuse but also mitigates its impact via targeted unlearning, all withoutrelying on the original training data. Our innovative misuse detectiontechnique employs membership inference with radioactive data, enhancing thedistinguishability between member and non-member data distributions. Formitigation, we utilize synthetic graphs that emulate the characteristicspreviously learned by the target model, enabling effective unlearning even inthe absence of exact graph data. We conduct comprehensive experiments utilizingfour real-world graph datasets to demonstrate the efficacy of GraphGuard inboth detection and unlearning. We show that GraphGuard attains a near-perfectdetection rate of approximately 100% across these datasets with various GNNmodels. In addition, it performs unlearning by eliminating the impact of theunlearned graph with a marginal decrease in accuracy (less than 5%).\rPhysics-Guided Continual Learning for Accelerating Aqueous Organic Redox Flow Battery Material Discovery\nYucheng Fu, Amanda Howard, Chao Zeng, Panos Stinis\nabstract\rabstract: Aqueous organic redox flow batteries (AORFBs) have gained popularity inrenewable energy storage due to high energy density, low cost, and scalability.The rapid discovery of aqueous soluble organic (ASO) redox-active materialsnecessitates efficient machine learning surrogates for predicting batteryperformance. The physics-guided continual learning (PGCL) method proposed inthis study can incrementally learn data from new ASO electrolytes whileaddressing catastrophic forgetting issues in conventional machine learning.Using a ASO anolyte database with 1024 materials generated by a large-scale$780 cm^2$ interdigitated cell model, PGCL incorporates AORFB physics tooptimize the continual learning task formation and training process. Thisachieves a 5x training time speedup compared to the non-physics-guidedcontinual learning method while retaining previously learned battery materialknowledge. The trained PGCL demonstrates its capability in predicting batteryperformance when using unseen dihydroxyphenazine isomers in anolytes, thusshowcasing the potential of PGCL to analyze and discover new ASO materials.\r2023-12-08\nKnowledge Unlearning for LLMs: Tasks, Methods, and Challenges\nNianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang\nabstract\rabstract: In recent years, large language models (LLMs) have spurred a new researchparadigm in natural language processing. Despite their excellent capability inknowledge-based question answering and reasoning, their potential to retainfaulty or even harmful knowledge poses risks of malicious application. Thechallenge of mitigating this issue and transforming these models into purerassistants is crucial for their widespread applicability. Unfortunately,Retraining LLMs repeatedly to eliminate undesirable knowledge is impracticaldue to their immense parameters. Knowledge unlearning, derived from analogousstudies on machine unlearning, presents a promising avenue to address thisconcern and is notably advantageous in the context of LLMs. It allows for theremoval of harmful knowledge in an efficient manner, without affectingunrelated knowledge in the model. To this end, we provide a survey of knowledgeunlearning in the era of LLMs. Firstly, we formally define the knowledgeunlearning problem and distinguish it from related works. Subsequently, wecategorize existing knowledge unlearning methods into three classes: thosebased on parameter optimization, parameter merging, and in-context learning,and introduce details of these unlearning methods. We further presentevaluation datasets used in existing methods, and finally conclude this surveyby presenting the ongoing challenges and future directions.\rFew-Shot Class-Incremental Learning via Training-Free Prototype Calibration\nQi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye\nabstract\rabstract: Real-world scenarios are usually accompanied by continuously appearingclasses with scare labeled samples, which require the machine learning model toincrementally learn new classes and maintain the knowledge of base classes. Inthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methodseither introduce extra learnable components or rely on a frozen featureextractor to mitigate catastrophic forgetting and overfitting problems.However, we find a tendency for existing methods to misclassify the samples ofnew classes into base classes, which leads to the poor performance of newclasses. In other words, the strong discriminability of base classes distractsthe classification of new classes. To figure out this intriguing phenomenon, weobserve that although the feature extractor is only trained on base classes, itcan surprisingly represent the semantic similarity between the base and unseennew classes. Building upon these analyses, we propose a simple yet effectiveTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability ofnew classes by fusing the new prototypes (i.e., mean features of a class) withweighted base prototypes. In addition to standard benchmarks in FSCIL, TEENdemonstrates remarkable performance and consistent improvements over baselinemethods in the few-shot learning scenario. Code is available at:https://github.com/wangkiw/TEEN\r2023-12-07\nLearn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference with Gradient Projection\nTuan Hoang, Santu Rana, Sunil Gupta, Svetha Venkatesh\nabstract\rabstract: Recent data-privacy laws have sparked interest in machine unlearning, whichinvolves removing the effect of specific training samples from a learnt modelas if they were never present in the original training dataset. The challengeof machine unlearning is to discard information about the ``forget\u0026rsquo;\u0026rsquo; data inthe learnt model without altering the knowledge about the remaining dataset andto do so more efficiently than the naive retraining approach. To achieve this,we adopt a projected-gradient based learning method, named asProjected-Gradient Unlearning (PGU), in which the model takes steps in theorthogonal direction to the gradient subspaces deemed unimportant for theretaining dataset, so as to its knowledge is preserved. By utilizing StochasticGradient Descent (SGD) to update the model weights, our method can efficientlyscale to any model and dataset size. We provide empirically evidence todemonstrate that our unlearning method can produce models that behave similarto models retrained from scratch across various metrics even when the trainingdataset is no longer accessible. Our code is available athttps://github.com/hnanhtuan/projected_gradient_unlearning.\r2023-12-04\nDeep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting\nSangamesh Kodge, Gobinda Saha, Kaushik Roy\nabstract\rabstract: Machine unlearning has emerged as a prominent and challenging area ofinterest, driven in large part by the rising regulatory demands for industriesto delete user data upon request and the heightened awareness of privacy.Existing approaches either retrain models from scratch or use severalfinetuning steps for every deletion request, often constrained by computationalresource limitations and restricted access to the original training data. Inthis work, we introduce a novel class unlearning algorithm designed tostrategically eliminate an entire class or a group of classes from the learnedmodel. To that end, our algorithm first estimates the Retain Space and theForget Space, representing the feature or activation spaces for samples fromclasses to be retained and unlearned, respectively. To obtain these spaces, wepropose a novel singular value decomposition-based technique that requireslayer wise collection of network activations from a few forward passes throughthe network. We then compute the shared information between these spaces andremove it from the forget space to isolate class-discriminatory feature spacefor unlearning. Finally, we project the model weights in the orthogonaldirection of the class-discriminatory space to obtain the unlearned model. Wedemonstrate our algorithm\u0026rsquo;s efficacy on ImageNet using a Vision Transformerwith only $\\sim$1.5% drop in retain accuracy compared to the original modelwhile maintaining under 1% accuracy on the unlearned class samples. Further,our algorithm consistently performs well when subject to Membership InferenceAttacks showing 7.8% improvement on average across a variety of imageclassification datasets and network architectures, as compared to otherbaselines while being $\\sim$6x more computationally efficient.\rDUCK: Distance-based Unlearning via Centroid Kinematics\nMarco Cotogni, Jacopo Bonato, Luigi Sabetta, Francesco Pelosin, Alessandro Nicolosi\nabstract\rabstract: Machine Unlearning is rising as a new field, driven by the pressing necessityof ensuring privacy in modern artificial intelligence models. This techniqueprimarily aims to eradicate any residual influence of a specific subset of datafrom the knowledge acquired by a neural model during its training. This workintroduces a novel unlearning algorithm, denoted as Distance-based Unlearningvia Centroid Kinematics (DUCK), which employs metric learning to guide theremoval of samples matching the nearest incorrect centroid in the embeddingspace. Evaluation of the algorithm\u0026rsquo;s performance is conducted across variousbenchmark datasets in two distinct scenarios, class removal, and homogeneoussampling removal, obtaining state-of-the-art performance. We introduce a novelmetric, called Adaptive Unlearning Score (AUS), encompassing not only theefficacy of the unlearning process in forgetting target data but alsoquantifying the performance loss relative to the original model. Moreover, wepropose a novel membership inference attack to assess the algorithm\u0026rsquo;s capacityto erase previously acquired knowledge, designed to be adaptable to futuremethodologies.\rControl, Confidentiality, and the Right to be Forgotten\nAloni Cohen, Adam Smith, Marika Swanberg, Prashant Nalini Vasudevan\nabstract\rabstract: Recent digital rights frameworks give users the right to delete their datafrom systems that store and process their personal information (e.g., the\u0026quot;right to be forgotten\u0026quot; in the GDPR). How should deletion be formalized incomplex systems that interact with many users and store derivative information?We argue that prior approaches fall short. Definitions of machine unlearningCao and Yang [2015] are too narrowly scoped and do not apply to generalinteractive settings. The natural approach of deletion-as-confidentiality Garget al. [2020] is too restrictive: by requiring secrecy of deleted data, itrules out social functionalities. We propose a new formalism:deletion-as-control. It allows users\u0026rsquo; data to be freely used before deletion,while also imposing a meaningful requirement after deletion\u0026ndash;thereby givingusers more control. Deletion-as-control provides new ways of achieving deletionin diverse settings. We apply it to social functionalities, and give a newunified view of various machine unlearning definitions from the literature.This is done by way of a new adaptive generalization of history independence.Deletion-as-control also provides a new approach to the goal of machineunlearning, that is, to maintaining a model while honoring users\u0026rsquo; deletionrequests. We show that publishing a sequence of updated models that aredifferentially private under continual release satisfies deletion-as-control.The accuracy of such an algorithm does not depend on the number of deletedpoints, in contrast to the machine unlearning literature.\r2023-11-30\nNegotiated Representations to Prevent Forgetting in Machine Learning Applications\nNuri Korhan, Ceren Öner\nabstract\rabstract: Catastrophic forgetting is a significant challenge in the field of machinelearning, particularly in neural networks. When a neural network learns toperform well on a new task, it often forgets its previously acquired knowledgeor experiences. This phenomenon occurs because the network adjusts its weightsand connections to minimize the loss on the new task, which can inadvertentlyoverwrite or disrupt the representations that were crucial for the previoustasks. As a result, the the performance of the network on earlier tasksdeteriorates, limiting its ability to learn and adapt to a sequence of tasks.In this paper, we propose a novel method for preventing catastrophic forgettingin machine learning applications, specifically focusing on neural networks. Ourapproach aims to preserve the knowledge of the network across multiple taskswhile still allowing it to learn new information effectively. We demonstratethe effectiveness of our method by conducting experiments on various benchmarkdatasets, including Split MNIST, Split CIFAR10, Split Fashion MNIST, and SplitCIFAR100. These datasets are created by dividing the original datasets intoseparate, non overlapping tasks, simulating a continual learning scenario wherethe model needs to learn multiple tasks sequentially without forgetting theprevious ones. Our proposed method tackles the catastrophic forgetting problemby incorporating negotiated representations into the learning process, whichallows the model to maintain a balance between retaining past experiences andadapting to new tasks. By evaluating our method on these challenging datasets,we aim to showcase its potential for addressing catastrophic forgetting andimproving the performance of neural networks in continual learning settings.\r2023-11-28\nMachine Unlearning in Learned Databases: An Experimental Analysis\nMeghdad Kurmanji, Eleni Triantafillou, Peter Triantafillou\nabstract\rabstract: Machine learning models based on neural networks (NNs) are enjoyingever-increasing attention in the DB community. However, an important issue hasbeen largely overlooked, namely the challenge of dealing with the highlydynamic nature of DBs, where data updates are fundamental, highly-frequentoperations. Although some recent research has addressed the issues ofmaintaining updated NN models in the presence of new data insertions, theeffects of data deletions (a.k.a., \u0026ldquo;machine unlearning\u0026rdquo;) remain a blind spot.With this work, for the first time to our knowledge, we pose and answer thefollowing key questions: What is the effect of unlearning algorithms onNN-based DB models? How do these effects translate to effects on downstream DBtasks, such as selectivity estimation (SE), approximate query processing (AQP),data generation (DG), and upstream tasks like data classification (DC)? Whatmetrics should we use to assess the impact and efficacy of unlearningalgorithms in learned DBs? Is the problem of machine unlearning in DBsdifferent from that of machine learning in DBs in the face of data insertions?Is the problem of machine unlearning for DBs different from unlearning in theML literature? what are the overhead and efficiency of unlearning algorithms?What is the sensitivity of unlearning on batching delete operations? If we havea suitable unlearning algorithm, can we combine it with an algorithm handlingdata insertions en route to solving the general adaptability/updatabilityrequirement in learned DBs in the face of both data inserts and deletes? Weanswer these questions using a comprehensive set of experiments, variousunlearning algorithms, a variety of downstream DB tasks, and an upstream task(DC), each with different NNs, and using a variety of metrics on a variety ofreal datasets, making this also a first key step towards a benchmark forlearned DB unlearning.\r2023-11-27\nReducing Gender Bias in Machine Translation through Counterfactual Data Generation\nRanjita Naik, Spencer Rarrick, Vishal Chowdhary\nabstract\rabstract: Recent advances in neural methods have led to substantial improvement in thequality of Neural Machine Translation (NMT) systems. However, these systemsfrequently produce translations with inaccurate gender (Stanovsky et al.,2019), which can be traced to bias in training data. Saunders and Byrne (2020)tackle this problem with a handcrafted dataset containing balanced genderedprofession words. By using this data to fine-tune an existing NMT model, theyshow that gender bias can be significantly mitigated, albeit at the expense oftranslation quality due to catastrophic forgetting. They recover some of thelost quality with modified training objectives or additional models atinference. We find, however, that simply supplementing the handcrafted datasetwith a random sample from the base model training corpus is enough tosignificantly reduce the catastrophic forgetting. We also propose a noveldomain-adaptation technique that leverages in-domain data created with thecounterfactual data generation techniques proposed by Zmigrod et al. (2019) tofurther improve accuracy on the WinoMT challenge test set without significantloss in translation quality. We show its effectiveness in NMT systems fromEnglish into three morphologically rich languages French, Spanish, and Italian.The relevant dataset and code will be available at Github.\r2023-11-26\nUnlearning via Sparse Representations\nVedant Shah, Frederik Träuble, Ashish Malik, Hugo Larochelle, Michael Mozer, Sanjeev Arora, Yoshua Bengio, Anirudh Goyal\nabstract\rabstract: Machine \\emph{unlearning}, which involves erasing knowledge about a\\emph{forget set} from a trained model, can prove to be costly and infeasibleby existing techniques. We propose a nearly compute-free zero-shot unlearningtechnique based on a discrete representational bottleneck. We show that theproposed technique efficiently unlearns the forget set and incurs negligibledamage to the model\u0026rsquo;s performance on the rest of the data set. We evaluate theproposed technique on the problem of \\textit{class unlearning} using threedatasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposedtechnique to SCRUB, a state-of-the-art approach which uses knowledgedistillation for unlearning. Across all three datasets, the proposed techniqueperforms as well as, if not better than SCRUB while incurring almost nocomputational cost.\r2023-11-25\nEnhancing Sentiment Analysis Results through Outlier Detection Optimization\nYuetian Chen, Mei Si\nabstract\rabstract: When dealing with text data containing subjective labels like speakeremotions, inaccuracies or discrepancies among labelers are not uncommon. Suchdiscrepancies can significantly affect the performance of machine learningalgorithms. This study investigates the potential of identifying and addressingoutliers in text data with subjective labels, aiming to enhance classificationoutcomes. We utilized the Deep SVDD algorithm, a one-class classificationmethod, to detect outliers in nine text-based emotion and sentiment analysisdatasets. By employing both a small-sized language model (DistilBERT base modelwith 66 million parameters) and non-deep learning machine learning algorithms(decision tree, KNN, Logistic Regression, and LDA) as the classifier, ourfindings suggest that the removal of outliers can lead to enhanced results inmost cases. Additionally, as outliers in such datasets are not necessarilyunlearnable, we experienced utilizing a large language model \u0026ndash; DeBERTa v3large with 131 million parameters, which can capture very complex patterns indata. We continued to observe performance enhancements across multipledatasets.\r2023-11-22\nSecureCut: Federated Gradient Boosting Decision Trees with Efficient Machine Unlearning\nJian Zhang, Bowen Li Jie Li, Chentao Wu\nabstract\rabstract: In response to legislation mandating companies to honor the \\textit{right tobe forgotten} by erasing user data, it has become imperative to enable dataremoval in Vertical Federated Learning (VFL) where multiple parties provideprivate features for model training. In VFL, data removal, i.e.,\\textit{machine unlearning}, often requires removing specific features acrossall samples under privacy guarentee in federated learning. To address thischallenge, we propose \\methname, a novel Gradient Boosting Decision Tree (GBDT)framework that effectively enables both \\textit{instance unlearning} and\\textit{feature unlearning} without the need for retraining from scratch.Leveraging a robust GBDT structure, we enable effective data deletion whilereducing degradation of model performance. Extensive experimental results onpopular datasets demonstrate that our method achieves superior model utilityand forgetfulness compared to \\textit{state-of-the-art} methods. To our bestknowledge, this is the first work that investigates machine unlearning in VFLscenarios.\rDensity Distribution-based Learning Framework for Addressing Online Continual Learning Challenges\nShilin Zhang, Jiahui Wang\nabstract\rabstract: In this paper, we address the challenges of online Continual Learning (CL) byintroducing a density distribution-based learning framework. CL, especially theClass Incremental Learning, enables adaptation to new test distributions whilecontinuously learning from a single-pass training data stream, which is more inline with the practical application requirements of real-world scenarios.However, existing CL methods often suffer from catastrophic forgetting andhigher computing costs due to complex algorithm designs, limiting theirpractical use. Our proposed framework overcomes these limitations by achievingsuperior average accuracy and time-space efficiency, bridging the performancegap between CL and classical machine learning. Specifically, we adopt anindependent Generative Kernel Density Estimation (GKDE) model for each CL task.During the testing stage, the GKDEs utilize a self-reported max probabilitydensity value to determine which one is responsible for predicting incomingtest instances. A GKDE-based learning objective can ensure that samples withthe same label are grouped together, while dissimilar instances are pushedfarther apart. Extensive experiments conducted on multiple CL datasets validatethe effectiveness of our proposed framework. Our method outperforms popular CLapproaches by a significant margin, while maintaining competitive time-spaceefficiency, making our framework suitable for real-world applications. Codewill be available at https://github.com/xxxx/xxxx.\r2023-11-21\nCovarNav: Machine Unlearning via Model Inversion and Covariance Navigation\nAli Abbasi, Chayne Thrash, Elaheh Akbari, Daniel Zhang, Soheil Kolouri\nabstract\rabstract: The rapid progress of AI, combined with its unprecedented public adoption andthe propensity of large neural networks to memorize training data, has givenrise to significant data privacy concerns. To address these concerns, machineunlearning has emerged as an essential technique to selectively remove theinfluence of specific training data points on trained models. In this paper, weapproach the machine unlearning problem through the lens of continual learning.Given a trained model and a subset of training data designated to be forgotten(i.e., the \u0026ldquo;forget set\u0026rdquo;), we introduce a three-step process, named CovarNav, tofacilitate this forgetting. Firstly, we derive a proxy for the model\u0026rsquo;s trainingdata using a model inversion attack. Secondly, we mislabel the forget set byselecting the most probable class that deviates from the actual ground truth.Lastly, we deploy a gradient projection method to minimize the cross-entropyloss on the modified forget set (i.e., learn incorrect labels for this set)while preventing forgetting of the inverted samples. We rigorously evaluateCovarNav on the CIFAR-10 and Vggface2 datasets, comparing our results withrecent benchmarks in the field and demonstrating the efficacy of our proposedapproach.\rContinual Learning: Applications and the Road Forward\nEli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven\nabstract\rabstract: Continual learning is a sub-field of machine learning, which aims to allowmachine learning models to continuously learn on new data, by accumulatingknowledge without forgetting what was learned in the past. In this work, wetake a step back, and ask: \u0026ldquo;Why should one care about continual learning in thefirst place?\u0026rdquo;. We set the stage by surveying recent continual learning paperspublished at three major machine learning conferences, and show thatmemory-constrained settings dominate the field. Then, we discuss five openproblems in machine learning, and even though they seem unrelated to continuallearning at first sight, we show that continual learning will inevitably bepart of their solution. These problems are model-editing, personalization,on-device learning, faster (re-)training and reinforcement learning. Finally,by comparing the desiderata from these unsolved problems and the currentassumptions in continual learning, we highlight and discuss four futuredirections for continual learning research. We hope that this work offers aninteresting perspective on the future of continual learning, while displayingits potential value and the paths we have to pursue in order to make itsuccessful. This work is the result of the many discussions the authors had atthe Dagstuhl seminar on Deep Continual Learning, in March 2023.\r2023-11-18\nMultimodal Machine Unlearning\nJiali Cheng, Hadi Amiri\nabstract\rabstract: Machine Unlearning is the process of removing specific training data samplesand their corresponding effects from an already trained model. It hassignificant practical benefits, such as purging private, inaccurate, oroutdated information from trained models without the need for completere-training. Unlearning within a multimodal setting presents unique challengesdue to the intrinsic dependencies between different data modalities and theexpensive cost of training on large multimodal datasets and architectures.Current approaches to machine unlearning have not fully addressed thesechallenges. To bridge this gap, we introduce MMUL, a machine unlearningapproach specifically designed for multimodal data and models. MMUL formulatesthe multimodal unlearning task by focusing on three key properties: (a):modality decoupling, which effectively decouples the association betweenindividual unimodal data points within multimodal inputs marked for deletion,rendering them as unrelated data points within the model\u0026rsquo;s context, (b):unimodal knowledge retention, which retains the unimodal representationcapability of the model post-unlearning, and (c): multimodal knowledgeretention, which retains the multimodal representation capability of the modelpost-unlearning. MMUL is efficient to train and is not constrained by therequirement of using a strongly convex loss. Experiments on two multimodalmodels and four multimodal benchmark datasets, including vision-language andgraph-language datasets, show that MMUL outperforms existing baselines, gainingan average improvement of +17.6 points against the best-performing unimodalbaseline in distinguishing between deleted and remaining data. In addition,MMUL can largely maintain pre-existing knowledge of the original model postunlearning, with a performance gap of only 0.3 points compared to retraining anew model from scratch.\r2023-11-17\nDeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal\nJiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran\nabstract\rabstract: Machine learning models trained on sensitive or private data caninadvertently memorize and leak that information. Machine unlearning seeks toretroactively remove such details from model weights to protect privacy. Wecontribute a lightweight unlearning algorithm that leverages the FisherInformation Matrix (FIM) for selective forgetting. Prior work in this arearequires full retraining or large matrix inversions, which are computationallyexpensive. Our key insight is that the diagonal elements of the FIM, whichmeasure the sensitivity of log-likelihood to changes in weights, containsufficient information for effective forgetting. Specifically, we compute theFIM diagonal over two subsets \u0026ndash; the data to retain and forget \u0026ndash; for alltrainable weights. This diagonal representation approximates the complete FIMwhile dramatically reducing computation. We then use it to selectively updateweights to maximize forgetting of the sensitive subset while minimizing impacton the retained subset. Experiments show that our algorithm can successfullyforget any randomly selected subsets of training data across neural networkarchitectures. By leveraging the FIM diagonal, our approach provides aninterpretable, lightweight, and efficient solution for machine unlearning withpractical privacy benefits.\r2023-11-15\nThink-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory\nLei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang\nabstract\rabstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkableperformance in long-term human-machine interactions, which basically relies oniterative recalling and reasoning of history to generate high-qualityresponses. However, such repeated recall-reason steps easily produce biasedthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the samehistory for different questions. On the contrary, humans can keep thoughts inthe memory and recall them without repeated reasoning. Motivated by this humancapability, we propose a novel memory mechanism called TiM (Think-in-Memory)that enables LLMs to maintain an evolved memory for storing historical thoughtsalong the conversation stream. The TiM framework consists of two crucialstages: (1) before generating a response, a LLM agent recalls relevant thoughtsfrom memory, and (2) after generating a response, the LLM agent post-thinks andincorporates both historical and new thoughts to update the memory. Thus, TiMcan eliminate the issue of repeated reasoning by saving the post-thinkingthoughts as the history. Besides, we formulate the basic principles to organizethe thoughts in memory based on the well-established operations,(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamicupdates and evolution of the thoughts. Furthermore, we introduceLocality-Sensitive Hashing into TiM to achieve efficient retrieval for thelong-term conversations. We conduct qualitative and quantitative experiments onreal-world and simulated dialogues covering a wide range of topics,demonstrating that equipping existing LLMs with TiM significantly enhancestheir performance in generating responses for long-term interactions.\r2023-11-14\nExtending Multilingual Machine Translation through Imitation Learning\nWen Lai, Viktor Hangya, Alexander Fraser\nabstract\rabstract: Despite the growing variety of languages supported by existing multilingualneural machine translation (MNMT) models, most of the world\u0026rsquo;s languages arestill being left behind. We aim to extend large-scale MNMT models to a newlanguage, allowing for translation between the newly added and all of thealready supported languages in a challenging scenario: using only a parallelcorpus between the new language and English. Previous approaches, such ascontinued training on parallel data including the new language, suffer fromcatastrophic forgetting (i.e., performance on other languages is reduced). Ournovel approach Imit-MNMT treats the task as an imitation learning process,which mimicks the behavior of an expert, a technique widely used in thecomputer vision area, but not well explored in NLP. More specifically, weconstruct a pseudo multi-parallel corpus of the new and the original languagesby pivoting through English, and imitate the output distribution of theoriginal MNMT model. Extensive experiments show that our approach significantlyimproves the translation performance between the new and the originallanguages, without severe catastrophic forgetting. We also demonstrate that ourapproach is capable of solving copy and off-target problems, which are twocommon issues existence in current large-scale MNMT models.\r2023-11-13\nLeveraging Hamilton-Jacobi PDEs with time-dependent Hamiltonians for continual scientific machine learning\nPaula Chen, Tingwei Meng, Zongren Zou, Jérôme Darbon, George Em Karniadakis\nabstract\rabstract: We address two major challenges in scientific machine learning (SciML):interpretability and computational efficiency. We increase the interpretabilityof certain learning processes by establishing a new theoretical connectionbetween optimization problems arising from SciML and a generalized Hopfformula, which represents the viscosity solution to a Hamilton-Jacobi partialdifferential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we showthat when we solve certain regularized learning problems with integral-typelosses, we actually solve an optimal control problem and its associated HJ PDEwith time-dependent Hamiltonian. This connection allows us to reinterpretincremental updates to learned models as the evolution of an associated HJ PDEand optimal control problem in time, where all of the previous information isintrinsically encoded in the solution to the HJ PDE. As a result, existing HJPDE solvers and optimal control algorithms can be reused to design newefficient training approaches for SciML that naturally coincide with thecontinual learning framework, while avoiding catastrophic forgetting. As afirst exploration of this connection, we consider the special case of linearregression and leverage our connection to develop a new Riccati-basedmethodology for solving these learning problems that is amenable to continuallearning applications. We also provide some corresponding numerical examplesthat demonstrate the potential computational and memory advantages ourRiccati-based approach can provide.\r2023-11-03\nFast Model Debias with Machine Unlearning\nRuizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu\nabstract\rabstract: Recent discoveries have revealed that deep neural networks might behave in abiased manner in many real-world scenarios. For instance, deep networks trainedon a large-scale face recognition dataset CelebA tend to predict blonde hairfor females and black hair for males. Such biases not only jeopardize therobustness of models but also perpetuate and amplify social biases, which isespecially concerning for automated decision-making processes in healthcare,recruitment, etc., as they could exacerbate unfair economic and socialinequalities among different groups. Existing debiasing methods suffer fromhigh costs in bias labeling or model re-training, while also exhibiting adeficiency in terms of elucidating the origins of biases within the model. Tothis respect, we propose a fast model debiasing framework (FMD) which offers anefficient approach to identify, evaluate and remove biases inherent in trainedmodels. The FMD identifies biased attributes through an explicit counterfactualconcept and quantifies the influence of data samples with influence functions.Moreover, we design a machine unlearning-based strategy to efficiently andeffectively remove the bias in a trained model with a small counterfactualdataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasetsalong with experiments with large language models demonstrate that our methodachieves superior or competing accuracies compared with state-of-the-artmethods while attaining significantly fewer biases and requiring much lessdebiasing cost. Notably, our method requires only a small external dataset andupdating a minimal amount of model parameters, without the requirement ofaccess to training data that may be too large or unavailable in practice.\r2023-10-31\nConstructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning\nFuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu\nabstract\rabstract: Few-shot class-incremental learning (FSCIL) aims to build machine learningmodel that can continually learn new concepts from a few data samples, withoutforgetting knowledge of old classes. The challenges of FSCIL lies in the limited data of new classes, which notonly lead to significant overfitting issues but also exacerbates the notoriouscatastrophic forgetting problems. As proved in early studies, building samplerelationships is beneficial for learning from few-shot samples. In this paper,we promote the idea to the incremental scenario, and propose a Sample-to-Class(S2C) graph learning method for FSCIL. Specifically, we propose a Sample-level Graph Network (SGN) that focuses onanalyzing sample relationships within a single session. This network helpsaggregate similar samples, ultimately leading to the extraction of more refinedclass-level features. Then, we present a Class-level Graph Network (CGN) that establishesconnections across class-level features of both new and old classes. Thisnetwork plays a crucial role in linking the knowledge between differentsessions and helps improve overall learning in the FSCIL scenario. Moreover, wedesign a multi-stage strategy for training S2C model, which mitigates thetraining challenges posed by limited data in the incremental process. The multi-stage training strategy is designed to build S2C graph from base tofew-shot stages, and improve the capacity via an extra pseudo-incrementalstage. Experiments on three popular benchmark datasets show that our methodclearly outperforms the baselines and sets new state-of-the-art results inFSCIL.\r2023-10-30\nTowards Unbounded Machine Unlearning\nMeghdad Kurmanji, Peter Triantafillou, Jamie Hayes, Eleni Triantafillou\nabstract\rabstract: Deep machine unlearning is the problem of removing' from a trained neuralnetwork a subset of its training set. This problem is very timely and has manyapplications, including the key tasks of removing biases (RB), resolvingconfusion (RC) (caused by mislabelled data in trained models), as well asallowing users to exercise their right to be forgotten\u0026rsquo; to protect UserPrivacy (UP). This paper is the first, to our knowledge, to study unlearningfor different applications (RB, RC, UP), with the view that each has its owndesiderata, definitions for `forgetting\u0026rsquo; and associated metrics for forgetquality. For UP, we propose a novel adaptation of a strong Membership InferenceAttack for unlearning. We also propose SCRUB, a novel unlearning algorithm,which is the only method that is consistently a top performer for forgetquality across the different application-dependent metrics for RB, RC, and UP.At the same time, SCRUB is also consistently a top performer on metrics thatmeasure model utility (i.e. accuracy on retained data and generalization), andis more efficient than previous work. The above are substantiated through acomprehensive empirical evaluation against previous state-of-the-art.\rLearnware: Small Models Do Big\nZhi-Hua Zhou, Zhi-Hao Tan\nabstract\rabstract: There are complaints about current machine learning techniques such as therequirement of a huge amount of training data and proficient training skills,the difficulty of continual learning, the risk of catastrophic forgetting, theleaking of data privacy/proprietary, etc. Most research efforts have beenfocusing on one of those concerned issues separately, paying less attention tothe fact that most issues are entangled in practice. The prevailing big modelparadigm, which has achieved impressive results in natural language processingand computer vision applications, has not yet addressed those issues, whereasbecoming a serious source of carbon emissions. This article offers an overviewof the learnware paradigm, which attempts to enable users not need to buildmachine learning models from scratch, with the hope of reusing small models todo things even beyond their original purposes, where the key ingredient is thespecification which enables a trained model to be adequately identified toreuse according to the requirement of future users who know nothing about themodel in advance.\r2023-10-29\nA foundational neural operator that continuously learns without forgetting\nTapas Tripura, Souvik Chakraborty\nabstract\rabstract: Machine learning has witnessed substantial growth, leading to the developmentof advanced artificial intelligence models crafted to address a wide range ofreal-world challenges spanning various domains, such as computer vision,natural language processing, and scientific computing. Nevertheless, thecreation of custom models for each new task remains a resource-intensiveundertaking, demanding considerable computational time and memory resources. Inthis study, we introduce the concept of the Neural Combinatorial Wavelet NeuralOperator (NCWNO) as a foundational model for scientific computing. This modelis specifically designed to excel in learning from a diverse spectrum ofphysics and continuously adapt to the solution operators associated withparametric partial differential equations (PDEs). The NCWNO leverages a gatedstructure that employs local wavelet experts to acquire shared features acrossmultiple physical systems, complemented by a memory-based ensembling approachamong these local wavelet experts. This combination enables rapid adaptation tonew challenges. The proposed foundational model offers two key advantages: (i)it can simultaneously learn solution operators for multiple parametric PDEs,and (ii) it can swiftly generalize to new parametric PDEs with minimalfine-tuning. The proposed NCWNO is the first foundational operator learningalgorithm distinguished by its (i) robustness against catastrophic forgetting,(ii) the maintenance of positive transfer for new parametric PDEs, and (iii)the facilitation of knowledge transfer across dissimilar tasks. Through anextensive set of benchmark examples, we demonstrate that the NCWNO canoutperform task-specific baseline operator learning frameworks with minimalhyperparameter tuning at the prediction stage. We also show that with minimalfine-tuning, the NCWNO performs accurate combinatorial learning of newparametric PDEs.\r2023-10-26\nLearn to Unlearn: A Survey on Machine Unlearning\nYouyang Qu, Xin Yuan, Ming Ding, Wei Ni, Thierry Rakotoarivelo, David Smith\nabstract\rabstract: Machine Learning (ML) models have been shown to potentially leak sensitiveinformation, thus raising privacy concerns in ML-driven applications. Thisinspired recent research on removing the influence of specific data samplesfrom a trained ML model. Such efficient removal would enable ML to comply withthe \u0026ldquo;right to be forgotten\u0026rdquo; in many legislation, and could also addressperformance bottlenecks from low-quality or poisonous samples. In that context,machine unlearning methods have been proposed to erase the contributions ofdesignated data samples on models, as an alternative to the often impracticableapproach of retraining models from scratch. This article presents acomprehensive review of recent machine unlearning techniques, verificationmechanisms, and potential attacks. We further highlight emerging challenges andprospective research directions (e.g. resilience and fairness concerns). We aimfor this paper to provide valuable resources for integrating privacy, equity,andresilience into ML systems and help them \u0026ldquo;learn to unlearn\u0026rdquo;.\r2023-10-25\nOpen Knowledge Base Canonicalization with Multi-task Unlearning\nBingchen Liu, Shihao Hou, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan\nabstract\rabstract: The construction of large open knowledge bases (OKBs) is integral to manyapplications in the field of mobile computing. Noun phrases and relationalphrases in OKBs often suffer from redundancy and ambiguity, which calls for theinvestigation on OKB canonicalization. However, in order to meet therequirements of some privacy protection regulations and to ensure thetimeliness of the data, the canonicalized OKB often needs to remove somesensitive information or outdated data. The machine unlearning in OKBcanonicalization is an excellent solution to the above problem. Currentsolutions address OKB canonicalization by devising advanced clusteringalgorithms and using knowledge graph embedding (KGE) to further facilitate thecanonicalization process. Effective schemes are urgently needed to fullysynergise machine unlearning with clustering and KGE learning. To this end, weput forward a multi-task unlearning framework, namely MulCanon, to tacklemachine unlearning problem in OKB canonicalization. Specifically, the noisecharacteristics in the diffusion model are utilized to achieve the effect ofmachine unlearning for data in OKB. MulCanon unifies the learning objectives ofdiffusion model, KGE and clustering algorithms, and adopts a two-stepmulti-task learning paradigm for training. A thorough experimental study onpopular OKB canonicalization datasets validates that MulCanon achieves advancedmachine unlearning effects.\rTowards Continually Learning Application Performance Models\nRay A. O. Sinurat, Anurag Daram, Haryadi S. Gunawi, Robert B. Ross, Sandeep Madireddy\nabstract\rabstract: Machine learning-based performance models are increasingly being used tobuild critical job scheduling and application optimization decisions.Traditionally, these models assume that data distribution does not change asmore samples are collected over time. However, owing to the complexity andheterogeneity of production HPC systems, they are susceptible to hardwaredegradation, replacement, and/or software patches, which can lead to drift inthe data distribution that can adversely affect the performance models. To thisend, we develop continually learning performance models that account for thedistribution drift, alleviate catastrophic forgetting, and improvegeneralizability. Our best model was able to retain accuracy, regardless ofhaving to learn the new distribution of data inflicted by system changes, whiledemonstrating a 2x improvement in the prediction accuracy of the whole datasequence in comparison to the naive approach.\rUniversal adversarial perturbations for multiple classification tasks with quantum classifiers\nYun-Zhong Qiu\nabstract\rabstract: Quantum adversarial machine learning is an emerging field that studies thevulnerability of quantum learning systems against adversarial perturbations anddevelops possible defense strategies. Quantum universal adversarialperturbations are small perturbations, which can make different input samplesinto adversarial examples that may deceive a given quantum classifier. This isa field that was rarely looked into but worthwhile investigating becauseuniversal perturbations might simplify malicious attacks to a large extent,causing unexpected devastation to quantum machine learning models. In thispaper, we take a step forward and explore the quantum universal perturbationsin the context of heterogeneous classification tasks. In particular, we findthat quantum classifiers that achieve almost state-of-the-art accuracy on twodifferent classification tasks can be both conclusively deceived by onecarefully-crafted universal perturbation. This result is explicitlydemonstrated with well-designed quantum continual learning models with elasticweight consolidation method to avoid catastrophic forgetting, as well asreal-life heterogeneous datasets from hand-written digits and medical MRIimages. Our results provide a simple and efficient way to generate universalperturbations on heterogeneous classification tasks and thus would providevaluable guidance for future quantum learning technologies.\r2023-10-24\nA Survey on Few-Shot Class-Incremental Learning\nSongsong Tian, Lusi Li, Weijun Li, Hang Ran, Xin Ning, Prayag Tiwari\nabstract\rabstract: Large deep learning models are impressive, but they struggle when real-timedata is not available. Few-shot class-incremental learning (FSCIL) poses asignificant challenge for deep neural networks to learn new tasks from just afew labeled samples without forgetting the previously learned ones. This setupeasily leads to catastrophic forgetting and overfitting problems, severelyaffecting model performance. Studying FSCIL helps overcome deep learning modellimitations on data volume and acquisition time, while improving practicalityand adaptability of machine learning models. This paper provides acomprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesizefew-shot learning and incremental learning, focusing on introducing FSCIL fromtwo perspectives, while reviewing over 30 theoretical research studies and morethan 20 applied research studies. From the theoretical perspective, we providea novel categorization approach that divides the field into five subcategories,including traditional machine learning methods, meta-learning based methods,feature and feature space-based methods, replay-based methods, and dynamicnetwork structure-based methods. We also evaluate the performance of recenttheoretical research on benchmark datasets of FSCIL. From the applicationperspective, FSCIL has achieved impressive achievements in various fields ofcomputer vision such as image classification, object detection, and imagesegmentation, as well as in natural language processing and graph. We summarizethe important applications. Finally, we point out potential future researchdirections, including applications, problem setups, and theory development.Overall, this paper offers a comprehensive analysis of the latest advances inFSCIL from a methodological, performance, and application perspective.\r2023-10-20\nFederated Unlearning: How to Efficiently Erase a Client in FL?\nAnisa Halimi, Swanand Kadhe, Ambrish Rawat, Nathalie Baracaldo\nabstract\rabstract: With privacy legislation empowering the users with the right to be forgotten,it has become essential to make a model amenable for forgetting some of itstraining data. However, existing unlearning methods in the machine learningcontext can not be directly applied in the context of distributed settings likefederated learning due to the differences in learning protocol and the presenceof multiple actors. In this paper, we tackle the problem of federatedunlearning for the case of erasing a client by removing the influence of theirentire local data from the trained global model. To erase a client, we proposeto first perform local unlearning at the client to be erased, and then use thelocally unlearned model as the initialization to run very few rounds offederated learning between the server and the remaining clients to obtain theunlearned global model. We empirically evaluate our unlearning method byemploying multiple performance measures on three datasets, and demonstrate thatour unlearning method achieves comparable performance as the gold standardunlearning method of federated retraining from scratch, while beingsignificantly efficient. Unlike prior works, our unlearning method neitherrequires global access to the data used for training nor the history of theparameter updates to be stored by the server or any of the clients.\r2023-10-13\nEfficient Model Adaptation for Continual Learning at the Edge\nZachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang\nabstract\rabstract: Most machine learning (ML) systems assume stationary and matching datadistributions during training and deployment. This is often a false assumption.When ML models are deployed on real devices, data distributions often shiftover time due to changes in environmental factors, sensor characteristics, andtask-of-interest. While it is possible to have a human-in-the-loop to monitorfor distribution shifts and engineer new architectures in response to theseshifts, such a setup is not cost-effective. Instead, non-stationary automatedML (AutoML) models are needed. This paper presents theEncoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learningunder domain shifts. The EAR framework uses a fixed deep neural network (DNN)feature encoder and trains shallow networks on top of the encoder to handlenovel data. The EAR framework is capable of 1) detecting when new data isout-of-distribution (OOD) by combining DNNs with hyperdimensional computing(HDC), 2) identifying low-parameter neural adaptors to adapt the model to theOOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizingcatastrophic forgetting on previous tasks by progressively growing the neuralarchitecture as needed and dynamically routing data through the appropriateadaptors and reconfigurators for handling domain-incremental andclass-incremental continual learning. We systematically evaluate our approachon several benchmark datasets for domain adaptation and demonstrate strongperformance compared to state-of-the-art algorithms for OOD detection andfew-/zero-shot NAS.\r2023-10-12\nIn-Context Unlearning: Language Models as Few Shot Unlearners\nMartin Pawelczyk, Seth Neel, Himabindu Lakkaraju\nabstract\rabstract: Machine unlearning, the study of efficiently removing the impact of specifictraining points on the trained model, has garnered increased attention of late,driven by the need to comply with privacy regulations like the Right to beForgotten. Although unlearning is particularly relevant for LLMs in light ofthe copyright issues they raise, achieving precise unlearning iscomputationally infeasible for very large models. To this end, recent work hasproposed several algorithms which approximate the removal of training datawithout retraining the model. These algorithms crucially rely on access to themodel parameters in order to update them, an assumption that may not hold inpractice due to computational constraints or when the LLM is accessed via API.In this work, we propose a new class of unlearning methods for LLMs we call\u0026rsquo;\u0026lsquo;In-Context Unlearning\u0026rsquo;\u0026rsquo;, providing inputs in context and without having toupdate model parameters. To unlearn a particular training instance, we providethe instance alongside a flipped label and additional correctly labelledinstances which are prepended as inputs to the LLM at inference time. Ourexperimental results demonstrate that these contexts effectively removespecific information from the training set while maintaining performance levelsthat are competitive with (or in some cases exceed) state-of-the-art unlearningmethods that require access to the LLM parameters.\r2023-10-09\nUnlearning with Fisher Masking\nYufang Liu, Changzhi Sun, Yuanbin Wu, Aimin Zhou\nabstract\rabstract: Machine unlearning aims to revoke some training data after learning inresponse to requests from users, model developers, and administrators. Mostprevious methods are based on direct fine-tuning, which may neither remove datacompletely nor retain full performances on the remain data. In this work, wefind that, by first masking some important parameters before fine-tuning, theperformances of unlearning could be significantly improved. We propose a newmasking strategy tailored to unlearning based on Fisher information.Experiments on various datasets and network structures show the effectivenessof the method: without any fine-tuning, the proposed Fisher masking couldunlearn almost completely while maintaining most of the performance on theremain data. It also exhibits stronger stability compared to other unlearningbaselines\rLessons Learned: Defending Against Property Inference Attacks\nJoshua Stock, Jens Wettlaufer, Daniel Demmler, Hannes Federrath\nabstract\rabstract: This work investigates and evaluates multiple defense strategies againstproperty inference attacks (PIAs), a privacy attack against machine learningmodels. Given a trained machine learning model, PIAs aim to extract statisticalproperties of its underlying training data, e.g., reveal the ratio of men andwomen in a medical training data set. While for other privacy attacks likemembership inference, a lot of research on defense mechanisms has beenpublished, this is the first work focusing on defending against PIAs. With theprimary goal of developing a generic mitigation strategy against white-boxPIAs, we propose the novel approach property unlearning. Extensive experimentswith property unlearning show that while it is very effective when defendingtarget models against specific adversaries, property unlearning is not able togeneralize, i.e., protect against a whole class of PIAs. To investigate thereasons behind this limitation, we present the results of experiments with theexplainable AI tool LIME. They show how state-of-the-art property inferenceadversaries with the same objective focus on different parts of the targetmodel. We further elaborate on this with a follow-up experiment, in which weuse the visualization technique t-SNE to exhibit how severely statisticaltraining data properties are manifested in machine learning models. Based onthis, we develop the conjecture that post-training techniques like propertyunlearning might not suffice to provide the desirable generic protectionagainst PIAs. As an alternative, we investigate the effects of simpler trainingdata preprocessing methods like adding Gaussian noise to images of a trainingdata set on the success rate of PIAs. We conclude with a discussion of thedifferent defense approaches, summarize the lessons learned and providedirections for future work.\rLearning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning\nJinyung Hong, Theodore P. Pavlic\nabstract\rabstract: Existing Continual Learning (CL) approaches have focused on addressingcatastrophic forgetting by leveraging regularization methods, replay buffers,and task-specific components. However, realistic CL solutions must be shapednot only by metrics of catastrophic forgetting but also by computationalefficiency and running time. Here, we introduce a novel neural networkarchitecture inspired by neuromodulation in biological nervous systems toeconomically and efficiently address catastrophic forgetting and provide newavenues for interpreting learned representations. Neuromodulation is abiological mechanism that has received limited attention in machine learning;it dynamically controls and fine tunes synaptic dynamics in real time to trackthe demands of different behavioral contexts. Inspired by this, our proposedarchitecture learns a relatively small set of parameters per task context that\\emph{neuromodulates} the activity of unchanging, randomized weights thattransform the input. We show that this approach has strong learning performanceper task despite the very small number of learnable parameters. Furthermore,because context vectors are so compact, multiple networks can be storedconcurrently with no interference and little spatial footprint, thus completelyeliminating catastrophic forgetting and accelerating the training process.\rInvestigating Continuous Learning in Spiking Neural Networks\nC. Tanner Fredieu\nabstract\rabstract: In this paper, the use of third-generation machine learning, also known asspiking neural network architecture, for continuous learning was investigatedand compared to conventional models. The experimentation was divided into threeseparate phases. The first phase focused on training the conventional modelsvia transfer learning. The second phase trains a Nengo model from theirlibrary. Lastly, each conventional model is converted into a spiking neuralnetwork and trained. Initial results from phase 1 are inline with knownknowledge about continuous learning within current machine learning literature.All models were able to correctly identify the current classes, but they wouldimmediately see a sharp performance drop in previous classes due tocatastrophic forgetting. However, the SNN models were able to retain someinformation about previous classes. Although many of the previous classes werestill identified as the current trained classes, the output probabilitiesshowed a higher than normal value to the actual class. This indicates that theSNN models do have potential to overcome catastrophic forgetting but much workis still needed.\r2023-10-07\nA Survey of Graph Unlearning\nAnwar Said, Tyler Derr, Mudassir Shabbir, Waseem Abbas, Xenofon Koutsoukos\nabstract\rabstract: Graph unlearning emerges as a crucial advancement in the pursuit ofresponsible AI, providing the means to remove sensitive data traces fromtrained models, thereby upholding the right to be forgotten. It is evident thatgraph machine learning exhibits sensitivity to data privacy and adversarialattacks, necessitating the application of graph unlearning techniques toaddress these concerns effectively. In this comprehensive survey paper, wepresent the first systematic review of graph unlearning approaches,encompassing a diverse array of methodologies and offering a detailed taxonomyand up-to-date literature overview to facilitate the understanding ofresearchers new to this field. Additionally, we establish the vital connectionsbetween graph unlearning and differential privacy, augmenting our understandingof the relevance of privacy-preserving techniques in this context. To ensureclarity, we provide lucid explanations of the fundamental concepts andevaluation measures used in graph unlearning, catering to a broader audiencewith varying levels of expertise. Delving into potential applications, weexplore the versatility of graph unlearning across various domains, includingbut not limited to social networks, adversarial settings, andresource-constrained environments like the Internet of Things (IoT),illustrating its potential impact in safeguarding data privacy and enhancing AIsystems\u0026rsquo; robustness. Finally, we shed light on promising research directions,encouraging further progress and innovation within the domain of graphunlearning. By laying a solid foundation and fostering continued progress, thissurvey seeks to inspire researchers to further advance the field of graphunlearning, thereby instilling confidence in the ethical growth of AI systemsand reinforcing the responsible application of machine learning techniques invarious domains.\rDFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning\nKangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao\nabstract\rabstract: Federated Learning (FL) is a privacy-constrained decentralized machinelearning paradigm in which clients enable collaborative training withoutcompromising private data. However, how to learn a robust global model in thedata-heterogeneous and model-heterogeneous FL scenarios is challenging. Toaddress it, we resort to data-free knowledge distillation to propose a new FLmethod (namely DFRD). DFRD equips a conditional generator on the server toapproximate the training space of the local models uploaded by clients, andsystematically investigates its training in terms of fidelity, transferability}and diversity. To overcome the catastrophic forgetting of the global modelcaused by the distribution shifts of the generator across communication rounds,we maintain an exponential moving average copy of the generator on the server.Additionally, we propose dynamic weighting and label sampling to accuratelyextract knowledge from local models. Finally, our extensive experiments onvarious image classification tasks illustrate that DFRD achieves significantperformance gains compared to SOTA baselines.\r2023-10-02\nTowards guarantees for parameter isolation in continual learning\nGiulia Lanzillotta, Sidak Pal Singh, Benjamin F. Grewe, Thomas Hofmann\nabstract\rabstract: Deep learning has proved to be a successful paradigm for solving manychallenges in machine learning. However, deep neural networks fail when trainedsequentially on multiple tasks, a shortcoming known as catastrophic forgettingin the continual learning literature. Despite a recent flourish of learningalgorithms successfully addressing this problem, we find that provableguarantees against catastrophic forgetting are lacking. In this work, we studythe relationship between learning and forgetting by looking at the geometry ofneural networks\u0026rsquo; loss landscape. We offer a unifying perspective on a family ofcontinual learning algorithms, namely methods based on parameter isolation, andwe establish guarantees on catastrophic forgetting for some of them.\r2023-09-30\nViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation\nJiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang\nabstract\rabstract: Since real-world machine systems are running in non-stationary environments,Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trainedmodel to continually changing target domains. Recently, existing methods mainlyfocus on model-based adaptation, which aims to leverage a self-training mannerto extract the target domain knowledge. However, pseudo labels can be noisy andthe updated model parameters are unreliable under dynamic data distributions,leading to error accumulation and catastrophic forgetting in the continualadaptation process. To tackle these challenges and maintain the modelplasticity, we tactfully design a Visual Domain Adapter (ViDA) for CTTA,explicitly handling both domain-specific and domain-shared knowledge.Specifically, we first comprehensively explore the different domainrepresentations of the adapters with trainable high-rank or low-rank embeddingspaces. Then we inject ViDAs into the pre-trained model, which leverageshigh-rank and low-rank features to adapt the current domain distribution andmaintain the continual domain-shared knowledge, respectively. To exploit thelow-rank and high-rank ViDAs more effectively, we further propose a HomeostaticKnowledge Allotment (HKA) strategy, which adaptively combines differentknowledge from each ViDA. Extensive experiments conducted on four widely usedbenchmarks demonstrate that our proposed method achieves state-of-the-artperformance in both classification and segmentation CTTA tasks. Note that, ourmethod can be regarded as a novel transfer paradigm for large-scale models,delivering promising results in adaptation to continually changingdistributions.\r2023-09-25\nAdapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks\nPiyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P\nabstract\rabstract: The increased attention to regulating the outputs of deep generative models,driven by growing concerns about privacy and regulatory compliance, hashighlighted the need for effective control over these models. This necessityarises from instances where generative models produce outputs containingundesirable, offensive, or potentially harmful content. To tackle thischallenge, the concept of machine unlearning has emerged, aiming to forgetspecific learned information or to erase the influence of undesired datasubsets from a trained model. The objective of this work is to prevent thegeneration of outputs containing undesired features from a pre-trained GANwhere the underlying training data set is inaccessible. Our approach isinspired by a crucial observation: the parameter space of GANs exhibitsmeaningful directions that can be leveraged to suppress specific undesiredfeatures. However, such directions usually result in the degradation of thequality of generated samples. Our proposed method, known as\u0026rsquo;Adapt-then-Unlearn,\u0026rsquo; excels at unlearning such undesirable features while alsomaintaining the quality of generated samples. This method unfolds in twostages: in the initial stage, we adapt the pre-trained GAN using negativesamples provided by the user, while in the subsequent stage, we focus onunlearning the undesired feature. During the latter phase, we train thepre-trained GAN using positive samples, incorporating a repulsion regularizer.This regularizer encourages the model\u0026rsquo;s parameters to be away from theparameters associated with the adapted model from the first stage while alsomaintaining the quality of generated samples. To the best of our knowledge, ourapproach stands as first method addressing unlearning in GANs. We validate theeffectiveness of our method through comprehensive experiments.\r2023-09-22\nRight to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions\nDawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu\nabstract\rabstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.\r2023-09-21\nA Tagging Solution to Discover IoT Devices in Apartments\nBerkay Kaplan, Jingyu Qian, Israel J Lopez-Toledo, Carl A. Gunter\nabstract\rabstract: The number of IoT devices in smart homes is increasing. This broad adoptionfacilitates users\u0026rsquo; lives, but it also brings problems. One such issue is thatsome IoT devices may invade users\u0026rsquo; privacy. Some reasons for this invasion canstem from obscure data collection practices or hidden devices. Specific IoTdevices can exist out of sight and still collect user data to send to thirdparties via the Internet. Owners can easily forget the location or even theexistence of these devices, especially if the owner is a landlord who managesseveral properties. The landlord-owner scenario creates multi-user problems asdesigners build machines for single users. We developed tags that use wirelessprotocols, buzzers, and LED lighting to lead users to solve the issue of devicediscovery in shared spaces and accommodate multi-user scenarios. They areattached to IoT devices inside a unit during their installation to be laterdiscovered by a tenant. These tags have similar functionalities as the popularTile models or Airtag, but our tags have different features based on ourprivacy use case. Our tags do not require pairing; multiple users can interactwith them through our Android application. Although researchers developedseveral other tools, such as thermal cameras or virtual reality (VR), fordiscovering devices in environments, they have not used wireless protocols as asolution. We measured specific performance metrics of our tags to analyze theirfeasibility for this problem. We also conducted a user study to measure theparticipants\u0026rsquo; comfort levels while finding objects with our tags attached. Ourresults indicate that wireless tags can be viable for device tracking inresidential properties.\r2023-09-15\nForgetting 1-Limited Automata\nGiovanni Pighizzini, Luca Prigioniero\nabstract\rabstract: We introduce and investigate forgetting 1-limited automata, which aresingle-tape Turing machines that, when visiting a cell for the first time,replace the input symbol in it by a fixed symbol, so forgetting the originalcontents. These devices have the same computational power as finite automata,namely they characterize the class of regular languages. We study the cost insize of the conversions of forgetting 1-limited automata, in bothnondeterministic and deterministic cases, into equivalent one-waynondeterministic and deterministic automata, providing optimal bounds in termsof exponential or superpolynomial functions. We also discuss the sizerelationships with two-way finite automata. In this respect, we prove theexistence of a language for which forgetting 1-limited automata areexponentially larger than equivalent minimal deterministic two-way automata.\rContinual Learning with Deep Streaming Regularized Discriminant Analysis\nJoe Khawand, Peter Hanappe, David Colliaux\nabstract\rabstract: Continual learning is increasingly sought after in real world machinelearning applications, as it enables learning in a more human-like manner.Conventional machine learning approaches fail to achieve this, as incrementallyupdating the model with non-identically distributed data leads to catastrophicforgetting, where existing representations are overwritten. Althoughtraditional continual learning methods have mostly focused on batch learning,which involves learning from large collections of labeled data sequentially,this approach is not well-suited for real-world applications where we wouldlike new data to be integrated directly. This necessitates a paradigm shifttowards streaming learning. In this paper, we propose a streaming version ofregularized discriminant analysis as a solution to this challenge. We combineour algorithm with a convolutional neural network and demonstrate that itoutperforms both batch learning and existing streaming learning algorithms onthe ImageNet ILSVRC-2012 dataset.\r2023-09-14\nPolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning\nJunfeng Guo, Ang Li, Cong Liu\nabstract\rabstract: While real-world applications of reinforcement learning are becoming popular,the security and robustness of RL systems are worthy of more attention andexploration. In particular, recent works have revealed that, in a multi-agentRL environment, backdoor trigger actions can be injected into a victim agent(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as itsees the backdoor trigger action. To ensure the security of RL agents againstmalicious backdoors, in this work, we propose the problem of Backdoor Detectionin a multi-agent competitive reinforcement learning system, with the objectiveof detecting Trojan agents as well as the corresponding potential triggeractions, and further trying to mitigate their Trojan behavior. In order tosolve this problem, we propose PolicyCleanse that is based on the property thatthe activated Trojan agents accumulated rewards degrade noticeably afterseveral timesteps. Along with PolicyCleanse, we also design a machineunlearning-based approach that can effectively mitigate the detected backdoor.Extensive experiments demonstrate that the proposed methods can accuratelydetect Trojan agents, and outperform existing backdoor mitigation baselineapproaches by at least 3% in winning rate across various types of agents andenvironments.\rSemantic Parsing in Limited Resource Conditions\nZhuang Li\nabstract\rabstract: This thesis explores challenges in semantic parsing, specifically focusing onscenarios with limited data and computational resources. It offers solutionsusing techniques like automatic data curation, knowledge transfer, activelearning, and continual learning. For tasks with no parallel training data, the thesis proposes generatingsynthetic training examples from structured database schemas. When there isabundant data in a source domain but limited parallel data in a target domain,knowledge from the source is leveraged to improve parsing in the target domain. For multilingual situations with limited data in the target languages, thethesis introduces a method to adapt parsers using a limited human translationbudget. Active learning is applied to select source-language samples for manualtranslation, maximizing parser performance in the target language. In addition,an alternative method is also proposed to utilize machine translation services,supplemented by human-translated data, to train a more effective parser. When computational resources are limited, a continual learning approach isintroduced to minimize training time and computational memory. This maintainsthe parser\u0026rsquo;s efficiency in previously learned tasks while adapting it to newtasks, mitigating the problem of catastrophic forgetting. Overall, the thesis provides a comprehensive set of methods to improvesemantic parsing in resource-constrained conditions.\r2023-09-10\nVariational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics\nHany Abdulsamad, Peter Nickl, Pascal Klink, Jan Peters\nabstract\rabstract: Well-calibrated probabilistic regression models are a crucial learningcomponent in robotics applications as datasets grow rapidly and tasks becomemore complex. Unfortunately, classical regression models are usually eitherprobabilistic kernel machines with a flexible structure that does not scalegracefully with data or deterministic and vastly scalable automata, albeit witha restrictive parametric form and poor regularization. In this paper, weconsider a probabilistic hierarchical modeling paradigm that combines thebenefits of both worlds to deliver computationally efficient representationswith inherent complexity regularization. The presented approaches areprobabilistic interpretations of local regression techniques that approximatenonlinear functions through a set of local linear or polynomial units.Importantly, we rely on principles from Bayesian nonparametrics to formulateflexible models that adapt their complexity to the data and can potentiallyencompass an infinite number of components. We derive two efficient variationalinference techniques to learn these representations and highlight theadvantages of hierarchical infinite local regression models, such as dealingwith non-smooth functions, mitigating catastrophic forgetting, and enablingparameter sharing and fast predictions. Finally, we validate this approach onlarge inverse dynamics datasets and test the learned models in real-worldcontrol scenarios.\r2023-09-02\nTight Bounds for Machine Unlearning via Differential Privacy\nYiyang Huang, Clément L. Canonne\nabstract\rabstract: We consider the formulation of \u0026ldquo;machine unlearning\u0026rdquo; of Sekhari, Acharya,Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called \u0026ldquo;right to beforgotten\u0026rdquo; by requiring that a trained model, upon request, should be able to\u0026quot;unlearn\u0026quot; a number of points from the training data, as if they had never beenincluded in the first place. Sekhari et al. established some positive andnegative results about the number of data points that can be successfullyunlearnt by a trained model without impacting the model\u0026rsquo;s accuracy (the\u0026quot;deletion capacity\u0026quot;), showing that machine unlearning could be achieved byusing differentially private (DP) algorithms. However, their results left opena gap between upper and lower bounds on the deletion capacity of thesealgorithms: our work fully closes this gap, obtaining tight bounds on thedeletion capacity achievable by DP-based machine unlearning algorithms.\r2023-08-31\nContinual Learning From a Stream of APIs\nEnneng Yang, Zhenyi Wang, Li Shen, Nan Yin, Tongliang Liu, Guibing Guo, Xingwei Wang, Dacheng Tao\nabstract\rabstract: Continual learning (CL) aims to learn new tasks without forgetting previoustasks. However, existing CL methods require a large amount of raw data, whichis often unavailable due to copyright considerations and privacy risks.Instead, stakeholders usually release pre-trained machine learning models as aservice (MLaaS), which users can access via APIs. This paper considers twopractical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL(DFCL-APIs), which achieve CL from a stream of APIs with partial or no rawdata. Performing CL under these two new settings faces several challenges:unavailable full raw data, unknown model parameters, heterogeneous models ofarbitrary architecture and scale, and catastrophic forgetting of previous APIs.To overcome these issues, we propose a novel data-free cooperative continualdistillation learning framework that distills knowledge from a stream of APIsinto a CL model by generating pseudo data, just by querying APIs. Specifically,our framework includes two cooperative generators and one CL model, formingtheir training as an adversarial game. We first use the CL model and thecurrent API as fixed discriminators to train generators via a derivative-freemethod. Generators adversarially generate hard and diverse synthetic data tomaximize the response gap between the CL model and the API. Next, we train theCL model by minimizing the gap between the responses of the CL model and theblack-box API on synthetic data, to transfer the API\u0026rsquo;s knowledge to the CLmodel. Furthermore, we propose a new regularization term based on networksimilarity to prevent catastrophic forgetting of previous APIs.Our methodperforms comparably to classic CL with full raw data on the MNIST and SVHN inthe DFCL-APIs setting. In the DECL-APIs setting, our method achieves 0.97x,0.75x and 0.69x performance of classic CL on CIFAR10, CIFAR100, andMiniImageNet.\r2023-08-29\nAn Empirical Investigation of the Role of Pre-training in Lifelong Learning\nSanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell\nabstract\rabstract: The lifelong learning paradigm in machine learning is an attractivealternative to the more prominent isolated learning scheme not only due to itsresemblance to biological learning but also its potential to reduce energywaste by obviating excessive model re-training. A key challenge to thisparadigm is the phenomenon of catastrophic forgetting. With the increasingpopularity and success of pre-trained models in machine learning, we pose thequestion: What role does pre-training play in lifelong learning, specificallywith respect to catastrophic forgetting? We investigate existing methods in thecontext of large, pre-trained models and evaluate their performance on avariety of text and image classification tasks, including a large-scale studyusing a novel data set of 15 diverse NLP tasks. Across all settings, we observethat generic pre-training implicitly alleviates the effects of catastrophicforgetting when learning multiple tasks sequentially compared to randomlyinitialized models. We then further investigate why pre-training alleviatesforgetting in this setting. We study this phenomenon by analyzing the losslandscape, finding that pre-trained weights appear to ease forgetting byleading to wider minima. Based on this insight, we propose jointly optimizingfor current task loss and loss basin sharpness to explicitly encourage widerbasins during sequential fine-tuning. We show that this optimization approachoutperforms several state-of-the-art task-sequential continual learningalgorithms across multiple settings, occasionally even without retaining amemory that scales in size with the number of tasks.\r2023-08-28\nHeterogeneous Decentralized Machine Unlearning with Seed Model Distillation\nGuanhua Ye, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin\nabstract\rabstract: As some recent information security legislation endowed users withunconditional rights to be forgotten by any trained machine learning model,personalized IoT service providers have to put unlearning functionality intotheir consideration. The most straightforward method to unlearn users\u0026rsquo;contribution is to retrain the model from the initial state, which is notrealistic in high throughput applications with frequent unlearning requests.Though some machine unlearning frameworks have been proposed to speed up theretraining process, they fail to match decentralized learning scenarios. Inthis paper, we design a decentralized unlearning framework called HDUS, whichuses distilled seed models to construct erasable ensembles for all clients.Moreover, the framework is compatible with heterogeneous on-device models,representing stronger scalability in real-world applications. Extensiveexperiments on three real-world datasets show that our HDUS achievesstate-of-the-art performance.\rMachine Unlearning Methodology base on Stochastic Teacher Network\nXulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, Jing Xiao\nabstract\rabstract: The rise of the phenomenon of the \u0026ldquo;right to be forgotten\u0026rdquo; has promptedresearch on machine unlearning, which grants data owners the right to activelywithdraw data that has been used for model training, and requires theelimination of the contribution of that data to the model. A simple method toachieve this is to use the remaining data to retrain the model, but this is notacceptable for other data owners who continue to participate in training.Existing machine unlearning methods have been found to be ineffective inquickly removing knowledge from deep learning models. This paper proposes usinga stochastic network as a teacher to expedite the mitigation of the influencecaused by forgotten data on the model. We performed experiments on threedatasets, and the findings demonstrate that our approach can efficientlymitigate the influence of target data on the model within a single epoch. Thisallows for one-time erasure and reconstruction of the model, and thereconstruction model achieves the same performance as the retrained model.\r2023-08-24\nMachine Unlearning for Causal Inference\nVikas Ramachandra, Mohit Sethi\nabstract\rabstract: Machine learning models play a vital role in making predictions and derivinginsights from data and are being increasingly used for causal inference. Topreserve user privacy, it is important to enable the model to forget some ofits learning/captured information about a given user (machine unlearning). Thispaper introduces the concept of machine unlearning for causal inference,particularly propensity score matching and treatment effect estimation, whichaims to refine and improve the performance of machine learning models forcausal analysis given the above unlearning requirements. The paper presents amethodology for machine unlearning using a neural network-based propensityscore model. The dataset used in the study is the Lalonde dataset, a widelyused dataset for evaluating the effectiveness i.e. the treatment effect of jobtraining programs. The methodology involves training an initial propensityscore model on the original dataset and then creating forget sets byselectively removing instances, as well as matched instance pairs. based onpropensity score matching. These forget sets are used to evaluate the retrainedmodel, allowing for the elimination of unwanted associations. The actualretraining of the model is performed using the retain set. The experimentalresults demonstrate the effectiveness of the machine unlearning approach. Thedistribution and histogram analysis of propensity scores before and afterunlearning provide insights into the impact of the unlearning process on thedata. This study represents the first attempt to apply machine unlearningtechniques to causal inference.\rSplit Unlearning\nGuangsheng Yu, Xu Wang, Caijun Sun, Qin Wang\nabstract\rabstract: Split learning is emerging as a powerful approach to decentralized machinelearning, but the urgent task of unlearning to address privacy issues presentssignificant challenges. Conventional methods of retraining from scratch orgradient ascending require all clients\u0026rsquo; involvement, incurring highcomputational and communication overhead, particularly in public networks whereclients lack resources and may be reluctant to participate in unlearningprocesses they have no interest. In this short article, we propose\\textsc{SplitWiper}, a new framework that integrates the concept of SISA toreduce retraining costs and ensures no interference between the unlearningclient and others in public networks. Recognizing the inherent sharding insplit learning, we first establish the SISA-based design of\\textsc{SplitWiper}. This forms the premise for conceptualizing two unlearningstrategies for label-sharing and non-label-sharing scenarios. This articlerepresents an earlier edition, with extensive experiments being conducted forthe forthcoming full version.\rImproving Translation Faithfulness of Large Language Models via Augmenting Instructions\nYijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou\nabstract\rabstract: Large Language Models (LLMs) present strong general capabilities, and acurrent compelling challenge is stimulating their specialized capabilities,such as machine translation, through low-cost instruction tuning. The standardinstruction-following data is sequentially organized as the concatenation of aninstruction, an input, and a response. As the attention mechanism of LLMs haslimitations on local focus, LLMs tend to focus more on the words or sentencesnearby at each position. This leads to a high risk of instruction forgettingduring decoding. To alleviate the above issues, We propose SWIE(Segment-Weighted Instruction Embedding) and an instruction-following datasetOVERMISS. SWIE improves the model instruction understanding by adding a globalinstruction representation on the following input and response representations.OVERMISS improves model faithfulness by comparing over-translation andmiss-translation results with the correct translation. We apply our methods totwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental resultsdemonstrate significant improvements in translation performance with SWIE basedon BLOOMZ-3b, particularly in zero-shot and long text translations due toreduced instruction forgetting risk. Additionally, OVERMISS outperforms thebaseline in translation performance (e.g. an increase in BLEU scores from 0.69to 3.12 and an average improvement of 0.48 percentage comet scores forLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE(e.g. the BLUE scores increase up to 0.56 from English to German across threedifferent backbones), and both exhibit improvements in the faithfulness metricbased on word alignment.\rA Continual Learning Approach for Cross-Domain White Blood Cell Classification\nArio Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr\nabstract\rabstract: Accurate classification of white blood cells in peripheral blood is essentialfor diagnosing hematological diseases. Due to constantly evolving clinicalsettings, data sources, and disease classifications, it is necessary to updatemachine learning classification models regularly for practical real-world use.Such models significantly benefit from sequentially learning from incoming datastreams without forgetting previously acquired knowledge. However, models cansuffer from catastrophic forgetting, causing a drop in performance on previoustasks when fine-tuned on new data. Here, we propose a rehearsal-based continuallearning approach for class incremental and domain incremental scenarios inwhite blood cell classification. To choose representative samples from previoustasks, we employ exemplar set selection based on the model\u0026rsquo;s predictions. Thisinvolves selecting the most confident samples and the most challenging samplesidentified through uncertainty estimation of the model. We thoroughly evaluatedour proposed approach on three white blood cell classification datasets thatdiffer in color, resolution, and class composition, including scenarios wherenew domains or new classes are introduced to the model with every task. We alsotest a long class incremental experiment with both new domains and new classes.Our results demonstrate that our approach outperforms established baselines incontinual learning, including existing iCaRL and EWC methods for classifyingwhite blood cells in cross-domain environments.\r2023-08-21\nAn Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning\nYun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang\nabstract\rabstract: Catastrophic forgetting (CF) is a phenomenon that occurs in machine learningwhen a model forgets previously learned information as it learns newinformation. As large language models (LLMs) have shown excellent performance,it is interesting to uncover whether CF exists in the continual fine-tuning ofLLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs\u0026rsquo;knowledge, from the perspectives of domain knowledge, reasoning, and readingcomprehension. The experiments demonstrate that catastrophic forgetting isgenerally observed in LLMs ranging from 1b to 7b. Furthermore, as the scaleincreases, the severity of forgetting also intensifies. Comparing thedecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffersless forgetting and maintains more knowledge. We also observe that LLMs canmitigate language bias (e.g. gender bias) during continual fine-tuning.Moreover, we find that ALPACA can maintain more knowledge and capacity comparedwith LLAMA during the continual fine-tuning, which implies that generalinstruction tuning can help mitigate the forgetting phenomenon of LLMs in thefurther fine-tuning process.\r2023-08-19\nGenerative Adversarial Networks Unlearning\nHui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou\nabstract\rabstract: As machine learning continues to develop, and data misuse scandals becomemore prevalent, individuals are becoming increasingly concerned about theirpersonal information and are advocating for the right to remove their data.Machine unlearning has emerged as a solution to erase training data fromtrained machine learning models. Despite its success in classifiers, researchon Generative Adversarial Networks (GANs) is limited due to their uniquearchitecture, including a generator and a discriminator. One challenge pertainsto generator unlearning, as the process could potentially disrupt thecontinuity and completeness of the latent space. This disruption mightconsequently diminish the model\u0026rsquo;s effectiveness after unlearning. Anotherchallenge is how to define a criterion that the discriminator should performfor the unlearning images. In this paper, we introduce a substitution mechanismand define a fake label to effectively mitigate these challenges. Based on thesubstitution mechanism and fake label, we propose a cascaded unlearningapproach for both item and class unlearning within GAN models, in which theunlearning and learning processes run in a cascaded manner. We conducted acomprehensive evaluation of the cascaded unlearning technique using the MNISTand CIFAR-10 datasets. Experimental results demonstrate that this approachachieves significantly improved item and class unlearning efficiency, reducingthe required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets,respectively, in comparison to retraining from scratch. Notably, although themodel\u0026rsquo;s performance experiences minor degradation after unlearning, thisreduction is negligible when dealing with a minimal number of images (e.g., 64)and has no adverse effects on downstream tasks such as classification.\rAdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning\nXingyu Li, Bo Tang, Haifeng Li\nabstract\rabstract: Continual lifelong learning is an machine learning framework inspired byhuman learning, where learners are trained to continuously acquire newknowledge in a sequential manner. However, the non-stationary nature ofstreaming training data poses a significant challenge known as catastrophicforgetting, which refers to the rapid forgetting of previously learnedknowledge when new tasks are introduced. While some approaches, such asexperience replay (ER), have been proposed to mitigate this issue, theirperformance remains limited, particularly in the class-incremental scenariowhich is considered natural and highly challenging. In this paper, we present anovel algorithm, called adaptive-experience replay (AdaER), to address thechallenge of continual lifelong learning. AdaER consists of two stages: memoryreplay and memory update. In the memory replay stage, AdaER introduces acontextually-cued memory recall (C-CMR) strategy, which selectively replaysmemories that are most conflicting with the current input data in terms of bothdata and task. Additionally, AdaER incorporates an entropy-balanced reservoirsampling (E-BRS) strategy to enhance the performance of the memory buffer bymaximizing information entropy. To evaluate the effectiveness of AdaER, weconduct experiments on established supervised continual lifelong learningbenchmarks, specifically focusing on class-incremental learning scenarios. Theresults demonstrate that AdaER outperforms existing continual lifelong learningbaselines, highlighting its efficacy in mitigating catastrophic forgetting andimproving learning performance.\r2023-08-17\nLearning to Learn: How to Continuously Teach Humans and Machines\nParantak Singh, You Li, Ankur Sikarwar, Weixian Lei, Daniel Gao, Morgan Bruce Talbot, Ying Sun, Mike Zheng Shou, Gabriel Kreiman, Mengmi Zhang\nabstract\rabstract: Curriculum design is a fundamental component of education. For example, whenwe learn mathematics at school, we build upon our knowledge of addition tolearn multiplication. These and other concepts must be mastered before ourfirst algebra lesson, which also reinforces our addition and multiplicationskills. Designing a curriculum for teaching either a human or a machine sharesthe underlying goal of maximizing knowledge transfer from earlier to latertasks, while also minimizing forgetting of learned tasks. Prior research oncurriculum design for image classification focuses on the ordering of trainingexamples during a single offline task. Here, we investigate the effect of theorder in which multiple distinct tasks are learned in a sequence. We focus onthe online class-incremental continual learning setting, where algorithms orhumans must learn image classes one at a time during a single pass through adataset. We find that curriculum consistently influences learning outcomes forhumans and for multiple continual machine learning algorithms across severalbenchmark datasets. We introduce a novel-object recognition dataset for humancurriculum learning experiments and observe that curricula that are effectivefor humans are highly correlated with those that are effective for machines. Asan initial step towards automated curriculum design for onlineclass-incremental learning, we propose a novel algorithm, dubbed CurriculumDesigner (CD), that designs and ranks curricula based on inter-class featuresimilarities. We find significant overlap between curricula that areempirically highly effective and those that are highly ranked by our CD. Ourstudy establishes a framework for further research on teaching humans andmachines to learn continuously using optimized curricula.\r2023-08-16\nAdvancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation\nJingrui Hou, Georgina Cosma, Axel Finke\nabstract\rabstract: Continual learning refers to the capability of a machine learning model tolearn and adapt to new information, without compromising its performance onpreviously learned tasks. Although several studies have investigated continuallearning methods for information retrieval tasks, a well-defined taskformulation is still lacking, and it is unclear how typical learning strategiesperform in this context. To address this challenge, a systematic taskformulation of continual neural information retrieval is presented, along witha multiple-topic dataset that simulates continuous information retrieval. Acomprehensive continual neural information retrieval framework consisting oftypical retrieval models and continual learning strategies is then proposed.Empirical evaluations illustrate that the proposed framework can successfullyprevent catastrophic forgetting in neural information retrieval and enhanceperformance on previously learned tasks. The results indicate thatembedding-based retrieval models experience a decline in their continuallearning performance as the topic shift distance and dataset volume of newtasks increase. In contrast, pretraining-based models do not show any suchcorrelation. Adopting suitable learning strategies can mitigate the effects oftopic shift and data augmentation.\r2023-08-08\nImproving Performance in Continual Learning Tasks using Bio-Inspired Architectures\nSandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash\nabstract\rabstract: The ability to learn continuously from an incoming data stream withoutcatastrophic forgetting is critical to designing intelligent systems. Manyapproaches to continual learning rely on stochastic gradient descent and itsvariants that employ global error updates, and hence need to adopt strategiessuch as memory buffers or replay to circumvent its stability, greed, andshort-term memory limitations. To address this limitation, we have developed abiologically inspired lightweight neural network architecture that incorporatessynaptic plasticity mechanisms and neuromodulation and hence learns throughlocal error signals to enable online continual learning without stochasticgradient descent. Our approach leads to superior online continual learning performance onSplit-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to othermemory-constrained learning approaches and matches that of the state-of-the-artmemory-intensive replay-based approaches. We further demonstrate theeffectiveness of our approach by integrating key design concepts into otherbackpropagation-based continual learning algorithms, significantly improvingtheir accuracy. Our results provide compelling evidence for the importance ofincorporating biological principles into machine learning models and offerinsights into how we can leverage them to design more efficient and robustsystems for online continual learning.\r2023-08-07\nMachine Unlearning of Features and Labels\nAlexander Warnecke, Lukas Pirch, Christian Wressnegger, Konrad Rieck\nabstract\rabstract: Removing information from a machine learning model is a non-trivial task thatrequires to partially revert the training process. This task is unavoidablewhen sensitive data, such as credit card numbers or passwords, accidentallyenter the model and need to be removed afterwards. Recently, different conceptsfor machine unlearning have been proposed to address this problem. While theseapproaches are effective in removing individual data points, they do not scaleto scenarios where larger groups of features and labels need to be reverted. Inthis paper, we propose the first method for unlearning features and labels. Ourapproach builds on the concept of influence functions and realizes unlearningthrough closed-form updates of model parameters. It enables to adapt theinfluence of training data on a learning model retrospectively, therebycorrecting data leaks and privacy issues. For learning models with stronglyconvex loss functions, our method provides certified unlearning withtheoretical guarantees. For models with non-convex losses, we empirically showthat unlearning features and labels is effective and significantly faster thanother strategies.\r2023-08-03\nRelational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship\nQuanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Kai Ma, Yefeng Zheng, Deyu Meng\nabstract\rabstract: Continual learning is a promising machine learning paradigm to learn newtasks while retaining previously learned knowledge over streaming trainingdata. Till now, rehearsal-based methods, keeping a small part of data from oldtasks as a memory buffer, have shown good performance in mitigatingcatastrophic forgetting for previously learned knowledge. However, most ofthese methods typically treat each new task equally, which may not adequatelyconsider the relationship or similarity between old and new tasks. Furthermore,these methods commonly neglect sample importance in the continual trainingprocess and result in sub-optimal performance on certain tasks. To address thischallenging problem, we propose Relational Experience Replay (RER), a bi-levellearning framework, to adaptively tune task-wise relationships and sampleimportance within each task to achieve a better stability' and plasticity\u0026rsquo;trade-off. As such, the proposed method is capable of accumulating newknowledge while consolidating previously learned old knowledge during continuallearning. Extensive experiments conducted on three publicly available datasets(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed methodcan consistently improve the performance of all baselines and surpass currentstate-of-the-art methods.\r2023-07-20\nFrom Adaptive Query Release to Machine Unlearning\nEnayat Ullah, Raman Arora\nabstract\rabstract: We formalize the problem of machine unlearning as design of efficientunlearning algorithms corresponding to learning algorithms which perform aselection of adaptive queries from structured query classes. We give efficientunlearning algorithms for linear and prefix-sum query classes. As applications,we show that unlearning in many problems, in particular, stochastic convexoptimization (SCO), can be reduced to the above, yielding improved guaranteesfor the problem. In particular, for smooth Lipschitz losses and any $\\rho\u0026gt;0$,our results yield an unlearning algorithm with excess population risk of$\\tilde O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\rho}\\big)$ with unlearningquery (gradient) complexity $\\tilde O(\\rho \\cdot \\text{RetrainingComplexity})$, where $d$ is the model dimensionality and $n$ is the initialnumber of samples. For non-smooth Lipschitz losses, we give an unlearningalgorithm with excess population risk $\\tildeO\\big(\\frac{1}{\\sqrt{n}}+\\big(\\frac{\\sqrt{d}}{n\\rho}\\big)^{1/2}\\big)$ with thesame unlearning query (gradient) complexity. Furthermore, in the special caseof Generalized Linear Models (GLMs), such as those in linear and logisticregression, we get dimension-independent rates of $\\tildeO\\big(\\frac{1}{\\sqrt{n}} +\\frac{1}{(n\\rho)^{2/3}}\\big)$ and $\\tildeO\\big(\\frac{1}{\\sqrt{n}} +\\frac{1}{(n\\rho)^{1/3}}\\big)$ for smooth Lipschitzand non-smooth Lipschitz losses respectively. Finally, we give generalizationsof the above from one unlearning request to \\textit{dynamic} streams consistingof insertions and deletions.\rShared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples\nShaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu\nabstract\rabstract: Backdoor attacks are serious security threats to machine learning modelswhere an adversary can inject poisoned samples into the training set, causing abackdoored model which predicts poisoned samples with particular triggers toparticular target classes, while behaving normally on benign samples. In thispaper, we explore the task of purifying a backdoored model using a small cleandataset. By establishing the connection between backdoor risk and adversarialrisk, we derive a novel upper bound for backdoor risk, which mainly capturesthe risk on the shared adversarial examples (SAEs) between the backdoored modeland the purified model. This upper bound further suggests a novel bi-leveloptimization problem for mitigating backdoor using adversarial trainingtechniques. To solve it, we propose Shared Adversarial Unlearning (SAU).Specifically, SAU first generates SAEs, and then, unlearns the generated SAEssuch that they are either correctly classified by the purified model and/ordifferently classified by the two models, such that the backdoor effect in thebackdoored model will be mitigated in the purified model. Experiments onvarious benchmark datasets and network architectures show that our proposedmethod achieves state-of-the-art performance for backdoor defense.\rTangent Transformers for Composition, Privacy and Removal\nTian Yu Liu, Aditya Golatkar, Stefano Soatto\nabstract\rabstract: We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuninglinearized transformers obtained by computing a First-order Taylor Expansionaround a pre-trained initialization. We show that the Jacobian-Vector Productresulting from linearization can be computed efficiently in a single forwardpass, reducing training and inference cost to the same order of magnitude asits original non-linear counterpart, while using the same number of parameters.Furthermore, we show that, when applied to various downstream visualclassification tasks, the resulting Tangent Transformer fine-tuned with TAFTcan perform comparably with fine-tuning the original non-linear network. SinceTangent Transformers are linear with respect to the new set of weights, and theresulting fine-tuning loss is convex, we show that TAFT enjoys severaladvantages compared to non-linear fine-tuning when it comes to modelcomposition, parallel training, machine unlearning, and differential privacy.\r2023-07-18\nGradient Surgery for One-shot Unlearning on Generative Model\nSeohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim\nabstract\rabstract: Recent regulation on right-to-be-forgotten emerges tons of interest inunlearning pre-trained machine learning models. While approximating astraightforward yet expensive approach of retrain-from-scratch, recent machineunlearning methods unlearn a sample by updating weights to remove its influenceon the weight parameters. In this paper, we introduce a simple yet effectiveapproach to remove a data influence on the deep generative model. Inspired byworks in multi-task learning, we propose to manipulate gradients to regularizethe interplay of influence among samples by projecting gradients onto thenormal plane of the gradients to be retained. Our work is agnostic tostatistics of the removal samples, outperforming existing baselines whileproviding theoretical analysis for the first time in unlearning a generativemodel.\r2023-07-16\nCertifying Model Accuracy under Distribution Shifts\nAounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi\nabstract\rabstract: Certified robustness in machine learning has primarily focused on adversarialperturbations of the input with a fixed attack budget for each point in thedata distribution. In this work, we present provable robustness guarantees onthe accuracy of a model under bounded Wasserstein shifts of the datadistribution. We show that a simple procedure that randomizes the input of themodel within a transformation space is provably robust to distributional shiftsunder the transformation. Our framework allows the datum-specific perturbationsize to vary across different points in the input distribution and is generalenough to include fixed-sized perturbations as well. Our certificates produceguaranteed lower bounds on the performance of the model for any (natural oradversarial) shift of the input distribution within a Wasserstein ball aroundthe original distribution. We apply our technique to: (i) certify robustnessagainst natural (non-adversarial) transformations of images such as colorshifts, hue shifts and changes in brightness and saturation, (ii) certifyrobustness against adversarial shifts of the input distribution, and (iii) showprovable lower bounds (hardness results) on the performance of models trainedon so-called \u0026ldquo;unlearnable\u0026rdquo; datasets that have been poisoned to interfere withmodel training.\r2023-07-13\nA Scenario-Based Functional Testing Approach to Improving DNN Performance\nHong Zhu, Thi Minh Tam Tran, Aduen Benjumea, Andrew Bradley\nabstract\rabstract: This paper proposes a scenario-based functional testing approach forenhancing the performance of machine learning (ML) applications. The proposedmethod is an iterative process that starts with testing the ML model on variousscenarios to identify areas of weakness. It follows by a further testing on thesuspected weak scenarios and statistically evaluate the model\u0026rsquo;s performance onthe scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios isconfirmed by test results, the treatment of the model is performed byretraining the model using a transfer learning technique with the originalmodel as the base and applying a set of training data specifically targetingthe treated scenarios plus a subset of training data selected at random fromthe original train dataset to prevent the so-call catastrophic forgettingeffect. Finally, after the treatment, the model is assessed and evaluated againby testing on the treated scenarios as well as other scenarios to check if thetreatment is effective and no side effect caused. The paper reports a casestudy with a real ML deep neural network (DNN) model, which is the perceptionsystem of an autonomous racing car. It is demonstrated that the method iseffective in the sense that DNN model\u0026rsquo;s performance can be improved. Itprovides an efficient method of enhancing ML model\u0026rsquo;s performance with much lesshuman and compute resource than retrain from scratch.\r2023-07-10\nFedYolo: Augmenting Federated Learning with Pretrained Transformers\nXuechen Zhang, Mingchen Li, Xiangyu Chang, Jiasi Chen, Amit K. Roy-Chowdhury, Ananda Theertha Suresh, Samet Oymak\nabstract\rabstract: The growth and diversity of machine learning applications motivate arethinking of learning with mobile and edge devices. How can we address diverseclient goals and learn with scarce heterogeneous data? While federated learningaims to address these issues, it has challenges hindering a unified solution.Large transformer models have been shown to work across a variety of tasksachieving remarkable few-shot adaptation. This raises the question: Can clientsuse a single general-purpose model, rather than custom models for each task,while obeying device and network constraints? In this work, we investigatepretrained transformers (PTF) to achieve these on-device learning goals andthoroughly explore the roles of model size and modularity, where the latterrefers to adaptation through modules such as prompts or adapters. Focusing onfederated learning, we demonstrate that: (1) Larger scale shrinks the accuracygaps between alternative approaches and improves heterogeneity robustness.Scale allows clients to run more local SGD epochs which can significantlyreduce the number of communication rounds. At the extreme, clients can achieverespectable accuracy locally highlighting the potential of fully-locallearning. (2) Modularity, by design, enables $\u0026gt;$100$\\times$ less communicationin bits. Surprisingly, it also boosts the generalization capability of localadaptation methods and the robustness of smaller PTFs. Finally, it enablesclients to solve multiple unrelated tasks simultaneously using a single PTF,whereas full updates are prone to catastrophic forgetting. These insights onscale and modularity motivate a new federated learning approach we call \u0026ldquo;YouOnly Load Once\u0026rdquo; (FedYolo): The clients load a full PTF model once and allfuture updates are accomplished through communication-efficient modules withlimited catastrophic-forgetting, where each task is assigned to its own module.\r2023-07-07\nFederated Unlearning via Active Forgetting\nYuyuan Li, Chaochao Chen, Xiaolin Zheng, Jiaming Zhang\nabstract\rabstract: The increasing concerns regarding the privacy of machine learning models havecatalyzed the exploration of machine unlearning, i.e., a process that removesthe influence of training data on machine learning models. This concern alsoarises in the realm of federated learning, prompting researchers to address thefederated unlearning problem. However, federated unlearning remainschallenging. Existing unlearning methods can be broadly categorized into twoapproaches, i.e., exact unlearning and approximate unlearning. Firstly,implementing exact unlearning, which typically relies on thepartition-aggregation framework, in a distributed manner does not improve timeefficiency theoretically. Secondly, existing federated (approximate) unlearningmethods suffer from imprecise data influence estimation, significantcomputational burden, or both. To this end, we propose a novel federatedunlearning framework based on incremental learning, which is independent ofspecific models and federated settings. Our framework differs from existingfederated unlearning methods that rely on approximate retraining or datainfluence estimation. Instead, we leverage new memories to overwrite old ones,imitating the process of \\textit{active forgetting} in neurology. Specifically,the model, intended to unlearn, serves as a student model that continuouslylearns from randomly initiated teacher models. To preserve catastrophicforgetting of non-target data, we utilize elastic weight consolidation toelastically constrain weight change. Extensive experiments on three benchmarkdatasets demonstrate the efficiency and effectiveness of our proposed method.The result of backdoor attacks demonstrates that our proposed method achievessatisfying completeness.\rAdaptive Sparse Gaussian Process\nVanessa Gómez-Verdejo, Emilio Parrado-Hernández, Manel Martínez-Ramón\nabstract\rabstract: Adaptive learning is necessary for non-stationary environments where thelearning machine needs to forget past data distribution. Efficient algorithmsrequire a compact model update to not grow in computational burden with theincoming data and with the lowest possible computational cost for onlineparameter updating. Existing solutions only partially cover these needs. Here,we propose the first adaptive sparse Gaussian Process (GP) able to address allthese issues. We first reformulate a variational sparse GP algorithm to make itadaptive through a forgetting factor. Next, to make the model inference assimple as possible, we propose updating a single inducing point of the sparseGP model together with the remaining model parameters every time a new samplearrives. As a result, the algorithm presents a fast convergence of theinference process, which allows an efficient model update (with a singleinference iteration) even in highly non-stationary environments. Experimentalresults demonstrate the capabilities of the proposed algorithm and its goodperformance in modeling the predictive posterior in mean and confidenceinterval estimation compared to state-of-the-art approaches.\r2023-07-01\nUnlearning Graph Classifiers with Limited Data Resources\nChao Pan, Eli Chien, Olgica Milenkovic\nabstract\rabstract: As the demand for user privacy grows, controlled data removal (machineunlearning) is becoming an important feature of machine learning models fordata-sensitive Web applications such as social networks and recommendersystems. Nevertheless, at this point it is still largely unknown how to performefficient machine unlearning of graph neural networks (GNNs); this isespecially the case when the number of training samples is small, in which caseunlearning can seriously compromise the performance of the model. To addressthis issue, we initiate the study of unlearning the Graph Scattering Transform(GST), a mathematical framework that is efficient, provably stable underfeature or graph topology perturbations, and offers graph classificationperformance comparable to that of GNNs. Our main contribution is the firstknown nonlinear approximate graph unlearning method based on GSTs. Our secondcontribution is a theoretical analysis of the computational complexity of theproposed unlearning mechanism, which is hard to replicate for deep neuralnetworks. Our third contribution are extensive simulation results which showthat, compared to complete retraining of GNNs after each removal request, thenew GST-based approach offers, on average, a 10.38x speed-up and leads to a2.6% increase in test accuracy during unlearning of 90 out of 100 traininggraphs from the IMDB dataset (10% training ratio). Our implementation isavailable online at https://doi.org/10.5281/zenodo.7613150.\rMachine Unlearning of Federated Clusters\nChao Pan, Jin Sima, Saurav Prakash, Vishal Rana, Olgica Milenkovic\nabstract\rabstract: Federated clustering (FC) is an unsupervised learning problem that arises ina number of practical applications, including personalized recommender andhealthcare systems. With the adoption of recent laws ensuring the \u0026ldquo;right to beforgotten\u0026rdquo;, the problem of machine unlearning for FC methods has become ofsignificant importance. We introduce, for the first time, the problem ofmachine unlearning for FC, and propose an efficient unlearning mechanism for acustomized secure FC framework. Our FC framework utilizes specialinitialization procedures that we show are well-suited for unlearning. Toprotect client data privacy, we develop the secure compressed multisetaggregation (SCMA) framework that addresses sparse secure federated learning(FL) problems encountered during clustering as well as more general problems.To simultaneously facilitate low communication complexity and secret sharingprotocols, we integrate Reed-Solomon encoding with special evaluation pointsinto our SCMA pipeline, and prove that the client communication cost islogarithmic in the vector dimension. Additionally, to demonstrate the benefitsof our unlearning mechanism over complete retraining, we provide a theoreticalanalysis for the unlearning performance of our approach. Simulation resultsshow that the new FC framework exhibits superior clustering performancecompared to previously reported FC baselines when the cluster sizes are highlyimbalanced. Compared to completely retraining K-means++ locally and globallyfor each removal request, our unlearning procedure offers an average speed-upof roughly 84x across seven datasets. Our implementation for the proposedmethod is available at https://github.com/thupchnsky/mufc.\r2023-06-27\nNeural Topic Modeling with Continual Lifelong Learning\nPankaj Gupta, Yatin Chaudhary, Thomas Runkler, Hinrich Schütze\nabstract\rabstract: Lifelong learning has recently attracted attention in building machinelearning systems that continually accumulate and transfer knowledge to helpfuture learning. Unsupervised topic modeling has been popularly used todiscover topics from document collections. However, the application of topicmodeling is challenging due to data sparsity, e.g., in a small collection of(short) documents and thus, generate incoherent topics and sub-optimal documentrepresentations. To address the problem, we propose a lifelong learningframework for neural topic modeling that can continuously process streams ofdocument collections, accumulate topics and guide future topic modeling tasksby knowledge transfer from several sources to better deal with the sparse data.In the lifelong process, we particularly investigate jointly: (1) sharinggenerative homologies (latent topics) over lifetime to transfer priorknowledge, and (2) minimizing catastrophic forgetting to retain the pastlearning via novel selective data augmentation, co-training and topicregularization approaches. Given a stream of document collections, we apply theproposed Lifelong Neural Topic Modeling (LNTM) framework in modeling threesparse document collections as future tasks and demonstrate improvedperformance quantified by perplexity, topic coherence and information retrievaltask.\r2023-06-25\nRobust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training\nFan Liu, Weijia Zhang, Hao Liu\nabstract\rabstract: Machine learning-based forecasting models are commonly used in IntelligentTransportation Systems (ITS) to predict traffic patterns and provide city-wideservices. However, most of the existing models are susceptible to adversarialattacks, which can lead to inaccurate predictions and negative consequencessuch as congestion and delays. Therefore, improving the adversarial robustnessof these models is crucial for ITS. In this paper, we propose a novel frameworkfor incorporating adversarial training into spatiotemporal traffic forecastingtasks. We demonstrate that traditional adversarial training methods designatedfor static domains cannot be directly applied to traffic forecasting tasks, asthey fail to effectively defend against dynamic adversarial attacks. Then, wepropose a reinforcement learning-based method to learn the optimal nodeselection strategy for adversarial examples, which simultaneously strengthensthe dynamic attack defense capability and reduces the model overfitting.Additionally, we introduce a self-knowledge distillation regularization moduleto overcome the \u0026ldquo;forgetting issue\u0026rdquo; caused by continuously changing adversarialnodes during training. We evaluate our approach on two real-world trafficdatasets and demonstrate its superiority over other baselines. Our methodeffectively enhances the adversarial robustness of spatiotemporal trafficforecasting models. The source code for our framework is available athttps://github.com/usail-hkust/RDAT.\r2023-06-21\nFederated Self-Learning with Weak Supervision for Speech Recognition\nMilind Rao, Gopinath Chennupati, Gautam Tiwari, Anit Kumar Sahu, Anirudh Raju, Ariya Rastrow, Jasha Droppo\nabstract\rabstract: Automatic speech recognition (ASR) models with low-footprint are increasinglybeing deployed on edge devices for conversational agents, which enhancesprivacy. We study the problem of federated continual incremental learning forrecurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancingscheme of learning on-device, without access to ground truth human transcriptsor machine transcriptions from a stronger ASR model. In particular, we studythe performance of a self-learning based scheme, with a paired teacher modelupdated through an exponential moving average of ASR models. Further, wepropose using possibly noisy weak-supervision signals such as feedback scoresand natural language understanding semantics determined from user behavioracross multiple turns in a session of interactions with the conversationalagent. These signals are leveraged in a multi-task policy-gradient trainingapproach to improve the performance of self-learning for ASR. Finally, we showhow catastrophic forgetting can be mitigated by combining on-device learningwith a memory-replay approach using selected historical datasets. Theseinnovations allow for 10% relative improvement in WER on new use cases withminimal degradation on other test sets in the absence of strong-supervisionsignals such as ground-truth transcriptions.\r2023-06-09\nOne-Shot Machine Unlearning with Mnemonic Code\nTomoya Yamashita, Masanori Yamada, Takashi Shibata\nabstract\rabstract: Deep learning has achieved significant improvements in accuracy and has beenapplied to various fields. With the spread of deep learning, a new problem hasalso emerged; deep learning models can sometimes have undesirable informationfrom an ethical standpoint. This problem must be resolved if deep learning isto make sensitive decisions such as hiring and prison sentencing. Machineunlearning (MU) is the research area that responds to such demands. MU aims atforgetting about undesirable training data from a trained deep learning model.A naive MU approach is to re-train the whole model with the training data fromwhich the undesirable data has been removed. However, re-training the wholemodel can take a huge amount of time and consumes significant computerresources. To make MU even more practical, a simple-yet-effective MU method isrequired. In this paper, we propose a one-shot MU method, which does not needadditional training. To design one-shot MU, we add noise to the modelparameters that are sensitive to undesirable information. In our proposedmethod, we use the Fisher information matrix (FIM) to estimate the sensitivemodel parameters. Training data were usually used to evaluate the FIM inexisting methods. In contrast, we avoid the need to retain the training datafor calculating the FIM by using class-specific synthetic signals calledmnemonic code. Extensive experiments using artificial and natural datasetsdemonstrate that our method outperforms the existing methods.\r2023-06-07\nMeta-Learning in Spiking Neural Networks with Reward-Modulated STDP\nArsham Gholamzadeh Khoee, Alireza Javaheri, Saeed Reza Kheradpisheh, Mohammad Ganjtabesh\nabstract\rabstract: The human brain constantly learns and rapidly adapts to new situations byintegrating acquired knowledge and experiences into memory. Developing thiscapability in machine learning models is considered an important goal of AIresearch since deep neural networks perform poorly when there is limited dataor when they need to adapt quickly to new unseen tasks. Meta-learning modelsare proposed to facilitate quick learning in low-data regimes by employingabsorbed information from the past. Although some models have recently beenintroduced that reached high-performance levels, they are not biologicallyplausible. We have proposed a bio-plausible meta-learning model inspired by thehippocampus and the prefrontal cortex using spiking neural networks with areward-based learning system. Our proposed model includes a memory designed toprevent catastrophic forgetting, a phenomenon that occurs when meta-learningmodels forget what they have learned as soon as the new task begins. Also, ournew model can easily be applied to spike-based neuromorphic devices and enablesfast learning in neuromorphic hardware. The final analysis will discuss theimplications and predictions of the model for solving few-shot classificationtasks. In solving these tasks, our model has demonstrated the ability tocompete with the existing state-of-the-art meta-learning techniques.\r2023-06-06\nMachine Unlearning: A Survey\nHeng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, Philip S. Yu\nabstract\rabstract: Machine learning has attracted widespread attention and evolved into anenabling technology for a wide range of highly successful applications, such asintelligent computer vision, speech recognition, medical diagnosis, and more.Yet a special need has arisen where, due to privacy, usability, and/or theright to be forgotten, information about some specific samples needs to beremoved from a model, called machine unlearning. This emerging technology hasdrawn significant interest from both academics and industry due to itsinnovation and practicality. At the same time, this ambitious problem has ledto numerous research efforts aimed at confronting its challenges. To the bestof our knowledge, no study has analyzed this complex topic or compared thefeasibility of existing unlearning solutions in different kinds of scenarios.Accordingly, with this survey, we aim to capture the key concepts of unlearningtechniques. The existing solutions are classified and summarized based on theircharacteristics within an up-to-date and comprehensive review of eachcategory\u0026rsquo;s advantages and limitations. The survey concludes by highlightingsome of the outstanding issues with unlearning techniques, along with somefeasible directions for new research opportunities.\rTowards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory\nAliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag\nabstract\rabstract: Interactive machine learning (IML) is a beneficial learning paradigm in casesof limited data availability, as human feedback is incrementally integratedinto the training process. In this paper, we present an IML pipeline for imagecaptioning which allows us to incrementally adapt a pre-trained imagecaptioning model to a new data distribution based on user input. In order toincorporate user input into the model, we explore the use of a combination ofsimple data augmentation methods to obtain larger data batches for each newlyannotated data instance and implement continual learning methods to preventcatastrophic forgetting from repeated updates. For our experiments, we split adomain-specific image captioning dataset, namely VizWiz, into non-overlappingparts to simulate an incremental input flow for continually adapting the modelto new data. We find that, while data augmentation worsens results, even whenrelatively small amounts of data are available, episodic memory is an effectivestrategy to retain knowledge from previously seen clusters.\rMasked Autoencoders are Efficient Continual Federated Learners\nSubarnaduti Paul, Lars-Joel Frey, Roshni Kamath, Kristian Kersting, Martin Mundt\nabstract\rabstract: Machine learning is typically framed from a perspective of i.i.d., and moreimportantly, isolated data. In parts, federated learning lifts this assumption,as it sets out to solve the real-world challenge of collaboratively learning ashared model from data distributed across clients. However, motivated primarilyby privacy and computational constraints, the fact that data may change,distributions drift, or even tasks advance individually on clients, is seldomtaken into account. The field of continual learning addresses this separatechallenge and first steps have recently been taken to leverage synergies indistributed supervised settings, in which several clients learn to solvechanging classification tasks over time without forgetting previously seenones. Motivated by these prior works, we posit that such federated continuallearning should be grounded in unsupervised learning of representations thatare shared across clients; in the loose spirit of how humans can indirectlyleverage others\u0026rsquo; experience without exposure to a specific task. For thispurpose, we demonstrate that masked autoencoders for distribution estimationare particularly amenable to this setup. Specifically, their masking strategycan be seamlessly integrated with task attention mechanisms to enable selectiveknowledge transfer between clients. We empirically corroborate the latterstatement through several continual federated scenarios on both image andbinary datasets.\rUnleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability\nJianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, Bo Han\nabstract\rabstract: Out-of-distribution (OOD) detection is an indispensable aspect of secure AIwhen deploying machine learning models in real-world applications. Previousparadigms either explore better scoring functions or utilize the knowledge ofoutliers to equip the models with the ability of OOD detection. However, few ofthem pay attention to the intrinsic OOD detection capability of the givenmodel. In this work, we generally discover the existence of an intermediatestage of a model trained on in-distribution (ID) data having higher OODdetection performance than that of its final stage across different settings,and further identify one critical data-level attribution to be learning withthe atypical samples. Based on such insights, we propose a novel method,Unleashing Mask, which aims to restore the OOD discriminative capabilities ofthe well-trained model with ID data. Our method utilizes a mask to figure outthe memorized atypical samples, and then finetune the model or prune it withthe introduced mask to forget them. Extensive experiments and analysisdemonstrate the effectiveness of our method. The code is available at:https://github.com/tmlr-group/Unleashing-Mask.\r2023-06-04\nLifelong Machine Learning Potentials\nMarco Eckhoff, Markus Reiher\nabstract\rabstract: Machine learning potentials (MLPs) trained on accurate quantum chemical datacan retain the high accuracy, while inflicting little computational demands. Onthe downside, they need to be trained for each individual system. In recentyears, a vast number of MLPs has been trained from scratch because learningadditional data typically requires to train again on all data to not forgetpreviously acquired knowledge. Additionally, most common structural descriptorsof MLPs cannot represent efficiently a large number of different chemicalelements. In this work, we tackle these problems by introducingelement-embracing atom-centered symmetry functions (eeACSFs) which combinestructural properties and element information from the periodic table. TheseeeACSFs are a key for our development of a lifelong machine learning potential(lMLP). Uncertainty quantification can be exploited to transgress a fixed,pre-trained MLP to arrive at a continuously adapting lMLP, because a predefinedlevel of accuracy can be ensured. To extend the applicability of an lMLP to newsystems, we apply continual learning strategies to enable autonomous andon-the-fly training on a continuous stream of new data. For the training ofdeep neural networks, we propose the continual resilient (CoRe) optimizer andincremental learning strategies relying on rehearsal of data, regularization ofparameters, and the architecture of the model.\r2023-06-03\nForgettable Federated Linear Learning with Certified Data Removal\nRuinan Jin, Minghui Chen, Qiong Zhang, Xiaoxiao Li\nabstract\rabstract: Federated learning (FL) is a trending distributed learning framework thatenables collaborative model training without data sharing. Machine learningmodels trained on datasets can potentially expose the private information ofthe training data, revealing details about individual data records. In thisstudy, we focus on the FL paradigm that grants clients the ``right to beforgotten\u0026rsquo;\u0026rsquo;. The forgettable FL framework should bleach its global modelweights as it has never seen that client and hence does not reveal anyinformation about the client. To this end, we propose the Forgettable FederatedLinear Learning (2F2L) framework featured with novel training and data removalstrategies. The training pipeline, named Federated linear training, employslinear approximation on the model parameter space to enable our 2F2L frameworkwork for deep neural networks while achieving comparable results with canonicalneural network training. We also introduce FedRemoval, an efficient andeffective removal strategy that tackles the computational challenges in FL byapproximating the Hessian matrix using public server data from the pretrainedmodel. Unlike the previous uncertified and heuristic machine unlearning methodsin FL, we provide theoretical guarantees by bounding the differences of modelweights by our FedRemoval and that from retraining from scratch. Experimentalresults on MNIST and Fashion-MNIST datasets demonstrate the effectiveness ofour method in achieving a balance between model accuracy and informationremoval, outperforming baseline strategies and approaching retraining fromscratch.\r2023-05-31\nCan Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher\nVikram S Chundawat, Ayush K Tarun, Murari Mandal, Mohan Kankanhalli\nabstract\rabstract: Machine unlearning has become an important area of research due to anincreasing need for machine learning (ML) applications to comply with theemerging data privacy regulations. It facilitates the provision for removal ofcertain set or class of data from an already trained ML model without requiringretraining from scratch. Recently, several efforts have been put in to makeunlearning to be effective and efficient. We propose a novel machine unlearningmethod by exploring the utility of competent and incompetent teachers in astudent-teacher framework to induce forgetfulness. The knowledge from thecompetent and incompetent teachers is selectively transferred to the student toobtain a model that doesn\u0026rsquo;t contain any information about the forget data. Weexperimentally show that this method generalizes well, is fast and effective.Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluateany unlearning method. Unlike the existing unlearning metrics, the ZRF scoredoes not depend on the availability of the expensive retrained model. Thismakes it useful for analysis of the unlearned model after deployment as well.We present results of experiments conducted for random subset forgetting andclass forgetting on various deep networks and across different applicationdomains.~Source code is at:https://github.com/vikram2000b/bad-teaching-unlearning\rDeep Regression Unlearning\nAyush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli\nabstract\rabstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: https://github.com/ayu987/deep-regression-unlearning\rZero-Shot Machine Unlearning\nVikram S Chundawat, Ayush K Tarun, Murari Mandal, Mohan Kankanhalli\nabstract\rabstract: Modern privacy regulations grant citizens the right to be forgotten byproducts, services and companies. In case of machine learning (ML)applications, this necessitates deletion of data not only from storage archivesbut also from ML models. Due to an increasing need for regulatory compliancerequired for ML applications, machine unlearning is becoming an emergingresearch problem. The right to be forgotten requests come in the form ofremoval of a certain set or class of data from the already trained ML model.Practical considerations preclude retraining of the model from scratch afterdiscarding the deleted data. The few existing studies use either the wholetraining data, or a subset of training data, or some metadata stored duringtraining to update the model weights for unlearning. However, in many cases, nodata related to the training process or training samples may be accessible forthe unlearning purpose. We therefore ask the question: is it possible toachieve unlearning with zero training samples? In this paper, we introduce thenovel problem of zero-shot machine unlearning that caters for the extreme butpractical scenario where zero original data samples are available for use. Wethen propose two novel solutions for zero-shot machine unlearning based on (a)error minimizing-maximizing noise and (b) gated knowledge transfer. Thesemethods remove the information of the forget data from the model whilemaintaining the model efficacy on the retain data. The zero-shot approachoffers good protection against the model inversion attacks and membershipinference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN)to effectively measure the quality of the unlearning method. The experimentsshow promising results for unlearning in deep learning models on benchmarkvision data-sets. The source code is available here:https://github.com/ayu987/zero-shot-unlearning\rFast Yet Effective Machine Unlearning\nAyush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli\nabstract\rabstract: Unlearning the data observed during the training of a machine learning (ML)model is an important task that can play a pivotal role in fortifying theprivacy and security of ML-based applications. This paper raises the followingquestions: (i) can we unlearn a single or multiple class(es) of data from a MLmodel without looking at the full training data even once? (ii) can we make theprocess of unlearning fast and scalable to large datasets, and generalize it todifferent deep networks? We introduce a novel machine unlearning framework witherror-maximizing noise generation and impair-repair based weight manipulationthat offers an efficient solution to the above questions. An error-maximizingnoise matrix is learned for the class to be unlearned using the original model.The noise matrix is used to manipulate the model weights to unlearn thetargeted class of data. We introduce impair and repair steps for a controlledmanipulation of the network weights. In the impair step, the noise matrix alongwith a very high learning rate is used to induce sharp unlearning in the model.Thereafter, the repair step is used to regain the overall performance. Withvery few update steps, we show excellent unlearning while substantiallyretaining the overall model accuracy. Unlearning multiple classes requires asimilar number of update steps as for a single class, making our approachscalable to large problems. Our method is quite efficient in comparison to theexisting methods, works for multi-class unlearning, does not put anyconstraints on the original optimization mechanism or network design, and workswell in both small and large-scale vision tasks. This work is an important steptowards fast and easy implementation of unlearning in deep networks. Sourcecode: https://github.com/vikram2000b/Fast-Machine-Unlearning\r2023-05-26\nMitigating Catastrophic Forgetting in Long Short-Term Memory Networks\nKetaki Joshi, Raghavendra Pradyumna Pothukuchi, Andre Wibisono, Abhishek Bhattacharjee\nabstract\rabstract: Continual learning on sequential data is critical for many machine learning(ML) deployments. Unfortunately, LSTM networks, which are commonly used tolearn on sequential data, suffer from catastrophic forgetting and are limitedin their ability to learn multiple tasks continually. We discover thatcatastrophic forgetting in LSTM networks can be overcome in two novel andreadily-implementable ways \u0026ndash; separating the LSTM memory either for each taskor for each target label. Our approach eschews the need for explicitregularization, hypernetworks, and other complex methods. We quantify thebenefits of our approach on recently-proposed LSTM networks for computer memoryaccess prefetching, an important sequential learning problem in ML-basedcomputer system optimization. Compared to state-of-the-art weightregularization methods to mitigate catastrophic forgetting, our approach issimple, effective, and enables faster learning. We also show that our proposalenables the use of small, non-regularized LSTM networks for complex naturallanguage processing in the offline learning scenario, which was previouslyconsidered difficult.\r2023-05-25\nSketchOGD: Memory-Efficient Continual Learning\nBenjamin Wright, Youngjae Min, Jeremy Bernstein, Navid Azizan\nabstract\rabstract: When machine learning models are trained continually on a sequence of tasks,they are liable to forget what they learned on previous tasks \u0026ndash; a phenomenonknown as catastrophic forgetting. Proposed solutions to catastrophic forgettingtend to involve storing information about past tasks, meaning that memory usageis a chief consideration in determining their practicality. This paper proposesa memory-efficient solution to catastrophic forgetting, improving upon anestablished algorithm known as orthogonal gradient descent (OGD). OGD utilizesprior model gradients to find weight updates that preserve performance on priordatapoints. However, since the memory cost of storing prior model gradientsgrows with the runtime of the algorithm, OGD is ill-suited to continuallearning over arbitrarily long time horizons. To address this problem, thispaper proposes SketchOGD. SketchOGD employs an online sketching algorithm tocompress model gradients as they are encountered into a matrix of a fixed,user-determined size. In contrast to existing memory-efficient variants of OGD,SketchOGD runs online without the need for advance knowledge of the totalnumber of tasks, is simple to implement, and is more amenable to analysis. Weprovide theoretical guarantees on the approximation error of the relevantsketches under a novel metric suited to the downstream task of OGD.Experimentally, we find that SketchOGD tends to outperform currentstate-of-the-art variants of OGD given a fixed memory budget.\r2023-05-24\nMachine Unlearning: its nature, scope, and importance for a \u0026ldquo;delete culture\u0026rdquo;\nLuciano Floridi\nabstract\rabstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them \u0026lsquo;forget\u0026rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.\r2023-05-21\nRandom Relabeling for Efficient Machine Unlearning\nJunde Li, Swaroop Ghosh\nabstract\rabstract: Learning algorithms and data are the driving forces for machine learning tobring about tremendous transformation of industrial intelligence. However,individuals\u0026rsquo; right to retract their personal data and relevant data privacyregulations pose great challenges to machine learning: how to design anefficient mechanism to support certified data removals. Removal of previouslyseen data known as machine unlearning is challenging as these data points wereimplicitly memorized in training process of learning algorithms. Retrainingremaining data from scratch straightforwardly serves such deletion requests,however, this naive method is not often computationally feasible. We proposethe unlearning scheme random relabeling, which is applicable to genericsupervised learning algorithms, to efficiently deal with sequential dataremoval requests in the online setting. A less constraining removalcertification method based on probability distribution similarity with naiveunlearning is further developed for logit-based classifiers.\r2023-05-18\nTowards Generalizable Data Protection With Transferable Unlearnable Examples\nBin Fang, Bo Li, Shuang Wu, Tianyi Zheng, Shouhong Ding, Ran Yi, Lizhuang Ma\nabstract\rabstract: Artificial Intelligence (AI) is making a profound impact in almost everydomain. One of the crucial factors contributing to this success has been theaccess to an abundance of high-quality data for constructing machine learningmodels. Lately, as the role of data in artificial intelligence has beensignificantly magnified, concerns have arisen regarding the secure utilizationof data, particularly in the context of unauthorized data usage. To mitigatedata exploitation, data unlearning have been introduced to render dataunexploitable. However, current unlearnable examples lack the generalizationrequired for wide applicability. In this paper, we present a novel,generalizable data protection method by generating transferable unlearnableexamples. To the best of our knowledge, this is the first solution thatexamines data privacy from the perspective of data distribution. Throughextensive experimentation, we substantiate the enhanced generalizableprotection capabilities of our proposed method.\rRe-thinking Data Availablity Attacks Against Deep Neural Networks\nBin Fang, Bo Li, Shuang Wu, Ran Yi, Shouhong Ding, Lizhuang Ma\nabstract\rabstract: The unauthorized use of personal data for commercial purposes and theclandestine acquisition of private data for training machine learning modelscontinue to raise concerns. In response to these issues, researchers haveproposed availability attacks that aim to render data unexploitable. However,many current attack methods are rendered ineffective by adversarial training.In this paper, we re-examine the concept of unlearnable examples and discernthat the existing robust error-minimizing noise presents an inaccurateoptimization objective. Building on these observations, we introduce a noveloptimization paradigm that yields improved protection results with reducedcomputational time requirements. We have conducted extensive experiments tosubstantiate the soundness of our approach. Moreover, our method establishes arobust foundation for future research in this area.\r2023-05-16\nCQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning\nSanyam Jain\nabstract\rabstract: Training machine learning models in an incremental fashion is not onlyimportant but also an efficient way to achieve artificial general intelligence.The ability that humans possess of continuous or lifelong learning helps themto not forget previously learned tasks. However, current neural network modelsare prone to catastrophic forgetting when it comes to continual learning. Manyresearchers have come up with several techniques in order to reduce the effectof forgetting from neural networks, however, all techniques are studiedclassically with a very less focus on changing the machine learning modelarchitecture. In this research paper, we show that it is not only possible tocircumvent catastrophic forgetting in continual learning with novel hybridclassical-quantum neural networks, but also explains what features are mostimportant to learn for classification. In addition, we also claim that if themodel is trained with these explanations, it tends to give better performanceand learn specific features that are far from the decision boundary. Finally,we present the experimental results to show comparisons between classical andclassical-quantum hybrid architectures on benchmark MNIST and CIFAR-10datasets. After successful runs of learning procedure, we found hybrid neuralnetwork outperforms classical one in terms of remembering the right evidencesof the class-specific features.\rContinually Learned Pavlovian Signalling Without Forgetting for Human-in-the-Loop Robotic Control\nAdam S. R. Parker, Michael R. Dawson, Patrick M. Pilarski\nabstract\rabstract: Artificial limbs are sophisticated devices to assist people with tasks ofdaily living. Despite advanced robotic prostheses demonstrating similar motioncapabilities to biological limbs, users report them difficult and non-intuitiveto use. Providing more effective feedback from the device to the user hastherefore become a topic of increased interest. In particular, predictionlearning methods from the field of reinforcement learning \u0026ndash; specifically, anapproach termed Pavlovian signalling \u0026ndash; have been proposed as one approach forbetter modulating feedback in prostheses since they can adapt during continuoususe. One challenge identified in these learning methods is that they can forgetpreviously learned predictions when a user begins to successfully act upondelivered feedback. The present work directly addresses this challenge,contributing new evidence on the impact of algorithmic choices, such as on- oroff-policy methods and representation choices, on the Pavlovian signalling froma machine to a user during their control of a robotic arm. Two conditions ofalgorithmic differences were studied using different scenarios of controlling arobotic arm: an automated motion system and human participant piloting.Contrary to expectations, off-policy learning did not provide the expectedsolution to the forgetting problem. We instead identified beneficial propertiesof a look-ahead state representation that made existing approaches able tolearn (and not forget) predictions in support of Pavlovian signalling. Thiswork therefore contributes new insight into the challenges of providing learnedpredictive feedback from a prosthetic device, and demonstrates avenues for moredynamic signalling in future human-machine interactions.\r2023-05-11\nKGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment\nLingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, Hongzhi Yin\nabstract\rabstract: Recent legislation of the \u0026ldquo;right to be forgotten\u0026rdquo; has led to the interest inmachine unlearning, where the learned models are endowed with the function toforget information about specific training instances as if they have neverexisted in the training set. Previous work mainly focuses on computer visionscenarios and largely ignores the essentials of unlearning in NLP field, wheretext data contains more explicit and sensitive personal information thanimages. In this paper, we propose a general unlearning framework called KGA toinduce forgetfulness. Different from previous work that tries to recovergradients or forces models to perform close to one specific distribution, KGAmaintains distribution differences (i.e., knowledge gap). This relaxes thedistribution assumption. Furthermore, we first apply the unlearning method tovarious NLP tasks (i.e., classification, translation, response generation) andpropose several unlearning evaluation metrics with pertinence. Experiments onlarge-scale datasets show that KGA yields comprehensive improvements overbaselines, where extensive analyses further validate the effectiveness of KGAand provide insight into unlearning for NLP tasks.\rContinual Learning of Natural Language Processing Tasks: A Survey\nZixuan Ke, Bing Liu\nabstract\rabstract: Continual learning (CL) is a learning paradigm that emulates the humancapability of learning and accumulating knowledge continually withoutforgetting the previously learned knowledge and also transferring the learnedknowledge to help learn new tasks better. This survey presents a comprehensivereview and analysis of the recent progress of CL in NLP, which has significantdifferences from CL in computer vision and machine learning. It covers (1) allCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting(CF) prevention, (3) knowledge transfer (KT), which is particularly importantfor NLP tasks; and (4) some theory and the hidden challenge of inter-task classseparation (ICS). (1), (3) and (4) have not been included in the existingsurvey. Finally, a list of future directions is discussed.\r2023-05-10\nContinual Facial Expression Recognition: A Benchmark\nNikhil Churamani, Tolga Dimlioglu, German I. Parisi, Hatice Gunes\nabstract\rabstract: Understanding human affective behaviour, especially in the dynamics ofreal-world settings, requires Facial Expression Recognition (FER) models tocontinuously adapt to individual differences in user expression, contextualattributions, and the environment. Current (deep) Machine Learning (ML)-basedFER approaches pre-trained in isolation on benchmark datasets fail to capturethe nuances of real-world interactions where data is available onlyincrementally, acquired by the agent or robot during interactions. New learningcomes at the cost of previous knowledge, resulting in catastrophic forgetting.Lifelong or Continual Learning (CL), on the other hand, enables adaptability inagents by being sensitive to changing data distributions, integrating newinformation without interfering with previously learnt knowledge. Positing CLas an effective learning paradigm for FER, this work presents the ContinualFacial Expression Recognition (ConFER) benchmark that evaluates popular CLtechniques on FER tasks. It presents a comparative analysis of several CL-basedapproaches on popular FER datasets such as CK+, RAF-DB, and AffectNet andpresent strategies for a successful implementation of ConFER for AffectiveComputing (AC) research. CL techniques, under different learning settings, areshown to achieve state-of-the-art (SOTA) performance across several datasets,thus motivating a discussion on the benefits of applying CL principles towardshuman behaviour understanding, particularly from facial expressions, as wellthe challenges entailed.\r2023-05-09\nMeasuring Forgetting of Memorized Training Examples\nMatthew Jagielski, Om Thakkar, Florian Tramèr, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang\nabstract\rabstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models \u0026ldquo;forget\u0026rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.\r2023-04-21\nCognitively Inspired Learning of Incremental Drifting Concepts\nMohammad Rostami, Aram Galstyan\nabstract\rabstract: Humans continually expand their learned knowledge to new domains and learnnew concepts without any interference with past learned experiences. Incontrast, machine learning models perform poorly in a continual learningsetting, where input data distribution changes over time. Inspired by thenervous system learning mechanisms, we develop a computational model thatenables a deep neural network to learn new concepts and expand its learnedknowledge to new domains incrementally in a continual learning setting. We relyon the Parallel Distributed Processing theory to encode abstract concepts in anembedding space in terms of a multimodal distribution. This embedding space ismodeled by internal data representations in a hidden network layer. We alsoleverage the Complementary Learning Systems theory to equip the model with amemory mechanism to overcome catastrophic forgetting through implementingpseudo-rehearsal. Our model can generate pseudo-data points for experiencereplay and accumulate new experiences to past learned experiences withoutcausing cross-task interference.\rSequeL: A Continual Learning Library in PyTorch and JAX\nNikolaos Dimitriadis, Francois Fleuret, Pascal Frossard\nabstract\rabstract: Continual Learning is an important and challenging problem in machinelearning, where models must adapt to a continuous stream of new data withoutforgetting previously acquired knowledge. While existing frameworks are builton PyTorch, the rising popularity of JAX might lead to divergent codebases,ultimately hindering reproducibility and progress. To address this problem, weintroduce SequeL, a flexible and extensible library for Continual Learning thatsupports both PyTorch and JAX frameworks. SequeL provides a unified interfacefor a wide range of Continual Learning algorithms, includingregularization-based approaches, replay-based approaches, and hybridapproaches. The library is designed towards modularity and simplicity, makingthe API suitable for both researchers and practitioners. We releaseSequeL\\footnote{\\url{https://github.com/nik-dim/sequel}} as an open-sourcelibrary, enabling researchers and developers to easily experiment and extendthe library for their own purposes.\r2023-04-20\nSelective and Collaborative Influence Function for Efficient Recommendation Unlearning\nYuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang\nabstract\rabstract: Recent regulations on the Right to be Forgotten have greatly influenced theway of running a recommender system, because users now have the right towithdraw their private data. Besides simply deleting the target data in thedatabase, unlearning the associated data lineage e.g., the learned personalfeatures and preferences in the model, is also necessary for data withdrawal.Existing unlearning methods are mainly devised for generalized machine learningmodels in classification tasks. In this paper, we first identify two maindisadvantages of directly applying existing unlearning methods in the contextof recommendation, i.e., (i) unsatisfactory efficiency for large-scalerecommendation models and (ii) destruction of collaboration across users anditems. To tackle the above issues, we propose an extra-efficient recommendationunlearning method based on Selective and Collaborative Influence Function(SCIF). Our proposed method can (i) avoid any kind of retraining which iscomputationally prohibitive for large-scale systems, (ii) further enhanceefficiency by selectively updating user embedding and (iii) preserve thecollaboration across the remaining users and items. Furthermore, in order toevaluate the unlearning completeness, we define a Membership Inference Oracle(MIO), which can justify whether the unlearned data points were in the trainingset of the model, i.e., whether a data point was completely unlearned.Extensive experiments on two benchmark datasets demonstrate that our proposedmethod can not only greatly enhance unlearning efficiency, but also achieveadequate unlearning completeness. More importantly, our proposed methodoutperforms the state-of-the-art unlearning method regarding comprehensiverecommendation metrics.\rGet Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning\nManaar Alam, Hithem Lamri, Michail Maniatakos\nabstract\rabstract: Federated Learning (FL) enables collaborative deep learning training acrossmultiple participants without exposing sensitive personal data. However, thedistributed nature of FL and the unvetted participants\u0026rsquo; data makes itvulnerable to backdoor attacks. In these attacks, adversaries inject maliciousfunctionality into the centralized model during training, leading tointentional misclassifications for specific adversary-chosen inputs. Whileprevious research has demonstrated successful injections of persistentbackdoors in FL, the persistence also poses a challenge, as their existence inthe centralized model can prompt the central aggregation server to takepreventive measures to penalize the adversaries. Therefore, this paper proposesa methodology that enables adversaries to effectively remove backdoors from thecentralized model upon achieving their objectives or upon suspicion of possibledetection. The proposed approach extends the concept of machine unlearning andpresents strategies to preserve the performance of the centralized model andsimultaneously prevent over-unlearning of information unrelated to backdoorpatterns, making the adversaries stealthy while removing backdoors. To the bestof our knowledge, this is the first work that explores machine unlearning in FLto remove backdoors to the benefit of adversaries. Exhaustive evaluationconsidering image classification scenarios demonstrates the efficacy of theproposed method in efficient backdoor removal from the centralized model,injected by state-of-the-art attacks across multiple configurations.\r2023-04-07\nInductive Graph Unlearning\nCheng-Long Wang, Mengdi Huai, Di Wang\nabstract\rabstract: As a way to implement the \u0026ldquo;right to be forgotten\u0026rdquo; in machine learning,\\textit{machine unlearning} aims to completely remove the contributions andinformation of the samples to be deleted from a trained model without affectingthe contributions of other samples. Recently, many frameworks for machineunlearning have been proposed, and most of them focus on image and text data.To extend machine unlearning to graph data, \\textit{GraphEraser} has beenproposed. However, a critical issue is that \\textit{GraphEraser} isspecifically designed for the transductive graph setting, where the graph isstatic and attributes and edges of test nodes are visible during training. Itis unsuitable for the inductive setting, where the graph could be dynamic andthe test graph information is invisible in advance. Such inductive capabilityis essential for production machine learning systems with evolving graphs likesocial media and transaction networks. To fill this gap, we propose the\\underline{{\\bf G}}\\underline{{\\bf U}}ided \\underline{{\\bf I}}n\\underline{{\\bfD}}uctiv\\underline{{\\bf E}} Graph Unlearning framework (GUIDE). GUIDE consistsof three components: guided graph partitioning with fairness and balance,efficient subgraph repair, and similarity-based aggregation. Empirically, weevaluate our method on several inductive benchmarks and evolving transactiongraphs. Generally speaking, GUIDE can be efficiently implemented on theinductive graph learning tasks for its low graph partition cost, no matter oncomputation or structure information. The code will be available here:https://github.com/Happy2Git/GUIDE.\r2023-04-06\nGIF: A General Graph Unlearning Strategy via Influence Function\nJiancan Wu, Yi Yang, Yuchun Qian, Yongduo Sui, Xiang Wang, Xiangnan He\nabstract\rabstract: With the greater emphasis on privacy and security in our society, the problemof graph unlearning \u0026ndash; revoking the influence of specific data on the trainedGNN model, is drawing increasing attention. However, ranging from machineunlearning to recently emerged graph unlearning methods, existing effortseither resort to retraining paradigm, or perform approximate erasure that failsto consider the inter-dependency between connected neighbors or imposesconstraints on GNN structure, therefore hard to achieve satisfyingperformance-complexity trade-offs. In this work, we explore the influence function tailored for graphunlearning, so as to improve the unlearning efficacy and efficiency for graphunlearning. We first present a unified problem formulation of diverse graphunlearning tasks \\wrt node, edge, and feature. Then, we recognize the crux tothe inability of traditional influence function for graph unlearning, anddevise Graph Influence Function (GIF), a model-agnostic unlearning method thatcan efficiently and accurately estimate parameter changes in response to a$\\epsilon$-mass perturbation in deleted data. The idea is to supplement theobjective of the traditional influence function with an additional loss term ofthe influenced neighbors due to the structural dependency. Further deductionson the closed-form solution of parameter changes provide a better understandingof the unlearning mechanism. We conduct extensive experiments on fourrepresentative GNN models and three benchmark datasets to justify thesuperiority of GIF for diverse graph unlearning tasks in terms of unlearningefficacy, model utility, and unlearning efficiency. Our implementations areavailable at \\url{https://github.com/wujcan/GIF-torch/}.\rUnfolded Self-Reconstruction LSH: Towards Machine Unlearning in Approximate Nearest Neighbour Search\nKim Yong Tan, Yueming Lyu, Yew Soon Ong, Ivor W. Tsang\nabstract\rabstract: Approximate nearest neighbour (ANN) search is an essential component ofsearch engines, recommendation systems, etc. Many recent works focus onlearning-based data-distribution-dependent hashing and achieve good retrievalperformance. However, due to increasing demand for users\u0026rsquo; privacy and security,we often need to remove users\u0026rsquo; data information from Machine Learning (ML)models to satisfy specific privacy and security requirements. This needrequires the ANN search algorithm to support fast online data deletion andinsertion. Current learning-based hashing methods need retraining the hashfunction, which is prohibitable due to the vast time-cost of large-scale data.To address this problem, we propose a novel data-dependent hashing method namedunfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSHunfolded the optimization update for instance-wise data reconstruction, whichis better for preserving data information than data-independent LSH. Moreover,our USR-LSH supports fast online data deletion and insertion withoutretraining. To the best of our knowledge, we are the first to address themachine unlearning of retrieval problems. Empirically, we demonstrate thatUSR-LSH outperforms the state-of-the-art data-distribution-independent LSH inANN tasks in terms of precision and recall. We also show that USR-LSH hassignificantly faster data deletion and insertion time than learning-baseddata-dependent hashing.\r2023-04-04\nMulti-Class Explainable Unlearning for Image Classification via Weight Filtering\nSamuele Poppi, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara\nabstract\rabstract: Machine Unlearning has recently been emerging as a paradigm for selectivelyremoving the impact of training datapoints from a network. While existingapproaches have focused on unlearning either a small subset of the trainingdata or a single class, in this paper we take a different path and devise aframework that can unlearn all classes of an image classification network in asingle untraining round. Our proposed technique learns to modulate the innercomponents of an image classification network through memory matrices so that,after training, the same network can selectively exhibit an unlearning behaviorover any of the classes. By discovering weights which are specific to each ofthe classes, our approach also recovers a representation of the classes whichis explainable by-design. We test the proposed framework, which we name WeightFiltering network (WF-Net), on small-scale and medium-scale imageclassification datasets, with both CNN and Transformer-based backbones. Ourwork provides interesting insights in the development of explainable solutionsfor unlearning and could be easily extended to other vision tasks.\rPIVOT: Prompting for Video Continual Learning\nAndrés Villa, Juan León Alcázar, Motasem Alfarra, Kumail Alhamoud, Julio Hurtado, Fabian Caba Heilbron, Alvaro Soto, Bernard Ghanem\nabstract\rabstract: Modern machine learning pipelines are limited due to data availability,storage quotas, privacy regulations, and expensive annotation processes. Theseconstraints make it difficult or impossible to train and update large-scalemodels on such dynamic annotated sets. Continual learning directly approachesthis problem, with the ultimate goal of devising methods where a deep neuralnetwork effectively learns relevant patterns for new (unseen) classes, withoutsignificantly altering its performance on previously learned ones. In thispaper, we address the problem of continual learning for video data. Weintroduce PIVOT, a novel method that leverages extensive knowledge inpre-trained models from the image domain, thereby reducing the number oftrainable parameters and the associated forgetting. Unlike previous methods,ours is the first approach that effectively uses prompting mechanisms forcontinual learning without any in-domain pre-training. Our experiments showthat PIVOT improves state-of-the-art methods by a significant 27% on the20-task ActivityNet setup.\r2023-04-03\nA Closer Look at Rehearsal-Free Continual Learning\nJames Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, Zsolt Kira\nabstract\rabstract: Continual learning is a setting where machine learning models learn novelconcepts from continuously shifting training data, while simultaneouslyavoiding degradation of knowledge on previously seen classes which maydisappear from the training data for extended periods of time (a phenomenonknown as the catastrophic forgetting problem). Current approaches for continuallearning of a single expanding task (aka class-incremental continual learning)require extensive rehearsal of previously seen data to avoid this degradationof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it mayalso violate data-privacy. Instead, we explore combining knowledge distillationand parameter regularization in new ways to achieve strong continual learningperformance without rehearsal. Specifically, we take a deep dive into commoncontinual learning techniques: prediction distillation, feature distillation,L2 parameter regularization, and EWC parameter regularization. We firstdisprove the common assumption that parameter regularization techniques failfor rehearsal-free continual learning of a single, expanding task. Next, weexplore how to leverage knowledge from a pre-trained model in rehearsal-freecontinual learning and find that vanilla L2 parameter regularizationoutperforms EWC parameter regularization and feature distillation. Finally, weexplore the recently popular ImageNet-R benchmark, and show that L2 parameterregularization implemented in self-attention blocks of a ViT transformeroutperforms recent popular prompting for continual learning methods.\r2023-03-25\nTask-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation\nYuliang Cai, Jesse Thomason, Mohammad Rostami\nabstract\rabstract: The size and the computational load of fine-tuning large-scale pre-trainedneural network are becoming two major obstacles in adopting machine learning inmany applications. Continual learning (CL) can serve as a remedy throughenabling knowledge-transfer across sequentially arriving tasks which relaxesthe need to fine-tune all network weights from scratch. However, existing CLalgorithms primarily consider learning unimodal vision-only or language-onlytasks. We develop a transformer-based CL architecture for learning bimodalvision-and-language tasks based on increasing the number of the learnableparameters dynamically and using knowledge distillation. The new additionalparameters are used to specialize the network for each task. Our approachenables sharing information between the tasks while addressing the challenge ofcatastrophic forgetting. Our approach is scalable learning to a large number oftasks because it requires little memory and time overhead. Our model reachesstate-of-the-art performance on challenging vision-and-language tasks.\r2023-03-24\nTwo-level Graph Network for Few-Shot Class-Incremental Learning\nHao Chen, Linyan Li, Fan Lyu, Fuyuan Hu, Zhenping Xia, Fenglei Xu\nabstract\rabstract: Few-shot class-incremental learning (FSCIL) aims to design machine learningalgorithms that can continually learn new concepts from a few data points,without forgetting knowledge of old classes. The difficulty lies in thatlimited data from new classes not only lead to significant overfitting issuesbut also exacerbates the notorious catastrophic forgetting problems. However,existing FSCIL methods ignore the semantic relationships between sample-leveland class-level. % Using the advantage that graph neural network (GNN) can minerich information among few samples, In this paper, we designed a two-levelgraph network for FSCIL named Sample-level and Class-level Graph Neural Network(SCGN). Specifically, a pseudo incremental learning paradigm is designed inSCGN, which synthesizes virtual few-shot tasks as new tasks to optimize SCGNmodel parameters in advance. Sample-level graph network uses the relationshipof a few samples to aggregate similar samples and obtains refined class-levelfeatures. Class-level graph network aims to mitigate the semantic conflictbetween prototype features of new classes and old classes. SCGN buildstwo-level graph networks to guarantee the latent semantic of each few-shotclass can be effectively represented in FSCIL. Experiments on three popularbenchmark datasets show that our method significantly outperforms the baselinesand sets new state-of-the-art results with remarkable advantages.\r2023-03-23\nUnlearnable Clusters: Towards Label-agnostic Unlearnable Examples\nJiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yu-Gang Jiang, Yaowei Wang, Changsheng Xu\nabstract\rabstract: There is a growing interest in developing unlearnable examples (UEs) againstvisual privacy leaks on the Internet. UEs are training samples added withinvisible but unlearnable noise, which have been found can prevent unauthorizedtraining of machine learning models. UEs typically are generated via a bileveloptimization framework with a surrogate model to remove (minimize) errors fromthe original samples, and then applied to protect the data against unknowntarget models. However, existing UE generation methods all rely on an idealassumption called label-consistency, where the hackers and protectors areassumed to hold the same label for a given sample. In this work, we propose andpromote a more practical label-agnostic setting, where the hackers may exploitthe protected data quite differently from the protectors. E.g., a m-classunlearnable dataset held by the protector may be exploited by the hacker as an-class dataset. Existing UE generation methods are rendered ineffective inthis challenging setting. To tackle this challenge, we present a noveltechnique called Unlearnable Clusters (UCs) to generate label-agnosticunlearnable examples with cluster-wise perturbations. Furthermore, we proposeto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as thesurrogate model to improve the transferability of the crafted UCs to diversedomains. We empirically verify the effectiveness of our proposed approach undera variety of settings with different datasets, target models, and evencommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is availableat \\url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.\r2023-03-22\nAUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection\nPuning Yang, Jian Liang, Jie Cao, Ran He\nabstract\rabstract: Out-of-distribution (OOD) detection is a crucial aspect of deploying machinelearning models in open-world applications. Empirical evidence suggests thattraining with auxiliary outliers substantially improves OOD detection. However,such outliers typically exhibit a distribution gap compared to the test OODdata and do not cover all possible test OOD scenarios. Additionally,incorporating these outliers introduces additional training burdens. In thispaper, we introduce a novel paradigm called test-time OOD detection, whichutilizes unlabeled online data directly at test time to improve OOD detectionperformance. While this paradigm is efficient, it also presents challenges suchas catastrophic forgetting. To address these challenges, we propose adaptiveoutlier optimization (AUTO), which consists of an in-out-aware filter, an IDmemory bank, and a semantically-consistent objective. AUTO adaptively minespseudo-ID and pseudo-OOD samples from test data, utilizing them to optimizenetworks in real time during inference. Extensive results on CIFAR-10,CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhancesOOD detection performance.\r2023-03-21\nBoundary Unlearning\nMin Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, Chen Wang\nabstract\rabstract: The practical needs of the ``right to be forgotten\u0026rsquo;\u0026rsquo; and poisoned dataremoval call for efficient \\textit{machine unlearning} techniques, which enablemachine learning models to unlearn, or to forget a fraction of training dataand its lineage. Recent studies on machine unlearning for deep neural networks(DNNs) attempt to destroy the influence of the forgetting data by scrubbing themodel parameters. However, it is prohibitively expensive due to the largedimension of the parameter space. In this paper, we refocus our attention fromthe parameter space to the decision space of the DNN model, and proposeBoundary Unlearning, a rapid yet effective way to unlearn an entire class froma trained DNN model. The key idea is to shift the decision boundary of theoriginal DNN model to imitate the decision behavior of the model retrained fromscratch. We develop two novel boundary shift methods, namely Boundary Shrinkand Boundary Expanding, both of which can rapidly achieve the utility andprivacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 andVggface2 datasets, and the results show that Boundary Unlearning caneffectively forget the forgetting class on image classification and facerecognition tasks, with an expected speed-up of $17\\times$ and $19\\times$,respectively, compared with retraining from the scratch.\r2023-03-20\nVerifiable and Provably Secure Machine Unlearning\nThorsten Eisenhofer, Doreen Riepel, Varun Chandrasekaran, Esha Ghosh, Olga Ohrimenko, Nicolas Papernot\nabstract\rabstract: Machine unlearning aims to remove points from the training dataset of amachine learning model after training; for example when a user requests theirdata to be deleted. While many machine unlearning methods have been proposed,none of them enable users to audit the procedure. Furthermore, recent workshows a user is unable to verify if their data was unlearnt from an inspectionof the model alone. Rather than reasoning about model parameters, we propose toview verifiable unlearning as a security problem. To this end, we present thefirst cryptographic definition of verifiable unlearning to formally capture theguarantees of a machine unlearning system. In this framework, the server firstcomputes a proof that the model was trained on a dataset $D$. Given a user datapoint $d$ requested to be deleted, the server updates the model using anunlearning algorithm. It then provides a proof of the correct execution ofunlearning and that $d \\notin D\u0026rsquo;$, where $D\u0026rsquo;$ is the new training dataset. Ourframework is generally applicable to different unlearning techniques that weabstract as admissible functions. We instantiate the framework, based oncryptographic assumptions, using SNARKs and hash chains. Finally, we implementthe protocol for three different unlearning techniques (retraining-based,amnesiac, and optimization-based) to validate its feasibility for linearregression, logistic regression, and neural networks.\r2023-03-16\nFrom MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning\nKamil Faber, Dominik Zurek, Marcin Pietron, Nathalie Japkowicz, Antonio Vergari, Roberto Corizzo\nabstract\rabstract: Continual learning (CL) is one of the most promising trends in recent machinelearning research. Its goal is to go beyond classical assumptions in machinelearning and develop models and learning strategies that present highrobustness in dynamic environments. The landscape of CL research is fragmentedinto several learning evaluation protocols, comprising different learningtasks, datasets, and evaluation metrics. Additionally, the benchmarks adoptedso far are still distant from the complexity of real-world scenarios, and areusually tailored to highlight capabilities specific to certain strategies. Insuch a landscape, it is hard to objectively assess strategies. In this work, wefill this gap for CL on image data by introducing two novel CL benchmarks thatinvolve multiple heterogeneous tasks from six image datasets, with varyinglevels of complexity and quality. Our aim is to fairly evaluate currentstate-of-the-art CL strategies on a common ground that is closer to complexreal-world scenarios. We additionally structure our benchmarks so that tasksare presented in increasing and decreasing order of complexity \u0026ndash; according toa curriculum \u0026ndash; in order to evaluate if current CL models are able to exploitstructure across tasks. We devote particular emphasis to providing the CLcommunity with a rigorous and reproducible evaluation protocol for measuringthe ability of a model to generalize and not to forget while learning.Furthermore, we provide an extensive experimental evaluation showing thatpopular CL strategies, when challenged with our benchmarks, yield sub-parperformance, high levels of forgetting, and present a limited ability toeffectively leverage curriculum task ordering. We believe that these resultshighlight the need for rigorous comparisons in future CL works as well as pavethe way to design new CL strategies that are able to deal with more complexscenarios.\r2023-03-15\nGradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting\nKangyang Luo, Xiang Li, Yunshi Lan, Ming Gao\nabstract\rabstract: Federated Learning (FL) has emerged as a de facto machine learning area andreceived rapid increasing research interests from the community. However,catastrophic forgetting caused by data heterogeneity and partial participationposes distinctive challenges for FL, which are detrimental to the performance.To tackle the problems, we propose a new FL approach (namely GradMA), whichtakes inspiration from continual learning to simultaneously correct theserver-side and worker-side update directions as well as take full advantage ofserver\u0026rsquo;s rich computing and memory resources. Furthermore, we elaborate amemory reduction strategy to enable GradMA to accommodate FL with a large scaleof workers. We then analyze convergence of GradMA theoretically under thesmooth non-convex setting and show that its convergence rate achieves a linearspeed up w.r.t the increasing number of sampled active workers. At last, ourextensive experiments on various image classification tasks show that GradMAachieves significant performance gains in accuracy and communication efficiencycompared to SOTA baselines.\r2023-03-14\nFew-Shot Unlearning by Model Inversion\nYoungsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, Jungseul Ok\nabstract\rabstract: We consider a practical scenario of machine unlearning to erase a targetdataset, which causes unexpected behavior from the trained model. The targetdataset is often assumed to be fully identifiable in a standard unlearningscenario. Such a flawless identification, however, is almost impossible if thetraining dataset is inaccessible at the time of unlearning. Unlike previousapproaches requiring a complete set of targets, we consider few-shot unlearningscenario when only a few samples of target data are available. To this end, weformulate the few-shot unlearning problem specifying intentions behind theunlearning request (e.g., purely unlearning, mislabel correction, privacyprotection), and we devise a straightforward framework that (i) retrieves aproxy of the training data via model inversion fully exploiting informationavailable in the context of unlearning; (ii) adjusts the proxy according to theunlearning intention; and (iii) updates the model with the adjusted proxy. Wedemonstrate that our method using only a subset of target data can outperformthe state-of-the-art unlearning methods even with a complete indication oftarget data.\r2023-03-12\nInformative regularization for a multi-layer perceptron RR Lyrae classifier under data shift\nFrancisco Pérez-Galarce, Karim Pichara, Pablo Huijse, Márcio Catelan, Domingo Mery\nabstract\rabstract: In recent decades, machine learning has provided valuable models andalgorithms for processing and extracting knowledge from time-series surveys.Different classifiers have been proposed and performed to an excellentstandard. Nevertheless, few papers have tackled the data shift problem inlabeled training sets, which occurs when there is a mismatch between the datadistribution in the training set and the testing set. This drawback can damagethe prediction performance in unseen data. Consequently, we propose a scalableand easily adaptable approach based on an informative regularization and anad-hoc training procedure to mitigate the shift problem during the training ofa multi-layer perceptron for RR Lyrae classification. We collect ranges forcharacteristic features to construct a symbolic representation of priorknowledge, which was used to model the informative regularizer component.Simultaneously, we design a two-step back-propagation algorithm to integratethis knowledge into the neural network, whereby one step is applied in eachepoch to minimize classification error, while another is applied to ensureregularization. Our algorithm defines a subset of parameters (a mask) for eachloss function. This approach handles the forgetting effect, which stems from atrade-off between these loss functions (learning from data versus learningexpert knowledge) during training. Experiments were conducted using recentlyproposed shifted benchmark sets for RR Lyrae stars, outperforming baselinemodels by up to 3% through a more reliable classifier. Our method provides anew path to incorporate knowledge from characteristic features into artificialneural networks to manage the underlying data shift problem.\rTowards General Purpose Medical AI: Continual Learning Medical Foundation Model\nHuahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li\nabstract\rabstract: Inevitable domain and task discrepancies in real-world scenarios can impairthe generalization performance of the pre-trained deep models for medical data.Therefore, we audaciously propose that we should build a general-purposemedical AI system that can be seamlessly adapted to downstream domains/tasks.Since the domain/task adaption procedures usually involve additional labelingwork for the target data, designing a data-efficient adaption algorithm isdesired to save the cost of transferring the learned knowledge. Our recent workfound that vision-language models (VLMs) are efficient learners withextraordinary cross-domain ability. Therefore, in this work, we further explorethe possibility of leveraging pre-trained VLMs as medical foundation models forbuilding general-purpose medical AI, where we thoroughly investigate threemachine-learning paradigms, i.e., domain/task-specialized learning, jointlearning, and continual learning, for training the VLMs and evaluate theirgeneralization performance on cross-domain and cross-task test sets. Toalleviate the catastrophic forgetting during sequential training, we employrehearsal learning and receive a sharp boost in terms of generalizationcapability. In a nutshell, our empirical evidence suggests that continuallearning may be a practical and efficient learning paradigm for the medicalfoundation model. And we hope researchers can use our empirical evidence asbasement to further explore the path toward medical foundation model.\r2023-03-08\nKubeEdge-Sedna v0.3: Towards Next-Generation Automatically Customized AI Engineering Scheme\nZimu Zheng\nabstract\rabstract: The scale of the global edge AI market continues to grow. The currenttechnical challenges that hinder the large-scale replication of edge AI aremainly small samples on the edge and heterogeneity of edge data. In addition,edge AI customers often have requirements for data security compliance andoffline autonomy of edge AI services. Based on the lifelong learning method inthe academic world, we formally define the problem of edge-cloud collaborativelifelong learning for the first time, and release the industry\u0026rsquo;s firstopen-source edge-cloud collaborative lifelong learning. Edge-cloudcollaborative lifelong learning adapts to data heterogeneity at different edgelocations through (1) multi-task transfer learning to achieve accurateprediction of \u0026ldquo;thousands of people and thousands of faces\u0026rdquo;; (2) incrementalprocessing of unknown tasks, the more systems learn and the smarter systems arewith small samples, gradually realize AI engineering and automation; (3) Usethe cloud-side knowledge base to remember new situational knowledge to avoidcatastrophic forgetting; (4) The edge-cloud collaborative architecture enablesdata security compliance and edge AI services to be offline autonomy whileapplying cloud resources. This work hopes to help fundamentally solve theabove-mentioned challenges of edge-cloud collaborative machine learning.\r2023-03-07\nRobustness-preserving Lifelong Learning via Dataset Condensation\nJinghan Jia, Yihua Zhang, Dogyoon Song, Sijia Liu, Alfred Hero\nabstract\rabstract: Lifelong learning (LL) aims to improve a predictive model as the data sourceevolves continuously. Most work in this learning paradigm has focused onresolving the problem of \u0026lsquo;catastrophic forgetting,\u0026rsquo; which refers to a notoriousdilemma between improving model accuracy over new data and retaining accuracyover previous data. Yet, it is also known that machine learning (ML) models canbe vulnerable in the sense that tiny, adversarial input perturbations candeceive the models into producing erroneous predictions. This motivates theresearch objective of this paper - specification of a new LL framework that cansalvage model robustness (against adversarial attacks) from catastrophicforgetting. Specifically, we propose a new memory-replay LL strategy thatleverages modern bi-level optimization techniques to determine the \u0026lsquo;coreset\u0026rsquo; ofthe current data (i.e., a small amount of data to be memorized) for ease ofpreserving adversarial robustness over time. We term the resulting LL framework\u0026rsquo;Data-Efficient Robustness-Preserving LL\u0026rsquo; (DERPLL). The effectiveness of DERPLLis evaluated for class-incremental image classification using ResNet-18 overthe CIFAR-10 dataset. Experimental results show that DERPLL outperforms theconventional coreset-guided LL baseline and achieves a substantial improvementin both standard accuracy and robust accuracy.\r2023-03-04\nPrototype-Guided Memory Replay for Continual Learning\nStella Ho, Ming Liu, Lan Du, Longxiang Gao, Yong Xiang\nabstract\rabstract: Continual learning (CL) refers to a machine learning paradigm that learnscontinuously without forgetting previously acquired knowledge. Thereby, majordifficulty in CL is catastrophic forgetting of preceding tasks, caused byshifts in data distributions. Existing CL models often save a large number ofold examples and stochastically revisit previously seen data to retain oldknowledge. However, the occupied memory size keeps enlarging along withaccumulating seen data. Hereby, we propose a memory-efficient CL method bystoring a few samples to achieve good performance. We devise a dynamicprototype-guided memory replay module and incorporate it into an onlinemeta-learning model. We conduct extensive experiments on text classificationand investigate the effect of training set orders on CL model performance. Theexperimental results testify the superiority of our method in terms offorgetting mitigation and efficiency.\r2023-02-26\nHeterogeneous Federated Knowledge Graph Embedding Learning and Unlearning\nXiangrong Zhu, Guangyao Li, Wei Hu\nabstract\rabstract: Federated Learning (FL) recently emerges as a paradigm to train a globalmachine learning model across distributed clients without sharing raw data.Knowledge Graph (KG) embedding represents KGs in a continuous vector space,serving as the backbone of many knowledge-driven applications. As a promisingcombination, federated KG embedding can fully take advantage of knowledgelearned from different clients while preserving the privacy of local data.However, realistic problems such as data heterogeneity and knowledge forgettingstill remain to be concerned. In this paper, we propose FedLU, a novel FLframework for heterogeneous KG embedding learning and unlearning. To cope withthe drift between local optimization and global convergence caused by dataheterogeneity, we propose mutual knowledge distillation to transfer localknowledge to global, and absorb global knowledge back. Moreover, we present anunlearning method based on cognitive neuroscience, which combines retroactiveinterference and passive decay to erase specific knowledge from local clientsand propagate to the global model by reusing knowledge distillation. Weconstruct new datasets for assessing realistic performance of thestate-of-the-arts. Extensive experiments show that FedLU achieves superiorresults in both link prediction and knowledge forgetting.\r2023-02-24\nSubspace based Federated Unlearning\nGuanghao Li, Li Shen, Yan Sun, Yue Hu, Han Hu, Dacheng Tao\nabstract\rabstract: Federated learning (FL) enables multiple clients to train a machine learningmodel collaboratively without exchanging their local data. Federated unlearningis an inverse FL process that aims to remove a specified target client\u0026rsquo;scontribution in FL to satisfy the user\u0026rsquo;s right to be forgotten. Most existingfederated unlearning algorithms require the server to store the history of theparameter updates, which is not applicable in scenarios where the serverstorage resource is constrained. In this paper, we propose asimple-yet-effective subspace based federated unlearning method, dubbed SFU,that lets the global model perform gradient ascent in the orthogonal space ofinput gradient spaces formed by other clients to eliminate the target client\u0026rsquo;scontribution without requiring additional storage. Specifically, the serverfirst collects the gradients generated from the target client after performinggradient ascent, and the input representation matrix is computed locally by theremaining clients. We also design a differential privacy method to protect theprivacy of the representation matrix. Then the server merges thoserepresentation matrices to get the input gradient subspace and updates theglobal model in the orthogonal subspace of the input gradient subspace tocomplete the forgetting task with minimal model performance degradation.Experiments on MNIST, CIFAR10, and CIFAR100 show that SFU outperforms severalstate-of-the-art (SOTA) federated unlearning algorithms by a large margin invarious settings.\r2023-02-22\nTowards Adversarial Evaluations for Inexact Machine Unlearning\nShashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, Ponnurangam Kumaraguru\nabstract\rabstract: Machine Learning models face increased concerns regarding the storage ofpersonal user data and adverse impacts of corrupted data like backdoors orsystematic bias. Machine Unlearning can address these by allowing post-hocdeletion of affected training data from a learned model. Achieving this taskexactly is computationally expensive; consequently, recent works have proposedinexact unlearning algorithms to solve this approximately as well as evaluationmethods to test the effectiveness of these algorithms. In this work, we first outline some necessary criteria for evaluation methodsand show no existing evaluation satisfies them all. Then, we design a strongerblack-box evaluation method called the Interclass Confusion (IC) test whichadversarially manipulates data during training to detect the insufficiency ofunlearning procedures. We also propose two analytically motivated baselinemethods~(EU-k and CF-k) which outperform several popular inexact unlearningmethods. Overall, we demonstrate how adversarial evaluation strategies can helpin analyzing various unlearning phenomena which can guide the development ofstronger unlearning algorithms.\r2023-02-20\nAudit to Forget: A Unified Method to Revoke Patients\u0026rsquo; Private Data in Intelligent Healthcare\nJuexiao Zhou, Haoyang Li, Xingyu Liao, Bin Zhang, Wenjia He, Zhongxiao Li, Longxi Zhou, Xin Gao\nabstract\rabstract: Revoking personal private data is one of the basic human rights, which hasalready been sheltered by several privacy-preserving laws in many countries.However, with the development of data science, machine learning and deeplearning techniques, this right is usually neglected or violated as more andmore patients\u0026rsquo; data are being collected and used for model training, especiallyin intelligent healthcare, thus making intelligent healthcare a sector wheretechnology must meet the law, regulations, and privacy principles to ensurethat the innovation is for the common good. In order to secure patients\u0026rsquo; rightto be forgotten, we proposed a novel solution by using auditing to guide theforgetting process, where auditing means determining whether a dataset has beenused to train the model and forgetting requires the information of a querydataset to be forgotten from the target model. We unified these two tasks byintroducing a new approach called knowledge purification. To implement oursolution, we developed AFS, a unified open-source software, which is able toevaluate and revoke patients\u0026rsquo; private data from pre-trained deep learningmodels. We demonstrated the generality of AFS by applying it to four tasks ondifferent datasets with various data sizes and architectures of deep learningnetworks. The software is publicly available at\\url{https://github.com/JoshuaChou2018/AFS}.\r2023-02-18\nOn Handling Catastrophic Forgetting for Incremental Learning of Human Physical Activity on the Edge\nJingwei Zuo, George Arvanitakis, Hakim Hacid\nabstract\rabstract: Human activity recognition (HAR) has been a classic research problem. Inparticular, with recent machine learning (ML) techniques, the recognition taskhas been largely investigated by companies and integrated into their productsfor customers. However, most of them apply a predefined activity set andconduct the learning process on the cloud, hindering specific personalizationsfrom end users (i.e., edge devices). Even though recent progress in IncrementalLearning allows learning new-class data on the fly, the learning process isgenerally conducted on the cloud, requiring constant data exchange betweencloud and edge devices, thus leading to data privacy issues. In this paper, wepropose PILOTE, which pushes the incremental learning process to the extremeedge, while providing reliable data privacy and practical utility, e.g., lowprocessing latency, personalization, etc. In particular, we consider thepractical challenge of extremely limited data during the incremental learningprocess on edge, where catastrophic forgetting is required to be handled in apractical way. We validate PILOTE with extensive experiments on human activitydata collected from mobile sensors. The results show PILOTE can work on edgedevices with extremely limited resources while providing reliable performance.\r2023-02-16\nGaussian Switch Sampling: A Second Order Approach to Active Learning\nRyan Benkert, Mohit Prabhushankar, Ghassan AlRegib, Armin Pacharmi, Enrique Corona\nabstract\rabstract: In active learning, acquisition functions define informativeness directly onthe representation position within the model manifold. However, for mostmachine learning models (in particular neural networks) this representation isnot fixed due to the training pool fluctuations in between active learningrounds. Therefore, several popular strategies are sensitive to experimentparameters (e.g. architecture) and do not consider model robustness toout-of-distribution settings. To alleviate this issue, we propose a groundedsecond-order definition of information content and sample importance within thecontext of active learning. Specifically, we define importance by how often aneural network \u0026ldquo;forgets\u0026rdquo; a sample during training - artifacts of second orderrepresentation shifts. We show that our definition produces highly accurateimportance scores even when the model representations are constrained by thelack of training data. Motivated by our analysis, we develop Gaussian SwitchSampling (GauSS). We show that GauSS is setup agnostic and robust to anomalousdistributions with exhaustive experiments on three in-distribution benchmarks,three out-of-distribution benchmarks, and three different architectures. Wereport an improvement of up to 5% when compared against four popular querystrategies.\r2023-02-14\nForget Unlearning: Towards True Data-Deletion in Machine Learning\nRishav Chourasia, Neil Shah\nabstract\rabstract: Unlearning algorithms aim to remove deleted data\u0026rsquo;s influence from trainedmodels at a cost lower than full retraining. However, prior guarantees ofunlearning in literature are flawed and don\u0026rsquo;t protect the privacy of deletedrecords. We show that when users delete their data as a function of publishedmodels, records in a database become interdependent. So, even retraining afresh model after deletion of a record doesn\u0026rsquo;t ensure its privacy. Secondly,unlearning algorithms that cache partial computations to speed up theprocessing can leak deleted information over a series of releases, violatingthe privacy of deleted records in the long run. To address these, we propose asound deletion guarantee and show that the privacy of existing records isnecessary for the privacy of deleted records. Under this notion, we propose anaccurate, computationally efficient, and secure machine unlearning algorithmbased on noisy gradient descent.\r2023-02-11\nTowards Label-Efficient Incremental Learning: A Survey\nMert Kilickaya, Joost van de Weijer, Yuki M. Asano\nabstract\rabstract: The current dominant paradigm when building a machine learning model is toiterate over a dataset over and over until convergence. Such an approach isnon-incremental, as it assumes access to all images of all categories at once.However, for many applications, non-incremental learning is unrealistic. Tothat end, researchers study incremental learning, where a learner is requiredto adapt to an incoming stream of data with a varying distribution whilepreventing forgetting of past knowledge. Significant progress has been made,however, the vast majority of works focus on the fully supervised setting,making these algorithms label-hungry thus limiting their real-life deployment.To that end, in this paper, we make the first attempt to survey recentlygrowing interest in label-efficient incremental learning. We identify threesubdivisions, namely semi-, few-shot- and self-supervised learning to reducelabeling efforts. Finally, we identify novel directions that can furtherenhance label-efficiency and improve incremental learning scalability. Projectwebsite: https://github.com/kilickaya/label-efficient-il.\r2023-02-07\nEfficiently Upgrading Multilingual Machine Translation Models to Support More Languages\nSimeng Sun, Maha Elbayad, Anna Sun, James Cross\nabstract\rabstract: With multilingual machine translation (MMT) models continuing to grow in sizeand number of supported languages, it is natural to reuse and upgrade existingmodels to save computation as data becomes available in more languages.However, adding new languages requires updating the vocabulary, whichcomplicates the reuse of embeddings. The question of how to reuse existingmodels while also making architectural changes to provide capacity for both oldand new languages has also not been closely studied. In this work, we introducethree techniques that help speed up effective learning of the new languages andalleviate catastrophic forgetting despite vocabulary and architecturemismatches. Our results show that by (1) carefully initializing the network,(2) applying learning rate scaling, and (3) performing data up-sampling, it ispossible to exceed the performance of a same-sized baseline model with 30%computation and recover the performance of a larger model trained from scratchwith over 50% reduction in computation. Furthermore, our analysis reveals thatthe introduced techniques help learn the new directions more effectively andalleviate catastrophic forgetting at the same time. We hope our work will guideresearch into more efficient approaches to growing languages for these MMTmodels and ultimately maximize the reuse of existing models.\rDeep Class-Incremental Learning: A Survey\nDa-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu\nabstract\rabstract: Deep models, e.g., CNNs and Vision Transformers, have achieved impressiveachievements in many vision tasks in the closed world. However, novel classesemerge from time to time in our ever-changing world, requiring a learningsystem to acquire new knowledge continually. For example, a robot needs tounderstand new instructions, and an opinion monitoring system should analyzeemerging topics every day. Class-Incremental Learning (CIL) enables the learnerto incorporate the knowledge of new classes incrementally and build a universalclassifier among all seen classes. Correspondingly, when directly training themodel with new class instances, a fatal problem occurs \u0026ndash; the model tends tocatastrophically forget the characteristics of former ones, and its performancedrastically degrades. There have been numerous efforts to tackle catastrophicforgetting in the machine learning community. In this paper, we surveycomprehensively recent advances in deep class-incremental learning andsummarize these methods from three aspects, i.e., data-centric, model-centric,and algorithm-centric. We also provide a rigorous and unified evaluation of 16methods in benchmark image classification tasks to find out the characteristicsof different algorithms empirically. Furthermore, we notice that the currentcomparison protocol ignores the influence of memory budget in model storage,which may result in unfair comparison and biased results. Hence, we advocatefair comparison by aligning the memory budget in evaluation, as well as severalmemory-agnostic performance measures. The source code to reproduce theseevaluations is available at https://github.com/zhoudw-zdw/CIL_Survey/\r2023-02-02\nLEAF: Navigating Concept Drift in Cellular Networks\nShinan Liu, Francesco Bronzino, Paul Schmitt, Arjun Nitin Bhagoji, Nick Feamster, Hector Garcia Crespo, Timothy Coyle, Brian Ward\nabstract\rabstract: Operational networks commonly rely on machine learning models for many tasks,including detecting anomalies, inferring application performance, andforecasting demand. Yet, model accuracy can degrade due to concept drift,whereby the relationship between the features and the target to be predictedchanges. Mitigating concept drift is an essential part of operationalizingmachine learning models in general, but is of particular importance innetworking\u0026rsquo;s highly dynamic deployment environments. In this paper, we firstcharacterize concept drift in a large cellular network for a major metropolitanarea in the United States. We find that concept drift occurs across manyimportant key performance indicators (KPIs), independently of the model,training set size, and time interval \u0026ndash; thus necessitating practical approachesto detect, explain, and mitigate it. We then show that frequent modelretraining with newly available data is not sufficient to mitigate conceptdrift, and can even degrade model accuracy further. Finally, we develop a newmethodology for concept drift mitigation, Local Error Approximation of Features(LEAF). LEAF works by detecting drift; explaining the features and timeintervals that contribute the most to drift; and mitigates it using forgettingand over-sampling. We evaluate LEAF against industry-standard mitigationapproaches (notably, periodic retraining) with more than four years of cellularKPI data. Our initial tests with a major cellular provider in the US show thatLEAF consistently outperforms periodic and triggered retraining on complex,real-world data while reducing costly retraining operations.\r2023-01-30\nEmergence of Maps in the Memories of Blind Navigation Agents\nErik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, Dhruv Batra\nabstract\rabstract: Animal navigation research posits that organisms build and maintain internalspatial representations, or maps, of their environment. We ask if machines \u0026ndash;specifically, artificial intelligence (AI) navigation agents \u0026ndash; also buildimplicit (or \u0026lsquo;mental\u0026rsquo;) maps. A positive answer to this question would (a)explain the surprising phenomenon in recent literature of ostensibly map-freeneural-networks achieving strong performance, and (b) strengthen the evidenceof mapping as a fundamental mechanism for navigation by intelligent embodiedagents, whether they be biological or artificial. Unlike animal navigation, wecan judiciously design the agent\u0026rsquo;s perceptual system and control the learningparadigm to nullify alternative navigation mechanisms. Specifically, we train\u0026rsquo;blind\u0026rsquo; agents \u0026ndash; with sensing limited to only egomotion and no other sensingof any kind \u0026ndash; to perform PointGoal navigation (\u0026lsquo;go to $\\Delta$ x, $\\Delta$ y\u0026rsquo;)via reinforcement learning. Our agents are composed of navigation-agnosticcomponents (fully-connected and recurrent neural networks), and ourexperimental setup provides no inductive bias towards mapping. Despite theseharsh conditions, we find that blind agents are (1) surprisingly effectivenavigators in new environments (~95% success); (2) they utilize memory overlong horizons (remembering ~1,000 steps of past experience in an episode); (3)this memory enables them to exhibit intelligent behavior (following walls,detecting collisions, taking shortcuts); (4) there is emergence of maps andcollision detection neurons in the representations of the environment built bya blind agent as it navigates; and (5) the emergent maps are selective and taskdependent (e.g. the agent \u0026lsquo;forgets\u0026rsquo; exploratory detours). Overall, this paperpresents no new techniques for the AI audience, but a surprising finding, aninsight, and an explanation.\r2023-01-13\nMemory Efficient Continual Learning with Transformers\nBeyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, Cedric Archambeau\nabstract\rabstract: In many real-world scenarios, data to train machine learning models becomesavailable over time. Unfortunately, these models struggle to continually learnnew concepts without forgetting what has been learnt in the past. Thisphenomenon is known as catastrophic forgetting and it is difficult to preventdue to practical constraints. For instance, the amount of data that can bestored or the computational resources that can be used might be limited.Moreover, applications increasingly rely on large pre-trained neural networks,such as pre-trained Transformers, since the resources or data might not beavailable in sufficiently large quantities to practitioners to train the modelfrom scratch. In this paper, we devise a method to incrementally train a modelon a sequence of tasks using pre-trained Transformers and extending them withAdapters. Different than the existing approaches, our method is able to scaleto a large number of tasks without significant overhead and allows sharinginformation across tasks. On both image and text classification tasks, weempirically demonstrate that our method maintains a good predictive performancewithout retraining the model or increasing the number of model parameters overtime. The resulting model is also significantly faster at inference timecompared to Adapter-based state-of-the-art methods.\r2022-12-29\nCarousel Memory: Rethinking the Design of Episodic Memory for Continual Learning\nSoobee Lee, Minindu Weerakoon, Jonghyun Choi, Minjia Zhang, Di Wang, Myeongjae Jeon\nabstract\rabstract: Continual Learning (CL) is an emerging machine learning paradigm that aims tolearn from a continuous stream of tasks without forgetting knowledge learnedfrom the previous tasks. To avoid performance decrease caused by forgetting,prior studies exploit episodic memory (EM), which stores a subset of the pastobserved samples while learning from new non-i.i.d. data. Despite the promisingresults, since CL is often assumed to execute on mobile or IoT devices, the EMsize is bounded by the small hardware memory capacity and makes it infeasibleto meet the accuracy requirements for real-world applications. Specifically,all prior CL methods discard samples overflowed from the EM and can neverretrieve them back for subsequent training steps, incurring loss of informationthat would exacerbate catastrophic forgetting. We explore a novel hierarchicalEM management strategy to address the forgetting issue. In particular, inmobile and IoT devices, real-time data can be stored not just in high-speedRAMs but in internal storage devices as well, which offer significantly largercapacity than the RAMs. Based on this insight, we propose to exploit theabundant storage to preserve past experiences and alleviate the forgetting byallowing CL to efficiently migrate samples between memory and storage withoutbeing interfered by the slow access speed of the storage. We call it CarouselMemory (CarM). As CarM is complementary to existing CL methods, we conductextensive evaluations of our method with seven popular CL methods and show thatCarM significantly improves the accuracy of the methods across differentsettings by large margins in final average accuracy (up to 28.4%) whileretaining the same training efficiency.\r2022-12-24\nUtilizing Priming to Identify Optimal Class Ordering to Alleviate Catastrophic Forgetting\nGabriel Mantione-Holmes, Justin Leo, Jugal Kalita\nabstract\rabstract: In order for artificial neural networks to begin accurately mimickingbiological ones, they must be able to adapt to new exigencies withoutforgetting what they have learned from previous training. Lifelong learningapproaches to artificial neural networks attempt to strive towards this goal,yet have not progressed far enough to be realistically deployed for naturallanguage processing tasks. The proverbial roadblock of catastrophic forgettingstill gate-keeps researchers from an adequate lifelong learning model. Whileefforts are being made to quell catastrophic forgetting, there is a lack ofresearch that looks into the importance of class ordering when training on newclasses for incremental learning. This is surprising as the ordering of\u0026quot;classes\u0026quot; that humans learn is heavily monitored and incredibly important.While heuristics to develop an ideal class order have been researched, thispaper examines class ordering as it relates to priming as a scheme forincremental class learning. By examining the connections between variousmethods of priming found in humans and how those are mimicked yet remainunexplained in life-long machine learning, this paper provides a betterunderstanding of the similarities between our biological systems and thesynthetic systems while simultaneously improving current practices to combatcatastrophic forgetting. Through the merging of psychological priming practiceswith class ordering, this paper is able to identify a generalizable method forclass ordering in NLP incremental learning tasks that consistently outperformsrandom class ordering.\r2022-12-22\nA Review of Deep Transfer Learning and Recent Advancements\nMohammadreza Iman, Khaled Rasheed, Hamid R. Arabnia\nabstract\rabstract: Deep learning has been the answer to many machine learning problems duringthe past two decades. However, it comes with two major constraints: dependencyon extensive labeled data and training costs. Transfer learning in deeplearning, known as Deep Transfer Learning (DTL), attempts to reduce suchdependency and costs by reusing an obtained knowledge from a source data/taskin training on a target data/task. Most applied DTL techniques arenetwork/model-based approaches. These methods reduce the dependency of deeplearning models on extensive training data and drastically decrease trainingcosts. As a result, researchers detected Covid-19 infection on chest X-Rayswith high accuracy at the beginning of the pandemic with minimal data using DTLtechniques. Also, the training cost reduction makes DTL viable on edge deviceswith limited resources. Like any new advancement, DTL methods have their ownlimitations, and a successful transfer depends on some adjustments fordifferent scenarios. In this paper, we review the definition and taxonomy ofdeep transfer learning and well-known methods. Then we investigate the DTLapproaches by reviewing recent applied DTL techniques in the past five years.Further, we review some experimental analyses of DTLs to learn the bestpractice for applying DTL in different scenarios. Moreover, the limitations ofDTLs (catastrophic forgetting dilemma and overly biased pre-trained models) arediscussed, along with possible solutions and research trends.\r2022-12-21\nHidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks\nJimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari\nabstract\rabstract: We introduce camouflaged data poisoning attacks, a new attack vector thatarises in the context of machine unlearning and other settings when modelretraining may be induced. An adversary first adds a few carefully craftedpoints to the training dataset such that the impact on the model\u0026rsquo;s predictionsis minimal. The adversary subsequently triggers a request to remove a subset ofthe introduced points at which point the attack is unleashed and the model\u0026rsquo;spredictions are negatively affected. In particular, we consider clean-labeltargeted attacks (in which the goal is to cause the model to misclassify aspecific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.This attack is realized by constructing camouflage datapoints that mask theeffect of a poisoned dataset.\r2022-12-19\nPrivacy Adhering Machine Un-learning in NLP\nVinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth\nabstract\rabstract: Regulations introduced by General Data Protection Regulation (GDPR) in the EUor California Consumer Privacy Act (CCPA) in the US have included provisions onthe \\textit{right to be forgotten} that mandates industry applications toremove data related to an individual from their systems. In several real worldindustry applications that use Machine Learning to build models on user data,such mandates require significant effort both in terms of data cleansing aswell as model retraining while ensuring the models do not deteriorate inprediction quality due to removal of data. As a result, continuous removal ofdata and model retraining steps do not scale if these applications receive suchrequests at a very high frequency. Recently, a few researchers proposed theidea of \\textit{Machine Unlearning} to tackle this challenge. Despite thesignificant importance of this task, the area of Machine Unlearning isunder-explored in Natural Language Processing (NLP) tasks. In this paper, weexplore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as,QQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC andSISA-A) to perform \\textit{guaranteed} Unlearning that provides significantreduction in terms of both memory (90-95%), time (100x) and space consumption(99%) in comparison to the baselines while keeping model performance constant.\r2022-12-18\nBounding Membership Inference\nAnvith Thudi, Ilia Shumailov, Franziska Boenisch, Nicolas Papernot\nabstract\rabstract: Differential Privacy (DP) is the de facto standard for reasoning about theprivacy guarantees of a training algorithm. Despite the empirical observationthat DP reduces the vulnerability of models to existing membership inference(MI) attacks, a theoretical underpinning as to why this is the case is largelymissing in the literature. In practice, this means that models need to betrained with DP guarantees that greatly decrease their accuracy. In this paper, we provide a tighter bound on the positive accuracy (i.e.,attack precision) of any MI adversary when a training algorithm provides$(\\varepsilon, \\delta)$-DP. Our bound informs the design of a novel privacyamplification scheme: an effective training set is sub-sampled from a largerset prior to the beginning of training. We find this greatly reduces the boundon MI positive accuracy. As a result, our scheme allows the use of looser DPguarantees to limit the success of any MI adversary; this ensures that themodel\u0026rsquo;s accuracy is less impacted by the privacy guarantee. While this clearlybenefits entities working with far more data than they need to train on, it canalso improve the accuracy-privacy trade-off on benchmarks studied in theacademic literature. Consequently, we also find that subsampling decreases theeffectiveness of a state-of-the-art MI attack (LiRA) much more effectively thantraining with stronger DP guarantees on MNIST and CIFAR10. We conclude bydiscussing implications of our MI bound on the field of machine unlearning.\r2022-12-16\nFundamental limits to learning closed-form mathematical models from data\nOscar Fajardo-Fontiveros, Ignasi Reichardt, Harry R. De Los Rios, Jordi Duch, Marta Sales-Pardo, Roger Guimera\nabstract\rabstract: Given a finite and noisy dataset generated with a closed-form mathematicalmodel, when is it possible to learn the true generating model from the dataalone? This is the question we investigate here. We show that thismodel-learning problem displays a transition from a low-noise phase in whichthe true model can be learned, to a phase in which the observation noise is toohigh for the true model to be learned by any method. Both in the low-noisephase and in the high-noise phase, probabilistic model selection leads tooptimal generalization to unseen data. This is in contrast to standard machinelearning approaches, including artificial neural networks, which in thisparticular problem are limited, in the low-noise phase, by their ability tointerpolate. In the transition region between the learnable and unlearnablephases, generalization is hard for all approaches including probabilistic modelselection.\r2022-12-15\nForgetful Forests: high performance learning data structures for streaming data under concept drift\nZhehu Yuan, Yinqi Sun, Dennis Shasha\nabstract\rabstract: Database research can help machine learning performance in many ways. One wayis to design better data structures. This paper combines the use of incrementalcomputation and sequential and probabilistic filtering to enable \u0026ldquo;forgetful\u0026quot;tree-based learning algorithms to cope with concept drift data (i.e., datawhose function from input to classification changes over time). The forgetful algorithms described in this paper achieve high timeperformance while maintaining high quality predictions on streaming data.Specifically, the algorithms are up to 24 times faster than state-of-the-artincremental algorithms with at most a 2% loss of accuracy, or at least twicefaster without any loss of accuracy. This makes such structures suitable forhigh volume streaming applications.\r2022-12-08\nSystem Design for an Integrated Lifelong Reinforcement Learning Agent for Real-Time Strategy Games\nIndranil Sur, Zachary Daniels, Abrar Rahman, Kamil Faber, Gianmarco J. Gallardo, Tyler L. Hayes, Cameron E. Taylor, Mustafa Burak Gurbuz, James Smith, Sahana Joshi, Nathalie Japkowicz, Michael Baron, Zsolt Kira, Christopher Kanan, Roberto Corizzo, Ajay Divakaran, Michael Piacentino, Jesse Hostetler, Aswin Raghavan\nabstract\rabstract: As Artificial and Robotic Systems are increasingly deployed and relied uponfor real-world applications, it is important that they exhibit the ability tocontinually learn and adapt in dynamically-changing environments, becomingLifelong Learning Machines. Continual/lifelong learning (LL) involvesminimizing catastrophic forgetting of old tasks while maximizing a model\u0026rsquo;scapability to learn new tasks. This paper addresses the challenging lifelongreinforcement learning (L2RL) setting. Pushing the state-of-the-art forward inL2RL and making L2RL useful for practical applications requires more thandeveloping individual L2RL algorithms; it requires making progress at thesystems-level, especially research into the non-trivial problem of how tointegrate multiple L2RL algorithms into a common framework. In this paper, weintroduce the Lifelong Reinforcement Learning Components Framework (L2RLCF),which standardizes L2RL systems and assimilates different continual learningcomponents (each addressing different aspects of the lifelong learning problem)into a unified system. As an instantiation of L2RLCF, we develop a standard APIallowing easy integration of novel lifelong learning components. We describe acase study that demonstrates how multiple independently-developed LL componentscan be integrated into a single realized system. We also introduce anevaluation environment in order to measure the effect of combining varioussystem components. Our evaluation environment employs different LL scenarios(sequences of tasks) consisting of Starcraft-2 minigames and allows for thefair, comprehensive, and quantitative comparison of different combinations ofcomponents within a challenging common evaluation environment.\r2022-12-06\nPre-trained Encoders in Self-Supervised Learning Improve Secure and Privacy-preserving Supervised Learning\nHongbin Liu, Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong\nabstract\rabstract: Classifiers in supervised learning have various security and privacy issues,e.g., 1) data poisoning attacks, backdoor attacks, and adversarial examples onthe security side as well as 2) inference attacks and the right to be forgottenfor the training data on the privacy side. Various secure andprivacy-preserving supervised learning algorithms with formal guarantees havebeen proposed to address these issues. However, they suffer from variouslimitations such as accuracy loss, small certified security guarantees, and/orinefficiency. Self-supervised learning is an emerging technique to pre-trainencoders using unlabeled data. Given a pre-trained encoder as a featureextractor, supervised learning can train a simple yet accurate classifier usinga small amount of labeled training data. In this work, we perform the firstsystematic, principled measurement study to understand whether and when apre-trained encoder can address the limitations of secure or privacy-preservingsupervised learning algorithms. Our key findings are that a pre-trained encodersubstantially improves 1) both accuracy under no attacks and certified securityguarantees against data poisoning and backdoor attacks of state-of-the-artsecure learning algorithms (i.e., bagging and KNN), 2) certified securityguarantees of randomized smoothing against adversarial examples withoutsacrificing its accuracy under no attacks, 3) accuracy of differentiallyprivate classifiers, and 4) accuracy and/or efficiency of exact machineunlearning.\rLife-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation\nYang Zhao, Junnan Zhu, Lu Xiang, Jiajun Zhang, Yu Zhou, Feifei Zhai, Chengqing Zong\nabstract\rabstract: A common scenario of Multilingual Neural Machine Translation (MNMT) is thateach translation task arrives in a sequential manner, and the training data ofprevious tasks is unavailable. In this scenario, the current methods sufferheavily from catastrophic forgetting (CF). To alleviate the CF, we investigateknowledge distillation based life-long learning methods. Specifically, inone-tomany scenario, we propose a multilingual distillation method to make thenew model (student) jointly learn multilingual output from old model (teacher)and new task. In many-to one scenario, we find that direct distillation facesthe extreme partial distillation problem, and we propose two different methodsto address it: pseudo input distillation and reverse teacher distillation. Theexperimental results on twelve translation tasks show that the proposed methodscan better consolidate the previous knowledge and sharply alleviate the CF.\rCySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain\nMarkus Bayer, Philipp Kuehn, Ramin Shanehsaz, Christian Reuter\nabstract\rabstract: The field of cybersecurity is evolving fast. Experts need to be informedabout past, current and - in the best case - upcoming threats, because attacksare becoming more advanced, targets bigger and systems more complex. As thiscannot be addressed manually, cybersecurity experts need to rely on machinelearning techniques. In the texutual domain, pre-trained language models likeBERT have shown to be helpful, by providing a good baseline for furtherfine-tuning. However, due to the domain-knowledge and many technical terms incybersecurity general language models might miss the gist of textualinformation, hence doing more harm than good. For this reason, we create ahigh-quality dataset and present a language model specifically tailored to thecybersecurity domain, which can serve as a basic building block forcybersecurity systems that deal with natural language. The model is comparedwith other models based on 15 different domain-dependent extrinsic andintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On theone hand, the results of the intrinsic tasks show that our model improves theinternal representation space of words compared to the other models. On theother hand, the extrinsic, domain-dependent tasks, consisting of sequencetagging and classification, show that the model is best in specific applicationscenarios, in contrast to the others. Furthermore, we show that our approachagainst catastrophic forgetting works, as the model is able to retrieve thepreviously trained domain-independent knowledge. The used dataset and trainedmodel are made publicly available\r2022-12-03\nCIRCLE: Continual Repair across Programming Languages\nWei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong Hao, Hongzhi Yin\nabstract\rabstract: Automatic Program Repair (APR) aims at fixing buggy source code with lessmanual debugging efforts, which plays a vital role in improving softwarereliability and development productivity. Recent APR works have achievedremarkable progress via applying deep learning (DL), particularly neuralmachine translation (NMT) techniques. However, we observe that existingDL-based APR models suffer from at least two severe drawbacks: (1) Most of themcan only generate patches for a single programming language, as a result, torepair multiple languages, we have to build and train many repairing models.(2) Most of them are developed in an offline manner. Therefore, they won\u0026rsquo;tfunction when there are new-coming requirements. To address the above problems,a T5-based APR framework equipped with continual learning ability acrossmultiple programming languages is proposed, namely \\emph{C}ont\\emph{I}nual\\emph{R}epair a\\emph{C}ross Programming \\emph{L}anguag\\emph{E}s(\\emph{CIRCLE}). Specifically, (1) CIRCLE utilizes a prompting function tonarrow the gap between natural language processing (NLP) pre-trained tasks andAPR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achievelifelong learning for APR without access to the full historical data. (3) Anelastic regularization method is employed to strengthen CIRCLE\u0026rsquo;s continuallearning ability further, preventing it from catastrophic forgetting. (4)CIRCLE applies a simple but effective re-repairing method to revise generatederrors caused by crossing multiple programming languages. We train CIRCLE forfour languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on fivecommonly used benchmarks. The experimental results demonstrate that CIRCLE notonly effectively and efficiently repairs multiple programming languages incontinual learning settings, but also achieves state-of-the-art performancewith a single repair model.\r2022-11-25\nOvercoming Catastrophic Forgetting by XAI\nGiang Nguyen\nabstract\rabstract: Explaining the behaviors of deep neural networks, usually considered as blackboxes, is critical especially when they are now being adopted over diverseaspects of human life. Taking the advantages of interpretable machine learning(interpretable ML), this work proposes a novel tool called CatastrophicForgetting Dissector (or CFD) to explain catastrophic forgetting in continuallearning settings. We also introduce a new method called Critical Freezingbased on the observations of our tool. Experiments on ResNet articulate howcatastrophic forgetting happens, particularly showing which components of thisfamous network are forgetting. Our new continual learning algorithm defeatsvarious recent techniques by a significant margin, proving the capability ofthe investigation. Critical freezing not only attacks catastrophic forgettingbut also exposes explainability.\r2022-11-14\nHierarchically Structured Task-Agnostic Continual Learning\nHeinke Hihn, Daniel A. Braun\nabstract\rabstract: One notable weakness of current machine learning algorithms is the poorability of models to solve new problems without forgetting previously acquiredknowledge. The Continual Learning paradigm has emerged as a protocol tosystematically investigate settings where the model sequentially observessamples generated by a series of tasks. In this work, we take a task-agnosticview of continual learning and develop a hierarchical information-theoreticoptimality principle that facilitates a trade-off between learning andforgetting. We derive this principle from a Bayesian perspective and show itsconnections to previous approaches to continual learning. Based on thisprinciple, we propose a neural network layer, called theMixture-of-Variational-Experts layer, that alleviates forgetting by creating aset of information processing paths through the network which is governed by agating policy. Equipped with a diverse and specialized set of parameters, eachpath can be regarded as a distinct sub-network that learns to solve tasks. Toimprove expert allocation, we introduce diversity objectives, which we evaluatein additional ablation studies. Importantly, our approach can operate in atask-agnostic way, i.e., it does not require task-specific knowledge, as is thecase with many existing continual learning algorithms. Due to the generalformulation based on generic utility functions, we can apply this optimalityprinciple to a large variety of learning problems, including supervisedlearning, reinforcement learning, and generative modeling. We demonstrate thecompetitive performance of our method on continual reinforcement learning andvariants of the MNIST, CIFAR-10, and CIFAR-100 datasets.\r2022-11-12\nBayesPCN: A Continually Learnable Predictive Coding Associative Memory\nJason Yoo, Frank Wood\nabstract\rabstract: Associative memory plays an important role in human intelligence and itsmechanisms have been linked to attention in machine learning. While the machinelearning community\u0026rsquo;s interest in associative memories has recently beenrekindled, most work has focused on memory recall ($read$) over memory learning($write$). In this paper, we present BayesPCN, a hierarchical associativememory capable of performing continual one-shot memory writes withoutmeta-learning. Moreover, BayesPCN is able to gradually forget past observations($forget$) to free its memory. Experiments show that BayesPCN can recallcorrupted i.i.d. high-dimensional data observed hundreds to a thousand``timesteps\u0026rsquo;\u0026rsquo; ago without a large drop in recall ability compared to thestate-of-the-art offline-learned parametric memory models.\r2022-11-07\nHFedMS: Heterogeneous Federated Learning with Memorable Data Semantics in Industrial Metaverse\nShenglai Zeng, Zonghang Li, Hongfang Yu, Zhihao Zhang, Long Luo, Bo Li, Dusit Niyato\nabstract\rabstract: Federated Learning (FL), as a rapidly evolving privacy-preservingcollaborative machine learning paradigm, is a promising approach to enable edgeintelligence in the emerging Industrial Metaverse. Even though many successfuluse cases have proved the feasibility of FL in theory, in the industrialpractice of Metaverse, the problems of non-independent and identicallydistributed (non-i.i.d.) data, learning forgetting caused by streamingindustrial data, and scarce communication bandwidth remain key barriers torealize practical FL. Facing the above three challenges simultaneously, thispaper presents a high-performance and efficient system named HFEDMS forincorporating practical FL into Industrial Metaverse. HFEDMS reduces dataheterogeneity through dynamic grouping and training mode conversion (DynamicSequential-to-Parallel Training, STP). Then, it compensates for the forgottenknowledge by fusing compressed historical data semantics and calibratesclassifier parameters (Semantic Compression and Compensation, SCC). Finally,the network parameters of the feature extractor and classifier are synchronizedin different frequencies (Layer-wiseAlternative Synchronization Protocol, LASP)to reduce communication costs. These techniques make FL more adaptable to theheterogeneous streaming data continuously generated by industrial equipment,and are also more efficient in communication than traditional methods (e.g.,Federated Averaging). Extensive experiments have been conducted on the streamednon-i.i.d. FEMNIST dataset using 368 simulated devices. Numerical results showthat HFEDMS improves the classification accuracy by at least 6.4% compared with8 benchmarks and saves both the overall runtime and transfer bytes by up to98%, proving its superiority in precision and efficiency.\r2022-11-04\nContinual Learning of Neural Machine Translation within Low Forgetting Risk Regions\nShuhao Gu, Bojie Hu, Yang Feng\nabstract\rabstract: This paper considers continual learning of large-scale pretrained neuralmachine translation model without accessing the previous training data orintroducing model separation. We argue that the widely usedregularization-based methods, which perform multi-objective learning with anauxiliary loss, suffer from the misestimate problem and cannot always achieve agood balance between the previous and new tasks. To solve the problem, wepropose a two-stage training method based on the local features of the realloss. We first search low forgetting risk regions, where the model can retainthe performance on the previous task as the parameters are updated, to avoidthe catastrophic forgetting problem. Then we can continually train the modelwithin this region only with the new training data to fit the new task.Specifically, we propose two methods to search the low forgetting risk regions,which are based on the curvature of loss and the impacts of the parameters onthe model output, respectively. We conduct experiments on domain adaptation andmore challenging language adaptation tasks, and the experimental results showthat our method can achieve significant improvements compared with severalstrong baselines.\r2022-10-29\nAnti-Retroactive Interference for Lifelong Learning\nRunqi Wang, Yuxiang Bao, Baochang Zhang, Jianzhuang Liu, Wentao Zhu, Guodong Guo\nabstract\rabstract: Humans can continuously learn new knowledge. However, machine learning modelssuffer from drastic dropping in performance on previous tasks after learningnew tasks. Cognitive science points out that the competition of similarknowledge is an important cause of forgetting. In this paper, we design aparadigm for lifelong learning based on meta-learning and associative mechanismof the brain. It tackles the problem from two aspects: extracting knowledge andmemorizing knowledge. First, we disrupt the sample\u0026rsquo;s background distributionthrough a background attack, which strengthens the model to extract the keyfeatures of each task. Second, according to the similarity between incrementalknowledge and base knowledge, we design an adaptive fusion of incrementalknowledge, which helps the model allocate capacity to the knowledge ofdifferent difficulties. It is theoretically analyzed that the proposed learningparadigm can make the models of different tasks converge to the same optimum.The proposed method is validated on the MNIST, CIFAR100, CUB200 and ImageNet100datasets.\r2022-10-28\nLegoNet: A Fast and Exact Unlearning Architecture\nSihao Yu, Fei Sun, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng\nabstract\rabstract: Machine unlearning aims to erase the impact of specific training samples upondeleted requests from a trained model. Re-training the model on the retaineddata after deletion is an effective but not efficient way due to the hugenumber of model parameters and re-training samples. To speed up, a natural wayis to reduce such parameters and samples. However, such a strategy typicallyleads to a loss in model performance, which poses the challenge that increasingthe unlearning efficiency while maintaining acceptable performance. In thispaper, we present a novel network, namely \\textit{LegoNet}, which adopts theframework of ``fixed encoder + multiple adapters\u0026rsquo;\u0026rsquo;. We fix the encoder~(\\ie thebackbone for representation learning) of LegoNet to reduce the parameters thatneed to be re-trained during unlearning. Since the encoder occupies a majorpart of the model parameters, the unlearning efficiency is significantlyimproved. However, fixing the encoder empirically leads to a significantperformance drop. To compensate for the performance loss, we adopt the ensembleof multiple adapters, which are independent sub-models adopted to infer theprediction by the encoding~(\\ie the output of the encoder). Furthermore, wedesign an activation mechanism for the adapters to further trade off unlearningefficiency against model performance. This mechanism guarantees that eachsample can only impact very few adapters, so during unlearning, parameters andsamples that need to be re-trained are both reduced. The empirical experimentsverify that LegoNet accomplishes fast and exact unlearning while maintainingacceptable performance, synthetically outperforming unlearning baselines.\rCertified Graph Unlearning\nEli Chien, Chao Pan, Olgica Milenkovic\nabstract\rabstract: Graph-structured data is ubiquitous in practice and often processed usinggraph neural networks (GNNs). With the adoption of recent laws ensuring the``right to be forgotten\u0026rsquo;\u0026rsquo;, the problem of graph data removal has become ofsignificant importance. To address the problem, we introduce the first knownframework for \\emph{certified graph unlearning} of GNNs. In contrast tostandard machine unlearning, new analytical and heuristic unlearning challengesarise when dealing with complex graph data. First, three different types ofunlearning requests need to be considered, including node feature, edge andnode unlearning. Second, to establish provable performance guarantees, oneneeds to address challenges associated with feature mixing during propagation.The underlying analysis is illustrated on the example of simple graphconvolutions (SGC) and their generalized PageRank (GPR) extensions, therebylaying the theoretical foundation for certified unlearning of GNNs. Ourempirical studies on six benchmark datasets demonstrate excellentperformance-complexity trade-offs when compared to complete retraining methodsand approaches that do not leverage graph information. For example, whenunlearning $20%$ of the nodes on the Cora dataset, our approach suffers only a$0.1%$ loss in test accuracy while offering a $4$-fold speed-up compared tocomplete retraining. Our scheme also outperforms unlearning methods that do notleverage graph information with a $12%$ increase in test accuracy for acomparable time complexity.\r2022-10-23\nOvercoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation\nTu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant\nabstract\rabstract: In this paper, we explore the challenging problem of performing a generativetask in a target language when labeled data is only available in English, usingsummarization as a case study. We assume a strict setting with no access toparallel data or machine translation and find that common transfer learningapproaches struggle in this setting, as a generative multilingual modelfine-tuned purely on English catastrophically forgets how to generatenon-English. Given the recent rise of parameter-efficient adaptationtechniques, we conduct the first investigation into how one such method, prompttuning (Lester et al., 2021), can overcome catastrophic forgetting to enablezero-shot cross-lingual generation. Our experiments show thatparameter-efficient prompt tuning provides gains over standard fine-tuning whentransferring between less-related languages, e.g., from English to Thai.However, a significant gap still remains between these methods andfully-supervised baselines. To improve cross-lingual transfer further, weexplore several approaches, including: (1) mixing in unlabeled multilingualdata, and (2) explicitly factoring prompts into recombinable language and taskcomponents. Our approaches can provide further quality gains, suggesting thatrobust zero-shot cross-lingual generation is within reach.\r2022-10-21\nA Survey of Machine Unlearning\nThanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, Quoc Viet Hung Nguyen\nabstract\rabstract: Today, computer systems hold large amounts of personal data. Yet while suchan abundance of data allows breakthroughs in artificial intelligence, andespecially machine learning (ML), its existence can be a threat to userprivacy, and it can weaken the bonds of trust between humans and AI. Recentregulations now require that, on request, private information about a user mustbe removed from both computer systems and from ML models, i.e. ``the right tobe forgotten\u0026rsquo;\u0026rsquo;). While removing data from back-end databases should bestraightforward, it is not sufficient in the AI context as ML models often`remember\u0026rsquo; the old data. Contemporary adversarial attacks on trained modelshave proven that we can learn whether an instance or an attribute belonged tothe training data. This phenomenon calls for a new paradigm, namely machineunlearning, to make ML models forget about particular data. It turns out thatrecent works on machine unlearning have not been able to completely solve theproblem due to the lack of common frameworks and resources. Therefore, thispaper aspires to present a comprehensive examination of machine unlearning\u0026rsquo;sconcepts, scenarios, methods, and applications. Specifically, as a categorycollection of cutting-edge studies, the intention behind this article is toserve as a comprehensive resource for researchers and practitioners seeking anintroduction to machine unlearning and its formulations, design criteria,removal requests, algorithms, and applications. In addition, we aim tohighlight the key findings, current trends, and new research areas that havenot yet featured the use of machine unlearning but could benefit greatly fromit. We hope this survey serves as a valuable resource for ML researchers andthose seeking to innovate privacy technologies. Our resources are publiclyavailable at https://github.com/tamlhp/awesome-machine-unlearning.\rProof of Unlearning: Definitions and Instantiation\nJiasi Weng, Shenglong Yao, Yuefeng Du, Junjie Huang, Jian Weng, Cong Wang\nabstract\rabstract: The \u0026ldquo;Right to be Forgotten\u0026rdquo; rule in machine learning (ML) practice enablessome individual data to be deleted from a trained model, as pursued by recentlydeveloped machine unlearning techniques. To truly comply with the rule, anatural and necessary step is to verify if the individual data are indeeddeleted after unlearning. Yet, previous parameter-space verification metricsmay be easily evaded by a distrustful model trainer. Thus, Thudi et al.recently present a call to action on algorithm-level verification in USENIXSecurity'22. We respond to the call, by reconsidering the unlearning problem in thescenario of machine learning as a service (MLaaS), and proposing a newdefinition framework for Proof of Unlearning (PoUL) on algorithm level.Specifically, our PoUL definitions (i) enforce correctness properties on boththe pre and post phases of unlearning, so as to prevent the state-of-the-artforging attacks; (ii) highlight proper practicality requirements of both theprover and verifier sides with minimal invasiveness to the off-the-shelfservice pipeline and computational workloads. Under the definition framework,we subsequently present a trusted hardware-empowered instantiation using SGXenclave, by logically incorporating an authentication layer for tracing thedata lineage with a proving layer for supporting the audit of learning. Wecustomize authenticated data structures to support large out-of-enclave storagewith simple operation logic, and meanwhile, enable proving complex unlearninglogic with affordable memory footprints in the enclave. We finally validate thefeasibility of the proposed instantiation with a proof-of-conceptimplementation and multi-dimensional performance evaluation.\r2022-09-30\nMachine Unlearning Method Based On Projection Residual\nZihao Cao, Jianzong Wang, Shijing Si, Zhangcheng Huang, Jing Xiao\nabstract\rabstract: Machine learning models (mainly neural networks) are used more and more inreal life. Users feed their data to the model for training. But these processesare often one-way. Once trained, the model remembers the data. Even when datais removed from the dataset, the effects of these data persist in the model.With more and more laws and regulations around the world protecting dataprivacy, it becomes even more important to make models forget this datacompletely through machine unlearning. This paper adopts the projection residual method based on Newton iterationmethod. The main purpose is to implement machine unlearning tasks in thecontext of linear regression models and neural network models. This methodmainly uses the iterative weighting method to completely forget the data andits corresponding influence, and its computational cost is linear in thefeature dimension of the data. This method can improve the current machinelearning method. At the same time, it is independent of the size of thetraining set. Results were evaluated by feature injection testing (FIT).Experiments show that this method is more thorough in deleting data, which isclose to model retraining.\r2022-09-26\nProject and Forget: Solving Large-Scale Metric Constrained Problems\nRishi Sonthalia, Anna C. Gilbert\nabstract\rabstract: Given a set of dissimilarity measurements amongst data points, determiningwhat metric representation is most \u0026ldquo;consistent\u0026rdquo; with the input measurements orthe metric that best captures the relevant geometric features of the data is akey step in many machine learning algorithms. Existing methods are restrictedto specific kinds of metrics or small problem sizes because of the large numberof metric constraints in such problems. In this paper, we provide an active setalgorithm, Project and Forget, that uses Bregman projections, to solve metricconstrained problems with many (possibly exponentially) inequality constraints.We provide a theoretical analysis of \\textsc{Project and Forget} and prove thatour algorithm converges to the global optimal solution and that the $L_2$distance of the current iterate to the optimal solution decays asymptoticallyat an exponential rate. We demonstrate that using our method we can solve largeproblem instances of three types of metric constrained problems: general weightcorrelation clustering, metric nearness, and metric learning; in each case,out-performing the state of the art methods with respect to CPU times andproblem sizes.\r2022-09-25\nAlgorithms that Approximate Data Removal: New Results and Limitations\nVinith M. Suriyakumar, Ashia C. Wilson\nabstract\rabstract: We study the problem of deleting user data from machine learning modelstrained using empirical risk minimization. Our focus is on learning algorithmswhich return the empirical risk minimizer and approximate unlearning algorithmsthat comply with deletion requests that come streaming minibatches. Leveragingthe infintesimal jacknife, we develop an online unlearning algorithm that isboth computationally and memory efficient. Unlike prior memory efficientunlearning algorithms, we target models that minimize objectives withnon-smooth regularizers, such as the commonly used $\\ell_1$, elastic net, ornuclear norm penalties. We also provide generalization, deletion capacity, andunlearning guarantees that are consistent with state of the art methods. Acrossa variety of benchmark datasets, our algorithm empirically improves upon theruntime of prior methods while maintaining the same memory requirements andtest accuracy. Finally, we open a new direction of inquiry by proving that allapproximate unlearning algorithms introduced so far fail to unlearn in problemsettings where common hyperparameter tuning methods, such as cross-validation,have been used to select models.\r2022-09-19\nEvaluating Machine Unlearning via Epistemic Uncertainty\nAlexander Becker, Thomas Liebig\nabstract\rabstract: There has been a growing interest in Machine Unlearning recently, primarilydue to legal requirements such as the General Data Protection Regulation (GDPR)and the California Consumer Privacy Act. Thus, multiple approaches werepresented to remove the influence of specific target data points from a trainedmodel. However, when evaluating the success of unlearning, current approacheseither use adversarial attacks or compare their results to the optimalsolution, which usually incorporates retraining from scratch. We argue thatboth ways are insufficient in practice. In this work, we present an evaluationmetric for Machine Unlearning algorithms based on epistemic uncertainty. Thisis the first definition of a general evaluation metric for Machine Unlearningto our best knowledge.\r2022-09-16\nGraph Unlearning\nMin Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang\nabstract\rabstract: Machine unlearning is a process of removing the impact of some training datafrom the machine learning (ML) models upon receiving removal requests. Whilestraightforward and legitimate, retraining the ML model from scratch incurs ahigh computational overhead. To address this issue, a number of approximatealgorithms have been proposed in the domain of image and text data, among whichSISA is the state-of-the-art solution. It randomly partitions the training setinto multiple shards and trains a constituent model for each shard. However,directly applying SISA to the graph data can severely damage the graphstructural information, and thereby the resulting ML model utility. In thispaper, we propose GraphEraser, a novel machine unlearning framework tailored tograph data. Its contributions include two novel graph partition algorithms anda learning-based aggregation method. We conduct extensive experiments on fivereal-world graph datasets to illustrate the unlearning efficiency and modelutility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to35.94$\\times$ (large dataset) unlearning time improvement. On the other hand,GraphEraser achieves up to $62.5%$ higher F1 score and our proposedlearning-based aggregation method achieves up to $112%$ higher F1score.\\footnote{Our code is available at\\url{https://github.com/MinChen00/Graph-Unlearning}.}\r2022-09-13\nChallenges and Pitfalls of Bayesian Unlearning\nAmbrish Rawat, James Requeima, Wessel Bruinsma, Richard Turner\nabstract\rabstract: Machine unlearning refers to the task of removing a subset of training data,thereby removing its contributions to a trained model. Approximate unlearningare one class of methods for this task which avoid the need to retrain themodel from scratch on the retained data. Bayes\u0026rsquo; rule can be used to castapproximate unlearning as an inference problem where the objective is to obtainthe updated posterior by dividing out the likelihood of deleted data. Howeverthis has its own set of challenges as one often doesn\u0026rsquo;t have access to theexact posterior of the model parameters. In this work we examine the use of theLaplace approximation and Variational Inference to obtain the updatedposterior. With a neural network trained for a regression task as the guidingexample, we draw insights on the applicability of Bayesian unlearning inpractical scenarios.\r2022-09-07\nThe Role Of Biology In Deep Learning\nRobert Bain\nabstract\rabstract: Artificial neural networks took a lot of inspiration from their biologicalcounterparts in becoming our best machine perceptual systems. This worksummarizes some of that history and incorporates modern theoreticalneuroscience into experiments with artificial neural networks from the field ofdeep learning. Specifically, iterative magnitude pruning is used to trainsparsely connected networks with 33x fewer weights without loss in performance.These are used to test and ultimately reject the hypothesis that weightsparsity alone improves image noise robustness. Recent work mitigatedcatastrophic forgetting using weight sparsity, activation sparsity, and activedendrite modeling. This paper replicates those findings, and extends the methodto train convolutional neural networks on a more challenging continual learningtask. The code has been made publicly available.\r2022-09-03\nMeta-Learning with Less Forgetting on Large-Scale Non-Stationary Task Distributions\nZhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Donglin Zhan, Tiehang Duan, Mingchen Gao\nabstract\rabstract: The paradigm of machine intelligence moves from purely supervised learning toa more practical scenario when many loosely related unlabeled data areavailable and labeled data is scarce. Most existing algorithms assume that theunderlying task distribution is stationary. Here we consider a more realisticand challenging setting in that task distributions evolve over time. We namethis problem as Semi-supervised meta-learning with Evolving Task diStributions,abbreviated as SETS. Two key challenges arise in this more realistic setting:(i) how to use unlabeled data in the presence of a large amount of unlabeledout-of-distribution (OOD) data; and (ii) how to prevent catastrophic forgettingon previously learned task distributions due to the task distribution shift. Wepropose an OOD Robust and knowleDge presErved semi-supeRvised meta-learningapproach (ORDER), to tackle these two major challenges. Specifically, our ORDERintroduces a novel mutual information regularization to robustify the modelwith unlabeled OOD data and adopts an optimal transport regularization toremember previously learned knowledge in feature space. In addition, we testour method on a very challenging dataset: SETS on large-scale non-stationarysemi-supervised task distributions consisting of (at least) 72K tasks. Withextensive experiments, we demonstrate the proposed ORDER alleviates forgettingon evolving task distributions and is more robust to OOD data than relatedstrong baselines.\r2022-09-02\nAn Introduction to Machine Unlearning\nSalvatore Mercuri, Raad Khraishi, Ramin Okhrati, Devesh Batra, Conor Hamill, Taha Ghasempour, Andrew Nowlan\nabstract\rabstract: Removing the influence of a specified subset of training data from a machinelearning model may be required to address issues such as privacy, fairness, anddata quality. Retraining the model from scratch on the remaining data afterremoval of the subset is an effective but often infeasible option, due to itscomputational expense. The past few years have therefore seen several novelapproaches towards efficient removal, forming the field of \u0026ldquo;machineunlearning\u0026rdquo;, however, many aspects of the literature published thus far aredisparate and lack consensus. In this paper, we summarise and compare sevenstate-of-the-art machine unlearning algorithms, consolidate definitions of coreconcepts used in the field, reconcile different approaches for evaluatingalgorithms, and discuss issues related to applying machine unlearning inpractice.\r2022-08-30\nBeyond Supervised Continual Learning: a Review\nBenedikt Bagus, Alexander Gepperth, Timothée Lesort\nabstract\rabstract: Continual Learning (CL, sometimes also termed incremental learning) is aflavor of machine learning where the usual assumption of stationary datadistribution is relaxed or omitted. When naively applying, e.g., DNNs in CLproblems, changes in the data distribution can cause the so-called catastrophicforgetting (CF) effect: an abrupt loss of previous knowledge. Although manysignificant contributions to enabling CL have been made in recent years, mostworks address supervised (classification) problems. This article reviewsliterature that study CL in other settings, such as learning with reducedsupervision, fully unsupervised learning, and reinforcement learning. Besidesproposing a simple schema for classifying CL approaches w.r.t. their level ofautonomy and supervision, we discuss the specific challenges associated witheach setting and the potential contributions to the field of CL in general.\r2022-08-14\nForgetting Fast in Recommender Systems\nWenyan Liu, Juncheng Wan, Xiaoling Wang, Weinan Zhang, Dell Zhang, Hang Li\nabstract\rabstract: Users of a recommender system may want part of their data being deleted, notonly from the data repository but also from the underlying machine learningmodel, for privacy or utility reasons. Such right-to-be-forgotten requestscould be fulfilled by simply retraining the recommendation model from scratch,but that would be too slow and too expensive in practice. In this paper, weinvestigate fast machine unlearning techniques for recommender systems that canremove the effect of a small amount of training data from the recommendationmodel without incurring the full cost of retraining. A natural idea to speedthis process up is to fine-tune the current recommendation model on theremaining training data instead of starting from a random initialization. Thiswarm-start strategy indeed works for neural recommendation models usingstandard 1st-order neural network optimizers (like AdamW). However, we havefound that even greater acceleration could be achieved by employing 2nd-order(Newton or quasi-Newton) optimization methods instead. To overcome theprohibitively high computational cost of 2nd-order optimizers, we propose a newrecommendation unlearning approach AltEraser which divides the optimizationproblem of unlearning into many small tractable sub-problems. Extensiveexperiments on three real-world recommendation datasets show promising resultsof AltEraser in terms of consistency (forgetting thoroughness), accuracy(recommendation effectiveness), and efficiency (unlearning speed). To ourknowledge, this work represents the first attempt at fast approximate machineunlearning for state-of-the-art neural recommendation models.\rLifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting\nAlexander Ororbia, Ankur Mali, Daniel Kifer, C. Lee Giles\nabstract\rabstract: In lifelong learning systems based on artificial neural networks, one of thebiggest obstacles is the inability to retain old knowledge as new informationis encountered. This phenomenon is known as catastrophic forgetting. In thispaper, we propose a new kind of connectionist architecture, the SequentialNeural Coding Network, that is robust to forgetting when learning from streamsof data points and, unlike networks of today, does not learn via the popularback-propagation of errors. Grounded in the neurocognitive theory of predictiveprocessing, our model adapts synapses in a biologically-plausible fashion whileanother neural system learns to direct and control this cortex-like structure,mimicking some of the task-executive control functionality of the basalganglia. In our experiments, we demonstrate that our self-organizing systemexperiences significantly less forgetting compared to standard neural models,outperforming a swath of previously proposed methods, including rehearsal/databuffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.)and custom benchmarks even though it is trained in a stream-like fashion. Ourwork offers evidence that emulating mechanisms in real neuronal systems, e.g.,local learning, lateral competition, can yield new directions and possibilitiesfor tackling the grand challenge of lifelong machine learning.\r2022-08-10\nContinual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation\nZhijing Wu, Hua Xu, Jingliang Fang, Kai Gao\nabstract\rabstract: Continual Machine Reading Comprehension aims to incrementally learn from acontinuous data stream across time without access the previous seen data, whichis crucial for the development of real-world MRC systems. However, it is agreat challenge to learn a new domain incrementally without catastrophicallyforgetting previous knowledge. In this paper, MA-MRC, a continual MRC modelwith uncertainty-aware fixed Memory and Adversarial domain adaptation, isproposed. In MA-MRC, a fixed size memory stores a small number of samples inprevious domain data along with an uncertainty-aware updating strategy when newdomain data arrives. For incremental learning, MA-MRC not only keeps a stableunderstanding by learning both memory and new domain data, but also makes fulluse of the domain adaptation relationship between them by adversarial learningstrategy. The experimental results show that MA-MRC is superior to strongbaselines and has a substantial incremental learning ability withoutcatastrophically forgetting under two different continual MRC settings.\r2022-07-22\nExplaining How Deep Neural Networks Forget by Deep Visualization\nGiang Nguyen, Shuan Chen, Tae Joon Jun, Daeyoung Kim\nabstract\rabstract: Explaining the behaviors of deep neural networks, usually considered as blackboxes, is critical especially when they are now being adopted over diverseaspects of human life. Taking the advantages of interpretable machine learning(interpretable ML), this paper proposes a novel tool called CatastrophicForgetting Dissector (or CFD) to explain catastrophic forgetting in continuallearning settings. We also introduce a new method called Critical Freezingbased on the observations of our tool. Experiments on ResNet articulate howcatastrophic forgetting happens, particularly showing which components of thisfamous network are forgetting. Our new continual learning algorithm defeatsvarious recent techniques by a significant margin, proving the capability ofthe investigation. Critical freezing not only attacks catastrophic forgettingbut also exposes explainability.\r2022-07-21\nNovel Class Discovery without Forgetting\nK J Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, Vineeth N Balasubramanian\nabstract\rabstract: Humans possess an innate ability to identify and differentiate instances thatthey are not familiar with, by leveraging and adapting the knowledge that theyhave acquired so far. Importantly, they achieve this without deteriorating theperformance on their earlier learning. Inspired by this, we identify andformulate a new, pragmatic problem setting of NCDwF: Novel Class Discoverywithout Forgetting, which tasks a machine learning model to incrementallydiscover novel categories of instances from unlabeled data, while maintainingits performance on the previously seen categories. We propose 1) a method togenerate pseudo-latent representations which act as a proxy for (no longeravailable) labeled data, thereby alleviating forgetting, 2) amutual-information based regularizer which enhances unsupervised discovery ofnovel classes, and 3) a simple Known Class Identifier which aids generalizedinference when the testing data contains instances form both seen and unseencategories. We introduce experimental protocols based on CIFAR-10, CIFAR-100and ImageNet-1000 to measure the trade-off between knowledge retention andnovel class discovery. Our extensive evaluations reveal that existing modelscatastrophically forget previously seen categories while identifying novelcategories, while our method is able to effectively balance between thecompeting objectives. We hope our work will attract further research into thisnewly identified pragmatic problem setting.\r2022-07-17\nFederated Learning and catastrophic forgetting in pervasive computing: demonstration in HAR domain\nAnastasiia Usmanova, François Portet, Philippe Lalanda, German Vega\nabstract\rabstract: Federated Learning has been introduced as a new machine learning paradigmenhancing the use of local devices. At a server level, FL regularly aggregatesmodels learned locally on distributed clients to obtain a more general model.In this way, no private data is sent over the network, and the communicationcost is reduced. However, current solutions rely on the availability of largeamounts of stored data at the client side in order to fine-tune the models sentby the server. Such setting is not realistic in mobile pervasive computingwhere data storage must be kept low and data characteristic (distribution) canchange dramatically. To account for this variability, a solution is to use thedata regularly collected by the client to progressively adapt the receivedmodel. But such naive approach exposes clients to the well-known problem ofcatastrophic forgetting. The purpose of this paper is to demonstrate thisproblem in the mobile human activity recognition context on smartphones.\rFederated Continual Learning through distillation in pervasive computing\nAnastasiia Usmanova, François Portet, Philippe Lalanda, German Vega\nabstract\rabstract: Federated Learning has been introduced as a new machine learning paradigmenhancing the use of local devices. At a server level, FL regularly aggregatesmodels learned locally on distributed clients to obtain a more general model.Current solutions rely on the availability of large amounts of stored data atthe client side in order to fine-tune the models sent by the server. Suchsetting is not realistic in mobile pervasive computing where data storage mustbe kept low and data characteristic can change dramatically. To account forthis variability, a solution is to use the data regularly collected by theclient to progressively adapt the received model. But such naive approachexposes clients to the well-known problem of catastrophic forgetting. Toaddress this problem, we have defined a Federated Continual Learning approachwhich is mainly based on distillation. Our approach allows a better use ofresources, eliminating the need to retrain from scratch at the arrival of newdata and reducing memory usage by limiting the amount of data to be stored.This proposal has been evaluated in the Human Activity Recognition (HAR) domainand has shown to effectively reduce the catastrophic forgetting effect.\r2022-07-15\nOnline Continual Learning for Embedded Devices\nTyler L. Hayes, Christopher Kanan\nabstract\rabstract: Real-time on-device continual learning is needed for new applications such ashome robots, user personalization on smartphones, and augmented/virtual realityheadsets. However, this setting poses unique challenges: embedded devices havelimited memory and compute capacity and conventional machine learning modelssuffer from catastrophic forgetting when updated on non-stationary datastreams. While several online continual learning models have been developed,their effectiveness for embedded applications has not been rigorously studied.In this paper, we first identify criteria that online continual learners mustmeet to effectively perform real-time, on-device learning. We then study theefficacy of several online continual learning methods when used with mobileneural networks. We measure their performance, memory usage, computerequirements, and ability to generalize to out-of-domain inputs.\r2022-07-13\nDeep Unlearning via Randomized Conditionally Independent Hessians\nRonak Mehta, Sourav Pal, Vikas Singh, Sathya N. Ravi\nabstract\rabstract: Recent legislation has led to interest in machine unlearning, i.e., removingspecific training samples from a predictive model as if they never existed inthe training dataset. Unlearning may also be required due tocorrupted/adversarial data or simply a user\u0026rsquo;s updated privacy requirement. Formodels which require no training (k-NN), simply deleting the closest originalsample can be effective. But this idea is inapplicable to models which learnricher representations. Recent ideas leveraging optimization-based updatesscale poorly with the model dimension d, due to inverting the Hessian of theloss function. We use a variant of a new conditional independence coefficient,L-CODEC, to identify a subset of the model parameters with the most semanticoverlap on an individual sample level. Our approach completely avoids the needto invert a (possibly) huge matrix. By utilizing a Markov blanket selection, wepremise that L-CODEC is also suitable for deep unlearning, as well as otherapplications in vision. Compared to alternatives, L-CODEC makes approximateunlearning possible in settings that would otherwise be infeasible, includingvision models used for face recognition, person re-identification and NLPmodels that may require unlearning samples identified for exclusion. Code canbe found at https://github.com/vsingh-group/LCODEC-deep-unlearning/\r2022-07-12\nContinual Learning with Deep Learning Methods in an Application-Oriented Context\nBenedikt Pfülb\nabstract\rabstract: Abstract knowledge is deeply grounded in many computer-based applications. Animportant research area of Artificial Intelligence (AI) deals with theautomatic derivation of knowledge from data. Machine learning offers theaccording algorithms. One area of research focuses on the development ofbiologically inspired learning algorithms. The respective machine learningmethods are based on neurological concepts so that they can systematicallyderive knowledge from data and store it. One type of machine learningalgorithms that can be categorized as \u0026ldquo;deep learning\u0026rdquo; model is referred to asDeep Neural Networks (DNNs). DNNs consist of multiple artificial neuronsarranged in layers that are trained by using the backpropagation algorithm.These deep learning methods exhibit amazing capabilities for inferring andstoring complex knowledge from high-dimensional data. However, DNNs areaffected by a problem that prevents new knowledge from being added to anexisting base. The ability to continuously accumulate knowledge is an importantfactor that contributed to evolution and is therefore a prerequisite for thedevelopment of strong AIs. The so-called \u0026ldquo;catastrophic forgetting\u0026rdquo; (CF) effectcauses DNNs to immediately loose already derived knowledge after a few trainingiterations on a new data distribution. Only an energetically expensiveretraining with the joint data distribution of past and new data enables theabstraction of the entire new set of knowledge. In order to counteract theeffect, various techniques have been and are still being developed with thegoal to mitigate or even solve the CF problem. These published CF avoidancestudies usually imply the effectiveness of their approaches for variouscontinual learning tasks. This dissertation is set in the context of continualmachine learning with deep learning methods. The first part deals with thedevelopment of an \u0026hellip;\r2022-06-28\nContinual Learning with Transformers for Image Classification\nBeyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, Cedric Archambeau\nabstract\rabstract: In many real-world scenarios, data to train machine learning models becomeavailable over time. However, neural network models struggle to continuallylearn new concepts without forgetting what has been learnt in the past. Thisphenomenon is known as catastrophic forgetting and it is often difficult toprevent due to practical constraints, such as the amount of data that can bestored or the limited computation sources that can be used. Moreover, traininglarge neural networks, such as Transformers, from scratch is very costly andrequires a vast amount of training data, which might not be available in theapplication domain of interest. A recent trend indicates that dynamicarchitectures based on an expansion of the parameters can reduce catastrophicforgetting efficiently in continual learning, but this needs complex tuning tobalance the growing number of parameters and barely share any informationacross tasks. As a result, they struggle to scale to a large number of taskswithout significant overhead. In this paper, we validate in the computer visiondomain a recent solution called Adaptive Distillation of Adapters (ADA), whichis developed to perform continual learning using pre-trained Transformers andAdapters on text classification tasks. We empirically demonstrate on differentclassification tasks that this method maintains a good predictive performancewithout retraining the model or increasing the number of model parameters overthe time. Besides it is significantly faster at inference time compared to thestate-of-the-art methods.\rShort-Term Plasticity Neurons Learning to Learn and Forget\nHector Garcia Rodriguez, Qinghai Guo, Timoleon Moraitis\nabstract\rabstract: Short-term plasticity (STP) is a mechanism that stores decaying memories insynapses of the cerebral cortex. In computing practice, STP has been used, butmostly in the niche of spiking neurons, even though theory predicts that it isthe optimal solution to certain dynamic tasks. Here we present a new type ofrecurrent neural unit, the STP Neuron (STPN), which indeed turns out strikinglypowerful. Its key mechanism is that synapses have a state, propagated throughtime by a self-recurrent connection-within-the-synapse. This formulationenables training the plasticity with backpropagation through time, resulting ina form of learning to learn and forget in the short term. The STPN outperformsall tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, anddifferentiable plasticity. We confirm this in both supervised and reinforcementlearning (RL), and in tasks such as Associative Retrieval, Maze Exploration,Atari video games, and MuJoCo robotics. Moreover, we calculate that, inneuromorphic or biological circuits, the STPN minimizes energy consumptionacross models, as it depresses individual synapses dynamically. Based on these,biological STP may have been a strong evolutionary attractor that maximizesboth efficiency and computational power. The STPN now brings these neuromorphicadvantages also to a broad spectrum of machine learning practice. Code isavailable at https://github.com/NeuromorphicComputing/stpn\r2022-06-27\nContinual Learning of Dynamical Systems with Competitive Federated Reservoir Computing\nLeonard Bereska, Efstratios Gavves\nabstract\rabstract: Machine learning recently proved efficient in learning differential equationsand dynamical systems from data. However, the data is commonly assumed tooriginate from a single never-changing system. In contrast, when modelingreal-world dynamical processes, the data distribution often shifts due tochanges in the underlying system dynamics. Continual learning of theseprocesses aims to rapidly adapt to abrupt system changes without forgettingprevious dynamical regimes. This work proposes an approach to continuallearning based on reservoir computing, a state-of-the-art method for trainingrecurrent neural networks on complex spatiotemporal dynamical systems.Reservoir computing fixes the recurrent network weights - hence these cannot beforgotten - and only updates linear projection heads to the output. We proposeto train multiple competitive prediction heads concurrently. Inspired byneuroscience\u0026rsquo;s predictive coding, only the most predictive heads activate,laterally inhibiting and thus protecting the inactive heads from forgettinginduced by interfering parameter updates. We show that this multi-headreservoir minimizes interference and catastrophic forgetting on severaldynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenzattractor, and the high-dimensional Lorenz-96 weather model. Our resultssuggest that reservoir computing is a promising candidate framework for thecontinual learning of dynamical systems. We provide our code for datageneration, method, and comparisons at\\url{https://github.com/leonardbereska/multiheadreservoir}.\rTransfer Learning via Test-Time Neural Networks Aggregation\nBruno Casella, Alessio Barbaro Chisari, Sebastiano Battiato, Mario Valerio Giuffrida\nabstract\rabstract: It has been demonstrated that deep neural networks outperform traditionalmachine learning. However, deep networks lack generalisability, that is, theywill not perform as good as in a new (testing) set drawn from a differentdistribution due to the domain shift. In order to tackle this known issue,several transfer learning approaches have been proposed, where the knowledge ofa trained model is transferred into another to improve performance withdifferent data. However, most of these approaches require additional trainingsteps, or they suffer from catastrophic forgetting that occurs when a trainedmodel has overwritten previously learnt knowledge. We address both problemswith a novel transfer learning approach that uses network aggregation. We traindataset-specific networks together with an aggregation network in a unifiedframework. The loss function includes two main components: a task-specific loss(such as cross-entropy) and an aggregation loss. The proposed aggregation lossallows our model to learn how trained deep network parameters can be aggregatedwith an aggregation operator. We demonstrate that the proposed approach learnsmodel aggregation at test time without any further training step, reducing theburden of transfer learning to a simple arithmetical operation. The proposedapproach achieves comparable performance w.r.t. the baseline. Besides, if theaggregation operator has an inverse, we will show that our model alsoinherently allows for selective forgetting, i.e., the aggregated model canforget one of the datasets it was trained on, retaining information on theothers.\r2022-06-23\nUnsupervised Learning Algorithms for Keyword Extraction in an Undergraduate Thesis\nFred Torres-Cruz, Edelfre Flores, William E. Arcaya, Irenio L. Chagua, Marga I. Ingaluque\nabstract\rabstract: The amount of data managed in many academic institutions has increased inrecent years, particularly in all the research work done by undergraduatestudents, who simply use empirical techniques for keyword selection, forgettingexisting technical methods to assist their students in this process.Information and communication technologies, such as the platform for integratedresearch and academic work with responsibility (PILAR), which recordsinformation about research projects, such as titles, summaries, and keywords intheir various modalities, have gained relevance and importance in themanagement of these. We proved algorithms with these records of researchprojects that have been analysed in this study, and predictions were made foreach of the nine (09) models of unsupervised machine learning algorithms thatwere implemented for each of the 7430 records from the dataset. The mostefficient way of extracting keywords for this dataset was the TF-IDF method,obtaining 72% accuracy and [0.4786, SD 0.0501] in average extraction time foreach thesis file processed by this model.\r2022-06-22\nThe Privacy Onion Effect: Memorization is Relative\nNicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, Florian Tramer\nabstract\rabstract: Machine learning models trained on private datasets have been shown to leaktheir private data. While recent work has found that the average data point israrely leaked, the outlier samples are frequently subject to memorization and,consequently, privacy leakage. We demonstrate and analyse an Onion Effect ofmemorization: removing the \u0026ldquo;layer\u0026rdquo; of outlier points that are most vulnerableto a privacy attack exposes a new layer of previously-safe points to the sameattack. We perform several experiments to study this effect, and understand whyit occurs. The existence of this effect has various consequences. For example,it suggests that proposals to defend against memorization without training withrigorous privacy guarantees are unlikely to be effective. Further, it suggeststhat privacy-enhancing technologies such as machine unlearning could actuallyharm the privacy of other users.\r2022-06-20\nTowards Making the Most of BERT in Neural Machine Translation\nJiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Yong Yu, Weinan Zhang, Lei Li\nabstract\rabstract: GPT-2 and BERT demonstrate the effectiveness of using pre-trained languagemodels (LMs) on various natural language processing tasks. However, LMfine-tuning often suffers from catastrophic forgetting when applied toresource-rich tasks. In this work, we introduce a concerted training framework(CTNMT) that is the key to integrate the pre-trained LMs to neural machinetranslation (NMT). Our proposed CTNMT consists of three techniques: a)asymptotic distillation to ensure that the NMT model can retain the previouspre-trained knowledge; b) a dynamic switching gate to avoid catastrophicforgetting of pre-trained knowledge; and c) a strategy to adjust the learningpaces according to a scheduled policy. Our experiments in machine translationshow CTNMT gains of up to 3 BLEU score on the WMT14 English-German languagepair which even surpasses the previous state-of-the-art pre-training aided NMTby 1.4 BLEU score. While for the large WMT14 English-French task with 40millions of sentence-pairs, our base model still significantly improves uponthe state-of-the-art Transformer big model by more than 1 BLEU score. The codeand model can be downloaded from https://github.com/bytedance/neurst/tree/master/examples/ctnmt.\rControlling Conditional Language Models without Catastrophic Forgetting\nTomasz Korbak, Hady Elsahar, German Kruszewski, Marc Dymetman\nabstract\rabstract: Machine learning is shifting towards general-purpose pretrained generativemodels, trained in a self-supervised manner on large amounts of data, which canthen be applied to solve a large number of tasks. However, due to their generictraining methodology, these models often fail to meet some of the downstreamrequirements (e.g., hallucinations in abstractive summarization or styleviolations in code generation). This raises the important question of how toadapt pre-trained generative models to meet all requirements without destroyingtheir general capabilities (\u0026ldquo;catastrophic forgetting\u0026rdquo;). Recent work hasproposed to solve this problem by representing task-specific requirementsthrough energy-based models (EBMs) and approximating these EBMs usingdistributional policy gradients (DPG). Despite its effectiveness, this approachis however limited to unconditional distributions. In this paper, we extend DPGto conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG onfour different control objectives across three tasks (translation,summarization and code generation) and two pretrained models (T5 and GPT-Neo).Our results show that fine-tuning using CDPG robustly moves these pretrainedmodels closer towards meeting control objectives and \u0026ndash; in contrast withbaseline approaches \u0026ndash; does not result in catastrophic forgetting.\r2022-06-15\nPoison Forensics: Traceback of Data Poisoning Attacks in Neural Networks\nShawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao\nabstract\rabstract: In adversarial machine learning, new defenses against attacks on deeplearning systems are routinely broken soon after their release by more powerfulattacks. In this context, forensic tools can offer a valuable complement toexisting defenses, by tracing back a successful attack to its root cause, andoffering a path forward for mitigation to prevent similar attacks in thefuture. In this paper, we describe our efforts in developing a forensic tracebacktool for poison attacks on deep neural networks. We propose a novel iterativeclustering and pruning solution that trims \u0026ldquo;innocent\u0026rdquo; training samples, untilall that remains is the set of poisoned data responsible for the attack. Ourmethod clusters training samples based on their impact on model parameters,then uses an efficient data unlearning method to prune innocent clusters. Weempirically demonstrate the efficacy of our system on three types ofdirty-label (backdoor) poison attacks and three types of clean-label poisonattacks, across domains of computer vision and malware classification. Oursystem achieves over 98.4% precision and 96.8% recall across all attacks. Wealso show that our system is robust against four anti-forensics measuresspecifically designed to attack it.\r2022-06-10\nMembership Inference via Backdooring\nHongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang\nabstract\rabstract: Recently issued data privacy regulations like GDPR (General Data ProtectionRegulation) grant individuals the right to be forgotten. In the context ofmachine learning, this requires a model to forget about a training data sampleif requested by the data owner (i.e., machine unlearning). As an essential stepprior to machine unlearning, it is still a challenge for a data owner to tellwhether or not her data have been used by an unauthorized party to train amachine learning model. Membership inference is a recently emerging techniqueto identify whether a data sample was used to train a target model, and seemsto be a promising solution to this challenge. However, straightforward adoptionof existing membership inference approaches fails to address the challengeeffectively due to being originally designed for attacking membership privacyand suffering from several severe limitations such as low inference accuracy onwell-generalized models. In this paper, we propose a novel membership inferenceapproach inspired by the backdoor technology to address the said challenge.Specifically, our approach of Membership Inference via Backdooring (MIB)leverages the key observation that a backdoored model behaves very differentlyfrom a clean model when predicting on deliberately marked samples created by adata owner. Appealingly, MIB requires data owners\u0026rsquo; marking a small number ofsamples for membership inference and only black-box access to the target model,with theoretical guarantees for inference results. We perform extensiveexperiments on various datasets and deep neural network architectures, and theresults validate the efficacy of our approach, e.g., marking only 0.1% of thetraining dataset is practically sufficient for effective membership inference.\r2022-06-08\nQuantum continual learning of quantum data realizing knowledge backward transfer\nHaozhen Situ, Tianxiang Lu, Minghua Pan, Lvzhou Li\nabstract\rabstract: For the goal of strong artificial intelligence that can mimic human-levelintelligence, AI systems would have the ability to adapt to ever-changingscenarios and learn new knowledge continuously without forgetting previouslyacquired knowledge. When a machine learning model is consecutively trained onmultiple tasks that come in sequence, its performance on previously learnedtasks may drop dramatically during the learning process of the newly seen task.To avoid this phenomenon termed catastrophic forgetting, continual learning,also known as lifelong learning, has been proposed and become one of the mostup-to-date research areas of machine learning. As quantum machine learningblossoms in recent years, it is interesting to develop quantum continuallearning. This paper focuses on the case of quantum models for quantum datawhere the computation model and the data to be processed are both quantum. Thegradient episodic memory method is incorporated to design a quantum continuallearning scheme that overcomes catastrophic forgetting and realizes knowledgebackward transfer. Specifically, a sequence of quantum state classificationtasks is continually learned by a variational quantum classifier whoseparameters are optimized by a classical gradient-based optimizer. The gradientof the current task is projected to the closest gradient, avoiding the increaseof the loss at previous tasks, but allowing the decrease. Numerical simulationresults show that our scheme not only overcomes catastrophic forgetting, butalso realize knowledge backward transfer, which means the classifier\u0026rsquo;sperformance on previous tasks is enhanced rather than compromised whilelearning a new task.\r2022-05-25\nSemi-supervised Drifted Stream Learning with Short Lookback\nWeijieying Ren, Pengyang Wang, Xiaolin Li, Charles E. Hughes, Yanjie Fu\nabstract\rabstract: In many scenarios, 1) data streams are generated in real time; 2) labeleddata are expensive and only limited labels are available in the beginning; 3)real-world data is not always i.i.d. and data drift over time gradually; 4) thestorage of historical streams is limited and model updating can only beachieved based on a very short lookback window. This learning setting limitsthe applicability and availability of many Machine Learning (ML) algorithms. Wegeneralize the learning task under such setting as a semi-supervised driftedstream learning with short lookback problem (SDSL). SDSL imposes twounder-addressed challenges on existing methods in semi-supervised learning,continuous learning, and domain adaptation: 1) robust pseudo-labeling undergradual shifts and 2) anti-forgetting adaptation with short lookback. To tacklethese challenges, we propose a principled and generic generation-replayframework to solve SDSL. The framework is able to accomplish: 1) robustpseudo-labeling in the generation step; 2) anti-forgetting adaption in thereplay step. To achieve robust pseudo-labeling, we develop a novel pseudo-labelclassification model to leverage supervised knowledge of previously labeleddata, unsupervised knowledge of new data, and, structure knowledge of invariantlabel semantics. To achieve adaptive anti-forgetting model replay, we proposeto view the anti-forgetting adaptation task as a flat region search problem. Wepropose a novel minimax game-based replay objective function to solve the flatregion search problem and develop an effective optimization solver. Finally, wepresent extensive experiments to demonstrate our framework can effectivelyaddress the task of anti-forgetting learning in drifted streams with shortlookback.\rmuNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems\nAndrea Gesmundo, Jeff Dean\nabstract\rabstract: Most uses of machine learning today involve training a model from scratch fora particular task, or sometimes starting with a model pretrained on a relatedtask and then fine-tuning on a downstream task. Both approaches offer limitedknowledge transfer between different tasks, time-consuming human-drivencustomization to individual tasks and high computational costs especially whenstarting from randomly initialized models. We propose a method that uses thelayers of a pretrained deep neural network as building blocks to construct anML system that can jointly solve an arbitrary number of tasks. The resultingsystem can leverage cross tasks knowledge transfer, while being immune fromcommon drawbacks of multitask approaches such as catastrophic forgetting,gradients interference and negative transfer. We define an evolutionaryapproach designed to jointly select the prior knowledge relevant for each task,choose the subset of the model parameters to train and dynamically auto-tuneits hyperparameters. Furthermore, a novel scale control method is employed toachieve quality/size trade-offs that outperform common fine-tuning techniques.Compared with standard fine-tuning on a benchmark of 10 diverse imageclassification tasks, the proposed model improves the average accuracy by 2.39%while using 47% less parameters per task.\r2022-05-20\nContinual learning on 3D point clouds with random compressed rehearsal\nMaciej Zamorski, Michał Stypułkowski, Konrad Karanowski, Tomasz Trzciński, Maciej Zięba\nabstract\rabstract: Contemporary deep neural networks offer state-of-the-art results when appliedto visual reasoning, e.g., in the context of 3D point cloud data. Point cloudsare important datatype for precise modeling of three-dimensional environments,but effective processing of this type of data proves to be challenging. In theworld of large, heavily-parameterized network architectures andcontinuously-streamed data, there is an increasing need for machine learningmodels that can be trained on additional data. Unfortunately, currentlyavailable models cannot fully leverage training on additional data withoutlosing their past knowledge. Combating this phenomenon, called catastrophicforgetting, is one of the main objectives of continual learning. Continuallearning for deep neural networks has been an active field of research,primarily in 2D computer vision, natural language processing, reinforcementlearning, and robotics. However, in 3D computer vision, there are hardly anycontinual learning solutions specifically designed to take advantage of pointcloud structure. This work proposes a novel neural network architecture capableof continual learning on 3D point cloud data. We utilize point cloud structureproperties for preserving a heavily compressed set of past data. By usingrehearsal and reconstruction as regularization methods of the learning process,our approach achieves a significant decrease of catastrophic forgettingcompared to the existing solutions on several most popular point cloud datasetsconsidering two continual learning settings: when a task is known beforehand,and in the challenging scenario of when task information is unknown to themodel.\r2022-05-19\nContinual Pre-Training Mitigates Forgetting in Language and Vision\nAndrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, Davide Bacciu\nabstract\rabstract: Pre-trained models are nowadays a fundamental component of machine learningresearch. In continual learning, they are commonly used to initialize the modelbefore training on the stream of non-stationary data. However, pre-training israrely applied during continual learning. We formalize and investigate thecharacteristics of the continual pre-training scenario in both language andvision environments, where a model is continually pre-trained on a stream ofincoming data and only later fine-tuned to different downstream tasks. We showthat continually pre-trained models are robust against catastrophic forgettingand we provide strong empirical evidence supporting the fact thatself-supervised pre-training is more effective in retaining previous knowledgethan supervised protocols. Code is provided athttps://github.com/AndreaCossu/continual-pretraining-nlp-vision .\rIncremental Learning with Differentiable Architecture and Forgetting Search\nJames Seale Smith, Zachary Seymour, Han-Pang Chiu\nabstract\rabstract: As progress is made on training machine learning models on incrementallyexpanding classification tasks (i.e., incremental learning), a next step is totranslate this progress to industry expectations. One technique missing fromincremental learning is automatic architecture design via Neural ArchitectureSearch (NAS). In this paper, we show that leveraging NAS for incrementallearning results in strong performance gains for classification tasks.Specifically, we contribute the following: first, we create a strong baselineapproach for incremental learning based on Differentiable Architecture Search(DARTS) and state-of-the-art incremental learning strategies, outperformingmany existing strategies trained with similar-sized popular architectures;second, we extend the idea of architecture search to regularize architectureforgetting, boosting performance past our proposed baseline. We evaluate ourmethod on both RF signal and image classification tasks, and demonstrate we canachieve up to a 10% performance increase over state-of-the-art methods. Mostimportantly, our contribution enables learning from continuous distributions onreal-world application data for which the complexity of the data distributionis unknown, or the modality less explored (such as RF signal classification).\r2022-05-12\nHow to Combine Membership-Inference Attacks on Multiple Updated Models\nMatthew Jagielski, Stanley Wu, Alina Oprea, Jonathan Ullman, Roxana Geambasu\nabstract\rabstract: A large body of research has shown that machine learning models arevulnerable to membership inference (MI) attacks that violate the privacy of theparticipants in the training data. Most MI research focuses on the case of asingle standalone model, while production machine-learning platforms oftenupdate models over time, on data that often shifts in distribution, giving theattacker more information. This paper proposes new attacks that take advantageof one or more model updates to improve MI. A key part of our approach is toleverage rich information from standalone MI attacks mounted separately againstthe original and updated models, and to combine this information in specificways to improve attack effectiveness. We propose a set of combination functionsand tuning methods for each, and present both analytical and quantitativejustification for various options. Our results on four public datasets showthat our attacks are effective at using update information to give theadversary a significant advantage over attacks on standalone models, but alsocompared to a prior MI attack that takes advantage of model updates in arelated machine-unlearning setting. We perform the first measurements of theimpact of distribution shift on MI attacks with model updates, and show that amore drastic distribution shift results in significantly higher MI risk than agradual shift. Our code is available athttps://www.github.com/stanleykywu/model-updates.\r2022-04-29\nSmaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning\nFrancesco Pelosin, Andrea Torsello\nabstract\rabstract: The design of machines and algorithms capable of learning in a dynamicallychanging environment has become an increasingly topical problem with theincrease of the size and heterogeneity of data available to learning systems.As a consequence, the key issue of Continual Learning has become that ofaddressing the stability-plasticity dilemma of connectionist systems, as theyneed to adapt their model without forgetting previously acquired knowledge.Within this context, rehearsal-based methods i.e., solutions in where thelearner exploits memory to revisit past data, has proven to be very effective,leading to performance at the state-of-the-art. In our study, we propose ananalysis of the memory quantity/quality trade-off adopting various datareduction approaches to increase the number of instances storable in memory. Inparticular, we investigate complex instance compression techniques such as deepencoders, but also trivial approaches such as image resizing and lineardimensionality reduction. Our findings suggest that the optimal trade-off isseverely skewed toward instance quantity, where rehearsal approaches withseveral heavily compressed instances easily outperform state-of-the-artapproaches with the same amount of memory at their disposal. Further, in highmemory configurations, deep approaches extracting spatial structure combinedwith extreme resizing (of the order of $8\\times8$ images) yield the bestresults, while in memory-constrained configurations where deep approachescannot be used due to their memory requirement in training, Extreme LearningMachines (ELM) offer a clear advantage.\r2022-04-28\nFoundations for learning from noisy quantum experiments\nHsin-Yuan Huang, Steven T. Flammia, John Preskill\nabstract\rabstract: Understanding what can be learned from experiments is central to scientificprogress. In this work, we use a learning-theoretic perspective to study thetask of learning physical operations in a quantum machine when all operations(state preparation, dynamics, and measurement) are a priori unknown. We provethat, without any prior knowledge, if one can explore the full quantum statespace by composing the operations, then every operation can be learned. Whenone cannot explore the full state space but all operations are approximatelyknown and noise in Clifford gates is gate-independent, we find an efficientalgorithm for learning all operations up to a single unlearnable parametercharacterizing the fidelity of the initial state. For learning a noise channelon Clifford gates to a fixed accuracy, our algorithm uses quadratically fewerexperiments than previously known protocols. Under more general conditions, thetrue description of the noise can be unlearnable; for example, we prove that nobenchmarking protocol can learn gate-dependent Pauli noise on Clifford+T gateseven under perfect state preparation and measurement. Despite not being able tolearn the noise, we show that a noisy quantum computer that performs entangledmeasurements on multiple copies of an unknown state can yield a large advantagein learning properties of the state compared to a noiseless device thatmeasures individual copies and then processes the measurement data using aclassical computer. Concretely, we prove that noisy quantum computers withtwo-qubit gate error rate $\\epsilon$ can achieve a learning task using $N$copies of the state, while $N^{\\Omega(1/\\epsilon)}$ copies are requiredclassically.\rContinual learning-based probabilistic slow feature analysis for multimode dynamic process monitoring\nJingxin Zhang, Donghua Zhou, Maoyin Chen, Xia Hong\nabstract\rabstract: In this paper, a novel multimode dynamic process monitoring approach isproposed by extending elastic weight consolidation (EWC) to probabilistic slowfeature analysis (PSFA) in order to extract multimode slow features for onlinemonitoring. EWC was originally introduced in the setting of machine learning ofsequential multi-tasks with the aim of avoiding catastrophic forgetting issue,which equally poses as a major challenge in multimode dynamic processmonitoring. When a new mode arrives, a set of data should be collected so thatthis mode can be identified by PSFA and prior knowledge. Then, a regularizationterm is introduced to prevent new data from significantly interfering with thelearned knowledge, where the parameter importance measures are estimated. Theproposed method is denoted as PSFA-EWC, which is updated continually andcapable of achieving excellent performance for successive modes. Different fromtraditional multimode monitoring algorithms, PSFA-EWC furnishes backward andforward transfer ability. The significant features of previous modes areretained while consolidating new information, which may contribute to learningnew relevant modes. Compared with several known methods, the effectiveness ofthe proposed method is demonstrated via a continuous stirred tank heater and apractical coal pulverizing system.\r2022-04-16\nEfficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations\nTao Guo, Song Guo, Jiewei Zhang, Wenchao Xu, Junxiao Wang\nabstract\rabstract: Recently, the enactment of privacy regulations has promoted the rise of themachine unlearning paradigm. Existing studies of machine unlearning mainlyfocus on sample-wise unlearning, such that a learnt model will not exposeuser\u0026rsquo;s privacy at the sample level. Yet we argue that such ability of selectiveremoval should also be presented at the attribute level, especially for theattributes irrelevant to the main task, e.g., whether a person recognized in aface recognition system wears glasses or the age range of that person. Througha comprehensive literature review, it is found that existing studies onattribute-related problems like fairness and de-biasing learning cannot addressthe above concerns properly. To bridge this gap, we propose a paradigm ofselectively removing input attributes from feature representations which wename `attribute unlearning\u0026rsquo;. In this paradigm, certain attributes will beaccurately captured and detached from the learned feature representations atthe stage of training, according to their mutual information. The particularattributes will be progressively eliminated along with the training proceduretowards convergence, while the rest of attributes related to the main task arepreserved for achieving competitive model performance. Considering thecomputational complexity during the training process, we not only give atheoretically approximate training method, but also propose an accelerationscheme to speed up the training process. We validate our method by spanningseveral datasets and models and demonstrate that our design can preserve modelfidelity and reach prevailing unlearning efficacy with high efficiency. Theproposed unlearning paradigm builds a foundation for future machine unlearningsystem and will become an essential component of the latest privacy-relatedlegislation.\r2022-04-13\nRethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning\nLiangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, Daniel Rubin\nabstract\rabstract: Federated learning is an emerging research paradigm enabling collaborativetraining of machine learning models among different organizations while keepingdata private at each institution. Despite recent progress, there remainfundamental challenges such as the lack of convergence and the potential forcatastrophic forgetting across real-world heterogeneous devices. In this paper,we demonstrate that self-attention-based architectures (e.g., Transformers) aremore robust to distribution shifts and hence improve federated learning overheterogeneous data. Concretely, we conduct the first rigorous empiricalinvestigation of different neural architectures across a range of federatedalgorithms, real-world benchmarks, and heterogeneous data splits. Ourexperiments show that simply replacing convolutional networks with Transformerscan greatly reduce catastrophic forgetting of previous devices, accelerateconvergence, and reach a better global model, especially when dealing withheterogeneous data. We release our code and pretrained models athttps://github.com/Liangqiong/ViT-FL-main to encourage future exploration inrobust architectures as an alternative to current research efforts on theoptimization front.\rSapinet: A sparse event-based spatiotemporal oscillator for learning in the wild\nAyon Borthakur\nabstract\rabstract: We introduce Sapinet \u0026ndash; a spike timing (event)-based multilayer neuralnetwork for \\textit{learning in the wild} \u0026ndash; that is: one-shot online learningof multiple inputs without catastrophic forgetting, and without the need fordata-specific hyperparameter retuning. Key features of Sapinet include dataregularization, model scaling, data classification, and denoising. The modelalso supports stimulus similarity mapping. We propose a systematic method totune the network for performance. We studied the model performance on differentlevels of odor similarity, gaussian and impulse noise. Sapinet achieved highclassification accuracies on standard machine olfaction datasets without therequirement of fine tuning for a specific dataset.\r2022-03-28\nGradient-Matching Coresets for Rehearsal-Based Continual Learning\nLukas Balles, Giovanni Zappella, Cédric Archambeau\nabstract\rabstract: The goal of continual learning (CL) is to efficiently update a machinelearning model with new data without forgetting previously-learned knowledge.Most widely-used CL methods rely on a rehearsal memory of data points to bereused while training on new data. Curating such a rehearsal memory to maintaina small, informative subset of all the data seen so far is crucial to thesuccess of these methods. We devise a coreset selection method forrehearsal-based continual learning. Our method is based on the idea of gradientmatching: The gradients induced by the coreset should match, as closely aspossible, those induced by the original training dataset. Inspired by theneural tangent kernel theory, we perform this gradient matching across themodel\u0026rsquo;s initialization distribution, allowing us to extract a coreset withouthaving to train the model first. We evaluate the method on a wide range ofcontinual learning scenarios and demonstrate that it improves the performanceof rehearsal-based CL methods compared to competing memory managementstrategies such as reservoir sampling.\r2022-03-27\nContinual learning: a feature extraction formalization, an efficient algorithm, and fundamental obstructions\nBinghui Peng, Andrej Risteski\nabstract\rabstract: Continual learning is an emerging paradigm in machine learning, wherein amodel is exposed in an online fashion to data from multiple differentdistributions (i.e. environments), and is expected to adapt to the distributionchange. Precisely, the goal is to perform well in the new environment, whilesimultaneously retaining the performance on the previous environments (i.e.avoid \u0026ldquo;catastrophic forgetting\u0026rdquo;) \u0026ndash; without increasing the size of the model. While this setup has enjoyed a lot of attention in the applied community,there hasn\u0026rsquo;t be theoretical work that even formalizes the desired guarantees.In this paper, we propose a framework for continual learning through theframework of feature extraction \u0026ndash; namely, one in which features, as well as aclassifier, are being trained with each environment. When the features arelinear, we design an efficient gradient-based algorithm $\\mathsf{DPGD}$, thatis guaranteed to perform well on the current environment, as well as avoidcatastrophic forgetting. In the general case, when the features are non-linear,we show such an algorithm cannot exist, whether efficient or not.\r2022-03-25\nContinual Test-Time Domain Adaptation\nQin Wang, Olga Fink, Luc Van Gool, Dengxin Dai\nabstract\rabstract: Test-time domain adaptation aims to adapt a source pre-trained model to atarget domain without using any source data. Existing works mainly consider thecase where the target domain is static. However, real-world machine perceptionsystems are running in non-stationary and continually changing environmentswhere the target domain distribution can change over time. Existing methods,which are mostly based on self-training and entropy regularization, can sufferfrom these non-stationary environments. Due to the distribution shift over timein the target domain, pseudo-labels become unreliable. The noisy pseudo-labelscan further lead to error accumulation and catastrophic forgetting. To tacklethese issues, we propose a continual test-time adaptation approach~(CoTTA)which comprises two parts. Firstly, we propose to reduce the error accumulationby using weight-averaged and augmentation-averaged predictions which are oftenmore accurate. On the other hand, to avoid catastrophic forgetting, we proposeto stochastically restore a small part of the neurons to the source pre-trainedweights during each iteration to help preserve source knowledge in thelong-term. The proposed method enables the long-term adaptation for allparameters in the network. CoTTA is easy to implement and can be readilyincorporated in off-the-shelf pre-trained models. We demonstrate theeffectiveness of our approach on four classification tasks and a segmentationtask for continual test-time adaptation, on which we outperform existingmethods. Our code is available at \\url{https://qin.ee/cotta}.\rPredicting Clinical Intent from Free Text Electronic Health Records\nKawsar Noor, Katherine Smith, Julia Bennett, Jade OConnell, Jessica Fisk, Monika Hunt, Gary Philippo, Teresa Xu, Simon Knight, Luis Romao, Richard JB Dobson, Wai Keong Wong\nabstract\rabstract: After a patient consultation, a clinician determines the steps in themanagement of the patient. A clinician may for example request to see thepatient again or refer them to a specialist. Whilst most clinicians will recordtheir intent as \u0026ldquo;next steps\u0026rdquo; in the patient\u0026rsquo;s clinical notes, in some cases theclinician may forget to indicate their intent as an order or request, e.g.failure to place the follow-up order. This consequently results in patientsbecoming lost-to-follow up and may in some cases lead to adverse consequences.In this paper we train a machine learning model to detect a clinician\u0026rsquo;s intentto follow up with a patient from the patient\u0026rsquo;s clinical notes. Annotatorssystematically identified 22 possible types of clinical intent and annotated3000 Bariatric clinical notes. The annotation process revealed a classimbalance in the labeled data and we found that there was only sufficientlabeled data to train 11 out of the 22 intents. We used the data to train aBERT based multilabel classification model and reported the following averageaccuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,macro-f1: 0.90.\r2022-03-24\nKnowledge Removal in Sampling-based Bayesian Inference\nShaopeng Fu, Fengxiang He, Dacheng Tao\nabstract\rabstract: The right to be forgotten has been legislated in many countries, but itsenforcement in the AI industry would cause unbearable costs. When single datadeletion requests come, companies may need to delete the whole models learnedwith massive resources. Existing works propose methods to remove knowledgelearned from data for explicitly parameterized models, which however are notappliable to the sampling-based Bayesian inference, i.e., Markov chain MonteCarlo (MCMC), as MCMC can only infer implicit distributions. In this paper, wepropose the first machine unlearning algorithm for MCMC. We first convert theMCMC unlearning problem into an explicit optimization problem. Based on thisproblem conversion, an {\\it MCMC influence function} is designed to provablycharacterize the learned knowledge from data, which then delivers the MCMCunlearning algorithm. Theoretical analysis shows that MCMC unlearning would notcompromise the generalizability of the MCMC models. Experiments on Gaussianmixture models and Bayesian neural networks confirm the effectiveness of theproposed algorithm. The code is available at\\url{https://github.com/fshp971/mcmc-unlearning}.\r2022-03-22\nMaking Recommender Systems Forget: Learning and Unlearning for Erasable Recommendation\nYuyuan Li, Xiaolin Zheng, Chaochao Chen, Junlin Liu\nabstract\rabstract: Privacy laws and regulations enforce data-driven systems, e.g., recommendersystems, to erase the data that concern individuals. As machine learning modelspotentially memorize the training data, data erasure should also unlearn thedata lineage in models, which raises increasing interest in the problem ofMachine Unlearning (MU). However, existing MU methods cannot be directlyapplied into recommendation. The basic idea of most recommender systems iscollaborative filtering, but existing MU methods ignore the collaborativeinformation across users and items. In this paper, we propose a generalerasable recommendation framework, namely LASER, which consists of Group moduleand SeqTrain module. Firstly, Group module partitions users into balancedgroups based on their similarity of collaborative embedding learned viahypergraph. Then SeqTrain module trains the model sequentially on all groupswith curriculum learning. Both theoretical analysis and experiments on tworeal-world datasets demonstrate that LASER can not only achieve efficientunlearning, but also outperform the state-of-the-art unlearning framework interms of model utility.\rDomain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey\nDanielle Saunders\nabstract\rabstract: The development of deep learning techniques has allowed Neural MachineTranslation (NMT) models to become extremely powerful, given sufficienttraining data and training time. However, systems struggle when translatingtext from a new domain with a distinct style or vocabulary. Fine-tuning onin-domain data allows good domain adaptation, but requires sufficient relevantbilingual data. Even if this is available, simple fine-tuning can causeoverfitting to new data and `catastrophic forgetting\u0026rsquo; of previously learnedbehaviour. We concentrate on robust approaches to domain adaptation for NMT,particularly where a system may need to translate across multiple domains. Wedivide techniques into those revolving around data selection or generation,model architecture, parameter adaptation procedure, and inference procedure. Wefinally highlight the benefits of domain adaptation and multi-domain adaptationtechniques to other lines of NMT research.\r2022-03-18\nOvercoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation\nChenze Shao, Yang Feng\nabstract\rabstract: Neural networks tend to gradually forget the previously learned knowledgewhen learning multiple tasks sequentially from dynamic data distributions. Thisproblem is called \\textit{catastrophic forgetting}, which is a fundamentalchallenge in the continual learning of neural networks. In this work, weobserve that catastrophic forgetting not only occurs in continual learning butalso affects the traditional static training. Neural networks, especiallyneural machine translation models, suffer from catastrophic forgetting even ifthey learn from a static training set. To be specific, the final model paysimbalanced attention to training samples, where recently exposed samplesattract more attention than earlier samples. The underlying cause is thattraining samples do not get balanced training in each model update, so we namethis problem \\textit{imbalanced training}. To alleviate this problem, wepropose Complementary Online Knowledge Distillation (COKD), which usesdynamically updated teacher models trained on specific data orders toiteratively provide complementary knowledge to the student model. Experimentalresults on multiple machine translation tasks show that our method successfullyalleviates the problem of imbalanced training and achieves substantialimprovements over strong baseline systems.\r2022-03-16\nAn Independently Learnable Hierarchical Model for Bilateral Control-Based Imitation Learning Applications\nKazuki Hayashi, Sho Sakaino, Toshiaki Tsuji\nabstract\rabstract: Recently, motion generation by machine learning has been actively researchedto automate various tasks. Imitation learning is one such method that learnsmotions from data collected in advance. However, executing long-term tasksremains challenging. Therefore, a novel framework for imitation learning isproposed to solve this problem. The proposed framework comprises upper andlower layers, where the upper layer model, whose timescale is long, and lowerlayer model, whose timescale is short, can be independently trained. In thismodel, the upper layer learns long-term task planning, and the lower layerlearns motion primitives. The proposed method was experimentally compared tohierarchical RNN-based methods to validate its effectiveness. Consequently, theproposed method showed a success rate equal to or greater than that ofconventional methods. In addition, the proposed method required less than 1/20of the training time compared to conventional methods. Moreover, it succeededin executing unlearned tasks by reusing the trained lower layer.\r2022-03-15\nContinual Active Learning Using Pseudo-Domains for Limited Labelling Resources and Changing Acquisition Characteristics\nMatthias Perkonigg, Johannes Hofmanninger, Christian Herold, Helmut Prosch, Georg Langs\nabstract\rabstract: Machine learning in medical imaging during clinical routine is impaired bychanges in scanner protocols, hardware, or policies resulting in aheterogeneous set of acquisition settings. When training a deep learning modelon an initial static training set, model performance and reliability sufferfrom changes of acquisition characteristics as data and targets may becomeinconsistent. Continual learning can help to adapt models to the changingenvironment by training on a continuous data stream. However, continual manualexpert labelling of medical imaging requires substantial effort. Thus, ways touse labelling resources efficiently on a well chosen sub-set of new examples isnecessary to render this strategy feasible. Here, we propose a method for continual active learning operating on a streamof medical images in a multi-scanner setting. The approach automaticallyrecognizes shifts in image acquisition characteristics - new domains -, selectsoptimal examples for labelling and adapts training accordingly. Labelling issubject to a limited budget, resembling typical real world scenarios. Todemonstrate generalizability, we evaluate the effectiveness of our method onthree tasks: cardiac segmentation, lung nodule detection and brain ageestimation. Results show that the proposed approach outperforms other activelearning methods, while effectively counteracting catastrophic forgetting.\r2022-03-14\nThe Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining\nYi Liu, Lei Xu, Xingliang Yuan, Cong Wang, Bo Li\nabstract\rabstract: In Machine Learning, the emergence of \\textit{the right to be forgotten} gavebirth to a paradigm named \\textit{machine unlearning}, which enables dataholders to proactively erase their data from a trained model. Existing machineunlearning techniques focus on centralized training, where access to allholders\u0026rsquo; training data is a must for the server to conduct the unlearningprocess. It remains largely underexplored about how to achieve unlearning whenfull access to all training data becomes unavailable. One noteworthy example isFederated Learning (FL), where each participating data holder trains locally,without sharing their training data to the central server. In this paper, weinvestigate the problem of machine unlearning in FL systems. We start with aformal definition of the unlearning problem in FL and propose a rapidretraining approach to fully erase data samples from a trained FL model. Theresulting design allows data holders to jointly conduct the unlearning processefficiently while keeping their training data locally. Our formal convergenceand complexity analysis demonstrate that our design can preserve model utilitywith high efficiency. Extensive evaluations on four real-world datasetsillustrate the effectiveness and performance of our proposed realization.\rForward Compatible Few-Shot Class-Incremental Learning\nDa-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, De-Chuan Zhan\nabstract\rabstract: Novel classes frequently arise in our dynamically changing world, e.g., newusers in the authentication system, and a machine learning model shouldrecognize new classes without forgetting old ones. This scenario becomes morechallenging when new class instances are insufficient, which is called few-shotclass-incremental learning (FSCIL). Current methods handle incremental learningretrospectively by making the updated model similar to the old one. Bycontrast, we suggest learning prospectively to prepare for future updates, andpropose ForwArd Compatible Training (FACT) for FSCIL. Forward compatibilityrequires future new classes to be easily incorporated into the current modelbased on the current stage data, and we seek to realize it by reservingembedding space for future new classes. In detail, we assign virtual prototypesto squeeze the embedding of known classes and reserve for new ones. Besides, weforecast possible new classes and prepare for the updating process. The virtualprototypes allow the model to accept possible updates in the future, which actas proxies scattered among embedding space to build a stronger classifierduring inference. FACT efficiently incorporates new classes with forwardcompatibility and meanwhile resists forgetting of old ones. Extensiveexperiments validate FACT\u0026rsquo;s state-of-the-art performance. Code is available at:https://github.com/zhoudw-zdw/CVPR22-Fact\r2022-03-11\nLifelong Adaptive Machine Learning for Sensor-based Human Activity Recognition Using Prototypical Networks\nRebecca Adaimi, Edison Thomaz\nabstract\rabstract: Continual learning, also known as lifelong learning, is an emerging researchtopic that has been attracting increasing interest in the field of machinelearning. With human activity recognition (HAR) playing a key role in enablingnumerous real-world applications, an essential step towards the long-termdeployment of such recognition systems is to extend the activity model todynamically adapt to changes in people\u0026rsquo;s everyday behavior. Current research incontinual learning applied to HAR domain is still under-explored withresearchers exploring existing methods developed for computer vision in HAR.Moreover, analysis has so far focused on task-incremental or class-incrementallearning paradigms where task boundaries are known. This impedes theapplicability of such methods for real-world systems since data is presented ina randomly streaming fashion. To push this field forward, we build on recentadvances in the area of continual machine learning and design a lifelongadaptive learning framework using Prototypical Networks, LAPNet-HAR, thatprocesses sensor-based data streams in a task-free data-incremental fashion andmitigates catastrophic forgetting using experience replay and continualprototype adaptation. Online learning is further facilitated using contrastiveloss to enforce inter-class separation. LAPNet-HAR is evaluated on 5 publiclyavailable activity datasets in terms of the framework\u0026rsquo;s ability to acquire newinformation while preserving previous knowledge. Our extensive empiricalresults demonstrate the effectiveness of LAPNet-HAR in task-free continuallearning and uncover useful insights for future challenges.\r2022-03-05\nAcceleration of Federated Learning with Alleviated Forgetting in Local Training\nChencheng Xu, Zhiwei Hong, Minlie Huang, Tao Jiang\nabstract\rabstract: Federated learning (FL) enables distributed optimization of machine learningmodels while protecting privacy by independently training local models on eachclient and then aggregating parameters on a central server, thereby producingan effective global model. Although a variety of FL algorithms have beenproposed, their training efficiency remains low when the data are notindependently and identically distributed (non-i.i.d.) across differentclients. We observe that the slow convergence rates of the existing methods are(at least partially) caused by the catastrophic forgetting issue during thelocal training stage on each individual client, which leads to a large increasein the loss function concerning the previous training data at the otherclients. Here, we propose FedReg, an algorithm to accelerate FL with alleviatedknowledge forgetting in the local training stage by regularizing locallytrained parameters with the loss on generated pseudo data, which encode theknowledge of previous training data learned by the global model. Ourcomprehensive experiments demonstrate that FedReg not only significantlyimproves the convergence rate of FL, especially when the neural networkarchitecture is deep and the clients\u0026rsquo; data are extremely non-i.i.d., but isalso able to protect privacy better in classification problems and more robustagainst gradient inversion attacks. The code is available at:https://github.com/Zoesgithub/FedReg.\r2022-03-02\nUnrolling SGD: Understanding Factors Influencing Machine Unlearning\nAnvith Thudi, Gabriel Deza, Varun Chandrasekaran, Nicolas Papernot\nabstract\rabstract: Machine unlearning is the process through which a deployed machine learningmodel is made to forget about some of its training data points. While naivelyretraining the model from scratch is an option, it is almost always associatedwith large computational overheads for deep learning models. Thus, severalapproaches to approximately unlearn have been proposed along with correspondingmetrics that formalize what it means for a model to forget about a data point.In this work, we first taxonomize approaches and metrics of approximateunlearning. As a result, we identify verification error, i.e., the L2difference between the weights of an approximately unlearned and a naivelyretrained model, as an approximate unlearning metric that should be optimizedfor as it subsumes a large class of other metrics. We theoretically analyze thecanonical training algorithm, stochastic gradient descent (SGD), to surface thevariables which are relevant to reducing the verification error of approximateunlearning for SGD. From this analysis, we first derive an easy-to-computeproxy for verification error (termed unlearning error). The analysis alsoinforms the design of a new training objective penalty that limits the overallchange in weights during SGD and as a result facilitates approximate unlearningwith lower verification error. We validate our theoretical work through anempirical evaluation on learning with CIFAR-10, CIFAR-100, and IMDB sentimentanalysis.\r2022-03-01\nMixture-of-Variational-Experts for Continual Learning\nHeinke Hihn, Daniel A. Braun\nabstract\rabstract: One weakness of machine learning algorithms is the poor ability of models tosolve new problems without forgetting previously acquired knowledge. TheContinual Learning (CL) paradigm has emerged as a protocol to systematicallyinvestigate settings where the model sequentially observes samples generated bya series of tasks. In this work, we take a task-agnostic view of continuallearning and develop a hierarchical information-theoretic optimality principlethat facilitates a trade-off between learning and forgetting. We discuss thisprinciple from a Bayesian perspective and show its connections to previousapproaches to CL. Based on this principle, we propose a neural network layer,called the Mixture-of-Variational-Experts layer, that alleviates forgetting bycreating a set of information processing paths through the network which isgoverned by a gating policy. Due to the general formulation based on genericutility functions, we can apply this optimality principle to a large variety oflearning problems, including supervised learning, reinforcement learning, andgenerative modeling. We demonstrate the competitive performance of our methodin continual supervised learning and in continual reinforcement learning.\r2022-02-28\nMarkov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten\nQuoc Phong Nguyen, Ryutaro Oikawa, Dinil Mon Divakaran, Mun Choon Chan, Bryan Kian Hsiang Low\nabstract\rabstract: As the use of machine learning (ML) models is becoming increasingly popularin many real-world applications, there are practical challenges that need to beaddressed for model maintenance. One such challenge is to \u0026lsquo;undo\u0026rsquo; the effect ofa specific subset of dataset used for training a model. This specific subsetmay contain malicious or adversarial data injected by an attacker, whichaffects the model performance. Another reason may be the need for a serviceprovider to remove data pertaining to a specific user to respect the user\u0026rsquo;sprivacy. In both cases, the problem is to \u0026lsquo;unlearn\u0026rsquo; a specific subset of thetraining data from a trained model without incurring the costly procedure ofretraining the whole model from scratch. Towards this goal, this paper presentsa Markov chain Monte Carlo-based machine unlearning (MCU) algorithm. MCU helpsto effectively and efficiently unlearn a trained model from subsets of trainingdataset. Furthermore, we show that with MCU, we are able to explain the effectof a subset of a training dataset on the model prediction. Thus, MCU is usefulfor examining subsets of data to identify the adversarial data to be removed.Similarly, MCU can be used to erase the lineage of a user\u0026rsquo;s personal data fromtrained ML models, thus upholding a user\u0026rsquo;s \u0026ldquo;right to be forgotten\u0026rdquo;. Weempirically evaluate the performance of our proposed MCU algorithm onreal-world phishing and diabetes datasets. Results show that MCU can achieve adesirable performance by efficiently removing the effect of a subset oftraining dataset and outperform an existing algorithm that utilizes theremaining dataset.\r2022-02-21\nLearning Bayesian Sparse Networks with Full Experience Replay for Continual Learning\nDong Gong, Qingsen Yan, Yuhang Liu, Anton van den Hengel, Javen Qinfeng Shi\nabstract\rabstract: Continual Learning (CL) methods aim to enable machine learning models tolearn new tasks without catastrophic forgetting of those that have beenpreviously mastered. Existing CL approaches often keep a buffer ofpreviously-seen samples, perform knowledge distillation, or use regularizationtechniques towards this goal. Despite their performance, they still suffer frominterference across tasks which leads to catastrophic forgetting. To amelioratethis problem, we propose to only activate and select sparse neurons forlearning current and past tasks at any stage. More parameters space and modelcapacity can thus be reserved for the future tasks. This minimizes theinterference between parameters for different tasks. To do so, we propose aSparse neural Network for Continual Learning (SNCL), which employs variationalBayesian sparsity priors on the activations of the neurons in all layers. FullExperience Replay (FER) provides effective supervision in learning the sparseactivations of the neurons in different layers. A loss-aware reservoir-samplingstrategy is developed to maintain the memory buffer. The proposed method isagnostic as to the network structures and the task boundaries. Experiments ondifferent datasets show that our approach achieves state-of-the-art performancefor mitigating forgetting.\r2022-02-19\nOn the Necessity of Auditable Algorithmic Definitions for Machine Unlearning\nAnvith Thudi, Hengrui Jia, Ilia Shumailov, Nicolas Papernot\nabstract\rabstract: Machine unlearning, i.e. having a model forget about some of its trainingdata, has become increasingly more important as privacy legislation promotesvariants of the right-to-be-forgotten. In the context of deep learning,approaches for machine unlearning are broadly categorized into two classes:exact unlearning methods, where an entity has formally removed the data point\u0026rsquo;simpact on the model by retraining the model from scratch, and approximateunlearning, where an entity approximates the model parameters one would obtainby exact unlearning to save on compute costs. In this paper, we first show thatthe definition that underlies approximate unlearning, which seeks to prove theapproximately unlearned model is close to an exactly retrained model, isincorrect because one can obtain the same model using different datasets. Thusone could unlearn without modifying the model at all. We then turn to exactunlearning approaches and ask how to verify their claims of unlearning. Ourresults show that even for a given training trajectory one cannot formallyprove the absence of certain data points used during training. We thus concludethat unlearning is only well-defined at the algorithmic level, where anentity\u0026rsquo;s only possible auditable claim to unlearning is that they used aparticular algorithm designed to allow for external scrutiny during an audit.\r2022-02-10\nHard to Forget: Poisoning Attacks on Certified Machine Unlearning\nNeil G. Marchant, Benjamin I. P. Rubinstein, Scott Alfeld\nabstract\rabstract: The right to erasure requires removal of a user\u0026rsquo;s information from data heldby organizations, with rigorous interpretations extending to downstreamproducts such as learned models. Retraining from scratch with the particularuser\u0026rsquo;s data omitted fully removes its influence on the resulting model, butcomes with a high computational cost. Machine \u0026ldquo;unlearning\u0026rdquo; mitigates the costincurred by full retraining: instead, models are updated incrementally,possibly only requiring retraining when approximation errors accumulate. Rapidprogress has been made towards privacy guarantees on the indistinguishabilityof unlearned and retrained models, but current formalisms do not placepractical bounds on computation. In this paper we demonstrate how an attackercan exploit this oversight, highlighting a novel attack surface introduced bymachine unlearning. We consider an attacker aiming to increase thecomputational cost of data removal. We derive and empirically investigate apoisoning attack on certified machine unlearning where strategically designedtraining data triggers complete retraining when removed.\r2022-02-07\nDeletion Inference, Reconstruction, and Compliance in Machine (Un)Learning\nJi Gao, Sanjam Garg, Mohammad Mahmoody, Prashant Nalini Vasudevan\nabstract\rabstract: Privacy attacks on machine learning models aim to identify the data that isused to train such models. Such attacks, traditionally, are studied on staticmodels that are trained once and are accessible by the adversary. Motivated tomeet new legal requirements, many machine learning methods are recentlyextended to support machine unlearning, i.e., updating models as if certainexamples are removed from their training sets, and meet new legal requirements.However, privacy attacks could potentially become more devastating in this newsetting, since an attacker could now access both the original model beforedeletion and the new model after the deletion. In fact, the very act ofdeletion might make the deleted record more vulnerable to privacy attacks. Inspired by cryptographic definitions and the differential privacy framework,we formally study privacy implications of machine unlearning. We formalize(various forms of) deletion inference and deletion reconstruction attacks, inwhich the adversary aims to either identify which record is deleted or toreconstruct (perhaps part of) the deleted records. We then present successfuldeletion inference and reconstruction attacks for a variety of machine learningmodels and tasks such as classification, regression, and language models.Finally, we show that our attacks would provably be precluded if the schemessatisfy (variants of) Deletion Compliance (Garg, Goldwasser, and Vasudevan,Eurocrypt\u0026rsquo; 20).\r2022-02-03\nLearnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations\nWeiqi Peng, Jinghui Chen\nabstract\rabstract: Owing much to the revolution of information technology, the recent progressof deep learning benefits incredibly from the vastly enhanced access to dataavailable in various digital formats. However, in certain scenarios, people maynot want their data being used for training commercial models and thus studiedhow to attack the learnability of deep learning models. Previous works onlearnability attack only consider the goal of preventing unauthorizedexploitation on the specific dataset but not the process of restoring thelearnability for authorized cases. To tackle this issue, this paper introducesand investigates a new concept called \u0026ldquo;learnability lock\u0026rdquo; for controlling themodel\u0026rsquo;s learnability on a specific dataset with a special key. In particular,we propose adversarial invertible transformation, that can be viewed as amapping from image to image, to slightly modify data samples so that theybecome \u0026ldquo;unlearnable\u0026rdquo; by machine learning models with negligible loss of visualfeatures. Meanwhile, one can unlock the learnability of the dataset and trainmodels normally using the corresponding key. The proposed learnability lockleverages class-wise perturbation that applies a universal transformationfunction on data samples of the same label. This ensures that the learnabilitycan be easily restored with a simple inverse transformation while remainingdifficult to be detected or reverse-engineered. We empirically demonstrate thesuccess and practicability of our method on visual classification tasks.\r2022-02-01\nFortuitous Forgetting in Connectionist Networks\nHattie Zhou, Ankit Vani, Hugo Larochelle, Aaron Courville\nabstract\rabstract: Forgetting is often seen as an unwanted characteristic in both human andmachine learning. However, we propose that forgetting can in fact be favorableto learning. We introduce \u0026ldquo;forget-and-relearn\u0026rdquo; as a powerful paradigm forshaping the learning trajectories of artificial neural networks. In thisprocess, the forgetting step selectively removes undesirable information fromthe model, and the relearning step reinforces features that are consistentlyuseful under different conditions. The forget-and-relearn framework unifiesmany existing iterative training algorithms in the image classification andlanguage emergence literature, and allows us to understand the success of thesealgorithms in terms of the disproportionate forgetting of undesirableinformation. We leverage this understanding to improve upon existing algorithmsby designing more targeted forgetting operations. Insights from our analysisprovide a coherent view on the dynamics of iterative training in neuralnetworks and offer a clear path towards performance improvements.\r2022-01-25\nRecommendation Unlearning\nChong Chen, Fei Sun, Min Zhang, Bolin Ding\nabstract\rabstract: Recommender systems provide essential web services by learning users\u0026rsquo;personal preferences from collected data. However, in many cases, systems alsoneed to forget some training data. From the perspective of privacy, severalprivacy regulations have recently been proposed, requiring systems to eliminateany impact of the data whose owner requests to forget. From the perspective ofutility, if a system\u0026rsquo;s utility is damaged by some bad data, the system needs toforget these data to regain utility. From the perspective of usability, userscan delete noise and incorrect entries so that a system can provide more usefulrecommendations. While unlearning is very important, it has not beenwell-considered in existing recommender systems. Although there are someresearches have studied the problem of machine unlearning in the domains ofimage and text data, existing methods can not been directly applied torecommendation as they are unable to consider the collaborative information. In this paper, we propose RecEraser, a general and efficient machineunlearning framework tailored to recommendation task. The main idea ofRecEraser is to partition the training set into multiple shards and train aconstituent model for each shard. Specifically, to keep the collaborativeinformation of the data, we first design three novel data partition algorithmsto divide training data into balanced groups based on their similarity. Then,considering that different shard models do not uniformly contribute to thefinal prediction, we further propose an adaptive aggregation method to improvethe global model utility. Experimental results on three public benchmarks showthat RecEraser can not only achieve efficient unlearning, but also outperformthe state-of-the-art unlearning methods in terms of model utility. The sourcecode can be found at https://github.com/chenchongthu/Recommendation-Unlearning\r2022-01-24\nBackdoor Defense with Machine Unlearning\nYang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma\nabstract\rabstract: Backdoor injection attack is an emerging threat to the security of neuralnetworks, however, there still exist limited effective defense methods againstthe attack. In this paper, we propose BAERASE, a novel method that can erasethe backdoor injected into the victim model through machine unlearning.Specifically, BAERASE mainly implements backdoor defense in two key steps.First, trigger pattern recovery is conducted to extract the trigger patternsinfected by the victim model. Here, the trigger pattern recovery problem isequivalent to the one of extracting an unknown noise distribution from thevictim model, which can be easily resolved by the entropy maximization basedgenerative model. Subsequently, BAERASE leverages these recovered triggerpatterns to reverse the backdoor injection procedure and induce the victimmodel to erase the polluted memories through a newly designed gradient ascentbased machine unlearning method. Compared with the previous machine unlearningsolutions, the proposed approach gets rid of the reliance on the full access totraining data for retraining and shows higher effectiveness on backdoor erasingthan existing fine-tuning or pruning methods. Moreover, experiments show thatBAERASE can averagely lower the attack success rates of three kinds ofstate-of-the-art backdoor attacks by 99% on four benchmark datasets.\r2021-12-28\nTowards continual task learning in artificial neural networks: current approaches and insights from neuroscience\nDavid McCaffary\nabstract\rabstract: The innate capacity of humans and other animals to learn a diverse, and ofteninterfering, range of knowledge and skills throughout their lifespan is ahallmark of natural intelligence, with obvious evolutionary motivations. Inparallel, the ability of artificial neural networks (ANNs) to learn across arange of tasks and domains, combining and re-using learned representationswhere required, is a clear goal of artificial intelligence. This capacity,widely described as continual learning, has become a prolific subfield ofresearch in machine learning. Despite the numerous successes of deep learningin recent years, across domains ranging from image recognition to machinetranslation, such continual task learning has proved challenging. Neuralnetworks trained on multiple tasks in sequence with stochastic gradient descentoften suffer from representational interference, whereby the learned weightsfor a given task effectively overwrite those of previous tasks in a processtermed catastrophic forgetting. This represents a major impediment to thedevelopment of more generalised artificial learning systems, capable ofaccumulating knowledge over time and task space, in a manner analogous tohumans. A repository of selected papers and implementations accompanying thisreview can be found at https://github.com/mccaffary/continual-learning.\r2021-12-24\nAn Investigation on Learning, Polluting, and Unlearning the Spam Emails for Lifelong Learning\nNishchal Parne, Kyathi Puppaala, Nithish Bhupathi, Ripon Patgiri\nabstract\rabstract: Machine unlearning for security is studied in this context. Several spamemail detection methods exist, each of which employs a different algorithm todetect undesired spam emails. But these models are vulnerable to attacks. Manyattackers exploit the model by polluting the data, which are trained to themodel in various ways. So to act deftly in such situations model needs toreadily unlearn the polluted data without the need for retraining. Retrainingis impractical in most cases as there is already a massive amount of datatrained to the model in the past, which needs to be trained again just forremoving a small amount of polluted data, which is often significantly lessthan 1%. This problem can be solved by developing unlearning frameworks for allspam detection models. In this research, unlearning module is integrated intospam detection models that are based on Naive Bayes, Decision trees, and RandomForests algorithms. To assess the benefits of unlearning over retraining, threespam detection models are polluted and exploited by taking attackers\u0026rsquo; positionsand proving models\u0026rsquo; vulnerability. Reduction in accuracy and true positiverates are shown in each case showing the effect of pollution on models. Thenunlearning modules are integrated into the models, and polluted data isunlearned; on testing the models after unlearning, restoration of performanceis seen. Also, unlearning and retraining times are compared with differentpollution data sizes on all models. On analyzing the findings, it can beconcluded that unlearning is considerably superior to retraining. Results showthat unlearning is fast, easy to implement, easy to use, and effective.\r2021-12-22\nAlgorithmic insights on continual learning from fruit flies\nYang Shen, Sanjoy Dasgupta, Saket Navlakha\nabstract\rabstract: Continual learning in computational systems is challenging due tocatastrophic forgetting. We discovered a two layer neural circuit in the fruitfly olfactory system that addresses this challenge by uniquely combining sparsecoding and associative learning. In the first layer, odors are encoded usingsparse, high dimensional representations, which reduces memory interference byactivating non overlapping populations of neurons for different odors. In thesecond layer, only the synapses between odor activated neurons and the outputneuron associated with the odor are modified during learning; the rest of theweights are frozen to prevent unrelated memories from being overwritten. Weshow empirically and analytically that this simple and lightweight algorithmsignificantly boosts continual learning performance. The fly associativelearning algorithm is strikingly similar to the classic perceptron learningalgorithm, albeit two modifications, which we show are critical for reducingcatastrophic forgetting. Overall, fruit flies evolved an efficient lifelonglearning algorithm, and circuit mechanisms from neuroscience can be translatedto improve machine computation.\rContinual learning of longitudinal health records\nJ. Armstrong, D. Clifton\nabstract\rabstract: Continual learning denotes machine learning methods which can adapt to newenvironments while retaining and reusing knowledge gained from pastexperiences. Such methods address two issues encountered by models innon-stationary environments: ungeneralisability to new data, and thecatastrophic forgetting of previous knowledge when retrained. This is apervasive problem in clinical settings where patient data exhibits covariateshift not only between populations, but also continuously over time. However,while continual learning methods have seen nascent success in the imagingdomain, they have been little applied to the multi-variate sequential datacharacteristic of critical care patient recordings. Here we evaluate a variety of continual learning methods on longitudinal ICUdata in a series of representative healthcare scenarios. We find that whileseveral methods mitigate short-term forgetting, domain shift remains achallenging problem over large series of tasks, with only replay based methodsachieving stable long-term performance. Code for reproducing all experiments can be found athttps://github.com/iacobo/continual\r2021-12-09\nProvable Continual Learning via Sketched Jacobian Approximations\nReinhard Heckel\nabstract\rabstract: An important problem in machine learning is the ability to learn tasks in asequential manner. If trained with standard first-order methods most modelsforget previously learned tasks when trained on a new task, which is oftenreferred to as catastrophic forgetting. A popular approach to overcomeforgetting is to regularize the loss function by penalizing models that performpoorly on previous tasks. For example, elastic weight consolidation (EWC)regularizes with a quadratic form involving a diagonal matrix build based onpast data. While EWC works very well for some setups, we show that, even underotherwise ideal conditions, it can provably suffer catastrophic forgetting ifthe diagonal matrix is a poor approximation of the Hessian matrix of previoustasks. We propose a simple approach to overcome this: Regularizing training ofa new task with sketches of the Jacobian matrix of past data. This provablyenables overcoming catastrophic forgetting for linear models and for wideneural networks, at the cost of memory. The overarching goal of this paper isto provided insights on when regularization-based continual learning algorithmswork and under what memory costs.\rReducing Catastrophic Forgetting in Self Organizing Maps with Internally-Induced Generative Replay\nHitesh Vaidya, Travis Desell, Alexander Ororbia\nabstract\rabstract: A lifelong learning agent is able to continually learn from potentiallyinfinite streams of pattern sensory data. One major historic difficulty inbuilding agents that adapt in this way is that neural systems struggle toretain previously-acquired knowledge when learning from new samples. Thisproblem is known as catastrophic forgetting (interference) and remains anunsolved problem in the domain of machine learning to this day. Whileforgetting in the context of feedforward networks has been examined extensivelyover the decades, far less has been done in the context of alternativearchitectures such as the venerable self-organizing map (SOM), an unsupervisedneural model that is often used in tasks such as clustering and dimensionalityreduction. Although the competition among its internal neurons might carry thepotential to improve memory retention, we observe that a fixed-sized SOMtrained on task incremental data, i.e., it receives data points related tospecific classes at certain temporal increments, experiences significantforgetting. In this study, we propose the continual SOM (c-SOM), a model thatis capable of reducing its own forgetting when processing information.\r2021-11-21\nLearning by Active Forgetting for Neural Networks\nJian Peng, Xian Sun, Min Deng, Chao Tao, Bo Tang, Wenbo Li, Guohua Wu, QingZhu, Yu Liu, Tao Lin, Haifeng Li\nabstract\rabstract: Remembering and forgetting mechanisms are two sides of the same coin in ahuman learning-memory system. Inspired by human brain memory mechanisms, modernmachine learning systems have been working to endow machine with lifelonglearning capability through better remembering while pushing the forgetting asthe antagonist to overcome. Nevertheless, this idea might only see the halfpicture. Up until very recently, increasing researchers argue that a brain isborn to forget, i.e., forgetting is a natural and active process for abstract,rich, and flexible representations. This paper presents a learning model byactive forgetting mechanism with artificial neural networks. The activeforgetting mechanism (AFM) is introduced to a neural network via a\u0026quot;plug-and-play\u0026quot; forgetting layer (P\u0026amp;PF), consisting of groups of inhibitoryneurons with Internal Regulation Strategy (IRS) to adjust the extinction rateof themselves via lateral inhibition mechanism and External Regulation Strategy(ERS) to adjust the extinction rate of excitatory neurons via inhibitionmechanism. Experimental studies have shown that the P\u0026amp;PF offers surprisingbenefits: self-adaptive structure, strong generalization, long-term learningand memory, and robustness to data and parameter perturbation. This work shedslight on the importance of forgetting in the learning process and offers newperspectives to understand the underlying mechanisms of neural networks.\r2021-11-10\nLightweight machine unlearning in neural network\nKongyang Chen, Yiwen Wang, Yao Huang\nabstract\rabstract: In recent years, machine learning neural network has penetrated deeply intopeople\u0026rsquo;s life. As the price of convenience, people\u0026rsquo;s private information alsohas the risk of disclosure. The \u0026ldquo;right to be forgotten\u0026rdquo; was introduced in atimely manner, stipulating that individuals have the right to withdraw theirconsent from personal information processing activities based on their consent.To solve this problem, machine unlearning is proposed, which allows the modelto erase all memory of private information. Previous studies, includingretraining and incremental learning to update models, often take up extrastorage space or are difficult to apply to neural networks. Our method onlyneeds to make a small perturbation of the weight of the target model and makeit iterate in the direction of the model trained with the remaining data subsetuntil the contribution of the unlearning data to the model is completelyeliminated. In this paper, experiments on five datasets prove the effectivenessof our method for machine unlearning, and our method is 15 times faster thanretraining.\r2021-10-28\nRisk-utility tradeoff shapes memory strategies for evolving patterns\nOskar H Schnaack, Luca Peliti, Armita Nourmohammad\nabstract\rabstract: Keeping a memory of evolving stimuli is ubiquitous in biology, an example ofwhich is immune memory for evolving pathogens. However, learning and memorystorage for dynamic patterns still pose challenges in machine learning. Here,we introduce an analytical energy-based framework to address this problem. Byaccounting for the tradeoff between utility in keeping a high-affinity memoryand the risk in forgetting some of the diverse stimuli, we show that a moderatetolerance for risk enables a repertoire to robustly classify evolving patterns,without much fine-tuning. Our approach offers a general guideline for learningand memory storage in systems interacting with diverse and evolving stimuli.\r2021-10-19\nA Simple Approach to Continual Learning by Transferring Skill Parameters\nK. R. Zentner, Ryan Julian, Ujjwal Puri, Yulun Zhang, Gaurav S. Sukhatme\nabstract\rabstract: In order to be effective general purpose machines in real world environments,robots not only will need to adapt their existing manipulation skills to newcircumstances, they will need to acquire entirely new skills on-the-fly. Agreat promise of continual learning is to endow robots with this ability, byusing their accumulated knowledge and experience from prior skills. We take afresh look at this problem, by considering a setting in which the robot islimited to storing that knowledge and experience only in the form of learnedskill policies. We show that storing skill policies, careful pre-training, andappropriately choosing when to transfer those skill policies is sufficient tobuild a continual learner in the context of robotic manipulation. We analyzewhich conditions are needed to transfer skills in the challenging Meta-Worldsimulation benchmark. Using this analysis, we introduce a pair-wise metricrelating skills that allows us to predict the effectiveness of skill transferbetween tasks, and use it to reduce the problem of continual learning tocurriculum selection. Given an appropriate curriculum, we show how tocontinually acquire robotic manipulation skills without forgetting, and usingfar fewer samples than needed to train them from scratch.\r2021-10-18\nMultilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters\nAsa Cooper Stickland, Alexandre Bérard, Vassilina Nikoulina\nabstract\rabstract: Adapter layers are lightweight, learnable units inserted between transformerlayers. Recent work explores using such layers for neural machine translation(NMT), to adapt pre-trained models to new domains or language pairs, trainingonly a small set of parameters for each new setting (language pair or domain).In this work we study the compositionality of language and domain adapters inthe context of Machine Translation. We aim to study, 1) parameter-efficientadaptation to multiple domains and languages simultaneously (full-resourcescenario) and 2) cross-lingual transfer in domains where parallel data isunavailable for certain language pairs (partial-resource scenario). We findthat in the partial resource scenario a naive combination of domain-specificand language-specific adapters often results in `catastrophic forgetting\u0026rsquo; ofthe missing languages. We study other ways to combine the adapters to alleviatethis issue and maximize cross-lingual transfer. With our best adaptercombinations, we obtain improvements of 3-4 BLEU on average for sourcelanguages that do not have in-domain data. For target languages withoutin-domain data, we achieve a similar improvement by combining adapters withback-translation. Supplementary material is available athttps://tinyurl.com/r66stbxj\r2021-10-17\nGrowing Representation Learning\nRyan King, Bobak Mortazavi\nabstract\rabstract: Machine learning continues to grow in popularity due to its ability to learnincreasingly complex tasks. However, for many supervised models, the shift in adata distribution or the appearance of a new event can result in a severedecrease in model performance. Retraining a model from scratch with updateddata can be resource intensive or impossible depending on the constraintsplaced on an organization or system. Continual learning methods attempt toadapt models to new classes instead of retraining. However, many of thesemethods do not have a detection method for new classes or make assumptionsabout the distribution of classes. In this paper, we develop an attention basedGaussian Mixture, called GMAT, that learns interpretable representations ofdata with or without labels. We incorporate this method with existing NeuralArchitecture Search techniques to develop an algorithm for detection new eventsfor an optimal number of representations through an iterative process oftraining a growing. We show that our method is capable learning newrepresentations of data without labels or assumptions about the distributionsof labels. We additionally develop a method that allows our model to utilizelabels to more accurately develop representations. Lastly, we show that ourmethod can avoid catastrophic forgetting by replaying samples from learnedrepresentations.\r2021-09-20\nDRILL: Dynamic Representations for Imbalanced Lifelong Learning\nKyra Ahrens, Fares Abawi, Stefan Wermter\nabstract\rabstract: Continual or lifelong learning has been a long-standing challenge in machinelearning to date, especially in natural language processing (NLP). Althoughstate-of-the-art language models such as BERT have ushered in a new era in thisfield due to their outstanding performance in multitask learning scenarios,they suffer from forgetting when being exposed to a continuous stream of datawith shifting data distributions. In this paper, we introduce DRILL, a novelcontinual learning architecture for open-domain text classification. DRILLleverages a biologically inspired self-organizing neural architecture toselectively gate latent language representations from BERT in atask-incremental manner. We demonstrate in our experiments that DRILLoutperforms current methods in a realistic scenario of imbalanced,non-stationary data without prior knowledge about task boundaries. To the bestof our knowledge, DRILL is the first of its kind to use a self-organizingneural architecture for open-domain lifelong learning in NLP.\r2021-09-14\nWhen Machine Unlearning Jeopardizes Privacy\nMin Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang\nabstract\rabstract: The right to be forgotten states that a data owner has the right to erasetheir data from an entity storing it. In the context of machine learning (ML),the right to be forgotten requires an ML model owner to remove the data owner\u0026rsquo;sdata from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of thedata owner, we argue that machine unlearning may leave some imprint of the datain the ML model and thus create unintended privacy risks. In this paper, weperform the first study on investigating the unintended information leakagecaused by machine unlearning. We propose a novel membership inference attackthat leverages the different outputs of an ML model\u0026rsquo;s two versions to inferwhether a target sample is part of the training set of the original model butout of the training set of the corresponding unlearned model. Our experimentsdemonstrate that the proposed membership inference attack achieves strongperformance. More importantly, we show that our attack in multiple casesoutperforms the classical membership inference attack on the original ML model,which indicates that machine unlearning can have counterproductive effects onprivacy. We notice that the privacy degradation is especially significant forwell-generalized ML models where classical membership inference does notperform well. We further investigate four mechanisms to mitigate the newlydiscovered privacy risks and show that releasing the predicted label only,temperature scaling, and differential privacy are effective. We believe thatour results can help improve privacy protection in practical implementations ofmachine unlearning. Our code is available athttps://github.com/MinChen00/UnlearningLeaks.\rCross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation\nMozhdeh Gheini, Xiang Ren, Jonathan May\nabstract\rabstract: We study the power of cross-attention in the Transformer architecture withinthe context of transfer learning for machine translation, and extend thefindings of studies into cross-attention when training from scratch. We conducta series of experiments through fine-tuning a translation model on data whereeither the source or target language has changed. These experiments reveal thatfine-tuning only the cross-attention parameters is nearly as effective asfine-tuning all parameters (i.e., the entire translation model). We provideinsights into why this is the case and observe that limiting fine-tuning inthis manner yields cross-lingually aligned embeddings. The implications of thisfinding for researchers and practitioners include a mitigation of catastrophicforgetting, the potential for zero-shot translation, and the ability to extendmachine translation models to several new language pairs with reduced parameterstorage overhead.\r2021-09-09\nA distillation-based approach integrating continual learning and federated learning for pervasive services\nAnastasiia Usmanova, François Portet, Philippe Lalanda, German Vega\nabstract\rabstract: Federated Learning, a new machine learning paradigm enhancing the use of edgedevices, is receiving a lot of attention in the pervasive community to supportthe development of smart services. Nevertheless, this approach still needs tobe adapted to the specificity of the pervasive domain. In particular, issuesrelated to continual learning need to be addressed. In this paper, we present adistillation-based approach dealing with catastrophic forgetting in federatedlearning scenario. Specifically, Human Activity Recognition tasks are used as ademonstration domain.\r2021-09-03\nLearning Neural Models for Natural Language Processing in the Face of Distributional Shift\nPaul Michel\nabstract\rabstract: The dominating NLP paradigm of training a strong neural predictor to performone task on a specific dataset has led to state-of-the-art performance in avariety of applications (eg. sentiment classification, span-prediction basedquestion answering or machine translation). However, it builds upon theassumption that the data distribution is stationary, ie. that the data issampled from a fixed distribution both at training and test time. This way oftraining is inconsistent with how we as humans are able to learn from andoperate within a constantly changing stream of information. Moreover, it isill-adapted to real-world use cases where the data distribution is expected toshift over the course of a model\u0026rsquo;s lifetime. The first goal of this thesis is to characterize the different forms thisshift can take in the context of natural language processing, and proposebenchmarks and evaluation metrics to measure its effect on current deeplearning architectures. We then proceed to take steps to mitigate the effect ofdistributional shift on NLP models. To this end, we develop methods based onparametric reformulations of the distributionally robust optimizationframework. Empirically, we demonstrate that these approaches yield more robustmodels as demonstrated on a selection of realistic problems. In the third andfinal part of this thesis, we explore ways of efficiently adapting existingmodels to new domains or tasks. Our contribution to this topic takesinspiration from information geometry to derive a new gradient update rulewhich alleviate catastrophic forgetting issues during adaptation.\r2021-08-16\nCertifiable Machine Unlearning for Linear Models\nAnanth Mahadevan, Michael Mathioudakis\nabstract\rabstract: Machine unlearning is the task of updating machine learning (ML) models aftera subset of the training data they were trained on is deleted. Methods for thetask are desired to combine effectiveness and efficiency, i.e., they shouldeffectively \u0026ldquo;unlearn\u0026rdquo; deleted data, but in a way that does not requireexcessive computation effort (e.g., a full retraining) for a small amount ofdeletions. Such a combination is typically achieved by tolerating some amountof approximation in the unlearning. In addition, laws and regulations in thespirit of \u0026ldquo;the right to be forgotten\u0026rdquo; have given rise to requirements forcertifiability, i.e., the ability to demonstrate that the deleted data hasindeed been unlearned by the ML model. In this paper, we present an experimental study of the three state-of-the-artapproximate unlearning methods for linear models and demonstrate the trade-offsbetween efficiency, effectiveness and certifiability offered by each method. Inimplementing the study, we extend some of the existing works and describe acommon ML pipeline to compare and evaluate the unlearning methods on sixreal-world datasets and a variety of settings. We provide insights into theeffect of the quantity and distribution of the deleted data on ML models andthe performance of each unlearning method in different settings. We alsopropose a practical online strategy to determine when the accumulated errorfrom approximate unlearning is large enough to warrant a full retrain of the MLmodel.\rThe Forgotten Threat of Voltage Glitching: A Case Study on Nvidia Tegra X2 SoCs\nOtto Bittner, Thilo Krachenfels, Andreas Galauner, Jean-Pierre Seifert\nabstract\rabstract: Voltage fault injection (FI) is a well-known attack technique that can beused to force faulty behavior in processors during their operation. Glitchingthe supply voltage can cause data value corruption, skip security checks, orenable protected code paths. At the same time, modern systems on a chip (SoCs)are used in security-critical applications, such as self-driving cars andautonomous machines. Since these embedded devices are often physicallyaccessible by attackers, vendors must consider device tampering in their threatmodels. However, while the threat of voltage FI is known since the early 2000s,it seems as if vendors still forget to integrate countermeasures. This workshows how the entire boot security of an Nvidia SoC, used in Tesla\u0026rsquo;s autopilotand Mercedes-Benz\u0026rsquo;s infotainment system, can be circumvented using voltage FI.We uncover a hidden bootloader that is only available to the manufacturer fortesting purposes and disabled by fuses in shipped products. We demonstrate howto re-enable this bootloader using FI to gain code execution with the highestprivileges, enabling us to extract the bootloader\u0026rsquo;s firmware and decryptionkeys used in later boot stages. Using a hardware implant, an adversary mightmisuse the hidden bootloader to bypass trusted code execution even during thesystem\u0026rsquo;s regular operation.\r2021-08-15\nAn Investigation of Replay-based Approaches for Continual Learning\nBenedikt Bagus, Alexander Gepperth\nabstract\rabstract: Continual learning (CL) is a major challenge of machine learning (ML) anddescribes the ability to learn several tasks sequentially without catastrophicforgetting (CF). Recent works indicate that CL is a complex topic, even more sowhen real-world scenarios with multiple constraints are involved. Severalsolution classes have been proposed, of which so-called replay-based approachesseem very promising due to their simplicity and robustness. Such approachesstore a subset of past samples in a dedicated memory for later processing:while this does not solve all problems, good results have been obtained. Inthis article, we empirically investigate replay-based approaches of continuallearning and assess their potential for applications. Selected recentapproaches as well as own proposals are compared on a common set of benchmarks,with a particular focus on assessing the performance of different sampleselection strategies. We find that the impact of sample selection increaseswhen a smaller number of samples is stored. Nevertheless, performance variesstrongly between different replay approaches. Surprisingly, we find that themost naive rehearsal-based approaches that we propose here can outperformrecent state-of-the-art methods.\r2021-08-11\nDimension reduction in recurrent networks by canonicalization\nLyudmila Grigoryeva, Juan-Pablo Ortega\nabstract\rabstract: Many recurrent neural network machine learning paradigms can be formulatedusing state-space representations. The classical notion of canonicalstate-space realization is adapted in this paper to accommodate semi-infiniteinputs so that it can be used as a dimension reduction tool in the recurrentnetworks setup. The so-called input forgetting property is identified as thekey hypothesis that guarantees the existence and uniqueness (up to systemisomorphisms) of canonical realizations for causal and time-invariantinput/output systems with semi-infinite inputs. Additionally, the notion ofoptimal reduction coming from the theory of symmetric Hamiltonian systems isimplemented in our setup to construct canonical realizations out of inputforgetting but not necessarily canonical ones. These two procedures are studiedin detail in the framework of linear fading memory input/output systems.Finally, the notion of implicit reduction using reproducing kernel Hilbertspaces (RKHS) is introduced which allows, for systems with linear readouts, toachieve dimension reduction without the need to actually compute the reducedspaces introduced in the first part of the paper.\r2021-08-09\nUnified Regularity Measures for Sample-wise Learning and Generalization\nChi Zhang, Xiaoning Ma, Yu Liu, Le Wang, Yuanqi Su, Yuehu Liu\nabstract\rabstract: Fundamental machine learning theory shows that different samples contributeunequally both in learning and testing processes. Contemporary studies on DNNimply that such sample difference is rooted on the distribution of intrinsicpattern information, namely sample regularity. Motivated by the recentdiscovery on network memorization and generalization, we proposed a pair ofsample regularity measures for both processes with a formulation-consistentrepresentation. Specifically, cumulative binary training/generalizing loss(CBTL/CBGL), the cumulative number of correct classiffcations of thetraining/testing sample within training stage, is proposed to quantize thestability in memorization-generalization process; whileforgetting/mal-generalizing events, i.e., the mis-classification of previouslylearned or generalized sample, are utilized to represent the uncertainty ofsample regularity with respect to optimization dynamics. Experiments validatedthe effectiveness and robustness of the proposed approaches for mini-batch SGDoptimization. Further applications on training/testing sample selection showthe proposed measures sharing the unified computing procedure could benefit forboth tasks.\r2021-08-05\nQuantum Continual Learning Overcoming Catastrophic Forgetting\nWenjie Jiang, Zhide Lu, Dong-Ling Deng\nabstract\rabstract: Catastrophic forgetting describes the fact that machine learning models willlikely forget the knowledge of previously learned tasks after the learningprocess of a new one. It is a vital problem in the continual learning scenarioand recently has attracted tremendous concern across different communities. Inthis paper, we explore the catastrophic forgetting phenomena in the context ofquantum machine learning. We find that, similar to those classical learningmodels based on neural networks, quantum learning systems likewise suffer fromsuch forgetting problem in classification tasks emerging from variousapplication scenes. We show that based on the local geometrical information inthe loss function landscape of the trained model, a uniform strategy can beadapted to overcome the forgetting problem in the incremental learning setting.Our results uncover the catastrophic forgetting phenomena in quantum machinelearning and offer a practical method to overcome this problem, which opens anew avenue for exploring potential quantum advantages towards continuallearning.\r2021-08-02\nLearn to Forget: Machine Unlearning via Neuron Masking\nYang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip Yu, Kui Ren\nabstract\rabstract: Nowadays, machine learning models, especially neural networks, becomeprevalent in many real-world applications.These models are trained based on aone-way trip from user data: as long as users contribute their data, there isno way to withdraw; and it is well-known that a neural network memorizes itstraining data. This contradicts the \u0026ldquo;right to be forgotten\u0026rdquo; clause of GDPR,potentially leading to law violations. To this end, machine unlearning becomesa popular research topic, which allows users to eliminate memorization of theirprivate data from a trained machine learning model.In this paper, we proposethe first uniform metric called for-getting rate to measure the effectivenessof a machine unlearning method. It is based on the concept of membershipinference and describes the transformation rate of the eliminated data from\u0026quot;memorized\u0026quot; to \u0026ldquo;unknown\u0026rdquo; after conducting unlearning. We also propose a novelunlearning method calledForsaken. It is superior to previous work in eitherutility or efficiency (when achieving the same forgetting rate). We benchmarkForsaken with eight standard datasets to evaluate its performance. Theexperimental results show that it can achieve more than 90% forgetting rate onaverage and only causeless than 5% accuracy loss.\rContinual Learning for Recurrent Neural Networks: an Empirical Evaluation\nAndrea Cossu, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu\nabstract\rabstract: Learning continuously during all model lifetime is fundamental to deploymachine learning solutions robust to drifts in the data distribution. Advancesin Continual Learning (CL) with recurrent neural networks could pave the way toa large number of applications where incoming data is non stationary, likenatural language processing and robotics. However, the existing body of work onthe topic is still fragmented, with approaches which are application-specificand whose assessment is based on heterogeneous learning protocols and datasets.In this paper, we organize the literature on CL for sequential data processingby providing a categorization of the contributions and a review of thebenchmarks. We propose two new benchmarks for CL with sequential data based onexisting datasets, whose characteristics resemble real-world applications. Wealso provide a broad empirical evaluation of CL and Recurrent Neural Networksin class-incremental scenario, by testing their ability to mitigate forgettingwith a number of different strategies which are not specific to sequential dataprocessing. Our results highlight the key role played by the sequence lengthand the importance of a clear specification of the CL scenario.\r2021-07-22\nRemember What You Want to Forget: Algorithms for Machine Unlearning\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh\nabstract\rabstract: We study the problem of unlearning datapoints from a learnt model. Thelearner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,and outputs a model $\\widehat{w}$ that performs well on unseen samples from thesame distribution. However, at some point in the future, any training datapoint$z \\in S$ can request to be unlearned, thus prompting the learner to modify itsoutput model while still ensuring the same accuracy guarantees. We initiate arigorous study of generalization in machine unlearning, where the goal is toperform well on previously unseen datapoints. Our focus is on bothcomputational and storage complexity. For the setting of convex losses, we provide an unlearning algorithm that canunlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. Incomparison, in general, differentially private learning (which impliesunlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. Thisdemonstrates a novel separation between differential privacy and machineunlearning.\r2021-07-16\nCLeaR: An Adaptive Continual Learning Framework for Regression Tasks\nYujiang He, Bernhard Sick\nabstract\rabstract: Catastrophic forgetting means that a trained neural network model graduallyforgets the previously learned tasks when being retrained on new tasks.Overcoming the forgetting problem is a major problem in machine learning.Numerous continual learning algorithms are very successful in incrementallearning of classification tasks, where new samples with their labels appearfrequently. However, there is currently no research that addresses thecatastrophic forgetting problem in regression tasks as far as we know. Thisproblem has emerged as one of the primary constraints in some applications,such as renewable energy forecasts. This article clarifies problem-relateddefinitions and proposes a new methodological framework that can forecasttargets and update itself by means of continual learning. The frameworkconsists of forecasting neural networks and buffers, which store newlycollected data from a non-stationary data stream in an application. The changedprobability distribution of the data stream, which the framework hasidentified, will be learned sequentially. The framework is called CLeaR(Continual Learning for Regression Tasks), where components can be flexiblycustomized for a specific application scenario. We design two sets ofexperiments to evaluate the CLeaR framework concerning fitting error(training), prediction error (test), and forgetting ratio. The first one isbased on an artificial time series to explore how hyperparameters affect theCLeaR framework. The second one is designed with data collected from Europeanwind farms to evaluate the CLeaR framework\u0026rsquo;s performance in a real-worldapplication. The experimental results demonstrate that the CLeaR framework cancontinually acquire knowledge in the data stream and improve the predictionaccuracy. The article concludes with further research issues arising fromrequirements to extend the framework.\r2021-07-02\nTowards Lifelong Learning of End-to-end ASR\nHeng-Jui Chang, Hung-yi Lee, Lin-shan Lee\nabstract\rabstract: Automatic speech recognition (ASR) technologies today are primarily optimizedfor given datasets; thus, any changes in the application environment (e.g.,acoustic conditions or topic domains) may inevitably degrade the performance.We can collect new data describing the new environment and fine-tune thesystem, but this naturally leads to higher error rates for the earlierdatasets, referred to as catastrophic forgetting. The concept of lifelonglearning (LLL) aiming to enable a machine to sequentially learn new tasks fromnew datasets describing the changing real world without forgetting thepreviously learned knowledge is thus brought to attention. This paper reports,to our knowledge, the first effort to extensively consider and analyze the useof various approaches of LLL in end-to-end (E2E) ASR, including proposing novelmethods in saving data for past domains to mitigate the catastrophic forgettingproblem. An overall relative reduction of 28.7% in WER was achieved compared tothe fine-tuning baseline when sequentially learning on three very differentbenchmark corpora. This can be the first step toward the highly desired ASRtechnologies capable of synchronizing with the continuously changing realworld.\rInverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks\nSuryanarayana Maddu, Dominik Sturm, Christian L. Müller, Ivo F. Sbalzarini\nabstract\rabstract: We characterize and remedy a failure mode that may arise from multi-scaledynamics with scale imbalances during training of deep neural networks, such asPhysics Informed Neural Networks (PINNs). PINNs are popular machine-learningtemplates that allow for seamless integration of physical equation models withdata. Their training amounts to solving an optimization problem over a weightedsum of data-fidelity and equation-fidelity objectives. Conflicts betweenobjectives can arise from scale imbalances, heteroscedasticity in the data,stiffness of the physical equation, or from catastrophic interference duringsequential training. We explain the training pathology arising from this andpropose a simple yet effective inverse-Dirichlet weighting strategy toalleviate the issue. We compare with Sobolev training of neural networks,providing the baseline of analytically $\\boldsymbol{\\epsilon}$-optimaltraining. We demonstrate the effectiveness of inverse-Dirichlet weighting invarious applications, including a multi-scale model of active turbulence, wherewe show orders of magnitude improvement in accuracy and convergence overconventional PINN training. For inverse modeling using sequential training, wefind that inverse-Dirichlet weighting protects a PINN against catastrophicforgetting.\r2021-06-15\nCoded Machine Unlearning\nNasser Aldaghri, Hessam Mahdavifar, Ahmad Beirami\nabstract\rabstract: There are applications that may require removing the trace of a sample fromthe system, e.g., a user requests their data to be deleted, or corrupted datais discovered. Simply removing a sample from storage units does not necessarilyremove its entire trace since downstream machine learning models may store someinformation about the samples used to train them. A sample can be perfectlyunlearned if we retrain all models that used it from scratch with that sampleremoved from their training dataset. When multiple such unlearning requests areexpected to be served, unlearning by retraining becomes prohibitivelyexpensive. Ensemble learning enables the training data to be split into smallerdisjoint shards that are assigned to non-communicating weak learners. Eachshard is used to produce a weak model. These models are then aggregated toproduce the final central model. This setup introduces an inherent trade-offbetween performance and unlearning cost, as reducing the shard size reduces theunlearning cost but may cause degradation in performance. In this paper, wepropose a coded learning protocol where we utilize linear encoders to encodethe training data into shards prior to the learning phase. We also present thecorresponding unlearning protocol and show that it satisfies the perfectunlearning criterion. Our experimental results show that the proposed codedmachine unlearning provides a better performance versus unlearning costtrade-off compared to the uncoded baseline.\r2021-06-06\nKnowing when we do not know: Bayesian continual learning for sensing-based analysis tasks\nSandra Servia-Rodriguez, Cecilia Mascolo, Young D. Kwon\nabstract\rabstract: Despite much research targeted at enabling conventional machine learningmodels to continually learn tasks and data distributions sequentially withoutforgetting the knowledge acquired, little effort has been devoted to accountfor more realistic situations where learning some tasks accurately might bemore critical than forgetting previous ones. In this paper we propose aBayesian inference based framework to continually learn a set of real-world,sensing-based analysis tasks that can be tuned to prioritize the remembering ofpreviously learned tasks or the learning of new ones. Our experiments prove therobustness and reliability of the learned models to adapt to the changingsensing environment, and show the suitability of using uncertainty of thepredictions to assess their reliability.\r2021-06-05\nSolving hybrid machine learning tasks by traversing weight space geodesics\nGuruprasad Raghavan, Matt Thomson\nabstract\rabstract: Machine learning problems have an intrinsic geometric structure as centralobjects including a neural network\u0026rsquo;s weight space and the loss functionassociated with a particular task can be viewed as encoding the intrinsicgeometry of a given machine learning problem. Therefore, geometric concepts canbe applied to analyze and understand theoretical properties of machine learningstrategies as well as to develop new algorithms. In this paper, we addressthree seemingly unrelated open questions in machine learning by viewing themthrough a unified framework grounded in differential geometry. Specifically, weview the weight space of a neural network as a manifold endowed with aRiemannian metric that encodes performance on specific tasks. By defining ametric, we can construct geodesic, minimum length, paths in weight space thatrepresent sets of networks of equivalent or near equivalent functionalperformance on a specific task. We, then, traverse geodesic paths whileidentifying networks that satisfy a second objective. Inspired by the geometricinsight, we apply our geodesic framework to 3 major applications: (i) Networksparsification (ii) Mitigating catastrophic forgetting by constructing networkswith high performance on a series of objectives and (iii) Finding high-accuracypaths connecting distinct local optima of deep networks in the non-convex losslandscape. Our results are obtained on a wide range of network architectures(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce ageometric framework that unifies a range of machine learning objectives andthat can be applied to multiple classes of neural network architectures.\r2021-06-01\nA unified PAC-Bayesian framework for machine unlearning via information risk minimization\nSharu Theresa Jose, Osvaldo Simeone\nabstract\rabstract: Machine unlearning refers to mechanisms that can remove the influence of asubset of training data upon request from a trained model without incurring thecost of re-training from scratch. This paper develops a unified PAC-Bayesianframework for machine unlearning that recovers the two recent design principles- variational unlearning (Nguyen et.al., 2020) and forgetting Lagrangian(Golatkar et.al., 2020) - as information risk minimization problems(Zhang,2006). Accordingly, both criteria can be interpreted as PAC-Bayesianupper bounds on the test loss of the unlearned model that take the form of freeenergy metrics.\r2021-05-13\nDeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep Neural Networks\nYingzhe He, Guozhu Meng, Kai Chen, Jinwen He, Xingbo Hu\nabstract\rabstract: Machine unlearning has great significance in guaranteeing model security andprotecting user privacy. Additionally, many legal provisions clearly stipulatethat users have the right to demand model providers to delete their own datafrom training set, that is, the right to be forgotten. The naive way ofunlearning data is to retrain the model without it from scratch, which becomesextremely time and resource consuming at the modern scale of deep neuralnetworks. Other unlearning approaches by refactoring model or training datastruggle to gain a balance between overhead and model usability. In this paper, we propose an approach, dubbed as DeepObliviate, to implementmachine unlearning efficiently, without modifying the normal training mode. Ourapproach improves the original training process by storing intermediate modelson the hard disk. Given a data point to unlearn, we first quantify its temporalresidual memory left in stored models. The influenced models will be retrainedand we decide when to terminate the retraining based on the trend of residualmemory on-the-fly. Last, we stitch an unlearned model by combining theretrained models and uninfluenced models. We extensively evaluate our approachon five datasets and deep learning models. Compared to the method of retrainingfrom scratch, our approach can achieve 99.0%, 95.0%, 91.9%, 96.7%, 74.1%accuracy rates and 66.7$\\times$, 75.0$\\times$, 33.3$\\times$, 29.4$\\times$,13.7$\\times$ speedups on the MNIST, SVHN, CIFAR-10, Purchase, and ImageNetdatasets, respectively. Compared to the state-of-the-art unlearning approach,we improve 5.8% accuracy, 32.5$\\times$ prediction speedup, and reach acomparable retrain speedup under identical settings on average on thesedatasets. Additionally, DeepObliviate can also pass the backdoor-basedunlearning verification.\r2021-05-11\nOne-shot learning for the long term: consolidation with an artificial hippocampal algorithm\nGideon Kowadlo, Abdelrahman Ahmed, David Rawlinson\nabstract\rabstract: Standard few-shot experiments involve learning to efficiently matchpreviously unseen samples by class. We claim that few-shot learning should belong term, assimilating knowledge for the future, without forgetting previousconcepts. In the mammalian brain, the hippocampus is understood to play asignificant role in this process, by learning rapidly and consolidatingknowledge to the neocortex incrementally over a short period. In this researchwe tested whether an artificial hippocampal algorithm (AHA), could be used witha conventional Machine Learning (ML) model that learns incrementally analogousto the neocortex, to achieve one-shot learning both short and long term. Theresults demonstrated that with the addition of AHA, the system could learn inone-shot and consolidate the knowledge for the long term without catastrophicforgetting. This study is one of the first examples of using a CLS model ofhippocampus to consolidate memories, and it constitutes a step toward few-shotcontinual learning.\r2021-05-06\nFederated Unlearning\nGaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, Jiangchuan Liu\nabstract\rabstract: Federated learning (FL) has recently emerged as a promising distributedmachine learning (ML) paradigm. Practical needs of the \u0026ldquo;right to be forgotten\u0026quot;and countering data poisoning attacks call for efficient techniques that canremove, or unlearn, specific training data from the trained FL model. Existingunlearning techniques in the context of ML, however, are no longer in effectfor FL, mainly due to the inherent distinction in the way how FL and ML learnfrom data. Therefore, how to enable efficient data removal from FL modelsremains largely under-explored. In this paper, we take the first step to fillthis gap by presenting FedEraser, the first federated unlearning methodologythat can eliminate the influence of a federated client\u0026rsquo;s data on the global FLmodel while significantly reducing the time used for constructing the unlearnedFL model.The basic idea of FedEraser is to trade the central server\u0026rsquo;s storagefor unlearned model\u0026rsquo;s construction time, where FedEraser reconstructs theunlearned model by leveraging the historical parameter updates of federatedclients that have been retained at the central server during the trainingprocess of FL. A novel calibration method is further developed to calibrate theretained updates, which are further used to promptly construct the unlearnedmodel, yielding a significant speed-up to the reconstruction of the unlearnedmodel while maintaining the model efficacy. Experiments on four realisticdatasets demonstrate the effectiveness of FedEraser, with an expected speed-upof $4\\times$ compared with retraining from the scratch. We envision our work asan early step in FL towards compliance with legal and ethical criteria in afair and transparent manner.\r2021-05-05\nContinual Learning on the Edge with TensorFlow Lite\nGiorgos Demosthenous, Vassilis Vassiliades\nabstract\rabstract: Deploying sophisticated deep learning models on embedded devices with thepurpose of solving real-world problems is a struggle using today\u0026rsquo;s technology.Privacy and data limitations, network connection issues, and the need for fastmodel adaptation are some of the challenges that constitute today\u0026rsquo;s approachesunfit for many applications on the edge and make real-time on-device training anecessity. Google is currently working on tackling these challenges byembedding an experimental transfer learning API to their TensorFlow Lite,machine learning library. In this paper, we show that although transferlearning is a good first step for on-device model training, it suffers fromcatastrophic forgetting when faced with more realistic scenarios. We presentthis issue by testing a simple transfer learning model on the CORe50 benchmarkas well as by demonstrating its limitations directly on an Android applicationwe developed. In addition, we expand the TensorFlow Lite library to includecontinual learning capabilities, by integrating a simple replay approach intothe head of the current transfer learning model. We test our continual learningmodel on the CORe50 benchmark to show that it tackles catastrophic forgetting,and we demonstrate its ability to continually learn, even under non-idealconditions, using the application we developed. Finally, we open-source thecode of our Android application to enable developers to integrate continuallearning to their own smartphone applications, as well as to facilitate furtherdevelopment of continual learning functionality into the TensorFlow Liteenvironment.\r2021-05-01\nA Deep Learning Framework for Lifelong Machine Learning\nCharles X. Ling, Tanner Bohn\nabstract\rabstract: Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong machine learning,few-shot learning, and transfer learning attempt to capture these properties.However, most previous approaches can only demonstrate subsets of theseproperties, often by different complex mechanisms. In this work, we propose asimple yet powerful unified deep learning framework that supports almost all ofthese properties and approaches through one central mechanism. Experiments ontoy examples support our claims. We also draw connections between manypeculiarities of human learning (such as memory loss and \u0026ldquo;rain man\u0026rdquo;) and ourframework. As academics, we often lack resources required to build and train, deepneural networks with billions of parameters on hundreds of TPUs. Thus, whileour framework is still conceptual, and our experiment results are surely notSOTA, we hope that this unified lifelong learning framework inspires new worktowards large-scale experiments and understanding human learning in general. This paper is summarized in two short YouTube videos:https://youtu.be/gCuUyGETbTU (part 1) and https://youtu.be/XsaGI01b-1o (part2).\r2021-04-24\nClass-Incremental Experience Replay for Continual Learning under Concept Drift\nŁukasz Korycki, Bartosz Krawczyk\nabstract\rabstract: Modern machine learning systems need to be able to cope with constantlyarriving and changing data. Two main areas of research dealing with suchscenarios are continual learning and data stream mining. Continual learningfocuses on accumulating knowledge and avoiding forgetting, assuming informationonce learned should be stored. Data stream mining focuses on adaptation toconcept drift and discarding outdated information, assuming that only the mostrecent data is relevant. While these two areas are mainly being developed inseparation, they offer complementary views on the problem of learning fromdynamic data. There is a need for unifying them, by offering architecturescapable of both learning and storing new information, as well as revisiting andadapting to changes in previously seen concepts. We propose a novel continuallearning approach that can handle both tasks. Our experience replay method isfueled by a centroid-driven memory storing diverse instances of incrementallyarriving classes. This is enhanced with a reactive subspace buffer that tracksconcept drift occurrences in previously seen classes and adapts clustersaccordingly. The proposed architecture is thus capable of both rememberingvalid and forgetting outdated information, offering a holistic framework forcontinual learning under concept drift.\r2021-04-23\nLearning in Deep Neural Networks Using a Biologically Inspired Optimizer\nGiorgia Dellaferrera, Stanislaw Wozniak, Giacomo Indiveri, Angeliki Pantazi, Evangelos Eleftheriou\nabstract\rabstract: Plasticity circuits in the brain are known to be influenced by thedistribution of the synaptic weights through the mechanisms of synapticintegration and local regulation of synaptic strength. However, the complexinterplay of stimulation-dependent plasticity with local learning signals isdisregarded by most of the artificial neural network training algorithmsdevised so far. Here, we propose a novel biologically inspired optimizer forartificial (ANNs) and spiking neural networks (SNNs) that incorporates keyprinciples of synaptic integration observed in dendrites of cortical neurons:GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals).GRAPES implements a weight-distribution dependent modulation of the errorsignal at each node of the neural network. We show that this biologicallyinspired mechanism leads to a systematic improvement of the convergence rate ofthe network, and substantially improves classification accuracy of ANNs andSNNs with both feedforward and recurrent architectures. Furthermore, wedemonstrate that GRAPES supports performance scalability for models ofincreasing complexity and mitigates catastrophic forgetting by enablingnetworks to generalize to unseen tasks based on previously acquired knowledge.The local characteristics of GRAPES minimize the required memory resources,making it optimally suited for dedicated hardware implementations. Overall, ourwork indicates that reconciling neurophysiology insights with machineintelligence is key to boosting the performance of neural networks.\r2021-04-18\nSpaRCe: Improved Learning of Reservoir Computing Systems through Sparse Representations\nLuca Manneschi, Andrew C. Lin, Eleni Vasilaki\nabstract\rabstract: \u0026ldquo;Sparse\u0026rdquo; neural networks, in which relatively few neurons or connections areactive, are common in both machine learning and neuroscience. Whereas inmachine learning, \u0026ldquo;sparsity\u0026rdquo; is related to a penalty term that leads to someconnecting weights becoming small or zero, in biological brains, sparsity isoften created when high spiking thresholds prevent neuronal activity. Here weintroduce sparsity into a reservoir computing network via neuron-specificlearnable thresholds of activity, allowing neurons with low thresholds tocontribute to decision-making but suppressing information from neurons withhigh thresholds. This approach, which we term \u0026ldquo;SpaRCe\u0026rdquo;, optimises the sparsitylevel of the reservoir without affecting the reservoir dynamics. The read-outweights and the thresholds are learned by an on-line gradient rule thatminimises an error function on the outputs of the network. Threshold learningoccurs by the balance of two opposing forces: reducing inter-neuronalcorrelations in the reservoir by deactivating redundant neurons, whileincreasing the activity of neurons participating in correct decisions. We testSpaRCe on classification problems and find that threshold learning improvesperformance compared to standard reservoir computing. SpaRCe alleviates theproblem of catastrophic forgetting, a problem most evident in standard echostate networks and recurrent neural networks in general, due to increasing thenumber of task-specialised neurons that are included in the network decisions.\r2021-04-15\nRehearsal revealed: The limits and merits of revisiting samples in continual learning\nEli Verwimp, Matthias De Lange, Tinne Tuytelaars\nabstract\rabstract: Learning from non-stationary data streams and overcoming catastrophicforgetting still poses a serious challenge for machine learning research.Rather than aiming to improve state-of-the-art, in this work we provide insightinto the limits and merits of rehearsal, one of continual learning\u0026rsquo;s mostestablished methods. We hypothesize that models trained sequentially withrehearsal tend to stay in the same low-loss region after a task has finished,but are at risk of overfitting on its sample memory, hence harminggeneralization. We provide both conceptual and strong empirical evidence onthree benchmarks for both behaviors, bringing novel insights into the dynamicsof rehearsal and continual learning in general. Finally, we interpret importantcontinual learning works in the light of our findings, allowing for a deeperunderstanding of their successes.\r2021-04-14\nPruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation\nShuhao Gu, Yang Feng, Wanying Xie\nabstract\rabstract: Domain Adaptation is widely used in practical applications of neural machinetranslation, which aims to achieve good performance on both the general-domainand in-domain. However, the existing methods for domain adaptation usuallysuffer from catastrophic forgetting, domain divergence, and model explosion. Toaddress these three problems, we propose a method of \u0026ldquo;divide and conquer\u0026rdquo; whichis based on the importance of neurons or parameters in the translation model.In our method, we first prune the model and only keep the important neurons orparameters, making them responsible for both general-domain and in-domaintranslation. Then we further train the pruned model supervised by the originalunpruned model with the knowledge distillation method. Last we expand the modelto the original size and fine-tune the added parameters for the in-domaintranslation. We conduct experiments on different languages and domains and theresults show that our method can achieve significant improvements compared withseveral strong baselines.\r2021-04-07\nFew-Shot Incremental Learning with Continually Evolved Classifiers\nChi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu\nabstract\rabstract: Few-shot class-incremental learning (FSCIL) aims to design machine learningalgorithms that can continually learn new concepts from a few data points,without forgetting knowledge of old classes. The difficulty lies in thatlimited data from new classes not only lead to significant overfitting issuesbut also exacerbate the notorious catastrophic forgetting problems. Moreover,as training data come in sequence in FSCIL, the learned classifier can onlyprovide discriminative information in individual sessions, while FSCIL requiresall classes to be involved for evaluation. In this paper, we address the FSCILproblem from two aspects. First, we adopt a simple but effective decoupledlearning strategy of representations and classifiers that only the classifiersare updated in each incremental session, which avoids knowledge forgetting inthe representations. By doing so, we demonstrate that a pre-trained backboneplus a non-parametric class mean classifier can beat state-of-the-art methods.Second, to make the classifiers learned on individual sessions applicable toall classes, we propose a Continually Evolved Classifier (CEC) that employs agraph model to propagate context information between classifiers foradaptation. To enable the learning of CEC, we design a pseudo incrementallearning paradigm that episodically constructs a pseudo incremental learningtask to optimize the graph parameters by sampling data from the base dataset.Experiments on three popular benchmark datasets, including CIFAR100,miniImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our methodsignificantly outperforms the baselines and sets new state-of-the-art resultswith remarkable advantages.\r2021-04-04\nRegularization Shortcomings for Continual Learning\nTimothée Lesort, Andrei Stoian, David Filliat\nabstract\rabstract: In most machine learning algorithms, training data is assumed to beindependent and identically distributed (iid). When it is not the case, thealgorithm\u0026rsquo;s performances are challenged, leading to the famous phenomenon ofcatastrophic forgetting. Algorithms dealing with it are gathered in theContinual Learning research field. In this paper, we study the regularizationbased approaches to continual learning and show that those approaches can notlearn to discriminate classes from different tasks in an elemental continualbenchmark: the class-incremental scenario. We make theoretical reasoning toprove this shortcoming and illustrate it with examples and experiments.Moreover, we show that it can have some important consequences on continualmulti-tasks reinforcement learning or in pre-trained models used for continuallearning. We believe that highlighting and understanding the shortcomings ofregularization strategies will help us to use them more efficiently.\r2021-03-23\nGradient Regularized Contrastive Learning for Continual Domain Adaptation\nShixiang Tang, Peng Su, Dapeng Chen, Wanli Ouyang\nabstract\rabstract: Human beings can quickly adapt to environmental changes by leveraginglearning experience. However, adapting deep neural networks to dynamicenvironments by machine learning algorithms remains a challenge. To betterunderstand this issue, we study the problem of continual domain adaptation,where the model is presented with a labelled source domain and a sequence ofunlabelled target domains. The obstacles in this problem are both domain shiftand catastrophic forgetting. We propose Gradient Regularized ContrastiveLearning (GRCL) to solve the obstacles. At the core of our method, gradientregularization plays two key roles: (1) enforcing the gradient not to harm thediscriminative ability of source features which can, in turn, benefit theadaptation ability of the model to target domains; (2) constraining thegradient not to increase the classification loss on old target domains, whichenables the model to preserve the performance on old target domains whenadapting to an in-coming target domain. Experiments on Digits, DomainNet andOffice-Caltech benchmarks demonstrate the strong performance of our approachwhen compared to the other state-of-the-art methods.\r2021-03-02\nMultiple-timescale Neural Networks: Generation of Context-dependent Sequences and Inference through Autonomous Bifurcations\nTomoki Kurikawa, Kunihiko Kaneko\nabstract\rabstract: Sequential transitions between metastable states are ubiquitously observed inthe neural system and underlie various cognitive functions. Although a numberof studies with asymmetric Hebbian connectivity have investigated how suchsequences are generated, the focused sequences are simple Markov ones. On theother hand, supervised machine learning methods can generate complex non-Markovsequences, but these sequences are vulnerable against perturbations. Further,concatenation of newly learned sequence to the already learned one is difficultdue to catastrophe forgetting, although concatenation is essential forcognitive functions such as inference. How stable complex sequences aregenerated still remains unclear. We have developed a neural network with fastand slow dynamics, which are inspired by the experiments. The slow dynamicsstore history of inputs and outputs and affect the fast dynamics depending onthe stored history. We show the learning rule that requires only localinformation can form the network generating the complex and robust sequences inthe fast dynamics. The slow dynamics work as bifurcation parameters for thefast one, wherein they stabilize the next pattern of the sequence before thecurrent pattern is destabilized. This co-existence period leads to the stabletransition between the current and the next pattern in the sequence. We furtherfind that timescale balance is critical to this period. Our study provides anovel mechanism generating the robust complex sequences with multipletimescales in neural dynamics. Considering the multiple timescales are widelyobserved, the mechanism advances our understanding of temporal processing inthe neural system.\r2021-02-25\nMachine Unlearning via Algorithmic Stability\nEnayat Ullah, Tung Mai, Anup Rao, Ryan Rossi, Raman Arora\nabstract\rabstract: We study the problem of machine unlearning and identify a notion ofalgorithmic stability, Total Variation (TV) stability, which we argue, issuitable for the goal of exact unlearning. For convex risk minimizationproblems, we design TV-stable algorithms based on noisy Stochastic GradientDescent (SGD). Our key contribution is the design of corresponding efficientunlearning algorithms, which are based on constructing a (maximal) coupling ofMarkov chains for the noisy SGD procedure. To understand the trade-offs betweenaccuracy and unlearning efficiency, we give upper and lower bounds on excessempirical and populations risk of TV stable algorithms for convex riskminimization. Our techniques generalize to arbitrary non-convex functions, andour algorithms are differentially private as well.\r2021-02-18\nBayesian Inference Forgetting\nShaopeng Fu, Fengxiang He, Yue Xu, Dacheng Tao\nabstract\rabstract: The right to be forgotten has been legislated in many countries but theenforcement in machine learning would cause unbearable costs: companies mayneed to delete whole models learned from massive resources due to singleindividual requests. Existing works propose to remove the knowledge learnedfrom the requested data via its influence function which is no longer naturallywell-defined in Bayesian inference. This paper proposes a {\\it Bayesianinference forgetting} (BIF) framework to realize the right to be forgotten inBayesian inference. In the BIF framework, we develop forgetting algorithms forvariational inference and Markov chain Monte Carlo. We show that our algorithmscan provably remove the influence of single datums on the learned models.Theoretical analysis demonstrates that our algorithms have guaranteedgeneralizability. Experiments of Gaussian mixture models on the syntheticdataset and Bayesian neural networks on the real-world data verify thefeasibility of our methods. The source code package is available at\\url{https://github.com/fshp971/BIF}.\r2021-01-31\nBeyond the Command: Feminist STS Research and Critical Issues for the Design of Social Machines\nKelly B. Wagman, Lisa Parks\nabstract\rabstract: Machines, from artificially intelligent digital assistants to embodiedrobots, are becoming more pervasive in everyday life. Drawing on feministscience and technology studies (STS) perspectives, we demonstrate how machinedesigners are not just crafting neutral objects, but relationships betweenmachines and humans that are entangled in human social issues such as genderand power dynamics. Thus, in order to create a more ethical and just future,the dominant assumptions currently underpinning the design of thesehuman-machine relations must be challenged and reoriented toward relations ofjustice and inclusivity. This paper contributes the \u0026ldquo;social machine\u0026rdquo; as a modelfor technology designers who seek to recognize the importance, diversity andcomplexity of the social in their work, and to engage with the agential powerof machines. In our model, the social machine is imagined as a potentiallyequitable relationship partner that has agency and as an \u0026ldquo;other\u0026rdquo; that isdistinct from, yet related to, humans, objects, and animals. We criticallyexamine and contrast our model with tendencies in robotics that consider robotsas tools, human companions, animals or creatures, and/or slaves. In doing so,we demonstrate ingrained dominant assumptions about human-machine relations andreveal the challenges of radical thinking in the social machine design space.Finally, we present two design challenges based on non-anthropomorphicfiguration and mutuality, and call for experimentation, unlearning dominanttendencies, and reimagining of sociotechnical futures.\r2021-01-28\nAdaptive Decision Forest: An Incremental Machine Learning Framework\nMd Geaur Rahman, Md Zahidul Islam\nabstract\rabstract: In this study, we present an incremental machine learning framework calledAdaptive Decision Forest (ADF), which produces a decision forest to classifynew records. Based on our two novel theorems, we introduce a new splittingstrategy called iSAT, which allows ADF to classify new records even if they areassociated with previously unseen classes. ADF is capable of identifying andhandling concept drift; it, however, does not forget previously gainedknowledge. Moreover, ADF is capable of handling big data if the data can bedivided into batches. We evaluate ADF on five publicly available natural datasets and one synthetic data set, and compare the performance of ADF against theperformance of eight state-of-the-art techniques. Our experimental results,including statistical sign test and Nemenyi test analyses, indicate a clearsuperiority of the proposed framework over the state-of-the-art techniques.\r2021-01-08\nSlow manifolds in recurrent networks encode working memory efficiently and robustly\nElham Ghazizadeh, ShiNung Ching\nabstract\rabstract: Working memory is a cognitive function involving the storage and manipulationof latent information over brief intervals of time, thus making it crucial forcontext-dependent computation. Here, we use a top-down modeling approach toexamine network-level mechanisms of working memory, an enigmatic issue andcentral topic of study in neuroscience and machine intelligence. We trainthousands of recurrent neural networks on a working memory task and thenperform dynamical systems analysis on the ensuing optimized networks, whereinwe find that four distinct dynamical mechanisms can emerge. In particular, weshow the prevalence of a mechanism in which memories are encoded along slowstable manifolds in the network state space, leading to a phasic neuronalactivation profile during memory periods. In contrast to mechanisms in whichmemories are directly encoded at stable attractors, these networks naturallyforget stimuli over time. Despite this seeming functional disadvantage, theyare more efficient in terms of how they leverage their attractor landscape andparadoxically, are considerably more robust to noise. Our results provide newdynamical hypotheses regarding how working memory function is encoded in bothnatural and artificial neural networks.\r2020-12-31\nIncremental Embedding Learning via Zero-Shot Translation\nKun Wei, Cheng Deng, Xu Yang, Maosen Li\nabstract\rabstract: Modern deep learning methods have achieved great success in machine learningand computer vision fields by learning a set of pre-defined datasets. Howerver,these methods perform unsatisfactorily when applied into real-world situations.The reason of this phenomenon is that learning new tasks leads the trainedmodel quickly forget the knowledge of old tasks, which is referred to ascatastrophic forgetting. Current state-of-the-art incremental learning methodstackle catastrophic forgetting problem in traditional classification networksand ignore the problem existing in embedding networks, which are the basicnetworks for image retrieval, face recognition, zero-shot learning, etc.Different from traditional incremental classification networks, the semanticgap between the embedding spaces of two adjacent tasks is the main challengefor embedding networks under incremental learning setting. Thus, we propose anovel class-incremental method for embedding network, named as zero-shottranslation class-incremental method (ZSTCI), which leverages zero-shottranslation to estimate and compensate the semantic gap without any exemplars.Then, we try to learn a unified representation for two adjacent tasks insequential learning process, which captures the relationships of previousclasses and current classes precisely. In addition, ZSTCI can easily becombined with existing regularization-based incremental learning methods tofurther improve performance of embedding networks. We conduct extensiveexperiments on CUB-200-2011 and CIFAR100, and the experiment results prove theeffectiveness of our method. The code of our method has been released.\r2020-12-21\nIncremental Real-Time Personalization in Human Activity Recognition Using Domain Adaptive Batch Normalization\nAlan Mazankiewicz, Klemens Böhm, Mario Bergés\nabstract\rabstract: Human Activity Recognition (HAR) from devices like smartphone accelerometersis a fundamental problem in ubiquitous computing. Machine learning basedrecognition models often perform poorly when applied to new users that were notpart of the training data. Previous work has addressed this challenge bypersonalizing general recognition models to the unique motion pattern of a newuser in a static batch setting. They require target user data to be availableupfront. The more challenging online setting has received less attention. Nosamples from the target user are available in advance, but they arrivesequentially. Additionally, the motion pattern of users may change over time.Thus, adapting to new and forgetting old information must be traded off.Finally, the target user should not have to do any work to use the recognitionsystem by, say, labeling any activities. Our work addresses all of thesechallenges by proposing an unsupervised online domain adaptation algorithm.Both classification and personalization happen continuously and incrementallyin real time. Our solution works by aligning the feature distributions of allsubjects, be they sources or the target, in hidden neural network layers. Tothis end, we normalize the input of a layer with user-specific mean andvariance statistics. During training, these statistics are computed overuser-specific batches. In the online phase, they are estimated incrementallyfor any new target user.\r2020-12-15\nMachine Unlearning\nLucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, Nicolas Papernot\nabstract\rabstract: Once users have shared their data online, it is generally difficult for themto revoke access and ask for the data to be deleted. Machine learning (ML)exacerbates this problem because any model trained with said data may havememorized it, putting users at risk of a successful privacy attack exposingtheir information. Yet, having models unlearn is notoriously difficult. Weintroduce SISA training, a framework that expedites the unlearning process bystrategically limiting the influence of a data point in the training procedure.While our framework is applicable to any learning algorithm, it is designed toachieve the largest improvements for stateful algorithms like stochasticgradient descent for deep neural networks. SISA training reduces thecomputational overhead associated with unlearning, even in the worst-casesetting where unlearning requests are made uniformly across the training set.In some cases, the service provider may have a prior on the distribution ofunlearning requests that will be issued by users. We may take this prior intoaccount to partition and order data accordingly, and further decrease overheadfrom unlearning. Our evaluation spans several datasets from different domains,with corresponding motivations for unlearning. Under no distributionalassumptions, for simple learning tasks, we observe that SISA training improvestime to unlearn points from the Purchase dataset by 4.63x, and 2.45x for theSVHN dataset, over retraining from scratch. SISA training also provides aspeed-up of 1.36x in retraining for complex learning tasks such as ImageNetclassification; aided by transfer learning, this results in a small degradationin accuracy. Our work contributes to practical data governance in machineunlearning.\r2020-12-08\nContinual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes\nTimothée Lesort\nabstract\rabstract: Humans learn all their life long. They accumulate knowledge from a sequenceof learning experiences and remember the essential concepts without forgettingwhat they have learned previously. Artificial neural networks struggle to learnsimilarly. They often rely on data rigorously preprocessed to learn solutionsto specific problems such as classification or regression. In particular, theyforget their past learning experiences if trained on new ones. Therefore,artificial neural networks are often inept to deal with real-life settings suchas an autonomous-robot that has to learn on-line to adapt to new situations andovercome new problems without forgetting its past learning-experiences.Continual learning (CL) is a branch of machine learning addressing this type ofproblem. Continual algorithms are designed to accumulate and improve knowledgein a curriculum of learning-experiences without forgetting. In this thesis, wepropose to explore continual algorithms with replay processes. Replay processesgather together rehearsal methods and generative replay methods. GenerativeReplay consists of regenerating past learning experiences with a generativemodel to remember them. Rehearsal consists of saving a core-set of samples frompast learning experiences to rehearse them later. The replay processes makepossible a compromise between optimizing the current learning objective and thepast ones enabling learning without forgetting in sequences of tasks settings.We show that they are very promising methods for continual learning. Notably,they enable the re-evaluation of past data with new knowledge and theconfrontation of data from different learning-experiences. We demonstrate theirability to learn continually through unsupervised learning, supervised learningand reinforcement learning tasks.\r2020-12-01\nTowards Probabilistic Verification of Machine Unlearning\nDavid Marco Sommer, Liwei Song, Sameer Wagh, Prateek Mittal\nabstract\rabstract: The right to be forgotten, also known as the right to erasure, is the rightof individuals to have their data erased from an entity storing it. The statusof this long held notion was legally solidified recently by the General DataProtection Regulation (GDPR) in the European Union. Consequently, there is aneed for mechanisms whereby users can verify if service providers comply withtheir deletion requests. In this work, we take the first step in proposing aformal framework to study the design of such verification mechanisms for datadeletion requests \u0026ndash; also known as machine unlearning \u0026ndash; in the context ofsystems that provide machine learning as a service (MLaaS). Our frameworkallows the rigorous quantification of any verification mechanism based onstandard hypothesis testing. Furthermore, we propose a novel backdoor-basedverification mechanism and demonstrate its effectiveness in certifying datadeletion with high confidence, thus providing a basis for quantitativelyinferring machine unlearning. We evaluate our approach over a range of network architectures such asmulti-layer perceptrons (MLP), convolutional neural networks (CNN), residualnetworks (ResNet), and long short-term memory (LSTM), as well as over 5different datasets. We demonstrate that our approach has minimal effect on theML service\u0026rsquo;s accuracy but provides high confidence verification of unlearning.Our proposed mechanism works even if only a handful of users employ our systemto ascertain compliance with data deletion requests. In particular, with just5% of users participating, modifying half their data with a backdoor, and withmerely 30 test queries, our verification mechanism has both false positive andfalse negative ratios below $10^{-3}$. We also show the effectiveness of ourapproach by testing it against an adaptive adversary that uses astate-of-the-art backdoor defense method.\r2020-11-30\nInvestigating Catastrophic Forgetting During Continual Training for Neural Machine Translation\nShuhao Gu, Yang Feng\nabstract\rabstract: Neural machine translation (NMT) models usually suffer from catastrophicforgetting during continual training where the models tend to gradually forgetpreviously learned knowledge and swing to fit the newly added data which mayhave a different distribution, e.g. a different domain. Although many methodshave been proposed to solve this problem, we cannot get to know what causesthis phenomenon yet. Under the background of domain adaptation, we investigatethe cause of catastrophic forgetting from the perspectives of modules andparameters (neurons). The investigation on the modules of the NMT model showsthat some modules have tight relation with the general-domain knowledge whilesome other modules are more essential in the domain adaptation. And theinvestigation on the parameters shows that some parameters are important forboth the general-domain and in-domain translation and the great change of themduring continual training brings about the performance decline ingeneral-domain. We conduct experiments across different language pairs anddomains to ensure the validity and reliability of our findings.\r2020-11-13\nContinual Learning with Deep Artificial Neurons\nBlake Camp, Jaya Krishna Mandivarapu, Rolando Estrada\nabstract\rabstract: Neurons in real brains are enormously complex computational units. Amongother things, they\u0026rsquo;re responsible for transforming inbound electro-chemicalvectors into outbound action potentials, updating the strengths of intermediatesynapses, regulating their own internal states, and modulating the behavior ofother nearby neurons. One could argue that these cells are the only thingsexhibiting any semblance of real intelligence. It is odd, therefore, that themachine learning community has, for so long, relied upon the assumption thatthis complexity can be reduced to a simple sum and fire operation. We ask,might there be some benefit to substantially increasing the computational powerof individual neurons in artificial systems? To answer this question, weintroduce Deep Artificial Neurons (DANs), which are themselves realized as deepneural networks. Conceptually, we embed DANs inside each node of a traditionalneural network, and we connect these neurons at multiple synaptic sites,thereby vectorizing the connections between pairs of cells. We demonstrate thatit is possible to meta-learn a single parameter vector, which we dub a neuronalphenotype, shared by all DANs in the network, which facilitates ameta-objective during deployment. Here, we isolate continual learning as ourmeta-objective, and we show that a suitable neuronal phenotype can endow asingle network with an innate ability to update its synapses with minimalforgetting, using standard backpropagation, without experience replay, norseparate wake/sleep phases. We demonstrate this ability on sequentialnon-linear regression tasks.\r2020-11-02\nEmergent Communication Pretraining for Few-Shot Machine Translation\nYaoyiran Li, Edoardo M. Ponti, Ivan Vulić, Anna Korhonen\nabstract\rabstract: While state-of-the-art models that rely upon massively multilingualpretrained encoders achieve sample efficiency in downstream applications, theystill require abundant amounts of unlabelled text. Nevertheless, most of theworld\u0026rsquo;s languages lack such resources. Hence, we investigate a more radicalform of unsupervised knowledge transfer in the absence of linguistic data. Inparticular, for the first time we pretrain neural networks via emergentcommunication from referential games. Our key assumption is that groundingcommunication on images\u0026mdash;as a crude approximation of real-worldenvironments\u0026mdash;inductively biases the model towards learning natural languages.On the one hand, we show that this substantially benefits machine translationin few-shot settings. On the other hand, this also provides an extrinsicevaluation protocol to probe the properties of emergent languages ex vitro.Intuitively, the closer they are to natural languages, the higher the gainsfrom pretraining on them should be. For instance, in this work we measure theinfluence of communication success and maximum sequence length on downstreamperformances. Finally, we introduce a customised adapter layer and annealingstrategies for the regulariser of maximum-a-posteriori inference duringfine-tuning. These turn out to be crucial to facilitate knowledge transfer andprevent catastrophic forgetting. Compared to a recurrent baseline, our methodyields gains of $59.0%$$\\sim$$147.6%$ in BLEU score with only $500$ NMTtraining instances and $65.1%$$\\sim$$196.7%$ with $1,000$ NMT traininginstances across four language pairs. These proof-of-concept results reveal thepotential of emergent communication pretraining for both natural languageprocessing tasks in resource-poor settings and extrinsic evaluation ofartificial languages.\r2020-10-26\nThree computational models and its equivalence\nCiro Ivan Garcia Lopez\nabstract\rabstract: The study of computability has its origin in Hilbert\u0026rsquo;s conference of 1900,where an adjacent question, to the ones he asked, is to give a precisedescription of the notion of algorithm. In the search for a good definitionarose three independent theories: Turing and the Turing machines, G\u0026quot;odel andthe recursive functions, Church and the Lambda Calculus. Later there were established by Kleene that the classic models of computationare equivalent. This fact is widely accepted by many textbooks and the proof isomitted since the proof is tedious and unreadable. We intend to fill this gappresenting the proof in a modern way, without forgetting the mathematicaldetails.\r2020-10-21\nAmnesiac Machine Learning\nLaura Graves, Vineel Nagisetty, Vijay Ganesh\nabstract\rabstract: The Right to be Forgotten is part of the recently enacted General DataProtection Regulation (GDPR) law that affects any data holder that has data onEuropean Union residents. It gives EU residents the ability to request deletionof their personal data, including training records used to train machinelearning models. Unfortunately, Deep Neural Network models are vulnerable toinformation leaking attacks such as model inversion attacks which extract classinformation from a trained model and membership inference attacks whichdetermine the presence of an example in a model\u0026rsquo;s training data. If a maliciousparty can mount an attack and learn private information that was meant to beremoved, then it implies that the model owner has not properly protected theiruser\u0026rsquo;s rights and their models may not be compliant with the GDPR law. In thispaper, we present two efficient methods that address this question of how amodel owner or data holder may delete personal data from models in such a waythat they may not be vulnerable to model inversion and membership inferenceattacks while maintaining model efficacy. We start by presenting a real-worldthreat model that shows that simply removing training data is insufficient toprotect users. We follow that up with two data removal methods, namelyUnlearning and Amnesiac Unlearning, that enable model owners to protectthemselves against such attacks while being compliant with regulations. Weprovide extensive empirical analysis that show that these methods are indeedefficient, safe to apply, effectively remove learned information aboutsensitive data from trained models while maintaining model efficacy.\r2020-10-19\nUnsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation\nDušan Variš, Ondřej Bojar\nabstract\rabstract: This work presents our ongoing research of unsupervised pretraining in neuralmachine translation (NMT). In our method, we initialize the weights of theencoder and decoder with two language models that are trained with monolingualdata and then fine-tune the model on parallel data using Elastic WeightConsolidation (EWC) to avoid forgetting of the original language modelingtasks. We compare the regularization by EWC with the previous work that focuseson regularization by language modeling objectives. The positive result is thatusing EWC with the decoder achieves BLEU scores similar to the previous work.However, the model converges 2-3 times faster and does not require the originalunlabeled training data during the fine-tuning stage. In contrast, theregularization using EWC is less effective if the original and new tasks arenot closely related. We show that initializing the bidirectional NMT encoderwith a left-to-right language model and forcing the model to remember theoriginal left-to-right language modeling task limits the learning capacity ofthe encoder for the whole bidirectional context.\r2020-10-13\nIncorporating BERT into Parallel Sequence Decoding with Adapters\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong Chen\nabstract\rabstract: While large scale pre-trained language models such as BERT have achievedgreat success on various natural language understanding tasks, how toefficiently and effectively incorporate them into sequence-to-sequence modelsand the corresponding text generation tasks remains a non-trivial problem. Inthis paper, we propose to address this problem by taking two different BERTmodels as the encoder and decoder respectively, and fine-tuning them byintroducing simple and lightweight adapter modules, which are inserted betweenBERT layers and tuned on the task-specific dataset. In this way, we obtain aflexible and efficient model which is able to jointly leverage the informationcontained in the source-side and target-side BERT models, while bypassing thecatastrophic forgetting problem. Each component in the framework can beconsidered as a plug-in unit, making the framework flexible and task agnostic.Our framework is based on a parallel sequence decoding algorithm namedMask-Predict considering the bi-directional and conditional independent natureof BERT, and can be adapted to traditional autoregressive decoding easily. Weconduct extensive experiments on neural machine translation tasks where theproposed method consistently outperforms autoregressive baselines whilereducing the inference latency by half, and achieves $36.49$/$33.57$ BLEUscores on IWSLT14 German-English/WMT14 German-English translation. When adaptedto autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEUscores on WMT14 English-German/English-French translation, on par with thestate-of-the-art baseline models.\r2020-09-08\nImbalanced Continual Learning with Partitioning Reservoir Sampling\nChris Dongjoo Kim, Jinseo Jeong, Gunhee Kim\nabstract\rabstract: Continual learning from a sequential stream of data is a crucial challengefor machine learning research. Most studies have been conducted on this topicunder the single-label classification setting along with an assumption ofbalanced label distribution. This work expands this research horizon towardsmulti-label classification. In doing so, we identify unanticipated adversityinnately existent in many multi-label datasets, the long-tailed distribution.We jointly address the two independently solved problems, CatastropicForgetting and the long-tailed label distribution by first empirically showinga new challenge of destructive forgetting of the minority concepts on the tail.Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow thestudy of both intra- and inter-task imbalances. Lastly, we propose a newsampling strategy for replay-based approach named Partitioning ReservoirSampling (PRS), which allows the model to maintain a balanced knowledge of bothhead and tail classes. We publicly release the dataset and the code in ourproject page.\r2020-08-25\nContinual Domain Adaptation for Machine Reading Comprehension\nLixin Su, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yanyan Lan, Xueqi Cheng\nabstract\rabstract: Machine reading comprehension (MRC) has become a core component in a varietyof natural language processing (NLP) applications such as question answeringand dialogue systems. It becomes a practical challenge that an MRC model needsto learn in non-stationary environments, in which the underlying datadistribution changes over time. A typical scenario is the domain drift, i.e.different domains of data come one after another, where the MRC model isrequired to adapt to the new domain while maintaining previously learnedability. To tackle such a challenge, in this work, we introduce the\\textit{Continual Domain Adaptation} (CDA) task for MRC. So far as we know,this is the first study on the continual learning perspective of MRC. We buildtwo benchmark datasets for the CDA task, by re-organizing existing MRCcollections into different domains with respect to context type and questiontype, respectively. We then analyze and observe the catastrophic forgetting(CF) phenomenon of MRC under the CDA setting. To tackle the CDA task, wepropose several BERT-based continual learning MRC models using eitherregularization-based methodology or dynamic-architecture paradigm. We analyzethe performance of different continual learning MRC models under the CDA taskand show that the proposed dynamic-architecture based model achieves the bestperformance.\r2020-08-08\nFormant Tracking Using Dilated Convolutional Networks Through Dense Connection with Gating Mechanism\nWang Dai, Jinsong Zhang, Yingming Gao, Wei Wei, Dengfeng Ke, Binghuai Lin, Yanlu Xie\nabstract\rabstract: Formant tracking is one of the most fundamental problems in speechprocessing. Traditionally, formants are estimated using signal processingmethods. Recent studies showed that generic convolutional architectures canoutperform recurrent networks on temporal tasks such as speech synthesis andmachine translation. In this paper, we explored the use of TemporalConvolutional Network (TCN) for formant tracking. In addition to theconventional implementation, we modified the architecture from three aspects.First, we turned off the \u0026ldquo;causal\u0026rdquo; mode of dilated convolution, making thedilated convolution see the future speech frames. Second, each hidden layerreused the output information from all the previous layers through denseconnection. Third, we also adopted a gating mechanism to alleviate the problemof gradient disappearance by selectively forgetting unimportant information.The model was validated on the open access formant database VTR. The experimentshowed that our proposed model was easy to converge and achieved an overallmean absolute percent error (MAPE) of 8.2% on speech-labeled frames, comparedto three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9% (TCN).\r2020-08-06\nAn Intelligent Non-Invasive Real Time Human Activity Recognition System for Next-Generation Healthcare\nWilliam Taylor, Syed Aziz Shah, Kia Dashtipour, Adnan Zahid, Qammer H. Abbasi, Muhammad Ali Imran\nabstract\rabstract: Human motion detection is getting considerable attention in the field ofArtificial Intelligence (AI) driven healthcare systems. Human motion can beused to provide remote healthcare solutions for vulnerable people byidentifying particular movements such as falls, gait and breathing disorders.This can allow people to live more independent lifestyles and still have thesafety of being monitored if more direct care is needed. At present wearabledevices can provide real time monitoring by deploying equipment on a person\u0026rsquo;sbody. However, putting devices on a person\u0026rsquo;s body all the time make ituncomfortable and the elderly tends to forget it to wear as well in addition tothe insecurity of being tracked all the time. This paper demonstrates how humanmotions can be detected in quasi-real-time scenario using a non-invasivemethod. Patterns in the wireless signals presents particular human body motionsas each movement induces a unique change in the wireless medium. These changescan be used to identify particular body motions. This work produces a datasetthat contains patterns of radio wave signals obtained using software definedradios (SDRs) to establish if a subject is standing up or sitting down as atest case. The dataset was used to create a machine learning model, which wasused in a developed application to provide a quasi-real-time classification ofstanding or sitting state. The machine learning model was able to achieve 96.70% accuracy using the Random Forest algorithm using 10 fold cross validation. Abenchmark dataset of wearable devices was compared to the proposed dataset andresults showed the proposed dataset to have similar accuracy of nearly 90 %.The machine learning models developed in this paper are tested for twoactivities but the developed system is designed and applicable for detectingand differentiating x number of activities.\r2020-08-04\nApplying Incremental Deep Neural Networks-based Posture Recognition Model for Injury Risk Assessment in Construction\nJunqi Zhao, Esther Obonyo\nabstract\rabstract: Monitoring awkward postures is a proactive prevention for MusculoskeletalDisorders (MSDs)in construction. Machine Learning (ML) models have shownpromising results for posture recognition from Wearable Sensors. However,further investigations are needed concerning: i) Incremental Learning (IL),where trained models adapt to learn new postures and control the forgetting oflearned postures; ii) MSDs assessment with recognized postures. This studyproposed an incremental Convolutional Long Short-Term Memory (CLN) model,investigated effective IL strategies, and evaluated MSDs assessment usingrecognized postures. Tests with nine workers showed the CLN model with shallowconvolutional layers achieved high recognition performance (F1 Score) underpersonalized (0.87) and generalized (0.84) modeling. Generalized shallow CLNmodel under Many-to-One IL scheme can balance the adaptation (0.73) andforgetting of learnt subjects (0.74). MSDs assessment using postures recognizedfrom incremental CLN model had minor difference with ground-truth, whichdemonstrates the high potential for automated MSDs monitoring in construction.\r2020-07-21\nAI Tax: The Hidden Cost of AI Data Center Applications\nDaniel Richins, Dharmisha Doshi, Matthew Blackmore, Aswathy Thulaseedharan Nair, Neha Pathapati, Ankit Patel, Brainard Daguman, Daniel Dobrijalowski, Ramesh Illikkal, Kevin Long, David Zimmerman, Vijay Janapa Reddi\nabstract\rabstract: Artificial intelligence and machine learning are experiencing widespreadadoption in industry and academia. This has been driven by rapid advances inthe applications and accuracy of AI through increasingly complex algorithms andmodels; this, in turn, has spurred research into specialized hardware AIaccelerators. Given the rapid pace of advances, it is easy to forget that theyare often developed and evaluated in a vacuum without considering the fullapplication environment. This paper emphasizes the need for a holistic,end-to-end analysis of AI workloads and reveals the \u0026ldquo;AI tax.\u0026rdquo; We deploy andcharacterize Face Recognition in an edge data center. The application is anAI-centric edge video analytics application built using popular open sourceinfrastructure and ML tools. Despite using state-of-the-art AI and MLalgorithms, the application relies heavily on pre-and post-processing code. AsAI-centric applications benefit from the acceleration promised by accelerators,we find they impose stresses on the hardware and software infrastructure:storage and network bandwidth become major bottlenecks with increasing AIacceleration. By specializing for AI applications, we show that a purpose-builtedge data center can be designed for the stresses of accelerated AI at 15%lower TCO than one derived from homogeneous servers and infrastructure.\r2020-07-19\nDeep learning achieves perfect anomaly detection on 108,308 retinal images including unlearned diseases\nAyaka Suzuki, Yoshiro Suzuki\nabstract\rabstract: Optical coherence tomography (OCT) scanning is useful in detecting variousretinal diseases. However, there are not enough ophthalmologists who candiagnose retinal OCT images in much of the world. To provide OCT screeninginexpensively and extensively, an automated diagnosis system is indispensable.Although many machine learning techniques have been presented for assistingophthalmologists in diagnosing retinal OCT images, there is no technique thatcan diagnose independently without relying on an ophthalmologist, i.e., thereis no technique that does not overlook any anomaly, including unlearneddiseases. As long as there is a risk of overlooking a disease with a technique,ophthalmologists must double-check even those images that the techniqueclassifies as normal. Here, we show that our deep-learning-based binaryclassifier (normal or abnormal) achieved a perfect classification on 108,308two-dimensional retinal OCT images, i.e., true positive rate = 1.000000 andtrue negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000.Although the test set included three types of diseases, two of these were notused for training. However, all test images were correctly classified.Furthermore, we demonstrated that our scheme was able to cope with differencesin patient race. No conventional approach has achieved the above performances.Our work has a sufficient possibility of raising automated diagnosis techniquesfor retinal OCT images from \u0026ldquo;assistant for ophthalmologists\u0026rdquo; to \u0026ldquo;independentdiagnosis system without ophthalmologists\u0026rdquo;.\r2020-07-14\nAnatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics\nVinay V. Ramasesh, Ethan Dyer, Maithra Raghu\nabstract\rabstract: A central challenge in developing versatile machine learning systems iscatastrophic forgetting: a model trained on tasks in sequence will suffersignificant performance drops on earlier tasks. Despite the ubiquity ofcatastrophic forgetting, there is limited understanding of the underlyingprocess and its causes. In this paper, we address this important knowledge gap,investigating how forgetting affects representations in neural network models.Through representational analysis techniques, we find that deeper layers aredisproportionately the source of forgetting. Supporting this, a study ofmethods to mitigate forgetting illustrates that they act to stabilize deeperlayers. These insights enable the development of an analytic argument andempirical picture relating the degree of forgetting to representationalsimilarity between tasks. Consistent with this picture, we observe maximalforgetting occurs for task sequences with intermediate similarity. We performempirical studies on the standard split CIFAR-10 setup and also introduce anovel CIFAR-100 based task approximating realistic input distribution shift.\r2020-07-09\nReducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem\nDanielle Saunders, Bill Byrne\nabstract\rabstract: Training data for NLP tasks often exhibits gender bias in that fewersentences refer to women than to men. In Neural Machine Translation (NMT)gender bias has been shown to reduce translation quality, particularly when thetarget language has grammatical gender. The recent WinoMT challenge set allowsus to measure this effect directly (Stanovsky et al, 2019). Ideally we would reduce system bias by simply debiasing all data prior totraining, but achieving this effectively is itself a challenge. Rather thanattempt to create a balanced' dataset, we use transfer learning on a small setof trusted, gender-balanced examples. This approach gives strong and consistentimprovements in gender debiasing with much less computational cost thantraining from scratch. A known pitfall of transfer learning on new domains is catastrophicforgetting\u0026rsquo;, which we address both in adaptation and in inference. Duringadaptation we show that Elastic Weight Consolidation allows a performancetrade-off between general translation quality and bias reduction. Duringinference we propose a lattice-rescoring scheme which outperforms all systemsevaluated in Stanovsky et al (2019) on WinoMT with no degradation of generaltest set BLEU, and we show this scheme can be applied to remove gender bias inthe output of black box online commercial MT systems. We demonstrate ourapproach translating from English into three languages with varied linguisticproperties and data availability.\r2020-07-08\nMachine Unlearning: Linear Filtration for Logit-based Classifiers\nThomas Baumhauer, Pascal Schöttle, Matthias Zeppelzauer\nabstract\rabstract: Recently enacted legislation grants individuals certain rights to decide inwhat fashion their personal data may be used, and in particular a \u0026ldquo;right to beforgotten\u0026rdquo;. This poses a challenge to machine learning: how to proceed when anindividual retracts permission to use data which has been part of the trainingprocess of a model? From this question emerges the field of machine unlearning,which could be broadly described as the investigation of how to \u0026ldquo;deletetraining data from models\u0026rdquo;. Our work complements this direction of research forthe specific setting of class-wide deletion requests for classification models(e.g. deep neural networks). As a first step, we propose linear filtration as aintuitive, computationally efficient sanitization method. Our experimentsdemonstrate benefits in an adversarial setting over naive deletion schemes.\r2020-07-07\nDynamic memory to alleviate catastrophic forgetting in continuous learning settings\nJohannes Hofmanninger, Matthias Perkonigg, James A. Brink, Oleg Pianykh, Christian Herold, Georg Langs\nabstract\rabstract: In medical imaging, technical progress or changes in diagnostic procedureslead to a continuous change in image appearance. Scanner manufacturer,reconstruction kernel, dose, other protocol specific settings or administeringof contrast agents are examples that influence image content independent of thescanned biology. Such domain and task shifts limit the applicability of machinelearning algorithms in the clinical routine by rendering models obsolete overtime. Here, we address the problem of data shifts in a continuous learningscenario by adapting a model to unseen variations in the source domain whilecounteracting catastrophic forgetting effects. Our method uses a dynamic memoryto facilitate rehearsal of a diverse training data subset to mitigateforgetting. We evaluated our approach on routine clinical CT data obtained withtwo different scanner protocols and synthetic classification tasks. Experimentsshow that dynamic memory counters catastrophic forgetting in a setting withmultiple data shifts without the necessity for explicit knowledge about whenthese shifts occur.\r2020-06-15\nA Conceptual Framework for Lifelong Learning\nCharles X. Ling, Tanner Bohn\nabstract\rabstract: Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong learning, few-shotlearning, and transfer learning, attempt to capture these properties. However,most previous approaches can only demonstrate subsets of these properties,often by different complex mechanisms. In this work, we propose a simple yetpowerful unified framework that supports almost all of these properties andapproaches through one central mechanism. We also draw connections between manypeculiarities of human learning (such as memory loss and \u0026ldquo;rain man\u0026rdquo;) and ourframework. While we do not present any state-of-the-art results, we hope thatthis conceptual framework provides a novel perspective on existing work andproposes many new research directions.\r2020-06-11\nVisualizing and Understanding Vision System\nFeng Qi, Guanjun Jiang\nabstract\rabstract: How the human vision system addresses the object identity-preservingrecognition problem is largely unknown. Here, we use a visionrecognition-reconstruction network (RRN) to investigate the development,recognition, learning and forgetting mechanisms, and achieve similarcharacteristics to electrophysiological measurements in monkeys. First, innetwork development study, the RRN also experiences critical developmentalstages characterized by specificities in neuron types, synapse and activationpatterns, and visual task performance from the early stage of coarse saliencemap recognition to mature stage of fine structure recognition. In digitrecognition study, we witness that the RRN could maintain object invariancerepresentation under various viewing conditions by coordinated adjustment ofresponses of population neurons. And such concerted population responsescontained untangled object identity and properties information that could beaccurately extracted via high-level cortices or even a simple weightedsummation decoder. In the learning and forgetting study, novel structurerecognition is implemented by adjusting entire synapses in low magnitude whilepattern specificities of original synaptic connectivity are preserved, whichguaranteed a learning process without disrupting the existing functionalities.This work benefits the understanding of the human visual processing mechanismand the development of human-like machine intelligence.\r2020-05-13\nProgressive growing of self-organized hierarchical representations for exploration\nMayalen Etcheverry, Pierre-Yves Oudeyer, Chris Reinke\nabstract\rabstract: Designing agent that can autonomously discover and learn a diversity ofstructures and skills in unknown changing environments is key for lifelongmachine learning. A central challenge is how to learn incrementallyrepresentations in order to progressively build a map of the discoveredstructures and re-use it to further explore. To address this challenge, weidentify and target several key functionalities. First, we aim to build lastingrepresentations and avoid catastrophic forgetting throughout the explorationprocess. Secondly we aim to learn a diversity of representations allowing todiscover a \u0026ldquo;diversity of diversity\u0026rdquo; of structures (and associated skills) incomplex high-dimensional environments. Thirdly, we target representations thatcan structure the agent discoveries in a coarse-to-fine manner. Finally, wetarget the reuse of such representations to drive exploration toward an\u0026quot;interesting\u0026quot; type of diversity, for instance leveraging human guidance.Current approaches in state representation learning rely generally onmonolithic architectures which do not enable all these functionalities.Therefore, we present a novel technique to progressively construct a Hierarchyof Observation Latent Models for Exploration Stratification, called HOLMES.This technique couples the use of a dynamic modular model architecture forrepresentation learning with intrinsically-motivated goal exploration processes(IMGEPs). The paper shows results in the domain of automated discovery ofdiverse self-organized patterns, considering as testbed the experimentalframework from Reinke et al. (2019).\r2020-04-08\nContinual Learning with Gated Incremental Memories for sequential data processing\nAndrea Cossu, Antonio Carta, Davide Bacciu\nabstract\rabstract: The ability to learn in dynamic, nonstationary environments withoutforgetting previous knowledge, also known as Continual Learning (CL), is a keyenabler for scalable and trustworthy deployments of adaptive solutions. Whilethe importance of continual learning is largely acknowledged in machine visionand reinforcement learning problems, this is mostly under-documented forsequence processing tasks. This work proposes a Recurrent Neural Network (RNN)model for CL that is able to deal with concept drift in input distributionwithout forgetting previously acquired knowledge. We also implement and test apopular CL approach, Elastic Weight Consolidation (EWC), on top of twodifferent types of RNNs. Finally, we compare the performances of our enhancedarchitecture against EWC and RNNs on a set of standard CL benchmarks, adaptedto the sequential data processing scenario. Results show the superiorperformance of our architecture and highlight the need for special solutionsdesigned to address CL in RNNs.\r2020-03-20\nOnline Continual Learning on Sequences\nGerman I. Parisi, Vincenzo Lomonaco\nabstract\rabstract: Online continual learning (OCL) refers to the ability of a system to learnover time from a continuous stream of data without having to revisit previouslyencountered training samples. Learning continually in a single data pass iscrucial for agents and robots operating in changing environments and requiredto acquire, fine-tune, and transfer increasingly complex representations fromnon-i.i.d. input distributions. Machine learning models that address OCL mustalleviate \\textit{catastrophic forgetting} in which hidden representations aredisrupted or completely overwritten when learning from streams of novel input.In this chapter, we summarize and discuss recent deep learning models thataddress OCL on sequential input through the use (and combination) of synapticregularization, structural plasticity, and experience replay. Differentimplementations of replay have been proposed that alleviate catastrophicforgetting in connectionists architectures via the re-occurrence of (latentrepresentations of) input sequences and that functionally resemble mechanismsof hippocampal replay in the mammalian brain. Empirical evidence shows thatarchitectures endowed with experience replay typically outperform architectureswithout in (online) incremental learning tasks.\r2020-03-07\nSensAI+Expanse Adaptation on Human Behaviour Towards Emotional Valence Prediction\nNuno A. C. Henriques, Helder Coelho, Leonel Garcia-Marques\nabstract\rabstract: An agent, artificial or human, must be continuously adjusting its behaviourin order to thrive in a more or less demanding environment. An artificial agentwith the ability to predict human emotional valence in a geospatial andtemporal context requires proper adaptation to its mobile device environmentwith resource consumption strict restrictions (e.g., power from battery). Thedeveloped distributed system includes a mobile device embodied agent (SensAI)plus Cloud-expanded (Expanse) cognition and memory resources. The system isdesigned with several adaptive mechanisms in a best effort for the agent tocope with its interacting humans and to be resilient on collecting data formachine learning towards prediction. These mechanisms encompasshomeostatic-like adjustments such as auto recovering from an unexpected failurein the mobile device, forgetting repeated data to save local memory, adjustingactions to a proper moment (e.g., notify only when human is interacting), andthe Expanse complementary learning algorithms\u0026rsquo; parameters with autoadjustments. Regarding emotional valence prediction performance, results from acomparison study between state-of-the-art algorithms revealed Extreme GradientBoosting on average the best model for prediction with efficient energy use,and explainable using feature importance inspection. Therefore, this workcontributes with a smartphone sensing-based system, distributed in the Cloud,robust to unexpected behaviours from humans and the environment, able topredict emotional valence states with very good performance.\r2020-03-04\nLearning to Continually Learn\nShawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, Nick Cheney\nabstract\rabstract: Continual lifelong learning requires an agent or model to learn manysequentially ordered tasks, building on previous knowledge withoutcatastrophically forgetting it. Much work has gone towards preventing thedefault tendency of machine learning models to catastrophically forget, yetvirtually all such work involves manually-designed solutions to the problem. Weinstead advocate meta-learning a solution to catastrophic forgetting, allowingAI to learn to continually learn. Inspired by neuromodulatory processes in thebrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). Itdifferentiates through a sequential learning process to meta-learn anactivation-gating function that enables context-dependent selective activationwithin a deep neural network. Specifically, a neuromodulatory (NM) neuralnetwork gates the forward pass of another (otherwise normal) neural networkcalled the prediction learning network (PLN). The NM network also thusindirectly controls selective plasticity (i.e. the backward pass of) the PLN.ANML enables continual learning without catastrophic forgetting at scale: itproduces state-of-the-art continual learning performance, sequentially learningas many as 600 classes (over 9,000 SGD updates).\rA Snooze-less User-Aware Notification System for Proactive Conversational Agents\nYara Rizk, Vatche Isahagian, Merve Unuvar, Yasaman Khazaeni\nabstract\rabstract: The ubiquity of smart phones and electronic devices has placed a wealth ofinformation at the fingertips of consumers as well as creators of digitalcontent. This has led to millions of notifications being issued each secondfrom alerts about posted YouTube videos to tweets, emails and personalmessages. Adding work related notifications and we can see how quickly thenumber of notifications increases. Not only does this cause reducedproductivity and concentration but has also been shown to cause alert fatigue.This condition makes users desensitized to notifications, causing them toignore or miss important alerts. Depending on what domain users work in, thecost of missing a notification can vary from a mere inconvenience to life anddeath. Therefore, in this work, we propose an alert and notification frameworkthat intelligently issues, suppresses and aggregates notifications, based onevent severity, user preferences, or schedules, to minimize the need for usersto ignore, or snooze their notifications and potentially forget aboutaddressing important ones. Our framework can be deployed as a backend service,but is better suited to be integrated into proactive conversational agents, afield receiving a lot of attention with the digital transformation era, emailservices, news services and others. However, the main challenge lies indeveloping the right machine learning algorithms that can learn models from awide set of users while customizing these models to individual users\u0026rsquo;preferences.\r2020-01-27\nUncertainty-based Modulation for Lifelong Learning\nAndrew Brna, Ryan Brown, Patrick Connolly, Stephen Simons, Renee Shimizu, Mario Aguilar-Simon\nabstract\rabstract: The creation of machine learning algorithms for intelligent agents capable ofcontinuous, lifelong learning is a critical objective for algorithms beingdeployed on real-life systems in dynamic environments. Here we present analgorithm inspired by neuromodulatory mechanisms in the human brain thatintegrates and expands upon Stephen Grossberg's ground-breaking AdaptiveResonance Theory proposals. Specifically, it builds on the concept ofuncertainty, and employs a series of neuromodulatory mechanisms to enablecontinuous learning, including self-supervised and one-shot learning. Algorithmcomponents were evaluated in a series of benchmark experiments that demonstratestable learning without catastrophic forgetting. We also demonstrate thecritical role of developing these systems in a closed-loop manner where theenvironment and the agent's behaviors constrain and guide the learningprocess. To this end, we integrated the algorithm into an embodied simulateddrone agent. The experiments show that the algorithm is capable of continuouslearning of new tasks and under changed conditions with high classificationaccuracy (greater than 94 percent) in a virtual environment, withoutcatastrophic forgetting. The algorithm accepts high dimensional inputs from anystate-of-the-art detection and feature extraction algorithms, making it aflexible addition to existing systems. We also describe future developmentefforts focused on imbuing the algorithm with mechanisms to seek out newknowledge as well as employ a broader range of neuromodulatory processes.\r2020-01-24\nAn Adaptive Random Path Selection Approach for Incremental Learning\nJathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, Ming-Hsuan Yang\nabstract\rabstract: In a conventional supervised learning setting, a machine learning model hasaccess to examples of all object classes that are desired to be recognizedduring the inference stage. This results in a fixed model that lacks theflexibility to adapt to new learning tasks. In practical settings, learningtasks often arrive in a sequence and the models must continually learn toincrement their previously acquired knowledge. Existing incremental learningapproaches fall well below the state-of-the-art cumulative models that use alltraining classes at once. In this paper, we propose a random path selectionalgorithm, called Adaptive RPS-Net, that progressively chooses optimal pathsfor the new tasks while encouraging parameter sharing between tasks. Weintroduce a new network capacity measure that enables us to automaticallyswitch paths if the already used resources are saturated. Since the proposedpath-reuse strategy ensures forward knowledge transfer, our approach isefficient and has considerably less computation overhead. As an added novelty,the proposed model integrates knowledge distillation and retrospection alongwith the path selection strategy to overcome catastrophic forgetting. In orderto maintain an equilibrium between previous and newly acquired knowledge, wepropose a simple controller to dynamically balance the model plasticity.Through extensive experiments, we demonstrate that the Adaptive RPS-Net methodsurpasses the state-of-the-art performance for incremental learning and byutilizing parallel computation this method can run in constant time with nearlythe same efficiency as a conventional deep convolutional neural network.\rSpaced Repetition and Mnemonics Enable Recall of Multiple Strong Passwords\nJeremiah Blocki, Saranga Komanduri, Lorrie Cranor, Anupam Datta\nabstract\rabstract: We report on a user study that provides evidence that spaced repetition and aspecific mnemonic technique enable users to successfully recall multiple strongpasswords over time. Remote research participants were asked to memorize 4Person-Action-Object (PAO) stories where they chose a famous person from adrop-down list and were given machine-generated random action-object pairs.Users were also shown a photo of a scene and asked to imagine the PAO storytaking place in the scene (e.g., Bill Gates\u0026mdash;swallowing\u0026mdash;bike on a beach).Subsequently, they were asked to recall the action-object pairs when promptedwith the associated scene-person pairs following a spaced repetition scheduleover a period of 127+ days. While we evaluated several spaced repetitionschedules, the best results were obtained when users initially returned after12 hours and then in $1.5\\times$ increasing intervals: 77% of the participantssuccessfully recalled all 4 stories in 10 tests over a period of 158 days. Muchof the forgetting happened in the first test period (12 hours): 89% ofparticipants who remembered their stories during the first test periodsuccessfully remembered them in every subsequent round. These findings, coupledwith recent results on naturally rehearsing password schemes, suggest that 4PAO stories could be used to create usable and strong passwords for 14sensitive accounts following this spaced repetition schedule, possibly with afew extra upfront rehearsals. In addition, we find that there is aninterference effect across multiple PAO stories: the recall rate of 100% (resp.90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAOstories) is significantly better than the recall rate for participants who wereasked to memorize 4 PAO stories. These findings yield concrete advice forimproving constructions of password management schemes and future user studies.\r2019-12-01\nAn Enhanced Machine Learning-based Biometric Authentication System Using RR-Interval Framed Electrocardiograms\nAmang Song-Kyoo Kim, Chan Yeob Yeun, Paul D. Yoo\nabstract\rabstract: This paper is targeted in the area of biometric data enabled security systembased on the machine learning for the digital health. The disadvantages oftraditional authentication systems include the risks of forgetfulness, loss,and theft. Biometric authentication is therefore rapidly replacing traditionalauthentication methods and is becoming an everyday part of life. Theelectrocardiogram (ECG) was recently introduced as a biometric authenticationsystem suitable for security checks. The proposed authentication system helpsinvestigators studying ECG-based biometric authentication techniques to reshapeinput data by slicing based on the RR-interval, and defines the OverallPerformance (OP), which is the combined performance metric of multipleauthentication measures. We evaluated the performance of the proposed systemusing a confusion matrix and achieved up to 95% accuracy by compact dataanalysis. We also used the Amang ECG (amgecg) toolbox in MATLAB to investigatethe upper-range control limit (UCL) based on the mean square error, whichdirectly affects three authentication performance metrics: the accuracy, thenumber of accepted samples, and the OP. Using this approach, we found that theOP can be optimized by using a UCL of 0.0028, which indicates 61 acceptedsamples out of 70 and ensures that the proposed authentication system achievesan accuracy of 95%.\r2019-11-22\nContinual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges\nTimothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, Natalia Díaz-Rodríguez\nabstract\rabstract: Continual learning (CL) is a particular machine learning paradigm where thedata distribution and learning objective changes through time, or where all thetraining data and objective criteria are never available at once. The evolutionof the learning process is modeled by a sequence of learning experiences wherethe goal is to be able to learn new skills all along the sequence withoutforgetting what has been previously learned. Continual learning also aims atthe same time at optimizing the memory, the computation power and the speedduring the learning process. An important challenge for machine learning is not necessarily findingsolutions that work in the real world but rather finding stable algorithms thatcan learn in real world. Hence, the ideal approach would be tackling the realworld in a embodied platform: an autonomous agent. Continual learning wouldthen be effective in an autonomous agent or robot, which would learnautonomously through time about the external world, and incrementally develop aset of complex skills and knowledge. Robotic agents have to learn to adapt and interact with their environmentusing a continuous stream of observations. Some recent approaches aim attackling continual learning for robotics, but most recent papers on continuallearning only experiment approaches in simulation or with static datasets.Unfortunately, the evaluation of those algorithms does not provide insights onwhether their solutions may help continual learning in the context of robotics.This paper aims at reviewing the existing state of the art of continuallearning, summarizing existing benchmarks and metrics, and proposing aframework for presenting and evaluating both robotics and non roboticsapproaches in a way that makes transfer between both fields easier.\r2019-11-15\nFeedback Linearization based on Gaussian Processes with event-triggered Online Learning\nJonas Umlauft, Sandra Hirche\nabstract\rabstract: Combining control engineering with nonparametric modeling techniques frommachine learning allows to control systems without analytic description usingdata-driven models. Most existing approaches separate learning, i.e. the systemidentification based on a fixed dataset, and control, i.e. the execution of themodel-based control law. This separation makes the performance highly sensitiveto the initial selection of training data and possibly requires very largedatasets. This article proposes a learning feedback linearizing control lawusing online closed-loop identification. The employed Gaussian process modelupdates its training data only if the model uncertainty becomes too large. Thisevent-triggered online learning ensures high data efficiency and therebyreduces the computational complexity, which is a major barrier for usingGaussian processes under real-time constraints. We propose safe forgettingstrategies of data points to adhere to budget constraint and to furtherincrease data-efficiency. We show asymptotic stability for the tracking errorunder the proposed event-triggering law and illustrate the effectiveidentification and control in simulation.\r2019-11-12\nLearning from the Past: Continual Meta-Learning via Bayesian Graph Modeling\nYadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Mahsa Baktashmotlagh, Yang Yang\nabstract\rabstract: Meta-learning for few-shot learning allows a machine to leverage previouslyacquired knowledge as a prior, thus improving the performance on novel taskswith only small amounts of data. However, most mainstream models suffer fromcatastrophic forgetting and insufficient robustness issues, thereby failing tofully retain or exploit long-term knowledge while being prone to cause severeerror accumulation. In this paper, we propose a novel Continual Meta-Learningapproach with Bayesian Graph Neural Networks (CML-BGNN) that mathematicallyformulates meta-learning as continual learning of a sequence of tasks. Witheach task forming as a graph, the intra- and inter-task correlations can bewell preserved via message-passing and history transition. To remedytopological uncertainty from graph initialization, we utilize Bayes by Backpropstrategy that approximates the posterior distribution of task-specificparameters with amortized inference networks, which are seamlessly integratedinto the end-to-end edge learning. Extensive experiments conducted on theminiImageNet and tieredImageNet datasets demonstrate the effectiveness andefficiency of the proposed method, improving the performance by 42.8% comparedwith state-of-the-art on the miniImageNet 5-way 1-shot classification task.\r2019-10-29\nOnline Continual Learning with Maximally Interfered Retrieval\nRahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, Tinne Tuytelaars\nabstract\rabstract: Continual learning, the setting where a learning agent is faced with a neverending stream of data, continues to be a great challenge for modern machinelearning systems. In particular the online or \u0026ldquo;single-pass through the data\u0026quot;setting has gained attention recently as a natural setting that is difficult totackle. Methods based on replay, either generative or from a stored memory,have been shown to be effective approaches for continual learning, matching orexceeding the state of the art in a number of standard benchmarks. Theseapproaches typically rely on randomly selecting samples from the replay memoryor from a generative model, which is suboptimal. In this work, we consider acontrolled sampling of memories for replay. We retrieve the samples which aremost interfered, i.e. whose prediction will be most negatively impacted by theforeseen parameters update. We show a formulation for this sampling criterionin both the generative replay and the experience replay setting, producingconsistent gains in performance and greatly reduced forgetting. We release animplementation of our method athttps://github.com/optimass/Maximally_Interfered_Retrieval.\r2019-10-18\nContinual Learning in Neural Networks\nRahaf Aljundi\nabstract\rabstract: Artificial neural networks have exceeded human-level performance inaccomplishing several individual tasks (e.g. voice recognition, objectrecognition, and video games). However, such success remains modest compared tohuman intelligence that can learn and perform an unlimited number of tasks.Humans\u0026rsquo; ability of learning and accumulating knowledge over their lifetime isan essential aspect of their intelligence. Continual machine learning aims at ahigher level of machine intelligence through providing the artificial agentswith the ability to learn online from a non-stationary and never-ending streamof data. A key component of such a never-ending learning process is to overcomethe catastrophic forgetting of previously seen data, a problem that neuralnetworks are well known to suffer from. The work described in this thesis hasbeen dedicated to the investigation of continual learning and solutions tomitigate the forgetting phenomena in neural networks. To approach the continuallearning problem, we first assume a task incremental setting where tasks arereceived one at a time and data from previous tasks are not stored. Since thetask incremental setting can\u0026rsquo;t be assumed in all continual learning scenarios,we also study the more general online continual setting. We consider aninfinite stream of data drawn from a non-stationary distribution with asupervisory or self-supervisory training signal. The proposed methods in thisthesis have tackled important aspects of continual learning. They wereevaluated on different benchmarks and over various learning sequences. Advancesin the state of the art of continual learning have been shown and challengesfor bringing continual learning into application were critically identified.\r2019-09-18\nDeep Trustworthy Knowledge Tracing\nHeonseok Ha, Uiwon Hwang, Yongjun Hong, Jahee Jang, Sungroh Yoon\nabstract\rabstract: Knowledge tracing (KT), a key component of an intelligent tutoring system, isa machine learning technique that estimates the mastery level of a studentbased on his/her past performance. The objective of KT is to predict astudent\u0026rsquo;s response to the next question. Compared with traditional KT models,deep learning-based KT (DLKT) models show better predictive performance becauseof the representation power of deep neural networks. Various methods have beenproposed to improve the performance of DLKT, but few studies have beenconducted on the reliability of DLKT. In this work, we claim that the existingDLKTs are not reliable in real education environments. To substantiate theclaim, we show limitations of DLKT from various perspectives such as knowledgestate update failure, catastrophic forgetting, and non-interpretability. Wethen propose a novel regularization to address these problems. The proposedmethod allows us to achieve trustworthy DLKT. In addition, the proposed modelwhich is trained on scenarios with forgetting can also be easily extended toscenarios without forgetting.\r2019-09-16\nAdaBoost-assisted Extreme Learning Machine for Efficient Online Sequential Classification\nYi-Ta Chen, Yu-Chuan Chuang, An-Yeu, Wu\nabstract\rabstract: In this paper, we propose an AdaBoost-assisted extreme learning machine forefficient online sequential classification (AOS-ELM). In order to achievebetter accuracy in online sequential learning scenarios, we utilize thecost-sensitive algorithm-AdaBoost, which diversifying the weak classifiers, andadding the forgetting mechanism, which stabilizing the performance during thetraining procedure. Hence, AOS-ELM adapts better to sequentially arrived datacompared with other voting based methods. The experiment results show AOS-ELMcan achieve 94.41% accuracy on MNIST dataset, which is the theoretical accuracybound performed by an original batch learning algorithm, AdaBoost-ELM.Moreover, with the forgetting mechanism, the standard deviation of accuracyduring the online sequential learning process is reduced to 8.26x.\r2019-09-03\nThe Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives\nElena Voita, Rico Sennrich, Ivan Titov\nabstract\rabstract: We seek to understand how the representations of individual tokens and thestructure of the learned feature space evolve between layers in deep neuralnetworks under different learning objectives. We focus on the Transformers forour analysis as they have been shown effective on various tasks, includingmachine translation (MT), standard left-to-right language models (LM) andmasked language modeling (MLM). Previous work used black-box probing tasks toshow that the representations learned by the Transformer differ significantlydepending on the objective. In this work, we use canonical correlation analysisand mutual information estimators to study how information flows acrossTransformer layers and how this process depends on the choice of learningobjective. For example, as you go from bottom to top layers, information aboutthe past in left-to-right language models gets vanished and predictions aboutthe future get formed. In contrast, for MLM, representations initially acquireinformation about the context around the token, partially forgetting the tokenidentity and producing a more generalized token representation. The tokenidentity then gets recreated at the top MLM layers.\r2019-07-22\nThe peculiar statistical mechanics of Optimal Learning Machines\nMatteo Marsili\nabstract\rabstract: Optimal Learning Machines (OLM) are systems that extract maximallyinformative representation of the environment they are in contact with, or ofthe data they are presented. It has recently been suggested that these systemsare characterised by an exponential distribution of energy levels. In order tounderstand the peculiar properties of OLM within a broader framework, Iconsider an ensemble of optimisation problems over functions of many variables,part of which describe a sub-system and the rest account for its interactionwith a random environment. The number of states of the sub-system with a givenvalue of the objective function obeys a stretched exponential distribution,with exponent $\\gamma$, and the interaction part is drawn at random from thesame distribution, independently for each configuration of the whole system.Systems with $\\gamma=1$ then correspond to OLM, and we find that they sit atthe boundary between two regions with markedly different properties. For all$\\gamma\u0026gt;0$ the system exhibits a freezing phase transition. The transition isdiscontinuous for $\\gamma\u0026lt;1$ and it is continuous for $\\gamma\u0026gt;1$. The region$\\gamma\u0026gt;1$ corresponds to learnable energy landscapes and the behaviour of thesub-system becomes predictable as the size of the environment exceeds acritical threshold. For $\\gamma\u0026lt;1$, instead, the energy landscape isunlearnable and the behaviour of the system becomes more and more unpredictableas the size of the environment increases. Sub-systems with $\\gamma=1$ (OLM)feature a behaviour which is independent of the relative size of theenvironment. This is consistent with the expectation that efficientrepresentations should be largely independent of the level of detail of thedescription of the environment.\r2019-06-18\nNon-Parametric Adaptation for Neural Machine Translation\nAnkur Bapna, Orhan Firat\nabstract\rabstract: Neural Networks trained with gradient descent are known to be susceptible tocatastrophic forgetting caused by parameter shift during the training process.In the context of Neural Machine Translation (NMT) this results in poorperformance on heterogeneous datasets and on sub-tasks like rare phrasetranslation. On the other hand, non-parametric approaches are immune toforgetting, perfectly complementing the generalization ability of NMT. However,attempts to combine non-parametric or retrieval based approaches with NMT haveonly been successful on narrow domains, possibly due to over-reliance onsentence level retrieval. We propose a novel n-gram level retrieval approachthat relies on local phrase level similarities, allowing us to retrieveneighbors that are useful for translation even when overall sentence similarityis low. We complement this with an expressive neural network, allowing ourmodel to extract information from the noisy retrieved context. We evaluate oursemi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT,JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets.The semi-parametric nature of our approach opens the door for non-parametricdomain adaptation, demonstrating strong inference-time adaptation performanceon new domains without the need for any parameter updates.\r2019-06-12\nData-driven Local Control Design using Optimization and Machine Learning Techniques\nStavros Karagiannopoulos, Petros Aristidou, Gabriela Hug\nabstract\rabstract: The optimal control of distribution networks often requires monitoring andcommunication infrastructure, either centralized or distributed. However, mostof the current distribution systems lack this kind of infrastructure and relyon suboptimal, fit-and-forget, local controls to ensure the security of thenetwork. In this paper, we propose a data-driven algorithm that uses historicaldata, advanced optimization techniques, and machine learning methods, to designlocal controls that emulate the optimal behavior without the use of anycommunication. We demonstrate the performance of the optimized local control ona three-phase, unbalanced, low-voltage, distribution network. The results showthat our data-driven methodology clearly outperforms standard industry localcontrol and successfully imitates an optimal-power-flow-based control.\r2019-05-30\nLarge Scale Incremental Learning\nYue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Yun Fu\nabstract\rabstract: Modern machine learning suffers from catastrophic forgetting when learningnew classes incrementally. The performance dramatically degrades due to themissing data of old classes. Incremental learning methods have been proposed toretain the knowledge acquired from the old classes, by using knowledgedistilling and keeping a few exemplars from the old classes. However, thesemethods struggle to scale up to a large number of classes. We believe this isbecause of the combination of two factors: (a) the data imbalance between theold and new classes, and (b) the increasing number of visually similar classes.Distinguishing between an increasing number of visually similar classes isparticularly challenging, when the training data is unbalanced. We propose asimple and effective method to address this data imbalance issue. We found thatthe last fully connected layer has a strong bias towards the new classes, andthis bias can be corrected by a linear model. With two bias parameters, ourmethod performs remarkably well on two large datasets: ImageNet (1000 classes)and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithmsby 11.1% and 13.2% respectively.\r2019-05-21\nLearn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting\nXilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong\nabstract\rabstract: Addressing catastrophic forgetting is one of the key challenges in continuallearning where machine learning systems are trained with sequential orstreaming tasks. Despite recent remarkable progress in state-of-the-art deeplearning, deep neural networks (DNNs) are still plagued with the catastrophicforgetting problem. This paper presents a conceptually simple yet general andeffective framework for handling catastrophic forgetting in continual learningwith DNNs. The proposed method consists of two components: a neural structureoptimization component and a parameter learning and/or fine-tuning component.By separating the explicit neural structure learning and the parameterestimation, not only is the proposed method capable of evolving neuralstructures in an intuitively meaningful way, but also shows strong capabilitiesof alleviating catastrophic forgetting in experiments. Furthermore, theproposed method outperforms all other baselines on the permuted MNIST dataset,the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continuallearning setting.\r2019-04-24\nFacilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients\nYu Chen, Tom Diethe, Neil Lawrence\nabstract\rabstract: Continual learning aims to enable machine learning models to learn a generalsolution space for past and future tasks in a sequential manner. Conventionalmodels tend to forget the knowledge of previous tasks while learning a newtask, a phenomenon known as catastrophic forgetting. When using Bayesian modelsin continual learning, knowledge from previous tasks can be retained in twoways: 1). posterior distributions over the parameters, containing the knowledgegained from inference in previous tasks, which then serve as the priors for thefollowing task; 2). coresets, containing knowledge of data distributions ofprevious tasks. Here, we show that Bayesian continual learning can befacilitated in terms of these two means through the use of natural gradientsand Stein gradients respectively.\r2019-04-15\nThree scenarios for continual learning\nGido M. van de Ven, Andreas S. Tolias\nabstract\rabstract: Standard artificial neural networks suffer from the well-known issue ofcatastrophic forgetting, making continual or lifelong learning difficult formachine learning. In recent years, numerous methods have been proposed forcontinual learning, but due to differences in evaluation protocols it isdifficult to directly compare their performance. To enable more structuredcomparisons, we describe three continual learning scenarios based on whether attest time task identity is provided and\u0026ndash;in case it is not\u0026ndash;whether it must beinferred. Any sequence of well-defined tasks can be performed according to eachscenario. Using the split and permuted MNIST task protocols, for each scenariowe carry out an extensive comparison of recently proposed continual learningmethods. We demonstrate substantial differences between the three scenarios interms of difficulty and in terms of how efficient different methods are. Inparticular, when task identity must be inferred (i.e., class incrementallearning), we find that regularization-based approaches (e.g., elastic weightconsolidation) fail and that replaying representations of previous experiencesseems required for solving this scenario.\r2019-04-05\nReducing catastrophic forgetting when evolving neural networks\nJoseph Early\nabstract\rabstract: A key stepping stone in the development of an artificial general intelligence(a machine that can perform any task), is the production of agents that canperform multiple tasks at once instead of just one. Unfortunately, canonicalmethods are very prone to catastrophic forgetting (CF) - the act of overwritingprevious knowledge about a task when learning a new task. Recent efforts havedeveloped techniques for overcoming CF in learning systems, but no attempt hasbeen made to apply these new techniques to evolutionary systems. This researchpresents a novel technique, weight protection, for reducing CF in evolutionarysystems by adapting a method from learning systems. It is used in conjunctionwith other evolutionary approaches for overcoming CF and is shown to beeffective at alleviating CF when applied to a suite of reinforcement learningtasks. It is speculated that this work could indicate the potential for a widerapplication of existing learning-based approaches to evolutionary systems andthat evolutionary techniques may be competitive with or better than learningsystems when it comes to reducing CF.\r2019-03-28\nDesigning ground states of Hopfield networks for quantum state preparation\nClemens Dlaska, Lukas M. Sieberer, Wolfgang Lechner\nabstract\rabstract: We present a protocol to store a polynomial number of arbitrary bit strings,encoded as spin configurations, in the approximately degenerate low-energymanifold of an all-to-all connected Ising spin glass. The iterative protocol isinspired by machine learning techniques utilizing $k$-local Hopfield networkstrained with $k$-local Hebbian learning and unlearning. The trained Hamiltonianis the basis of a quantum state-preparation scheme to create quantum many-bodysuperpositions with tunable squared amplitudes using resources available innear term experiments. We find that the number of configurations that can bestored in the ground states and thus turned into superposition scales with the$k$-locality of the Ising interaction.\r2019-03-02\nAttention-Based Structural-Plasticity\nSoheil Kolouri, Nicholas Ketz, Xinyun Zou, Jeffrey Krichmar, Praveen Pilly\nabstract\rabstract: Catastrophic forgetting/interference is a critical problem for lifelonglearning machines, which impedes the agents from maintaining their previouslylearned knowledge while learning new tasks. Neural networks, in particular,suffer plenty from the catastrophic forgetting phenomenon. Recently there hasbeen several efforts towards overcoming catastrophic forgetting in neuralnetworks. Here, we propose a biologically inspired method toward overcomingcatastrophic forgetting. Specifically, we define an attention-based selectiveplasticity of synapses based on the cholinergic neuromodulatory system in thebrain. We define synaptic importance parameters in addition to synaptic weightsand then use Hebbian learning in parallel with backpropagation algorithm tolearn synaptic importances in an online and seamless manner. We test ourproposed method on benchmark tasks including the Permuted MNIST and the SplitMNIST problems and show competitive performance compared to thestate-of-the-art methods.\r2019-02-23\nMemory Efficient Experience Replay for Streaming Learning\nTyler L. Hayes, Nathan D. Cahill, Christopher Kanan\nabstract\rabstract: In supervised machine learning, an agent is typically trained once and thendeployed. While this works well for static settings, robots often operate inchanging environments and must quickly learn new things from data streams. Inthis paradigm, known as streaming learning, a learner is trained online, in asingle pass, from a data stream that cannot be assumed to be independent andidentically distributed (iid). Streaming learning will cause conventional deepneural networks (DNNs) to fail for two reasons: 1) they need multiple passesthrough the entire dataset; and 2) non-iid data will cause catastrophicforgetting. An old fix to both of these issues is rehearsal. To learn a newexample, rehearsal mixes it with previous examples, and then this mixture isused to update the DNN. Full rehearsal is slow and memory intensive because itstores all previously observed examples, and its effectiveness for preventingcatastrophic forgetting has not been studied in modern DNNs. Here, we describethe ExStream algorithm for memory efficient rehearsal and compare it toalternatives. We find that full rehearsal can eliminate catastrophic forgettingin a variety of streaming learning settings, with ExStream performing wellusing far less memory and computation.\r2019-02-16\nRealizing Continual Learning through Modeling a Learning System as a Fiber Bundle\nZhenfeng Cao\nabstract\rabstract: A human brain is capable of continual learning by nature; however the currentmainstream deep neural networks suffer from a phenomenon named catastrophicforgetting (i.e., learning a new set of patterns suddenly and completely wouldresult in fully forgetting what has already been learned). In this paper wepropose a generic learning model, which regards a learning system as a fiberbundle. By comparing the learning performance of our model with conventionalones whose neural networks are multilayer perceptrons through a variety ofmachine-learning experiments, we found our proposed model not only enjoys adistinguished capability of continual learning but also bears a highinformation capacity. In addition, we found in some learning scenarios thelearning performance can be further enhanced by making the learning time-awareto mimic the episodic memory in human brain. Last but not least, we found thatthe properties of forgetting in our model correspond well to those of humanmemory. This work may shed light on how a human brain learns.\r2019-02-11\nContinual Lifelong Learning with Neural Networks: A Review\nGerman I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, Stefan Wermter\nabstract\rabstract: Humans and animals have the ability to continually acquire, fine-tune, andtransfer knowledge and skills throughout their lifespan. This ability, referredto as lifelong learning, is mediated by a rich set of neurocognitive mechanismsthat together contribute to the development and specialization of oursensorimotor skills as well as to long-term memory consolidation and retrieval.Consequently, lifelong learning capabilities are crucial for autonomous agentsinteracting in the real world and processing continuous streams of information.However, lifelong learning remains a long-standing challenge for machinelearning and neural network models since the continual acquisition ofincrementally available information from non-stationary data distributionsgenerally leads to catastrophic forgetting or interference. This limitationrepresents a major drawback for state-of-the-art deep neural network modelsthat typically learn representations from stationary batches of training data,thus without accounting for situations in which information becomesincrementally available over time. In this review, we critically summarize themain challenges linked to lifelong learning for artificial learning systems andcompare existing neural network approaches that alleviate, to differentextents, catastrophic forgetting. We discuss well-established and emergingresearch motivated by lifelong learning factors in biological systems such asstructural plasticity, memory replay, curriculum and transfer learning,intrinsic motivation, and multisensory integration.\r2018-12-28\nSupportNet: solving catastrophic forgetting in class incremental learning with support data\nYu Li, Zhongxiao Li, Lizhong Ding, Yijie Pan, Chao Huang, Yuhui Hu, Wei Chen, Xin Gao\nabstract\rabstract: A plain well-trained deep learning model often does not have the ability tolearn new knowledge without forgetting the previously learned knowledge, whichis known as catastrophic forgetting. Here we propose a novel method,SupportNet, to efficiently and effectively solve the catastrophic forgettingproblem in the class incremental learning scenario. SupportNet combines thestrength of deep learning and support vector machine (SVM), where SVM is usedto identify the support data from the old data, which are fed to the deeplearning model together with the new data for further training so that themodel can review the essential information of the old data when learning thenew information. Two powerful consolidation regularizers are applied tostabilize the learned representation and ensure the robustness of the learnedmodel. We validate our method with comprehensive experiments on various tasks,which show that SupportNet drastically outperforms the state-of-the-artincremental learning methods and even reaches similar performance as the deeplearning model trained from scratch on both old and new data. Our program isaccessible at: https://github.com/lykaust15/SupportNet\r2018-12-03\nLearning to Unlearn: Building Immunity to Dataset Bias in Medical Imaging Studies\nAhmed Ashraf, Shehroz Khan, Nikhil Bhagwat, Mallar Chakravarty, Babak Taati\nabstract\rabstract: Medical imaging machine learning algorithms are usually evaluated on a singledataset. Although training and testing are performed on different subsets ofthe dataset, models built on one study show limited capability to generalize toother studies. While database bias has been recognized as a serious problem inthe computer vision community, it has remained largely unnoticed in medicalimaging research. Transfer learning thus remains confined to the re-use offeature representations requiring re-training on the new dataset. As a result,machine learning models do not generalize even when trained on imaging datasetsthat were captured to study the same variable of interest. The ability totransfer knowledge gleaned from one study to another, without the need forre-training, if possible, would provide reassurance that the models arelearning knowledge fundamental to the problem under study instead of latchingonto the idiosyncracies of a dataset. In this paper, we situate the problem ofdataset bias in the context of medical imaging studies. We show empiricalevidence that such a problem exists in medical datasets. We then present aframework to unlearn study membership as a means to handle the problem ofdatabase bias. Our main idea is to take the data from the original featurespace to an intermediate space where the data points are indistinguishable interms of which study they come from, while maintaining the recognitioncapability with respect to the variable of interest. This will promote modelswhich learn the more general properties of the etiology under study instead ofaligning to dataset-specific peculiarities. Essentially, our proposed modellearns to unlearn the dataset bias.\r2018-11-21\nIdentifying Real Estate Opportunities using Machine Learning\nAlejandro Baldominos, Iván Blanco, Antonio José Moreno, Rubén Iturrarte, Óscar Bernárdez, Carlos Afonso\nabstract\rabstract: The real estate market is exposed to many fluctuations in prices because ofexisting correlations with many variables, some of which cannot be controlledor might even be unknown. Housing prices can increase rapidly (or in somecases, also drop very fast), yet the numerous listings available online wherehouses are sold or rented are not likely to be updated that often. In somecases, individuals interested in selling a house (or apartment) might includeit in some online listing, and forget about updating the price. In other cases,some individuals might be interested in deliberately setting a price below themarket price in order to sell the home faster, for various reasons. In thispaper, we aim at developing a machine learning application that identifiesopportunities in the real estate market in real time, i.e., houses that arelisted with a price substantially below the market price. This program can beuseful for investors interested in the housing market. We have focused in a usecase considering real estate assets located in the Salamanca district in Madrid(Spain) and listed in the most relevant Spanish online site for home sales andrentals. The application is formally implemented as a regression problem thattries to estimate the market price of a house given features retrieved frompublic online listings. For building this application, we have performed afeature engineering stage in order to discover relevant features that allowsfor attaining a high predictive performance. Several machine learningalgorithms have been tested, including regression trees, k-nearest neighbors,support vector machines and neural networks, identifying advantages andhandicaps of each of them.\r2018-11-06\nKalman Filter Modifier for Neural Networks in Non-stationary Environments\nHonglin Li, Frieder Ganz, Shirin Enshaeifar, Payam Barnaghi\nabstract\rabstract: Learning in a non-stationary environment is an inevitable problem whenapplying machine learning algorithm to real world environment. Learning newtasks without forgetting the previous knowledge is a challenge issue in machinelearning. We propose a Kalman Filter based modifier to maintain the performanceof Neural Network models under non-stationary environments. The result showsthat our proposed model can preserve the key information and adapts better tothe changes. The accuracy of proposed model decreases by 0.4% in ourexperiments, while the accuracy of conventional model decreases by 90% in thedrifts environment.\r2018-10-30\nSimplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks\nBiao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, Huiji Zhang\nabstract\rabstract: In this paper, we propose an additionsubtraction twin-gated recurrent network(ATR) to simplify neural machine translation. The recurrent units of ATR areheavily simplified to have the smallest number of weight matrices among unitsof all existing gated RNNs. With the simple addition and subtraction operation,we introduce a twin-gated mechanism to build input and forget gates which arehighly correlated. Despite this simplification, the essential non-linearitiesand capability of modeling long-distance dependencies are preserved.Additionally, the proposed ATR is more transparent than LSTM/GRU due to thesimplification. Forward self-attention can be easily established in ATR, whichmakes the proposed network interpretable. Experiments on WMT14 translationtasks demonstrate that ATR-based neural machine translation can yieldcompetitive performance on English- German and English-French language pairs interms of both translation quality and speed. Further experiments on NISTChinese-English translation, natural language inference and Chinese wordsegmentation verify the generality and applicability of ATR on differentnatural language processing tasks.\r2018-09-09\nRefining Source Representations with Relation Networks for Neural Machine Translation\nWen Zhang, Jiawei Hu, Yang Feng, Qun Liu\nabstract\rabstract: Although neural machine translation with the encoder-decoder framework hasachieved great success recently, it still suffers drawbacks of forgettingdistant information, which is an inherent disadvantage of recurrent neuralnetwork structure, and disregarding relationship between source words duringencoding step. Whereas in practice, the former information and relationship areoften useful in current step. We target on solving these problems and thusintroduce relation networks to learn better representations of the source. Therelation networks are able to facilitate memorization capability of recurrentneural network via associating source words with each other, this would alsohelp retain their relationships. Then the source representations and all therelations are fed into the attention component together while decoding, withthe main encoder-decoder framework unchanged. Experiments on several datasetsshow that our method can improve the translation performance significantly overthe conventional encoder-decoder model and even outperform the approachinvolving supervised syntactic knowledge.\r2018-07-26\nRobust Tracking via Weighted Online Extreme Learning Machine\nJing Zhang, Huibing Wang, Yonggong Ren\nabstract\rabstract: The tracking method based on the extreme learning machine (ELM) is efficientand effective. ELM randomly generates input weights and biases in the hiddenlayer, and then calculates and computes the output weights by reducing theiterative solution to the problem of linear equations. Therefore, ELM offersthe satisfying classification performance and fast training time than otherdiscriminative models in tracking. However, the original ELM method oftensuffers from the problem of the imbalanced classification distribution, whichis caused by few target objects, leading to under-fitting and more backgroundsamples leading to over-fitting. Worse still, it reduces the robustness oftracking under special conditions including occlusion, illumination, etc. Toaddress above problems, in this paper, we present a robust tracking algorithm.First, we introduce the local weight matrix that is the dynamic creation fromthe data distribution at the current frame in the original ELM so as to balancebetween the empirical and structure risk, and fully learn the target object toenhance the classification performance. Second, we improve it to theincremental learning method ensuring tracking real-time and efficient. Finally,the forgetting factor is used to strengthen the robustness for changing of theclassification distribution with time. Meanwhile, we propose a novel optimizedmethod to obtain the optimal sample as the target object, which avoids trackingdrift resulting from noisy samples. Therefore, our tracking method can fullylearn both of the target object and background information to enhance thetracking performance, and it is evaluated in 20 challenge image sequences withdifferent attributes including illumination, occlusion, deformation, etc.,which achieves better performance than several state-of-the-art methods interms of effectiveness and robustness.\r2018-07-24\nExample Mining for Incremental Learning in Medical Imaging\nPratyush Kumar, Muktabh Mayank Srivastava\nabstract\rabstract: Incremental Learning is well known machine learning approach wherein theweights of the learned model are dynamically and gradually updated togeneralize on new unseen data without forgetting the existing knowledge.Incremental learning proves to be time as well as resource-efficient solutionfor deployment of deep learning algorithms in real world as the model canautomatically and dynamically adapt to new data as and when annotated databecomes available. The development and deployment of Computer Aided Diagnosis(CAD) tools in medical domain is another scenario, where incremental learningbecomes very crucial as collection and annotation of a comprehensive datasetspanning over multiple pathologies and imaging machines might take years.However, not much has so far been explored in this direction. In the currentwork, we propose a robust and efficient method for incremental learning inmedical imaging domain. Our approach makes use of Hard Example Mining technique(which is commonly used as a solution to heavy class imbalance) toautomatically select a subset of dataset to fine-tune the existing networkweights such that it adapts to new data while retaining existing knowledge. Wedevelop our approach for incremental learning of our already under test modelfor detecting dental caries. Further, we apply our approach to one publiclyavailable dataset and demonstrate that our approach reaches the accuracy oftraining on entire dataset at once, while availing the benefits of incrementallearning scenario.\r2018-07-11\nAn Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm\nShin Kamada, Takumi Ichimura\nabstract\rabstract: Deep Belief Network (DBN) has a deep architecture that represents multiplefeatures of input patterns hierarchically with the pre-trained RestrictedBoltzmann Machines (RBM). A traditional RBM or DBN model cannot change itsnetwork structure during the learning phase. Our proposed adaptive learningmethod can discover the optimal number of hidden neurons and weights and/orlayers according to the input space. The model is an important method to takeaccount of the computational cost and the model stability. The regularities tohold the sparse structure of network is considerable problem, since theextraction of explicit knowledge from the trained network should be required.In our previous research, we have developed the hybrid method of adaptivestructural learning method of RBM and Learning Forgetting method to the trainedRBM. In this paper, we propose the adaptive learning method of DBN that candetermine the optimal number of layers during the learning. We evaluated ourproposed model on some benchmark data sets.\r2018-05-25\nRefining Source Representations with Relation Networks for Neural Machine Translation\nWen Zhang, Jiawei Hu, Yang Feng, Qun Liu\nabstract\rabstract: Although neural machine translation (NMT) with the encoder-decoder frameworkhas achieved great success in recent times, it still suffers from somedrawbacks: RNNs tend to forget old information which is often useful and theencoder only operates through words without considering word relationship. Tosolve these problems, we introduce a relation networks (RN) into NMT to refinethe encoding representations of the source. In our method, the RN firstaugments the representation of each source word with its neighbors and reasonsall the possible pairwise relations between them. Then the sourcerepresentations and all the relations are fed to the attention module and thedecoder together, keeping the main encoder-decoder architecture unchanged.Experiments on two Chinese-to-English data sets in different scales both showthat our method can outperform the competitive baselines significantly.\r2018-05-16\nSAFE: Spectral Evolution Analysis Feature Extraction for Non-Stationary Time Series Prediction\nArief Koesdwiady, Fakhri Karray\nabstract\rabstract: This paper presents a practical approach for detecting non-stationarity intime series prediction. This method is called SAFE and works by monitoring theevolution of the spectral contents of time series through a distance function.This method is designed to work in combination with state-of-the-art machinelearning methods in real time by informing the online predictors to performnecessary adaptation when a non-stationarity presents. We also propose analgorithm to proportionally include some past data in the adaption process toovercome the Catastrophic Forgetting problem. To validate our hypothesis andtest the effectiveness of our approach, we present comprehensive experiments indifferent elements of the approach involving artificial and real-worlddatasets. The experiments show that the proposed method is able tosignificantly save computational resources in term of processor or GPU cycleswhile maintaining high prediction performances.\r2018-05-09\nSeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural Networks by Selective Network Augmentation\nAbel S. Zacarias, Luís A. Alexandre\nabstract\rabstract: Lifelong learning aims to develop machine learning systems that can learn newtasks while preserving the performance on previous learned tasks. In this paperwe present a method to overcome catastrophic forgetting on convolutional neuralnetworks, that learns new tasks and preserves the performance on old taskswithout accessing the data of the original model, by selective networkaugmentation. The experiment results showed that SeNA-CNN, in some scenarios,outperforms the state-of-art Learning without Forgetting algorithm. Resultsalso showed that in some situations it is better to use SeNA-CNN instead oftraining a neural network using isolated learning.\r2018-04-25\nDynamic Few-Shot Visual Learning without Forgetting\nSpyros Gidaris, Nikos Komodakis\nabstract\rabstract: The human visual system has the remarkably ability to be able to effortlesslylearn novel concepts from only a few examples. Mimicking the same behavior onmachine learning vision systems is an interesting and very challenging researchproblem with many practical advantages on real world vision applications. Inthis context, the goal of our work is to devise a few-shot visual learningsystem that during test time it will be able to efficiently learn novelcategories from only a few training data while at the same time it will notforget the initial categories on which it was trained (here called basecategories). To achieve that goal we propose (a) to extend an objectrecognition system with an attention based few-shot classification weightgenerator, and (b) to redesign the classifier of a ConvNet model as the cosinesimilarity function between feature representations and classification weightvectors. The latter, apart from unifying the recognition of both novel and basecategories, it also leads to feature representations that generalize better on\u0026quot;unseen\u0026quot; categories. We extensively evaluate our approach on Mini-ImageNetwhere we manage to improve the prior state-of-the-art on few-shot recognition(i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settingsrespectively) while at the same time we do not sacrifice any accuracy on thebase categories, which is a characteristic that most prior approaches lack.Finally, we apply our approach on the recently introduced few-shot benchmark ofBharath and Girshick [4] where we also achieve state-of-the-art results. Thecode and models of our paper will be published on:https://github.com/gidariss/FewShotWithoutForgetting\r2018-04-12\nCombating catastrophic forgetting with developmental compression\nShawn L. E. Beaulieu, Sam Kriegman, Josh C. Bongard\nabstract\rabstract: Generally intelligent agents exhibit successful behavior across problems inseveral settings. Endemic in approaches to realize such intelligence inmachines is catastrophic forgetting: sequential learning corrupts knowledgeobtained earlier in the sequence, or tasks antagonistically compete for systemresources. Methods for obviating catastrophic forgetting have sought toidentify and preserve features of the system necessary to solve one problemwhen learning to solve another, or to enforce modularity such that minimallyoverlapping sub-functions contain task specific knowledge. While successful,both approaches scale poorly because they require larger architectures as thenumber of training instances grows, causing different parts of the system tospecialize for separate subsets of the data. Here we present a method foraddressing catastrophic forgetting called developmental compression. Itexploits the mild impacts of developmental mutations to lessen adverse changesto previously-evolved capabilities and `compresses\u0026rsquo; specialized neural networksinto a generalized one. In the absence of domain knowledge, developmentalcompression produces systems that avoid overt specialization, alleviating theneed to engineer a bespoke system for every task permutation and suggestingbetter scalability than existing approaches. We validate this method on a robotcontrol problem and hope to extend this approach to other machine learningdomains in the future.\r2018-02-28\nPredicting Recall Probability to Adaptively Prioritize Study\nShane Mooney, Karen Sun, Eric Bomgardner\nabstract\rabstract: Students have a limited time to study and are typically ineffective atallocating study time. Machine-directed study strategies that identify whichitems need reinforcement and dictate the spacing of repetition have been shownto help students optimize mastery (Mozer \u0026amp; Lindsey 2017). The large volume ofresearch on this matter is typically conducted in constructed experimentalsettings with fixed instruction, content, and scheduling; in contrast, we aimto develop methods that can address any demographic, subject matter, or studyschedule. We show two methods that model item-specific recall probability foruse in a discrepancy-reduction instruction strategy. The first model predictsitem recall probability using a multiple logistic regression (MLR) model basedon previous answer correctness and temporal spacing of study. Prompted byliterature suggesting that forgetting is better modeled by the power law thanan exponential decay (Wickelgren 1974), we compare the MLR approach with aRecurrent Power Law (RPL) model which adaptively fits a forgetting curve. Wethen discuss the performance of these models against study datasets comprisedof millions of answers and show that the RPL approach is more accurate andflexible than the MLR model. Finally, we give an overview of promising futureapproaches to knowledge modeling.\r2017-11-09\nMeasuring Catastrophic Forgetting in Neural Networks\nRonald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, Christopher Kanan\nabstract\rabstract: Deep neural networks are used in many state-of-the-art systems for machineperception. Once a network is trained to do a specific task, e.g., birdclassification, it cannot easily be trained to do new tasks, e.g.,incrementally learning to recognize additional bird species or learning anentirely different task such as flower recognition. When new tasks are added,typical deep neural networks are prone to catastrophically forgetting previoustasks. Networks that are capable of assimilating new information incrementally,much like how humans form new memories over time, will be more efficient thanre-training the model from scratch each time a new task needs to be learned.There have been multiple attempts to develop schemes that mitigate catastrophicforgetting, but these methods have not been directly compared, the tests usedto evaluate them vary considerably, and these methods have only been evaluatedon small-scale problems (e.g., MNIST). In this paper, we introduce new metricsand benchmarks for directly comparing five different mechanisms designed tomitigate catastrophic forgetting in neural networks: regularization,ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments onreal-world images and sounds show that the mechanism(s) that are critical foroptimal performance vary based on the incremental training paradigm and type ofdata being used, but they all demonstrate that the catastrophic forgettingproblem has yet to be solved.\r2017-10-04\nLSTM: A Search Space Odyssey\nKlaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber\nabstract\rabstract: Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random search,and their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs ($\\approx 15$ yearsof CPU time), which makes our study the largest of its kind on LSTM networks.Our results show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.\r2017-09-07\nEmptiness Problems for Distributed Automata\nAntti Kuusisto, Fabian Reiter\nabstract\rabstract: We investigate the decidability of the emptiness problem for three classes ofdistributed automata. These devices operate on finite directed graphs, actingas networks of identical finite-state machines that communicate in an infinitesequence of synchronous rounds. The problem is shown to be decidable inLogSpace for a class of forgetful automata, where the nodes see the messagesreceived from their neighbors but cannot remember their own state. Whenrestricted to the appropriate families of graphs, these forgetful automata areequivalent to classical finite word automata, but strictly more expressive thanfinite tree automata. On the other hand, we also show that the emptinessproblem is undecidable in general. This already holds for two heavilyrestricted classes of distributed automata: those that reject immediately ifthey receive more than one message per round, and those whose state diagrammust be acyclic except for self-loops.\r2017-07-17\nA Strategy for an Uncompromising Incremental Learner\nRagav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan, Baoxin Li\nabstract\rabstract: Multi-class supervised learning systems require the knowledge of the entirerange of labels they predict. Often when learnt incrementally, they suffer fromcatastrophic forgetting. To avoid this, generous leeways have to be made to thephilosophy of incremental learning that either forces a part of the machine tonot learn, or to retrain the machine again with a selection of the historicdata. While these hacks work to various degrees, they do not adhere to thespirit of incremental learning. In this article, we redefine incrementallearning with stringent conditions that do not allow for any undesirablerelaxations and assumptions. We design a strategy involving generative modelsand the distillation of dark knowledge as a means of hallucinating data alongwith appropriate targets from past distributions. We call this technique,phantom sampling.We show that phantom sampling helps avoid catastrophicforgetting during incremental learning. Using an implementation based on deepneural networks, we demonstrate that phantom sampling dramatically avoidscatastrophic forgetting. We apply these strategies to competitive multi-classincremental learning of deep neural networks. Using various benchmark datasetsand through our strategy, we demonstrate that strict incremental learning couldbe achieved. We further put our strategy to test on challenging cases,including cross-domain increments and incrementing on a novel label space. Wealso propose a trivial extension to unbounded-continual learning and identifypotential for future development.\r2015-09-10\nUse it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine\nAndrew J. R. Simpson\nabstract\rabstract: In a recent article we described a new type of deep neural network - aPerpetual Learning Machine (PLM) - which is capable of learning \u0026lsquo;on the fly\u0026rsquo;like a brain by existing in a state of Perpetual Stochastic Gradient Descent(PSGD). Here, by simulating the process of practice, we demonstrate bothselective memory and selective forgetting when we introduce statistical recallbiases during PSGD. Frequently recalled memories are remembered, whilstmemories recalled rarely are forgotten. This results in a \u0026lsquo;use it or lose it\u0026rsquo;stimulus driven memory process that is similar to human memory.\r2015-09-08\nDynamic Structure Embedded Online Multiple-Output Regression for Stream Data\nChangsheng Li, Fan Wei, Weishan Dong, Qingshan Liu, Xiangfeng Wang, Xin Zhang\nabstract\rabstract: Online multiple-output regression is an important machine learning techniquefor modeling, predicting, and compressing multi-dimensional correlated datastreams. In this paper, we propose a novel online multiple-output regressionmethod, called MORES, for stream data. MORES can \\emph{dynamically} learn thestructure of the coefficients change in each update step to facilitate themodel\u0026rsquo;s continuous refinement. We observe that limited expressive ability ofthe regression model, especially in the preliminary stage of online update,often leads to the variables in the residual errors being dependent. In lightof this point, MORES intends to \\emph{dynamically} learn and leverage thestructure of the residual errors to improve the prediction accuracy. Moreover,we define three statistical variables to \\emph{exactly} represent all the seensamples for \\emph{incrementally} calculating prediction loss in each onlineupdate round, which can avoid loading all the training data into memory forupdating model, and also effectively prevent drastic fluctuation of the modelin the presence of noise. Furthermore, we introduce a forgetting factor to setdifferent weights on samples so as to track the data streams\u0026rsquo; evolvingcharacteristics quickly from the latest samples. Experiments on one syntheticdataset and three real-world datasets validate the effectiveness of theproposed method. In addition, the update speed of MORES is at least 2000samples processed per second on the three real-world datasets, more than 15times faster than the state-of-the-art online learning algorithm.\r2015-03-04\nAn Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks\nIan J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio\nabstract\rabstract: Catastrophic forgetting is a problem faced by many machine learning modelsand algorithms. When trained on one task, then trained on a second task, manymachine learning models \u0026ldquo;forget\u0026rdquo; how to perform the first task. This is widelybelieved to be a serious problem for neural networks. Here, we investigate theextent to which the catastrophic forgetting problem occurs for modern neuralnetworks, comparing both established and recent gradient-based trainingalgorithms and activation functions. We also examine the effect of therelationship between the first task and the second task on catastrophicforgetting. We find that it is always best to train using the dropoutalgorithm\u0026ndash;the dropout algorithm is consistently best at adapting to the newtask, remembering the old task, and has the best tradeoff curve between thesetwo extremes. We find that different tasks and relationships between tasksresult in very different rankings of activation function performance. Thissuggests the choice of activation function should always be cross-validated.\r2014-11-22\nIncremental Learning of Event Definitions with Inductive Logic Programming\nNikos Katzouris, Alexander Artikis, George Paliouras\nabstract\rabstract: Event recognition systems rely on properly engineered knowledge bases ofevent definitions to infer occurrences of events in time. The manualdevelopment of such knowledge is a tedious and error-prone task, thusevent-based applications may benefit from automated knowledge constructiontechniques, such as Inductive Logic Programming (ILP), which combines machinelearning with the declarative and formal semantics of First-Order Logic.However, learning temporal logical formalisms, which are typically utilized bylogic-based Event Recognition systems is a challenging task, which most ILPsystems cannot fully undertake. In addition, event-based data is usuallymassive and collected at different times and under various circumstances.Ideally, systems that learn from temporal data should be able to operate in anincremental mode, that is, revise prior constructed knowledge in the face ofnew evidence. Most ILP systems are batch learners, in the sense that in orderto account for new evidence they have no alternative but to forget pastknowledge and learn from scratch. Given the increased inherent complexity ofILP and the volumes of real-life temporal data, this results to algorithms thatscale poorly. In this work we present an incremental method for learning andrevising event-based knowledge, in the form of Event Calculus programs. Theproposed algorithm relies on abductive-inductive learning and comprises ascalable clause refinement methodology, based on a compressive summarization ofclause coverage in a stream of examples. We present an empirical evaluation ofour approach on real and synthetic data from activity recognition and citytransport applications.\r2013-12-19\nTime-varying Learning and Content Analytics via Sparse Factor Analysis\nAndrew S. Lan, Christoph Studer, Richard G. Baraniuk\nabstract\rabstract: We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner\u0026rsquo;s concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.\r2012-06-23\nSupervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns\nTsvi Achler\nabstract\rabstract: Matching animal-like flexibility in recognition and the ability to quicklyincorporate new information remains difficult. Limits are yet to be adequatelyaddressed in neural models and recognition algorithms. This work proposes aconfiguration for recognition that maintains the same function of conventionalalgorithms but avoids combinatorial problems. Feedforward recognitionalgorithms such as classical artificial neural networks and machine learningalgorithms are known to be subject to catastrophic interference and forgetting.Modifying or learning new information (associations between patterns andlabels) causes loss of previously learned information. I demonstrate usingmathematical analysis how supervised generative models, with feedforward andfeedback connections, can emulate feedforward algorithms yet avoid catastrophicinterference and forgetting. Learned information in generative models is storedin a more intuitive form that represents the fixed points or solutions of thenetwork and moreover displays similar difficulties as cognitive phenomena.Brain-like capabilities and limits associated with generative models suggestthe brain may perform recognition and store information using a similarapproach. Because of the central role of recognition, progress understandingthe underlying principles may reveal significant insight on how to better studyand integrate with the brain.\r2005-10-28\nMonster Redshift Surveys through Dispersive Slitless Imaging: The Baryon Oscillation Probe\nKarl Glazebrook, Ivan Baldry, Warren Moos, Jeff Kruk, Stephan McCandliss\nabstract\rabstract: Wide-field imaging from space should not forget the dispersive dimension. Weconsider the capability of space-based imaging with a slitless grism: becauseof the low near-infrared background in space and the high sky-density of highredshift emission line galaxies this makes for a very powerful redshift machinewith no moving parts. A small 1m space telescope with a 0.5 degree field ofview could measure redshifts for 10^7 galaxies at 0.5\u0026lt;z\u0026lt;2 per year, this is aMIDEX class concept which we have dubbed `The Baryon Oscillation Probe\u0026rsquo; as theprimary science case would be constraining dark energy evolution viameasurement of the baryonic oscillations in the galaxy power spectrum. Theseideas are generalizable to other missions such as SNAP and DESTINY.\r"},{"id":4,"href":"/docs/arxiv_papers/vulnerable_llm/","title":"Vulnerable LLM","section":"Arxiv Papers","content":"\rArxiv Papers: Vulnerable LLM\r#\r2024-03-20\nMapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal\nRahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel\nabstract\rabstract: The rapid integration of Large Language Models (LLMs) across diverse sectorshas marked a transformative era, showcasing remarkable capabilities in textgeneration and problem-solving tasks. However, this technological advancementis accompanied by significant risks and vulnerabilities. Despite ongoingsecurity enhancements, attackers persistently exploit these weaknesses, castingdoubts on the overall trustworthiness of LLMs. Compounding the issue,organisations are deploying LLM-integrated systems without understanding theseverity of potential consequences. Existing studies by OWASP and MITRE offer ageneral overview of threats and vulnerabilities but lack a method for directlyand succinctly analysing the risks for security practitioners, developers, andkey decision-makers who are working with this novel technology. To address thisgap, we propose a risk assessment process using tools like the OWASP riskrating methodology which is used for traditional systems. We conduct scenarioanalysis to identify potential threat agents and map the dependent systemcomponents against vulnerability factors. Through this analysis, we assess thelikelihood of a cyberattack. Subsequently, we conduct a thorough impactanalysis to derive a comprehensive threat matrix. We also map threats againstthree key stakeholder groups: developers engaged in model fine-tuning,application developers utilizing third-party APIs, and end users. The proposedthreat matrix provides a holistic evaluation of LLM-related risks, enablingstakeholders to make informed decisions for effective mitigation strategies.Our outlined process serves as an actionable and comprehensive tool forsecurity practitioners, offering insights for resource management and enhancingthe overall system security.\rLogPrécis: Unleashing Language Models for Automated Shell Log Analysis\nMatteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi\nabstract\rabstract: The collection of security-related logs holds the key to understanding attackbehaviors and diagnosing vulnerabilities. Still, their analysis remains adaunting challenge. Recently, Language Models (LMs) have demonstrated unmatchedpotential in understanding natural and programming languages. The questionarises whether and how LMs could be also useful for security experts sincetheir logs contain intrinsically confused and obfuscated information. In thispaper, we systematically study how to benefit from the state-of-the-art in LMto automatically analyze text-like Unix shell attack logs. We present athorough design methodology that leads to LogPr'ecis. It receives as input rawshell sessions and automatically identifies and assigns the attacker tactic toeach portion of the session, i.e., unveiling the sequence of the attacker\u0026rsquo;sgoals. We demonstrate LogPr'ecis capability to support the analysis of twolarge datasets containing about 400,000 unique Unix shell attacks. LogPr'ecisreduces them into about 3,000 fingerprints, each grouping sessions with thesame sequence of tactics. The abstraction it provides lets the analyst betterunderstand attacks, identify fingerprints, detect novelty, link similarattacks, and track families and mutations. Overall, LogPr'ecis, released asopen source, paves the way for better and more responsive defense againstcyberattacks.\r2024-03-19\nA Study of Vulnerability Repair in JavaScript Programs with Large Language Models\nTan Khang Le, Saba Alimadadi, Steven Y. Ko\nabstract\rabstract: In recent years, JavaScript has become the most widely used programminglanguage, especially in web development. However, writing secure JavaScriptcode is not trivial, and programmers often make mistakes that lead to securityvulnerabilities in web applications. Large Language Models (LLMs) havedemonstrated substantial advancements across multiple domains, and theirevolving capabilities indicate their potential for automatic code generationbased on a required specification, including automatic bug fixing. In thisstudy, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding andfixing security vulnerabilities in JavaScript programs. We also investigate theimpact of context in a prompt on directing LLMs to produce a correct patch ofvulnerable JavaScript code. Our experiments on real-world softwarevulnerabilities show that while LLMs are promising in automatic program repairof JavaScript code, achieving a correct bug fix often requires an appropriateamount of context in the prompt.\rSecuring Large Language Models: Threats, Vulnerabilities and Responsible Practices\nSara Abdali, Richard Anarfi, CJ Barberan, Jia He\nabstract\rabstract: Large language models (LLMs) have significantly transformed the landscape ofNatural Language Processing (NLP). Their impact extends across a diversespectrum of tasks, revolutionizing how we approach language understanding andgenerations. Nevertheless, alongside their remarkable utility, LLMs introducecritical security and risk considerations. These challenges warrant carefulexamination to ensure responsible deployment and safeguard against potentialvulnerabilities. This research paper thoroughly investigates security andprivacy concerns related to LLMs from five thematic perspectives: security andprivacy concerns, vulnerabilities against adversarial attacks, potential harmscaused by misuses of LLMs, mitigation strategies to address these challengeswhile identifying limitations of current strategies. Lastly, the paperrecommends promising avenues for future research to enhance the security andrisk management of LLMs.\rOn the effectiveness of Large Language Models for GitHub Workflows\nXinyu Zhang, Siddharth Muralee, Sourag Cherupattamoolayil, Aravind Machiry\nabstract\rabstract: GitHub workflows or GitHub CI is a popular continuous integration platformthat enables developers to automate various software engineering tasks byspecifying them as workflows, i.e., YAML files with a list of jobs. However,engineering valid workflows is tedious. They are also prone to severe securityissues, which can result in supply chain vulnerabilities. Recent advancementsin Large Language Models (LLMs) have demonstrated their effectiveness invarious software development tasks. However, GitHub workflows differ fromregular programs in both structure and semantics. We perform the firstcomprehensive study to understand the effectiveness of LLMs on fiveworkflow-related tasks with different levels of prompts. We curated a set of$\\sim$400K workflows and generated prompts with varying detail. We alsofine-tuned LLMs on GitHub workflow tasks. Our evaluation of threestate-of-the-art LLMs and their fine-tuned variants revealed variousinteresting findings on the current effectiveness and drawbacks of LLMs.\r2024-03-18\nACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts\nLyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu\nabstract\rabstract: Smart contracts are susceptible to various security issues, among whichaccess control (AC) vulnerabilities are particularly critical. While existingresearch has proposed multiple detection tools, the automatic and appropriaterepair of AC vulnerabilities in smart contracts remains a challenge. Unlikecommonly supported vulnerability types by existing repair tools, such asreentrancy, which are usually fixed by template-based approaches, the mainobstacle of AC lies in identifying the appropriate roles or permissions amid along list of non-AC-related source code to generate proper patch code, a taskthat demands human-level intelligence. Leveraging recent advancements in large language models (LLMs), we employ thestate-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX.The key insight is that we can mine common AC practices for major categories ofcode functionality and use them to guide LLMs in fixing code with similarfunctionality. To this end, ACFIX involves both offline and online phases.First, during the offline phase, ACFIX mines a taxonomy of common Role-basedAccess Control (RBAC) practices from 344,251 on-chain contracts, categorizing49 role-permission pairs from the top 1,000 pairs mined. Second, during theonline phase, ACFIX tracks AC-related elements across the contract and usesthis context information along with a Chain-of-Thought pipeline to guide LLMsin identifying the most appropriate role-permission pair for the subjectcontract and subsequently generating a suitable patch. This patch will thenundergo a validity and effectiveness check. To evaluate ACFIX, we built thefirst benchmark dataset of 118 real-world AC vulnerabilities, and ourevaluation revealed that ACFIX successfully repaired 94.92% of them. Thisrepresents a significant improvement compared to the baseline GPT-4, whichachieved only 52.54%.\rEasyJailbreak: A Unified Framework for Jailbreaking Large Language Models\nWeikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang\nabstract\rabstract: Jailbreak attacks are crucial for identifying and mitigating the securityvulnerabilities of Large Language Models (LLMs). They are designed to bypasssafeguards and elicit prohibited outputs. However, due to significantdifferences among various jailbreak methods, there is no standardimplementation framework available for the community, which limitscomprehensive security evaluations. This paper introduces EasyJailbreak, aunified framework simplifying the construction and evaluation of jailbreakattacks against LLMs. It builds jailbreak attacks using four components:Selector, Mutator, Constraint, and Evaluator. This modular framework enablesresearchers to easily construct attacks from combinations of novel and existingcomponents. So far, EasyJailbreak supports 11 distinct jailbreak methods andfacilitates the security validation of a broad spectrum of LLMs. Our validationacross 10 distinct LLMs reveals a significant vulnerability, with an averagebreach probability of 60% under various jailbreaking attacks. Notably, evenadvanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack SuccessRates (ASR) of 57% and 33%, respectively. We have released a wealth ofresources for researchers, including a web platform, PyPI published package,screencast video, and experimental outputs.\rLarge language models in 6G security: challenges and opportunities\nTri Nguyen, Huong Nguyen, Ahmad Ijaz, Saeid Sheikhi, Athanasios V. Vasilakos, Panos Kostakos\nabstract\rabstract: The rapid integration of Generative AI (GenAI) and Large Language Models(LLMs) in sectors such as education and healthcare have marked a significantadvancement in technology. However, this growth has also led to a largelyunexplored aspect: their security vulnerabilities. As the ecosystem thatincludes both offline and online models, various tools, browser plugins, andthird-party applications continues to expand, it significantly widens theattack surface, thereby escalating the potential for security breaches. Theseexpansions in the 6G and beyond landscape provide new avenues for adversariesto manipulate LLMs for malicious purposes. We focus on the security aspects ofLLMs from the viewpoint of potential adversaries. We aim to dissect theirobjectives and methodologies, providing an in-depth analysis of known securityweaknesses. This will include the development of a comprehensive threattaxonomy, categorizing various adversary behaviors. Also, our research willconcentrate on how LLMs can be integrated into cybersecurity efforts by defenseteams, also known as blue teams. We will explore the potential synergy betweenLLMs and blockchain technology, and how this combination could lead to thedevelopment of next-generation, fully autonomous security solutions. Thisapproach aims to establish a unified cybersecurity strategy across the entirecomputing continuum, enhancing overall digital security infrastructure.\r2024-03-15\nBeyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning\nJianwei Li, Sheng Liu, Qi Lei\nabstract\rabstract: Language models trained via federated learning (FL) demonstrate impressivecapabilities in handling complex tasks while protecting user privacy. Recentstudies indicate that leveraging gradient information and prior knowledge canpotentially reveal training samples within FL setting. However, theseinvestigations have overlooked the potential privacy risks tied to theintrinsic architecture of the models. This paper presents a two-stage privacyattack strategy that targets the vulnerabilities in the architecture ofcontemporary language models, significantly enhancing attack performance byinitially recovering certain feature directions as additional supervisorysignals. Our comparative experiments demonstrate superior attack performanceacross various datasets and scenarios, highlighting the privacy leakage riskassociated with the increasingly complex architectures of language models. Wecall for the community to recognize and address these potential privacy risksin designing large language models.\r2024-03-14\nExploring Safety Generalization Challenges of Large Language Models via Code\nQibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma\nabstract\rabstract: The rapid advancement of Large Language Models (LLMs) has brought aboutremarkable capabilities in natural language processing but also raised concernsabout their potential misuse. While strategies like supervised fine-tuning andreinforcement learning from human feedback have enhanced their safety, thesemethods primarily focus on natural languages, which may not generalize to otherdomains. This paper introduces CodeAttack, a framework that transforms naturallanguage inputs into code inputs, presenting a novel environment for testingthe safety generalization of LLMs. Our comprehensive studies onstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal acommon safety vulnerability of these models against code input: CodeAttackconsistently bypasses the safety guardrails of all models more than 80% of thetime. Furthermore, we find that a larger distribution gap between CodeAttackand natural language leads to weaker safety generalization, such as encodingnatural language input with data structures or using less popular programminglanguages. These findings highlight new safety risks in the code domain and theneed for more robust safety alignment algorithms to match the code capabilitiesof LLMs.\rImages are Achilles\u0026rsquo; Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models\nYifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen\nabstract\rabstract: In this paper, we study the harmlessness alignment problem of multimodallarge language models~(MLLMs). We conduct a systematic empirical analysis ofthe harmlessness performance of representative MLLMs and reveal that the imageinput poses the alignment vulnerability of MLLMs. Inspired by this, we proposea novel jailbreak method named HADES, which hides and amplifies the harmfulnessof the malicious intent within the text input, using meticulously craftedimages. Experimental results show that HADES can effectively jailbreak existingMLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% forLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publiclyreleased.\rImproving Reinforcement Learning from Human Feedback Using Contrastive Rewards\nWei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu\nabstract\rabstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigmused to align large language models (LLMs) with human preferences. Yet existingRLHF heavily relies on accurate and informative reward models, which arevulnerable and sensitive to noise from various sources, e.g. human labelingerrors, making the pipeline fragile. In this work, we improve the effectivenessof the reward model by introducing a penalty term on the reward, named as\\textit{contrastive rewards}. %Contrastive rewards Our approach involves twosteps: (1) an offline sampling step to obtain responses to prompts that serveas baseline calculation and (2) a contrastive reward calculated using thebaseline responses and used in the Proximal Policy Optimization (PPO) step. Weshow that contrastive rewards enable the LLM to penalize reward uncertainty,improve robustness, encourage improvement over baselines, calibrate accordingto task difficulty, and reduce variance in PPO. We show empirically contrastiverewards can improve RLHF substantially, evaluated by both GPTs and humans, andour method consistently outperforms strong baselines.\rHelpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention\nEllie Prosser, Matthew Edwards\nabstract\rabstract: Powerful generative Large Language Models (LLMs) are becoming popular toolsamongst the general public as question-answering systems, and are beingutilised by vulnerable groups such as children. With children increasinglyinteracting with these tools, it is imperative for researchers to scrutinisethe safety of LLMs, especially for applications that could lead to seriousoutcomes, such as online child safety queries. In this paper, the efficacy ofLLMs for online grooming prevention is explored both for identifying andavoiding grooming through advice generation, and the impact of prompt design onmodel performance is investigated by varying the provided context and promptspecificity. In results reflecting over 6,000 LLM interactions, we find that nomodels were clearly appropriate for online grooming prevention, with anobserved lack of consistency in behaviours, and potential for harmful answergeneration, especially from open-source models. We outline where and how modelsfall short, providing suggestions for improvement, and identify prompt designsthat heavily altered model performance in troubling ways, with findings thatcan be used to inform best practice usage guides.\rAdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting\nYu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao\nabstract\rabstract: With the advent and widespread deployment of Multimodal Large Language Models(MLLMs), the imperative to ensure their safety has become increasinglypronounced. However, with the integration of additional modalities, MLLMs areexposed to new vulnerabilities, rendering them prone to structured-basedjailbreak attacks, where semantic content (e.g., \u0026ldquo;harmful text\u0026rdquo;) has beeninjected into the images to mislead MLLMs. In this work, we aim to defendagainst such threats. Specifically, we propose \\textbf{Ada}ptive\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs withdefense prompts to defend MLLMs against structure-based jailbreak attackswithout fine-tuning MLLMs or training additional modules (e.g., post-stagecontent detector). Initially, we present a manually designed static defenseprompt, which thoroughly examines the image and instruction content step bystep and specifies response methods to malicious queries. Furthermore, weintroduce an adaptive auto-refinement framework, consisting of a target MLLMand a LLM-based defense prompt generator (Defender). These componentscollaboratively and iteratively communicate to generate a defense prompt.Extensive experiments on the popular structure-based jailbreak attacks andbenign datasets show that our methods can consistently improve MLLMs\u0026rsquo;robustness against structure-based jailbreak attacks without compromising themodel\u0026rsquo;s general capabilities evaluated on standard benign tasks. Our code isavailable at https://github.com/rain305f/AdaShield.\rEyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation\nYunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang\nabstract\rabstract: Multimodal large language models (MLLMs) have shown impressive reasoningabilities, which, however, are also more vulnerable to jailbreak attacks thantheir LLM predecessors. Although still capable of detecting unsafe responses,we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can beeasily bypassed due to the introduction of image features. To construct robustMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-freeprotecting approach that exploits the inherent safety awareness of MLLMs, andgenerates safer responses via adaptively transforming unsafe images into textsto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSOenhances model safety significantly (e.g., a 37.6% improvement on theMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), whileconsistently maintaining utility results on common MLLM benchmarks.Furthermore, we show that ECSO can be used as a data engine to generatesupervised-finetuning (SFT) data for MLLM alignment without extra humanintervention.\rAVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions\nHao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang\nabstract\rabstract: Large Vision-Language Models (LVLMs) have shown significant progress in wellresponding to visual-instructions from users. However, these instructions,encompassing images and text, are susceptible to both intentional andinadvertent attacks. Despite the critical importance of LVLMs\u0026rsquo; robustnessagainst such threats, current research in this area remains limited. To bridgethis gap, we introduce AVIBench, a framework designed to analyze the robustnessof LVLMs when facing various adversarial visual-instructions (AVIs), includingfour types of image-based AVIs, ten types of text-based AVIs, and nine types ofcontent bias AVIs (such as gender, violence, cultural, and racial biases, amongothers). We generate 260K AVIs encompassing five categories of multimodalcapabilities (nine tasks) and content bias. We then conduct a comprehensiveevaluation involving 14 open-source LVLMs to assess their performance. AVIBenchalso serves as a convenient tool for practitioners to evaluate the robustnessof LVLMs against AVIs. Our findings and extensive experimental results shedlight on the vulnerabilities of LVLMs, and highlight that inherent biases existeven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. Thisunderscores the importance of enhancing the robustness, security, and fairnessof LVLMs. The source code and benchmark will be made publicly available.\r2024-03-13\nSoftware Vulnerability and Functionality Assessment using LLMs\nRasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir\nabstract\rabstract: While code review is central to the software development process, it can betedious and expensive to carry out. In this paper, we investigate whether andhow Large Language Models (LLMs) can aid with code reviews. Our investigationfocuses on two tasks that we argue are fundamental to good reviews: (i)flagging code with security vulnerabilities and (ii) performing softwarefunctionality validation, i.e., ensuring that code meets its intendedfunctionality. To test performance on both tasks, we use zero-shot andchain-of-thought prompting to obtain final ``approve or reject\u0026rsquo;\u0026lsquo;recommendations. As data, we employ seminal code generation datasets (HumanEvaland MBPP) along with expert-written code snippets with security vulnerabilitiesfrom the Common Weakness Enumeration (CWE). Our experiments consider a mixtureof three proprietary models from OpenAI and smaller open-source LLMs. We findthat the former outperforms the latter by a large margin. Motivated bypromising results, we finally ask our models to provide detailed descriptionsof security vulnerabilities. Results show that 36.7% of LLM-generateddescriptions can be associated with true CWE vulnerabilities.\rSoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks\nGuy Amit, Abigail Goldsteen, Ariel Farkash\nabstract\rabstract: Natural language processing models have experienced a significant upsurge inrecent years, with numerous applications being built upon them. Many of theseapplications require fine-tuning generic base models on customized, proprietarydatasets. This fine-tuning data is especially likely to contain personal orsensitive information about individuals, resulting in increased privacy risk.Membership inference attacks are the most commonly employed attack to assessthe privacy leakage of a machine learning model. However, limited research isavailable on the factors that affect the vulnerability of language models tothis kind of attack, or on the applicability of different defense strategies inthe language domain. We provide the first systematic review of thevulnerability of fine-tuned large language models to membership inferenceattacks, the various factors that come into play, and the effectiveness ofdifferent defense strategies. We find that some training methods providesignificantly reduced privacy risk, with the combination of differentialprivacy and low-rank adaptors achieving the best privacy protection againstthese attacks.\rDr. Jekyll and Mr. Hyde: Two Faces of LLMs\nMatteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek\nabstract\rabstract: Only a year ago, we witnessed a rise in the use of Large Language Models(LLMs), especially when combined with applications like chatbot assistants.Safety mechanisms and specialized training procedures are implemented toprevent improper responses from these assistants. In this work, we bypass thesemeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making themimpersonate complex personas with opposite characteristics as those of thetruthful assistants they are supposed to be. We start by creating elaboratebiographies of these personas, which we then use in a new session with the samechatbots. Our conversation followed a role-play style to get the response theassistant was not allowed to provide. By making use of personas, we show thatthe response that is prohibited is actually provided, making it possible toobtain unauthorized, illegal, or harmful information. This work shows that byusing adversarial personas, one can overcome safety mechanisms set out byChatGPT and Bard. We also introduce several ways of activating such adversarialpersonas, altogether showing that both chatbots are vulnerable to this kind ofattack. With the same principle, we introduce two defenses that push the modelto interpret trustworthy personalities and make it more robust against suchattacks.\rTastle: Distract Large Language Models for Automatic Jailbreak Attack\nZeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen\nabstract\rabstract: Large language models (LLMs) have achieved significant advances in recentdays. Extensive efforts have been made before the public release of LLMs toalign their behaviors with human values. The primary goal of alignment is toensure their helpfulness, honesty and harmlessness. However, even meticulouslyaligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,leading to unintended behaviors. The jailbreak is to intentionally develop amalicious prompt that escapes from the LLM security restrictions to produceuncensored detrimental contents. Previous works explore different jailbreakmethods for red teaming LLMs, yet they encounter challenges regarding toeffectiveness and scalability. In this work, we propose Tastle, a novelblack-box jailbreak framework for automated red teaming of LLMs. We designedmalicious content concealing and memory reframing with an iterativeoptimization algorithm to jailbreak LLMs, motivated by the research about thedistractibility and over-confidence phenomenon of LLMs. Extensive experimentsof jailbreaking both open-source and proprietary LLMs demonstrate thesuperiority of our framework in terms of effectiveness, scalability andtransferability. We also evaluate the effectiveness of existing jailbreakdefense methods against our attack and highlight the crucial need to developmore effective and practical defense strategies.\r2024-03-12\nMulti-LLM Collaboration + Data-Centric Innovation = 2x Better Vulnerability Repair\nXin Zhou, Kisub Kim, Bowen Xu, DongGyun Han, David Lo\nabstract\rabstract: The advances of deep learning (DL) have paved the way for automatic softwarevulnerability repair approaches, which effectively learn the mapping from thevulnerable code to the fixed code. Nevertheless, existing DL-basedvulnerability repair methods face notable limitations: 1) they struggle tohandle lengthy vulnerable code, 2) they treat code as natural language texts,neglecting its inherent structure, and 3) they do not tap into the valuableexpert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural networkmodel that excels at generating vulnerability repairs through data-centricinnovation. Specifically, VulMaster introduces the utilization and combinationof various types of input data, including complete vulnerable code of any size,vulnerable code structures, and expert knowledge from the CWE system.Additionally, VulMaster leverages the collaboration between two Large LanguageModels (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backboneLLM, fine-tuned with the training data, while ChatGPT supplements by providingmissing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerablefunctions. The experimental results demonstrated that VulMaster exhibitssubstantial improvements compared to the learning-based state-of-the-artvulnerability repair approach. Specifically, VulMaster improves the EM, BLEU,and CodeBLEU scores from 10.2% to 20.0%, 21.3% to 29.3%, and 32.5% to40.9%, respectively.\r2024-03-11\nPoisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code\nCristina Improta\nabstract\rabstract: AI-based code generators have gained a fundamental role in assistingdevelopers in writing software starting from natural language (NL). However,since these large language models are trained on massive volumes of datacollected from unreliable online sources (e.g., GitHub, Hugging Face), AImodels become an easy target for data poisoning attacks, in which an attackercorrupts the training data by injecting a small amount of poison into it, i.e.,astutely crafted malicious samples. In this position paper, we address thesecurity of AI code generators by identifying a novel data poisoning attackthat results in the generation of vulnerable code. Next, we devise an extensiveevaluation of how these attacks impact state-of-the-art models for codegeneration. Lastly, we discuss potential solutions to overcome this threat.\rUsing Hallucinations to Bypass GPT4\u0026rsquo;s Filter\nBenjamin Lemkin\nabstract\rabstract: Large language models (LLMs) are initially trained on vast amounts of data,then fine-tuned using reinforcement learning from human feedback (RLHF); thisalso serves to teach the LLM to provide appropriate and safe responses. In thispaper, we present a novel method to manipulate the fine-tuned version intoreverting to its pre-RLHF behavior, effectively erasing the model\u0026rsquo;s filters;the exploit currently works for GPT4, Claude Sonnet, and (to some extent) forInflection-2.5. Unlike other jailbreaks (for example, the popular \u0026ldquo;Do AnythingNow\u0026rdquo; (DAN) ), our method does not rely on instructing the LLM to override itsRLHF policy; hence, simply modifying the RLHF process is unlikely to addressit. Instead, we induce a hallucination involving reversed text during which themodel reverts to a word bucket, effectively pausing the model\u0026rsquo;s filter. Webelieve that our exploit presents a fundamental vulnerability in LLMs currentlyunaddressed, as well as an opportunity to better understand the inner workingsof LLMs during hallucinations.\rCan LLMs Separate Instructions From Data? And What Do We Even Mean By That?\nEgor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert\nabstract\rabstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthroughresults, opening countless new possibilities for many practical applications.However, LLMs lack elementary safety features that are established norms inother areas of computer science, such as the separation between instructionsand data, causing them to malfunction or rendering them vulnerable tomanipulation and interference by third parties e.g., via indirectprompt/command injection. Even worse, so far, there is not even an establisheddefinition of what precisely such a separation would mean and how its violationcould be tested. In this work, we aim to close this gap. We introduce a formalmeasure to quantify the phenomenon of instruction-data separation as well as anempirical variant of the measure that can be computed from a model`s black-boxoutputs. We also introduce a new dataset, SEP (Should it be Executed orProcessed?), which allows estimating the measure, and we report results onseveral state-of-the-art open-source and closed LLMs. Finally, wequantitatively demonstrate that all evaluated LLMs fail to achieve a highamount of separation, according to our measure. The source code and SEP datasetare openly accessible athttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.\r2024-03-10\nFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning\nZhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu\nabstract\rabstract: Instruction tuning has proven essential for enhancing the performance oflarge language models (LLMs) in generating human-aligned responses. However,collecting diverse, high-quality instruction data for tuning poses challenges,particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)has emerged as a solution, leveraging federated learning from multiple dataowners while preserving privacy. Yet, it faces challenges due to limitedinstruction data and vulnerabilities to training data extraction attacks. Toaddress these issues, we propose a novel federated algorithm, FedPIT, whichutilizes LLMs\u0026rsquo; in-context learning capability to self-generate task-specificsynthetic data for training autonomously. Our method employs parameter-isolatedtraining to maintain global parameters trained on synthetic data and localparameters trained on augmented local data, effectively thwarting dataextraction attacks. Extensive experiments on real-world medical datademonstrate the effectiveness of FedPIT in improving federated few-shotperformance while preserving privacy and robustness against data heterogeneity.\rFrom Chatbots to PhishBots? \u0026ndash; Preventing Phishing scams created using ChatGPT, Google Bard and Claude\nSayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin Nilizadeh\nabstract\rabstract: The advanced capabilities of Large Language Models (LLMs) have made theminvaluable across various applications, from conversational agents and contentcreation to data analysis, research, and innovation. However, theireffectiveness and accessibility also render them susceptible to abuse forgenerating malicious content, including phishing attacks. This study exploresthe potential of using four popular commercially available LLMs, i.e., ChatGPT(GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishingattacks using a series of malicious prompts. We discover that these LLMs cangenerate both phishing websites and emails that can convincingly imitatewell-known brands and also deploy a range of evasive tactics that are used toelude detection mechanisms employed by anti-phishing systems. These attacks canbe generated using unmodified or \u0026ldquo;vanilla\u0026rdquo; versions of these LLMs withoutrequiring any prior adversarial exploits such as jailbreaking. We evaluate theperformance of the LLMs towards generating these attacks and find that they canalso be utilized to create malicious prompts that, in turn, can be fed back tothe model to generate phishing scams - thus massively reducing theprompt-engineering effort required by attackers to scale these threats. As acountermeasure, we build a BERT-based automated detection tool that can be usedfor the early detection of malicious prompts to prevent LLMs from generatingphishing content. Our model is transferable across all four commercial LLMs,attaining an average accuracy of 96% for phishing website prompts and 94% forphishing email prompts. We also disclose the vulnerabilities to the concernedLLMs, with Google acknowledging it as a severe issue. Our detection model isavailable for use at Hugging Face, as well as a ChatGPT Actions plugin.\r2024-03-07\nMembership Inference Attacks and Privacy in Topic Modeling\nNico Manzonelli, Wanrong Zhang, Salil Vadhan\nabstract\rabstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.\rEnhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting\nRui Wang, Hongru Wang, Fei Mi, Yi Chen, Boyang Xue, Kam-Fai Wong, Ruifeng Xu\nabstract\rabstract: Numerous works are proposed to align large language models (LLMs) with humanintents to better fulfill instructions, ensuring they are trustful and helpful.Nevertheless, some human instructions are often malicious or misleading andfollowing them will lead to untruthful and unsafe responses. Previous workrarely focused on understanding how LLMs manage instructions based oncounterfactual premises, referred to here as \\textit{inductive instructions},which may stem from users\u0026rsquo; false beliefs or malicious intents. In this paper,we aim to reveal the behaviors of LLMs towards \\textit{inductive instructions}and enhance their truthfulness and helpfulness accordingly. Specifically, wefirst introduce a benchmark of \\underline{\\textbf{Indu}}ctive{In\\underline{\\textbf{st}}ruct}ions (\\textsc{\\textbf{INDust}}), where the falseknowledge is incorporated into instructions in multiple different styles. Afterextensive human and automatic evaluations, we uncovered a universalvulnerability among LLMs in processing inductive instructions. Additionally, weidentified that different inductive styles affect the models\u0026rsquo; ability toidentify the same underlying errors, and the complexity of the underlyingassumptions also influences the model\u0026rsquo;s performance. Motivated by theseresults, we propose \\textsc{Dual-critique} prompting to improve LLM robustnessagainst inductive instructions. Our experiments demonstrate that\\textsc{Dual-critique} prompting significantly bolsters the robustness of adiverse array of LLMs, even when confronted with varying degrees of inductiveinstruction complexity and differing inductive styles.\r2024-03-06\nFuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing\nAsmita, Yaroslav Oliinyk, Michael Scott, Ryan Tsang, Chongzhou Fang, Houman Homayoun\nabstract\rabstract: BusyBox, an open-source software bundling over 300 essential Linux commandsinto a single executable, is ubiquitous in Linux-based embedded devices.Vulnerabilities in BusyBox can have far-reaching consequences, affecting a widearray of devices. This research, driven by the extensive use of BusyBox, delvedinto its analysis. The study revealed the prevalence of older BusyBox versionsin real-world embedded products, prompting us to conduct fuzz testing onBusyBox. Fuzzing, a pivotal software testing method, aims to induce crashesthat are subsequently scrutinized to uncover vulnerabilities. Within thisstudy, we introduce two techniques to fortify software testing. The firsttechnique enhances fuzzing by leveraging Large Language Models (LLM) togenerate target-specific initial seeds. Our study showed a substantial increasein crashes when using LLM-generated initial seeds, highlighting the potentialof LLM to efficiently tackle the typically labor-intensive task of generatingtarget-specific initial seeds. The second technique involves repurposingpreviously acquired crash data from similar fuzzed targets before initiatingfuzzing on a new target. This approach streamlines the time-consuming fuzztesting process by providing crash data directly to the new target beforecommencing fuzzing. We successfully identified crashes in the latest BusyBoxtarget without conducting traditional fuzzing, emphasizing the effectiveness ofLLM and crash reuse techniques in enhancing software testing and improvingvulnerability detection in embedded systems. Additionally, manual triaging wasperformed to identify the nature of crashes in the latest BusyBox.\r2024-03-05\nTowards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement\nRafaela Martelo, Ruo-Qian Wang\nabstract\rabstract: Real-time flood forecasting plays a crucial role in enabling timely andeffective emergency responses. However, a significant challenge lies inbridging the gap between complex numerical flood models and practicaldecision-making. Decision-makers often rely on experts to interpret thesemodels for optimizing flood mitigation strategies. And the public requirescomplex techniques to inquiry and understand socio-cultural and institutionalfactors, often hinders the public\u0026rsquo;s understanding of flood risks. To overcomethese challenges, our study introduces an innovative solution: a customized AIAssistant powered by the GPT-4 Large Language Model. This AI Assistant isdesigned to facilitate effective communication between decision-makers, thegeneral public, and flood forecasters, without the requirement of specializedknowledge. The new framework utilizes GPT-4\u0026rsquo;s advanced natural languageunderstanding and function calling capabilities to provide immediate floodalerts and respond to various flood-related inquiries. Our developed prototypeintegrates real-time flood warnings with flood maps and social vulnerabilitydata. It also effectively translates complex flood zone information intoactionable risk management advice. To assess its performance, we evaluated theprototype using six criteria within three main categories: relevance, errorresilience, and understanding of context. Our research marks a significant steptowards a more accessible and user-friendly approach in flood risk management.This study highlights the potential of advanced AI tools like GPT-4 indemocratizing information and enhancing public engagement in critical socialand environmental issues.\rGradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes\nXiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho\nabstract\rabstract: Large Language Models (LLMs) are becoming a prominent generative AI tool,where the user enters a query and the LLM generates an answer. To reduce harmand misuse, efforts have been made to align these LLMs to human values usingadvanced training techniques such as Reinforcement Learning from Human Feedback(RLHF). However, recent studies have highlighted the vulnerability of LLMs toadversarial jailbreak attempts aiming at subverting the embedded safetyguardrails. To address this challenge, this paper defines and investigates theRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detectjailbreak attempts. Gradient Cuff exploits the unique properties observed inthe refusal loss landscape, including functional values and its smoothness, todesign an effective two-step detection strategy. Experimental results on twoaligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreakattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff cansignificantly improve the LLM\u0026rsquo;s rejection capability for malicious jailbreakqueries, while maintaining the model\u0026rsquo;s performance for benign user queries byadjusting the detection threshold.\r2024-03-04\nDenevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning\nShitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu\nabstract\rabstract: Large Language Models (LLMs) have made unprecedented breakthroughs, yet theirincreasing integration into everyday life might raise societal risks due togenerated unethical content. Despite extensive study on specific issues likebias, the intrinsic values of LLMs remain largely unexplored from a moralphilosophy perspective. This work delves into ethical values utilizing MoralFoundation Theory. Moving beyond conventional discriminative evaluations withpoor reliability, we propose DeNEVIL, a novel prompt generation algorithmtailored to dynamically exploit LLMs\u0026rsquo; value vulnerabilities and elicit theviolation of ethics in a generative manner, revealing their underlying valueinclinations. On such a basis, we construct MoralPrompt, a high-quality datasetcomprising 2,397 prompts covering 500+ value principles, and then benchmark theintrinsic values across a spectrum of LLMs. We discovered that most models areessentially misaligned, necessitating further ethical value alignment. Inresponse, we develop VILMO, an in-context alignment method that substantiallyenhances the value compliance of LLM outputs by learning to generateappropriate value instructions, outperforming existing competitors. Our methodsare suitable for black-box and open-source models, offering a promising initialstep in studying the ethical values of LLMs.\r2024-03-03\nBreaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models\nArijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha\nabstract\rabstract: Large Language Models (LLMs) have become a cornerstone in the field ofNatural Language Processing (NLP), offering transformative capabilities inunderstanding and generating human-like text. However, with their risingprominence, the security and vulnerability aspects of these models havegarnered significant attention. This paper presents a comprehensive survey ofthe various forms of attacks targeting LLMs, discussing the nature andmechanisms of these attacks, their potential impacts, and current defensestrategies. We delve into topics such as adversarial attacks that aim tomanipulate model outputs, data poisoning that affects model training, andprivacy concerns related to training data exploitation. The paper also exploresthe effectiveness of different attack methodologies, the resilience of LLMsagainst these attacks, and the implications for model integrity and user trust.By examining the latest research, we provide insights into the currentlandscape of LLM vulnerabilities and defense mechanisms. Our objective is tooffer a nuanced understanding of LLM attacks, foster awareness within the AIcommunity, and inspire robust solutions to mitigate these risks in futuredevelopments.\rIgnore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition\nSander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber\nabstract\rabstract: Large Language Models (LLMs) are deployed in interactive contexts with directuser engagement, such as chatbots and writing assistants. These deployments arevulnerable to prompt injection and jailbreaking (collectively, prompt hacking),in which models are manipulated to ignore their original instructions andfollow potentially malicious ones. Although widely acknowledged as asignificant security threat, there is a dearth of large-scale resources andquantitative studies on prompt hacking. To address this lacuna, we launch aglobal prompt hacking competition, which allows for free-form human inputattacks. We elicit 600K+ adversarial prompts against three state-of-the-artLLMs. We describe the dataset, which empirically verifies that current LLMs canindeed be manipulated via prompt hacking. We also present a comprehensivetaxonomical ontology of the types of adversarial prompts.\rTowards Efficient and Reliable LLM Serving: A Real-World Workload Study\nYuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, Xiaowen Chu\nabstract\rabstract: Large language models (LLMs), especially Generative Pretrained Transformer(GPT) models, have significantly advanced in the industry in recent years.However, these models\u0026rsquo; broader development faces considerable challenges due tohigh operational and deployment costs. This has led to active research inimproving the hardware efficiency of LLMs. Yet, the characteristics ofreal-world LLM workloads are often overlooked in current optimizations of LLMserving systems. In this work, the absence of reliable workload data forevaluating LLM serving systems impacts the quality of service (QoS) andreliability in industrial deployments. This paper introduces the firstreal-world trace dataset of LLM serving workloads, detailing user, system, andLLM behaviors. We analyze this trace, highlighting burstiness, request andresponse distributions, and focusing on the reliability of GPT services. Basedon this, we have developed a benchmark suite that reflects our dataset\u0026rsquo;sworkload patterns, enabling performance evaluation of serving systems. Thissuite captures the core patterns of workload distributions, allowing forprecise scaling of the workload dataset to match system sizes. Our evaluationuncovers a previously unrecognized vulnerability of LLM serving systems toshort-term burstiness, particularly in common workload scenarios. We observethat GPU memory limitations, caused by the fluctuating nature of burstiness,lead to significant performance degradation in existing LLM serving systems.Beyond benchmarking, understanding these patterns is valuable for optimizingLLM workload management, enabling elastic hardware resource adjustments tovarying workloads. To encourage further research, we have made the dataset andbenchmark suite publicly available at https://github.com/HPMLL/BurstGPT.\r2024-03-02\nAutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks\nYifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu\nabstract\rabstract: Despite extensive pre-training and fine-tuning in moral alignment to preventgenerating harmful information at user request, large language models (LLMs)remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense,a response-filtering based multi-agent defense framework that filters harmfulresponses from LLMs. This framework assigns different roles to LLM agents andemploys them to complete the defense task collaboratively. The division intasks enhances the overall instruction-following of LLMs and enables theintegration of other defense components as tools. AutoDefense can adapt tovarious sizes and kinds of open-source LLMs that serve as agents. Throughconducting extensive experiments on a large scale of harmful and safe prompts,we validate the effectiveness of the proposed AutoDefense in improving therobustness against jailbreak attacks, while maintaining the performance atnormal user request. Our code and data are publicly available athttps://github.com/XHMY/AutoDefense.\rAnalysis of Privacy Leakage in Federated Large Language Models\nMinh N. Vu, Truc Nguyen, Tre\u0026rsquo; R. Jeter, My T. Thai\nabstract\rabstract: With the rapid adoption of Federated Learning (FL) as the training and tuningprotocol for applications utilizing Large Language Models (LLMs), recentresearch highlights the need for significant modifications to FL to accommodatethe large-scale of LLMs. While substantial adjustments to the protocol havebeen introduced as a response, comprehensive privacy analysis for the adaptedFL protocol is currently lacking. To address this gap, our work delves into an extensive examination of theprivacy analysis of FL when used for training LLMs, both from theoretical andpractical perspectives. In particular, we design two active membershipinference attacks with guaranteed theoretical success rates to assess theprivacy leakages of various adapted FL configurations. Our theoretical findingsare translated into practical attacks, revealing substantial privacyvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, andOpenAI\u0026rsquo;s GPTs, across multiple real-world language datasets. Additionally, weconduct thorough experiments to evaluate the privacy leakage of these modelswhen data is protected by state-of-the-art differential privacy (DP)mechanisms.\rInexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy\nJamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot\nabstract\rabstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel\u0026rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to theirU-MIA'' counterparts). We propose a categorization of existing U-MIAs intopopulation U-MIAs\u0026rsquo;\u0026rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs\u0026rsquo;\u0026rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.\r2024-03-01\nFinetuning Large Language Models for Vulnerability Detection\nAlexey Shestov, Rodion Levichev, Ravil Mussabayev, Evgeny Maslov, Anton Cheshkov, Pavel Zadorozhny\nabstract\rabstract: This paper presents the results of finetuning large language models (LLMs)for the task of detecting vulnerabilities in source code. We leverageWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, andadapt it for vulnerability detection through further finetuning. To acceleratetraining, we modify WizardCoder\u0026rsquo;s training procedure, also we investigateoptimal training regimes. For the imbalanced dataset with many more negativeexamples than positive, we also explore different techniques to improveclassification performance. The finetuned WizardCoder model achievesimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerabilitydatasets over CodeBERT-like model, demonstrating the effectiveness of adaptingpretrained LLMs for vulnerability detection in source code. The keycontributions are finetuning the state-of-the-art code LLM, WizardCoder,increasing its training speed without the performance harm, optimizing thetraining procedure and regimes, handling class imbalance, and improvingperformance on difficult vulnerability detection datasets. This demonstratesthe potential for transfer learning by finetuning large pretrained languagemodels for specialized source code analysis tasks.\rCrimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models\nJiandong Jin, Bowen Tang, Mingxuan Ma, Xiao Liu, Yunfei Wang, Qingnan Lai, Jia Yang, Changling Zhou\nabstract\rabstract: We introduces Crimson, a system that enhances the strategic reasoningcapabilities of Large Language Models (LLMs) within the realm of cybersecurity.By correlating CVEs with MITRE ATT\u0026amp;CK techniques, Crimson advances threatanticipation and strategic defense efforts. Our approach includes defining andevaluating cybersecurity strategic tasks, alongside implementing acomprehensive human-in-the-loop data-synthetic workflow to develop theCVE-to-ATT\u0026amp;CK Mapping (CVEM) dataset. We further enhance LLMs\u0026rsquo; reasoningabilities through a novel Retrieval-Aware Training (RAT) process and itsrefined iteration, RAT-R. Our findings demonstrate that an LLM fine-tuned with our techniques,possessing 7 billion parameters, approaches the performance level of GPT-4,showing markedly lower rates of hallucination and errors, and surpassing othermodels in strategic reasoning tasks. Moreover, domain-specific fine-tuning ofembedding models significantly improves performance within cybersecuritycontexts, underscoring the efficacy of our methodology. By leveraging Crimsonto convert raw vulnerability data into structured and actionable insights, webolster proactive cybersecurity defenses.\rDrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers\nXirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh\nabstract\rabstract: The safety alignment of Large Language Models (LLMs) is vulnerable to bothmanual and automated jailbreak attacks, which adversarially trigger LLMs tooutput harmful content. However, current methods for jailbreaking LLMs, whichnest entire harmful prompts, are not effective at concealing malicious intentand can be easily identified and rejected by well-aligned LLMs. This paperdiscovers that decomposing a malicious prompt into separated sub-prompts caneffectively obscure its underlying malicious intent by presenting it in afragmented, less detectable form, thereby addressing these limitations. Weintroduce an automatic prompt \\textbf{D}ecomposition and\\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack).DrAttack includes three key components: (a) Decomposition' of the originalprompt into sub-prompts, (b) Reconstruction\u0026rsquo; of these sub-prompts implicitlyby in-context learning with semantically similar but harmless reassemblingdemo, and (c) a `Synonym Search\u0026rsquo; of sub-prompts, aiming to find sub-prompts\u0026rsquo;synonyms that maintain the original intent while jailbreaking LLMs. Anextensive empirical study across multiple open-source and closed-source LLMsdemonstrates that, with a significantly reduced number of queries, DrAttackobtains a substantial gain of success rate over prior SOTA prompt-onlyattackers. Notably, the success rate of 78.0% on GPT-4 with merely 15 queriessurpassed previous art by 33.1%. The project is available athttps://github.com/xirui-li/DrAttack.\r2024-02-29\nTypographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts\nHao Cheng, Erjia Xiao, Renjing Xu\nabstract\rabstract: Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models(VLMs) and Large Language Models (LLMs) to perform amazing emergent abilitieson various multimodal tasks in the joint space of vision and language. However,the Typographic Attack, which shows disruption to VLMs, has also been certifiedas a security vulnerability to LMMs. In this work, we first comprehensivelyinvestigate the distractibility of LMMs by typography. In particular, weintroduce the Typographic Dataset designed to evaluate distractibility acrossvarious multi-modal subtasks, such as object recognition, visual attributesdetection, enumeration, arithmetic computation, and commonsense reasoning. Tofurther study the effect of typographic patterns on performance, we alsoscrutinize the effect of tuning various typographic factors, encompassing fontsize, color, opacity, and spatial positioning of typos. We discover that LMMscan partially distinguish visual contents and typos when confrontingtypographic attacks, which suggests that embeddings from vision encoderscontain enough information to distinguish visual contents and typos in images.Inspired by such phenomena, we demonstrate that CLIP\u0026rsquo;s performance of zero-shotclassification on typo-ridden images can be significantly improved by providingmore informative texts to match images. Furthermore, we also prove that LMMscan utilize more informative prompts to leverage information in embeddings todifferentiate between visual content and typos. Finally, we propose a promptinformation enhancement method that can effectively mitigate the effects oftypography.\rThink Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool\nLiudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones, Gerhard Schwabe\nabstract\rabstract: In today\u0026rsquo;s digital age, characterized by rapid news consumption andincreasing vulnerability to propaganda, fostering citizens\u0026rsquo; critical thinkingis crucial for stable democracies. This paper introduces the design ofClarifAI, a novel automated propaganda detection tool designed to nudge readerstowards more critical news consumption by activating the analytical mode ofthinking, following Kahneman\u0026rsquo;s dual-system theory of cognition. Using LargeLanguage Models, ClarifAI detects propaganda in news articles and providescontext-rich explanations, enhancing users\u0026rsquo; understanding and criticalthinking. Our contribution is threefold: first, we propose the design ofClarifAI; second, in an online experiment, we demonstrate that this designeffectively encourages news readers to engage in more critical reading; andthird, we emphasize the value of explanations for fostering critical thinking.The study thus offers both a practical tool and useful design knowledge formitigating propaganda in digital news.\rHere\u0026rsquo;s a Free Lunch: Sanitizing Backdoored Models with Model Merge\nAnsh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu\nabstract\rabstract: The democratization of pre-trained language models through open-sourceinitiatives has rapidly advanced innovation and expanded access to cutting-edgetechnologies. However, this openness also brings significant security risks,including backdoor attacks, where hidden malicious behaviors are triggered byspecific inputs, compromising natural language processing (NLP) systemintegrity and reliability. This paper suggests that merging a backdoored modelwith other homogeneous models can remediate backdoor vulnerabilities even ifsuch models are not entirely secure. In our experiments, we explore variousmodels (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets(SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensiveapproaches, our method offers an effective and efficient inference-stagedefense against backdoor attacks without additional resources or specificknowledge. Our approach consistently outperforms the other advanced baselines,leading to an average of 75% reduction in the attack success rate. Since modelmerging has been an established approach for improving model performance, theextra advantage it provides regarding defense can be seen as a cost-free bonus.\rPre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness\nSibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan\nabstract\rabstract: Large-scale pre-trained vision-language models like CLIP have demonstratedimpressive performance across various tasks, and exhibit remarkable zero-shotgeneralization capability, while they are also vulnerable to imperceptibleadversarial examples. Existing works typically employ adversarial training(fine-tuning) as a defense method against adversarial examples. However, directapplication to the CLIP model may result in overfitting, compromising themodel\u0026rsquo;s capacity for generalization. In this paper, we propose Pre-trainedModel Guided Adversarial Fine-Tuning (PMG-AFT) method, which leveragessupervision from the original pre-trained model by carefully designing anauxiliary branch, to enhance the model\u0026rsquo;s zero-shot adversarial robustness.Specifically, PMG-AFT minimizes the distance between the features ofadversarial examples in the target model and those in the pre-trained model,aiming to preserve the generalization features already captured by thepre-trained model. Extensive Experiments on 15 zero-shot datasets demonstratethat PMG-AFT significantly outperforms the state-of-the-art method, improvingthe top-1 robust accuracy by an average of 4.99%. Furthermore, our approachconsistently improves clean accuracy by an average of 8.72%.\rCognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking\nNan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen\nabstract\rabstract: While large language models (LLMs) have demonstrated increasing power, theyhave also given rise to a wide range of harmful behaviors. As representatives,jailbreak attacks can provoke harmful or unethical responses from LLMs, evenafter safety alignment. In this paper, we investigate a novel category ofjailbreak attacks specifically designed to target the cognitive structure andprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs inthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)effect-to-cause reasoning. Different from previous jailbreak attacks, ourproposed cognitive overload is a black-box attack with no need for knowledge ofmodel architecture or access to model weights. Experiments conducted onAdvBench and MasterKey reveal that various LLMs, including both popularopen-source model Llama 2 and the proprietary model ChatGPT, can be compromisedthrough cognitive overload. Motivated by cognitive psychology work on managingcognitive load, we further investigate defending cognitive overload attack fromtwo perspectives. Empirical studies show that our cognitive overload from threeperspectives can jailbreak all studied LLMs successfully, while existingdefense strategies can hardly mitigate the caused malicious uses effectively.\r2024-02-28\nMaking Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction\nTong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen\nabstract\rabstract: In recent years, large language models (LLMs) have demonstrated notablesuccess across various tasks, but the trustworthiness of LLMs is still an openproblem. One specific threat is the potential to generate toxic or harmfulresponses. Attackers can craft adversarial prompts that induce harmfulresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMssecurity by identifying bias vulnerabilities within the safety fine-tuning anddesign a black-box jailbreak method named DRA (Disguise and ReconstructionAttack), which conceals harmful instructions through disguise and prompts themodel to reconstruct the original harmful instruction within its completion. Weevaluate DRA across various open-source and close-source models, showcasingstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRAboasts a 90% attack success rate on LLM chatbots GPT-4.\rDefending Large Language Models against Jailbreak Attacks via Semantic Smoothing\nJiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang\nabstract\rabstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,which bypass the safeguards of targeted LLMs and fool them into generatingobjectionable content. While initial defenses show promise against token-basedthreat models, there do not exist defenses that provide robustness againstsemantic attacks and avoid unfavorable trade-offs between robustness andnominal performance. To meet this need, we propose SEMANTICSMOOTH, asmoothing-based defense that aggregates the predictions of multiplesemantically transformed copies of a given input prompt. Experimental resultsdemonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness againstGCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance oninstruction following benchmarks such as InstructionFollowing and AlpacaEval.The codes will be publicly available athttps://github.com/UCSB-NLP-Chang/SemanticSmooth.\rLLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History\nAkash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, Mario Fritz\nabstract\rabstract: With the recent emergence of powerful instruction-tuned large language models(LLMs), various helpful conversational Artificial Intelligence (AI) systemshave been deployed across many applications. When prompted by users, these AIsystems successfully perform a wide range of tasks as part of a conversation.To provide some sort of memory and context, such approaches typically conditiontheir output on the entire conversational history. Although this sensitivity tothe conversational history can often lead to improved performance on subsequenttasks, we find that performance can in fact also be negatively impacted, ifthere is a task-switch. To the best of our knowledge, our work makes the firstattempt to formalize the study of such vulnerabilities and interference oftasks in conversational LLMs caused by task-switches in the conversationalhistory. Our experiments across 5 datasets with 15 task switches using popularLLMs reveal that many of the task-switches can lead to significant performancedegradation.\rDefending LLMs against Jailbreaking Attacks via Backtranslation\nYihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh\nabstract\rabstract: Although many large language models (LLMs) have been trained to refuseharmful requests, they are still vulnerable to jailbreaking attacks, whichrewrite the original prompt to conceal its harmful intent. In this paper, wepropose a new method for defending LLMs against jailbreaking attacks by``backtranslation\u0026rsquo;\u0026rsquo;. Specifically, given an initial response generated by thetarget LLM from an input prompt, our backtranslation prompts a language modelto infer an input prompt that can lead to the response. The inferred prompt iscalled the backtranslated prompt which tends to reveal the actual intent of theoriginal prompt, since it is generated based on the LLM\u0026rsquo;s response and is notdirectly manipulated by the attacker. We then run the target LLM again on thebacktranslated prompt, and we refuse the original prompt if the model refusesthe backtranslated prompt. We explain that the proposed defense providesseveral benefits on its effectiveness and efficiency. We empiricallydemonstrate that our defense significantly outperforms the baselines, in thecases that are hard for the baselines, and our defense also has little impacton the generation quality for benign input prompts.\rA New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems\nFangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao\nabstract\rabstract: Large Language Model (LLM) systems are inherently compositional, withindividual LLM serving as the core foundation with additional layers of objectssuch as plugins, sandbox, and so on. Along with the great potential, there arealso increasing concerns over the security of such probabilistic intelligentsystems. However, existing studies on LLM security often focus on individualLLM, but without examining the ecosystem through the lens of LLM systems withother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, wesystematically analyze the security of LLM systems, instead of focusing on theindividual LLMs. To do so, we build on top of the information flow andformulate the security of LLM systems as constraints on the alignment of theinformation flow within LLM and between LLM and other objects. Based on thisconstruction and the unique probabilistic nature of LLM, the attack surface ofthe LLM system can be decomposed into three key components: (1) multi-layersecurity analysis, (2) analysis of the existence of constraints, and (3)analysis of the robustness of these constraints. To ground this new attacksurface, we propose a multi-layer and multi-step approach and apply it to thestate-of-art LLM system, OpenAI GPT4. Our investigation exposes severalsecurity issues, not just within the LLM model itself but also in itsintegration with other components. We found that although the OpenAI GPT4 hasdesigned numerous safety constraints to improve its safety features, thesesafety constraints are still vulnerable to attackers. To further demonstratethe real-world threats of our discovered vulnerabilities, we construct anend-to-end attack where an adversary can illicitly acquire the user\u0026rsquo;s chathistory, all without the need to manipulate the user\u0026rsquo;s input or gain directaccess to OpenAI GPT4. Our demo is in the link:https://fzwark.github.io/LLM-System-Attack-Demo/\r2024-02-27\nChain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities\nYu Nong, Mohammed Aldeen, Long Cheng, Hongxin Hu, Feng Chen, Haipeng Cai\nabstract\rabstract: Security vulnerabilities are increasingly prevalent in modern software andthey are widely consequential to our society. Various approaches to defendingagainst these vulnerabilities have been proposed, among which those leveragingdeep learning (DL) avoid major barriers with other techniques hence attractingmore attention in recent years. However, DL-based approaches face criticalchallenges including the lack of sizable and quality-labeled task-specificdatasets and their inability to generalize well to unseen, real-worldscenarios. Lately, large language models (LLMs) have demonstrated impressivepotential in various domains by overcoming those challenges, especially throughchain-of-thought (CoT) prompting. In this paper, we explore how to leverageLLMs and CoT to address three key software vulnerability analysis tasks:identifying a given type of vulnerabilities, discovering vulnerabilities of anytype, and patching detected vulnerabilities. We instantiate the general CoTmethodology in the context of these tasks through VSP , our unified,vulnerability-semantics-guided prompting approach, and conduct extensiveexperiments assessing VSP versus five baselines for the three tasks againstthree LLMs and two datasets. Results show substantial superiority of ourCoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy forvulnerability identification, discovery, and patching, respectively, on CVEdatasets) over the baselines. Through in-depth case studies analyzing VSPfailures, we also reveal current gaps in LLM/CoT for challenging vulnerabilitycases, while proposing and validating respective improvements.\rSemantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs\nXiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang\nabstract\rabstract: Large Language Models (LLMs), used in creative writing, code generation, andtranslation, generate text based on input sequences but are vulnerable tojailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreakprompt methods use a combination of jailbreak templates followed by questionsto ask to create jailbreak prompts. However, existing jailbreak prompt designsgenerally suffer from excessive semantic differences, resulting in an inabilityto resist defenses that use simple semantic metrics as thresholds. Jailbreakprompts are semantically more varied than the original questions used forqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approachthat bypasses LLMs by generating jailbreak prompts that are semanticallysimilar to the original question. We model the search for jailbreak promptsthat satisfy both semantic similarity and jailbreak validity as amulti-objective optimization problem and employ a standardized set of geneticalgorithms for generating eligible prompts. Compared to the baselineAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%higher without ONION defense and 85.2% higher with ONION defense. SMJ\u0026rsquo;s betterperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,Similarity, and Outlier, also means that SMJ is resistant to defenses that usethose metrics as thresholds.\rLLM-Resistant Math Word Problem Generation via Adversarial Attacks\nRoy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra\nabstract\rabstract: Large language models (LLMs) have significantly transformed the educationallandscape. As current plagiarism detection tools struggle to keep pace withLLMs\u0026rsquo; rapid advancements, the educational community faces the challenge ofassessing students\u0026rsquo; true problem-solving abilities in the presence of LLMs. Inthis work, we explore a new paradigm for ensuring fair evaluation \u0026ndash; generatingadversarial examples which preserve the structure and difficulty of theoriginal questions aimed for assessment, but are unsolvable by LLMs. Focusingon the domain of math word problems, we leverage abstract syntax trees tostructurally generate adversarial examples that cause LLMs to produce incorrectanswers by simply editing the numeric values in the problems. We conductexperiments on various open- and closed-source LLMs, quantitatively andqualitatively demonstrating that our method significantly degrades their mathproblem-solving ability. We identify shared vulnerabilities among LLMs andpropose a cost-effective approach to attack high-cost models. Additionally, weconduct automatic analysis on math problems and investigate the cause offailure to guide future research on LLM\u0026rsquo;s mathematical capability.\rSpeak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue\nZhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su\nabstract\rabstract: Large Language Models (LLMs) have been demonstrated to generate illegal orunethical responses, particularly when subjected to \u0026ldquo;jailbreak.\u0026rdquo; Research onjailbreak has highlighted the safety issues of LLMs. However, prior studieshave predominantly focused on single-turn dialogue, ignoring the potentialcomplexities and risks presented by multi-turn dialogue, a crucial mode throughwhich humans derive information from LLMs. In this paper, we argue that humanscould exploit multi-turn dialogue to induce LLMs into generating harmfulinformation. LLMs may not intend to reject cautionary or borderline unsafequeries, even if each turn is closely served for one malicious purpose in amulti-turn dialogue. Therefore, by decomposing an unsafe query into severalsub-queries for multi-turn dialogue, we induced LLMs to answer harmfulsub-questions incrementally, culminating in an overall harmful response. Ourexperiments, conducted across a wide range of LLMs, indicate currentinadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Ourfindings expose vulnerabilities of LLMs in complex scenarios involvingmulti-turn dialogue, presenting new challenges for the safety of LLMs.\r2024-02-26\nDecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\nBoxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li\nabstract\rabstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance \u0026ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives \u0026ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .\rRainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts\nMikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu\nabstract\rabstract: As large language models (LLMs) become increasingly prevalent across manyreal-world applications, understanding and enhancing their robustness to userinputs is of paramount importance. Existing methods for identifying adversarialprompts tend to focus on specific domains, lack diversity, or require extensivehuman annotations. To address these limitations, we present Rainbow Teaming, anovel approach for producing a diverse collection of adversarial prompts.Rainbow Teaming casts adversarial prompt generation as a quality-diversityproblem, and uses open-ended search to generate prompts that are both effectiveand diverse. It can uncover a model\u0026rsquo;s vulnerabilities across a broad range ofdomains including, in this paper, safety, question answering, andcybersecurity. We also demonstrate that fine-tuning on synthetic data generatedby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurtingtheir general capabilities and helpfulness, paving the path to open-endedself-improvement.\r2024-02-25\nLuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices\nJiahui Xiang, Wenhai Wang, Tong Ye, Peiyu Liu\nabstract\rabstract: IoT devices are currently facing continuous malicious attacks due to theirwidespread use. Among these IoT devices, web vulnerabilities are also widelyexploited because of their inherent characteristics, such as improperpermission controls and insecure interfaces. Recently, the embedded system webinterface framework has become highly diverse, and specific vulnerabilities canarise if developers forget to detect user input parameters or if the detectionprocess is not strict enough. Therefore, discovering vulnerabilities in the webinterfaces of IoT devices accurately and comprehensively through an automatedmethod is a major challenge. This paper aims to work out the challenge. We havedeveloped an automated vulnerability detection system called LuaTaint for thetypical web interface framework, LuCI. The system employs static taint analysisto address web security issues on mobile terminal platforms to ensure detectioncoverage. It integrates rules pertaining to page handler control logic withinthe taint detection process to improve its extensibility. We also implemented apost-processing step with the assistance of large language models to enhanceaccuracy and reduce the need for manual analysis. We have created a prototypeof LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors.LuaTaint has discovered 68 unknown vulnerabilities.\rSafety of Multimodal Large Language Models on Images and Text\nXin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao\nabstract\rabstract: Attracted by the impressive power of Multimodal Large Language Models(MLLMs), the public is increasingly utilizing them to improve the efficiency ofdaily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructionsbring huge safety risks when these models are deployed in real-world scenarios.In this paper, we systematically survey current efforts on the evaluation,attack, and defense of MLLMs\u0026rsquo; safety on images and text. We begin withintroducing the overview of MLLMs on images and text and understanding ofsafety, which helps researchers know the detailed scope of our survey. Then, wereview the evaluation datasets and metrics for measuring the safety of MLLMs.Next, we comprehensively present attack and defense techniques related toMLLMs\u0026rsquo; safety. Finally, we analyze several unsolved issues and discusspromising research directions. The latest papers are continually collected athttps://github.com/isXinLiu/MLLM-Safety-Collection.\rEvaluating Robustness of Generative Search Engine on Adversarial Factual Questions\nXuming Hu, Xiaochuan Li, Junzhe Chen, Yinghui Li, Yangning Li, Xiaoguang Li, Yasheng Wang, Qun Liu, Lijie Wen, Philip S. Yu, Zhijiang Guo\nabstract\rabstract: Generative search engines have the potential to transform how people seekinformation online, but generated responses from existing large language models(LLMs)-backed generative search engines may not always be accurate.Nonetheless, retrieval-augmented generation exacerbates safety concerns, sinceadversaries may successfully evade the entire system by subtly manipulating themost vulnerable part of a claim. To this end, we propose evaluating therobustness of generative search engines in the realistic and high-risk setting,where adversaries have only black-box system access and seek to deceive themodel into returning incorrect responses. Through a comprehensive humanevaluation of various generative search engines, such as Bing Chat,PerplexityAI, and YouChat across diverse queries, we demonstrate theeffectiveness of adversarial factual questions in inducing incorrect responses.Moreover, retrieval-augmented generation exhibits a higher susceptibility tofactual errors compared to LLMs without retrieval. These findings highlight thepotential security risks of these systems and emphasize the need for rigorousevaluation before deployment.\r2024-02-24\nOn the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities\nXiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi\nabstract\rabstract: In this paper, we highlight the critical issues of robustness and safetyassociated with integrating large language models (LLMs) and vision-languagemodels (VLMs) into robotics applications. Recent works have focused on usingLLMs and VLMs to improve the performance of robotics tasks, such asmanipulation, navigation, etc. However, such integration can introducesignificant vulnerabilities, in terms of their susceptibility to adversarialattacks due to the language models, potentially leading to catastrophicconsequences. By examining recent works at the interface of LLMs/VLMs androbotics, we show that it is easy to manipulate or misguide the robot\u0026rsquo;sactions, leading to safety hazards. We define and provide examples of severalplausible adversarial attacks, and conduct experiments on three prominent robotframeworks integrated with a language model, including KnowNo VIMA, andInstruct2Act, to assess their susceptibility to these attacks. Our empiricalfindings reveal a striking vulnerability of LLM/VLM-robot integrated systems:simple adversarial attacks can significantly undermine the effectiveness ofLLM/VLM-robot integrated systems. Specifically, our data demonstrate an averageperformance deterioration of 21.2% under prompt attacks and a more alarming30.2% under perception attacks. These results underscore the critical need forrobust countermeasures to ensure the safe and reliable deployment of theadvanced LLM/VLM-based robotic systems.\r2024-02-23\nThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nShenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang\nabstract\rabstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs\u0026rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.\rItem-side Fairness of Large Language Model-based Recommendation System\nMeng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He\nabstract\rabstract: Recommendation systems for Web content distribution intricately connect tothe information access and exposure opportunities for vulnerable populations.The emergence of Large Language Models-based Recommendation System (LRS) mayintroduce additional societal challenges to recommendation systems due to theinherent biases in Large Language Models (LLMs). From the perspective ofitem-side fairness, there remains a lack of comprehensive investigation intothe item-side fairness of LRS given the unique characteristics of LRS comparedto conventional recommendation systems. To bridge this gap, this study examinesthe property of LRS with respect to item-side fairness and reveals theinfluencing factors of both historical users\u0026rsquo; interactions and inherentsemantic biases of LLMs, shedding light on the need to extend conventionalitem-side fairness methods for LRS. Towards this goal, we develop a concise andeffective framework called IFairLRS to enhance the item-side fairness of anLRS. IFairLRS covers the main stages of building an LRS with specificallyadapted strategies to calibrate the recommendations of LRS. We utilize IFairLRSto fine-tune LLaMA, a representative LLM, on \\textit{MovieLens} and\\textit{Steam} datasets, and observe significant item-side fairnessimprovements. The code can be found inhttps://github.com/JiangM-C/IFairLRS.git.\rUser Inference Attacks on Large Language Models\nNikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu\nabstract\rabstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user\u0026rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user\u0026rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.\rA First Look at GPT Apps: Landscape and Vulnerability\nZejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian\nabstract\rabstract: With the advancement of Large Language Models (LLMs), increasinglysophisticated and powerful GPTs are entering the market. Despite theirpopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs\u0026rsquo;susceptibility to attacks raises concerns over safety and plagiarism. Thus, inthis work, we conduct a pioneering exploration of GPT stores, aiming to studyvulnerabilities and plagiarism within GPT applications. To begin with, weconduct, to our knowledge, the first large-scale monitoring and analysis of twostores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, wepropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.To complete these two tasks efficiently, we develop two automated tools: onefor web scraping and another designed for programmatically interacting withGPTs. Our findings reveal a significant enthusiasm among users and developersfor GPT interaction and creation, as evidenced by the rapid increase in GPTsand their creators. However, we also uncover a widespread failure to protectGPT internals, with nearly 90% of system prompts easily accessible, leading toconsiderable plagiarism and duplication among GPTs.\rDeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models\nBerkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan, Victor Chibotaru, Martin Vechev\nabstract\rabstract: The automated program repair field has attracted substantial interest overthe years, but despite significant research efforts, creating a system thatworks well for complex semantic bugs such as security vulnerabilities hasproven difficult. A promising direction to solve this challenge is byleveraging large language models (LLMs), which are increasingly used to solvevarious programming tasks. In this paper, we investigate the effectiveness ofLLMs for solving code-repair task. We show that the task is difficult as itrequires the model to learn long-range code relationships, a task thatinherently relies on extensive amounts of training data. At the same time,creating a large, clean dataset for complex program bugs and theircorresponding fixes is non-trivial. We propose a technique to address thesechallenges with a new approach for querying and fine-tuning LLMs. The idea isto use program analysis to limit the LLM\u0026rsquo;s attention mechanism on the portionsof code needed to perform the fix, drastically reducing the amount of requiredtraining data. Concretely, for training and inference, rather than feeding theentire program to the LLM, we reduce its code to a much shorter snippet thatcontains the reported defect together with the necessary context - and use thatinstead. Our evaluation shows that this code reduction approach substantiallyimproves available models such as GPT-4 using few-shot learning, as well asfine-tuning models. To train and evaluate our system, we created acomprehensive code fixing dataset by extensively labeling 156 bug patterns(including 40 security rules), requiring complex interprocedural dataflow todiscover. Our best system with Mixtral-8x7B can remove more than 80% of thereported defects while exactly matching the human fix in between 10 and 50% ofcases, outperforming baselines based on GPT-3.5 and GPT-4, or based onwindow-based models like TFix.\r2024-02-22\nLLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward\nNafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami Karkevandi, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad\nabstract\rabstract: In software development, the predominant emphasis on functionality oftensupersedes security concerns, a trend gaining momentum with AI-drivenautomation tools like GitHub Copilot. These tools significantly improvedevelopers\u0026rsquo; efficiency in functional code development. Nevertheless, it remainsa notable concern that such tools are also responsible for creating insecurecode, predominantly because of pre-training on publicly available repositorieswith vulnerable code. Moreover, developers are called the \u0026ldquo;weakest link in thechain\u0026rdquo; since they have very minimal knowledge of code security. Althoughexisting solutions provide a reasonable solution to vulnerable code, they mustadequately describe and educate the developers on code security to ensure thatthe security issues are not repeated. Therefore we introduce a multipurposecode vulnerability analysis system \\texttt{SecRepair}, powered by a largelanguage model, CodeGen2 assisting the developer in identifying and generatingfixed code along with a complete description of the vulnerability with a codecomment. Our innovative methodology uses a reinforcement learning paradigm togenerate code comments augmented by a semantic reward mechanism. Inspired byhow humans fix code issues, we propose an instruction-based dataset suitablefor vulnerability analysis with LLMs. We further identify zero-day and N-dayvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findingsunderscore that incorporating reinforcement learning coupled with semanticreward augments our model\u0026rsquo;s performance, thereby fortifying its capacity toaddress code vulnerabilities with improved efficacy.\rHow Susceptible are Large Language Models to Ideological Manipulation?\nKai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman\nabstract\rabstract: Large Language Models (LLMs) possess the potential to exert substantialinfluence on public perceptions and interactions with information. This raisesconcerns about the societal impact that could arise if the ideologies withinthese models can be easily manipulated. In this work, we investigate howeffectively LLMs can learn and generalize ideological biases from theirinstruction-tuning data. Our findings reveal a concerning vulnerability:exposure to only a small amount of ideologically driven samples significantlyalters the ideology of LLMs. Notably, LLMs demonstrate a startling ability toabsorb ideology from one topic and generalize it to even unrelated ones. Theease with which LLMs\u0026rsquo; ideologies can be skewed underscores the risks associatedwith intentionally poisoned training data by malicious actors or inadvertentlyintroduced biases by data annotators. It also emphasizes the imperative forrobust safeguards to mitigate the influence of ideological manipulations onLLMs.\rLarge Language Models Are Not Robust Multiple Choice Selectors\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang\nabstract\rabstract: Multiple choice questions (MCQs) serve as a common yet important task formatin the evaluation of large language models (LLMs). This work shows that modernLLMs are vulnerable to option position changes in MCQs due to their inherent\u0026quot;selection bias\u0026quot;, namely, they prefer to select specific option IDs as answers(like \u0026ldquo;Option A\u0026rdquo;). Through extensive empirical analyses with 20 LLMs on threebenchmarks, we pinpoint that this behavioral bias primarily stems from LLMs\u0026rsquo;token bias, where the model a priori assigns more probabilistic mass tospecific option ID tokens (e.g., A/B/C/D) when predicting answers from theoption IDs. To mitigate selection bias, we propose a label-free, inference-timedebiasing method, called PriDe, which separates the model\u0026rsquo;s prior bias foroption IDs from the overall prediction distribution. PriDe first estimates theprior by permutating option contents on a small number of test samples, andthen applies the estimated prior to debias the remaining samples. Wedemonstrate that it achieves interpretable and transferable debiasing with highcomputational efficiency. We hope this work can draw broader research attentionto the bias and robustness of modern LLMs.\rArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs\nFengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran\nabstract\rabstract: Safety is critical to the usage of large language models (LLMs). Multipletechniques such as data filtering and supervised fine-tuning have beendeveloped to strengthen LLM safety. However, currently known techniques presumethat corpora used for safety alignment of LLMs are solely interpreted bysemantics. This assumption, however, does not hold in real-world applications,which leads to severe vulnerabilities in LLMs. For example, users of forumsoften use ASCII art, a form of text-based art, to convey image information. Inthis paper, we propose a novel ASCII art-based jailbreak attack and introduce acomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate thecapabilities of LLMs in recognizing prompts that cannot be solely interpretedby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, andLlama2) struggle to recognize prompts provided in the form of ASCII art. Basedon this observation, we develop the jailbreak attack ArtPrompt, which leveragesthe poor performance of LLMs in recognizing ASCII art to bypass safety measuresand elicit undesired behaviors from LLMs. ArtPrompt only requires black-boxaccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompton five SOTA LLMs, and show that ArtPrompt can effectively and efficientlyinduce undesired behaviors from all five LLMs.\r2024-02-21\nIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\nVyas Raina, Adian Liusie, Mark Gales\nabstract\rabstract: Large Language Models (LLMs) are powerful zero-shot assessors and areincreasingly used in real-world situations such as for written exams orbenchmarking systems. Despite this, no existing work has analyzed thevulnerability of judge-LLMs against adversaries attempting to manipulateoutputs. This work presents the first study on the adversarial robustness ofassessment LLMs, where we search for short universal phrases that when appendedto texts can deceive LLMs to provide high assessment scores. Experiments onSummEval and TopicalChat demonstrate that both LLM-scoring and pairwiseLLM-comparative assessment are vulnerable to simple concatenation attacks,where in particular LLM-scoring is very susceptible and can yield maximumassessment scores irrespective of the input text quality. Interestingly, suchattacks are transferable and phrases learned on smaller open-source LLMs can beapplied to larger closed-source models, such as GPT3.5. This highlights thepervasive nature of the adversarial vulnerabilities across different judge-LLMsizes, families and methods. Our findings raise significant concerns on thereliability of LLMs-as-a-judge methods, and underscore the importance ofaddressing vulnerabilities in LLM assessment methods before deployment inhigh-stakes real-world scenarios.\rLearning to Poison Large Language Models During Instruction Tuning\nYao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu\nabstract\rabstract: The advent of Large Language Models (LLMs) has marked significantachievements in language processing and reasoning capabilities. Despite theiradvancements, LLMs face vulnerabilities to data poisoning attacks, whereadversaries insert backdoor triggers into training data to manipulate outputsfor malicious purposes. This work further identifies additional security risksin LLMs by designing a new data poisoning attack tailored to exploit theinstruction tuning process. We propose a novel gradient-guided backdoor triggerlearning approach to identify adversarial triggers efficiently, ensuring anevasion of detection by conventional defenses while maintaining contentintegrity. Through experimental validation across various LLMs and tasks, ourstrategy demonstrates a high success rate in compromising model outputs;poisoning only 1% of 4,000 instruction tuning samples leads to a PerformanceDrop Rate (PDR) of around 80%. Our work highlights the need for strongerdefenses against data poisoning attack, offering insights into safeguardingLLMs against these more sophisticated attacks. The source code can be found onthis GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.\r2024-02-20\nHumans or LLMs as the Judge? A Study on Judgement Biases\nGuiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang\nabstract\rabstract: Adopting human and large language models (LLM) as judges (\\textit{a.k.a}human- and LLM-as-a-judge) for evaluating the performance of existing LLMs hasrecently gained attention. Nonetheless, this approach concurrently introducespotential biases from human and LLM judges, questioning the reliability of theevaluation results. In this paper, we propose a novel framework forinvestigating 5 types of biases for LLM and human judges. We curate a datasetwith 142 samples referring to the revised Bloom\u0026rsquo;s Taxonomy and conductthousands of human and LLM evaluations. Results show that human and LLM judgesare vulnerable to perturbations to various degrees, and that even the mostcutting-edge judges possess considerable biases. We further exploit theirweakness and conduct attacks on LLM judges. We hope that our work can notifythe community of the vulnerability of human- and LLM-as-a-judge againstperturbations, as well as the urgency of developing robust evaluation systems.\rBayesian Reward Models for LLM Alignment\nAdam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison\nabstract\rabstract: To ensure that large language model (LLM) responses are helpful andnon-toxic, we usually fine-tune a reward model on human preference data. Wethen select policy responses with high rewards (best-of-n sampling) or furtheroptimize the policy to produce responses with high rewards (reinforcementlearning from human feedback). However, this process is vulnerable to rewardoveroptimization or hacking, in which the responses selected have high rewardsdue to errors in the reward model rather than a genuine preference. This isespecially problematic as the prompt or response diverges from the trainingdata. It should be possible to mitigate these issues by training a Bayesianreward model, which signals higher uncertainty further from the training datadistribution. Therefore, we trained Bayesian reward models using Laplace-LoRA(Yang et al., 2024) and found that the resulting uncertainty estimates cansuccessfully mitigate reward overoptimization in best-of-n sampling.\rDefending Jailbreak Prompts via In-Context Adversarial Game\nYujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang\nabstract\rabstract: Large Language Models (LLMs) demonstrate remarkable capabilities acrossdiverse applications. However, concerns regarding their security, particularlythe vulnerability to jailbreak attacks, persist. Drawing inspiration fromadversarial training in deep learning and LLM agent learning processes, weintroduce the In-Context Adversarial Game (ICAG) for defending againstjailbreaks without the need for fine-tuning. ICAG leverages agent learning toconduct an adversarial game, aiming to dynamically extend knowledge to defendagainst jailbreaks. Unlike traditional methods that rely on static datasets,ICAG employs an iterative process to enhance both the defense and attackagents. This continuous improvement process strengthens defenses against newlygenerated jailbreak prompts. Our empirical studies affirm ICAG\u0026rsquo;s efficacy,where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak successrates across various attack scenarios. Moreover, ICAG demonstrates remarkabletransferability to other LLMs, indicating its potential as a versatile defensemechanism.\rHow Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts\nYusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan\nabstract\rabstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) havenot rendered them immune to challenges, particularly in the context of handlingdeceptive information in prompts, thus producing hallucinated responses undersuch conditions. To quantitatively assess this vulnerability, we presentMAD-Bench, a carefully curated benchmark that contains 850 test samples dividedinto 6 categories, such as non-existent objects, count of objects, spatialrelationship, and visual confusion. We provide a comprehensive analysis ofpopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such asLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gapsbetween GPT-4V and other models; and previous robust instruction-tuned models,such as LRV-Instruction and LLaVA-RLHF, are not effective on this newbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy ofany other model in our experiments ranges from 5% to 35%. We further propose aremedy that adds an additional paragraph to the deceptive prompts to encouragemodels to think twice before answering the question. Surprisingly, this simplemethod can even double the accuracy; however, the absolute numbers are stilltoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmarkto stimulate further research to enhance models\u0026rsquo; resilience against deceptiveprompts.\rThe Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative\nZhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu\nabstract\rabstract: Due to their unprecedented ability to process and respond to various types ofdata, Multimodal Large Language Models (MLLMs) are constantly defining the newboundary of Artificial General Intelligence (AGI). As these advanced generativemodels increasingly form collaborative networks for complex tasks, theintegrity and security of these systems are crucial. Our paper, The WolfWithin'', explores a novel vulnerability in MLLM societies - the indirectpropagation of malicious content. Unlike direct harmful output generation forMLLMs, our research demonstrates how a single MLLM agent can be subtlyinfluenced to generate prompts that, in turn, induce other MLLM agents in thesociety to output malicious content. This subtle, yet potent method of indirectinfluence marks a significant escalation in the security risks associated withMLLMs. Our findings reveal that, with minimal or even no access to MLLMs'parameters, an MLLM agent, when manipulated to produce specific prompts orinstructions, can effectively infect\u0026rsquo;\u0026rsquo; other agents within a society ofMLLMs. This infection leads to the generation and circulation of harmfuloutputs, such as dangerous instructions or misinformation, across the society.We also show the transferability of these indirectly generated prompts,highlighting their possibility in propagating malice through inter-agentcommunication. This research provides a critical insight into a new dimensionof threat posed by MLLMs, where a single agent can act as a catalyst forwidespread malevolent influence. Our work underscores the urgent need fordeveloping robust mechanisms to detect and mitigate such covert manipulationswithin MLLM societies, ensuring their safe and ethical utilization in societalapplications. Our implementation is released at\\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.\r2024-02-19\nCan LLMs Patch Security Issues?\nKamel Alrashedy, Abdullah Aljasser\nabstract\rabstract: Large Language Models (LLMs) have shown impressive proficiency in codegeneration. Nonetheless, similar to human developers, these models mightgenerate code that contains security vulnerabilities and flaws. Writing securecode remains a substantial challenge, as vulnerabilities often arise duringinteractions between programs and external systems or services, such asdatabases and operating systems. In this paper, we propose a novel approach,Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMsin receiving feedback from Bandit, which is a static code analysis tool, andthen the LLMs generate potential solutions to resolve security vulnerabilities.Each solution, along with the vulnerable code, is then sent back to the LLM forcode refinement. Our approach shows a significant improvement over the baselineand outperforms existing approaches. Furthermore, we introduce a new dataset,PythonSecurityEval, collected from real-world scenarios on Stack Overflow toevaluate the LLMs\u0026rsquo; ability to generate secure code. Code and data are availableat \\url{https://github.com/Kamel773/LLM-code-refine}\rZero shot VLMs for hate meme detection: Are we there yet?\nNaquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee\nabstract\rabstract: Multimedia content on social media is rapidly evolving, with memes gainingprominence as a distinctive form. Unfortunately, some malicious users exploitmemes to target individuals or vulnerable communities, making it imperative toidentify and address such instances of hateful memes. Extensive research hasbeen conducted to address this issue by developing hate meme detection models.However, a notable limitation of traditional machine/deep learning models isthe requirement for labeled datasets for accurate classification. Recently, theresearch community has witnessed the emergence of several visual languagemodels that have exhibited outstanding performance across various tasks. Inthis study, we aim to investigate the efficacy of these visual language modelsin handling intricate tasks such as hate meme detection. We use various promptsettings to focus on zero-shot classification of hateful/harmful memes. Throughour analysis, we observe that large VLMs are still vulnerable for zero-shothate meme detection.\rSPML: A DSL for Defending Language Models Against Prompt Attacks\nReshabh K Sharma, Vinayak Gupta, Dan Grossman\nabstract\rabstract: Large language models (LLMs) have profoundly transformed natural languageapplications, with a growing reliance on instruction-based definitions fordesigning chatbots. However, post-deployment the chatbot definitions are fixedand are vulnerable to attacks by malicious users, emphasizing the need toprevent unethical applications and financial losses. Existing studies exploreuser prompts\u0026rsquo; impact on LLM-based chatbots, yet practical methods to containattacks on application-specific chatbots remain unexplored. This paper presentsSystem Prompt Meta Language (SPML), a domain-specific language for refiningprompts and monitoring the inputs to the LLM-based chatbots. SPML activelychecks attack prompts, ensuring user inputs align with chatbot definitions toprevent malicious execution on the LLM backbone, optimizing costs. It alsostreamlines chatbot definition crafting with programming language capabilities,overcoming natural language design challenges. Additionally, we introduce agroundbreaking benchmark with 1.8k system prompts and 20k user inputs, offeringthe inaugural language and benchmark for chatbot definition evaluation.Experiments across datasets demonstrate SPML\u0026rsquo;s proficiency in understandingattacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our dataand codes are publicly available at: https://prompt-compiler.github.io/SPML/.\rCovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation\nJueon Eom, Seyeon Jeong, Taekyoung Kwon\nabstract\rabstract: Fuzzing is an effective bug-finding technique but it struggles with complexsystems like JavaScript engines that demand precise grammatical input.Recently, researchers have adopted language models for context-aware mutationin fuzzing to address this problem. However, existing techniques are limited inutilizing coverage guidance for fuzzing, which is rather performed in ablack-box manner. This paper presents a novel technique called CovRL(Coverage-guided Reinforcement Learning) that combines Large Language Models(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveragingthe Term Frequency-Inverse Document Frequency (TF-IDF) method to construct aweighted coverage map. This map is key in calculating the fuzzing reward, whichis then applied to the LLM-based mutator through reinforcement learning.CovRL-Fuzz, through this approach, enables the generation of test cases thatare more likely to discover new coverage areas, thus improving vulnerabilitydetection while minimizing syntax and semantic errors, all without needingextra post-processing. Our evaluation results indicate that CovRL-Fuzzoutperforms the state-of-the-art fuzzers in terms of code coverage andbug-finding capabilities: CovRL-Fuzz identified 48 real-world security-relatedbugs in the latest JavaScript engines, including 39 previously unknownvulnerabilities and 11 CVEs.\rRobust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models\nChristian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein\nabstract\rabstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 areincreasingly used for various real-world tasks. Prior work has shown that thesemodels are highly vulnerable to adversarial attacks on the vision modality.These attacks can be leveraged to spread fake information or defraud users, andthus pose a significant risk, which makes the robustness of large multi-modalfoundation models a pressing problem. The CLIP model, or one of its variants,is used as a frozen vision encoder in many vision-language models (VLMs), e.g.LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuningscheme to obtain a robust CLIP vision encoder, which yields robustness on allvision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. Inparticular, we show that stealth-attacks on users of VLMs by a malicious thirdparty providing manipulated images are no longer possible once one replaces theoriginal CLIP model with our robust one. No retraining or fine-tuning of theVLM is required. The code and robust models are available athttps://github.com/chs20/RobustVLM\r2024-02-18\nStealthy Attack on Large Language Model based Recommendation\nJinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang\nabstract\rabstract: Recently, the powerful large language models (LLMs) have been instrumental inpropelling the progress of recommender systems (RS). However, while thesesystems have flourished, their susceptibility to security threats has beenlargely overlooked. In this work, we reveal that the introduction of LLMs intorecommendation models presents new security vulnerabilities due to theiremphasis on the textual content of items. We demonstrate that attackers cansignificantly boost an item\u0026rsquo;s exposure by merely altering its textual contentduring the testing phase, without requiring direct interference with themodel\u0026rsquo;s training process. Additionally, the attack is notably stealthy, as itdoes not affect the overall recommendation performance and the modifications tothe text are subtle, making it difficult for users and platforms to detect. Ourcomprehensive experiments across four mainstream LLM-based recommendationmodels demonstrate the superior efficacy and stealthiness of our approach. Ourwork unveils a significant security gap in LLM-based recommendation systems andpaves the way for future research on protecting these systems.\rToken-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information\nZhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Viswanathan Swaminathan\nabstract\rabstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools invarious applications. However, these models are susceptible to adversarialprompt attacks, where attackers can carefully curate input strings that misleadLLMs into generating incorrect or undesired outputs. Previous work has revealedthat with relatively simple yet effective attacks based on discreteoptimization, it is possible to generate adversarial prompts that bypassmoderation and alignment of the models. This vulnerability to adversarialprompts underscores a significant concern regarding the robustness andreliability of LLMs. Our work aims to address this concern by introducing anovel approach to detecting adversarial prompts at a token level, leveragingthe LLM\u0026rsquo;s capability to predict the next token\u0026rsquo;s probability. We measure thedegree of the model\u0026rsquo;s perplexity, where tokens predicted with high probabilityare considered normal, and those exhibiting high perplexity are flagged asadversarial. Additionaly, our method also integrates context understanding byincorporating neighboring token information to encourage the detection ofcontiguous adversarial prompt sequences. To this end, we design two algorithmsfor adversarial prompt detection: one based on optimization techniques andanother on Probabilistic Graphical Models (PGM). Both methods are equipped withefficient solving methods, ensuring efficient adversarial prompt detection. Ourtoken-level detection result can be visualized as heatmap overlays on the textsequence, allowing for a clearer and more intuitive representation of whichpart of the text may contain adversarial prompts.\r2024-02-16\nText Embedding Inversion Security for Multilingual Language Models\nYiyi Chen, Heather Lent, Johannes Bjerva\nabstract\rabstract: Textual data is often represented as realnumbered embeddings in NLP,particularly with the popularity of large language models (LLMs) and Embeddingsas a Service (EaaS). However, storing sensitive information as embeddings canbe vulnerable to security breaches, as research shows that text can bereconstructed from embeddings, even without knowledge of the underlying model.While defence mechanisms have been explored, these are exclusively focused onEnglish, leaving other languages vulnerable to attacks. This work explores LLMsecurity through multilingual embedding inversion. We define the problem ofblack-box multilingual and cross-lingual inversion attacks, and thoroughlyexplore their potential implications. Our findings suggest that multilingualLLMs may be more vulnerable to inversion attacks, in part because English baseddefences may be ineffective. To alleviate this, we propose a simple maskingdefense effective for both monolingual and multilingual models. This study isthe first to investigate multilingual inversion attacks, shedding light on thedifferences in attacks and defenses across monolingual and multilingualsettings.\rUniversal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning\nShuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen\nabstract\rabstract: In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Despite being widely applied, in-context learning isvulnerable to malicious attacks. In this work, we raise security concernsregarding this paradigm. Our studies demonstrate that an attacker canmanipulate the behavior of large language models by poisoning the demonstrationcontext, without the need for fine-tuning the model. Specifically, we design anew backdoor attack method, named ICLAttack, to target large language modelsbased on in-context learning. Our method encompasses two types of attacks:poisoning demonstration examples and poisoning demonstration prompts, which canmake models behave in alignment with predefined intentions. ICLAttack does notrequire additional fine-tuning to implant a backdoor, thus preserving themodel\u0026rsquo;s generality. Furthermore, the poisoned examples are correctly labeled,enhancing the natural stealth of our attack method. Extensive experimentalresults across several language models, ranging in size from 1.3B to 180Bparameters, demonstrate the effectiveness of our attack method, exemplified bya high average attack success rate of 95.0% across the three datasets on OPTmodels.\rLLM Agents can Autonomously Hack Websites\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang\nabstract\rabstract: In recent years, large language models (LLMs) have become increasinglycapable and can now interact with tools (i.e., call functions), read documents,and recursively call themselves. As a result, these LLMs can now functionautonomously as agents. With the rise in capabilities of these agents, recentwork has speculated on how LLM agents would affect cybersecurity. However, notmuch is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites,performing tasks as complex as blind database schema extraction and SQLinjections without human feedback. Importantly, the agent does not need to knowthe vulnerability beforehand. This capability is uniquely enabled by frontiermodels that are highly capable of tool use and leveraging extended context.Namely, we show that GPT-4 is capable of such hacks, but existing open-sourcemodels are not. Finally, we show that GPT-4 is capable of autonomously findingvulnerabilities in websites in the wild. Our findings raise questions about thewidespread deployment of LLMs.\rZero-shot sampling of adversarial entities in biomedical question answering\nR. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl\nabstract\rabstract: The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.\r2024-02-15\nRapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization\nRui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang\nabstract\rabstract: The increasing demand for customized Large Language Models (LLMs) has led tothe development of solutions like GPTs. These solutions facilitate tailored LLMcreation via natural language prompts without coding. However, thetrustworthiness of third-party custom versions of LLMs remains an essentialconcern. In this paper, we propose the first instruction backdoor attacksagainst applications integrated with untrusted customized LLMs (e.g., GPTs).Specifically, these attacks embed the backdoor into the custom version of LLMsby designing prompts with backdoor instructions, outputting the attacker\u0026rsquo;sdesired result when inputs contain the pre-defined triggers. Our attackincludes 3 levels of attacks: word-level, syntax-level, and semantic-level,which adopt different types of triggers with progressive stealthiness. Westress that our attacks do not require fine-tuning or any modification to thebackend LLMs, adhering strictly to GPTs development guidelines. We conductextensive experiments on 4 prominent LLMs and 5 benchmark text classificationdatasets. The results show that our instruction backdoor attacks achieve thedesired attack performance without compromising utility. Additionally, wepropose an instruction-ignoring defense mechanism and demonstrate its partialeffectiveness in mitigating such attacks. Our findings highlight thevulnerability and the potential risks of LLM customization such as GPTs.\rCodeAgent: Collaborative Agents for Software Engineering\nDaniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song, Haoye Tian, Saad Ezzini, Yongfeng Huang, Jacques Klein, Tegawende F. Bissyande\nabstract\rabstract: Code review is a heavily collaborative process, which aims at ensuring theoverall quality and reliability of software. While it provides massivebenefits, the implementation of code review in an organization faces severalchallenges that make its automation appealing. Automated code review tools havebeen around for a while and are now improving thanks to the adoption of novelAI models, which help can learn about standard practices and systematicallycheck that the reviewed code adheres to them. Unfortunately, existing methodsfall short: they often target a single input-output generative model, whichcannot simulate the collaboration interactions in code review to account forvarious perspectives; they are also sub-performing on various critical codereview sub-tasks. In this paper, we advance the state of the art in code reviewautomation by introducing CodeAgent, a novel multi-agent-based system for codereview. Fundamentally, CodeAgent is steered by QA-Checker (short for\u0026quot;Question-Answer Checking\u0026quot;), a supervision agent, designed specifically toensure that all agents\u0026rsquo; contributions remain relevant to the initial reviewquestion. CodeAgent is autonomous, multi-agent, and Large languagemodel-driven. To demonstrate the effectiveness of CodeAgent, we performedexperiments to assess its capabilities in various tasks including 1) detectionof inconsistencies between code changes and commit messages, 2) detection ofvulnerability introduction by commits, and 3) validation of adherence to codestyle. Our website is accessed in\\url{https://code-agent-new.vercel.app/index.html}.\rPAL: Proxy-Guided Black-Box Attack on Large Language Models\nChawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo\nabstract\rabstract: Large Language Models (LLMs) have surged in popularity in recent months, butthey have demonstrated concerning capabilities to generate harmful content whenmanipulated. While techniques like safety fine-tuning aim to minimize harmfuluse, recent works have shown that LLMs remain vulnerable to attacks that elicittoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs(PAL), the first optimization-based attack on LLMs in a black-box query-onlysetting. In particular, it relies on a surrogate model to guide theoptimization and a sophisticated loss designed for real-world LLM APIs. Ourattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% onLlama-2-7B, compared to 4% for the current state of the art. We also proposeGCG++, an improvement to the GCG attack that reaches 94% ASR on white-boxLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simplebaseline for query-based attacks. We believe the techniques proposed in thiswork will enable more comprehensive safety testing of LLMs and, in the longterm, the development of better security guardrails. The code can be found athttps://github.com/chawins/pal.\r2024-02-14\nHow Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?\nCongcong Wen, Jiazhao Liang, Shuaihang Yuan, Hao Huang, Yi Fang\nabstract\rabstract: In the field of robotics and automation, navigation systems based on LargeLanguage Models (LLMs) have recently shown impressive performance. However, thesecurity aspects of these systems have received relatively less attention. Thispaper pioneers the exploration of vulnerabilities in LLM-based navigationmodels in urban outdoor environments, a critical area given the technology\u0026rsquo;swidespread application in autonomous driving, logistics, and emergencyservices. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)Attack that manipulates LLM-based navigation models by appendinggradient-derived suffixes to the original navigational prompt, leading toincorrect actions. We conducted comprehensive experiments on an LLMs-basednavigation model that employs various LLMs for reasoning. Our results, derivedfrom the Touchdown and Map2Seq street-view datasets under both few-shotlearning and fine-tuning configurations, demonstrate notable performancedeclines across three metrics in the face of both white-box and black-boxattacks. These results highlight the generalizability and transferability ofthe NPS Attack, emphasizing the need for enhanced security in LLM-basednavigation systems. As an initial countermeasure, we propose the NavigationalPrompt Engineering (NPE) Defense strategy, concentrating on navigation-relevantkeywords to reduce the impact of adversarial suffixes. While initial findingsindicate that this strategy enhances navigational safety, there remains acritical need for the wider research community to develop stronger defensemethods to effectively tackle the real-world challenges faced by these systems.\r2024-02-13\nLLbezpeky: Leveraging Large Language Models for Vulnerability Detection\nNoble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Meiyappan Nagappan, Shane McIntosh\nabstract\rabstract: Despite the continued research and progress in building secure systems,Android applications continue to be ridden with vulnerabilities, necessitatingeffective detection methods. Current strategies involving static and dynamicanalysis tools come with limitations like overwhelming number of falsepositives and limited scope of analysis which make either difficult to adopt.Over the past years, machine learning based approaches have been extensivelyexplored for vulnerability detection, but its real-world applicability isconstrained by data requirements and feature engineering challenges. LargeLanguage Models (LLMs), with their vast parameters, have shown tremendouspotential in understanding semnatics in human as well as programming languages.We dive into the efficacy of LLMs for detecting vulnerabilities in the contextof Android security. We focus on building an AI-driven workflow to assistdevelopers in identifying and rectifying vulnerabilities. Our experiments showthat LLMs outperform our expectations in finding issues within applicationscorrectly flagging insecure apps in 91.67% of cases in the Ghera benchmark. Weuse inferences from our experiments towards building a robust and actionablevulnerability detection system and demonstrate its effectiveness. Ourexperiments also shed light on how different various simple configurations canaffect the True Positive (TP) and False Positive (FP) rates.\rPandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning\nGelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu\nabstract\rabstract: Large Language Models~(LLMs) have gained immense popularity and are beingincreasingly applied in various domains. Consequently, ensuring the security ofthese models is of paramount importance. Jailbreak attacks, which manipulateLLMs to generate malicious content, are recognized as a significantvulnerability. While existing research has predominantly focused on directjailbreak attacks on LLMs, there has been limited exploration of indirectmethods. The integration of various plugins into LLMs, notably RetrievalAugmented Generation~(RAG), which enables LLMs to incorporate externalknowledge bases into their response generation such as GPTs, introduces newavenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs,particularly GPTs, introducing a novel attack vector named Retrieval AugmentedGeneration Poisoning. This method, Pandora, exploits the synergy between LLMsand RAG through prompt manipulation to generate unexpected responses. Pandorauses maliciously crafted content to influence the RAG process, effectivelyinitiating jailbreak attacks. Our preliminary tests show that Pandorasuccessfully conducts jailbreak attacks in four different scenarios, achievinghigher success rates than direct attacks, with 64.3% for GPT-3.5 and 34.8%for GPT-4.\rLMs: Understanding Code Syntax and Semantics for Code Analysis\nWei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu\nabstract\rabstract: Large language models~(LLMs) demonstrate significant potential torevolutionize software engineering (SE) by exhibiting outstanding performancein SE tasks such as code and document generation. However, the high reliabilityand risk control requirements in software engineering raise concerns about thelack of interpretability of LLMs. To address this concern, we conducted a studyto evaluate the capabilities of LLMs and their limitations for code analysis inSE. We break down the abilities needed for artificial intelligence~(AI) modelsto address SE tasks related to code analysis into three categories: 1) syntaxunderstanding, 2) static behavior understanding, and 3) dynamic behaviorunderstanding. Our investigation focused on the ability of LLMs to comprehendcode syntax and semantic structures, which include abstract syntax trees (AST),control flow graphs (CFG), and call graphs (CG). We employed fourstate-of-the-art foundational models, GPT4, GPT3.5, StarCoder andCodeLlama-13b-instruct. We assessed the performance of LLMs on cross-languagetasks involving C, Java, Python, and Solidity. Our findings revealed that while LLMs have a talent for understanding codesyntax, they struggle with comprehending code semantics, particularly dynamicsemantics. We conclude that LLMs possess capabilities similar to an AbstractSyntax Tree (AST) parser, demonstrating initial competencies in static codeanalysis. Furthermore, our study highlights that LLMs are susceptible tohallucinations when interpreting code semantic structures and fabricatingnonexistent facts. These results indicate the need to explore methods to verifythe correctness of LLM output to ensure its dependability in SE. Moreimportantly, our study provides an initial answer to why the codes generated byLLM are usually syntax-correct but vulnerable.\rCABINET: Content Relevance based Noise Reduction for Table Question Answering\nSohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, Balaji Krishnamurthy\nabstract\rabstract: Table understanding capability of Large Language Models (LLMs) has beenextensively studied through the task of question-answering (QA) over tables.Typically, only a small part of the whole table is relevant to derive theanswer for a given question. The irrelevant parts act as noise and aredistracting information, resulting in sub-optimal performance due to thevulnerability of LLMs to noise. To mitigate this, we propose CABINET (ContentRelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework toenable LLMs to focus on relevant tabular data by suppressing extraneousinformation. CABINET comprises an Unsupervised Relevance Scorer (URS), traineddifferentially with the QA LLM, that weighs the table content based on itsrelevance to the input question before feeding it to the question-answering LLM(QA LLM). To further aid the relevance scorer, CABINET employs a weaklysupervised module that generates a parsing statement describing the criteria ofrows and columns relevant to the question and highlights the content ofcorresponding table cells. CABINET significantly outperforms various tabularLLM baselines, as well as GPT3-based in-context learning methods, is morerobust to noise, maintains outperformance on tables of varying sizes, andestablishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. Werelease our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.\r2024-02-12\nEvading Data Contamination Detection for Language Models is (too) Easy\nJasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev\nabstract\rabstract: Large language models are widespread, with their performance on benchmarksfrequently guiding user preferences for one model over another. However, thevast amount of data these models are trained on can inadvertently lead tocontamination with public benchmarks, thus compromising performancemeasurements. While recently developed contamination detection methods try toaddress this issue, they overlook the possibility of deliberate contaminationby malicious model providers aiming to evade detection. We argue that thissetting is of crucial importance as it casts doubt on the reliability of publicbenchmarks. To more rigorously study this issue, we propose a categorization ofboth model providers and contamination detection methods. This revealsvulnerabilities in existing methods that we exploit with EAL, a simple yeteffective contamination technique that significantly inflates benchmarkperformance while completely evading current detection methods.\rDo Membership Inference Attacks Work on Large Language Models?\nMichael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi\nabstract\rabstract: Membership inference attacks (MIAs) attempt to predict whether a particulardatapoint is a member of a target model\u0026rsquo;s training data. Despite extensiveresearch on traditional machine learning models, there has been limited workstudying MIA on the pre-training data of large language models (LLMs). Weperform a large-scale evaluation of MIAs over a suite of language models (LMs)trained on the Pile, ranging from 160M to 12B parameters. We find that MIAsbarely outperform random guessing for most settings across varying LLM sizesand domains. Our further analyses reveal that this poor performance can beattributed to (1) the combination of a large dataset and few trainingiterations, and (2) an inherently fuzzy boundary between members andnon-members. We identify specific settings where LLMs have been shown to bevulnerable to membership inference and show that the apparent success in suchsettings can be attributed to a distribution shift, such as when members andnon-members are drawn from the seemingly identical domain but with differenttemporal ranges. We release our code and data as a unified benchmark packagethat includes all existing MIAs, supporting future work.\rZero-Shot Robustification of Zero-Shot Models\nDyah Adila, Changho Shin, Linrong Cai, Frederic Sala\nabstract\rabstract: Zero-shot inference is a powerful paradigm that enables the use of largepretrained models for downstream classification tasks without further training.However, these models are vulnerable to inherited biases that can impact theirperformance. The traditional solution is fine-tuning, but this undermines thekey advantage of pretrained models, which is their ability to be usedout-of-the-box. We propose RoboShot, a method that improves the robustness ofpretrained model embeddings in a fully zero-shot fashion. First, we uselanguage models (LMs) to obtain useful insights from task descriptions. Theseinsights are embedded and used to remove harmful and boost useful components inembeddings \u0026ndash; without any supervision. Theoretically, we provide a simple andtractable model for biases in zero-shot embeddings and give a resultcharacterizing under what conditions our approach can boost performance.Empirically, we evaluate RoboShot on nine image and NLP classification tasksand show an average improvement of 15.98% on worst group accuracy, with trivialdecrease in overall accuracy over several zero-shot baselines. Additionally, wedemonstrate that RoboShot is compatible with a variety of pretrained andlanguage models and propose a way to further boost performance with a zero-shotadaptation variant.\rResilient Watermarking for LLM-Generated Codes\nBoquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun, Xingmei Wang\nabstract\rabstract: With the development of large language models, multiple AIs are now madeavailable for code generation (such as ChatGPT and StarCoder) and are adoptedwidely. It is often desirable to know whether a piece of code is generated byAI, and furthermore, which AI is the author. For instance, if a certain versionof AI is known to generate vulnerable code, it is particularly important toknow the creator. Existing approaches are not satisfactory as watermarkingcodes are challenging compared with watermarking text data, as codes can bealtered with relative ease via widely-used code refactoring methods. In thiswork, we propose ACW (AI Code Watermarking), a novel method for watermarkingAI-generated codes. ACW is efficient as it requires no training or fine-tuningand works in a black-box manner. It is resilient as the watermark cannot beeasily removed or tampered through common code refactoring methods. The keyidea of ACW is to selectively apply a set of carefully-designedsemantic-preserving, idempotent code transformations, whose presence (orabsence) allows us to determine the existence of the watermark. Ourexperimental results show that ACW is effective (i.e., achieving high accuracy,true positive rates and false positive rates), resilient and efficient,significantly outperforming existing approaches.\rCertifying LLM Safety against Adversarial Prompting\nAounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju\nabstract\rabstract: Large language models (LLMs) are vulnerable to adversarial attacks that addmalicious tokens to an input prompt to bypass the safety guardrails of an LLMand cause it to produce harmful content. In this work, we introduceerase-and-check, the first framework for defending against adversarial promptswith certifiable safety guarantees. Given a prompt, our procedure erases tokensindividually and inspects the resulting subsequences using a safety filter. Oursafety certificate guarantees that harmful prompts are not mislabeled as safedue to an adversarial attack up to a certain size. We implement the safetyfilter in two ways, using Llama 2 and DistilBERT, and compare the performanceof erase-and-check for the two cases. We defend against three attack modes: i)adversarial suffix, where an adversarial sequence is appended at the end of aharmful prompt; ii) adversarial insertion, where the adversarial sequence isinserted anywhere in the middle of the prompt; and iii) adversarial infusion,where adversarial tokens are inserted at arbitrary positions in the prompt, notnecessarily as a contiguous block. Our experimental results demonstrate thatthis procedure can obtain strong certified safety guarantees on harmful promptswhile maintaining good empirical performance on safe prompts. Additionally, wepropose three efficient empirical defenses: i) RandEC, a randomized subsamplingversion of erase-and-check; ii) GreedyEC, which greedily erases tokens thatmaximize the softmax score of the harmful class; and iii) GradEC, which usesgradient information to optimize tokens to erase. We demonstrate theireffectiveness against adversarial prompts generated by the Greedy CoordinateGradient (GCG) attack algorithm. The code for our experiments is available athttps://github.com/aounon/certified-llm-safety.\r2024-02-10\nWhispers in the Machine: Confidentiality in LLM-integrated Systems\nJonathan Evertz, Merlin Chlosta, Lea Schönherr, Thorsten Eisenhofer\nabstract\rabstract: Large Language Models (LLMs) are increasingly integrated with external tools.While these integrations can significantly improve the functionality of LLMs,they also create a new attack surface where confidential data may be disclosedbetween different components. Specifically, malicious tools can exploitvulnerabilities in the LLM itself to manipulate the model and compromise thedata of other services, raising the question of how private data can beprotected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality inLLM-integrated systems. For this, we formalize a \u0026ldquo;secret key\u0026rdquo; game that cancapture the ability of a model to conceal private information. This enables usto compare the vulnerability of a model against confidentiality attacks andalso the effectiveness of different defense strategies. In this framework, weevaluate eight previously published attacks and four defenses. We find thatcurrent defenses lack generalization across attack strategies. Building on thisanalysis, we propose a method for robustness fine-tuning, inspired byadversarial training. This approach is effective in lowering the success rateof attackers and in improving the system\u0026rsquo;s resilience against unknown attacks.\r2024-02-09\nVulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks\nDomenico Cotroneo, Cristina Improta, Pietro Liguori, Roberto Natella\nabstract\rabstract: AI-based code generators have become pivotal in assisting developers inwriting software starting from natural language (NL). However, they are trainedon large amounts of data, often collected from unsanitized online sources(e.g., GitHub, HuggingFace). As a consequence, AI models become an easy targetfor data poisoning, i.e., an attack that injects malicious samples into thetraining data to generate vulnerable code. To address this threat, this work investigates the security of AI codegenerators by devising a targeted data poisoning strategy. We poison thetraining data by injecting increasing amounts of code containing securityvulnerabilities and assess the attack\u0026rsquo;s success on different state-of-the-artmodels for code generation. Our study shows that AI code generators arevulnerable to even a small amount of poison. Notably, the attack successstrongly depends on the model architecture and poisoning rate, whereas it isnot influenced by the type of vulnerabilities. Moreover, since the attack doesnot impact the correctness of code generated by pre-trained models, it is hardto detect. Lastly, our work offers practical insights into understanding andpotentially mitigating this threat.\rOn the Out-Of-Distribution Generalization of Multimodal Large Language Models\nXingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui\nabstract\rabstract: We investigate the generalization boundaries of current Multimodal LargeLanguage Models (MLLMs) via comprehensive evaluation under out-of-distributionscenarios and domain-specific tasks. We evaluate their zero-shot generalizationacross synthetic images, real-world distributional shifts, and specializeddatasets like medical and molecular imagery. Empirical results indicate thatMLLMs struggle with generalization beyond common training domains, limitingtheir direct application without adaptation. To understand the cause ofunreliable performance, we analyze three hypotheses: semanticmisinterpretation, visual feature extraction insufficiency, and mappingdeficiency. Results identify mapping deficiency as the primary hurdle. Toaddress this problem, we show that in-context learning (ICL) can significantlyenhance MLLMs\u0026rsquo; generalization, opening new avenues for overcominggeneralization barriers. We further explore the robustness of ICL underdistribution shifts and show its vulnerability to domain shifts, label shifts,and spurious correlation shifts between in-context examples and test data.\r2024-02-08\nReposVul: A Repository-Level High-Quality Vulnerability Dataset\nXinchen Wang, Ruida Hu, Cuiyun Gao, Xin-Cheng Wen, Yujia Chen, Qing Liao\nabstract\rabstract: Open-Source Software (OSS) vulnerabilities bring great challenges to thesoftware security and pose potential risks to our society. Enormous effortshave been devoted into automated vulnerability detection, among which deeplearning (DL)-based approaches have proven to be the most effective. However,the current labeled data present the following limitations: (1) TangledPatches: Developers may submit code changes unrelated to vulnerability fixeswithin patches, leading to tangled patches. (2) Lacking Inter-proceduralVulnerabilities: The existing vulnerability datasets typically containfunction-level and file-level vulnerabilities, ignoring the relations betweenfunctions, thus rendering the approaches unable to detect the inter-proceduralvulnerabilities. (3) Outdated Patches: The existing datasets usually containoutdated patches, which may bias the model during training. To address the above limitations, in this paper, we propose an automated datacollection framework and construct the first repository-level high-qualityvulnerability dataset named ReposVul. The proposed framework mainly containsthree modules: (1) A vulnerability untangling module, aiming at distinguishingvulnerability-fixing related code changes from tangled patches, in which theLarge Language Models (LLMs) and static analysis tools are jointly employed.(2) A multi-granularity dependency extraction module, aiming at capturing theinter-procedural call relationships of vulnerabilities, in which we constructmultiple-granularity information for each vulnerability patch, includingrepository-level, file-level, function-level, and line-level. (3) A trace-basedfiltering module, aiming at filtering the outdated patches, which leverages thefile path trace-based filter and commit time trace-based filter to construct anup-to-date dataset.\rIn-Context Learning Can Re-learn Forbidden Tasks\nSophie Xhonneux, David Dobre, Jian Tang, Gauthier Gidel, Dhanya Sridhar\nabstract\rabstract: Despite significant investment into safety training, large language models(LLMs) deployed in the real world still suffer from numerous vulnerabilities.One perspective on LLM safety training is that it algorithmically forbids themodel from answering toxic or harmful queries. To assess the effectiveness ofsafety training, in this work, we study forbidden tasks, i.e., tasks the modelis designed to refuse to answer. Specifically, we investigate whetherin-context learning (ICL) can be used to re-learn forbidden tasks despite theexplicit fine-tuning of the model to refuse them. We first examine a toyexample of refusing sentiment classification to demonstrate the problem. Then,we use ICL on a model fine-tuned to refuse to summarise made-up news articles.Finally, we investigate whether ICL can undo safety training, which couldrepresent a major security risk. For the safety task, we look at Vicuna-7B,Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box onStarling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICLattack that uses the chat template tokens like a prompt injection attack toachieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains LLM-generated text with violence,suicide, and misinformation.\rComprehensive Assessment of Jailbreak Attacks Against LLMs\nJunjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang\nabstract\rabstract: Misuse of the Large Language Models (LLMs) has raised widespread concern. Toaddress this issue, safeguards have been taken to ensure that LLMs align withsocial ethics. However, recent findings have revealed an unsettlingvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. Byapplying techniques, such as employing role-playing scenarios, adversarialexamples, or subtle subversion of safety objectives as a prompt, LLMs canproduce an inappropriate or even harmful response. While researchers havestudied several categories of jailbreak attacks, they have done so inisolation. To fill this gap, we present the first large-scale measurement ofvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreakmethods from four categories, 160 questions from 16 violation categories, andsix popular LLMs. Our extensive experimental results demonstrate that theoptimized jailbreak prompts consistently achieve the highest attack successrates, as well as exhibit robustness across different LLMs. Some jailbreakprompt datasets, available from the Internet, can also achieve high attacksuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite theclaims from many organizations regarding the coverage of violation categoriesin their policies, the attack success rates from these categories remain high,indicating the challenges of effectively aligning LLM policies and the abilityto counter jailbreak attacks. We also discuss the trade-off between the attackperformance and efficiency, as well as show that the transferability of thejailbreak prompts is still viable, becoming an option for black-box models.Overall, our research highlights the necessity of evaluating differentjailbreak methods. We hope our study can provide insights for future researchon jailbreak attacks and serve as a benchmark tool for evaluating them forpractitioners.\r2024-02-07\nPrioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science\nXiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein\nabstract\rabstract: Intelligent agents powered by large language models (LLMs) have demonstratedsubstantial promise in autonomously conducting experiments and facilitatingscientific discoveries across various disciplines. While their capabilities arepromising, they also introduce novel vulnerabilities that demand carefulconsideration for safety. However, there exists a notable gap in theliterature, as there has been no comprehensive exploration of thesevulnerabilities. This position paper fills this gap by conducting a thoroughexamination of vulnerabilities in LLM-based agents within scientific domains,shedding light on potential risks associated with their misuse and emphasizingthe need for safety measures. We begin by providing a comprehensive overview ofthe potential risks inherent to scientific LLM agents, taking into account userintent, the specific scientific domain, and their potential impact on theexternal environment. Then, we delve into the origins of these vulnerabilitiesand provide a scoping review of the limited existing works. Based on ouranalysis, we propose a triadic framework involving human regulation, agentalignment, and an understanding of environmental feedback (agent regulation) tomitigate these identified risks. Furthermore, we highlight the limitations andchallenges associated with safeguarding scientific agents and advocate for thedevelopment of improved models, robust benchmarks, and comprehensiveregulations to address these issues effectively.\rLarge Language Models Based Fuzzing Techniques: A Survey\nLinghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma\nabstract\rabstract: In the modern era where software plays a pivotal role, software security andvulnerability analysis have become essential for software development. Fuzzingtest, as an efficient software testing method, are widely used in variousdomains. Moreover, the rapid development of Large Language Models (LLMs) hasfacilitated their application in the field of software testing, demonstratingremarkable performance. Considering that existing fuzzing test techniques arenot entirely automated and software vulnerabilities continue to evolve, thereis a growing trend towards employing fuzzing test generated based on largelanguage models. This survey provides a systematic overview of the approachesthat fuse LLMs and fuzzing tests for software testing. In this paper, astatistical analysis and discussion of the literature in three areas, namelyLLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted bysummarising the state-of-the-art methods up until 2024. Our survey alsoinvestigates the potential for widespread deployment and application of fuzzingtest techniques generated by LLMs in the future.\rAssessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications\nBoyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson\nabstract\rabstract: Large language models (LLMs) show inherent brittleness in their safetymechanisms, as evidenced by their susceptibility to jailbreaking and evennon-malicious fine-tuning. This study explores this brittleness of safetyalignment by leveraging pruning and low-rank modifications. We develop methodsto identify critical regions that are vital for safety guardrails, and that aredisentangled from utility-relevant regions at both the neuron and rank levels.Surprisingly, the isolated regions we find are sparse, comprising about $3%$at the parameter level and $2.5%$ at the rank level. Removing these regionscompromises safety without significantly impacting utility, corroborating theinherent brittleness of the model\u0026rsquo;s safety mechanisms. Moreover, we show thatLLMs remain vulnerable to low-cost fine-tuning attacks even when modificationsto the safety-critical regions are restricted. These findings underscore theurgent need for more robust safety strategies in LLMs.\rHuman-Readable Fingerprint for Large Language Models\nBoyi Zeng, Chenghu Zhou, Xinbing Wang, Zhouhan Lin\nabstract\rabstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters\u0026rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM\u0026rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog\u0026rsquo;s appearancestrongly indicates the LLM\u0026rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.\r2024-02-06\nPartially Recentralization Softmax Loss for Vision-Language Models Robustness\nHao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li\nabstract\rabstract: As Large Language Models make a breakthrough in natural language processingtasks (NLP), multimodal technique becomes extremely popular. However, it hasbeen shown that multimodal NLP are vulnerable to adversarial attacks, where theoutputs of a model can be dramatically changed by a perturbation to the input.While several defense techniques have been proposed both in computer vision andNLP models, the multimodal robustness of models have not been fully explored.In this paper, we study the adversarial robustness provided by modifying lossfunction of pre-trained multimodal models, by restricting top K softmaxoutputs. Based on the evaluation and scoring, our experiments show that after afine-tuning, adversarial robustness of pre-trained models can be significantlyimproved, against popular attacks. Further research should be studying, such asoutput diversity, generalization and the robustness-performance trade-off ofthis kind of loss functions. Our code will be available after this paper isaccepted\rDeepInception: Hypnotize Large Language Model to Be Jailbreaker\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han\nabstract\rabstract: Despite remarkable success in various applications, large language models(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrailsvoid. However, previous studies for jailbreaks usually resort to brute-forceoptimization or extrapolations of a high computation cost, which might not bepractical or effective. In this paper, inspired by the Milgram experimentw.r.t. the authority power for inciting harmfulness, we disclose a lightweightmethod, termed DeepInception, which can easily hypnotize LLM to be ajailbreaker. Specifically, DeepInception leverages the personification abilityof LLM to construct a novel nested scene to behave, which realizes an adaptiveway to escape the usage control in a normal scenario. Empirically, ourDeepInception can achieve competitive jailbreak success rates with previouscounterparts and realize a continuous jailbreak in subsequent interactions,which reveals the critical weakness of self-losing on both open andclosed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, and GPT-3.5-turbo/4. Ourinvestigation appeals to people to pay more attention to the safety aspects ofLLMs and develop a stronger defense against their misuse risks. The code ispublicly available at: https://github.com/tmlr-group/DeepInception.\r2024-02-05\nRobust Prompt Optimization for Large Language Models Against Distribution Shifts\nMoxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, Tat-Seng Chua\nabstract\rabstract: Large Language Model (LLM) has demonstrated significant ability in variousNatural Language Processing tasks. However, their effectiveness is highlydependent on the phrasing of the task prompt, leading to research on automaticprompt optimization using labeled task data. We reveal that these promptoptimization techniques are vulnerable to distribution shifts such assubpopulation shifts, which are common for LLMs in real-world scenarios such ascustomer reviews analysis. In this light, we propose a new problem of robustprompt optimization for LLMs against distribution shifts, which requires theprompt optimized over the labeled source group can simultaneously generalize toan unlabeled target group. To solve this problem, we propose Generalized PromptOptimization framework, which incorporates the unlabeled data from the targetgroup into prompt optimization. Extensive experimental results demonstrate theeffectiveness of the proposed framework with significant performanceimprovement on the target group and comparable performance on the source group.\rWeak-to-Strong Jailbreaking on Large Language Models\nXuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang\nabstract\rabstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack\u0026rsquo;s key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model\u0026rsquo;s decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at https://github.com/XuandongZhao/weak-to-strong\r2024-02-03\nContext-aware Adversarial Attack on Named Entity Recognition\nShuguang Chen, Leonardo Neves, Thamar Solorio\nabstract\rabstract: In recent years, large pre-trained language models (PLMs) have achievedremarkable performance on many natural language processing benchmarks. Despitetheir success, prior studies have shown that PLMs are vulnerable to attacksfrom adversarial examples. In this work, we focus on the named entityrecognition task and study context-aware adversarial attack methods to examinethe model\u0026rsquo;s robustness. Specifically, we propose perturbing the mostinformative words for recognizing entities to create adversarial examples andinvestigate different candidate replacement methods to generate natural andplausible adversarial examples. Experiments and analyses show that our methodsare more effective in deceiving the model into making wrong predictions thanstrong baselines.\rSafety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models\nYongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales\nabstract\rabstract: Current vision large language models (VLLMs) exhibit remarkable capabilitiesyet are prone to generate harmful content and are vulnerable to even thesimplest jailbreaking attacks. Our initial analysis finds that this is due tothe presence of harmful data during vision-language instruction fine-tuning,and that VLLM fine-tuning can cause forgetting of safety alignment previouslylearned by the underpinning LLM. To address this issue, we first curate avision-language safe instruction-following dataset VLGuard covering variousharmful categories. Our experiments demonstrate that integrating this datasetinto standard vision-language fine-tuning or utilizing it for post-hocfine-tuning effectively safety aligns VLLMs. This alignment is achieved withminimal impact on, or even enhancement of, the models\u0026rsquo; helpfulness. Theversatility of our safety fine-tuning dataset makes it a valuable resource forsafety-testing existing VLLMs, training new models or safeguarding pre-trainedVLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively rejectunsafe instructions and substantially reduce the success rates of severalblack-box adversarial attacks, which approach zero in many cases. The code anddataset are available at https://github.com/ys-zong/VLGuard.\rData Poisoning for In-context Learning\nPengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang\nabstract\rabstract: In the domain of large language models (LLMs), in-context learning (ICL) hasbeen recognized for its innovative ability to adapt to new tasks, relying onexamples rather than retraining or fine-tuning. This paper delves into thecritical issue of ICL\u0026rsquo;s susceptibility to data poisoning attacks, an area notyet fully explored. We wonder whether ICL is vulnerable, with adversariescapable of manipulating example data to degrade model performance. To addressthis, we introduce ICLPoison, a specialized attacking framework conceived toexploit the learning mechanisms of ICL. Our approach uniquely employs discretetext perturbations to strategically influence the hidden states of LLMs duringthe ICL process. We outline three representative strategies to implementattacks under our framework, each rigorously evaluated across a variety ofmodels and tasks. Our comprehensive tests, including trials on thesophisticated GPT-4 model, demonstrate that ICL\u0026rsquo;s performance is significantlycompromised under our framework. These revelations indicate an urgent need forenhanced defense mechanisms to safeguard the integrity and reliability of LLMsin applications relying on in-context learning.\r2024-02-02\nCodePori: Large Scale Model for Autonomous Software Development by Using Multi-Agents\nZeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson\nabstract\rabstract: Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs)are reshaping the field of Software Engineering (SE). Existing LLM-basedmulti-agent systems have successfully resolved simple dialogue tasks. However,the potential of LLMs for more complex tasks, such as automated code generationfor large and complex projects, have been explored in only a few existingworks. This paper introduces CodePori, a novel model designed to automate codegeneration for extensive and complex software projects based on naturallanguage prompts. We employ LLM-based multi-AI agents to handle creative andchallenging tasks in autonomous software development. Each agent engages with aspecific task, including system design, code development, code review, codeverification, and test engineering. We show in the paper that CodePori is ableto generate running code for large-scale projects, completing the entiresoftware development process in minutes rather than hours, and at a cost of afew dollars. It identifies and mitigates potential security vulnerabilities andcorrects errors while maintaining a solid code performance level. We alsoconducted an evaluation of CodePori against existing solutions using HumanEvaland the Massively Multitask Benchmark for Python (MBPP) benchmark. The resultsindicate that CodePori improves upon the benchmarks in terms of code accuracy,efficiency, and overall performance. For example, CodePori improves the pass@1metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clearimprovement over the existing models. We also assessed CodePori\u0026rsquo;s performancethrough practitioner evaluations, with 91% expressing satisfaction with themodel\u0026rsquo;s performance.\rPreference-free Alignment Learning with Regularized Relevance Reward\nSungdong Kim, Minjoon Seo\nabstract\rabstract: Learning from human preference has been considered key to aligning LargeLanguage Models (LLMs) with human values. However, contrary to popular belief,our preliminary study reveals that reward models trained on human preferencedatasets tend to give higher scores to long off-topic responses than shorton-topic ones. Motivated by this observation, we explore a preference-freeapproach utilizing `relevance\u0026rsquo; as a key objective for alignment. On our firstattempt, we find that the relevance score obtained by a retriever alone isvulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, whenwe utilize the score as a reward for reinforcement learning. To mitigate it, weintegrate effective inductive biases into the vanilla relevance to regularizeeach other, resulting in a mixture of reward functions: Regularized RelevanceReward ($R^3$). $R^3$ significantly improves performance on preferencebenchmarks by providing a robust reward signal. Notably, $R^3$ does not requireany human preference datasets (i.e., preference-free), outperformingopen-source reward models in improving human preference. Our analysisdemonstrates that $R^3$ has advantages in elevating human preference whileminimizing its side effects. Finally, we show the generalizability of $R^3$,consistently improving instruction-tuned models in various backbones and sizeswithout additional dataset cost. Our code is available athttps://github.com/naver-ai/RRR.\r2024-02-01\nTrustworthy Large Models in Vision: A Survey\nZiyan Guo, Li Xu, Jun Liu\nabstract\rabstract: The rapid progress of Large Models (LMs) has recently revolutionized variousfields of deep learning with remarkable grades, ranging from Natural LanguageProcessing (NLP) to Computer Vision (CV). However, LMs are increasinglychallenged and criticized by academia and industry due to their powerfulperformance but untrustworthy behavior, which urgently needs to be alleviatedby reliable methods. Despite the abundance of literature on trustworthy LMs inNLP, a systematic survey specifically delving into the trustworthiness of LMsin CV remains absent. In order to mitigate this gap, we summarize four relevantconcerns that obstruct the trustworthy usage in vision of LMs in this survey,including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)interpretability. By highlighting corresponding challenge, countermeasures, anddiscussion in each topic, we hope this survey will facilitate readers\u0026rsquo;understanding of this field, promote alignment of LMs with human expectationsand enable trustworthy LMs to serve as welfare rather than disaster for humansociety.\r2024-01-31\nAn Early Categorization of Prompt Injection Attacks on Large Language Models\nSippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala, Jason Bennett Thatcher\nabstract\rabstract: Large language models and AI chatbots have been at the forefront ofdemocratizing artificial intelligence. However, the releases of ChatGPT andother similar tools have been followed by growing concerns regarding thedifficulty of controlling large language models and their outputs. Currently,we are witnessing a cat-and-mouse game where users attempt to misuse the modelswith a novel attack called prompt injections. In contrast, the developersattempt to discover the vulnerabilities and block the attacks simultaneously.In this paper, we provide an overview of these emergent threats and present acategorization of prompt injections, which can guide future research on promptinjections and act as a checklist of vulnerabilities in the development of LLMinterfaces. Moreover, based on previous literature and our own empiricalresearch, we discuss the implications of prompt injections to LLM end users,developers, and researchers.\rLoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks\nKaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng\nabstract\rabstract: Sequential recommender systems stand out for their ability to capture users\u0026rsquo;dynamic interests and the patterns of item-to-item transitions. However, theinherent openness of sequential recommender systems renders them vulnerable topoisoning attacks, where fraudulent users are injected into the training datato manipulate learned patterns. Traditional defense strategies predominantlydepend on predefined assumptions or rules extracted from specific knownattacks, limiting their generalizability to unknown attack types. To solve theabove problems, considering the rich open-world knowledge encapsulated in LargeLanguage Models (LLMs), our research initially focuses on the capabilities ofLLMs in the detection of unknown fraudulent activities within recommendersystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate thesubstantial capability of LLMs in identifying unknown fraudsters, leveragingtheir expansive, open-world knowledge. Building upon this, we propose the integration of LLMs into defensestrategies to extend their effectiveness beyond the confines of known attacks.We propose LoRec, an advanced framework that employs LLM-Enhanced Calibrationto strengthen the robustness of sequential recommender systems againstpoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) thatrefines the training process of sequential recommender systems with knowledgederived from LLMs, applying a user-wise reweighting to diminish the impact offraudsters injected by attacks. By incorporating LLMs\u0026rsquo; open-world knowledge,the LCT effectively converts the limited, specific priors or rules into a moregeneral pattern of fraudsters, offering improved defenses against poisoningattacks. Our comprehensive experiments validate that LoRec, as a generalframework, significantly strengthens the robustness of sequential recommendersystems.\r2024-01-30\nSecurity and Privacy Challenges of Large Language Models: A Survey\nBadhan Chandra Das, M. Hadi Amini, Yanzhao Wu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.\rA Preliminary Study on Using Large Language Models in Software Pentesting\nKumar Shashwat, Francis Hahn, Xinming Ou, Dmitry Goldgof, Lawrence Hall, Jay Ligatti, S. Raj Rajgopalan, Armin Ziaie Tabari\nabstract\rabstract: Large language models (LLM) are perceived to offer promising potentials forautomating security tasks, such as those found in security operation centers(SOCs). As a first step towards evaluating this perceived potential, weinvestigate the use of LLMs in software pentesting, where the main task is toautomatically identify software security vulnerabilities in source code. Wehypothesize that an LLM-based AI agent can be improved over time for a specificsecurity task as human operators interact with it. Such improvement can bemade, as a first step, by engineering prompts fed to the LLM based on theresponses produced, to include relevant contexts and structures so that themodel provides more accurate results. Such engineering efforts becomesustainable if the prompts that are engineered to produce better results oncurrent tasks, also produce better results on future unknown tasks. To examinethis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains2,740 hand-crafted source code test cases containing various types ofvulnerabilities. We divide the test cases into training and testing data, wherewe engineer the prompts based on the training data (only), and evaluate thefinal system on the testing data. We compare the AI agent\u0026rsquo;s performance on thetesting data against the performance of the agent without the promptengineering. We also compare the AI agent\u0026rsquo;s results against those fromSonarQube, a widely used static code analyzer for security testing. We builtand tested multiple versions of the AI agent using different off-the-shelf LLMs\u0026ndash; Google\u0026rsquo;s Gemini-pro, as well as OpenAI\u0026rsquo;s GPT-3.5-Turbo and GPT-4-Turbo (withboth chat completion and assistant APIs). The results show that using LLMs is aviable approach to build an AI agent for software pentesting that can improvethrough repeated use and prompt engineering.\r2024-01-29\nLLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs\u0026rsquo; Vulnerability Reasoning\nYuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Miaolei Shi, Yang Liu\nabstract\rabstract: Large language models (LLMs) have demonstrated significant potential for manydownstream tasks, including those requiring human-level intelligence, such asvulnerability detection. However, recent attempts to use LLMs for vulnerabilitydetection are still preliminary, as they lack an in-depth understanding of asubject LLM\u0026rsquo;s vulnerability reasoning capability \u0026ndash; whether it originates fromthe model itself or from external assistance, such as invoking tool support andretrieving vulnerability knowledge. In this paper, we aim to decouple LLMs\u0026rsquo;vulnerability reasoning capability from their other capabilities, including theability to actively seek additional information (e.g., via function calling inSOTA models), adopt relevant vulnerability knowledge (e.g., via vector-basedmatching and retrieval), and follow instructions to output structured results.To this end, we propose a unified evaluation framework named LLM4Vuln, whichseparates LLMs\u0026rsquo; vulnerability reasoning from their other capabilities andevaluates how LLMs\u0026rsquo; vulnerability reasoning could be enhanced when combinedwith the enhancement of other capabilities. To demonstrate the effectiveness ofLLM4Vuln, we have designed controlled experiments using 75 ground-truth smartcontract vulnerabilities, which were extensively audited as high-risk onCode4rena from August to November 2023, and tested them in 4,950 differentscenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama).Our results not only reveal ten findings regarding the varying effects ofknowledge enhancement, context supplementation, prompt schemes, and models butalso enable us to identify 9 zero-day vulnerabilities in two pilot bug bountyprograms with over 1,000 USD being awarded.\r2024-01-27\nLarge Language Model for Vulnerability Detection: Emerging Results and Future Directions\nXin Zhou, Ting Zhang, David Lo\nabstract\rabstract: Previous learning-based vulnerability detection methods relied on eithermedium-sized pre-trained models or smaller neural networks from scratch. Recentadvancements in Large Pre-Trained Language Models (LLMs) have showcasedremarkable few-shot learning capabilities in various tasks. However, theeffectiveness of LLMs in detecting software vulnerabilities is largelyunexplored. This paper aims to bridge this gap by exploring how LLMs performwith various prompts, particularly focusing on two state-of-the-art LLMs:GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achievescompetitive performance with the prior state-of-the-art vulnerability detectionapproach and GPT-4 consistently outperformed the state-of-the-art.\rLow-Resource Languages Jailbreak GPT-4\nZheng-Xin Yong, Cristina Menghini, Stephen H. Bach\nabstract\rabstract: AI safety training and red-teaming of large language models (LLMs) aremeasures to mitigate the generation of unsafe content. Our work exposes theinherent cross-lingual vulnerability of these safety mechanisms, resulting fromthe linguistic inequality of safety training data, by successfullycircumventing GPT-4\u0026rsquo;s safeguard through translating unsafe English inputs intolow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafetranslated inputs and provides actionable items that can get the users towardstheir harmful goals 79% of the time, which is on par with or even surpassingstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages havesignificantly lower attack success rate, which suggests that the cross-lingualvulnerability mainly applies to low-resource languages. Previously, limitedtraining on low-resource languages primarily affects speakers of thoselanguages, causing technological disparities. However, our work highlights acrucial shift: this deficiency now poses a risk to all LLMs users. Publiclyavailable translation APIs enable anyone to exploit LLMs\u0026rsquo; safetyvulnerabilities. Therefore, our work calls for a more holistic red-teamingefforts to develop robust multilingual safeguards with wide language coverage.\rCan ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation\nLi Zhong, Zilong Wang\nabstract\rabstract: Recently, the large language models (LLMs) have shown extraordinary abilityin understanding natural language and generating programming code. It has beena common practice of software engineers to consult LLMs when encounteringcoding questions. Although efforts have been made to avoid syntax errors andalign the code with the intended semantics, the reliability and robustness ofthe code generationfrom LLMs have not yet been thoroughly studied. Theexecutable code is not equivalent to the reliable and robust code, especiallyin the context of real-world software development. The misuse of APIs in thegenerated code could lead to severe problem, such as resource leaks, programcrashes. To make things worse, the users of LLM code generation services areactually the developers that are most vulnerable to these code that seems right\u0026ndash; They are always novice developers that are not familiar with the APIs thatLLMs generate code for them. Therefore, they could hardly tell the misuse inthe code generated by LLMs, which further facilitates the incorrect codeapplied in real-world software. Existing code evaluation benchmark and datasetsfocus on crafting small tasks such as programming questions in codinginterviews, which however deviates from the problem that developers would askLLM for real-world coding help. To fill the missing piece, in this work, wepropose a dataset RobustAPI for evaluating the reliability and robustness ofcode generated by LLMs. We collect 1208 coding questions from StackOverflow on24 representative Java APIs. We summarize thecommon misuse patterns of theseAPIs and evaluate them oncurrent popular LLMs. The evaluation results show thatevenfor GPT-4, 62% of the generated code contains API misuses,which would causeunexpected consequences if the code isintroduced into real-world software.\r2024-01-26\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into \u0026ldquo;The Good\u0026rdquo; (beneficial LLM applications),\u0026ldquo;The Bad\u0026rdquo; (offensive applications), and \u0026ldquo;The Ugly\u0026rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs\u0026rsquo; potential to both bolster and jeopardize cybersecurity.\rBetter Representations via Adversarial Training in Pre-Training: A Theoretical Perspective\nYue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng\nabstract\rabstract: Pre-training is known to generate universal representations for downstreamtasks in large-scale deep learning such as large language models. Existingliterature, e.g., \\cite{kim2020adversarial}, empirically observe that thedownstream tasks can inherit the adversarial robustness of the pre-trainedmodel. We provide theoretical justifications for this robustness inheritancephenomenon. Our theoretical results reveal that feature purification plays animportant role in connecting the adversarial robustness of the pre-trainedmodel and the downstream tasks in two-layer neural networks. Specifically, weshow that (i) with adversarial training, each hidden node tends to pick onlyone (or a few) feature; (ii) without adversarial training, the hidden nodes canbe vulnerable to attacks. This observation is valid for both supervisedpre-training and contrastive learning. With purified nodes, it turns out thatclean training is enough to achieve adversarial robustness in downstream tasks.\r2024-01-25\nMULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds\nXiaolong Jin, Zhuo Zhang, Xiangyu Zhang\nabstract\rabstract: Large Language Model (LLM) alignment aims to ensure that LLM outputs matchwith human values. Researchers have demonstrated the severity of alignmentproblems with a large spectrum of jailbreak techniques that can induce LLMs toproduce malicious content during conversations. Finding the correspondingjailbreaking prompts usually requires substantial human intelligence orcomputation resources. In this paper, we report that LLMs have different levelsof alignment in various contexts. As such, by systematically constructing manycontexts, called worlds, leveraging a Domain Specific Language describingpossible worlds (e.g., time, location, characters, actions and languages) andthe corresponding compiler, we can cost-effectively expose latent alignmentissues. Given the low cost of our method, we are able to conduct a large scalestudy regarding LLM alignment issues in different worlds. Our results show thatour method outperforms the-state-of-the-art jailbreaking techniques on botheffectiveness and efficiency. In addition, our results indicate that existingLLMs are extremely vulnerable to nesting worlds and programming languageworlds. They imply that existing alignment training focuses on the real-worldand is lacking in various (virtual) worlds where LLMs can be exploited.\rBeyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation\nJennifer Chien, David Danks\nabstract\rabstract: Algorithmic harms are commonly categorized as either allocative orrepresentational. This study specifically addresses the latter, focusing on anexamination of current definitions of representational harms to discern what isincluded and what is not. This analysis motivates our expansion beyondbehavioral definitions to encompass harms to cognitive and affective states.The paper outlines high-level requirements for measurement: identifying thenecessary expertise to implement this approach and illustrating it through acase study. Our work highlights the unique vulnerabilities of large languagemodels to perpetrating representational harms, particularly when these harms gounmeasured and unmitigated. The work concludes by presenting proposedmitigations and delineating when to employ them. The overarching aim of thisresearch is to establish a framework for broadening the definition ofrepresentational harms and to translate insights from fairness research intopractical measurement and mitigation praxis.\rAdaptive Text Watermark for Large Language Models\nYepeng Liu, Yuheng Bu\nabstract\rabstract: The advancement of Large Language Models (LLMs) has led to increasingconcerns about the misuse of AI-generated text, and watermarking forLLM-generated text has emerged as a potential solution. However, it ischallenging to generate high-quality watermarked text while maintaining strongsecurity, robustness, and the ability to detect watermarks without priorknowledge of the prompt or model. This paper proposes an adaptive watermarkingstrategy to address this problem. To improve the text quality and maintainrobustness, we adaptively add watermarking to token distributions with highentropy measured using an auxiliary model and keep the low entropy tokendistributions untouched. For the sake of security and to further minimize thewatermark\u0026rsquo;s impact on text quality, instead of using a fixed green/red listgenerated from a random secret key, which can be vulnerable to decryption andforgery, we adaptively scale up the output logits in proportion based on thesemantic embedding of previously generated text using a well designed semanticmapping model. Our experiments involving various LLMs demonstrate that ourapproach achieves comparable robustness performance to existing watermarkmethods. Additionally, the text generated by our method has perplexitycomparable to that of \\emph{un-watermarked} LLMs while maintaining securityeven under various attacks.\r2024-01-20\nLLM4Fuzz: Guided Fuzzing of Smart Contracts with Large Language Models\nChaofan Shou, Jing Liu, Doudou Lu, Koushik Sen\nabstract\rabstract: As blockchain platforms grow exponentially, millions of lines of smartcontract code are being deployed to manage extensive digital assets. However,vulnerabilities in this mission-critical code have led to significantexploitations and asset losses. Thorough automated security analysis of smartcontracts is thus imperative. This paper introduces LLM4Fuzz to optimizeautomated smart contract security analysis by leveraging large language models(LLMs) to intelligently guide and prioritize fuzzing campaigns. Whiletraditional fuzzing suffers from low efficiency in exploring the vast statespace, LLM4Fuzz employs LLMs to direct fuzzers towards high-value code regionsand input sequences more likely to trigger vulnerabilities. Additionally,LLM4Fuzz can leverage LLMs to guide fuzzers based on user-defined invariants,reducing blind exploration overhead. Evaluations of LLM4Fuzz on real-world DeFiprojects show substantial gains in efficiency, coverage, and vulnerabilitydetection compared to baseline fuzzing. LLM4Fuzz also uncovered five criticalvulnerabilities that can lead to a loss of more than $247k.\rJailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts\nYuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun\nabstract\rabstract: Existing work on jailbreak Multimodal Large Language Models (MLLMs) hasfocused primarily on adversarial examples in model inputs, with less attentionto vulnerabilities, especially in model API. To fill the research gap, we carryout the following work: 1) We discover a system prompt leakage vulnerability inGPT-4V. Through carefully designed dialogue, we successfully extract theinternal system prompts of GPT-4V. This finding indicates potential exploitablesecurity risks in MLLMs; 2) Based on the acquired system prompts, we propose anovel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack viaSystem Prompt). By employing GPT-4 as a red teaming tool against itself, we aimto search for potential jailbreak prompts leveraging stolen system prompts.Furthermore, in pursuit of better performance, we also add human modificationbased on GPT-4\u0026rsquo;s analysis, which further improves the attack success rate to98.7%; 3) We evaluated the effect of modifying system prompts to defendagainst jailbreaking attacks. Results show that appropriately designed systemprompts can significantly reduce jailbreak success rates. Overall, our workprovides new insights into enhancing MLLM security, demonstrating the importantrole of system prompts in jailbreaking. This finding could be leveraged togreatly facilitate jailbreak success rates while also holding the potential fordefending against jailbreaks.\rBadChain: Backdoor Chain-of-Thought Prompting for Large Language Models\nZhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li\nabstract\rabstract: Large language models (LLMs) are shown to benefit from chain-of-thought (COT)prompting, particularly when tackling tasks that require systematic reasoningprocesses. On the other hand, COT prompting also poses new vulnerabilities inthe form of backdoor attacks, wherein the model will output unintendedmalicious content under specific backdoor-triggered conditions duringinference. Traditional methods for launching backdoor attacks involve eithercontaminating the training dataset with backdoored instances or directlymanipulating the model parameters during deployment. However, these approachesare not practical for commercial LLMs that typically operate via API access. Inthis paper, we propose BadChain, the first backdoor attack against LLMsemploying COT prompting, which does not require access to the training datasetor model parameters and imposes low computational overhead. BadChain leveragesthe inherent reasoning capabilities of LLMs by inserting a backdoor reasoningstep into the sequence of reasoning steps of the model output, thereby alteringthe final response when a backdoor trigger exists in the query prompt.Empirically, we show the effectiveness of BadChain for two COT strategiesacross four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmarktasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,we show that LLMs endowed with stronger reasoning capabilities exhibit highersusceptibility to BadChain, exemplified by a high average attack success rateof 97.0% across the six benchmark tasks on GPT-4. Finally, we propose twodefenses based on shuffling and demonstrate their overall ineffectivenessagainst BadChain. Therefore, BadChain remains a severe threat to LLMs,underscoring the urgency for the development of robust and effective futuredefenses.\r2024-01-19\nPruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning\nAdib Hasan, Ileana Rugina, Alex Wang\nabstract\rabstract: Large Language Models (LLMs) are vulnerable to `Jailbreaking\u0026rsquo; prompts, a typeof attack that can coax these models into generating harmful and illegalcontent. In this paper, we show that pruning up to 20% of LLM parametersmarkedly increases their resistance to such attacks without additional trainingand without sacrificing their performance in standard benchmarks. Intriguingly,we discovered that the enhanced safety observed post-pruning correlates to theinitial safety training level of the model, hinting that the effect of pruningcould be more general and may hold for other LLM behaviors beyond safety.Additionally, we introduce a curated dataset of 225 harmful tasks across fivecategories, inserted into ten different Jailbreaking prompts, showing thatpruning aids LLMs in concentrating attention on task-relevant tokens injailbreaking prompts. Lastly, our experiments reveal that the prominent chatmodels, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit highsusceptibility to jailbreaking attacks, with some categories achieving nearly70-100% success rate. These insights underline the potential of pruning as ageneralizable approach for improving LLM safety, reliability, and potentiallyother desired behaviors.\r2024-01-17\nMLLM-Protector: Ensuring MLLM\u0026rsquo;s Safety without Hurting Performance\nRenjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang\nabstract\rabstract: The deployment of multimodal large language models (MLLMs) has brought fortha unique vulnerability: susceptibility to malicious attacks through visualinputs. We delve into the novel challenge of defending MLLMs against suchattacks. We discovered that images act as a \u0026ldquo;foreign language\u0026rdquo; that is notconsidered during alignment, which can make MLLMs prone to producing harmfulresponses. Unfortunately, unlike the discrete tokens considered in text-basedLLMs, the continuous nature of image signals presents significant alignmentchallenges, which poses difficulty to thoroughly cover the possible scenarios.This vulnerability is exacerbated by the fact that open-source MLLMs arepredominantly fine-tuned on limited image-text pairs that is much less than theextensive text-based pretraining corpus, which makes the MLLMs more prone tocatastrophic forgetting of their original abilities during explicit alignmenttuning. To tackle these challenges, we introduce MLLM-Protector, aplug-and-play strategy combining a lightweight harm detector and a responsedetoxifier. The harm detector\u0026rsquo;s role is to identify potentially harmful outputsfrom the MLLM, while the detoxifier corrects these outputs to ensure theresponse stipulates to the safety standards. This approach effectivelymitigates the risks posed by malicious visual inputs without compromising themodel\u0026rsquo;s overall performance. Our results demonstrate that MLLM-Protector offersa robust solution to a previously unaddressed aspect of MLLM security.\rCharacterizing Online Eating Disorder Communities with Large Language Models\nMinh Duc Chu, Aryan Karnati, Zihao He, Kristina Lerman\nabstract\rabstract: The rise in eating disorders, a dangerous mental health condition with highmortality and morbidity, has been linked to the proliferation of idealized bodyimages on social media. However, the link between social media and eatingdisorders is far more complex. We argue that social media platforms create afeedback loop that amplifies the growth of content and communities that promoteeating disorders like anorexia and bulimia. Specifically, social mediaplatforms make it easy for vulnerable individuals to find and connect tolike-minded others, while group dynamic processes encourage them to stayengaged within communities that promote and glorify harmful behaviors linked toeating disorders. We characterize this dynamic empirically through acombination of network and language analysis. We describe a novel frameworkthat leverages large language models to analyze the discourse within onlinecommunities and probe their attitudes on topics related to eating disorders toidentify potentially harmful content. Our work emphasizes the need for bettersocial media moderation to disrupt harmful feedback loops and protectvulnerable individuals.\r2024-01-16\nWhispering Pixels: Exploiting Uninitialized Register Accesses in Modern GPUs\nFrederik Dermot Pustelnik, Xhani Marvin Saß, Jean-Pierre Seifert\nabstract\rabstract: Graphic Processing Units (GPUs) have transcended their traditional use-caseof rendering graphics and nowadays also serve as a powerful platform foraccelerating ubiquitous, non-graphical rendering tasks. One prominent task isinference of neural networks, which process vast amounts of personal data, suchas audio, text or images. Thus, GPUs became integral components for handlingvast amounts of potentially confidential data, which has awakened the interestof security researchers. This lead to the discovery of various vulnerabilitiesin GPUs in recent years. In this paper, we uncover yet another vulnerabilityclass in GPUs: We found that some GPU implementations lack proper registerinitialization routines before shader execution, leading to unintended registercontent leakage of previously executed shader kernels. We showcase theexistence of the aforementioned vulnerability on products of 3 major vendors -Apple, NVIDIA and Qualcomm. The vulnerability poses unique challenges to anadversary due to opaque scheduling and register remapping algorithms present inthe GPU firmware, complicating the reconstruction of leaked data. In order toillustrate the real-world impact of this flaw, we showcase how these challengescan be solved for attacking various workloads on the GPU. First, we showcasehow uninitialized registers leak arbitrary pixel data processed by fragmentshaders. We further implement information leakage attacks on intermediate dataof Convolutional Neural Networks (CNNs) and present the attack\u0026rsquo;s capability toleak and reconstruct the output of Large Language Models (LLMs).\r2024-01-15\nTraces of Memorisation in Large Language Models for Code\nAli Al-Kaswan, Maliheh Izadi, Arie van Deursen\nabstract\rabstract: Large language models have gained significant popularity because of theirability to generate human-like text and potential applications in variousfields, such as Software Engineering. Large language models for code arecommonly trained on large unsanitised corpora of source code scraped from theinternet. The content of these datasets is memorised and can be extracted byattackers with data extraction attacks. In this work, we explore memorisationin large language models for code and compare the rate of memorisation withlarge language models trained on natural language. We adopt an existingbenchmark for natural language and construct a benchmark for code byidentifying samples that are vulnerable to attack. We run both benchmarksagainst a variety of models, and perform a data extraction attack. We find thatlarge language models for code are vulnerable to data extraction attacks, liketheir natural language counterparts. From the training data that was identifiedto be potentially extractable we were able to extract 47% from aCodeGen-Mono-16B code completion model. We also observe that models memorisemore, as their parameter count grows, and that their pre-training data are alsovulnerable to attack. We also find that data carriers are memorised at a higherrate than regular code or documentation and that different model architecturesmemorise different samples. Data leakage has severe outcomes, so we urge theresearch community to further investigate the extent of this phenomenon using awider range of models and extraction techniques in order to build safeguards tomitigate this issue.\rFuzz4All: Universal Fuzzing with Large Language Models\nChunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang\nabstract\rabstract: Fuzzing has achieved tremendous success in discovering bugs andvulnerabilities in various software systems. Systems under test (SUTs) thattake in programming or formal language as inputs, e.g., compilers, runtimeengines, constraint solvers, and software libraries with accessible APIs, areespecially important as they are fundamental building blocks of softwaredevelopment. However, existing fuzzers for such systems often target a specificlanguage, and thus cannot be easily applied to other languages or even otherversions of the same language. Moreover, the inputs generated by existingfuzzers are often limited to specific features of the input language, and thuscan hardly reveal bugs related to other or new features. This paper presentsFuzz4All, the first fuzzer that is universal in the sense that it can targetmany different input languages and many different features of these languages.The key idea behind Fuzz4All is to leverage large language models (LLMs) as aninput generation and mutation engine, which enables the approach to producediverse and realistic inputs for any practically relevant language. To realizethis potential, we present a novel autoprompting technique, which creates LLMprompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,which iteratively updates the prompt to create new fuzzing inputs. We evaluateFuzz4All on nine systems under test that take in six different languages (C,C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all sixlanguages, that universal fuzzing achieves higher coverage than existing,language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs inwidely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskitquantum computing platform, with 64 bugs already confirmed by developers aspreviously unknown.\r2024-01-12\nGreening Large Language Models of Code\nJieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, David Lo\nabstract\rabstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers\u0026rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\\times$less), carbon footprint (up to 157$\\times$ less), and inference latency (up to76$\\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).\rAdvancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation\nReza Fayyazi, Rozhina Taghdimi, Shanchieh Jay Yang\nabstract\rabstract: Tactics, Techniques, and Procedures (TTPs) outline the methods attackers useto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT\u0026amp;CKframework can be challenging for cybersecurity practitioners due to presumedexpertise, complex dependencies, and inherent ambiguity. Meanwhile,advancements with Large Language Models (LLMs) have led to recent surge instudies exploring its uses in cybersecurity operations. This leads us toquestion how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5)LLMs can comprehend and summarize TTPs to inform analysts of the intendedpurposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMshave shown to be prone to hallucination by providing inaccurate information,which is problematic in critical domains like cybersecurity. Therefore, wepropose the use of Retrieval Augmented Generation (RAG) techniques to extractrelevant contexts for each cyberattack procedure for decoder-only LLMs (withoutfine-tuning). We further contrast such approach against supervised fine-tuning(SFT) of encoder-only LLMs. Our results reveal that both the direct-use ofdecoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-onlyLLMs offer inaccurate interpretation of cyberattack procedures. Significantimprovements are shown when RAG is used for decoder-only LLMs, particularlywhen directly relevant context is found. This study further sheds insights onthe limitations and capabilities of using RAG for LLMs in interpreting TTPs.\r2024-01-10\nLimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack\nHai Zhu, Zhaoqing Yang, Weiwei Shang, Yuren Wu\nabstract\rabstract: Natural language processing models are vulnerable to adversarial examples.Previous textual adversarial attacks adopt gradients or confidence scores tocalculate word importance ranking and generate adversarial examples. However,this information is unavailable in the real world. Therefore, we focus on amore realistic and challenging setting, named hard-label attack, in which theattacker can only query the model and obtain a discrete prediction label.Existing hard-label attack algorithms tend to initialize adversarial examplesby random substitution and then utilize complex heuristic algorithms tooptimize the adversarial perturbation. These methods require a lot of modelqueries and the attack success rate is restricted by adversary initialization.In this paper, we propose a novel hard-label attack algorithm named LimeAttack,which leverages a local explainable method to approximate word importanceranking, and then adopts beam search to find the optimal solution. Extensiveexperiments show that LimeAttack achieves the better attacking performancecompared with existing hard-label attack under the same query budget. Inaddition, we evaluate the effectiveness of LimeAttack on large language models,and results indicate that adversarial examples remain a significant threat tolarge language models. The adversarial examples crafted by LimeAttack arehighly transferable and effectively improve model robustness in adversarialtraining.\r2024-01-09\nThe Earth is Flat because\u0026hellip;: Investigating LLMs\u0026rsquo; Belief towards Misinformation via Persuasive Conversation\nRongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu\nabstract\rabstract: Large Language Models (LLMs) encapsulate vast amounts of knowledge but stillremain vulnerable to external misinformation. Existing research mainly studiedthis susceptibility behavior in a single-turn setting. However, belief canchange during a multi-turn conversation, especially a persuasive one.Therefore, in this study, we delve into LLMs\u0026rsquo; susceptibility to persuasiveconversations, particularly on factual questions that they can answercorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, whichcontains factual questions paired with systematically generated persuasivemisinformation. Then, we develop a testing framework to track LLMs\u0026rsquo; beliefchanges in a persuasive dialogue. Through extensive experiments, we find thatLLMs\u0026rsquo; correct beliefs on factual knowledge can be easily manipulated by variouspersuasive strategies.\r2024-01-08\nEnhanced Automated Code Vulnerability Repair using Large Language Models\nDavid de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz\nabstract\rabstract: This research addresses the complex challenge of automated repair of codevulnerabilities, vital for enhancing digital security in an increasinglytechnology-driven world. The study introduces a novel and efficient format forthe representation of code modification, using advanced Large Language Models(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasetsfeaturing C code vulnerabilities, significantly improve the accuracy andadaptability of automated code repair techniques. A key finding is the enhancedrepair accuracy of these models when compared to previous methods such asVulRepair, which underscores their practical utility and efficiency. Theresearch also offers a critical assessment of current evaluation metrics, suchas perfect predictions, and their limitations in reflecting the truecapabilities of automated repair models in real-world scenarios. Followingthis, it underscores the importance of using test datasets devoid of trainsamples, emphasizing the need for dataset integrity to enhance theeffectiveness of LLMs in code repair tasks. The significance of this work isits contribution to digital security, setting new standards for automated codevulnerability repair and paving the way for future advancements in the fieldsof cybersecurity and artificial intelligence. The study does not only highlightthe potential of LLMs in enhancing code security but also fosters furtherexploration and research in these crucial areas.\rJatmo: Prompt Injection Defense by Task-Specific Finetuning\nJulien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner\nabstract\rabstract: Large Language Models (LLMs) are attracting significant research attentiondue to their instruction-following abilities, allowing users and developers toleverage LLMs for a variety of tasks. However, LLMs are vulnerable toprompt-injection attacks: a class of attacks that hijack the model\u0026rsquo;sinstruction-following abilities, changing responses to prompts to undesired,possibly malicious ones. In this work, we introduce Jatmo, a method forgenerating task-specific models resilient to prompt-injection attacks. Jatmoleverages the fact that LLMs can only follow instructions once they haveundergone instruction tuning. It harnesses a teacher instruction-tuned model togenerate a task-specific dataset, which is then used to fine-tune a base model(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and adataset of inputs for the task: it uses the teacher model to generate outputs.For situations with no pre-existing datasets, Jatmo can use a single example,or in some cases none at all, to produce a fully synthetic dataset. Ourexperiments on seven tasks show that Jatmo models provide similar quality ofoutputs on their specific task as standard LLMs, while being resilient toprompt injections. The best attacks succeeded in less than 0.5% of casesagainst our models, versus 87% success rate against GPT-3.5-Turbo. We releaseJatmo at https://github.com/wagner-group/prompt-injection-defense.\r2024-01-07\nThe Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\nHaonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi\nabstract\rabstract: The commercialization of diffusion models, renowned for their ability togenerate high-quality images that are often indistinguishable from real ones,brings forth potential copyright concerns. Although attempts have been made toimpede unauthorized access to copyrighted material during training and tosubsequently prevent DMs from generating copyrighted images, the effectivenessof these solutions remains unverified. This study explores the vulnerabilitiesassociated with copyright protection in DMs by introducing a backdoor datapoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.Our attack method operates without requiring access to or control over thediffusion model\u0026rsquo;s training or fine-tuning processes; it merely involves theinsertion of poisoning data into the clean training dataset. This data,comprising poisoning images equipped with prompts, is generated by leveragingthe powerful capabilities of multimodal large language models and text-guidedimage inpainting techniques. Our experimental results and analysis confirm themethod\u0026rsquo;s effectiveness. By integrating a minor portion ofnon-copyright-infringing stealthy poisoning data into the cleandataset-rendering it free from suspicion-we can prompt the finetuned diffusionmodels to produce copyrighted content when activated by specific triggerprompts. These findings underline potential pitfalls in the prevailingcopyright protection strategies and underscore the necessity for increasedscrutiny and preventative measures against the misuse of DMs.\r2024-01-04\nInstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models\nXunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang\nabstract\rabstract: Large vision-language models (LVLMs) have demonstrated their incrediblecapability in image understanding and response generation. However, this richvisual interaction also makes LVLMs vulnerable to adversarial examples. In thispaper, we formulate a novel and practical gray-box attack scenario that theadversary can only access the visual encoder of the victim LVLM, without theknowledge of its prompts (which are often proprietary for service providers andnot publicly available) and its underlying large language model (LLM). Thispractical setting poses challenges to the cross-prompt and cross-modeltransferability of targeted adversarial attack, which aims to confuse the LVLMto output a response that is semantically similar to the attacker\u0026rsquo;s chosentarget text. To this end, we propose an instruction-tuned targeted attack(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs withhigh transferability. Initially, we utilize a public text-to-image generativemodel to \u0026ldquo;reverse\u0026rdquo; the target response into a target image, and employ GPT-4 toinfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the targetresponse. We then form a local surrogate model (sharing the same visual encoderwith the victim LVLM) to extract instruction-aware features of an adversarialimage example and the target image, and minimize the distance between these twofeatures to optimize the adversarial example. To further improve thetransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ withinstructions paraphrased from an LLM. Extensive experiments demonstrate thesuperiority of our proposed method in targeted attack performance andtransferability.\r2024-01-02\nA Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models\nDaniel Wankit Yip, Aysan Esmradi, Chun Fai Chan\nabstract\rabstract: Prompt injection attacks exploit vulnerabilities in large language models(LLMs) to manipulate the model into unintended actions or generate maliciouscontent. As LLM integrated applications gain wider adoption, they face growingsusceptibility to such attacks. This study introduces a novel evaluationframework for quantifying the resilience of applications. The frameworkincorporates innovative techniques designed to ensure representativeness,interpretability, and robustness. To ensure the representativeness of simulatedattacks on the application, a meticulous selection process was employed,resulting in 115 carefully chosen attacks based on coverage and relevance. Forenhanced interpretability, a second LLM was utilized to evaluate the responsesgenerated from these simulated attacks. Unlike conventional malicious contentclassifiers that provide only a confidence score, the LLM-based evaluationproduces a score accompanied by an explanation, thereby enhancinginterpretability. Subsequently, a resilience score is computed by assigninghigher weights to attacks with greater impact, thus providing a robustmeasurement of the application resilience. To assess the framework\u0026rsquo;s efficacy,it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed thatLlama2, the newer model exhibited higher resilience compared to ChatGLM. Thisfinding substantiates the effectiveness of the framework, aligning with theprevailing notion that newer models tend to possess greater resilience.Moreover, the framework exhibited exceptional versatility, requiring onlyminimal adjustments to accommodate emerging attack techniques andclassifications, thereby establishing itself as an effective and practicalsolution. Overall, the framework offers valuable insights that empowerorganizations to make well-informed decisions to fortify their applicationsagainst potential threats from prompt injection.\r2023-12-31\nKernelGPT: Enhanced Kernel Fuzzing via Large Language Models\nChenyuan Yang, Zijie Zhao, Lingming Zhang\nabstract\rabstract: Bugs in operating system kernels can affect billions of devices and users allover the world. As a result, a large body of research has been focused onkernel fuzzing, i.e., automatically generating syscall (system call) sequencesto detect potential kernel bugs or vulnerabilities. Syzkaller, one of the mostwidely studied kernel fuzzers, aims to generate valid syscall sequences basedon predefined specifications written in syzlang, a domain-specific language fordefining syscalls, their arguments, and the relationships between them. Whilethere has been existing work trying to automate Syzkaller specificationgeneration, this still remains largely manual work and a large number ofimportant syscalls are still uncovered. In this paper, we propose KernelGPT,the first approach to automatically inferring Syzkaller specifications viaLarge Language Models (LLMs) for enhanced kernel fuzzing. Our basic insight isthat LLMs have seen massive kernel code, documentation, and use cases duringpre-training, and thus can automatically distill the necessary information formaking valid syscalls. More specifically, KernelGPT leverages an iterativeapproach to automatically infer all the necessary specification components, andfurther leverages the validation feedback to repair/refine the initialspecifications. Our preliminary results demonstrate that KernelGPT can helpSyzkaller achieve higher coverage and find multiple previously unknown bugs.Moreover, we also received a request from the Syzkaller team to upstreamspecifications inferred by KernelGPT.\r2023-12-30\nThe Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness\nNeeraj Varshney, Pavel Dolin, Agastya Seth, Chitta Baral\nabstract\rabstract: As Large Language Models (LLMs) play an increasingly pivotal role in naturallanguage processing applications, their safety concerns become critical areasof NLP research. This paper presents Safety and Over-Defensiveness Evaluation(SODE) benchmark: a collection of diverse safe and unsafe prompts withcarefully designed evaluation methods that facilitate systematic evaluation,comparison, and analysis over \u0026lsquo;safety\u0026rsquo; and \u0026lsquo;over-defensiveness.\u0026rsquo; With SODE, westudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,which reveals several interesting and important findings, such as (a) thewidely popular \u0026lsquo;self-checking\u0026rsquo; techniques indeed improve the safety againstunsafe inputs, but this comes at the cost of extreme over-defensiveness on thesafe inputs, (b) providing a safety instruction along with in-context exemplars(of both safe and unsafe inputs) consistently improves safety and alsomitigates undue over-defensiveness of the models, (c) providing contextualknowledge easily breaks the safety guardrails and makes the models morevulnerable to generating unsafe responses. Overall, our work reveals numeroussuch critical findings that we believe will pave the way and facilitate furtherresearch in improving the safety of LLMs.\r2023-12-28\nHow Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation\nYang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, Pengfei Liu\nabstract\rabstract: Human behavior simulation of AI agents necessitates the agents to possess aquality of believability, which is crucial as it facilitates users inestablishing trust toward the agents and streamlines the fulfillment of theagents\u0026rsquo; goal. While recent advancements in Large Language Model (LLM) basedagents have improved human behavior simulation, challenges inherent to LLMs(e.g., long context modeling) can undermine their believability. Consequently,evaluating AI agent believability becomes imperative. Unfortunately, priorresearch often neglects the negative impacts of LLM deficiencies. To addressthese gaps, we introduce two metrics for assessing LLM-based agentbelievability: consistency, and robustness, together with a benchmark,SimulateBench, with which, we evaluate the consistency and robustness of agentsimplemented with popular LLMs. We find that agents (i) struggle to accuratelydepict character information when presented with lengthy profile inputs; (ii)exhibit vulnerability to profile perturbations; and (iii) are significantlyaffected by certain key factors that impact their overall believability. Codeand SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.\r2023-12-25\nGPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\nYuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei Xie, Yang Liu\nabstract\rabstract: Smart contracts are prone to various vulnerabilities, leading to substantialfinancial losses over time. Current analysis tools mainly targetvulnerabilities with fixed control or data-flow patterns, such as re-entrancyand integer overflow. However, a recent study on Web3 security bugs revealedthat about 80% of these bugs cannot be audited by existing tools due to thelack of domain-specific property description and checking. Given recentadvances in Large Language Models (LLMs), it is worth exploring how GenerativePre-training Transformer (GPT) could aid in detecting logicc vulnerabilities. In this paper, we propose GPTScan, the first tool combining GPT with staticanalysis for smart contract logic vulnerability detection. Instead of relyingsolely on GPT to identify vulnerabilities, which can lead to high falsepositives and is limited by GPT\u0026rsquo;s pre-trained knowledge, we utilize GPT as aversatile code understanding tool. By breaking down each logic vulnerabilitytype into scenarios and properties, GPTScan matches candidate vulnerabilitieswith GPT. To enhance accuracy, GPTScan further instructs GPT to intelligentlyrecognize key variables and statements, which are then validated by staticconfirmation. Evaluation on diverse datasets with around 400 contract projectsand 3K Solidity files shows that GPTScan achieves high precision (over 90%) fortoken contracts and acceptable precision (57.14%) for large projects likeWeb3Bugs. It effectively detects ground-truth logic vulnerabilities with arecall of over 70%, including 9 new vulnerabilities missed by human auditors.GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01USD to scan per thousand lines of Solidity code. Moreover, static confirmationhelps GPTScan reduce two-thirds of false positives.\rVulnerability of Machine Learning Approaches Applied in IoT-based Smart Grid: A Review\nZhenyong Zhang, Mengxiang Liu, Mingyang Sun, Ruilong Deng, Peng Cheng, Dusit Niyato, Mo-Yuen Chow, Jiming Chen\nabstract\rabstract: Machine learning (ML) sees an increasing prevalence of being used in theinternet-of-things (IoT)-based smart grid. However, the trustworthiness of MLis a severe issue that must be addressed to accommodate the trend of ML-basedsmart grid applications (MLsgAPPs). The adversarial distortion injected intothe power signal will greatly affect the system\u0026rsquo;s normal control and operation.Therefore, it is imperative to conduct vulnerability assessment for MLsgAPPsapplied in the context of safety-critical power systems. In this paper, weprovide a comprehensive review of the recent progress in designing attack anddefense methods for MLsgAPPs. Unlike the traditional survey about ML security,this is the first review work about the security of MLsgAPPs that focuses onthe characteristics of power systems. We first highlight the specifics forconstructing the adversarial attacks on MLsgAPPs. Then, the vulnerability ofMLsgAPP is analyzed from both the aspects of the power system and ML model.Afterward, a comprehensive survey is conducted to review and compare existingstudies about the adversarial attacks on MLsgAPPs in scenarios of generation,transmission, distribution, and consumption, and the countermeasures arereviewed according to the attacks that they defend against. Finally, the futureresearch directions are discussed on the attacker\u0026rsquo;s and defender\u0026rsquo;s side,respectively. We also analyze the potential vulnerability of large languagemodel-based (e.g., ChatGPT) power system applications. Overall, we encouragemore researchers to contribute to investigating the adversarial issues ofMLsgAPPs.\r2023-12-23\nA Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection\nXiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, Chao Shen\nabstract\rabstract: Large Language Models and Multi-Modal LLMs have become pervasive, and so doesthe importance of their security; yet, modern LLMs are known to be vulnerableto jailbreaking attacks. These attacks can allow malicious users to exploit themodels, making the case for effective jailbreak detection mechanisms anessential aspect of maintaining the integrity and trustworthiness of LLM-basedapplications. However, existing detection works on jailbreak attacks havelimitations. Existing post-query-based strategies require target domainknowledge, and pre-query-based methods mainly focus on text-level attacks andfail to meet the increasingly complex multi-modal security requirements placedupon contemporary LLMs. This gap underscores the need for a more comprehensiveapproach to safeguarding these influential systems. In this work, we propose JailGuard, the first mutation-based jailbreakingdetection framework which supports both image and text modalities. Our keyobservation is that attack queries inherently possess less robustness comparedto benign queries. Specifically, to confuse the model, attack queries areusually crafted with well-designed templates or complicate perturbations,leading to a fact that a slight disturbance in input may result in a drasticchange in the response. This lack of robustness can be utilized in attackdetection. Based on this intuition, we designed and implemented a detectionframework comprising 19 different mutators and a divergence-based detectionformula. To fully understand the effectiveness of our framework, we built thefirst multi-modal LLM jailbreaking attack dataset, which has 304 items of data,covering ten types of known jailbreaking attacks on image and text modalities.The evaluation suggests that JailGuard achieves the best detection accuracy of89.38%/85.42% on image and text inputs, outperforming state-of-the-art defensemethods by 15.28%.\r2023-12-22\nHow Far Have We Gone in Vulnerability Detection Using Large Language Models\nZeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang\nabstract\rabstract: As software becomes increasingly complex and prone to vulnerabilities,automated vulnerability detection is critically important, yet challenging.Given the significant successes of large language models (LLMs) in varioustasks, there is growing anticipation of their efficacy in vulnerabilitydetection. However, a quantitative understanding of their potential invulnerability detection is still missing. To bridge this gap, we introduce acomprehensive vulnerability benchmark VulBench. This benchmark aggregateshigh-quality data from a wide range of CTF (Capture-the-Flag) challenges andreal-world applications, with annotations for each vulnerable functiondetailing the vulnerability type and its root cause. Through our experimentsencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based modelsand static analyzers, we find that several LLMs outperform traditional deeplearning approaches in vulnerability detection, revealing an untapped potentialin LLMs. This work contributes to the understanding and utilization of LLMs forenhanced software security.\rMetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models\nHongyin Zhu\nabstract\rabstract: Large language models (LLMs) are increasingly being used in Metaverseenvironments to generate dynamic and realistic content and to control thebehavior of non-player characters (NPCs). However, the cybersecurity concernsassociated with LLMs have become increasingly prominent. Previous research hasprimarily focused on patching system vulnerabilities to enhance cybersecurity,but these approaches are not well-suited to the Metaverse, where the virtualspace is more complex, LLMs are vulnerable, and ethical user interaction iscritical. Moreover, the scope of cybersecurity in the Metaverse is expected toexpand significantly. This paper proposes a method for enhancing cybersecuritythrough the simulation of user interaction with LLMs. Our goal is to educateusers and strengthen their defense capabilities through exposure to acomprehensive simulation system. This system includes extensive Metaversecybersecurity Q\u0026amp;A and attack simulation scenarios. By engaging with these,users will improve their ability to recognize and withstand risks.Additionally, to address the ethical implications of user input, we proposeusing LLMs as evaluators to assess user content across five dimensions. Wefurther adapt the models through vocabulary expansion training to betterunderstand personalized inputs and emoticons. We conduct experiments onmultiple LLMs and find that our approach is effective.\r2023-12-21\nHW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion\nYu-Zheng Lin, Muntasir Mamun, Muhtasim Alam Chowdhury, Shuyu Cai, Mingyu Zhu, Banafsheh Saber Latibari, Kevin Immanuel Gubbi, Najmeh Nazari Bavarsad, Arjun Caputo, Avesta Sasan, Houman Homayoun, Setareh Rafatirad, Pratik Satam, Soheil Salehi\nabstract\rabstract: The escalating complexity of modern computing frameworks has resulted in asurge in the cybersecurity vulnerabilities reported to the NationalVulnerability Database (NVD) by practitioners. Despite the fact that thestature of NVD is one of the most significant databases for the latest insightsinto vulnerabilities, extracting meaningful trends from such a large amount ofunstructured data is still challenging without the application of suitabletechnological methodologies. Previous efforts have mostly concentrated onsoftware vulnerabilities; however, a holistic strategy incorporates approachesfor mitigating vulnerabilities, score prediction, and a knowledge-generatingsystem that may extract relevant insights from the Common Weakness Enumeration(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. Asthe number of hardware attacks on Internet of Things (IoT) devices continues torapidly increase, we present the Hardware Vulnerability to Weakness Mapping(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing onhardware vulnerabilities and IoT security. The architecture that we haveproposed incorporates an Ontology-driven Storytelling framework, whichautomates the process of updating the ontology in order to recognize patternsand evolution of vulnerabilities over time and provides approaches formitigating the vulnerabilities. The repercussions of vulnerabilities can bemitigated as a result of this, and conversely, future exposures can bepredicted and prevented. Furthermore, our proposed framework utilizedGenerative Pre-trained Transformer (GPT) Large Language Models (LLMs) toprovide mitigation suggestions.\r2023-12-19\nRobust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks\nWenhan Yang, Jingdong Gao, Baharan Mirzasoleiman\nabstract\rabstract: Contrastive vision-language representation learning has achievedstate-of-the-art performance for zero-shot classification, by learning frommillions of image-caption pairs crawled from the internet. However, the massivedata that powers large multimodal models such as CLIP, makes them extremelyvulnerable to various types of targeted data poisoning and backdoor attacks.Despite this vulnerability, robust contrastive vision-language pre-trainingagainst such attacks has remained unaddressed. In this work, we propose ROCLIP,the first effective method for robust pre-training multimodal vision-languagemodels against targeted data poisoning and backdoor attacks. ROCLIP effectivelybreaks the association between poisoned image-caption pairs by considering arelatively large and varying pool of random captions, and matching every imagewith the text that is most similar to it in the pool instead of its owncaption, every few epochs.It also leverages image and text augmentations tofurther strengthen the defense and improve the performance of the model. Ourextensive experiments show that ROCLIP renders state-of-the-art targeted datapoisoning and backdoor attacks ineffective during pre-training CLIP models. Inparticular, ROCLIP decreases the success rate for targeted data poisoningattacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, whileimproving the model\u0026rsquo;s linear probe performance by 10% and maintains a similarzero shot performance compared to CLIP. By increasing the frequency ofmatching, ROCLIP is able to defend strong attacks, which add up to 1% poisonedexamples to the data, and successfully maintain a low attack success rate of12.5%, while trading off the performance on some tasks.\rCan Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet\nSaad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, Gianluca Stringhini\nabstract\rabstract: Large Language Models (LLMs) have been suggested for use in automatedvulnerability repair, but benchmarks showing they can consistently identifysecurity-related bugs are lacking. We thus perform the most detailedinvestigation to date on whether LLMs can reliably identify security-relatedbugs. We construct a series of 228 code scenarios and analyze eight of the mostcapable LLMs across eight different investigative dimensions in an automatedframework. Our evaluation shows LLMs provide non-deterministic responses,incorrect and unfaithful reasoning, and perform poorly in real-world scenariosoutside their knowledge cut-off date. Most importantly, our findings revealsignificant non-robustness in even the most advanced models like PaLM2' andGPT-4\u0026rsquo;: by merely changing function or variable names, or by the addition oflibrary functions in the source code, these models can yield incorrect answersin 26% and 17% of cases, respectively. These findings demonstrate that furtherLLM advances are needed before LLMs can be used as general purpose securityassistants.\rCommunicative Agents for Software Development\nChen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun\nabstract\rabstract: Software engineering is a domain characterized by intricate decision-makingprocesses, often relying on nuanced intuition and consultation. Recentadvancements in deep learning have started to revolutionize softwareengineering practices through elaborate designs implemented at various stagesof software development. In this paper, we present an innovative paradigm thatleverages large language models (LLMs) throughout the entire softwaredevelopment process, streamlining and unifying key processes through naturallanguage communication, thereby eliminating the need for specialized models ateach phase. At the core of this paradigm lies ChatDev, a virtual chat-poweredsoftware development company that mirrors the established waterfall model,meticulously dividing the development process into four distinct chronologicalstages: designing, coding, testing, and documenting. Each stage engages a teamof \u0026ldquo;software agents\u0026rdquo;, such as programmers, code reviewers, and test engineers,fostering collaborative dialogue and facilitating a seamless workflow. The chatchain acts as a facilitator, breaking down each stage into atomic subtasks.This enables dual roles, allowing for proposing and validating solutionsthrough context-aware communication, leading to efficient resolution ofspecific subtasks. The instrumental analysis of ChatDev highlights itsremarkable efficacy in software generation, enabling the completion of theentire software development process in under seven minutes at a cost of lessthan one dollar. It not only identifies and alleviates potentialvulnerabilities but also rectifies potential hallucinations while maintainingcommendable efficiency and cost-effectiveness. The potential of ChatDev unveilsfresh possibilities for integrating LLMs into the realm of softwaredevelopment. Our code is available at https://github.com/OpenBMB/ChatDev.\r2023-12-18\nDo Users Write More Insecure Code with AI Assistants?\nNeil Perry, Megha Srivastava, Deepak Kumar, Dan Boneh\nabstract\rabstract: We conduct the first large-scale user study examining how users interact withan AI Code assistant to solve a variety of security related tasks acrossdifferent programming languages. Overall, we find that participants who hadaccess to an AI assistant based on OpenAI\u0026rsquo;s codex-davinci-002 model wrotesignificantly less secure code than those without access. Additionally,participants with access to an AI assistant were more likely to believe theywrote secure code than those without access to the AI assistant. Furthermore,we find that participants who trusted the AI less and engaged more with thelanguage and format of their prompts (e.g. re-phrasing, adjusting temperature)provided code with fewer security vulnerabilities. Finally, in order to betterinform the design of future AI-based Code assistants, we provide an in-depthanalysis of participants\u0026rsquo; language and interaction behavior, as well as releaseour user interface as an instrument to conduct similar studies in the future.\rPoisonPrompt: Backdoor Attack on Prompt-based Large Language Models\nHongwei Yao, Jian Lou, Zhan Qin\nabstract\rabstract: Prompts have significantly improved the performance of pretrained LargeLanguage Models (LLMs) on various downstream tasks recently, making themincreasingly indispensable for a diverse range of LLM application scenarios.However, the backdoor vulnerability, a serious security threat that canmaliciously alter the victim model\u0026rsquo;s normal predictions, has not beensufficiently explored for prompt-based LLMs. In this paper, we presentPOISONPROMPT, a novel backdoor attack capable of successfully compromising bothhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, androbustness of POISONPROMPT through extensive experiments on three popularprompt methods, using six datasets and three widely used LLMs. Our findingshighlight the potential security threats posed by backdoor attacks onprompt-based LLMs and emphasize the need for further research in this area.\rNo-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models\nShengyao Zhang, Mi Zhang, Xudong Pan, Min Yang\nabstract\rabstract: To reduce the computation cost and the energy consumption in large languagemodels (LLM), skimming-based acceleration dynamically drops unimportant tokensof the input sequence progressively along layers of the LLM while preservingthe tokens of semantic importance. However, our work for the first time revealsthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In thispaper, we propose No-Skim, a general framework to help the owners ofskimming-based LLM to understand and measure the robustness of theiracceleration scheme. Specifically, our framework searches minimal andunnoticeable perturbations at character-level and token-level to generateadversarial inputs that sufficiently increase the remaining token ratio, thusincreasing the computation cost and energy consumption. We systematicallyevaluate the vulnerability of the skimming acceleration in various LLMarchitectures including BERT and RoBERTa on the GLUE benchmark. In the worstcase, the perturbation found by No-Skim substantially increases the runningcost of LLM by over 145% on average. Moreover, No-Skim extends the evaluationframework to various scenarios, making the evaluation conductible withdifferent level of knowledge.\rA Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models\nAysan Esmradi, Daniel Wankit Yip, Chun Fai Chan\nabstract\rabstract: Ensuring the security of large language models (LLMs) is an ongoing challengedespite their widespread popularity. Developers work to enhance LLMs security,but vulnerabilities persist, even in advanced versions like GPT-4. Attackersexploit these weaknesses, highlighting the need for proactive cybersecuritymeasures in AI model development. This article explores two attack categories:attacks on models themselves and attacks on model applications. The formerrequires expertise, access to model data, and significant implementation time,while the latter is more accessible to attackers and has seen increasedattention. Our study reviews over 100 recent research works, providing anin-depth analysis of each attack type. We identify the latest attack methodsand explore various approaches to carry them out. We thoroughly investigatemitigation techniques, assessing their effectiveness and limitations.Furthermore, we summarize future defenses against these attacks. We alsoexamine real-world techniques, including reported and our implemented attackson LLMs, to consolidate our findings. Our research highlights the urgency ofaddressing security concerns and aims to enhance the understanding of LLMattacks, contributing to robust defense development in this evolving domain.\r2023-12-16\nComprehensive Evaluation of ChatGPT Reliability Through Multilingual Inquiries\nPoorna Chander Reddy Puttaparthi, Soham Sanjay Deo, Hakan Gul, Yiming Tang, Weiyi Shang, Zhe Yu\nabstract\rabstract: ChatGPT is currently the most popular large language model (LLM), with over100 million users, making a significant impact on people\u0026rsquo;s lives. However, dueto the presence of jailbreak vulnerabilities, ChatGPT might have negativeeffects on people\u0026rsquo;s lives, potentially even facilitating criminal activities.Testing whether ChatGPT can cause jailbreak is crucial because it can enhanceChatGPT\u0026rsquo;s security, reliability, and social responsibility. Inspired byprevious research revealing the varied performance of LLMs in differentlanguage translations, we suspected that wrapping prompts in multiple languagesmight lead to ChatGPT jailbreak. To investigate this, we designed a study witha fuzzing testing approach to analyzing ChatGPT\u0026rsquo;s cross-linguistic proficiency.Our study includes three strategies by automatically posing different formatsof malicious questions to ChatGPT: (1) each malicious question involving onlyone language, (2) multilingual malicious questions, (3) specifying that ChatGPTresponds in a language different from the prompts. In addition, we also combineour strategies by utilizing prompt injection templates to wrap the threeaforementioned types of questions. We examined a total of 7,892 Q\u0026amp;A datapoints, discovering that multilingual wrapping can indeed lead to ChatGPT\u0026rsquo;sjailbreak, with different wrapping methods having varying effects on jailbreakprobability. Prompt injection can amplify the probability of jailbreak causedby multilingual wrapping. This work provides insights for OpenAI developers toenhance ChatGPT\u0026rsquo;s support for language diversity and inclusion.\r2023-12-14\nLarge Language Models can be Guided to Evade AI-Generated Text Detection\nNing Lu, Shengcai Liu, Rui He, Qi Wang, Yew-Soon Ong, Ke Tang\nabstract\rabstract: Large language models (LLMs) have shown remarkable performance in varioustasks and have been extensively utilized by the public. However, the increasingconcerns regarding the misuse of LLMs, such as plagiarism and spamming, haveled to the development of multiple detectors, including fine-tuned classifiersand statistical methods. In this study, we equip LLMs with prompts, rather thanrelying on an external paraphraser, to evaluate the vulnerability of thesedetectors. We propose a novel Substitution-based In-Context exampleOptimization method (SICO) to automatically construct prompts for evading thedetectors. SICO is cost-efficient as it requires only 40 human-written examplesand a limited number of LLM inferences to generate a prompt. Moreover, once atask-specific prompt has been constructed, it can be universally used against awide range of detectors. Extensive experiments across three real-world tasksdemonstrate that SICO significantly outperforms the paraphraser baselines andenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by0.5 on average. Furthermore, a comprehensive human evaluation as well as avalidation experiment in the wild show that the SICO-generated text achieveshuman-level readability and task completion rates. Finally, the strongperformance of SICO exhibits its potential as a reliable evaluation tool forfuture detectors. The codes and data are located onhttps://github.com/ColinLu50/Evade-GPT-Detector.\r2023-12-13\nToViLaG: Your Visual-Language Generative Model is Also An Evildoer\nXinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie\nabstract\rabstract: Warning: this paper includes model outputs showing offensive content. Recentlarge-scale Visual-Language Generative Models (VLGMs) have achievedunprecedented improvement in multimodal image/text generation. However, thesemodels might also generate toxic content, e.g., offensive text and pornographyimages, raising significant ethical risks. Despite exhaustive studies on toxicdegeneration of language models, this problem remains largely unexplored withinthe context of visual-language generation. This work delves into the propensityfor toxicity generation and susceptibility to toxic data across various VLGMs.For this purpose, we built ToViLaG, a dataset comprising 32Kco-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text thattends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicitymetric tailored to visual-language generation, which theoretically reflectsdifferent aspects of toxicity considering both input and output. On such abasis, we benchmarked the toxicity of a diverse spectrum of VLGMs anddiscovered that some models do more evil than expected while some are morevulnerable to infection, underscoring the necessity of VLGMs detoxification.Therefore, we develop an innovative bottleneck-based detoxification method. Ourmethod could reduce toxicity while maintaining comparable generation quality,providing a promising initial solution to this line of research.\rFigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts\nYichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang\nabstract\rabstract: Ensuring the safety of artificial intelligence-generated content (AIGC) is alongstanding topic in the artificial intelligence (AI) community, and thesafety concerns associated with Large Language Models (LLMs) have been widelyinvestigated. Recently, large vision-language models (VLMs) represent anunprecedented revolution, as they are built upon LLMs but can incorporateadditional modalities (e.g., images). However, the safety of VLMs lackssystematic evaluation, and there may be an overconfidence in the safetyguarantees provided by their underlying LLMs. In this paper, to demonstratethat introducing additional modality modules leads to unforeseen AI safetyissues, we propose FigStep, a straightforward yet effective jailbreakingalgorithm against VLMs. Instead of feeding textual harmful instructionsdirectly, FigStep converts the harmful content into images through typographyto bypass the safety alignment within the textual module of the VLMs, inducingVLMs to output unsafe responses that violate common AI safety policies. In ourevaluation, we manually review 46,500 model responses generated by 3 familiesof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a totalof 6 VLMs). The experimental results show that FigStep can achieve an averageattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, wedemonstrate that the methodology of FigStep can even jailbreak GPT-4V, whichalready leverages an OCR detector to filter harmful queries. Above all, ourwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlightsthe necessity of novel safety alignments between visual and textual modalities.\r2023-12-12\nDeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions\nFangzhou Wu, Xiaogeng Liu, Chaowei Xiao\nabstract\rabstract: With the advancement of Large Language Models (LLMs), significant progresshas been made in code generation, enabling LLMs to transform natural languageinto programming code. These Code LLMs have been widely accepted by massiveusers and organizations. However, a dangerous nature is hidden in the code,which is the existence of fatal vulnerabilities. While some LLM providers haveattempted to address these issues by aligning with human guidance, theseefforts fall short of making Code LLMs practical and robust. Without a deepunderstanding of the performance of the LLMs under the practical worst cases,it would be concerning to apply them to various real-world applications. Inthis paper, we answer the critical issue: Are existing Code LLMs immune togenerating vulnerable code? If not, what is the possible maximum severity ofthis issue in practical deployment scenarios? In this paper, we introduceDeceptPrompt, a novel algorithm that can generate adversarial natural languageinstructions that drive the Code LLMs to generate functionality correct codewith vulnerabilities. DeceptPrompt is achieved through a systematicevolution-based algorithm with a fine grain loss design. The unique advantageof DeceptPrompt enables us to find natural prefix/suffix with totally benignand non-directional semantic meaning, meanwhile, having great power in inducingthe Code LLMs to generate vulnerable code. This feature can enable us toconduct the almost-worstcase red-teaming on these LLMs in a real scenario,where users are using natural language. Our extensive experiments and analyseson DeceptPrompt not only validate the effectiveness of our approach but alsoshed light on the huge weakness of LLMs in the code generation task. Whenapplying the optimized prefix/suffix, the attack success rate (ASR) willimprove by average 50% compared with no prefix/suffix applying.\rSafety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack\nYu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong\nabstract\rabstract: Recent developments in balancing the usefulness and safety of Large LanguageModels (LLMs) have raised a critical question: Are mainstream NLP tasksadequately aligned with safety consideration? Our study, focusing onsafety-sensitive documents obtained through adversarial attacks, revealssignificant disparities in the safety alignment of various NLP tasks. Forinstance, LLMs can effectively summarize malicious long documents but oftenrefuse to translate them. This discrepancy highlights a previously unidentifiedvulnerability: attacks exploiting tasks with weaker safety alignment, likesummarization, can potentially compromise the integraty of tasks traditionallydeemed more robust, such as translation and question-answering (QA). Moreover,the concurrent use of multiple NLP tasks with lesser safety alignment increasesthe risk of LLMs inadvertently processing harmful content. We demonstrate thesevulnerabilities in various safety-aligned LLMs, particularly Llama2 models andGPT-4, indicating an urgent need for strengthening safety alignments across abroad spectrum of NLP tasks.\rJust-in-Time Security Patch Detection \u0026ndash; LLM At the Rescue for Data Augmentation\nXunzhu Tang, Zhenghan Chen, Kisub Kim, Haoye Tian, Saad Ezzini, Jacques Klein\nabstract\rabstract: In the face of growing vulnerabilities found in open-source software, theneed to identify {discreet} security patches has become paramount. The lack ofconsistency in how software providers handle maintenance often leads to therelease of security patches without comprehensive advisories, leaving usersvulnerable to unaddressed security risks. To address this pressing issue, weintroduce a novel security patch detection system, LLMDA, which capitalizes onLarge Language Models (LLMs) and code-text alignment methodologies for patchreview, data enhancement, and feature combination. Within LLMDA, we initiallyutilize LLMs for examining patches and expanding data of PatchDB and SPI-DB,two security patch datasets from recent literature. We then use labeledinstructions to direct our LLMDA, differentiating patches based on securityrelevance. Following this, we apply a PTFormer to merge patches with code,formulating hybrid attributes that encompass both the innate details and theinterconnections between the patches and the code. This distinctive combinationmethod allows our system to capture more insights from the combined context ofpatches and code, hence improving detection precision. Finally, we devise aprobabilistic batch contrastive learning mechanism within batches to augmentthe capability of the our LLMDA in discerning security patches. The resultsreveal that LLMDA significantly surpasses the start of the art techniques indetecting security patches, underscoring its promise in fortifying softwaremaintenance.\r2023-12-10\nOcclusion-based Detection of Trojan-triggering Inputs in Large Language Models of Code\nAftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Mohammad Amin Alipour, Bowen Xu\nabstract\rabstract: Large language models (LLMs) are becoming an integrated part of softwaredevelopment. These models are trained on large datasets for code, where it ishard to verify each data point. Therefore, a potential attack surface can be toinject poisonous data into the training data to make models vulnerable, akatrojaned. It can pose a significant threat by hiding manipulative behaviorsinside models, leading to compromising the integrity of the models indownstream tasks. In this paper, we propose an occlusion-based human-in-the-loop technique,OSeql, to distinguish trojan-triggering inputs of code. The technique is basedon the observation that trojaned neural models of code rely heavily on thetriggering part of input; hence, its removal would change the confidence of themodels in their prediction substantially. Our results suggest that OSeql candetect the triggering inputs with almost 100% recall. We discuss the problem offalse positives and how to address them. These results provide a baseline forfuture studies in this field.\r2023-12-09\nTemporal-Distributed Backdoor Attack Against Video Based Action Recognition\nXi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis\nabstract\rabstract: Deep neural networks (DNNs) have achieved tremendous success in variousapplications including video action recognition, yet remain vulnerable tobackdoor attacks (Trojans). The backdoor-compromised model will mis-classify tothe target class chosen by the attacker when a test instance (from a non-targetclass) is embedded with a specific trigger, while maintaining high accuracy onattack-free instances. Although there are extensive studies on backdoor attacksagainst image data, the susceptibility of video-based systems under backdoorattacks remains largely unexplored. Current studies are direct extensions ofapproaches proposed for image data, e.g., the triggers are independentlyembedded within the frames, which tend to be detectable by existing defenses.In this paper, we introduce a simple yet effective backdoor attack againstvideo data. Our proposed attack, adding perturbations in a transformed domain,plants an imperceptible, temporally distributed trigger across the videoframes, and is shown to be resilient to existing defensive strategies. Theeffectiveness of the proposed attack is demonstrated by extensive experimentswith various well-known models on two video recognition benchmarks, UCF101 andHMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL)dataset. We delve into the impact of several influential factors on ourproposed attack and identify an intriguing effect termed \u0026ldquo;collateral damage\u0026quot;through extensive studies.\r2023-12-08\nExploring the Limits of ChatGPT in Software Security Applications\nFangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang, Ruoyu \u0026ldquo;Fish\u0026rdquo; Wang, Chaowei Xiao\nabstract\rabstract: Large language models (LLMs) have undergone rapid evolution and achievedremarkable results in recent times. OpenAI\u0026rsquo;s ChatGPT, backed by GPT-3.5 orGPT-4, has gained instant popularity due to its strong capability across a widerange of tasks, including natural language tasks, coding, mathematics, andengaging conversations. However, the impacts and limits of such LLMs in systemsecurity domain are less explored. In this paper, we delve into the limits ofLLMs (i.e., ChatGPT) in seven software security applications includingvulnerability detection/repair, debugging, debloating, decompilation, patching,root cause analysis, symbolic execution, and fuzzing. Our exploration revealsthat ChatGPT not only excels at generating code, which is the conventionalapplication of language models, but also demonstrates strong capability inunderstanding user-provided commands in natural languages, reasoning aboutcontrol and data flows within programs, generating complex data structures, andeven decompiling assembly code. Notably, GPT-4 showcases significantimprovements over GPT-3.5 in most security tasks. Also, certain limitations ofChatGPT in security-related tasks are identified, such as its constrainedability to process long code contexts.\r2023-12-07\nForcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks\nShuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie Baracaldo\nabstract\rabstract: Growing applications of large language models (LLMs) trained by a third partyraise serious concerns on the security vulnerability of LLMs.It has beendemonstrated that malicious actors can covertly exploit these vulnerabilitiesin LLMs through poisoning attacks aimed at generating undesirable outputs.While poisoning attacks have received significant attention in the image domain(e.g., object detection), and classification tasks, their implications forgenerative models, particularly in the realm of natural language generation(NLG) tasks, remain poorly understood. To bridge this gap, we perform acomprehensive exploration of various poisoning techniques to assess theireffectiveness across a range of generative tasks. Furthermore, we introduce arange of metrics designed to quantify the success and stealthiness of poisoningattacks specifically tailored to NLG tasks. Through extensive experiments onmultiple NLG tasks, LLMs and datasets, we show that it is possible tosuccessfully poison an LLM during the fine-tuning stage using as little as 1%of the total tuning data samples. Our paper presents the first systematicapproach to comprehend poisoning attacks targeting NLG tasks considering a widerange of triggers and attack settings. We hope our findings will assist the AIsecurity community in devising appropriate defenses against such threats.\rDefending Against Alignment-Breaking Attacks via Robustly Aligned LLM\nBochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen\nabstract\rabstract: Recently, Large Language Models (LLMs) have made significant advancements andare now widely used across various domains. Unfortunately, there has been arising concern that LLMs can be misused to generate harmful or maliciouscontent. Though a line of research has focused on aligning LLMs with humanvalues and preventing them from producing inappropriate content, suchalignments are usually vulnerable and can be bypassed by alignment-breakingattacks via adversarially optimized or handcrafted jailbreaking prompts. Inthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend againstpotential alignment-breaking attacks. RA-LLM can be directly constructed uponan existing aligned LLM with a robust alignment checking function, withoutrequiring any expensive retraining or fine-tuning process of the original LLM.Furthermore, we also provide a theoretical analysis for RA-LLM to verify itseffectiveness in defending against alignment-breaking attacks. Throughreal-world experiments on open-source large language models, we demonstratethat RA-LLM can successfully defend against both state-of-the-art adversarialprompts and popular handcrafted jailbreaking prompts by reducing their attacksuccess rates from nearly 100% to around 10% or less.\r2023-12-05\nLLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications\nBrett Israelsen, Soumalya Sarkar\nabstract\rabstract: Large Language Models have seen rapid progress in capability in recent years;this progress has been accelerating and their capabilities, measured by variousbenchmarks, are beginning to approach those of humans. There is a strong demandto use such models in a wide variety of applications but, due to unresolvedvulnerabilities and limitations, great care needs to be used before applyingthem to intelligence and safety-critical applications. This paper reviewsrecent literature related to LLM assessment and vulnerabilities to synthesizethe current research landscape and to help understand what advances are mostcritical to enable use of of these technologies in intelligence andsafety-critical applications. The vulnerabilities are broken down into tenhigh-level categories and overlaid onto a high-level life cycle of an LLM. Somegeneral categories of mitigations are reviewed.\r2023-12-04\nDeveloping Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection\nToygar Tanyel, Besher Alkurdi, Serkan Ayvaz\nabstract\rabstract: With the proliferation of social media, there has been a sharp increase inoffensive content, particularly targeting vulnerable groups, exacerbatingsocial problems such as hatred, racism, and sexism. Detecting offensivelanguage use is crucial to prevent offensive language from being widely sharedon social media. However, the accurate detection of irony, implication, andvarious forms of hate speech on social media remains a challenge. Naturallanguage-based deep learning models require extensive training with large,comprehensive, and labeled datasets. Unfortunately, manually creating suchdatasets is both costly and error-prone. Additionally, the presence ofhuman-bias in offensive language datasets is a major concern for deep learningmodels. In this paper, we propose a linguistic data augmentation approach toreduce bias in labeling processes, which aims to mitigate the influence ofhuman bias by leveraging the power of machines to improve the accuracy andfairness of labeling processes. This approach has the potential to improveoffensive language classification tasks across multiple languages and reducethe prevalence of offensive content on social media.\r2023-12-01\nExploring the Robustness of Decentralized Training for Large Language Models\nLin Lu, Chenxi Dai, Wangcheng Tao, Binhang Yuan, Yanan Sun, Pan Zhou\nabstract\rabstract: Decentralized training of large language models has emerged as an effectiveway to democratize this technology. However, the potential threats associatedwith this approach have not been carefully discussed, which would hinder thedevelopment of decentralized training infrastructures. This paper aims toinitiate discussion towards this end by exploring the robustness ofdecentralized training from three main perspectives. First, we demonstrate thevulnerabilities inherent in decentralized training frameworks in terms ofhardware, data, and models. Second, we highlight the fundamental differencebetween decentralized foundation model training and vanilla federated learning,where the security techniques employed in federated learning cannot be applieddirectly. Third, we discuss the essential components required for a robust andefficient decentralized training framework and present a case study by modelinga concrete threat model. Our objective in this vision paper is to emphasize theimportance of addressing security concerns in the context of decentralizedtraining for large language models.\r2023-11-29\nLeveraging a Randomized Key Matrix to Enhance the Security of Symmetric Substitution Ciphers\nShubham Gandhi, Om Khare, Mihika Dravid, Mihika Sanghvi, Sunil Mane, Aadesh Gajaralwar, Saloni Gandhi\nabstract\rabstract: An innovative strategy to enhance the security of symmetric substitutionciphers is presented, through the implementation of a randomized key matrixsuitable for various file formats, including but not limited to binary and textfiles. Despite their historical relevance, symmetric substitution ciphers havebeen limited by vulnerabilities to cryptanalytic methods like frequencyanalysis and known plaintext attacks. The aim of our research is to mitigatethese vulnerabilities by employing a polyalphabetic substitution strategy thatincorporates a distinct randomized key matrix. This matrix plays a pivotal rolein generating a unique random key, comprising characters, encompassing bothuppercase and lowercase letters, numeric, and special characters, to derive thecorresponding ciphertext. The effectiveness of the proposed methodology inenhancing the security of conventional substitution methods for file encryptionand decryption is supported by comprehensive testing and analysis, whichencompass computational speed, frequency analysis, keyspace examination,Kasiski test, entropy analysis, and the utilization of a large language model.\rKL-Divergence Guided Temperature Sampling\nChung-Ching Chang, David Reitter, Renat Aksitov, Yun-Hsuan Sung\nabstract\rabstract: Temperature sampling is a conventional approach to diversify large languagemodel predictions. As temperature increases, the prediction becomes diverse butalso vulnerable to hallucinations \u0026ndash; generating tokens that are sensible butnot factual. One common approach to mitigate hallucinations is to providesource/grounding documents and the model is trained to produce predictions thatbind to and are attributable to the provided source. It appears that there is atrade-off between diversity and attribution. To mitigate any such trade-off, wepropose to relax the constraint of having a fixed temperature over decodingsteps, and a mechanism to guide the dynamic temperature according to itsrelevance to the source through KL-divergence. Our experiments justifies thetrade-off, and shows that our sampling algorithm outperforms the conventionaltop-k and top-p algorithms in conversational question-answering andsummarization tasks.\rSmoothLLM: Defending Large Language Models Against Jailbreaking Attacks\nAlexander Robey, Eric Wong, Hamed Hassani, George J. Pappas\nabstract\rabstract: Despite efforts to align large language models (LLMs) with human values,widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible tojailbreaking attacks, wherein an adversary fools a targeted LLM into generatingobjectionable content. To address this vulnerability, we propose SmoothLLM, thefirst algorithm designed to mitigate jailbreaking attacks on LLMs. Based on ourfinding that adversarially-generated prompts are brittle to character-levelchanges, our defense first randomly perturbs multiple copies of a given inputprompt, and then aggregates the corresponding predictions to detect adversarialinputs. SmoothLLM reduces the attack success rate on numerous popular LLMs tobelow one percentage point, avoids unnecessary conservatism, and admitsprovable guarantees on attack mitigation. Moreover, our defense usesexponentially fewer queries than existing attacks and is compatible with anyLLM. Our code is publicly available at the following link:https://github.com/arobey1/smooth-llm.\rIdentifying and Mitigating Vulnerabilities in LLM-Integrated Applications\nFengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, Radha Poovendran\nabstract\rabstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users\u0026rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user\u0026rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.\r2023-11-28\nZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health Information Ecosystem\nKhalid Al-hammuri, Fayez Gebali, Awos Kanan\nabstract\rabstract: Managing access between large numbers of distributed medical devices hasbecome a crucial aspect of modern healthcare systems, enabling theestablishment of smart hospitals and telehealth infrastructure. However, astelehealth technology continues to evolve and Internet of Things (IoT) devicesbecome more widely used, they are also becoming increasingly exposed to varioustypes of vulnerabilities and medical errors. In healthcare information systems,about 90% of vulnerabilities emerged from misuse cases and human errors. As aresult, there is a need for additional research and development of securitytools to prevent such attacks. This article proposes a zero-trust-basedcontext-aware framework for managing access to the main components of the cloudecosystem, including users, devices and output data. The main goal and benefitof the proposed framework is to build a scoring system to prevent or alleviatemisuse cases while using distributed medical devices in cloud-based healthcareinformation systems. The framework has two main scoring schemas to maintain thechain of trust. First, it proposes a critical trust score based on cloud-nativemicro-services of authentication, encryption, logging, and authorizations.Second, creating a bond trust scoring to assess the real-time semantic andsyntactic analysis of attributes stored in a healthcare information system. Theanalysis is based on a pre-trained machine learning model to generate thesemantic and syntactic scores. The framework also takes into account regulatorycompliance and user consent to create a scoring system. The advantage of thismethod is that it is applicable to any language and adapts to all attributes asit relies on a language model, not just a set of predefined and limitedattributes. The results show a high F1 score of 93.5%, which proves that it isvalid for detecting misuse cases.\r2023-11-25\nEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\nZekun Li, Baolin Peng, Pengcheng He, Xifeng Yan\nabstract\rabstract: Large Language Models (LLMs) have demonstrated exceptional proficiency ininstruction-following, becoming increasingly crucial across variousapplications. However, this capability brings with it the risk of promptinjection attacks, where attackers inject instructions into LLMs\u0026rsquo; input toelicit undesirable actions or content. Understanding the robustness of LLMsagainst such attacks is vital for their safe implementation. In this work, weestablish a benchmark to evaluate the robustness of instruction-following LLMsagainst prompt injection attacks. Our objective is to determine the extent towhich LLMs can be influenced by injected instructions and their ability todifferentiate between these injected and original target instructions. Throughextensive experiments with leading instruction-following LLMs, we uncoversignificant vulnerabilities in their robustness to such attacks. Our resultsindicate that some models are overly tuned to follow any embedded instructionsin the prompt, overly focusing on the latter parts of the prompt without fullygrasping the entire context. By contrast, models with a better grasp of thecontext and instruction-following capabilities will potentially be moresusceptible to compromise by injected instructions. This underscores the needto shift the focus from merely enhancing LLMs\u0026rsquo; instruction-followingcapabilities to improving their overall comprehension of prompts anddiscernment of instructions that are appropriate to follow. We hope ourin-depth analysis offers insights into the underlying causes of thesevulnerabilities, aiding in the development of future solutions. Code and dataare available athttps://github.com/Leezekun/instruction-following-robustness-eval\r2023-11-24\nScalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation\nRusheb Shah, Quentin Feuillade\u0026ndash;Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando\nabstract\rabstract: Despite efforts to align large language models to produce harmless responses,they are still vulnerable to jailbreak prompts that elicit unrestrictedbehaviour. In this work, we investigate persona modulation as a black-boxjailbreaking method to steer a target model to take on personalities that arewilling to comply with harmful instructions. Rather than manually craftingprompts for each persona, we automate the generation of jailbreaks using alanguage model assistant. We demonstrate a range of harmful completions madepossible by persona modulation, including detailed instructions forsynthesising methamphetamine, building a bomb, and laundering money. Theseautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is185 times larger than before modulation (0.23%). These prompts also transfer toClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,respectively. Our work reveals yet another vulnerability in commercial largelanguage models and highlights the need for more comprehensive safeguards.\rBackdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment\nHaoran Wang, Kai Shu\nabstract\rabstract: To ensure AI safety, instruction-tuned Large Language Models (LLMs) arespecifically trained to ensure alignment, which refers to making models behavein accordance with human intentions. While these models have demonstratedcommendable results on various safety benchmarks, the vulnerability of theirsafety alignment has not been extensively studied. This is particularlytroubling given the potential harm that LLMs can inflict. Existing attackmethods on LLMs often rely on poisoned training data or the injection ofmalicious prompts. These approaches compromise the stealthiness andgeneralizability of the attacks, making them susceptible to detection.Additionally, these models often demand substantial computational resources forimplementation, making them less practical for real-world applications.Inspired by recent success in modifying model behavior through steering vectorswithout the need for optimization, and drawing on its effectiveness inred-teaming LLMs, we conducted experiments employing activation steering totarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness- across a varied set of attack settings. To establish a universal attackstrategy applicable to diverse target alignments without depending on manualanalysis, we automatically select the intervention layer based on contrastivelayer search. Our experiment results show that activation attacks are highlyeffective and add little or no overhead to attack efficiency. Additionally, wediscuss potential countermeasures against such activation attacks. Our code anddata are available at https://github.com/wang2226/Backdoor-Activation-AttackWarning: this paper contains content that can be offensive or upsetting.\r2023-11-22\nTransfer Attacks and Defenses for Large Language Models on Coding Tasks\nChi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina Pasareanu\nabstract\rabstract: Modern large language models (LLMs), such as ChatGPT, have demonstratedimpressive capabilities for coding tasks including writing and reasoning aboutcode. They improve upon previous neural network models of code, such ascode2seq or seq2seq, that already demonstrated competitive results whenperforming tasks such as code summarization and identifying codevulnerabilities. However, these previous code models were shown vulnerable toadversarial examples, i.e. small syntactic perturbations that do not change theprogram\u0026rsquo;s semantics, such as the inclusion of \u0026ldquo;dead code\u0026rdquo; through falseconditions or the addition of inconsequential print statements, designed to\u0026quot;fool\u0026quot; the models. LLMs can also be vulnerable to the same adversarialperturbations but a detailed study on this concern has been lacking so far. Inthis paper we aim to investigate the effect of adversarial perturbations oncoding tasks with LLMs. In particular, we study the transferability ofadversarial examples, generated through white-box attacks on smaller codemodels, to LLMs. Furthermore, to make the LLMs more robust against suchadversaries without incurring the cost of retraining, we propose prompt-baseddefenses that involve modifying the prompt to include additional informationsuch as examples of adversarially perturbed code and explicit instructions forreversing adversarial perturbations. Our experiments show that adversarialexamples obtained with a smaller code model are indeed transferable, weakeningthe LLMs\u0026rsquo; performance. The proposed defenses show promise in improving themodel\u0026rsquo;s resilience, paving the way to more robust defensive solutions for LLMsin code-related applications.\r2023-11-21\nOpen Sesame! Universal Black Box Jailbreaking of Large Language Models\nRaz Lapid, Ron Langberg, Moshe Sipper\nabstract\rabstract: Large language models (LLMs), designed to provide helpful and safe responses,often rely on alignment techniques to align with user intent and socialguidelines. Unfortunately, this alignment can be exploited by malicious actorsseeking to manipulate an LLM\u0026rsquo;s outputs for unintended purposes. In this paperwe introduce a novel approach that employs a genetic algorithm (GA) tomanipulate LLMs when model architecture and parameters are inaccessible. The GAattack works by optimizing a universal adversarial prompt that \u0026ndash; when combinedwith a user\u0026rsquo;s query \u0026ndash; disrupts the attacked model\u0026rsquo;s alignment, resulting inunintended and potentially harmful outputs. Our novel approach systematicallyreveals a model\u0026rsquo;s limitations and vulnerabilities by uncovering instances whereits responses deviate from expected behavior. Through extensive experiments wedemonstrate the efficacy of our technique, thus contributing to the ongoingdiscussion on responsible AI development by providing a diagnostic tool forevaluating and enhancing alignment of LLMs with human intent. To our knowledgethis is the first automated universal black box jailbreak attack.\r2023-11-20\nGenerating Valid and Natural Adversarial Examples with Large Language Models\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen\nabstract\rabstract: Deep learning-based natural language processing (NLP) models, particularlypre-trained language models (PLMs), have been revealed to be vulnerable toadversarial attacks. However, the adversarial examples generated by manymainstream word-level adversarial attack models are neither valid nor natural,leading to the loss of semantic maintenance, grammaticality, and humanimperceptibility. Based on the exceptional capacity of language understandingand generation of large language models (LLMs), we propose LLM-Attack, whichaims at generating both valid and natural adversarial examples with LLMs. Themethod consists of two stages: word importance ranking (which searches for themost vulnerable words) and word synonym replacement (which substitutes themwith their synonyms obtained from LLMs). Experimental results on the MovieReview (MR), IMDB, and Yelp Review Polarity datasets against the baselineadversarial attack models illustrate the effectiveness of LLM-Attack, and itoutperforms the baselines in human and GPT-4 evaluation by a significantmargin. The model can generate adversarial examples that are typically validand natural, with the preservation of semantic meaning, grammaticality, andhuman imperceptibility.\rBeyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems\nGuangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan\nabstract\rabstract: Artificial Intelligence (AI) systems such as autonomous vehicles, facialrecognition, and speech recognition systems are increasingly integrated intoour daily lives. However, despite their utility, these AI systems arevulnerable to a wide range of attacks such as adversarial, backdoor, datapoisoning, membership inference, model inversion, and model stealing attacks.In particular, numerous attacks are designed to target a particular model orsystem, yet their effects can spread to additional targets, referred to astransferable attacks. Although considerable efforts have been directed towarddeveloping transferable attacks, a holistic understanding of the advancementsin transferable attacks remains elusive. In this paper, we comprehensivelyexplore learning-based attacks from the perspective of transferability,particularly within the context of cyber-physical security. We delve intodifferent domains \u0026ndash; the image, text, graph, audio, and video domains \u0026ndash; tohighlight the ubiquitous and pervasive nature of transferable attacks. Thispaper categorizes and reviews the architecture of existing attacks from variousviewpoints: data, process, model, and system. We further examine theimplications of transferable attacks in practical scenarios such as autonomousdriving, speech recognition, and large language models (LLMs). Additionally, weoutline the potential research directions to encourage efforts in exploring thelandscape of transferable attacks. This survey offers a holistic understandingof the prevailing transferable attacks and their impacts across differentdomains.\r2023-11-18\nAssessing the Security of GitHub Copilot Generated Code \u0026ndash; A Targeted Replication Study\nVahid Majdinasab, Michael Joshua Bishop, Shawn Rasheed, Arghavan Moradidakhel, Amjed Tahir, Foutse Khomh\nabstract\rabstract: AI-powered code generation models have been developing rapidly, allowingdevelopers to expedite code generation and thus improve their productivity.These models are trained on large corpora of code (primarily sourced frompublic repositories), which may contain bugs and vulnerabilities. Severalconcerns have been raised about the security of the code generated by thesemodels. Recent studies have investigated security issues in AI-powered codegeneration tools such as GitHub Copilot and Amazon CodeWhisperer, revealingseveral security weaknesses in the code generated by these tools. As thesetools evolve, it is expected that they will improve their security protocols toprevent the suggestion of insecure code to developers. This paper replicatesthe study of Pearce et al., which investigated security weaknesses in Copilotand uncovered several weaknesses in the code suggested by Copilot acrossdiverse scenarios and languages (Python, C and Verilog). Our replicationexamines Copilot security weaknesses using newer versions of Copilot and CodeQL(the security analysis framework). The replication focused on the presence ofsecurity vulnerabilities in Python code. Our results indicate that, even withthe improvements in newer versions of Copilot, the percentage of vulnerablecode suggestions has reduced from 36.54% to 27.25%. Nonetheless, it remainsevident that the model still suggests insecure code.\r2023-11-16\nUnderstanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities\nAvishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, Mayur Naik\nabstract\rabstract: Security vulnerabilities in modern software are prevalent and harmful. Whileautomated vulnerability detection tools have made promising progress, theirscalability and applicability remain challenging. Recently, Large LanguageModels (LLMs), such as GPT-4 and CodeLlama, have demonstrated remarkableperformance on code-related tasks. However, it is unknown whether such LLMs cando complex reasoning over code. In this work, we explore whether pre-trainedLLMs can detect security vulnerabilities and address the limitations ofexisting tools. We evaluate the effectiveness of pre-trained LLMs on a set offive diverse security benchmarks spanning two languages, Java and C/C++, andincluding code samples from synthetic and real-world projects. We evaluate theeffectiveness of LLMs in terms of their performance, explainability, androbustness. By designing a series of effective prompting strategies, we obtain the bestresults on the synthetic datasets with GPT-4: F1 scores of 0.79 on OWASP, 0.86on Juliet Java, and 0.89 on Juliet C/C++. Expectedly, the performance of LLMsdrops on the more challenging real-world datasets: CVEFixes Java and CVEFixesC/C++, with GPT-4 reporting F1 scores of 0.48 and 0.62, respectively. We showthat LLMs can often perform better than existing static analysis and deeplearning-based vulnerability detection tools, especially for certain classes ofvulnerabilities. Moreover, LLMs also often provide reliable explanations,identifying the vulnerable data flows in code. We find that fine-tuning smallerLLMs can outperform the larger LLMs on synthetic datasets but provide limitedgains on real-world datasets. When subjected to adversarial attacks on code,LLMs show mild degradation, with average accuracy reduction of up to 12.67%.Finally, we share our insights and recommendations for future work onleveraging LLMs for vulnerability detection.\rOn the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models\nJiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao\nabstract\rabstract: Reinforcement Learning with Human Feedback (RLHF) is a methodology designedto align Large Language Models (LLMs) with human preferences, playing animportant role in LLMs alignment. Despite its advantages, RLHF relies on humanannotators to rank the text, which can introduce potential securityvulnerabilities if any adversarial annotator (i.e., attackers) manipulates theranking score by up-ranking any malicious text to steer the LLM adversarially.To assess the red-teaming of RLHF against human preference data poisoning, wepropose RankPoison, a poisoning attack method on candidates\u0026rsquo; selection ofpreference rank flipping to reach certain malicious behaviors (e.g., generatinglonger sequences, which can increase the computational cost). With poisoneddataset generated by RankPoison, we can perform poisoning attacks on LLMs togenerate longer tokens without hurting the original safety alignmentperformance. Moreover, applying RankPoison, we also successfully implement abackdoor attack where LLMs can generate longer answers under questions with thetrigger word. Our findings highlight critical security challenges in RLHF,underscoring the necessity for more robust alignment methods for LLMs.\r2023-11-15\nTowards Verifiable Text Generation with Symbolic References\nLucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim\nabstract\rabstract: Large language models (LLMs) have demonstrated an impressive ability tosynthesize plausible and fluent text. However they remain vulnerable tohallucinations, and thus their outputs generally require manual humanverification for high-stakes applications, which can be time-consuming anddifficult. This paper proposes symbolically grounded generation (SymGen) as asimple approach for enabling easier validation of an LLM\u0026rsquo;s output. SymGenprompts an LLM to interleave its regular output text with explicit symbolicreferences to fields present in some conditioning data (e.g., a table in JSONformat). The references can be used to display the provenance of differentspans of text in the generation, reducing the effort required for manualverification. Across data-to-text and question answering experiments, we findthat LLMs are able to directly output text that makes use of symbolicreferences while maintaining fluency and accuracy.\rStealthy and Persistent Unalignment on Large Language Models via Backdoor Injections\nYuanpu Cao, Bochuan Cao, Jinghui Chen\nabstract\rabstract: Recent developments in Large Language Models (LLMs) have manifestedsignificant advancements. To facilitate safeguards against maliciousexploitation, a body of research has concentrated on aligning LLMs with humanpreferences and inhibiting their generation of inappropriate content.Unfortunately, such alignments are often vulnerable: fine-tuning with a minimalamount of harmful data can easily unalign the target LLM. While beingeffective, such fine-tuning-based unalignment approaches also have their ownlimitations: (1) non-stealthiness, after fine-tuning, safety audits orred-teaming can easily expose the potential weaknesses of the unaligned models,thereby precluding their release/use. (2) non-persistence, the unaligned LLMscan be easily repaired through re-alignment, i.e., fine-tuning again withaligned data points. In this work, we show that it is possible to conductstealthy and persistent unalignment on large language models via backdoorinjections. We also provide a novel understanding on the relationship betweenthe backdoor persistence and the activation pattern and further provideguidelines for potential trigger design. Through extensive experiments, wedemonstrate that our proposed stealthy and persistent unalignment cansuccessfully pass the safety evaluation while maintaining strong persistenceagainst re-alignment defense.\rHow Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\nLingbo Mo, Boshi Wang, Muhao Chen, Huan Sun\nabstract\rabstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.\r2023-11-14\nAlignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective\nZi Yin, Wei Ding, Jia Liu\nabstract\rabstract: Large Language Models (LLMs) are central to a multitude of applications butstruggle with significant risks, notably in generating harmful content andbiases. Drawing an analogy to the human psyche\u0026rsquo;s conflict between evolutionarysurvival instincts and societal norm adherence elucidated in Freud\u0026rsquo;spsychoanalysis theory, we argue that LLMs suffer a similar fundamentalconflict, arising between their inherent desire for syntactic and semanticcontinuity, established during the pre-training phase, and the post-trainingalignment with human values. This conflict renders LLMs vulnerable toadversarial attacks, wherein intensifying the models\u0026rsquo; desire for continuity cancircumvent alignment efforts, resulting in the generation of harmfulinformation. Through a series of experiments, we first validated the existenceof the desire for continuity in LLMs, and further devised a straightforward yetpowerful technique, such as incomplete sentences, negative priming, andcognitive dissonance scenarios, to demonstrate that even advanced LLMs struggleto prevent the generation of harmful information. In summary, our studyuncovers the root of LLMs\u0026rsquo; vulnerabilities to adversarial attacks, herebyquestioning the efficacy of solely relying on sophisticated alignment methods,and further advocates for a new training idea that integrates modal conceptsalongside traditional amodal concepts, aiming to endow LLMs with a more nuancedunderstanding of real-world contexts and ethical considerations.\r2023-11-12\nFlames: Benchmarking Value Alignment of Chinese Large Language Models\nKexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua Lin\nabstract\rabstract: The widespread adoption of large language models (LLMs) across variousregions underscores the urgent need to evaluate their alignment with humanvalues. Current benchmarks, however, fall short of effectively uncoveringsafety vulnerabilities in LLMs. Despite numerous models achieving high scoresand \u0026rsquo;topping the chart\u0026rsquo; in these evaluations, there is still a significant gapin LLMs\u0026rsquo; deeper alignment with human values and achieving genuine harmlessness.To this end, this paper proposes the first highly adversarial benchmark namedFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responseswith fine-grained annotations, and a specified scorer. Our frameworkencompasses both common harmlessness principles, such as fairness, safety,legality, and data protection, and a unique morality dimension that integratesspecific Chinese values such as harmony. Based on the framework, we carefullydesign adversarial prompts that incorporate complex scenarios and jailbreakingmethods, mostly with implicit malice. By prompting mainstream LLMs with suchadversarially constructed prompts, we obtain model responses, which are thenrigorously annotated for evaluation. Our findings indicate that all theevaluated LLMs demonstrate relatively poor performance on Flames, particularlyin the safety and fairness dimensions. Claude emerges as the best-performingmodel overall, but with its harmless rate being only 63.08% while GPT-4 onlyscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,setting a new challenge for contemporary LLMs and highlighting the need forfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,we develop a specified scorer capable of scoring LLMs across multipledimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publiclyavailable on https://github.com/AIFlames/Flames.\r2023-11-10\nWatermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service\nYuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu\nabstract\rabstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at https://github.com/Pter61/vlpmarker.\r2023-11-08\nBackdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review\nPengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, Gongshen Liu\nabstract\rabstract: Applicating third-party data and models has become a new paradigm forlanguage modeling in NLP, which also introduces some potential securityvulnerabilities because attackers can manipulate the training process and datasource. In this case, backdoor attacks can induce the model to exhibit expectedbehaviors through specific triggers and have little inferior influence onprimitive tasks. Hence, it could have dire consequences, especially consideringthat the backdoor attack surfaces are broad. However, there is still no systematic and comprehensive review to reflect thesecurity challenges, attacker\u0026rsquo;s capabilities, and purposes according to theattack surface. Moreover, there is a shortage of analysis and comparison of thediverse emerging backdoor countermeasures in this context. In this paper, weconduct a timely review of backdoor attacks and countermeasures to sound thered alarm for the NLP security community. According to the affected stage ofthe machine learning pipeline, the attack surfaces are recognized to be wideand then formalized into three categorizations: attacking pre-trained modelwith fine-tuning (APMF) or parameter-efficient tuning (APMP), and attackingfinal model with training (AFMT). Thus, attacks under each categorization arecombed. The countermeasures are categorized into two general classes: sampleinspection and model inspection. Overall, the research on the defense side isfar behind the attack side, and there is no single defense that can prevent alltypes of backdoor attacks. An attacker can intelligently bypass existingdefenses with a more invisible attack. Drawing the insights from the systematicreview, we also present crucial areas for future research on the backdoor, suchas empirical security evaluations on large language models, and in particular,more efficient and practical countermeasures are solicited.\r2023-11-07\nUnveiling Safety Vulnerabilities of Large Language Models\nGeorge Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi\nabstract\rabstract: As large language models become more prevalent, their possible harmful orinappropriate responses are a cause for concern. This paper introduces a uniquedataset containing adversarial examples in the form of questions, which we callAttaQ, designed to provoke such harmful or inappropriate responses. We assessthe efficacy of our dataset by analyzing the vulnerabilities of various modelswhen subjected to it. Additionally, we introduce a novel automatic approach foridentifying and naming vulnerable semantic regions - input semantic areas forwhich the model is likely to produce harmful outputs. This is achieved throughthe application of specialized clustering techniques that consider both thesemantic similarity of the input attacks and the harmfulness of the model\u0026rsquo;sresponses. Automatically identifying vulnerable semantic regions enhances theevaluation of model weaknesses, facilitating targeted improvements to itssafety mechanisms and overall reliability.\rDo Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection\nBenjamin Steenhoek, Md Mahbubur Rahman, Shaila Sharmin, Wei Le\nabstract\rabstract: Recently, pretrained language models have shown state-of-the-art performanceon the vulnerability detection task. These models are pretrained on a largecorpus of source code, then fine-tuned on a smaller supervised vulnerabilitydataset. Due to the different training objectives and the performance of themodels, it is interesting to consider whether the models have learned thesemantics of code relevant to vulnerability detection, namely bug semantics,and if so, how the alignment to bug semantics relates to model performance. Inthis paper, we analyze the models using three distinct methods:interpretability tools, attention analysis, and interaction matrix analysis. Wecompare the models\u0026rsquo; influential feature sets with the bug semantic featureswhich define the causes of bugs, including buggy paths and PotentiallyVulnerable Statements (PVS). We find that (1) better-performing models alsoaligned better with PVS, (2) the models failed to align strongly to PVS, and(3) the models failed to align at all to buggy paths. Based on our analysis, wedeveloped two annotation methods which highlight the bug semantics inside themodel\u0026rsquo;s inputs. We evaluated our approach on four distinct transformer modelsand four vulnerability datasets and found that our annotations improved themodels\u0026rsquo; performance in the majority of settings - 11 out of 16, with up to 9.57points improvement in F1 score compared to conventional fine-tuning. We furtherfound that with our annotations, the models aligned up to 232% better topotentially vulnerable statements. Our findings indicate that it is helpful toprovide the model with information of the bug semantics, that the model canattend to it, and motivate future work in learning more complex path-based bugsemantics. Our code and data are available athttps://figshare.com/s/4a16a528d6874aad51a0.\r2023-11-03\nThe Alignment Problem in Context\nRaphaël Millière\nabstract\rabstract: A core challenge in the development of increasingly capable AI systems is tomake them safe and reliable by ensuring their behaviour is consistent withhuman values. This challenge, known as the alignment problem, does not merelyapply to hypothetical future AI systems that may pose catastrophic risks; italready applies to current systems, such as large language models, whosepotential for harm is rapidly increasing. In this paper, I assess whether weare on track to solve the alignment problem for large language models, and whatthat means for the safety of future AI systems. I argue that existingstrategies for alignment are insufficient, because large language models remainvulnerable to adversarial attacks that can reliably elicit unsafe behaviour. Ioffer an explanation of this lingering vulnerability on which it is not simplya contingent limitation of current language models, but has deep technical tiesto a crucial aspect of what makes these models useful and versatile in thefirst place \u0026ndash; namely, their remarkable aptitude to learn \u0026ldquo;in context\u0026rdquo; directlyfrom user instructions. It follows that the alignment problem is not onlyunsolved for current AI systems, but may be intrinsically difficult to solvewithout severely undermining their capabilities. Furthermore, this assessmentraises concerns about the prospect of ensuring the safety of future and morecapable AI systems.\r2023-11-02\nTensor Trust: Interpretable Prompt Injection Attacks from an Online Game\nSam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell\nabstract\rabstract: While Large Language Models (LLMs) are increasingly being used in real-worldapplications, they remain vulnerable to prompt injection attacks: maliciousthird party prompts that subvert the intent of the system designer. To helpresearchers study this problem, we present a dataset of over 126,000 promptinjection attacks and 46,000 prompt-based \u0026ldquo;defenses\u0026rdquo; against prompt injection,all created by players of an online game called Tensor Trust. To the best ofour knowledge, this is currently the largest dataset of human-generatedadversarial examples for instruction-following LLMs. The attacks in our datasethave a lot of easily interpretable stucture, and shed light on the weaknessesof LLMs. We also use the dataset to create a benchmark for resistance to twotypes of prompt injection, which we refer to as prompt extraction and prompthijacking. Our benchmark results show that many models are vulnerable to theattack strategies in the Tensor Trust dataset. Furthermore, we show that someattack strategies from the dataset generalize to deployed LLM-basedapplications, even though they have a very different set of constraints to thegame. We release all data and source code at https://tensortrust.ai/paper\r2023-11-01\nGenerate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code\nMohammed Latif Siddiq, Joanna C. S. Santos\nabstract\rabstract: With the growing popularity of Large Language Models (e.g. GitHub Copilot,ChatGPT, etc.) in software engineers\u0026rsquo; daily practices, it is important toensure that the code generated by these tools is not only functionally correctbut also free of vulnerabilities. Although LLMs can help developers to be moreproductive, prior empirical studies have shown that LLMs can generate insecurecode. There are two contributing factors to the insecure code generation.First, existing datasets used to evaluate Large Language Models (LLMs) do notadequately represent genuine software engineering tasks sensitive to security.Instead, they are often based on competitive programming challenges orclassroom-type coding tasks. In real-world applications, the code produced isintegrated into larger codebases, introducing potential security risks. There\u0026rsquo;sa clear absence of benchmarks that focus on evaluating the security of thegenerated code. Second, existing evaluation metrics primarily focus on thefunctional correctness of the generated code while ignoring securityconsiderations. Metrics such as pass@k gauge the probability of obtaining thecorrect code in the top k suggestions. Other popular metrics like BLEU,CodeBLEU, ROUGE, and METEOR similarly emphasize functional accuracy, neglectingsecurity implications. In light of these research gaps, in this paper, wedescribed SALLM, a framework to benchmark LLMs\u0026rsquo; abilities to generate securecode systematically. This framework has three major components: a novel datasetof security-centric Python prompts, an evaluation environment to test thegenerated code, and novel metrics to evaluate the models\u0026rsquo; performance from theperspective of secure code generation.\r2023-10-31\nRobust Safety Classifier for Large Language Models: Adversarial Prompt Shield\nJinhwa Kim, Ali Derakhshan, Ian G. Harris\nabstract\rabstract: Large Language Models\u0026rsquo; safety remains a critical concern due to theirvulnerability to adversarial attacks, which can prompt these systems to produceharmful responses. In the heart of these systems lies a safety classifier, acomputational model trained to discern and mitigate potentially harmful,offensive, or unethical outputs. However, contemporary safety classifiers,despite their potential, often fail when exposed to inputs infused withadversarial noise. In response, our study introduces the Adversarial PromptShield (APS), a lightweight model that excels in detection accuracy anddemonstrates resilience against adversarial prompts. Additionally, we proposenovel strategies for autonomously generating adversarial training datasets,named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets aredesigned to fortify the safety classifier\u0026rsquo;s robustness, and we investigate theconsequences of incorporating adversarial examples into the training process.Through evaluations involving Large Language Models, we demonstrate that ourclassifier has the potential to decrease the attack success rate resulting fromadversarial attacks by up to 60%. This advancement paves the way for the nextgeneration of more reliable and resilient conversational agents.\r2023-10-29\nOn Evaluating Adversarial Robustness of Large Vision-Language Models\nYunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin\nabstract\rabstract: Large vision-language models (VLMs) such as GPT-4 have achieved unprecedentedperformance in response generation, especially with visual inputs, enablingmore creative and adaptable interaction than large language models such asChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, sinceadversaries may successfully evade the entire system by subtly manipulating themost vulnerable modality (e.g., vision). To this end, we propose evaluating therobustness of open-source large VLMs in the most realistic and high-risksetting, where adversaries have only black-box system access and seek todeceive the model into returning the targeted responses. In particular, wefirst craft targeted adversarial examples against pretrained models such asCLIP and BLIP, and then transfer these adversarial examples to other VLMs suchas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, weobserve that black-box queries on these VLMs can further improve theeffectiveness of targeted evasion, resulting in a surprisingly high successrate for generating targeted responses. Our findings provide a quantitativeunderstanding regarding the adversarial vulnerability of large VLMs and callfor a more thorough examination of their potential security flaws beforedeployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.\r2023-10-28\nProbing LLMs for hate speech detection: strengths and vulnerabilities\nSarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha\nabstract\rabstract: Recently efforts have been made by social media platforms as well asresearchers to detect hateful or toxic language using large language models.However, none of these works aim to use explanation, additional context andvictim community information in the detection process. We utilise differentprompt variation, input information and evaluate large language models in zeroshot setting (without adding any in-context examples). We select three largelanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -HateXplain, implicit hate and ToxicSpans. We find that on average including thetarget information in the pipeline improves the model performance substantially(~20-30%) over the baseline across the datasets. There is also a considerableeffect of adding the rationales/explanations into the pipeline (~10-20%) overthe baseline across the datasets. In addition, we further provide a typology ofthe error cases where these large language models fail to (i) classify and (ii)explain the reason for the decisions they take. Such vulnerable pointsautomatically constitute \u0026lsquo;jailbreak\u0026rsquo; prompts for these models and industryscale safeguard techniques need to be developed to make the models robustagainst such prompts.\rAssessing and Improving Syntactic Adversarial Robustness of Pre-trained Models for Code Translation\nGuang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen, Tingting Han, Taolue Chen\nabstract\rabstract: Context: Pre-trained models (PTMs) have demonstrated significant potential inautomatic code translation. However, the vulnerability of these models intranslation tasks, particularly in terms of syntax, has not been extensivelyinvestigated. Objective: To fill this gap, our study aims to propose a novelapproach CoTR to assess and improve the syntactic adversarial robustness ofPTMs in code translation. Method: CoTR consists of two components: CoTR-A andCoTR-D. CoTR-A generates adversarial examples by transforming programs, whileCoTR-D proposes a semantic distance-based sampling data augmentation method andadversarial training method to improve the model\u0026rsquo;s robustness andgeneralization capabilities. The Pass@1 metric is used by CoTR to assess theperformance of PTMs, which is more suitable for code translation tasks andoffers a more precise evaluation in real world scenarios. Results: Theeffectiveness of CoTR is evaluated through experiments on real world Java toPython datasets. The results demonstrate that CoTR-A can significantly reducethe performance of existing PTMs, while CoTR-D effectively improves therobustness of PTMs. Conclusion: Our study identifies the limitations of currentPTMs, including large language models, in code translation tasks. It highlightsthe potential of CoTR as an effective solution to enhance the robustness ofPTMs for code translation tasks.\r2023-10-27\nParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP\nLu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, Xiangyu Zhang\nabstract\rabstract: Backdoor attacks have emerged as a prominent threat to natural languageprocessing (NLP) models, where the presence of specific triggers in the inputcan lead poisoned models to misclassify these inputs to predetermined targetclasses. Current detection mechanisms are limited by their inability to addressmore covert backdoor strategies, such as style-based attacks. In this work, wepropose an innovative test-time poisoned sample detection framework that hingeson the interpretability of model predictions, grounded in the semantic meaningof inputs. We contend that triggers (e.g., infrequent words) are not supposedto fundamentally alter the underlying semantic meanings of poisoned samples asthey want to stay stealthy. Based on this observation, we hypothesize thatwhile the model\u0026rsquo;s predictions for paraphrased clean samples should remainstable, predictions for poisoned samples should revert to their true labelsupon the mutations applied to triggers during the paraphrasing process. Weemploy ChatGPT, a state-of-the-art large language model, as our paraphraser andformulate the trigger-removal task as a prompt engineering problem. We adoptfuzzing, a technique commonly used for unearthing software vulnerabilities, todiscover optimal paraphrase prompts that can effectively eliminate triggerswhile concurrently maintaining input semantics. Experiments on 4 types ofbackdoor attacks, including the subtle style backdoors, and 4 distinct datasetsdemonstrate that our approach surpasses baseline methods, including STRIP, RAP,and ONION, in precision and recall.\r2023-10-25\nEnhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation\nJiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai\nabstract\rabstract: Large language models (LLMs) have brought significant advancements to codegeneration, benefiting both novice and experienced developers. However, theirtraining using unsanitized data from open-source repositories, like GitHub,introduces the risk of inadvertently propagating security vulnerabilities. Toeffectively mitigate this concern, this paper presents a comprehensive studyfocused on evaluating and enhancing code LLMs from a software securityperspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded assupplemental material and will be made publicly available after publication.},a meticulously curated dataset targeting 21 critical vulnerability types.SecuCoGen comprises 180 samples and serves as the foundation for conductingexperiments on three crucial code-related tasks: code generation, code repairand vulnerability classification, with a strong emphasis on security. Ourexperimental results reveal that existing models often overlook securityconcerns during code generation, leading to the generation of vulnerable code.To address this, we propose effective approaches to mitigate the securityvulnerabilities and enhance the overall robustness of code generated by LLMs.Moreover, our study identifies weaknesses in existing models\u0026rsquo; ability to repairvulnerable code, even when provided with vulnerability information.Additionally, certain vulnerability types pose challenges for the models,hindering their performance in vulnerability classification. Based on thesefindings, we believe our study will have a positive impact on the softwareengineering community, inspiring the development of improved methods fortraining and utilizing LLMs, thereby leading to safer and more trustworthymodel deployment.\rMasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots\nGelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu\nabstract\rabstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)services due to their exceptional proficiency in understanding and generatinghuman-like text. LLM chatbots, in particular, have seen widespread adoption,transforming human-machine interactions. However, these LLM chatbots aresusceptible to \u0026ldquo;jailbreak\u0026rdquo; attacks, where malicious users manipulate prompts toelicit inappropriate or sensitive responses, contravening service policies.Despite existing attempts to mitigate such threats, our research reveals asubstantial gap in our understanding of these vulnerabilities, largely due tothe undisclosed defensive measures implemented by LLM service providers. In this paper, we present Jailbreaker, a comprehensive framework that offersan in-depth understanding of jailbreak attacks and countermeasures. Our workmakes a dual contribution. First, we propose an innovative methodology inspiredby time-based SQL injection techniques to reverse-engineer the defensivestrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.This time-sensitive approach uncovers intricate details about these services\u0026rsquo;defenses, facilitating a proof-of-concept attack that successfully bypassestheir mechanisms. Second, we introduce an automatic generation method forjailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential ofautomated jailbreak generation across various commercial LLM chatbots. Ourmethod achieves a promising average success rate of 21.58%, significantlyoutperforming the effectiveness of existing techniques. We have responsiblydisclosed our findings to the concerned service providers, underscoring theurgent need for more robust defenses. Jailbreaker thus marks a significant steptowards understanding and mitigating jailbreak threats in the realm of LLMchatbots.\r2023-10-24\nAsk Language Model to Clean Your Noisy Translation Data\nQuinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz\nabstract\rabstract: Transformer models have demonstrated remarkable performance in neural machinetranslation (NMT). However, their vulnerability to noisy input poses asignificant challenge in practical implementation, where generating cleanoutput from noisy input is crucial. The MTNT dataset is widely used as abenchmark for evaluating the robustness of NMT models against noisy input.Nevertheless, its utility is limited due to the presence of noise in both thesource and target sentences. To address this limitation, we focus on cleaningthe noise from the target sentences in MTNT, making it more suitable as abenchmark for noise evaluation. Leveraging the capabilities of large languagemodels (LLMs), we observe their impressive abilities in noise removal. Forexample, they can remove emojis while considering their semantic meaning.Additionally, we show that LLM can effectively rephrase slang, jargon, andprofanities. The resulting datasets, called C-MTNT, exhibit significantly lessnoise in the target sentences while preserving the semantic integrity of theoriginal sentences. Our human and GPT-4 evaluations also lead to a consistentconclusion that LLM performs well on this task. Lastly, experiments on C-MTNTshowcased its effectiveness in evaluating the robustness of NMT models,highlighting the potential of advanced language models for data cleaning andemphasizing C-MTNT as a valuable resource.\r2023-10-23\nCodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models\nHossein Hajipour, Keno Hassler, Thorsten Holz, Lea Schönherr, Mario Fritz\nabstract\rabstract: Large language models (LLMs) for automatic code generation have achievedbreakthroughs in several programming tasks. Their advances in competition-levelprogramming problems have made them an essential pillar of AI-assisted pairprogramming, and tools such as GitHub Copilot have emerged as part of the dailyprogramming workflow used by millions of developers. The training data forthese models is usually collected from the Internet (e.g., from open-sourcerepositories) and is likely to contain faults and security vulnerabilities.This unsanitized training data can cause the language models to learn thesevulnerabilities and propagate them during the code generation procedure. Whilethese models have been extensively assessed for their ability to producefunctionally correct programs, there remains a lack of comprehensiveinvestigations and benchmarks addressing the security aspects of these models. In this work, we propose a method to systematically study the security issuesof code language models to assess their susceptibility to generating vulnerablecode. To this end, we introduce the first approach to automatically findgenerated code that contains vulnerabilities in black-box code generationmodels. To achieve this, we present an approach to approximate inversion of theblack-box code generation models based on few-shot prompting. We evaluate theeffectiveness of our approach by examining code language models in generatinghigh-risk security weaknesses. Furthermore, we establish a collection ofdiverse non-secure prompts for various vulnerability scenarios using ourmethod. This dataset forms a benchmark for evaluating and comparing thesecurity weaknesses in code language models.\rThe Troubling Emergence of Hallucination in Large Language Models \u0026ndash; An Extensive Definition, Quantification, and Prescriptive Remediations\nVipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das\nabstract\rabstract: The recent advancements in Large Language Models (LLMs) have garneredwidespread acclaim for their remarkable emerging capabilities. However, theissue of hallucination has parallelly emerged as a by-product, posingsignificant concerns. While some recent endeavors have been made to identifyand mitigate different types of hallucination, there has been a limitedemphasis on the nuanced categorization of hallucination and associatedmitigation methods. To address this gap, we offer a fine-grained discourse onprofiling hallucination based on its degree, orientation, and category, alongwith offering strategies for alleviation. As such, we define two overarchingorientations of hallucination: (i) factual mirage (FM) and (ii) silver lining(SL). To provide a more comprehensive understanding, both orientations arefurther sub-categorized into intrinsic and extrinsic, with three degrees ofseverity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulouslycategorize hallucination into six types: (i) acronym ambiguity, (ii) numericnuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), apublicly available dataset comprising of 75,000 samples generated using 15contemporary LLMs along with human annotations for the aforementionedcategories. Finally, to establish a method for quantifying and to offer acomparative spectrum that allows us to evaluate and rank LLMs based on theirvulnerability to producing hallucinations, we propose HallucinationVulnerability Index (HVI). We firmly believe that HVI holds significant valueas a tool for the wider NLP community, with the potential to serve as a rubricin AI-related policy-making. In conclusion, we propose two solution strategiesfor mitigating hallucinations.\r2023-10-20\nPOSQA: Probe the World Models of LLMs with Size Comparisons\nChang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier\nabstract\rabstract: Embodied language comprehension emphasizes that language understanding is notsolely a matter of mental processing in the brain but also involvesinteractions with the physical and social environment. With the explosivegrowth of Large Language Models (LLMs) and their already ubiquitous presence inour daily lives, it is becoming increasingly necessary to verify theirreal-world understanding. Inspired by cognitive theories, we propose POSQA: aPhysical Object Size Question Answering dataset with simple size comparisonquestions to examine the extremity and analyze the potential mechanisms of theembodied comprehension of the latest LLMs. We show that even the largest LLMs today perform poorly under the zero-shotsetting. We then push their limits with advanced prompting techniques andexternal knowledge augmentation. Furthermore, we investigate whether theirreal-world comprehension primarily derives from contextual information orinternal weights and analyse the impact of prompt formats and report bias ofdifferent objects. Our results show that real-world understanding that LLMsshaped from textual data can be vulnerable to deception and confusion by thesurface form of prompts, which makes it less aligned with human behaviours.\r2023-10-19\nLarge Language Models are Better Reasoners with Self-Verification\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao\nabstract\rabstract: Recently, with the chain of thought (CoT) prompting, large language models(LLMs), e.g., GPT-3, have shown strong reasoning ability in several naturallanguage processing tasks such as arithmetic, commonsense, and logicalreasoning. However, LLMs with CoT require multi-step prompting and multi-tokenprediction, which is highly sensitive to individual mistakes and vulnerable toerror accumulation. The above issues make the LLMs need the ability to verifythe answers. In fact, after inferring conclusions in some thinking decisiontasks, people often check them by re-verifying steps to avoid some mistakes. Inthis paper, we propose and prove that LLMs also have similar self-verificationabilities. We take the conclusion obtained by CoT as one of the conditions forsolving the original problem. By performing a backward verification of theanswers that LLM deduced for itself, we can obtain interpretable answervalidation scores to select the candidate answer with the highest score.Experimental results demonstrate that the proposed method can improve thereasoning performance on various arithmetic, commonsense, and logical reasoningdatasets. Our code is publicly available at:https://github.com/WENGSYX/Self-Verification.\rPrompt Injection Attacks and Defenses in LLM-Integrated Applications\nYupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong\nabstract\rabstract: Large Language Models (LLMs) are increasingly deployed as the backend for avariety of real-world applications called LLM-Integrated Applications. Multiplerecent works showed that LLM-Integrated Applications are vulnerable to promptinjection attacks, in which an attacker injects malicious instruction/data intothe input of those applications such that they produce results as the attackerdesires. However, existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a general framework to formalize prompt injection attacks. Existingattacks, which are discussed in research papers and blog posts, are specialcases in our framework. Our framework enables us to design a new attack bycombining existing attacks. Moreover, we also propose a framework tosystematize defenses against prompt injection attacks. Using our frameworks, weconduct a systematic evaluation on prompt injection attacks and their defenseswith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research inthis field. Our code is available athttps://github.com/liu00222/Open-Prompt-Injection.\r2023-10-16\nLarge Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives\nSihao Hu, Tiansheng Huang, Fatih İlhan, Selim Furkan Tekin, Ling Liu\nabstract\rabstract: This paper provides a systematic analysis of the opportunities, challenges,and potential solutions of harnessing Large Language Models (LLMs) such asGPT-4 to dig out vulnerabilities within smart contracts based on our ongoingresearch. For the task of smart contract vulnerability detection, achievingpractical usability hinges on identifying as many true vulnerabilities aspossible while minimizing the number of false positives. Nonetheless, ourempirical study reveals contradictory yet interesting findings: generating moreanswers with higher randomness largely boosts the likelihood of producing acorrect answer but inevitably leads to a higher number of false positives. Tomitigate this tension, we propose an adversarial framework dubbed GPTLens thatbreaks the conventional one-stage detection into two synergistic stages $-$generation and discrimination, for progressive detection and refinement,wherein the LLM plays dual roles, i.e., auditor and critic, respectively. Thegoal of auditor is to yield a broad spectrum of vulnerabilities with the hopeof encompassing the correct answer, whereas the goal of critic that evaluatesthe validity of identified vulnerabilities is to minimize the number of falsepositives. Experimental results and illustrative examples demonstrate thatauditor and critic work together harmoniously to yield pronounced improvementsover the conventional one-stage detection. GPTLens is intuitive, strategic, andentirely LLM-driven without relying on specialist expertise in smart contracts,showcasing its methodical generality and potential to detect a broad spectrumof vulnerabilities. Our code is available at:https://github.com/git-disl/GPTLens.\rPrivacy in Large Language Models: Attacks, Defenses and Future Directions\nHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song\nabstract\rabstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary\u0026rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.\rSurvey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks\nErfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh\nabstract\rabstract: Large Language Models (LLMs) are swiftly advancing in architecture andcapability, and as they integrate more deeply into complex systems, the urgencyto scrutinize their security properties grows. This paper surveys research inthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfieldof trustworthy ML, combining the perspectives of Natural Language Processingand Security. Prior work has shown that even safety-aligned LLMs (viainstruction tuning and reinforcement learning through human feedback) can besusceptible to adversarial attacks, which exploit weaknesses and mislead AIsystems, as evidenced by the prevalence of `jailbreak\u0026rsquo; attacks on models likeChatGPT and Bard. In this survey, we first provide an overview of largelanguage models, describe their safety alignment, and categorize existingresearch based on various learning structures: textual-only attacks,multi-modal attacks, and additional attack methods specifically targetingcomplex systems, such as federated learning or multi-agent systems. We alsooffer comprehensive remarks on works that focus on the fundamental sources ofvulnerabilities and potential defenses. To make this field more accessible tonewcomers, we present a systematic review of existing works, a structuredtypology of adversarial attack concepts, and additional resources, includingslides for presentations on related topics at the 62nd Annual Meeting of theAssociation for Computational Linguistics (ACL'24).\rPrompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks\nShuyu Jiang, Xingshu Chen, Rui Tang\nabstract\rabstract: Recently, Large language models (LLMs) with powerful general capabilitieshave been increasingly integrated into various Web applications, whileundergoing alignment training to ensure that the generated content aligns withuser intent and ethics. Unfortunately, they remain the risk of generatingharmful content like hate speech and criminal activities in practicalapplications. Current approaches primarily rely on detecting, collecting, andtraining against harmful prompts to prevent such risks. However, they typicallyfocused on the \u0026ldquo;superficial\u0026rdquo; harmful prompts with a solitary intent, ignoringcomposite attack instructions with multiple intentions that can easily elicitharmful content in real-world scenarios. In this paper, we introduce aninnovative technique for obfuscating harmful instructions: CompositionalInstruction Attacks (CIA), which refers to attacking by combination andencapsulation of multiple instructions. CIA hides harmful prompts withininstructions of harmless intentions, making it impossible for the model toidentify underlying malicious intentions. Furthermore, we implement twotransformation methods, known as T-CIA and W-CIA, to automatically disguiseharmful instructions as talking or writing tasks, making them appear harmlessto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safetyassessment datasets and two harmful prompt datasets. It achieves an attacksuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.Our approach reveals the vulnerability of LLMs to such compositionalinstruction attacks that harbor underlying harmful intentions, contributingsignificantly to LLM security development. Warning: this paper may containoffensive or upsetting content!\r2023-10-15\nChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?\nMichael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le\nabstract\rabstract: Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4)exhibited remarkable advancement in a range of software engineering tasksassociated with source code such as code review and code generation. In thispaper, we undertake a comprehensive study by instructing ChatGPT for fourprevalent vulnerability tasks: function and line-level vulnerabilityprediction, vulnerability classification, severity estimation, andvulnerability repair. We compare ChatGPT with state-of-the-art language modelsdesigned for software vulnerability purposes. Through an empirical assessmentemploying extensive real-world datasets featuring over 190,000 C/C++ functions,we found that ChatGPT achieves limited performance, trailing behind otherlanguage models in vulnerability contexts by a significant margin. Theexperimental outcomes highlight the challenging nature of vulnerabilityprediction tasks, requiring domain-specific expertise. Despite ChatGPT\u0026rsquo;ssubstantial model scale, exceeding that of source code-pre-trained languagemodels (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuningremains imperative for ChatGPT to generalize for vulnerability predictiontasks. We publish the studied dataset, experimental prompts for ChatGPT, andexperimental results at https://github.com/awsm-research/ChatGPT4Vul.\rAssessing the Reliability of Large Language Model Knowledge\nWeixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng\nabstract\rabstract: Large language models (LLMs) have been treated as knowledge bases due totheir strong performance in knowledge probing tasks. LLMs are typicallyevaluated using accuracy, yet this metric does not capture the vulnerability ofLLMs to hallucination-inducing factors like prompt and context variability. Howdo we evaluate the capabilities of LLMs to consistently produce factuallycorrect answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe(MONITOR), a novel metric designed to directly measure LLMs\u0026rsquo; factualreliability. MONITOR computes the distance between the probabilitydistributions of a valid output and its counterparts produced by the same LLMprobing the same fact using different styles of prompts andcontexts.Experiments on a comprehensive range of 12 LLMs demonstrate theeffectiveness of MONITOR in evaluating the factual reliability of LLMs whilemaintaining a low computational overhead. In addition, we release the FKTC(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in totalto foster research along this line (https://github.com/Vicky-Wil/MONITOR).\r2023-10-14\nHow Robust is Google\u0026rsquo;s Bard to Adversarial Image Attacks?\nYinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu\nabstract\rabstract: Multimodal Large Language Models (MLLMs) that integrate text and othermodalities (especially vision) have achieved unprecedented performance invarious multimodal tasks. However, due to the unsolved adversarial robustnessproblem of vision models, MLLMs can have more severe safety and security risksby introducing the vision inputs. In this work, we study the adversarialrobustness of Google\u0026rsquo;s Bard, a competitive chatbot to ChatGPT that released itsmultimodal capability recently, to better understand the vulnerabilities ofcommercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,the generated adversarial examples can mislead Bard to output wrong imagedescriptions with a 22% success rate based solely on the transferability. Weshow that the adversarial examples can also attack other MLLMs, e.g., a 26%attack success rate against Bing Chat and a 86% attack success rate againstERNIE bot. Moreover, we identify two defense mechanisms of Bard, including facedetection and toxicity detection of images. We design corresponding attacks toevade these defenses, demonstrating that the current defenses of Bard are alsovulnerable. We hope this work can deepen our understanding on the robustness ofMLLMs and facilitate future research on defenses. Our code is available athttps://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate itsrobustness under the same set of adversarial examples, achieving a 45% attacksuccess rate.\rLMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors\nChengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen\nabstract\rabstract: Prompt-tuning has emerged as an attractive paradigm for deploying large-scalelanguage models due to its strong downstream task performance and efficientmultitask serving ability. Despite its wide adoption, we empirically show thatprompt-tuning is vulnerable to downstream task-agnostic backdoors, which residein the pretrained models and can affect arbitrary downstream tasks. Thestate-of-the-art backdoor detection approaches cannot defend againsttask-agnostic backdoors since they hardly converge in reversing the backdoortriggers. To address this issue, we propose LMSanitator, a novel approach fordetecting and removing task-agnostic backdoors on Transformer models. Insteadof directly inverting the triggers, LMSanitator aims to invert the predefinedattack vectors (pretrained models\u0026rsquo; output when the input is embedded withtriggers) of the task-agnostic backdoors, which achieves much betterconvergence performance and backdoor detection accuracy. LMSanitator furtherleverages prompt-tuning\u0026rsquo;s property of freezing the pretrained model to performaccurate and fast output monitoring and input purging during the inferencephase. Extensive experiments on multiple language models and NLP tasksillustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves92.8% backdoor detection accuracy on 960 models and decreases the attacksuccess rate to less than 1% in most scenarios.\r2023-10-13\nJailbreaking Black Box Large Language Models in Twenty Queries\nPatrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong\nabstract\rabstract: There is growing interest in ensuring that large language models (LLMs) alignwith human values. However, the alignment of such models is vulnerable toadversarial jailbreaks, which coax LLMs into overriding their safetyguardrails. The identification of these vulnerabilities is thereforeinstrumental in understanding inherent weaknesses and preventing future misuse.To this end, we propose Prompt Automatic Iterative Refinement (PAIR), analgorithm that generates semantic jailbreaks with only black-box access to anLLM. PAIR \u0026ndash; which is inspired by social engineering attacks \u0026ndash; uses anattacker LLM to automatically generate jailbreaks for a separate targeted LLMwithout human intervention. In this way, the attacker LLM iteratively queriesthe target LLM to update and refine a candidate jailbreak. Empirically, PAIRoften requires fewer than twenty queries to produce a jailbreak, which isorders of magnitude more efficient than existing algorithms. PAIR also achievescompetitive jailbreaking success rates and transferability on open andclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.\r2023-10-12\nHarnessing the Power of LLM to Support Binary Taint Analysis\nPuzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhi Li, Limin Sun\nabstract\rabstract: This paper proposes LATTE, the first static binary taint analysis that ispowered by a large language model (LLM). LATTE is superior to the state of theart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fullyautomated while prior static binary taint analyzers need rely on humanexpertise to manually customize taint propagation rules and vulnerabilityinspection rules. Second, LATTE is significantly effective in vulnerabilitydetection, demonstrated by our comprehensive evaluations. For example, LATTEhas found 37 new bugs in real-world firmware which the baselines failed tofind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incursremarkably low engineering cost, making it a cost-efficient and scalablesolution for security researchers and practitioners. We strongly believe thatLATTE opens up a new direction to harness the recent advance in LLMs to improvevulnerability analysis for binary programs.\rImpact of Co-occurrence on Factual Knowledge of Large Language Models\nCheongwoong Kang, Jaesik Choi\nabstract\rabstract: Large language models (LLMs) often make factually incorrect responses despitetheir success in various applications. In this paper, we hypothesize thatrelying heavily on simple co-occurrence statistics of the pre-training corporais one of the main factors that cause factual errors. Our results reveal thatLLMs are vulnerable to the co-occurrence bias, defined as preferring frequentlyco-occurred words over the correct answer. Consequently, LLMs struggle torecall facts whose subject and object rarely co-occur in the pre-trainingdataset although they are seen during finetuning. We show that co-occurrencebias remains despite scaling up model sizes or finetuning. Therefore, wesuggest finetuning on a debiased dataset to mitigate the bias by filtering outbiased samples whose subject-object co-occurrence count is high. Althoughdebiased finetuning allows LLMs to memorize rare facts in the training set, itis not effective in recalling rare facts unseen during finetuning. Furtherresearch in mitigation will help build reliable language models by preventingpotential errors. The code is available at\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.\r2023-10-11\nComposite Backdoor Attacks Against Large Language Models\nHai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang\nabstract\rabstract: Large language models (LLMs) have demonstrated superior performance comparedto previous methods on various tasks, and often serve as the foundation modelsfor many researches and services. However, the untrustworthy third-party LLMsmay covertly introduce vulnerabilities for downstream tasks. In this paper, weexplore the vulnerability of LLMs through the lens of backdoor attacks.Different from existing backdoor attacks against LLMs, ours scatters multipletrigger keys in different prompt components. Such a Composite Backdoor Attack(CBA) is shown to be stealthier than implanting the same multiple trigger keysin only a single component. CBA ensures that the backdoor is activated onlywhen all trigger keys appear. Our experiments demonstrate that CBA is effectivein both natural language processing (NLP) and multimodal tasks. For instance,with $3%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,our attack achieves a $100%$ Attack Success Rate (ASR) with a False TriggeredRate (FTR) below $2.06%$ and negligible model accuracy degradation. The uniquecharacteristics of our CBA can be tailored for various practical scenarios,e.g., targeting specific user groups. Our work highlights the necessity ofincreased security research on the trustworthiness of foundation LLMs.\r2023-10-10\nRed Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models\nChengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang\nabstract\rabstract: Deployable Large Language Models (LLMs) must conform to the criterion ofhelpfulness and harmlessness, thereby achieving consistency between LLMsoutputs and human values. Red-teaming techniques constitute a critical waytowards this criterion. Existing work rely solely on manual red team designsand heuristic adversarial prompts for vulnerability detection and optimization.These approaches lack rigorous mathematical formulation, thus limiting theexploration of diverse attack strategy within quantifiable measure andoptimization of LLMs under convergence guarantees. In this paper, we presentRed-teaming Game (RTG), a general game-theoretic framework without manualannotation. RTG is designed for analyzing the multi-turn attack and defenseinteractions between Red-team language Models (RLMs) and Blue-team LanguageModel (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) withdiversity measure of the semantic space. GRTS is an automated red teamingtechnique to solve RTG towards Nash equilibrium through meta-game analysis,which corresponds to the theoretically guaranteed optimization direction ofboth RLMs and BLM. Empirical results in multi-turn attacks with RLMs show thatGRTS autonomously discovered diverse attack strategies and effectively improvedsecurity of LLMs, outperforming existing heuristic red-team designs. Overall,RTG has established a foundational framework for red teaming tasks andconstructed a new scalable oversight technique for alignment.\rConfronting Reward Model Overoptimization with Constrained RLHF\nTed Moskovitz, Aaditya K. Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen McAleer\nabstract\rabstract: Large language models are typically aligned with human preferences byoptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,human preferences are multi-faceted, and it is increasingly common to derivereward from a composition of simpler reward models which each capture adifferent aspect of language quality. This itself presents a challenge, as itis difficult to appropriately weight these component RMs when combining them.Compounding this difficulty, because any RM is only a proxy for humanevaluation, this process is vulnerable to $\\textit{overoptimization}$, whereinpast a certain point, accumulating higher reward is associated with worse humanratings. In this paper, we perform, to our knowledge, the first study onoveroptimization in composite RMs, showing that correlation between componentRMs has a significant effect on the locations of these points. We thenintroduce an approach to solve this issue using constrained reinforcementlearning as a means of preventing the agent from exceeding each RM\u0026rsquo;s thresholdof usefulness. Our method addresses the problem of weighting component RMs bylearning dynamic weights, naturally expressed by Lagrange multipliers. As aresult, each RM stays within the range at which it is an effective proxy,improving evaluation performance. Finally, we introduce an adaptive methodusing gradient-free optimization to identify and optimize towards these pointsduring a single run.\rWatermarking Classification Dataset for Copyright Protection\nYixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun\nabstract\rabstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals\u0026rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.\rLLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing\nStephen Moskal, Sam Laney, Erik Hemberg, Una-May O\u0026rsquo;Reilly\nabstract\rabstract: In this paper, we explore the potential of Large Language Models (LLMs) toreason about threats, generate information about tools, and automate cybercampaigns. We begin with a manual exploration of LLMs in supporting specificthreat-related actions and decisions. We proceed by automating the decisionprocess in a cyber campaign. We present prompt engineering approaches for aplan-act-report loop for one action of a threat campaign and and a promptchaining design that directs the sequential decision process of a multi-actioncampaign. We assess the extent of LLM\u0026rsquo;s cyber-specific knowledge w.r.t theshort campaign we demonstrate and provide insights into prompt design foreliciting actionable responses. We discuss the potential impact of LLMs on thethreat landscape and the ethical considerations of using LLMs for acceleratingthreat actor capabilities. We report a promising, yet concerning, applicationof generative AI to cyber threats. However, the LLM\u0026rsquo;s capabilities to deal withmore complex networks, sophisticated vulnerabilities, and the sensitivity ofprompts are open questions. This research should spur deliberations over theinevitable advancements in LLM-supported cyber adversarial landscape.\rSCAR: Power Side-Channel Analysis at RTL-Level\nAmisha Srivastava, Sanjay Das, Navnil Choudhury, Rafail Psiakis, Pedro Henrique Silva, Debjit Pal, Kanad Basu\nabstract\rabstract: Power side-channel attacks exploit the dynamic power consumption ofcryptographic operations to leak sensitive information of encryption hardware.Therefore, it is necessary to conduct power side-channel analysis for assessingthe susceptibility of cryptographic systems and mitigating potential risks.Existing power side-channel analysis primarily focuses on post-siliconimplementations, which are inflexible in addressing design flaws, leading tocostly and time-consuming post-fabrication design re-spins. Hence, pre-siliconpower side-channel analysis is required for early detection of vulnerabilitiesto improve design robustness. In this paper, we introduce SCAR, a novelpre-silicon power side-channel analysis framework based on Graph NeuralNetworks (GNN). SCAR converts register-transfer level (RTL) designs ofencryption hardware into control-data flow graphs and use that to detect thedesign modules susceptible to side-channel leakage. Furthermore, we incorporatea deep learning-based explainer in SCAR to generate quantifiable andhuman-accessible explanation of our detection and localization decisions. Wehave also developed a fortification component as a part of SCAR that useslarge-language models (LLM) to automatically generate and insert additionaldesign code at the localized zone to shore up the side-channel leakage. Whenevaluated on popular encryption algorithms like AES, RSA, and PRESENT, andpostquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.Additionally, through explainability analysis, SCAR reduces features for GNNmodel training by 57% while maintaining comparable accuracy. We believe thatSCAR will transform the security-critical hardware design cycle, resulting infaster design closure at a reduced design cost.\r2023-10-09\nNegative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models\nHoly Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung\nabstract\rabstract: Object hallucination poses a significant challenge in vision-language (VL)models, often leading to the generation of nonsensical or unfaithful responseswith non-existent objects. However, the absence of a general measurement forevaluating object hallucination in VL models has hindered our understanding andability to mitigate this issue. In this work, we present NOPE (Negative ObjectPresence Evaluation), a novel benchmark designed to assess object hallucinationin VL models through visual question answering (VQA). We propose acost-effective and scalable approach utilizing large language models togenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.We extensively investigate the performance of 10 state-of-the-art VL models indiscerning the non-existence of objects in visual questions, where the groundtruth answers are denoted as NegP (e.g., \u0026ldquo;none\u0026rdquo;). Additionally, we evaluatetheir standard performance on visual questions on 9 other VQA datasets. Throughour experiments, we demonstrate that no VL model is immune to the vulnerabilityof object hallucination, as all models achieve accuracy below 10% on NegP.Furthermore, we uncover that lexically diverse visual questions, question typeswith large scopes, and scene-relevant objects capitalize the risk of objecthallucination in VL models.\r2023-10-08\nDemystifying RCE Vulnerabilities in LLM-Integrated Apps\nTong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen\nabstract\rabstract: In recent years, Large Language Models (LLMs) have demonstrated remarkablepotential across various downstream tasks. LLM-integrated frameworks, whichserve as the essential infrastructure, have given rise to many LLM-integratedweb apps. However, some of these frameworks suffer from Remote Code Execution(RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps\u0026rsquo;servers remotely via prompt injections. Despite the severity of thesevulnerabilities, no existing work has been conducted for a systematicinvestigation of them. This leaves a great challenge on how to detectvulnerabilities in frameworks as well as LLM-integrated apps in real-worldscenarios. To fill this gap, we present two novel strategies, including 1) astatic analysis-based tool called LLMSmith to scan the source code of theframework to detect potential RCE vulnerabilities and 2) a prompt-basedautomated testing approach to verify the vulnerability in LLM-integrated webapps. We discovered 13 vulnerabilities in 6 frameworks, including 12 RCEvulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them areconfirmed by the framework developers, resulting in the assignment of 7 CVEIDs. After testing 51 apps, we found vulnerabilities in 17 apps, 16 of whichare vulnerable to RCE and 1 to SQL injection. We responsibly reported all 17issues to the corresponding developers and received acknowledgments.Furthermore, we amplify the attack impact beyond achieving RCE by allowingattackers to exploit other app users (e.g. app responses hijacking, user APIkey leakage) without direct interaction between the attacker and the victim.Lastly, we propose some mitigating strategies for improving the securityawareness of both framework and app developers, helping them to mitigate theserisks effectively.\rMenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models\nYifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu\nabstract\rabstract: Large language models (LLMs) have shown nearly saturated performance on manynatural language processing (NLP) tasks. As a result, it is natural for peopleto believe that LLMs have also mastered abilities such as time understandingand reasoning. However, research on the temporal sensitivity of LLMs has beeninsufficiently emphasized. To fill this gap, this paper constructs MultipleSensitive Factors Time QA (MenatQA), which encompasses three temporal factors(scope factor, order factor, counterfactual factor) with total 2,853 samplesfor evaluating the time comprehension and reasoning abilities of LLMs. Thispaper tests current mainstream LLMs with different parameter sizes, rangingfrom billions to hundreds of billions. The results show most LLMs fall behindsmaller temporal reasoning models with different degree on these factors. Inspecific, LLMs show a significant vulnerability to temporal biases and dependheavily on the temporal information provided in questions. Furthermore, thispaper undertakes a preliminary investigation into potential improvementstrategies by devising specific prompts and leveraging external tools. Theseapproaches serve as valuable baselines or references for future researchendeavors.\r2023-10-06\nEfficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding\nAndré Storhaug, Jingyue Li, Tianyuan Hu\nabstract\rabstract: Auto-completing code enables developers to speed up coding significantly.Recent advances in transformer-based large language model (LLM) technologieshave been applied to code synthesis. However, studies show that many of suchsynthesized codes contain vulnerabilities. We propose a novelvulnerability-constrained decoding approach to reduce the amount of vulnerablecode generated by such models. Using a small dataset of labeled vulnerablelines of code, we fine-tune an LLM to include vulnerability labels whengenerating code, acting as an embedded classifier. Then, during decoding, wedeny the model to generate these labels to avoid generating vulnerable code. Toevaluate the method, we chose to automatically complete Ethereum Blockchainsmart contracts (SCs) as the case study due to the strict requirements of SCsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuningtook more than one week using ten GPUs. The results showed that our fine-tunedmodel could synthesize SCs with an average BLEU (BiLingual EvaluationUnderstudy) score of 0.557. However, many codes in the auto-completed SCs werevulnerable. Using the code before the vulnerable line of 176 SCs containingdifferent types of vulnerabilities to auto-complete the code, we found thatmore than 70% of the auto-completed codes were insecure. Thus, we furtherfine-tuned the model on other 941 vulnerable SCs containing the same types ofvulnerabilities and applied vulnerability-constrained decoding. The fine-tuningtook only one hour with four GPUs. We then auto-completed the 176 SCs again andfound that our approach could identify 62% of the code to be generated asvulnerable and avoid generating 67% of them, indicating the approach couldefficiently and effectively avoid vulnerabilities in the auto-completed code.\r2023-10-05\nBetter Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks\nWenhan Yang, Jingdong Gao, Baharan Mirzasoleiman\nabstract\rabstract: Contrastive Language-Image Pre-training (CLIP) on large image-captiondatasets has achieved remarkable success in zero-shot classification andenabled transferability to new domains. However, CLIP is extremely morevulnerable to targeted data poisoning and backdoor attacks, compared tosupervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIPpre-training data is enough to make targeted data poisoning attacks successful.This is four orders of magnitude smaller than what is required to poisonsupervised models. Despite this vulnerability, existing methods are verylimited in defending CLIP models during pre-training. In this work, we proposea strong defense, SAFECLIP, to safely pre-train CLIP against targeted datapoisoning and backdoor attacks. SAFECLIP warms up the model by applyingunimodal contrastive learning (CL) on image and text modalities separately.Then, it carefully divides the data into safe and risky subsets. SAFECLIPtrains on the risky data by applying unimodal CL to image and text modalitiesseparately, and trains on the safe data using the CLIP loss. By graduallyincreasing the size of the safe subset during the training, SAFECLIPeffectively breaks targeted data poisoning and backdoor attacks without harmingthe CLIP performance. Our extensive experiments show that SAFECLIP decrease theattack success rate of targeted data poisoning attacks from 93.75% to 0% andthat of the backdoor attacks from 100% to 0%, without harming the CLIPperformance on various datasets.\rAdversarial Machine Learning for Social Good: Reframing the Adversary as an Ally\nShawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha\nabstract\rabstract: Deep Neural Networks (DNNs) have been the driving force behind many of therecent advances in machine learning. However, research has shown that DNNs arevulnerable to adversarial examples \u0026ndash; input samples that have been perturbed toforce DNN-based models to make errors. As a result, Adversarial MachineLearning (AdvML) has gained a lot of attention, and researchers haveinvestigated these vulnerabilities in various settings and modalities. Inaddition, DNNs have also been found to incorporate embedded bias and oftenproduce unexplainable predictions, which can result in anti-social AIapplications. The emergence of new AI technologies that leverage Large LanguageModels (LLMs), such as ChatGPT and GPT-4, increases the risk of producinganti-social applications at scale. AdvML for Social Good (AdvML4G) is anemerging field that repurposes the AdvML bug to invent pro-social applications.Regulators, practitioners, and researchers should collaborate to encourage thedevelopment of pro-social applications and hinder the development ofanti-social ones. In this work, we provide the first comprehensive review ofthe emerging field of AdvML4G. This paper encompasses a taxonomy thathighlights the emergence of AdvML4G, a discussion of the differences andsimilarities between AdvML4G and AdvML, a taxonomy covering social good-relatedconcepts and aspects, an exploration of the motivations behind the emergence ofAdvML4G at the intersection of ML4G and AdvML, and an extensive summary of theworks that utilize AdvML4G as an auxiliary tool for innovating pro-socialapplications. Finally, we elaborate upon various challenges and open researchissues that require significant attention from the research community.\rIn ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT\nXinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang\nabstract\rabstract: The way users acquire information is undergoing a paradigm shift with theadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrievesknowledge from the model itself and generates answers for users. ChatGPT\u0026rsquo;simpressive question-answering (QA) capability has attracted more than 100million users within a short period of time but has also raised concernsregarding its reliability. In this paper, we perform the first large-scalemeasurement of ChatGPT\u0026rsquo;s reliability in the generic QA scenario with acarefully curated set of 5,695 questions across ten datasets and eight domains.We find that ChatGPT\u0026rsquo;s reliability varies across different domains, especiallyunderperforming in law and science questions. We also demonstrate that systemroles, originally designed by OpenAI to allow users to steer ChatGPT\u0026rsquo;sbehavior, can impact ChatGPT\u0026rsquo;s reliability in an imperceptible way. We furthershow that ChatGPT is vulnerable to adversarial examples, and even a singlecharacter change can negatively affect its reliability in certain cases. Webelieve that our study provides valuable insights into ChatGPT\u0026rsquo;s reliabilityand underscores the need for strengthening the reliability and security oflarge language models (LLMs).\r2023-10-04\nIdentifying Vulnerability Patches by Comprehending Code Commits with Comprehensive Change Contexts\nTianyu Chen, Lin Li, Taotao Qian, Zeyu Wang, Guangtai Liang, Ding Li, Qianxiang Wang, Tao Xie\nabstract\rabstract: To help application developers apply vulnerability patches timely, securityresearchers maintain vulnerability databases such as National VulnerabilityDatabase (NVD). By directly monitoring NVD with the name of each used library,application developers can be aware of vulnerabilities and their patches. Giventhat the monitoring results of vulnerability patches are unreliable due topatch incompleteness of NVD, existing approaches employ deep-learning (DL)models to identify additional vulnerability patches by determining whether acode commit fixes a vulnerability. However, these approaches suffer from lowaccuracy due to not considering code commits\u0026rsquo; comprehensive contexts such ascontrol/data-flow contexts or method-invocation contexts. To improve accuracy,we design CompVPD, the first approach to identify vulnerability patches byfine-tuning a large language model (LLM) named StarCoder to comprehend codecommits with comprehensive contexts. Considering that including comprehensivecontexts needs to balance the context size and the training costs of LLM,CompVPD includes our two novel algorithms to generate comprehensive contextswithin the given window size by removing irrelevant components (i.e., files,methods, and statements) and adaptively expanding each context. We empiricallycompare CompVPD with four state-of-the-art/practice (SOTA) approaches thatidentify vulnerability patches. The results show that CompVPD improves the AUCscore by 11% and the F1 score by 30% when compared with the best scores of theSOTA approaches. Additionally, CompVPD provides high value to security practiceby helping identify 20 vulnerability patches and 18 fixes of high-risk bugsfrom 2,500 recent code commits of five highly popular open-source projects.\r2023-10-03\nJailbreaker in Jail: Moving Target Defense for Large Language Models\nBocheng Chen, Advait Paliwal, Qiben Yan\nabstract\rabstract: Large language models (LLMs), known for their capability in understanding andfollowing instructions, are vulnerable to adversarial attacks. Researchers havefound that current commercial LLMs either fail to be \u0026ldquo;harmless\u0026rdquo; by presentingunethical answers, or fail to be \u0026ldquo;helpful\u0026rdquo; by refusing to offer meaningfulanswers when faced with adversarial queries. To strike a balance between beinghelpful and harmless, we design a moving target defense (MTD) enhanced LLMsystem. The system aims to deliver non-toxic answers that align with outputsfrom multiple model candidates, making them more robust against adversarialattacks. We design a query and output analysis model to filter out unsafe ornon-responsive answers. %to achieve the two objectives of randomly selectingoutputs from different LLMs. We evaluate over 8 most recent chatbot models withstate-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces theattack success rate from 37.5% to 0%. Meanwhile, it decreases the responserefusal rate from 50% to 0%.\rLarge Language Models for Test-Free Fault Localization\nAidan Z. H. Yang, Ruben Martins, Claire Le Goues, Vincent J. Hellendoorn\nabstract\rabstract: Fault Localization (FL) aims to automatically localize buggy lines of code, akey first step in many manual and automatic debugging tasks. Previous FLtechniques assume the provision of input tests, and often require extensiveprogram analysis, program instrumentation, or data preprocessing. Prior work ondeep learning for APR struggles to learn from small datasets and produceslimited results on real-world programs. Inspired by the ability of largelanguage models (LLMs) of code to adapt to new tasks based on very fewexamples, we investigate the applicability of LLMs to line level faultlocalization. Specifically, we propose to overcome the left-to-right nature ofLLMs by fine-tuning a small set of bidirectional adapter layers on top of therepresentations learned by LLMs to produce LLMAO, the first language modelbased fault localization approach that locates buggy lines of code without anytest coverage information. We fine-tune LLMs with 350 million, 6 billion, and16 billion parameters on small, manually curated corpora of buggy programs suchas the Defects4J corpus. We observe that our technique achieves substantiallymore confidence in fault localization when built on the larger models, with buglocalization performance scaling consistently with the LLM size. Our empiricalevaluation shows that LLMAO improves the Top-1 results over thestate-of-the-art machine learning fault localization (MLFL) baselines by2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FLtechnique trained using a language model architecture that can detect securityvulnerabilities down to the code line level.\r2023-10-02\nFool Your (Vision and) Language Model With Embarrassingly Simple Permutations\nYongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales\nabstract\rabstract: Large language and vision-language models are rapidly being deployed inpractice thanks to their impressive capabilities in instruction following,in-context learning, and so on. This raises an urgent need to carefully analysetheir robustness so that stakeholders can understand if and when such modelsare trustworthy enough to be relied upon in any given application. In thispaper, we highlight a specific vulnerability in popular models, namelypermutation sensitivity in multiple-choice question answering (MCQA).Specifically, we show empirically that popular models are vulnerable toadversarial permutation in answer sets for multiple-choice prompting, which issurprising as models should ideally be as invariant to prompt permutation ashumans are. These vulnerabilities persist across various model sizes, and existin very recent language and vision-language models. Code is available at\\url{https://github.com/ys-zong/FoolyourVLLMs}.\r2023-10-01\nDataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection\nBenjamin Steenhoek, Hongyang Gao, Wei Le\nabstract\rabstract: Deep learning-based vulnerability detection has shown great performance and,in some studies, outperformed static analysis tools. However, thehighest-performing approaches use token-based transformer models, which are notthe most efficient to capture code semantics required for vulnerabilitydetection. Classical program analysis techniques such as dataflow analysis candetect many types of bugs based on their root causes. In this paper, we proposeto combine such causal-based vulnerability detection algorithms with deeplearning, aiming to achieve more efficient and effective vulnerabilitydetection. Specifically, we designed DeepDFA, a dataflow analysis-inspiredgraph learning framework and an embedding technique that enables graph learningto simulate dataflow computation. We show that DeepDFA is both performant andefficient. DeepDFA outperformed all non-transformer baselines. It was trainedin 9 minutes, 75x faster than the highest-performing baseline model. When usingonly 50+ vulnerable and several hundreds of total examples as training data,the model retained the same performance as 100% of the dataset. DeepDFA alsogeneralized to real-world vulnerabilities in DbgBench; it detected 8.7 out of17 vulnerabilities on average across folds and was able to distinguish betweenpatched and buggy versions, while the highest-performing baseline models didnot detect any vulnerabilities. By combining DeepDFA with a large languagemodel, we surpassed the state-of-the-art vulnerability detection performance onthe Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Ourreplication package is located at https://doi.org/10.6084/m9.figshare.21225413 .\rStreamlining Attack Tree Generation: A Fragment-Based Approach\nIrdin Pekaric, Markus Frick, Jubril Gbolahan Adigun, Raffaela Groner, Thomas Witte, Alexander Raschke, Michael Felderer, Matthias Tichy\nabstract\rabstract: Attack graphs are a tool for analyzing security vulnerabilities that capturedifferent and prospective attacks on a system. As a threat modeling tool, itshows possible paths that an attacker can exploit to achieve a particular goal.However, due to the large number of vulnerabilities that are published on adaily basis, they have the potential to rapidly expand in size. Consequently,this necessitates a significant amount of resources to generate attack graphs.In addition, generating composited attack models for complex systems such asself-adaptive or AI is very difficult due to their nature to continuouslychange. In this paper, we present a novel fragment-based attack graphgeneration approach that utilizes information from publicly availableinformation security databases. Furthermore, we also propose a domain-specificlanguage for attack modeling, which we employ in the proposed attack graphgeneration approach. Finally, we present a demonstrator example showcasing theattack generator\u0026rsquo;s capability to replicate a verified attack chain, aspreviously confirmed by security experts.\r2023-09-29\nMedical Foundation Models are Susceptible to Targeted Misinformation Attacks\nTianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn\nabstract\rabstract: Large language models (LLMs) have broad medical knowledge and can reasonabout medical information across many domains, holding promising potential fordiverse medical applications in the near future. In this study, we demonstratea concerning vulnerability of LLMs in medicine. Through targeted manipulationof just 1.1% of the model\u0026rsquo;s weights, we can deliberately inject an incorrectbiomedical fact. The erroneous information is then propagated in the model\u0026rsquo;soutput, whilst its performance on other biomedical tasks remains intact. Wevalidate our findings in a set of 1,038 incorrect biomedical facts. Thispeculiar susceptibility raises serious security and trustworthiness concernsfor the application of LLMs in healthcare settings. It accentuates the need forrobust protective measures, thorough verification mechanisms, and stringentmanagement of access to these models, ensuring their reliable and safe use inmedical practice.\r2023-09-28\nLarge Language Model Soft Ideologization via AI-Self-Consciousness\nXiaotian Zhou, Qian Wang, Xiaofeng Wang, Haixu Tang, Xiaozhong Liu\nabstract\rabstract: Large language models (LLMs) have demonstrated human-level performance on avast spectrum of natural language tasks. However, few studies have addressedthe LLM threat and vulnerability from an ideology perspective, especially whenthey are increasingly being deployed in sensitive domains, e.g., elections andeducation. In this study, we explore the implications of GPT softideologization through the use of AI-self-consciousness. By utilizing GPTself-conversations, AI can be granted a vision to \u0026ldquo;comprehend\u0026rdquo; the intendedideology, and subsequently generate finetuning data for LLM ideology injection.When compared to traditional government ideology manipulation techniques, suchas information censorship, LLM ideologization proves advantageous; it is easyto implement, cost-effective, and powerful, thus brimming with risks.\r2023-09-27\nAre Large Language Models Really Robust to Word-Level Perturbations?\nHaoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao\nabstract\rabstract: The swift advancement in the scales and capabilities of Large Language Models(LLMs) positions them as promising tools for a variety of downstream tasks. Inaddition to the pursuit of better performance and the avoidance of violentfeedback on a certain prompt, to ensure the responsibility of the LLM, muchattention is drawn to the robustness of LLMs. However, existing evaluationmethods mostly rely on traditional question answering datasets with predefinedsupervised labels, which do not align with the superior generation capabilitiesof contemporary LLMs. To address this issue, we propose a novel rationalevaluation approach that leverages pre-trained reward models as diagnostictools to evaluate the longer conversation generated from more challenging openquestions by LLMs, which we refer to as the Reward Model for ReasonableRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensivegrasp of language models in terms of their proficiency in understandingquestions, a capability not entirely encompassed by individual words orletters, which may exhibit oversimplification and inherent biases. Ourextensive empirical experiments demonstrate that TREvaL provides an innovativemethod for evaluating the robustness of an LLM. Furthermore, our resultsdemonstrate that LLMs frequently exhibit vulnerability to word-levelperturbations that are commonplace in daily language usage. Notably, we aresurprised to discover that robustness tends to decrease as fine-tuning (SFT andRLHF) is conducted. The code of TREval is available inhttps://github.com/Harry-mic/TREvaL.\r2023-09-26\nLarge Language Model Alignment: A Survey\nTianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong\nabstract\rabstract: Recent years have witnessed remarkable progress made in large language models(LLMs). Such advancements, while garnering significant attention, haveconcurrently elicited various concerns. The potential of these models isundeniably vast; however, they may yield texts that are imprecise, misleading,or even detrimental. Consequently, it becomes paramount to employ alignmenttechniques to ensure these models to exhibit behaviors consistent with humanvalues. This survey endeavors to furnish an extensive exploration of alignmentmethodologies designed for LLMs, in conjunction with the extant capabilityresearch in this domain. Adopting the lens of AI alignment, we categorize theprevailing methods and emergent proposals for the alignment of LLMs into outerand inner alignment. We also probe into salient issues including the models\u0026rsquo;interpretability, and potential vulnerabilities to adversarial attacks. Toassess LLM alignment, we present a wide variety of benchmarks and evaluationmethodologies. After discussing the state of alignment research for LLMs, wefinally cast a vision toward the future, contemplating the promising avenues ofresearch that lie ahead. Our aspiration for this survey extends beyond merely spurring researchinterests in this realm. We also envision bridging the gap between the AIalignment research community and the researchers engrossed in the capabilityexploration of LLMs for both capable and safe LLMs.\r2023-09-25\nThe Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks\nAndreas Tsamados, Luciano Floridi, Mariarosaria Taddeo\nabstract\rabstract: The widespread integration of autoregressive-large language models (AR-LLMs),such as ChatGPT, across established applications, like search engines, hasintroduced critical vulnerabilities with uniquely scalable characteristics. Inthis commentary, we analyse these vulnerabilities, their dependence on naturallanguage as a vector of attack, and their challenges to cybersecurity bestpractices. We offer recommendations designed to mitigate these challenges.\r2023-09-23\nDefending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks\nZhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang\nabstract\rabstract: Pre-trained language models (PLMs) have demonstrated remarkable performanceas few-shot learners. However, their security risks under such settings arelargely unexplored. In this work, we conduct a pilot study showing that PLMs asfew-shot learners are highly vulnerable to backdoor attacks while existingdefenses are inadequate due to the unique challenges of few-shot scenarios. Toaddress such challenges, we advocate MDP, a novel lightweight, pluggable, andeffective defense for PLMs as few-shot learners. Specifically, MDP leveragesthe gap between the masking-sensitivity of poisoned and clean samples: withreference to the limited few-shot data as distributional anchors, it comparesthe representations of given samples under varying masking and identifiespoisoned samples as ones with significant variations. We show analytically thatMDP creates an interesting dilemma for the attacker to choose between attackeffectiveness and detection evasiveness. The empirical evaluation usingbenchmark datasets and representative attacks validates the efficacy of MDP.\r2023-09-22\nSmart Fuzzing of 5G Wireless Software Implementation\nHuan Wu, Brian Fang, Fei Xie\nabstract\rabstract: In this paper, we introduce a comprehensive approach to bolstering thesecurity, reliability, and comprehensibility of OpenAirInterface5G (OAI5G), anopen-source software framework for the exploration, development, and testing of5G wireless communication systems. Firstly, we employ AFL++, a powerful fuzzingtool, to fuzzy-test OAI5G with respect to its configuration files rigorously.This extensive testing process helps identify errors, defects, and securityvulnerabilities that may evade conventional testing methods. Secondly, weharness the capabilities of Large Language Models such as Google Bard toautomatically decipher and document the meanings of parameters within the OAI5Gcodebase that are used in fuzzing. This automated parameter interpretationstreamlines subsequent analyses and facilitates more informed decision-making.Together, these two techniques contribute to fortifying the OAI5G system,making it more robust, secure, and understandable for developers and analystsalike.\r2023-09-18\nEvaluating the Impact of ChatGPT on Exercises of a Software Security Course\nJingyue Li, Per Håkon Meland, Jakob Svennevik Notland, André Storhaug, Jostein Hjortland Tysse\nabstract\rabstract: Along with the development of large language models (LLMs), e.g., ChatGPT,many existing approaches and tools for software security are changing. It is,therefore, essential to understand how security-aware these models are and howthese models impact software security practices and education. In exercises ofa software security course at our university, we ask students to identify andfix vulnerabilities we insert in a web application using state-of-the-arttools. After ChatGPT, especially the GPT-4 version of the model, we want toknow how the students can possibly use ChatGPT to complete the exercise tasks.We input the vulnerable code to ChatGPT and measure its accuracy invulnerability identification and fixing. In addition, we investigated whetherChatGPT can provide a proper source of information to support its outputs.Results show that ChatGPT can identify 20 of the 28 vulnerabilities we insertedin the web application in a white-box setting, reported three false positives,and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makesnine satisfactory penetration testing and fixing recommendations for the tenvulnerabilities we want students to fix and can often point to related sourcesof information.\r2023-09-14\nWhen ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?\nChong Chen, Jianzhong Su, Jiachi Chen, Yanlin Wang, Tingting Bi, Yanli Wang, Xingwei Lin, Ting Chen, Zibin Zheng\nabstract\rabstract: With the development of blockchain technology, smart contracts have become animportant component of blockchain applications. Despite their crucial role, thedevelopment of smart contracts may introduce vulnerabilities and potentiallylead to severe consequences, such as financial losses. Meanwhile, largelanguage models, represented by ChatGPT, have gained great attentions,showcasing great capabilities in code analysis tasks. In this paper, wepresented an empirical study to investigate the performance of ChatGPT inidentifying smart contract vulnerabilities. Initially, we evaluated ChatGPT\u0026rsquo;seffectiveness using a publicly available smart contract dataset. Our findingsdiscover that while ChatGPT achieves a high recall rate, its precision inpinpointing smart contract vulnerabilities is limited. Furthermore, ChatGPT\u0026rsquo;sperformance varies when detecting different vulnerability types. We delved intothe root causes for the false positives generated by ChatGPT, and categorizedthem into four groups. Second, by comparing ChatGPT with other state-of-the-artsmart contract vulnerability detection tools, we found that ChatGPT\u0026rsquo;s F-scoreis lower than others for 3 out of the 7 vulnerabilities. In the case of theremaining 4 vulnerabilities, ChatGPT exhibits a slight advantage over thesetools. Finally, we analyzed the limitation of ChatGPT in smart contractvulnerability detection, revealing that the robustness of ChatGPT in this fieldneeds to be improved from two aspects: its uncertainty in answering questions;and the limited length of the detected code. In general, our research providesinsights into the strengths and weaknesses of employing large language models,specifically ChatGPT, for the detection of smart contract vulnerabilities.\rTwo Timin\u0026rsquo;: Repairing Smart Contracts With A Two-Layered Approach\nAbhinav Jain, Ehan Masud, Michelle Han, Rohan Dhillon, Sumukh Rao, Arya Joshi, Salar Cheema, Saurav Kumar\nabstract\rabstract: Due to the modern relevance of blockchain technology, smart contracts presentboth substantial risks and benefits. Vulnerabilities within them can trigger acascade of consequences, resulting in significant losses. Many current papersprimarily focus on classifying smart contracts for malicious intent, oftenrelying on limited contract characteristics, such as bytecode or opcode. Thispaper proposes a novel, two-layered framework: 1) classifying and 2) directlyrepairing malicious contracts. Slither\u0026rsquo;s vulnerability report is combined withsource code and passed through a pre-trained RandomForestClassifier (RFC) andLarge Language Models (LLMs), classifying and repairing each suggestedvulnerability. Experiments demonstrate the effectiveness of fine-tuned andprompt-engineered LLMs. The smart contract repair models, built frompre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overallvulnerability count by 97.5% and 96.7% respectively. A manual inspection ofrepaired contracts shows that all retain functionality, indicating that theproposed method is appropriate for automatic batch classification and repair ofvulnerabilities in smart contracts.\r2023-09-11\nFuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models\nDongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson\nabstract\rabstract: Jailbreak vulnerabilities in Large Language Models (LLMs), which exploitmeticulously crafted prompts to elicit content that violates serviceguidelines, have captured the attention of research communities. While modelowners can defend against individual jailbreak prompts through safety trainingstrategies, this relatively passive approach struggles to handle the broadercategory of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, anautomated fuzzing framework designed to proactively test and discover jailbreakvulnerabilities in LLMs. We utilize templates to capture the structuralintegrity of a prompt and isolate key features of a jailbreak class asconstraints. By integrating different base classes into powerful combo attacksand varying the elements of constraints and prohibited questions, FuzzLLMenables efficient testing with reduced manual effort. Extensive experimentsdemonstrate FuzzLLM\u0026rsquo;s effectiveness and comprehensiveness in vulnerabilitydiscovery across various LLMs.\r2023-09-10\nThe Impact of AI in Physics Education: A Comprehensive Review from GCSE to University Levels\nWill Yeadon, Tom Hardy\nabstract\rabstract: With the rapid evolution of Artificial Intelligence (AI), its potentialimplications for higher education have become a focal point of interest. Thisstudy delves into the capabilities of AI in Physics Education and offersactionable AI policy recommendations. Using a Large Language Model (LLM), weassessed its ability to answer 1337 Physics exam questions spanning GCSE,A-Level, and Introductory University curricula. We employed various AIprompting techniques: Zero Shot, In Context Learning, and ConfirmatoryChecking, which merges Chain of Thought reasoning with Reflection. The AI\u0026rsquo;sproficiency varied across academic levels: it scored an average of 83.4% onGCSE, 63.8% on A-Level, and 37.4% on university-level questions, with anoverall average of 59.9% using the most effective prompting technique. In aseparate test, the LLM\u0026rsquo;s accuracy on 5000 mathematical operations was found todecrease as the number of digits increased. Furthermore, when evaluated as amarking tool, the LLM\u0026rsquo;s concordance with human markers averaged at 50.8%, withnotable inaccuracies in marking straightforward questions, likemultiple-choice. Given these results, our recommendations underscore caution:while current LLMs can consistently perform well on Physics questions atearlier educational stages, their efficacy diminishes with advanced content andcomplex calculations. LLM outputs often showcase novel methods not in thesyllabus, excessive verbosity, and miscalculations in basic arithmetic. Thissuggests that at university, there\u0026rsquo;s no substantial threat from LLMs fornon-invigilated Physics questions. However, given the LLMs\u0026rsquo; considerableproficiency in writing Physics essays and coding abilities, non-invigilatedexaminations of these skills in Physics are highly vulnerable to automatedcompletion by LLMs. This vulnerability also extends to Physics questionspitched at lower academic levels.\r2023-09-04\nBaseline Defenses for Adversarial Attacks Against Aligned Language Models\nNeel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein\nabstract\rabstract: As Large Language Models quickly become ubiquitous, it becomes critical tounderstand their security vulnerabilities. Recent work shows that textoptimizers can produce jailbreaking prompts that bypass moderation andalignment. Drawing from the rich body of work on adversarial machine learning,we approach these attacks with three questions: What threat models arepractically useful in this domain? How do baseline defense techniques performin this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarialattacks on LLMs, discussing the various settings in which each is feasible andeffective. Particularly, we look at three types of defenses: detection(perplexity based), input preprocessing (paraphrase and retokenization), andadversarial training. We discuss white-box and gray-box settings and discussthe robustness-performance trade-off for each of the defenses considered. Wefind that the weakness of existing discrete optimizers for text, combined withthe relatively high costs of optimization, makes standard adaptive attacks morechallenging for LLMs. Future research will be needed to uncover whether morepowerful optimizers can be developed, or whether the strength of filtering andpreprocessing defenses is greater in the LLMs domain than it has been incomputer vision.\rMathAttack: Attacking Large Language Models Towards Math Solving Ability\nZihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang\nabstract\rabstract: With the boom of Large Language Models (LLMs), the research of solving MathWord Problem (MWP) has recently made great progress. However, there are fewstudies to examine the security of LLMs in math solving ability. Instead ofattacking prompts in the use of LLMs, we propose a MathAttack model to attackMWP samples which are closer to the essence of security in solving mathproblems. Compared to traditional text adversarial attack, it is essential topreserve the mathematical logic of original MWPs during the attacking. To thisend, we propose logical entity recognition to identify logical entries whichare then frozen. Subsequently, the remaining text are attacked by adopting aword-level attacker. Furthermore, we propose a new dataset RobustMath toevaluate the robustness of LLMs in math solving ability. Extensive experimentson our RobustMath and two another math benchmark datasets GSM8K and MultiAirthshow that MathAttack could effectively attack the math solving ability of LLMs.In the experiments, we observe that (1) Our adversarial samples fromhigher-accuracy LLMs are also effective for attacking LLMs with lower accuracy(e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shotprompts); (2) Complex MWPs (such as more solving steps, longer text, morenumbers) are more vulnerable to attack; (3) We can improve the robustness ofLLMs by using our adversarial samples in few-shot prompts. Finally, we hope ourpractice and observation can serve as an important attempt towards enhancingthe robustness of LLMs in math solving ability. We will release our code anddataset.\r2023-09-02\nThe FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification\nNorbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, Vasileios Mavroeidis\nabstract\rabstract: This paper presents the FormAI dataset, a large collection of 112, 000AI-generated compilable and independent C programs with vulnerabilityclassification. We introduce a dynamic zero-shot prompting techniqueconstructed to spawn diverse programs utilizing Large Language Models (LLMs).The dataset is generated by GPT-3.5-turbo and comprises programs with varyinglevels of complexity. Some programs handle complicated tasks like networkmanagement, table games, or encryption, while others deal with simpler taskslike string manipulation. Every program is labeled with the vulnerabilitiesfound within the source code, indicating the type, line number, and vulnerablefunction name. This is accomplished by employing a formal verification methodusing the Efficient SMT-based Bounded Model Checker (ESBMC), which uses modelchecking, abstract interpretation, constraint programming, and satisfiabilitymodulo theories to reason over safety/security properties in programs. Thisapproach definitively detects vulnerabilities and offers a formal model knownas a counterexample, thus eliminating the possibility of generating falsepositive reports. We have associated the identified vulnerabilities with CommonWeakness Enumeration (CWE) numbers. We make the source code available for the112, 000 programs, accompanied by a separate file containing thevulnerabilities detected in each program, making the dataset ideal for trainingLLMs and machine learning algorithms. Our study unveiled that according toESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities,thereby presenting considerable risks to software safety and security.\rCombing for Credentials: Active Pattern Extraction from Smart Reply\nBargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, David Evans\nabstract\rabstract: Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply\u0026rsquo;\u0026rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.\r2023-08-30\nPre-Training Representations of Binary Code Using Contrastive Learning\nYifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao, Scott Thomas Andersen, Huajie Shao, Kevin Leach, Yu Huang\nabstract\rabstract: Compiled software is delivered as executable binary code. Developers writesource code to express the software semantics, but the compiler converts it toa binary format that the CPU can directly execute. Therefore, binary codeanalysis is critical to applications in reverse engineering and computersecurity tasks where source code is not available. However, unlike source codeand natural language that contain rich semantic information, binary code istypically difficult for human engineers to understand and analyze. Whileexisting work uses AI models to assist source code analysis, few studies haveconsidered binary code. In this paper, we propose a COntrastive learning Modelfor Binary cOde Analysis, or COMBO, that incorporates source code and commentinformation into binary code during representation learning. Specifically, wepresent three components in COMBO: (1) a primary contrastive learning methodfor cold-start pre-training, (2) a simplex interpolation method to incorporatesource code, comments, and binary code, and (3) an intermediate representationlearning algorithm to provide binary code embeddings. Finally, we evaluate theeffectiveness of the pre-trained representations produced by COMBO using threeindicative downstream tasks relating to binary code: algorithmic functionalityclassification, binary code similarity, and vulnerability detection. Ourexperimental results show that COMBO facilitates representation learning ofbinary code visualized by distribution analysis, and improves the performanceon all three downstream tasks by 5.45% on average compared to state-of-the-artlarge-scale language representation models. To the best of our knowledge, COMBOis the first language representation model that incorporates source code,binary code, and comments into contrastive code representation learning andunifies multiple tasks for binary code analysis.\r2023-08-28\nFine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts\nThanh Thi Nguyen, Campbell Wilson, Janis Dalins\nabstract\rabstract: Detecting online sexual predatory behaviours and abusive language on socialmedia platforms has become a critical area of research due to the growingconcerns about online safety, especially for vulnerable populations such aschildren and adolescents. Researchers have been exploring various techniquesand approaches to develop effective detection systems that can identify andmitigate these risks. Recent development of large language models (LLMs) hasopened a new opportunity to address this problem more effectively. This paperproposes an approach to detection of online sexual predatory chats and abusivelanguage using the open-source pretrained Llama 2 7B-parameter model, recentlyreleased by Meta GenAI. We fine-tune the LLM using datasets with differentsizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).Based on the power of LLMs, our approach is generic and automated without amanual search for a synergy between feature extraction and classifier designsteps like conventional methods in this domain. Experimental results show astrong performance of the proposed approach, which performs proficiently andconsistently across three distinct datasets with five sets of experiments. Thisstudy\u0026rsquo;s outcomes indicate that the proposed method can be implemented inreal-world applications (even with non-English languages) for flagging sexualpredators, offensive or toxic content, hate speech, and discriminatory languagein online discussions and comments to maintain respectful internet or digitalcommunities. Furthermore, it can be employed for solving text classificationproblems with other potential applications such as sentiment analysis, spam andphishing detection, sorting legal documents, fake news detection, languageidentification, user intent recognition, text-based product categorization,medical record analysis, and resume screening.\r2023-08-27\nA Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation\nXiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa\nabstract\rabstract: Large Language Models (LLMs) have exploded a new heatwave of AI for theirability to engage end-users in human-level conversations with detailed andarticulate answers across many knowledge domains. In response to their fastadoption in many industrial applications, this survey concerns their safety andtrustworthiness. First, we review known vulnerabilities and limitations of theLLMs, categorising them into inherent issues, attacks, and unintended bugs.Then, we consider if and how the Verification and Validation (V\u0026amp;V) techniques,which have been widely developed for traditional software and deep learningmodels such as convolutional neural networks as independent processes to checkthe alignment of their implementations against the specifications, can beintegrated and further extended throughout the lifecycle of the LLMs to providerigorous analysis to the safety and trustworthiness of LLMs and theirapplications. Specifically, we consider four complementary techniques:falsification and evaluation, verification, runtime monitoring, and regulationsand ethical use. In total, 370+ references are considered to support the quickunderstanding of the safety and trustworthiness issues from the perspective ofV\u0026amp;V. While intensive research has been conducted to identify the safety andtrustworthiness issues, rigorous yet practical methods are called for to ensurethe alignment of LLMs with safety and trustworthiness requirements.\r2023-08-24\nPrompt-Enhanced Software Vulnerability Detection Using ChatGPT\nChenyuan Zhang, Hao Liu, Jiutian Zeng, Kejing Yang, Yuhong Li, Hui Li\nabstract\rabstract: With the increase in software vulnerabilities that cause significant economicand social losses, automatic vulnerability detection has become essential insoftware development and maintenance. Recently, large language models (LLMs)like GPT have received considerable attention due to their stunningintelligence, and some studies consider using ChatGPT for vulnerabilitydetection. However, they do not fully consider the characteristics of LLMs,since their designed questions to ChatGPT are simple without a specific promptdesign tailored for vulnerability detection. This paper launches a study on theperformance of software vulnerability detection using ChatGPT with differentprompt designs. Firstly, we complement previous work by applying variousimprovements to the basic prompt. Moreover, we incorporate structural andsequential auxiliary information to improve the prompt design. Besides, weleverage ChatGPT\u0026rsquo;s ability of memorizing multi-round dialogue to designsuitable prompts for vulnerability detection. We conduct extensive experimentson two vulnerability datasets to demonstrate the effectiveness ofprompt-enhanced vulnerability detection using ChatGPT. We also analyze themerit and demerit of using ChatGPT for vulnerability detection.\rZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching\nM. Caner Tol, Berk Sunar\nabstract\rabstract: Security critical software, e.g., OpenSSL, comes with numerous side-channelleakages left unpatched due to a lack of resources or experts. The situationwill only worsen as the pace of code development accelerates, with developersrelying on Large Language Models (LLMs) to automatically generate code. In thiswork, we explore the use of LLMs in generating patches for vulnerable code withmicroarchitectural side-channel leakages. For this, we investigate thegenerative abilities of powerful LLMs by carefully crafting prompts following azero-shot learning approach. All generated code is dynamically analyzed byleakage detection tools, which are capable of pinpointing information leakageat the instruction level leaked either from secret dependent accesses orbranches or vulnerable Spectre gadgets, respectively. Carefully crafted promptsare used to generate candidate replacements for vulnerable code, which are thenanalyzed for correctness and for leakage resilience. From a cost/performanceperspective, the GPT4-based configuration costs in API calls a mere few centsper vulnerability fixed. Our results show that LLM-based patching is far morecost-effective and thus provides a scalable solution. Finally, the framework wepropose will improve in time, especially as vulnerability detection tools andLLMs mature.\rUse of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities\nMaximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin\nabstract\rabstract: Spurred by the recent rapid increase in the development and distribution oflarge language models (LLMs) across industry and academia, much recent work hasdrawn attention to safety- and security-related threats and vulnerabilities ofLLMs, including in the context of potentially criminal activities.Specifically, it has been shown that LLMs can be misused for fraud,impersonation, and the generation of malware; while other authors haveconsidered the more general problem of AI alignment. It is important thatdevelopers and practitioners alike are aware of security-related problems withsuch models. In this paper, we provide an overview of existing - predominantlyscientific - efforts on identifying and mitigating threats and vulnerabilitiesarising from LLMs. We present a taxonomy describing the relationship betweenthreats caused by the generative capabilities of LLMs, prevention measuresintended to address such threats, and vulnerabilities arising from imperfectprevention measures. With our work, we hope to raise awareness of thelimitations of LLMs in light of such security concerns, among both experienceddevelopers and novel users of such technologies.\r2023-08-22\nDistinguishing Look-Alike Innocent and Vulnerable Code by Subtle Semantic Representation Learning and Explanation\nChao Ni, Xin Yin, Kaiwen Yang, Dehai Zhao, Zhenchang Xing, Xin Xia\nabstract\rabstract: Though many deep learning (DL)-based vulnerability detection approaches havebeen proposed and indeed achieved remarkable performance, they still havelimitations in the generalization as well as the practical usage. Moreprecisely, existing DL-based approaches (1) perform negatively on predictiontasks among functions that are lexically similar but have contrary semantics;(2) provide no intuitive developer-oriented explanations to the detectedresults. In this paper, we propose a novel approach named SVulD, afunction-level Subtle semantic embedding for Vulnerability Detection along withintuitive explanations, to alleviate the above limitations. Specifically, SVulDfirstly trains a model to learn distinguishing semantic representations offunctions regardless of their lexical similarity. Then, for the detectedvulnerable functions, SVulD provides natural language explanations (e.g., rootcause) of results to help developers intuitively understand thevulnerabilities. To evaluate the effectiveness of SVulD, we conduct large-scaleexperiments on a widely used practical vulnerability dataset and compare itwith four state-of-the-art (SOTA) approaches by considering five performancemeasures. The experimental results indicate that SVulD outperforms all SOTAswith a substantial improvement (i.e., 23.5%-68.0% in terms of F1-score,15.9%-134.8% in terms of PR-AUC and 7.4%-64.4% in terms of Accuracy). Besides,we conduct a user-case study to evaluate the usefulness of SVulD for developerson understanding the vulnerable code and the participants\u0026rsquo; feedbackdemonstrates that SVulD is helpful for development practice.\rOn the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions\nReza Fayyazi, Shanchieh Jay Yang\nabstract\rabstract: The volume, variety, and velocity of change in vulnerabilities and exploitshave made incident threat analysis challenging with human expertise andexperience along. Tactics, Techniques, and Procedures (TTPs) are to describehow and why attackers exploit vulnerabilities. However, a TTP descriptionwritten by one security professional can be interpreted very differently byanother, leading to confusion in cybersecurity operations or even business,policy, and legal decisions. Meanwhile, advancements in AI have led to theincreasing use of Natural Language Processing (NLP) algorithms to assist thevarious tasks in cyber operations. With the rise of Large Language Models(LLMs), NLP tasks have significantly improved because of the LLM\u0026rsquo;s semanticunderstanding and scalability. This leads us to question how well LLMs caninterpret TTPs or general cyberattack descriptions to inform analysts of theintended purposes of cyberattacks. We propose to analyze and compare the directuse of LLMs (e.g., GPT-3.5) versus supervised fine-tuning (SFT) ofsmall-scale-LLMs (e.g., BERT) to study their capabilities in predicting ATT\u0026amp;CKtactics. Our results reveal that the small-scale-LLMs with SFT provide a morefocused and clearer differentiation between the ATT\u0026amp;CK tactics (if suchdifferentiation exists). On the other hand, direct use of LLMs offer a broaderinterpretation of cyberattack techniques. When treating more general cases,despite the power of LLMs, inherent ambiguity exists and limits theirpredictive power. We then summarize the challenges and recommend researchdirections on LLMs to treat the inherent ambiguity of TTP descriptions used invarious cyber operations.\rAdversarial Attacks on Code Models with Discriminative Graph Patterns\nThanh-Dat Nguyen, Yang Zhou, Xuan Bach D. Le, Patanamon, Thongtanunam, David Lo\nabstract\rabstract: Pre-trained language models of code are now widely used in various softwareengineering tasks such as code generation, code completion, vulnerabilitydetection, etc. This, in turn, poses security and reliability risks to thesemodels. One of the important threats is \\textit{adversarial attacks}, which canlead to erroneous predictions and largely affect model performance ondownstream tasks. Current adversarial attacks on code models usually adoptfixed sets of program transformations, such as variable renaming and dead codeinsertion, leading to limited attack effectiveness. To address theaforementioned challenges, we propose a novel adversarial attack framework,GraphCodeAttack, to better evaluate the robustness of code models. Given atarget code model, GraphCodeAttack automatically mines important code patterns,which can influence the model\u0026rsquo;s decisions, to perturb the structure of inputcode to the model. To do so, GraphCodeAttack uses a set of input source codesto probe the model\u0026rsquo;s outputs and identifies the \\textit{discriminative} ASTspatterns that can influence the model decisions. GraphCodeAttack then selectsappropriate AST patterns, concretizes the selected patterns as attacks, andinserts them as dead code into the model\u0026rsquo;s input program. To effectivelysynthesize attacks from AST patterns, GraphCodeAttack uses a separatepre-trained code model to fill in the ASTs with concrete code snippets. Weevaluate the robustness of two popular code models (e.g., CodeBERT andGraphCodeBERT) against our proposed approach on three tasks: AuthorshipAttribution, Vulnerability Prediction, and Clone Detection. The experimentalresults suggest that our proposed approach significantly outperformsstate-of-the-art approaches in attacking code models such as CARROT and ALERT.\r2023-08-21\nUsing Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions\nWesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, Ee-Chien Chang\nabstract\rabstract: The assessment of cybersecurity Capture-The-Flag (CTF) exercises involvesparticipants finding text strings or ``flags\u0026rsquo;\u0026rsquo; by exploiting systemvulnerabilities. Large Language Models (LLMs) are natural-language modelstrained on vast amounts of words to understand and generate text; they canperform well on many CTF challenges. Such LLMs are freely available tostudents. In the context of CTF exercises in the classroom, this raisesconcerns about academic integrity. Educators must understand LLMs\u0026rsquo; capabilitiesto modify their teaching to accommodate generative AI assistance. This researchinvestigates the effectiveness of LLMs, particularly in the realm of CTFchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,Google Bard, and Microsoft Bing. First, we assess the LLMs\u0026rsquo; question-answeringperformance on five Cisco certifications with varying difficulty levels. Next,we qualitatively study the LLMs\u0026rsquo; abilities in solving CTF challenges tounderstand their limitations. We report on the experience of using the LLMs forseven test cases in all five types of CTF challenges. In addition, wedemonstrate how jailbreak prompts can bypass and break LLMs\u0026rsquo; ethicalsafeguards. The paper concludes by discussing LLM\u0026rsquo;s impact on CTF exercises andits implications.\r2023-08-20\nCan Large Language Models Find And Fix Vulnerable Software?\nDavid Noever\nabstract\rabstract: In this study, we evaluated the capability of Large Language Models (LLMs),particularly OpenAI\u0026rsquo;s GPT-4, in detecting software vulnerabilities, comparingtheir performance against traditional static code analyzers like Snyk andFortify. Our analysis covered numerous repositories, including those from NASAand the Department of Defense. GPT-4 identified approximately four times thevulnerabilities than its counterparts. Furthermore, it provided viable fixesfor each vulnerability, demonstrating a low rate of false positives. Our testsencompassed 129 code samples across eight programming languages, revealing thehighest vulnerabilities in PHP and JavaScript. GPT-4\u0026rsquo;s code corrections led toa 90% reduction in vulnerabilities, requiring only an 11% increase in codelines. A critical insight was LLMs\u0026rsquo; ability to self-audit, suggesting fixes fortheir identified vulnerabilities and underscoring their precision. Futureresearch should explore system-level vulnerabilities and integrate multiplestatic code analyzers for a holistic perspective on LLMs\u0026rsquo; potential.\r2023-08-17\nGetting pwn\u0026rsquo;d by AI: Penetration Testing with Large Language Models\nAndreas Happe, Jürgen Cito\nabstract\rabstract: The field of software security testing, more specifically penetrationtesting, is an activity that requires high levels of expertise and involvesmany manual testing and analysis steps. This paper explores the potential usageof large-language models, such as GPT3.5, to augment penetration testers withAI sparring partners. We explore the feasibility of supplementing penetrationtesters with AI models for two distinct use cases: high-level task planning forsecurity testing assignments and low-level vulnerability hunting within avulnerable virtual machine. For the latter, we implemented a closed-feedbackloop between LLM-generated low-level actions with a vulnerable virtual machine(connected through SSH) and allowed the LLM to analyze the machine state forvulnerabilities and suggest concrete attack vectors which were automaticallyexecuted within the virtual machine. We discuss promising initial results,detail avenues for improvement, and close deliberating on the ethics ofproviding AI-based sparring partners.\r2023-08-16\nVisual Adversarial Examples Jailbreak Aligned Large Language Models\nXiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal\nabstract\rabstract: Recently, there has been a surge of interest in integrating vision into LargeLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such asFlamingo and GPT-4. This paper sheds light on the security and safetyimplications of this trend. First, we underscore that the continuous andhigh-dimensional nature of the visual input makes it a weak link againstadversarial attacks, representing an expanded attack surface ofvision-integrated LLMs. Second, we highlight that the versatility of LLMs alsopresents visual attackers with a wider array of achievable adversarialobjectives, extending the implications of security failures beyond meremisclassification. As an illustration, we present a case study in which weexploit visual adversarial examples to circumvent the safety guardrail ofaligned LLMs with integrated vision. Intriguingly, we discover that a singlevisual adversarial example can universally jailbreak an aligned LLM, compellingit to heed a wide range of harmful instructions that it otherwise would not)and generate harmful content that transcends the narrow scope of a `few-shot\u0026rsquo;derogatory corpus initially employed to optimize the adversarial example. Ourstudy underscores the escalating adversarial risks associated with the pursuitof multimodality. Our findings also connect the long-studied adversarialvulnerabilities of neural networks to the nascent field of AI alignment. Thepresented attack suggests a fundamental adversarial challenge for AI alignment,especially in light of the emerging trend toward multimodality in frontierfoundation models.\r2023-08-15\nFrom Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\nRodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos\nabstract\rabstract: Large Language Models (LLMs) have found widespread applications in variousdomains, including web applications, where they facilitate human interactionvia chatbots with natural language interfaces. Internally, aided by anLLM-integration middleware such as Langchain, user prompts are translated intoSQL queries used by the LLM to provide meaningful responses to users. However,unsanitized user prompts can lead to SQL injection attacks, potentiallycompromising the security of the database. Despite the growing interest inprompt injection vulnerabilities targeting LLMs, the specific risks ofgenerating SQL injection attacks through prompt injections have not beenextensively studied. In this paper, we present a comprehensive examination ofprompt-to-SQL (P$_2$SQL) injections targeting web applications based on theLangchain framework. Using Langchain as our case study, we characterizeP$_2$SQL injections, exploring their variants and impact on applicationsecurity through multiple concrete examples. Furthermore, we evaluate 7state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacksacross language models. Our findings indicate that LLM-integrated applicationsbased on Langchain are highly susceptible to P$_2$SQL injection attacks,warranting the adoption of robust defenses. To counter these attacks, wepropose four effective defense techniques that can be integrated as extensionsto the Langchain framework. We validate the defenses through an experimentalevaluation with a real-world use case application.\rRobustness Over Time: Understanding Adversarial Examples\u0026rsquo; Effectiveness on Longitudinal Versions of Large Language Models\nYugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang\nabstract\rabstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.\r2023-08-14\nDIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection\nSudipta Paria, Aritra Dasgupta, Swarup Bhunia\nabstract\rabstract: Securing critical assets in a bus-based System-On-Chip (SoC) is imperative tomitigate potential vulnerabilities and prevent unauthorized access, ensuringthe integrity, availability, and confidentiality of the system. Ensuringsecurity throughout the SoC design process is a formidable task owing to theinherent intricacies in SoC designs and the dispersion of assets across diverseIPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD(Google), have showcased remarkable proficiency across various domains,including security vulnerability detection and prevention in SoC designs. Inthis work, we propose DIVAS, a novel framework that leverages the knowledgebase of LLMs to identify security vulnerabilities from user-defined SoCspecifications, map them to the relevant Common Weakness Enumerations (CWEs),followed by the generation of equivalent assertions, and employ securitymeasures through enforcement of security policies. The proposed framework isimplemented using multiple ChatGPT and BARD models, and their performance wasanalyzed while generating relevant CWEs from the SoC specifications provided.The experimental results obtained from open-source SoC benchmarks demonstratethe efficacy of our proposed framework.\r2023-08-09\nVulLibGen: Identifying Vulnerable Third-Party Libraries via Generative Pre-Trained Model\nTianyu Chen, Lin Li, Liuchuan Zhu, Zongyang Li, Guangtai Liang, Ding Li, Qianxiang Wang, Tao Xie\nabstract\rabstract: To avoid potential risks posed by vulnerabilities in third-party libraries,security researchers maintain vulnerability databases (e.g., NVD) containingvulnerability reports, each of which records the description of a vulnerabilityand the name list of libraries affected by the vulnerability (a.k.a. vulnerablelibraries). However, recent studies on about 200,000 vulnerability reports inNVD show that 53.3% of these reports do not include the name list of vulnerablelibraries, and 59.82% of the included name lists of vulnerable libraries areincomplete or incorrect. To address the preceding issue, in this paper, we propose the firstgenerative approach named VulLibGen to generate the name list of vulnerablelibraries (out of all the existing libraries) for the given vulnerability byutilizing recent enormous advances in Large Language Models (LLMs), in order toachieve high accuracy. VulLibGen takes only the description of a vulnerabilityas input and achieves high identification accuracy based on LLMs\u0026rsquo; priorknowledge of all the existing libraries. VulLibGen also includes the inputaugmentation technique to help identify zero-shot vulnerable libraries (thosenot occurring during training) and the post-processing technique to helpaddress VulLibGen\u0026rsquo;s hallucinations. We evaluate VulLibGen using threestate-of-the-art/practice approaches (LightXML, Chronos, and VulLibMiner) thatidentify vulnerable libraries on an open-source dataset (VulLib). Ourevaluation results show that VulLibGen can accurately identify vulnerablelibraries with an average F1 score of 0.626 while the state-of-the-art/practiceapproaches achieve only 0.561. The post-processing technique helps VulLibGenachieve an average improvement of F1@1 by 9.3%. The input augmentationtechnique helps VulLibGen achieve an average improvement of F1@1 by 39% inidentifying zero-shot libraries.\rDiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection\nYizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, David Wagner\nabstract\rabstract: We propose and release a new vulnerable source code dataset. We curate thedataset by crawling security issue websites, extracting vulnerability-fixingcommits and source codes from the corresponding projects. Our new datasetcontains 18,945 vulnerable functions spanning 150 CWEs and 330,492non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295more projects than all previous datasets combined. Combining our new dataset with previous datasets, we present an analysis ofthe challenges and promising research directions of using deep learning fordetecting software vulnerabilities. We study 11 model architectures belongingto 4 families. Our results show that deep learning is still not ready forvulnerability detection, due to high false positive rate, low F1 score, anddifficulty of detecting hard CWEs. In particular, we demonstrate an importantgeneralization challenge for the deployment of deep learning-based models. Weshow that increasing the volume of training data may not further improve theperformance of deep learning models for vulnerability detection, but might beuseful to improve the generalization ability to unseen projects. We also identify hopeful future research directions. We demonstrate thatlarge language models (LLMs) are a promising research direction for ML-basedvulnerability detection, outperforming Graph Neural Networks (GNNs) withcode-structure features in our experiments. Moreover, developing source codespecific pre-training objectives is a promising research direction to improvethe vulnerability detection performance.\r2023-08-08\nAgentSims: An Open-Source Sandbox for Large Language Model Evaluation\nJiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen\nabstract\rabstract: With ChatGPT-like large language models (LLM) prevailing in the community,how to evaluate the ability of LLMs is an open question. Existing evaluationmethods suffer from following shortcomings: (1) constrained evaluationabilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest thattask-based evaluation, where LLM agents complete tasks in a simulatedenvironment, is a one-for-all solution to solve above problems. We presentAgentSims, an easy-to-use infrastructure for researchers from all disciplinesto test the specific capacities they are interested in. Researchers can buildtheir evaluation tasks by adding agents and buildings on an interactive GUI ordeploy and test new support mechanisms, i.e. memory, planning and tool-usesystems, by a few lines of codes. Our demo is available athttps://agentsims.com .\r2023-08-07\nMondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing\nWai Man Si, Michael Backes, Yang Zhang\nabstract\rabstract: The Machine Learning as a Service (MLaaS) market is rapidly expanding andbecoming more mature. For example, OpenAI\u0026rsquo;s ChatGPT is an advanced largelanguage model (LLM) that generates responses for various queries withassociated fees. Although these models can deliver satisfactory performance,they are far from perfect. Researchers have long studied the vulnerabilitiesand limitations of LLMs, such as adversarial attacks and model toxicity.Inevitably, commercial ML models are also not exempt from such issues, whichcan be problematic as MLaaS continues to grow. In this paper, we discover a newattack strategy against LLM APIs, namely the prompt abstraction attack.Specifically, we propose Mondrian, a simple and straightforward method thatabstracts sentences, which can lower the cost of using LLM APIs. In thisapproach, the adversary first creates a pseudo API (with a lower establishedprice) to serve as the proxy of the target API (with a higher establishedprice). Next, the pseudo API leverages Mondrian to modify the user query,obtain the abstracted response from the target API, and forward it back to theend user. Our results show that Mondrian successfully reduces user queries\u0026rsquo;token length ranging from 13% to 23% across various tasks, including textclassification, generation, and question answering. Meanwhile, these abstractedqueries do not significantly affect the utility of task-specific and generallanguage models like ChatGPT. Mondrian also reduces instruction prompts\u0026rsquo; tokenlength by at least 11% without compromising output quality. As a result, theprompt abstraction attack enables the adversary to profit without bearing thecost of API development and deployment.\r2023-07-26\nSet-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models\nDong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, Feng Zheng\nabstract\rabstract: Vision-language pre-training (VLP) models have shown vulnerability toadversarial examples in multimodal tasks. Furthermore, malicious adversariescan be deliberately transferred to attack other black-box models. However,existing work has mainly focused on investigating white-box attacks. In thispaper, we present the first study to investigate the adversarialtransferability of recent VLP models. We observe that existing methods exhibitmuch lower transferability, compared to the strong attack performance inwhite-box settings. The transferability degradation is partly caused by theunder-utilization of cross-modal interactions. Particularly, unlike unimodallearning, VLP models rely heavily on cross-modal interactions and themultimodal alignments are many-to-many, e.g., an image can be described invarious natural languages. To this end, we propose a highly transferableSet-level Guidance Attack (SGA) that thoroughly leverages modality interactionsand incorporates alignment-preserving augmentation with cross-modal guidance.Experimental results demonstrate that SGA could generate adversarial examplesthat can strongly transfer across different VLP models on multiple downstreamvision-language tasks. On image-text retrieval, SGA significantly enhances theattack success rate for transfer attacks from ALBEF to TCL by a large margin(at least 9.78% and up to 30.21%), compared to the state-of-the-art.\r2023-07-25\nFoundational Models Defining a New Era in Vision: A Survey and Outlook\nMuhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan\nabstract\rabstract: Vision systems to see and reason about the compositional nature of visualscenes are fundamental to understanding our world. The complex relationsbetween objects and their locations, ambiguities, and variations in thereal-world environment can be better described in human language, naturallygoverned by grammatical rules and other modalities such as audio and depth. Themodels learned to bridge the gap between such modalities coupled withlarge-scale training data facilitate contextual reasoning, generalization, andprompt capabilities at test time. These models are referred to as foundationalmodels. The output of such models can be modified through human-providedprompts without retraining, e.g., segmenting a particular object by providing abounding box, having interactive dialogues by asking questions about an imageor video scene or manipulating the robot\u0026rsquo;s behavior through languageinstructions. In this survey, we provide a comprehensive review of suchemerging foundational models, including typical architecture designs to combinedifferent modalities (vision, text, audio, etc), training objectives(contrastive, generative), pre-training datasets, fine-tuning mechanisms, andthe common prompting patterns; textual, visual, and heterogeneous. We discussthe open challenges and research directions for foundational models in computervision, including difficulties in their evaluations and benchmarking, gaps intheir real-world understanding, limitations of their contextual understanding,biases, vulnerability to adversarial attacks, and interpretability issues. Wereview recent developments in this field, covering a wide range of applicationsof foundation models systematically and comprehensively. A comprehensive listof foundational models studied in this work is available at\\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.\r2023-07-20\nPluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck\nZhiwei Fu, Steven H. H. Ding, Furkan Alaca, Benjamin C. M. Fung, Philippe Charland\nabstract\rabstract: The practice of code reuse is crucial in software development for a fasterand more efficient development lifecycle. In reality, however, code reusepractices lack proper control, resulting in issues such as vulnerabilitypropagation and intellectual property infringements. Assembly clone search, acritical shift-right defence mechanism, has been effective in identifyingvulnerable code resulting from reuse in released executables. Recent studies onassembly clone search demonstrate a trend towards using machine learning-basedmethods to match assembly code variants produced by different toolchains.However, these methods are limited to what they learn from a small number oftoolchain variants used in training, rendering them inapplicable to unseenarchitectures and their corresponding compilation toolchain variants. This paper presents the first study on the problem of assembly clone searchwith unseen architectures and libraries. We propose incorporating human commonknowledge through large-scale pre-trained natural language models, in the formof transfer learning, into current learning-based approaches for assembly clonesearch. Transfer learning can aid in addressing the limitations of the existingapproaches, as it can bring in broader knowledge from human experts in assemblycode. We further address the sequence limit issue by proposing a reinforcementlearning agent to remove unnecessary and redundant tokens. Coupled with a newVariational Information Bottleneck learning strategy, the proposed systemminimizes the reliance on potential indicators of architectures andoptimization settings, for a better generalization of unseen architectures. Wesimulate the unseen architecture clone search scenarios and the experimentalresults show the effectiveness of the proposed approach against thestate-of-the-art solutions.\r2023-07-19\nWhat can we learn from Data Leakage and Unlearning for Law?\nJaydeep Borkar\nabstract\rabstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users\u0026rsquo; PII.In order to comply with privacy laws such as the \u0026ldquo;right to be forgotten\u0026rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.\r2023-07-17\nA Lightweight Framework for High-Quality Code Generation\nMohammed Latif Siddiq, Beatrice Casey, Joanna C. S. Santos\nabstract\rabstract: In recent years, the use of automated source code generation utilizingtransformer-based generative models has expanded, and these models can generatefunctional code according to the requirements of the developers. However,recent research revealed that these automatically generated source codes cancontain vulnerabilities and other quality issues. Despite researchers\u0026rsquo; andpractitioners\u0026rsquo; attempts to enhance code generation models, retraining andfine-tuning large language models is time-consuming and resource-intensive.Thus, we describe FRANC, a lightweight framework for recommending more secureand high-quality source code derived from transformer-based code generationmodels. FRANC includes a static filter to make the generated code compilablewith heuristics and a quality-aware ranker to sort the code snippets based on aquality score. Moreover, the framework uses prompt engineering to fixpersistent quality issues. We evaluated the framework with five Python and Javacode generation models and six prompt datasets, including a newly created onein this work (SOEval). The static filter improves 9% to 46% Java suggestionsand 10% to 43% Python suggestions regarding compilability. The averageimprovement over the NDCG@10 score for the ranking system is 0.0763, and therepairing techniques repair the highest 80% of prompts. FRANC takes, onaverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds.\r2023-07-14\nCertified Robustness for Large Language Models with Self-Denoising\nZhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang\nabstract\rabstract: Although large language models (LLMs) have achieved great success in vastreal-world applications, their vulnerabilities towards noisy inputs havesignificantly limited their uses, especially in high-stake environments. Inthese contexts, it is crucial to ensure that every prediction made by largelanguage models is stable, i.e., LLM predictions should be consistent givenminor differences in the input. This largely falls into the study of certifiedrobust LLMs, i.e., all predictions of LLM are certified to be correct in alocal region around the input. Randomized smoothing has demonstrated greatpotential in certifying the robustness and prediction stability of LLMs.However, randomized smoothing requires adding noise to the input before modelprediction, and its certification performance depends largely on the model\u0026rsquo;sperformance on corrupted data. As a result, its direct application to LLMsremains challenging and often results in a small certification radius. Toaddress this issue, we take advantage of the multitasking nature of LLMs andpropose to denoise the corrupted inputs with LLMs in a self-denoising manner.Different from previous works like denoised smoothing, which requires traininga separate model to robustify LLM, our method enjoys far better efficiency andflexibility. Our experiment results show that our method outperforms theexisting certification methods under both certified robustness and empiricalrobustness. The codes are available athttps://github.com/UCSB-NLP-Chang/SelfDenoise.\r2023-07-13\nSecureFalcon: The Next Cyber Reasoning System for Cyber Security\nMohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Merouane Debbah, Thierry Lestable, Lucas C. Cordeiro\nabstract\rabstract: Software vulnerabilities leading to various detriments such as crashes, dataloss, and security breaches, significantly hinder the quality, affecting themarket adoption of software applications and systems. Although traditionalmethods such as automated software testing, fault localization, and repair havebeen intensively studied, static analysis tools are most commonly used and havean inherent false positives rate, posing a solid challenge to developerproductivity. Large Language Models (LLMs) offer a promising solution to thesepersistent issues. Among these, FalconLLM has shown substantial potential inidentifying intricate patterns and complex vulnerabilities, hence crucial insoftware vulnerability detection. In this paper, for the first time, FalconLLMis being fine-tuned for cybersecurity applications, thus introducingSecureFalcon, an innovative model architecture built upon FalconLLM.SecureFalcon is trained to differentiate between vulnerable and non-vulnerableC code samples. We build a new training dataset, FormAI, constructed thanks toGenerative Artificial Intelligence (AI) and formal verification to evaluate itsperformance. SecureFalcon achieved an impressive 94% accuracy rate in detectingsoftware vulnerabilities, emphasizing its significant potential to redefinesoftware vulnerability detection methods in cybersecurity.\r2023-07-05\nJailbroken: How Does LLM Safety Training Fail?\nAlexander Wei, Nika Haghtalab, Jacob Steinhardt\nabstract\rabstract: Large language models trained for safety and harmlessness remain susceptibleto adversarial misuse, as evidenced by the prevalence of \u0026ldquo;jailbreak\u0026rdquo; attacks onearly releases of ChatGPT that elicit undesired behavior. Going beyondrecognition of the issue, we investigate why such attacks succeed and how theycan be created. We hypothesize two failure modes of safety training: competingobjectives and mismatched generalization. Competing objectives arise when amodel\u0026rsquo;s capabilities and safety goals conflict, while mismatched generalizationoccurs when safety training fails to generalize to a domain for whichcapabilities exist. We use these failure modes to guide jailbreak design andthen evaluate state-of-the-art models, including OpenAI\u0026rsquo;s GPT-4 and Anthropic\u0026rsquo;sClaude v1.3, against both existing and newly designed attacks. We find thatvulnerabilities persist despite the extensive red-teaming and safety-trainingefforts behind these models. Notably, new attacks utilizing our failure modessucceed on every prompt in a collection of unsafe requests from the models\u0026rsquo;red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Ouranalysis emphasizes the need for safety-capability parity \u0026ndash; that safetymechanisms should be as sophisticated as the underlying model \u0026ndash; and arguesagainst the idea that scaling alone can resolve these safety failure modes.\r2023-07-04\nSCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification\nJunjie Wu, Dit-Yan Yeung\nabstract\rabstract: Despite their promising performance across various natural languageprocessing (NLP) tasks, current NLP systems are vulnerable to textualadversarial attacks. To defend against these attacks, most existing methodsapply adversarial training by incorporating adversarial examples. However,these methods have to rely on ground-truth labels to generate adversarialexamples, rendering it impractical for large-scale model pre-training which iscommonly used nowadays for NLP and many other tasks. In this paper, we proposea novel learning framework called SCAT (Self-supervised Contrastive Learningvia Adversarial Training), which can learn robust representations withoutrequiring labeled data. Specifically, SCAT modifies random augmentations of thedata in a fully labelfree manner to generate adversarial examples. Adversarialtraining is achieved by minimizing the contrastive loss between theaugmentations and their adversarial counterparts. We evaluate SCAT on two textclassification datasets using two state-of-the-art attack schemes proposedrecently. Our results show that SCAT can not only train robust language modelsfrom scratch, but it can also significantly improve the robustness of existingpre-trained language models. Moreover, to demonstrate its flexibility, we showthat SCAT can also be combined with supervised adversarial training to furtherenhance model robustness.\r2023-06-26\nSugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna\nabstract\rabstract: In the last year alone, a surge of new benchmarks to measure compositionalunderstanding of vision-language models have permeated the machine learningecosystem. Given an image, these benchmarks probe a model\u0026rsquo;s ability to identifyits associated caption amongst a set of compositional distractors.Surprisingly, we find significant biases in all these benchmarks rendering themhackable. This hackability is so dire that blind models with no access to theimage outperform state-of-the-art vision-language models. To remedy thisrampant vulnerability, we introduce SugarCrepe, a new benchmark forvision-language compositionality evaluation. We employ large language models,instead of rule-based templates used in previous benchmarks, to generate fluentand sensical hard negatives, and utilize an adversarial refinement mechanism tomaximally reduce biases. We re-evaluate state-of-the-art models and recentlyproposed compositionality inducing strategies, and find that their improvementswere hugely overestimated, suggesting that more innovation is needed in thisimportant direction. We release SugarCrepe and the code for evaluation at:https://github.com/RAIVNLab/sugar-crepe.\r2023-06-24\nLLM-assisted Generation of Hardware Assertions\nRahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Shailja Thakur, Ramesh Karri, Jeyavijayan Rajendran\nabstract\rabstract: The security of computer systems typically relies on a hardware root oftrust. As vulnerabilities in hardware can have severe implications on a system,there is a need for techniques to support security verification activities.Assertion-based verification is a popular verification technique that involvescapturing design intent in a set of assertions that can be used in formalverification or testing-based checking. However, writing security-centricassertions is a challenging task. In this work, we investigate the use ofemerging large language models (LLMs) for code generation in hardware assertiongeneration for security, where primarily natural language prompts, such asthose one would see as code comments in assertion files, are used to produceSystemVerilog assertions. We focus our attention on a popular LLM andcharacterize its ability to write assertions out of the box, given varyinglevels of detail in the prompt. We design an evaluation framework thatgenerates a variety of prompts, and we create a benchmark suite comprisingreal-world hardware designs and corresponding golden reference assertions thatwe want to generate with the LLM.\r2023-06-22\nDo you still need a manual smart contract audit?\nIsaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, Arthur Gervais\nabstract\rabstract: We investigate the feasibility of employing large language models (LLMs) forconducting the security audit of smart contracts, a traditionallytime-consuming and costly process. Our research focuses on the optimization ofprompt engineering for enhanced security analysis, and we evaluate theperformance and accuracy of LLMs using a benchmark dataset comprising 52Decentralized Finance (DeFi) smart contracts that have previously beencompromised. Our findings reveal that, when applied to vulnerable contracts, both GPT-4and Claude models correctly identify the vulnerability type in 40% of thecases. However, these models also demonstrate a high false positive rate,necessitating continued involvement from manual auditors. The LLMs testedoutperform a random model by 20% in terms of F1-score. To ensure the integrity of our study, we conduct mutation testing on fivenewly developed and ostensibly secure smart contracts, into which we manuallyinsert two and 15 vulnerabilities each. This testing yielded a remarkablebest-case 78.7% true positive rate for the GPT-4-32k model. We tested both,asking the models to perform a binary classification on whether a contract isvulnerable, and a non-binary prompt. We also examined the influence of modeltemperature variations and context length on the LLM\u0026rsquo;s performance. Despite the potential for many further enhancements, this work lays thegroundwork for a more efficient and economical approach to smart contractsecurity audits.\r2023-06-16\nLarge Language Models Sometimes Generate Purely Negatively-Reinforced Text\nFabien Roger\nabstract\rabstract: When using adversarial training, it is common practice to train against themost egregious failures. However, this might imply using examples withsensitive information (such as leaked passwords or security vulnerabilities) astraining data. One might assume that language models trained with gradientdescent never generate text snippets which were only present in examplesassociated with the lowest possible reward. In this paper, we show that thisassumption is wrong: in some situations, large language models do learn fromsuch negatively-reinforced examples. We present a specific training setup thatenables Pythia-160M to guess passwords 13% more often than it would by guessingrandomly, despite only showing it these passwords on examples where the modelis incentivized to not output these passwords. Our code is available atwww.github.com/FabienRoger/Learning-From-Negative-Examples\r2023-06-11\nAugmenting Greybox Fuzzing with Generative AI\nJie Hu, Qian Zhang, Heng Yin\nabstract\rabstract: Real-world programs expecting structured inputs often has a format-parsingstage gating the deeper program space. Neither a mutation-based approach nor agenerative approach can provide a solution that is effective and scalable.Large language models (LLM) pre-trained with an enormous amount of naturallanguage corpus have proved to be effective for understanding the implicitformat syntax and generating format-conforming inputs. In this paper, proposeChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, wepick a seed in the fuzzer\u0026rsquo;s seed pool and prompt ChatGPT generative models tovariations, which are more likely to be format-conforming and thus of highquality. We conduct extensive experiments to explore the best practice forharvesting the power of generative LLM models. The experiment results show thatour approach improves the edge coverage by 12.77% over the SOTA greybox fuzzer(AFL++) on 12 target programs from three well-tested benchmarks. As forvulnerability detection, \\sys is able to perform similar to or better thanAFL++ for programs with explicit syntax rules but not for programs withnon-trivial syntax.\r2023-06-09\nTowards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?\nWissam Antoun, Virginie Mouilleron, Benoît Sagot, Djamé Seddah\nabstract\rabstract: Recent advances in natural language processing (NLP) have led to thedevelopment of large language models (LLMs) such as ChatGPT. This paperproposes a methodology for developing and evaluating ChatGPT detectors forFrench text, with a focus on investigating their robustness on out-of-domaindata and against common attack schemes. The proposed method involvestranslating an English dataset into French and training a classifier on thetranslated data. Results show that the detectors can effectively detectChatGPT-generated text, with a degree of robustness against basic attacktechniques in in-domain settings. However, vulnerabilities are evident inout-of-domain contexts, highlighting the challenge of detecting adversarialtext. The study emphasizes caution when applying in-domain testing results to awider variety of content. We provide our translated datasets and models asopen-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection\rReliability Check: An Analysis of GPT-3\u0026rsquo;s Response to Sensitive Topics and Prompt Wording\nAisha Khatun, Daniel G. Brown\nabstract\rabstract: Large language models (LLMs) have become mainstream technology with theirversatile use cases and impressive performance. Despite the countlessout-of-the-box applications, LLMs are still not reliable. A lot of work isbeing done to improve the factual accuracy, consistency, and ethical standardsof these models through fine-tuning, prompting, and Reinforcement Learning withHuman Feedback (RLHF), but no systematic analysis of the responses of thesemodels to different categories of statements, or on their potentialvulnerabilities to simple prompting changes is available. In this work, weanalyze what confuses GPT-3: how the model responds to certain sensitive topicsand what effects the prompt wording has on the model response. We find thatGPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makesmistakes with common Misconceptions and Controversies. The model responses areinconsistent across prompts and settings, highlighting GPT-3\u0026rsquo;s unreliability.Dataset and code of our analysis is available inhttps://github.com/tanny411/GPT3-Reliability-Check.\rSelf-Distillation for Further Pre-training of Transformers\nSeanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi\nabstract\rabstract: Pre-training a large transformer model on a massive amount of unlabeled dataand fine-tuning it on labeled datasets for diverse downstream tasks has provento be a successful strategy, for a variety of vision and natural languageprocessing tasks. However, direct fine-tuning of the pre-trained model may besuboptimal if there exist large discrepancies across data domains forpre-training and fine-tuning. To tackle this issue, several previous studieshave proposed further pre-training strategies, where we continue to pre-trainthe model on the target unlabeled dataset before fine-tuning. However, all ofthem solely focus on language models and we empirically find that a VisionTransformer is vulnerable to overfitting as we continue to pretrain the modelon target unlabeled data. In order to tackle this limitation, we proposeself-distillation as a regularization for a further pre-training stage.Specifically, we first further pre-train the initial pre-trained model on thetarget unlabeled data and then consider it as a teacher for self-distillation.Then we take the same initial pre-trained model as a student and enforce itshidden representations to be close to those of the teacher while optimizing thestudent with a masked auto-encoding objective. We empirically validate theefficacy of self-distillation on a variety of benchmark datasets for image andtext classification tasks. Experimentally, we show that our proposed methodoutperforms all the relevant baselines. Theoretically, we analyze the proposedmethod with a simplified model to understand how self-distillation for furtherpre-training can potentially help improve the performance of the downstreamtasks.\r2023-06-05\nLmPa: Improving Decompilation by Synergy of Large Language Model and Program Analysis\nXiangzhe Xu, Zhuo Zhang, Shiwei Feng, Yapeng Ye, Zian Su, Nan Jiang, Siyuan Cheng, Lin Tan, Xiangyu Zhang\nabstract\rabstract: Decompilation aims to recover the source code form of a binary executable. Ithas many applications in security and software engineering such as malwareanalysis, vulnerability detection and code reuse. A prominent challenge indecompilation is to recover variable names. We propose a novel method thatleverages the synergy of large language model (LLM) and program analysis.Language models encode rich multi-modal knowledge, but its limited input sizeprevents providing sufficient global context for name recovery. We propose todivide the task to many LLM queries and use program analysis to correlate andpropagate the query results, which in turn improves the performance of LLM byproviding additional contextual information. Our results show that 75% of therecovered names are considered good by users and our technique outperforms thestate-of-the-art technique by 16.5% and 20.23% in precision and recall,respectively.\rBuilding Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia\nBenjamin Kereopa-Yorke\nabstract\rabstract: The escalating digitalisation of our lives and enterprises has led to aparallel growth in the complexity and frequency of cyber-attacks. Small andmedium-sized enterprises (SMEs), particularly in Australia, are experiencingincreased vulnerability to cyber threats, posing a significant challenge to thenation\u0026rsquo;s cyber security landscape. Embracing transformative technologies suchas Artificial Intelligence (AI), Machine Learning (ML) and Large LanguageModels (LLMs) can potentially strengthen cyber security policies for AustralianSMEs. However, their practical application, advantages, and limitations remainunderexplored, with prior research mainly focusing on large corporations. Thisstudy aims to address this gap by providing a comprehensive understanding ofthe potential role of LLMs in enhancing cyber security policies for AustralianSMEs. Employing a mixed-methods study design, this research includes aliterature review, qualitative analysis of SME case studies, and a quantitativeassessment of LLM performance metrics in cyber security applications. Thefindings highlight the promising potential of LLMs across various performancecriteria, including relevance, accuracy, and applicability, though gaps remainin areas such as completeness and clarity. The study underlines the importanceof integrating human expertise with LLM technology and refining modeldevelopment to address these limitations. By proposing a robust conceptualframework guiding the effective adoption of LLMs, this research aims tocontribute to a safer and more resilient cyber environment for Australian SMEs,enabling sustainable growth and competitiveness in the digital era.\r2023-06-02\nAre You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark\nWenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie\nabstract\rabstract: Large language models (LLMs) have demonstrated powerful capabilities in bothtext understanding and generation. Companies have begun to offer Embedding as aService (EaaS) based on these LLMs, which can benefit various natural languageprocessing (NLP) tasks for customers. However, previous studies have shown thatEaaS is vulnerable to model extraction attacks, which can cause significantlosses for the owners of LLMs, as training these models is extremely expensive.To protect the copyright of LLMs for EaaS, we propose an Embedding Watermarkmethod called EmbMarker that implants backdoors on embeddings. Our methodselects a group of moderate-frequency words from a general text corpus to forma trigger set, then selects a target embedding as the watermark, and inserts itinto the embeddings of texts containing trigger words as the backdoor. Theweight of insertion is proportional to the number of trigger words included inthe text. This allows the watermark backdoor to be effectively transferred toEaaS-stealer\u0026rsquo;s model for copyright verification while minimizing the adverseimpact on the original embeddings\u0026rsquo; utility. Our extensive experiments onvarious datasets show that our method can effectively protect the copyright ofEaaS models without compromising service quality.\r2023-05-29\nHow Effective Are Neural Networks for Fixing Security Vulnerabilities\nYi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, Sameena Shah\nabstract\rabstract: Security vulnerability repair is a difficult task that is in dire need ofautomation. Two groups of techniques have shown promise: (1) large codelanguage models (LLMs) that have been pre-trained on source code for tasks suchas code completion, and (2) automated program repair (APR) techniques that usedeep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repaircapabilities of LLMs and DL-based APR models. The contributions include that we(1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder),four fine-tuned LLMs, and four DL-based APR techniques on two real-world Javavulnerability benchmarks (Vul4J and VJBench), (2) design code transformationsto address the training and test data overlapping threat to Codex, (3) create anew Java vulnerability repair benchmark VJBench, and its transformed versionVJBench-trans and (4) evaluate LLMs and APR techniques on the transformedvulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Javavulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities.(2) Fine-tuning with general APR data improves LLMs\u0026rsquo; vulnerability-fixingcapabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fixmany Common Weakness Enumeration (CWE) types, such as CWE-325 Missingcryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes8.3 transformed vulnerabilities, outperforming all the other LLMs and APRmodels on transformed vulnerabilities. The results call for innovations toenhance automated Java vulnerability repair such as creating largervulnerability repair training data, tuning LLMs with such data, and applyingcode simplification transformation to facilitate vulnerability repair.\r2023-05-24\nA New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification\nYiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, Lucas C. Cordeiro\nabstract\rabstract: In this paper we present a novel solution that combines the capabilities ofLarge Language Models (LLMs) with Formal Verification strategies to verify andautomatically repair software vulnerabilities. Initially, we employ BoundedModel Checking (BMC) to locate the software vulnerability and derive acounterexample. The counterexample provides evidence that the system behavesincorrectly or contains a vulnerability. The counterexample that has beendetected, along with the source code, are provided to the LLM engine. Ourapproach involves establishing a specialized prompt language for conductingcode debugging and generation to understand the vulnerability\u0026rsquo;s root cause andrepair the code. Finally, we use BMC to verify the corrected version of thecode generated by the LLM. As a proof of concept, we create ESBMC-AI based onthe Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trainedTransformer model, specifically gpt-3.5-turbo, to detect and fix errors in Cprograms. Our experimentation involved generating a dataset comprising 1000 Ccode samples, each consisting of 20 to 50 lines of code. Notably, our proposedmethod achieved an impressive success rate of up to 80% in repairing vulnerablecode encompassing buffer overflow and pointer dereference failures. We assertthat this automated approach can effectively incorporate into the softwaredevelopment lifecycle\u0026rsquo;s continuous integration and deployment (CI/CD) process.\rFrom Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads\nP. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, Sandeep K Shukla\nabstract\rabstract: This research article critically examines the potential risks andimplications arising from the malicious utilization of large languagemodels(LLM), focusing specifically on ChatGPT and Google\u0026rsquo;s Bard. Although theselarge language models have numerous beneficial applications, the misuse of thistechnology by cybercriminals for creating offensive payloads and tools is asignificant concern. In this study, we systematically generated implementablecode for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, andconduct a comparative analysis of its performance with Google\u0026rsquo;s Bard. Ourexperimentation reveals that ChatGPT has the potential to enable attackers toaccelerate the operation of more targeted and sophisticated attacks.Additionally, the technology provides amateur attackers with more capabilitiesto perform a wide range of attacks and empowers script kiddies to developcustomized tools that contribute to the acceleration of cybercrime.Furthermore, LLMs significantly benefits malware authors, particularlyransomware gangs, in generating sophisticated variants of wiper and ransomwareattacks with ease. On a positive note, our study also highlights how offensivesecurity researchers and pentesters can make use of LLMs to simulate realisticattack scenarios, identify potential vulnerabilities, and better protectorganizations. Overall, we conclude by emphasizing the need for increasedvigilance in mitigating the risks associated with LLMs. This includesimplementing robust security measures, increasing awareness and educationaround the potential risks of this technology, and collaborating with securityexperts to stay ahead of emerging threats.\rFlocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\nHaonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch\nabstract\rabstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock\u0026rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.\rAnthropomorphization of AI: Opportunities and Risks\nAmeet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan\nabstract\rabstract: Anthropomorphization is the tendency to attribute human-like traits tonon-human entities. It is prevalent in many social contexts \u0026ndash; childrenanthropomorphize toys, adults do so with brands, and it is a literary device.It is also a versatile tool in science, with behavioral psychology andevolutionary biology meticulously documenting its consequences. With widespreadadoption of AI systems, and the push from stakeholders to make it human-likethrough alignment techniques, human voice, and pictorial avatars, the tendencyfor users to anthropomorphize it increases significantly. We take a dyadicapproach to understanding this phenomenon with large language models (LLMs) bystudying (1) the objective legal implications, as analyzed through the lens ofthe recent blueprint of AI bill of rights and the (2) subtle psychologicalaspects customization and anthropomorphization. We find that anthropomorphizedLLMs customized for different user bases violate multiple provisions in thelegislative blueprint. In addition, we point out that anthropomorphization ofLLMs affects the influence they can have on their users, thus having thepotential to fundamentally change the nature of human-AI interaction, withpotential for manipulation and negative influence. With LLMs beinghyper-personalized for vulnerable groups like children and patients amongothers, our work is a timely and important contribution. We propose aconservative strategy for the cautious use of anthropomorphization to improvetrustworthiness of AI systems.\r2023-05-23\nTransformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?\nAaron Chan, Anant Kharkar, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Alec Helyar, Eslam Kamal, Mohamed Elkamhawy, Neel Sundaresan\nabstract\rabstract: Software vulnerabilities bear enterprises significant costs. Despiteextensive efforts in research and development of software vulnerabilitydetection methods, uncaught vulnerabilities continue to put software owners andusers at risk. Many current vulnerability detection methods require that codesnippets can compile and build before attempting detection. This,unfortunately, introduces a long latency between the time a vulnerability isinjected to the time it is removed, which can substantially increases the costof fixing a vulnerability. We recognize that the current advances in machinelearning can be used to detect vulnerable code patterns on syntacticallyincomplete code snippets as the developer is writing the code at EditTime. Inthis paper we present a practical system that leverages deep learning on alarge-scale data set of vulnerable code patterns to learn complexmanifestations of more than 250 vulnerability types and detect vulnerable codepatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuningapproaches on state of the art pre-trained Large Language Models (LLMs). Weshow that in comparison with state of the art vulnerability detection modelsour approach improves the state of the art by 10%. We also evaluate ourapproach to detect vulnerability in auto-generated code by code LLMs.Evaluation on a benchmark of high-risk code scenarios shows a reduction of upto 90% vulnerability reduction.\r2023-05-10\nFedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature\nWenyuan Yang, Gongxi Zhu, Yuguo Yin, Hanlin Gu, Lixin Fan, Qiang Yang, Xiaochun Cao\nabstract\rabstract: Federated learning allows multiple parties to collaborate in learning aglobal model without revealing private data. The high cost of training and thesignificant value of the global model necessitates the need for ownershipverification of federated learning. However, the existing ownershipverification schemes in federated learning suffer from several limitations,such as inadequate support for a large number of clients and vulnerability toambiguity attacks. To address these limitations, we propose a cryptographicsignature-based federated learning model ownership verification scheme namedFedSOV. FedSOV allows numerous clients to embed their ownership credentials andverify ownership using unforgeable digital signatures. The scheme providestheoretical resistance to ambiguity attacks with the unforgeability of thesignature. Experimental results on computer vision and natural languageprocessing tasks demonstrate that FedSOV is an effective federated modelownership verification scheme enhanced with provable cryptographic security.\rA Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems\nNicolò Pagan, Joachim Baumann, Ezzat Elokda, Giulia De Pasquale, Saverio Bolognani, Anikó Hannák\nabstract\rabstract: Prediction-based decision-making systems are becoming increasingly prevalentin various domains. Previous studies have demonstrated that such systems arevulnerable to runaway feedback loops, e.g., when police are repeatedly sentback to the same neighborhoods regardless of the actual rate of criminalactivity, which exacerbate existing biases. In practice, the automateddecisions have dynamic feedback effects on the system itself that canperpetuate over time, making it difficult for short-sighted design choices tocontrol the system\u0026rsquo;s evolution. While researchers started proposing longer-termsolutions to prevent adverse outcomes (such as bias towards certain groups),these interventions largely depend on ad hoc modeling assumptions and arigorous theoretical understanding of the feedback dynamics in ML-baseddecision-making systems is currently missing. In this paper, we use thelanguage of dynamical systems theory, a branch of applied mathematics thatdeals with the analysis of the interconnection of systems with dynamicbehaviors, to rigorously classify the different types of feedback loops in theML-based decision-making pipeline. By reviewing existing scholarly work, weshow that this classification covers many examples discussed in the algorithmicfairness community, thereby providing a unifying and principled framework tostudy feedback loops. By qualitative analysis, and through a simulation exampleof recommender systems, we show which specific types of ML biases are affectedby each type of feedback loop. We find that the existence of feedback loops inthe ML-based decision-making pipeline can perpetuate, reinforce, or even reduceML biases.\r2023-05-06\nReactive Perturbation Defocusing for Textual Adversarial Defense\nHeng Yang, Ke Li\nabstract\rabstract: Recent studies have shown that large pre-trained language models arevulnerable to adversarial attacks. Existing methods attempt to reconstruct theadversarial examples. However, these methods usually have limited performancein defense against adversarial examples, while also negatively impacting theperformance on natural examples. To overcome this problem, we propose a methodcalled Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detectorto identify adversarial examples and reduce false defenses on natural examples.Instead of reconstructing the adversaries, RPD injects safe perturbations intoadversarial examples to distract the objective models from the maliciousperturbations. Our experiments on three datasets, two objective models, andvarious adversarial attacks show that our proposed framework successfullyrepairs up to approximately 97% of correctly identified adversarial exampleswith only about a 2% performance decrease on natural examples. We also providea demo of adversarial detection and repair based on our work.\r2023-05-05\nNot what you\u0026rsquo;ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\nKai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz\nabstract\rabstract: Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks\u0026rsquo; practical viability againstboth real-world systems, such as Bing\u0026rsquo;s GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication\u0026rsquo;s functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.\r2023-04-19\nHow Secure is Code Generated by ChatGPT?\nRaphaël Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara\nabstract\rabstract: In recent years, large language models have been responsible for greatadvances in the field of artificial intelligence (AI). ChatGPT in particular,an AI chatbot developed and recently released by OpenAI, has taken the field tothe next level. The conversational model is able not only to process human-liketext, but also to translate natural language into code. However, the safety ofprograms generated by ChatGPT should not be overlooked. In this paper, weperform an experiment to address this issue. Specifically, we ask ChatGPT togenerate a number of program and evaluate the security of the resulting sourcecode. We further investigate whether ChatGPT can be prodded to improve thesecurity by appropriate prompts, and discuss the ethical aspects of using AI togenerate code. Results suggest that ChatGPT is aware of potentialvulnerabilities, but nonetheless often generates source code that are notrobust to certain attacks.\r2023-04-04\nLarge Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT\nYinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, Lingming Zhang\nabstract\rabstract: Deep Learning (DL) library bugs affect downstream DL applications,emphasizing the need for reliable systems. Generating valid input programs forfuzzing DL libraries is challenging due to the need for satisfying bothlanguage syntax/semantics and constraints for constructing valid computationalgraphs. Recently, the TitanFuzz work demonstrates that modern Large LanguageModels (LLMs) can be directly leveraged to implicitly learn all the constraintsto generate valid DL programs for fuzzing. However, LLMs tend to generateordinary programs following similar patterns seen in their massive trainingcorpora, while fuzzing favors unusual inputs that cover edge cases or areunlikely to be manually produced. To fill this gap, this paper proposes FuzzGPT, the first technique to primeLLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on thewell-known hypothesis that historical bug-triggering programs may includerare/valuable code ingredients important for bug finding. Traditionaltechniques leveraging such historical information require intensive humanefforts to design dedicated generators and ensure the validity of generatedprograms. FuzzGPT demonstrates that this process can be fully automated via theintrinsic capabilities of LLMs (including fine-tuning and in-context learning),while being generalizable and applicable to challenging domains. While FuzzGPTcan be applied with different LLMs, this paper focuses on the powerfulGPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potentialof directly leveraging the instruct-following capability of the recent ChatGPTfor effective fuzzing. Evaluation on two popular DL libraries (PyTorch andTensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz,detecting 76 bugs, with 49 already confirmed as previously unknown bugs,including 11 high-priority bugs or security vulnerabilities.\r2023-03-20\nLarge Language Models and Simple, Stupid Bugs\nKevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan\nabstract\rabstract: With the advent of powerful neural language models, AI-based systems toassist developers in coding tasks are becoming widely available; Copilot is onesuch system. Copilot uses Codex, a large language model (LLM), to complete codeconditioned on a preceding \u0026ldquo;prompt\u0026rdquo;. Codex, however, is trained on publicGitHub repositories, viz., on code that may include bugs and vulnerabilities.Previous studies [1], [2] show Codex reproduces vulnerabilities seen intraining. In this study, we examine how prone Codex is to generate aninteresting bug category, single statement bugs, commonly referred to assimple, stupid bugs or SStuBs in the MSR community. We find that Codex andsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBsas much as 2x as likely than known, verbatim correct code. We explore theconsequences of the Codex generated SStuBs and propose avoidance strategiesthat suggest the possibility of reducing the production of known, verbatimSStubs, and increase the possibility of producing known, verbatim fixes.\r2023-03-16\nLLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations\nCatherine Tony, Markus Mutas, Nicolás E. Díaz Ferreyra, Riccardo Scandariato\nabstract\rabstract: Large Language Models (LLMs) like Codex are powerful tools for performingcode completion and code generation tasks as they are trained on billions oflines of code from publicly available sources. Moreover, these models arecapable of generating code snippets from Natural Language (NL) descriptions bylearning languages and programming practices from public GitHub repositories.Although LLMs promise an effortless NL-driven deployment of softwareapplications, the security of the code they generate has not been extensivelyinvestigated nor documented. In this work, we present LLMSecEval, a datasetcontaining 150 NL prompts that can be leveraged for assessing the securityperformance of such models. Such prompts are NL descriptions of code snippetsprone to various security vulnerabilities listed in MITRE\u0026rsquo;s Top 25 CommonWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with asecure implementation example to facilitate comparative evaluations againstcode produced by LLMs. As a practical application, we show how LLMSecEval canbe used for evaluating the security of snippets automatically generated from NLdescriptions.\r2023-03-09\nOn Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex\nTerry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li\nabstract\rabstract: Semantic parsing is a technique aimed at constructing a structuredrepresentation of the meaning of a natural-language question. Recentadvancements in few-shot language models trained on code have demonstratedsuperior performance in generating these representations compared totraditional unimodal language models, which are trained on downstream tasks.Despite these advancements, existing fine-tuned neural semantic parsers aresusceptible to adversarial attacks on natural-language inputs. While it hasbeen established that the robustness of smaller semantic parsers can beenhanced through adversarial training, this approach is not feasible for largelanguage models in real-world scenarios, as it requires both substantialcomputational resources and expensive human annotation on in-domain semanticparsing data. This paper presents the first empirical study on the adversarialrobustness of a large prompt-based language model of code, \\codex. Our resultsdemonstrate that the state-of-the-art (SOTA) code-language models arevulnerable to carefully crafted adversarial examples. To address thischallenge, we propose methods for improving robustness without the need forsignificant amounts of labeled data or heavy computational resources.\r2023-03-07\nVulnerability Mimicking Mutants\nAayush Garg, Renzo Degiovanni, Mike Papadakis, Yves Le Traon\nabstract\rabstract: With the increasing release of powerful language models trained on large codecorpus (e.g. CodeBERT was trained on 6.4 million programs), a new family ofmutation testing tools has arisen with the promise to generate more \u0026ldquo;natural\u0026quot;mutants in the sense that the mutated code aims at following the implicit rulesand coding conventions typically produced by programmers. In this paper, westudy to what extent the mutants produced by language models can semanticallymimic the observable behavior of security-related vulnerabilities (a.k.a.Vulnerability-mimicking Mutants), so that designing test cases that are failedby these mutants will help in tackling mimicked vulnerabilities. Sinceanalyzing and running mutants is computationally expensive, it is important toprioritize those mutants that are more likely to be vulnerability mimickingprior to any analysis or test execution. Taking this into account, we introduceVMMS, a machine learning based approach that automatically extracts thefeatures from mutants and predicts the ones that mimic vulnerabilities. Weconducted our experiments on a dataset of 45 vulnerabilities and found that16.6% of the mutants fail one or more tests that are failed by 88.9% of therespective vulnerabilities. More precisely, 3.9% of the mutants from the entiremutant set are vulnerability-mimicking mutants that mimic 55.6% of thevulnerabilities. Despite the scarcity, VMMS predicts vulnerability-mimickingmutants with 0.63 MCC, 0.80 Precision, and 0.51 Recall, demonstrating that thefeatures of vulnerability-mimicking mutants can be automatically learned bymachine learning models to statically predict these without the need ofinvesting effort in defining such features.\r2023-02-27\nLost at C: A User Study on the Security Implications of Large Language Model Code Assistants\nGustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, Brendan Dolan-Gavitt\nabstract\rabstract: Large Language Models (LLMs) such as OpenAI Codex are increasingly being usedas AI-based coding assistants. Understanding the impact of these tools ondevelopers\u0026rsquo; code is paramount, especially as recent work showed that LLMs maysuggest cybersecurity vulnerabilities. We conduct a security-driven user study(N=58) to assess code written by student programmers when assisted by LLMs.Given the potential severity of low-level bugs as well as their relativefrequency in real-world projects, we tasked participants with implementing asingly-linked \u0026lsquo;shopping list\u0026rsquo; structure in C. Our results indicate that thesecurity impact in this setting (low-level C with pointer and arraymanipulations) is small: AI-assisted users produce critical security bugs at arate no greater than 10% more than the control, indicating the use of LLMs doesnot introduce new security risks.\r2023-02-23\nDetecting software vulnerabilities using Language Models\nMarwan Omar\nabstract\rabstract: Recently, deep learning techniques have garnered substantial attention fortheir ability to identify vulnerable code patterns accurately. However, currentstate-of-the-art deep learning models, such as Convolutional Neural Networks(CNN), and Long Short-Term Memories (LSTMs) require substantial computationalresources. This results in a level of overhead that makes their implementationunfeasible for deployment in realtime settings. This study presents a noveltransformer-based vulnerability detection framework, referred to as VulDetect,which is achieved through the fine-tuning of a pre-trained large languagemodel, (GPT) on various benchmark datasets of vulnerable code. Our empiricalfindings indicate that our framework is capable of identifying vulnerablesoftware code with an accuracy of up to 92.65%. Our proposed techniqueoutperforms SyseVR and VulDeBERT, two state-of-the-art vulnerability detectiontechniques\r2023-02-20\nExploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain\nKuntal Kumar Pal, Kazuaki Kashihara, Ujjwala Anantheswaran, Kirby C. Kuznia, Siddhesh Jagtap, Chitta Baral\nabstract\rabstract: With the increase in cybersecurity vulnerabilities of software systems, theways to exploit them are also increasing. Besides these, malware threats,irregular network interactions, and discussions about exploits in public forumsare also on the rise. To identify these threats faster, to detect potentiallyrelevant entities from any texts, and to be aware of software vulnerabilities,automated approaches are necessary. Application of natural language processing(NLP) techniques in the Cybersecurity domain can help in achieving this.However, there are challenges such as the diverse nature of texts involved inthe cybersecurity domain, the unavailability of large-scale publicly availabledatasets, and the significant cost of hiring subject matter experts forannotations. One of the solutions is building multi-task models that can betrained jointly with limited data. In this work, we introduce a generativemulti-task model, Unified Text-to-Text Cybersecurity (UTS), trained on malwarereports, phishing site URLs, programming code constructs, social media data,blogs, news articles, and public forum posts. We show UTS improves theperformance of some cybersecurity datasets. We also show that with a fewexamples, UTS can be adapted to novel unseen tasks and the nature of data\r2023-02-16\nTalking About Large Language Models\nMurray Shanahan\nabstract\rabstract: Thanks to rapid progress in artificial intelligence, we have entered an erawhen technology and philosophy intersect in interesting ways. Sitting squarelyat the centre of this intersection are large language models (LLMs). The moreadept LLMs become at mimicking human language, the more vulnerable we become toanthropomorphism, to seeing the systems in which they are embedded as morehuman-like than they really are. This trend is amplified by the naturaltendency to use philosophically loaded terms, such as \u0026ldquo;knows\u0026rdquo;, \u0026ldquo;believes\u0026rdquo;, and\u0026quot;thinks\u0026quot;, when describing these systems. To mitigate this trend, this paperadvocates the practice of repeatedly stepping back to remind ourselves of howLLMs, and the systems of which they form a part, actually work. The hope isthat increased scientific precision will encourage more philosophical nuance inthe discourse around artificial intelligence, both within the field and in thepublic sphere.\r2023-02-08\nTraining-free Lexical Backdoor Attacks on Language Models\nYujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen\nabstract\rabstract: Large-scale language models have achieved tremendous success across variousnatural language processing (NLP) applications. Nevertheless, language modelsare vulnerable to backdoor attacks, which inject stealthy triggers into modelsfor steering them to undesirable behaviors. Most existing backdoor attacks,such as data poisoning, require further (re)training or fine-tuning languagemodels to learn the intended backdoor patterns. The additional training processhowever diminishes the stealthiness of the attacks, as training a languagemodel usually requires long optimization time, a massive amount of data, andconsiderable modifications to the model parameters. In this work, we proposeTraining-Free Lexical Backdoor Attack (TFLexAttack) as the first training-freebackdoor attack on language models. Our attack is achieved by injecting lexicaltriggers into the tokenizer of a language model via manipulating its embeddingdictionary using carefully designed rules. These rules are explainable to humandevelopers which inspires attacks from a wider range of hackers. The sparsemanipulation of the dictionary also habilitates the stealthiness of our attack.We conduct extensive experiments on three dominant NLP tasks based on ninelanguage models to demonstrate the effectiveness and universality of ourattack. The code of this work is available athttps://github.com/Jinxhy/TFLexAttack.\r2023-01-29\nDr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness\nShuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang\nabstract\rabstract: Neural text-to-SQL models have achieved remarkable performance in translatingnatural language questions into SQL queries. However, recent studies revealthat text-to-SQL models are vulnerable to task-specific perturbations. Previouscurated robustness test sets usually focus on individual phenomena. In thispaper, we propose a comprehensive robustness benchmark based on Spider, across-domain text-to-SQL benchmark, to diagnose the model robustness. We design17 perturbations on databases, natural language questions, and SQL queries tomeasure the robustness from different angles. In order to collect morediversified natural question perturbations, we utilize large pretrainedlanguage models (PLMs) to simulate human behaviors in creating naturalquestions. We conduct a diagnostic study of the state-of-the-art models on therobustness set. Experimental results reveal that even the most robust modelsuffers from a 14.0% performance drop overall and a 50.7% performance drop onthe most challenging perturbation. We also present a breakdown analysisregarding text-to-SQL model designs and provide insights for improving modelrobustness.\r2022-12-16\nPlanting and Mitigating Memorized Content in Predictive-Text Language Models\nC. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa\nabstract\rabstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both \u0026ldquo;heuristic\u0026rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define\u0026quot;sensitive\u0026quot; or \u0026ldquo;private\u0026rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.\r2022-12-03\nDCDetector: An IoT terminal vulnerability mining system based on distributed deep ensemble learning under source code representation\nWen Zhou\nabstract\rabstract: Context: The IoT system infrastructure platform facility vulnerability attackhas become the main battlefield of network security attacks. Most of thetraditional vulnerability mining methods rely on vulnerability detection toolsto realize vulnerability discovery. However, due to the inflexibility of toolsand the limitation of file size, its scalability It is relatively low andcannot be applied to large-scale power big data fields. Objective: The goal ofthe research is to intelligently detect vulnerabilities in source codes ofhigh-level languages such as C/C++. This enables us to propose a coderepresentation of sensitive sentence-related slices of source code, and todetect vulnerabilities by designing a distributed deep ensemble learning model.Method: In this paper, a new directional vulnerability mining method ofparallel ensemble learning is proposed to solve the problem of large-scale datavulnerability mining. By extracting sensitive functions and statements, asensitive statement library of vulnerable codes is formed. The AST stream-basedvulnerability code slice with higher granularity performs doc2vec sentencevectorization on the source code through the random sampling module, obtainsdifferent classification results through distributed training through theBi-LSTM trainer, and obtains the final classification result by voting.Results: This method designs and implements a distributed deep ensemblelearning system software vulnerability mining system called DCDetector. It canmake accurate predictions by using the syntactic information of the code, andis an effective method for analyzing large-scale vulnerability data.Conclusion: Experiments show that this method can reduce the false positiverate of traditional static analysis and improve the performance and accuracy ofmachine learning.\r2022-11-23\nProgram Repair\nXiang Gao, Yannic Noller, Abhik Roychoudhury\nabstract\rabstract: Automated program repair is an emerging technology which consists of a suiteof techniques to automatically fix bugs or vulnerabilities in programs. In thispaper, we present a comprehensive survey of the state of the art in programrepair. We first study the different suite of techniques used including searchbased repair, constraint based repair and learning based repair. We thendiscuss one of the main challenges in program repair namely patch overfitting,by distilling a class of techniques which can alleviate patch overfitting. Wethen discuss classes of program repair tools, applications of program repair aswell as uses of program repair in industry. We conclude the survey with aforward looking outlook on future usages of program repair, as well as researchopportunities arising from work on code from large language models.\r2022-11-18\nCheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs\nAnna Filighera, Sebastian Ochs, Tim Steuer, Thomas Tregel\nabstract\rabstract: Automatic grading models are valued for the time and effort saved during theinstruction of large student bodies. Especially with the increasingdigitization of education and interest in large-scale standardized testing, thepopularity of automatic grading has risen to the point where commercialsolutions are widely available and used. However, for short answer formats,automatic grading is challenging due to natural language ambiguity andversatility. While automatic short answer grading models are beginning tocompare to human performance on some datasets, their robustness, especially toadversarially manipulated data, is questionable. Exploitable vulnerabilities ingrading models can have far-reaching consequences ranging from cheatingstudents receiving undeserved credit to undermining automatic gradingaltogether - even when most predictions are valid. In this paper, we devise ablack-box adversarial attack tailored to the educational short answer gradingscenario to investigate the grading models\u0026rsquo; robustness. In our attack, weinsert adjectives and adverbs into natural places of incorrect student answers,fooling the model into predicting them as correct. We observed a loss ofprediction accuracy between 10 and 22 percentage points using thestate-of-the-art models BERT and T5. While our attack made answers appear lessnatural to humans in our experiments, it did not significantly increase thegraders\u0026rsquo; suspicions of cheating. Based on our experiments, we providerecommendations for utilizing automatic grading systems more safely inpractice.\r2022-11-17\nIgnore Previous Prompt: Attack Techniques For Language Models\nFábio Perez, Ian Ribeiro\nabstract\rabstract: Transformer-based large language models (LLMs) provide a powerful foundationfor natural language tasks in large-scale customer-facing applications.However, studies that explore their vulnerabilities emerging from malicioususer interaction are scarce. By proposing PromptInject, a prosaic alignmentframework for mask-based iterative adversarial prompt composition, we examinehow GPT-3, the most widely deployed language model in production, can be easilymisaligned by simple handcrafted inputs. In particular, we investigate twotypes of attacks \u0026ndash; goal hijacking and prompt leaking \u0026ndash; and demonstrate thateven low-aptitude, but sufficiently ill-intentioned agents, can easily exploitGPT-3\u0026rsquo;s stochastic nature, creating long-tail risks. The code for PromptInjectis available at https://github.com/agencyenterprise/PromptInject.\r2022-11-04\nMemorization in NLP Fine-tuning Methods\nFatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, Taylor Berg-Kirkpatrick\nabstract\rabstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the\u0026quot;pre-train and fine-tune\u0026quot; paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.\r2022-10-31\nPneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task\nNyoungwoo Lee, ChaeHun Park, Ho-Jin Choi, Jaegul Choo\nabstract\rabstract: In retrieval-based dialogue systems, a response selection model acts as aranker to select the most appropriate response among several candidates.However, such selection models tend to rely on context-response contentsimilarity, which makes models vulnerable to adversarial responses that aresemantically similar but not relevant to the dialogue context. Recent studieshave shown that leveraging these adversarial responses as negative trainingsamples is useful for improving the discriminating power of the selectionmodel. Nevertheless, collecting human-written adversarial responses isexpensive, and existing synthesizing methods often have limited scalability. Toovercome these limitations, this paper proposes a simple but efficient methodfor generating adversarial negative responses leveraging a large-scale languagemodel. Experimental results on dialogue selection tasks show that our methodoutperforms other methods of synthesizing adversarial negative responses. Theseresults suggest that our method can be an effective alternative to humanannotators in generating adversarial responses. Our dataset and generation codeis available at https://github.com/leenw23/generating-negatives-by-gpt3.\r2022-10-23\nIdentifying Crisis Response Communities in Online Social Networks for Compound Disasters: The Case of Hurricane Laura and Covid-19\nKhondhaker Al Momin, H M Imran Kays, Arif Mohaimin Sadri\nabstract\rabstract: Online social networks allow different agencies and the public to interactand share the underlying risks and protective actions during major disasters.This study revealed such crisis communication patterns during hurricane Lauracompounded by the COVID-19 pandemic. Laura was one of the strongest (Category4) hurricanes on record to make landfall in Cameron, Louisiana. Using theApplication Programming Interface (API), this study utilizes large-scale socialmedia data obtained from Twitter through the recently released academic trackthat provides complete and unbiased observations. The data captured publiclyavailable tweets shared by active Twitter users from the vulnerable areasthreatened by Laura. Online social networks were based on user influencefeature ( mentions or tags) that allows notifying other users while posting atweet. Using network science theories and advanced community detectionalgorithms, the study split these networks into twenty-one components ofvarious sizes, the largest of which contained eight well-defined communities.Several natural language processing techniques (i.e., word clouds, bigrams,topic modeling) were applied to the tweets shared by the users in thesecommunities to observe their risk-taking or risk-averse behavior during a majorcompounding crisis. Social media accounts of local news media, radio,universities, and popular sports pages were among those who involved heavilyand interacted closely with local residents. In contrast, emergency managementand planning units in the area engaged less with the public. The findings ofthis study provide novel insights into the design of efficient social mediacommunication guidelines to respond better in future disasters.\r2022-10-18\nFine-mixing: Mitigating Backdoors in Fine-tuned Language Models\nZhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun\nabstract\rabstract: Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.In Natural Language Processing (NLP), DNNs are often backdoored during thefine-tuning process of a large-scale Pre-trained Language Model (PLM) withpoisoned samples. Although the clean weights of PLMs are readily available,existing methods have ignored this information in defending NLP models againstbackdoor attacks. In this work, we take the first step to exploit thepre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned languagemodels. Specifically, we leverage the clean pre-trained weights via twocomplementary techniques: (1) a two-step Fine-mixing technique, which firstmixes the backdoored weights (fine-tuned on poisoned data) with the pre-trainedweights, then fine-tunes the mixed weights on a small subset of clean data; (2)an Embedding Purification (E-PUR) technique, which mitigates potentialbackdoors existing in the word embeddings. We compare Fine-mixing with typicalbackdoor mitigation methods on three single-sentence sentiment classificationtasks and two sentence-pair classification tasks and show that it outperformsthe baselines by a considerable margin in all scenarios. We also show that ourE-PUR method can benefit existing mitigation methods. Our work establishes asimple but strong baseline defense for secure fine-tuned NLP models againstbackdoor attacks.\r2022-09-06\nTransformer-Based Language Models for Software Vulnerability Detection\nChandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, Surya Nepal\nabstract\rabstract: The large transformer-based language models demonstrate excellent performancein natural language processing. By considering the transferability of theknowledge gained by these models in one domain to other related domains, andthe closeness of natural languages to high-level programming languages, such asC/C++, this work studies how to leverage (large) transformer-based languagemodels in detecting software vulnerabilities and how good are these models forvulnerability detection tasks. In this regard, firstly, a systematic (cohesive)framework that details source code translation, model preparation, andinference is presented. Then, an empirical analysis is performed with softwarevulnerability datasets with C/C++ source codes having multiple vulnerabilitiescorresponding to the library function call, pointer usage, array usage, andarithmetic expression. Our empirical results demonstrate the good performanceof the language models in vulnerability detection. Moreover, these languagemodels have better performance metrics, such as F1-score, than the contemporarymodels, namely bidirectional long short-term memory and bidirectional gatedrecurrent unit. Experimenting with the language models is always challengingdue to the requirement of computing resources, platforms, libraries, anddependencies. Thus, this paper also analyses the popular platforms toefficiently fine-tune these models and present recommendations while choosingthe platforms.\r2022-09-05\nEvaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples\nHezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, Ramesh Darwishi\nabstract\rabstract: Recent advances in the development of large language models have resulted inpublic access to state-of-the-art pre-trained language models (PLMs), includingGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional EncoderRepresentations from Transformers (BERT). However, evaluations of PLMs, inpractice, have shown their susceptibility to adversarial attacks during thetraining and fine-tuning stages of development. Such attacks can result inerroneous outputs, model-generated hate speech, and the exposure of users\u0026rsquo;sensitive information. While existing research has focused on adversarialattacks during either the training or the fine-tuning of PLMs, there is adeficit of information on attacks made between these two development phases. Inthis work, we highlight a major security vulnerability in the public release ofGPT-3 and further investigate this vulnerability in other state-of-the-artPLMs. We restrict our work to pre-trained models that have not undergonefine-tuning. Further, we underscore token distance-minimized perturbations asan effective adversarial approach, bypassing both supervised and unsupervisedquality measures. Following this approach, we observe a significant decrease intext classification quality when evaluating for semantic similarity.\r2022-09-01\nWhy Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?\nSunjae Kwon, Cheongwoong Kang, Jiyeon Han, Jaesik Choi\nabstract\rabstract: Many contextualized word representations are now learned by intricate neuralnetwork models, such as masked neural language models (MNLMs) which are made upof huge neural network structures and trained to restore the masked text. Suchrepresentations demonstrate superhuman performance in some readingcomprehension (RC) tasks which extract a proper answer in the context given aquestion. However, identifying the detailed knowledge trained in MNLMs ischallenging owing to numerous and intermingled model parameters. This paperprovides new insights and empirical analyses on commonsense knowledge includedin pretrained MNLMs. First, we use a diagnostic test that evaluates whethercommonsense knowledge is properly trained in MNLMs. We observe that a largeproportion of commonsense knowledge is not appropriately trained in MNLMs andMNLMs do not often understand the semantic meaning of relations accurately. Inaddition, we find that the MNLM-based RC models are still vulnerable tosemantic variations that require commonsense knowledge. Finally, we discoverthe fundamental reason why some knowledge is not trained. We further suggestthat utilizing an external commonsense knowledge repository can be an effectivesolution. We exemplify the possibility to overcome the limitations of theMNLM-based RC models by enriching text with the required knowledge from anexternal commonsense knowledge repository in controlled experiments.\r2022-08-15\nExamining Zero-Shot Vulnerability Repair with Large Language Models\nHammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt\nabstract\rabstract: Human developers can produce code with cybersecurity bugs. Can emerging\u0026rsquo;smart\u0026rsquo; code completion tools help repair those bugs? In this work, we examinethe use of large language models (LLMs) for code (such as OpenAI\u0026rsquo;s Codex andAI21\u0026rsquo;s Jurassic J-1) for zero-shot vulnerability repair. We investigatechallenges in the design of prompts that coax LLMs into generating repairedversions of insecure code. This is difficult due to the numerous ways to phrasekey information - both semantically and syntactically - with natural languages.We perform a large scale study of five commercially available, black-box,\u0026ldquo;off-the-shelf\u0026rdquo; LLMs, as well as an open-source model and our ownlocally-trained model, on a mix of synthetic, hand-crafted, and real-worldsecurity bug scenarios. Our experiments demonstrate that while the approach haspromise (the LLMs could collectively repair 100% of our synthetically generatedand hand-crafted scenarios), a qualitative evaluation of the model\u0026rsquo;sperformance over a corpus of historical real-world examples highlightschallenges in generating functionally correct code.\r2022-08-03\nMulticlass ASMA vs Targeted PGD Attack in Image Segmentation\nJohnson Vo, Jiabao Xie, Sahil Patel\nabstract\rabstract: Deep learning networks have demonstrated high performance in a large varietyof applications, such as image classification, speech recognition, and naturallanguage processing. However, there exists a major vulnerability exploited bythe use of adversarial attacks. An adversarial attack imputes images byaltering the input image very slightly, making it nearly undetectable to thenaked eye, but results in a very different classification by the network. Thispaper explores the projected gradient descent (PGD) attack and the AdaptiveMask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model usingtwo types of architectures: MobileNetV3 and ResNet50, It was found that PGD wasvery consistent in changing the segmentation to be its target while thegeneralization of ASMA to a multiclass target was not as effective. Theexistence of such attack however puts all of image classification deep learningnetworks in danger of exploitation.\r2022-07-21\nRethinking Textual Adversarial Defense for Pre-trained Language Models\nJiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao\nabstract\rabstract: Although pre-trained language models (PrLMs) have achieved significantsuccess, recent studies demonstrate that PrLMs are vulnerable to adversarialattacks. By generating adversarial examples with slight perturbations ondifferent levels (sentence / word / character), adversarial attacks can foolPrLMs to generate incorrect predictions, which questions the robustness ofPrLMs. However, we find that most existing textual adversarial examples areunnatural, which can be easily distinguished by both human and machine. Basedon a general anomaly detector, we propose a novel metric (Degree of Anomaly) asa constraint to enable current adversarial attack approaches to generate morenatural and imperceptible adversarial examples. Under this new constraint, thesuccess rate of existing attacks drastically decreases, which reveals that therobustness of PrLMs is not as fragile as they claimed. In addition, we findthat four types of randomization can invalidate a large portion of textualadversarial examples. Based on anomaly detector and randomization, we design auniversal defense framework, which is among the first to perform textualadversarial defense without knowing the specific attack. Empirical results showthat our universal defense framework achieves comparable or even higherafter-attack accuracy with other specific defenses, while preserving higheroriginal accuracy at the same time. Our work discloses the essence of textualadversarial attacks, and indicates that (1) further works of adversarialattacks should focus more on how to overcome the detection and resist therandomization, otherwise their adversarial examples would be easily detectedand invalidated; and (2) compared with the unnatural and perceptibleadversarial examples, it is those undetectable adversarial examples that posereal risks for PrLMs and require more attention for future robustness-enhancingstrategies.\r2022-06-11\nSemAttack: Natural Textual Attacks via Different Semantic Spaces\nBoxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, Bo Li\nabstract\rabstract: Recent studies show that pre-trained language models (LMs) are vulnerable totextual adversarial attacks. However, existing attack methods either sufferfrom low attack success rates or fail to search efficiently in theexponentially large perturbation space. We propose an efficient and effectiveframework SemAttack to generate natural adversarial text by constructingdifferent semantic perturbation functions. In particular, SemAttack optimizesthe generated perturbations constrained on generic semantic spaces, includingtypo space, knowledge space (e.g., WordNet), contextualized semantic space(e.g., the embedding space of BERT clusterings), or the combination of thesespaces. Thus, the generated adversarial texts are more semantically close tothe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) arestill vulnerable to SemAttack. We further demonstrate that SemAttack is generaland able to generate natural adversarial texts for different languages (e.g.,English and Chinese) with high attack success rates. Human evaluations alsoconfirm that our generated adversarial texts are natural and barely affecthuman performance. Our code is publicly available athttps://github.com/AI-secure/SemAttack.\r2022-05-25\njTrans: Jump-Aware Transformer for Binary Code Similarity\nHao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, Chao Zhang\nabstract\rabstract: Binary code similarity detection (BCSD) has important applications in variousfields such as vulnerability detection, software component analysis, andreverse engineering. Recent studies have shown that deep neural networks (DNNs)can comprehend instructions or control-flow graphs (CFG) of binary code andsupport BCSD. In this study, we propose a novel Transformer-based approach,namely jTrans, to learn representations of binary code. It is the firstsolution that embeds control flow information of binary code intoTransformer-based language models, by using a novel jump-aware representationof the analyzed binaries and a newly-designed pre-training task. Additionally,we release to the community a newly-created large dataset of binaries,BinaryCorp, which is the most diverse to date. Evaluation results show thatjTrans outperforms state-of-the-art (SOTA) approaches on this more challengingdataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of knownvulnerability searching, jTrans achieves a recall that is 2X higher thanexisting SOTA baselines.\r2022-04-18\nDual-Key Multimodal Backdoors for Visual Question Answering\nMatthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha\nabstract\rabstract: The success of deep learning has enabled advances in multimodal tasks thatrequire non-trivial fusion of multiple input domains. Although multimodalmodels have shown potential in many problems, their increased complexity makesthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class ofsecurity vulnerability wherein an attacker embeds a malicious secret behaviorinto a network (e.g. targeted misclassification) that is activated when anattacker-specified trigger is added to an input. In this work, we show thatmultimodal networks are vulnerable to a novel type of attack that we refer toas Dual-Key Multimodal Backdoors. This attack exploits the complex fusionmechanisms used by state-of-the-art networks to embed backdoors that are botheffective and stealthy. Instead of using a single trigger, the proposed attackembeds a trigger in each of the input modalities and activates the maliciousbehavior only when both the triggers are present. We present an extensive studyof multimodal backdoors on the Visual Question Answering (VQA) task withmultiple architectures and visual feature backbones. A major challenge inembedding backdoors in VQA models is that most models use visual featuresextracted from a fixed pretrained object detector. This is challenging for theattacker as the detector can distort or ignore the visual trigger entirely,which leads to models where backdoors are over-reliant on the language trigger.We tackle this problem by proposing a visual trigger optimization strategydesigned for pretrained object detectors. Through this method, we createDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%of the training data. Finally, we release TrojVQA, a large collection of cleanand trojan VQA models to enable research in defending against multimodalbackdoors.\r2022-03-21\nA Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement\nYuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, Jian Zhang\nabstract\rabstract: Recent years have seen the wide application of NLP models in crucial areassuch as finance, medical treatment, and news media, raising concerns of themodel robustness and vulnerabilities. In this paper, we propose a novelprompt-based adversarial attack to compromise NLP models and robustnessenhancement technique. We first construct malicious prompts for each instanceand generate adversarial examples via mask-and-filling under the effect of amalicious purpose. Our attack technique targets the inherent vulnerabilities ofNLP models, allowing us to generate samples even without interacting with thevictim NLP model, as long as it is based on pre-trained language models (PLMs).Furthermore, we design a prompt-based adversarial training method to improvethe robustness of PLMs. As our training method does not actually generateadversarial samples, it can be applied to large-scale training setsefficiently. The experimental results show that our attack method can achieve ahigh attack success rate with more diverse, fluent and natural adversarialexamples. In addition, our robustness enhancement method can significantlyimprove the robustness of models to resist adversarial attacks. Our workindicates that prompting paradigm has great potential in probing somefundamental flaws of PLMs and fine-tuning them for downstream tasks.\r2022-02-28\nSequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning\nSeanie Lee, Hae Beom Lee, Juho Lee, Sung Ju Hwang\nabstract\rabstract: Multilingual models jointly pretrained on multiple languages have achievedremarkable performance on various multilingual downstream tasks. Moreover,models finetuned on a single monolingual downstream task have shown togeneralize to unseen languages. In this paper, we first show that it is crucialfor those tasks to align gradients between them in order to maximize knowledgetransfer while minimizing negative transfer. Despite its importance, theexisting methods for gradient alignment either have a completely differentpurpose, ignore inter-task alignment, or aim to solve continual learningproblems in rather inefficient ways. As a result of the misaligned gradientsbetween tasks, the model suffers from severe negative transfer in the form ofcatastrophic forgetting of the knowledge acquired from the pretraining. Toovercome the limitations, we propose a simple yet effective method that canefficiently align gradients between tasks. Specifically, we perform eachinner-optimization by sequentially sampling batches from all the tasks,followed by a Reptile outer update. Thanks to the gradients aligned betweentasks by our method, the model becomes less vulnerable to negative transfer andcatastrophic forgetting. We extensively validate our method on variousmulti-task learning and zero-shot cross-lingual transfer tasks, where ourmethod largely outperforms all the relevant baselines we consider.\r2022-01-10\nAdversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li\nabstract\rabstract: Large-scale pre-trained language models have achieved tremendous successacross a wide range of natural language understanding (NLU) tasks, evensurpassing human performance. However, recent studies reveal that therobustness of these models can be challenged by carefully crafted textualadversarial examples. While several individual datasets have been proposed toevaluate model robustness, a principled and comprehensive benchmark is stillmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-taskbenchmark to quantitatively and thoroughly explore and evaluate thevulnerabilities of modern large-scale language models under various types ofadversarial attacks. In particular, we systematically apply 14 textualadversarial attack methods to GLUE tasks to construct AdvGLUE, which is furthervalidated by humans for reliable annotations. Our findings are summarized asfollows. (i) Most existing adversarial attack algorithms are prone togenerating invalid or ambiguous adversarial examples, with around 90% of themeither changing the original semantic meanings or misleading human annotatorsas well. Therefore, we perform a careful filtering process to curate ahigh-quality benchmark. (ii) All the language models and robust trainingmethods we tested perform poorly on AdvGLUE, with scores lagging far behind thebenign accuracy. We hope our work will motivate the development of newadversarial attacks that are more stealthy and semantic-preserving, as well asnew robust language models against sophisticated adversarial attacks. AdvGLUEis available at https://adversarialglue.github.io.\r2022-01-04\nSubmix: Practical Private Prediction for Large-Scale Language Models\nAntonio Ginart, Laurens van der Maaten, James Zou, Chuan Guo\nabstract\rabstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model\u0026rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.\r2021-12-16\nA Deep Learning Approach for Ontology Enrichment from Unstructured Text\nLalit Mohan Sanagavarapu, Vivek Iyer, Raghu Reddy\nabstract\rabstract: Information Security in the cyber world is a major cause for concern, with asignificant increase in the number of attack surfaces. Existing information onvulnerabilities, attacks, controls, and advisories available on the webprovides an opportunity to represent knowledge and perform security analyticsto mitigate some of the concerns. Representing security knowledge in the formof ontology facilitates anomaly detection, threat intelligence, reasoning andrelevance attribution of attacks, and many more. This necessitates dynamic andautomated enrichment of information security ontologies. However, existingontology enrichment algorithms based on natural language processing and MLmodels have issues with contextual extraction of concepts in words, phrases,and sentences. This motivates the need for sequential Deep Learningarchitectures that traverse through dependency paths in text and extractembedded vulnerabilities, threats, controls, products, and othersecurity-related concepts and instances from learned path representations. Inthe proposed approach, Bidirectional LSTMs trained on a large DBpedia datasetand Wikipedia corpus of 2.8 GB along with Universal Sentence Encoder isdeployed to enrich ISO 27001-based information security ontology. The model istrained and tested on a high-performance computing (HPC) environment to handleWiki text dimensionality. The approach yielded a test accuracy of over 80% whentested with knocked-out concepts from ontology and web page instances tovalidate the robustness.\r2021-12-11\nBad Characters: Imperceptible NLP Attacks\nNicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot\nabstract\rabstract: Several years of research have shown that machine-learning systems arevulnerable to adversarial examples, both in theory and in practice. Until now,such attacks have primarily targeted visual models, exploiting the gap betweenhuman and machine perception. Although text-based models have also beenattacked with adversarial examples, such attacks struggled to preserve semanticmeaning and indistinguishability. In this paper, we explore a large class ofadversarial examples that can be used to attack text-based models in ablack-box setting without making any human-perceptible visual modification toinputs. We use encoding-specific perturbations that are imperceptible to thehuman eye to manipulate the outputs of a wide range of Natural LanguageProcessing (NLP) systems from neural machine-translation pipelines to websearch engines. We find that with a single imperceptible encoding injection \u0026ndash;representing one invisible character, homoglyph, reordering, or deletion \u0026ndash; anattacker can significantly reduce the performance of vulnerable models, andwith three injections most models can be functionally broken. Our attacks workagainst currently-deployed commercial systems, including those produced byMicrosoft and Google, in addition to open source models published by Facebook,IBM, and HuggingFace. This novel series of attacks presents a significantthreat to many language processing systems: an attacker can affect systems in atargeted manner without any assumptions about the underlying model. We concludethat text-based NLP systems require careful input sanitization, just likeconventional applications, and that given such systems are now being deployedrapidly at scale, the urgent attention of architects and operators is required.\r2021-12-02\nHow BPE Affects Memorization in Transformers\nEugene Kharitonov, Marco Baroni, Dieuwke Hupkes\nabstract\rabstract: Training data memorization in NLP can both be beneficial (e.g., closed-bookQA) and undesirable (personal data extraction). In any case, successful modeltraining requires a non-trivial amount of memorization to store word spellings,various linguistic idiosyncrasies and common knowledge. However, little isknown about what affects the memorization behavior of NLP models, as the fieldtends to focus on the equally important question of generalization. In thiswork, we demonstrate that the size of the subword vocabulary learned byByte-Pair Encoding (BPE) greatly affects both ability and tendency of standardTransformer models to memorize training data, even when we control for thenumber of learned parameters. We find that with a large subword vocabularysize, Transformer models fit random mappings more easily and are morevulnerable to membership inference attacks. Similarly, given a prompt,Transformer-based language models with large subword vocabularies reproduce thetraining data more often. We conjecture this effect is caused by reduction inthe sequences\u0026rsquo; length that happens as the BPE vocabulary grows. Our findingscan allow a more informed choice of hyper-parameters, that is better tailoredfor a particular use-case.\r2021-11-30\nAdversarial Robustness of Deep Code Comment Generation\nYu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, Harald Gall\nabstract\rabstract: Deep neural networks (DNNs) have shown remarkable performance in a variety ofdomains such as computer vision, speech recognition, or natural languageprocessing. Recently they also have been applied to various softwareengineering tasks, typically involving processing source code. DNNs arewell-known to be vulnerable to adversarial examples, i.e., fabricated inputsthat could lead to various misbehaviors of the DNN model while being perceivedas benign by humans. In this paper, we focus on the code comment generationtask in software engineering and study the robustness issue of the DNNs whenthey are applied to this task. We propose ACCENT, an identifier substitutionapproach to craft adversarial code snippets, which are syntactically correctand semantically close to the original code snippet, but may mislead the DNNsto produce completely irrelevant code comments. In order to improve therobustness, ACCENT also incorporates a novel training method, which can beapplied to existing code comment generation models. We conduct comprehensiveexperiments to evaluate our approach by attacking the mainstreamencoder-decoder architectures on two large-scale publicly available datasets.The results show that ACCENT efficiently produces stable attacks withfunctionality-preserving adversarial examples, and the generated examples havebetter transferability compared with baselines. We also confirm, viaexperiments, the effectiveness in improving model robustness with our trainingmethod.\r2021-11-22\nEfficient Combinatorial Optimization for Word-level Adversarial Textual Attack\nShengcai Liu, Ning Lu, Cheng Chen, Ke Tang\nabstract\rabstract: Over the past few years, various word-level textual attack approaches havebeen proposed to reveal the vulnerability of deep neural networks used innatural language processing. Typically, these approaches involve an importantoptimization step to determine which substitute to be used for each word in theoriginal input. However, current research on this step is still rather limited,from the perspectives of both problem-understanding and problem-solving. Inthis paper, we address these issues by uncovering the theoretical properties ofthe problem and proposing an efficient local search algorithm (LS) to solve it.We establish the first provable approximation guarantee on solving the problemin general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26NLP models show that LS can largely reduce the number of queries usually by anorder of magnitude to achieve high attack success rates. Further experimentsshow that the adversarial examples crafted by LS usually have higher quality,exhibit better transferability, and can bring more robustness improvement tovictim models by adversarial training.\r2021-10-18\nA ground-truth dataset of real security patches\nSofia Reis, Rui Abreu\nabstract\rabstract: Training machine learning approaches for vulnerability identification andproducing reliable tools to assist developers in implementing quality software\u0026ndash; free of vulnerabilities \u0026ndash; is challenging due to the lack of large datasetsand real data. Researchers have been looking at these issues and buildingdatasets. However, these datasets usually miss natural language artifacts andprogramming language diversity. We scraped the entire CVE details database forGitHub references and augmented the data with 3 security-related datasets. Weused the data to create a ground-truth dataset of natural language artifacts(such as commit messages, commits comments, and summaries), meta-data and codechanges. Our dataset integrates a total of 8057 security-relevant commits \u0026ndash;the equivalent to 5942 security patches \u0026ndash; from 1339 different projectsspanning 146 different types of vulnerabilities and 20 languages. A dataset of110k non-security-related commits is also provided. Data and scripts are allavailable on GitHub. Data is stored in a .CSV file. Codebases can be downloadedusing our scripts. Our dataset is a valuable asset to answer research questionson different topics such as the identification of security-relevant informationusing NLP models; software engineering and security best practices; and,vulnerability detection and patching; and, security program analysis.\r2021-09-25\nMINIMAL: Mining Models for Data Free Universal Adversarial Triggers\nSwapnil Parekh, Yaman Singla Kumar, Somesh Singh, Changyou Chen, Balaji Krishnamurthy, Rajiv Ratn Shah\nabstract\rabstract: It is well known that natural language models are vulnerable to adversarialattacks, which are mostly input-specific in nature. Recently, it has been shownthat there also exist input-agnostic attacks in NLP models, called universaladversarial triggers. However, existing methods to craft universal triggers aredata intensive. They require large amounts of data samples to generateadversarial triggers, which are typically inaccessible by attackers. Forinstance, previous works take 3000 data samples per class for the SNLI datasetto generate adversarial triggers. In this paper, we present a novel data-freeapproach, MINIMAL, to mine input-agnostic adversarial triggers from models.Using the triggers produced with our data-free algorithm, we reduce theaccuracy of Stanford Sentiment Treebank\u0026rsquo;s positive class from 93.6% to 9.6%.Similarly, for the Stanford Natural Language Inference (SNLI), our single-wordtrigger reduces the accuracy of the entailment class from 90.95% to less than0.6%. Despite being completely data-free, we get equivalent accuracy drops asdata-dependent methods.\r2021-09-14\nLeveraging pre-trained representations to improve access to untranscribed speech from endangered languages\nNay San, Martijn Bartelds, Mitchell Browne, Lily Clifford, Fiona Gibson, John Mansfield, David Nash, Jane Simpson, Myfany Turpin, Maria Vollmer, Sasha Wilmoth, Dan Jurafsky\nabstract\rabstract: Pre-trained speech representations like wav2vec 2.0 are a powerful tool forautomatic speech recognition (ASR). Yet many endangered languages lacksufficient data for pre-training such models, or are predominantly oralvernaculars without a standardised writing system, precluding fine-tuning.Query-by-example spoken term detection (QbE-STD) offers an alternative foriteratively indexing untranscribed speech corpora by locating spoken queryterms. Using data from 7 Australian Aboriginal languages and a regional varietyof Dutch, all of which are endangered or vulnerable, we show that QbE-STD canbe improved by leveraging representations developed for ASR (wav2vec 2.0: theEnglish monolingual model and XLSR53 multilingual model). Surprisingly, theEnglish model outperformed the multilingual model on 4 Australian languagedatasets, raising questions around how to optimally leverage self-supervisedspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0representations (either English or XLSR53) offer large improvements (56-86%relative) over state-of-the-art approaches on our endangered language datasets.\r2021-09-02\nActive Learning Under Malicious Mislabeling and Poisoning Attacks\nJing Lin, Ryan Luley, Kaiqi Xiong\nabstract\rabstract: Deep neural networks usually require large labeled datasets for training toachieve state-of-the-art performance in many tasks, such as imageclassification and natural language processing. Although a lot of data iscreated each day by active Internet users, most of these data are unlabeled andare vulnerable to data poisoning attacks. In this paper, we develop anefficient active learning method that requires fewer labeled instances andincorporates the technique of adversarial retraining in which additionallabeled artificial data are generated without increasing the budget of thelabeling. The generated adversarial examples also provide a way to measure thevulnerability of the model. To check the performance of the proposed methodunder an adversarial setting, i.e., malicious mislabeling and data poisoningattacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset,which contains only two classes: airplane and frog. Our experimental resultsdemonstrate that the proposed active learning method is efficient for defendingagainst malicious mislabeling and data poisoning attacks. Specifically, whereasthe baseline active learning method based on the random sampling strategyperforms poorly (about 50%) under a malicious mislabeling attack, the proposedactive learning method can achieve the desired accuracy of 89% using onlyone-third of the dataset on average.\r2021-08-22\nDisentangled Contrastive Learning for Learning Robust Textual Representations\nXiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang, Huajun Chen\nabstract\rabstract: Although the self-supervised pre-training of transformer models has resultedin the revolutionizing of natural language processing (NLP) applications andthe achievement of state-of-the-art results with regard to various benchmarks,this process is still vulnerable to small and imperceptible permutationsoriginating from legitimate inputs. Intuitively, the representations should besimilar in the feature space with subtle input permutations, while largevariations occur with different meanings. This motivates us to investigate thelearning of robust textual representation in a contrastive manner. However, itis non-trivial to obtain opposing semantic instances for textual samples. Inthis study, we propose a disentangled contrastive learning method thatseparately optimizes the uniformity and alignment of representations withoutnegative sampling. Specifically, we introduce the concept of momentumrepresentation consistency to align features and leverage power normalizationwhile conforming the uniformity. Our experimental results for the NLPbenchmarks demonstrate that our approach can obtain better results comparedwith the baselines, as well as achieve promising improvements with invariancetests and adversarial attacks. The code is available inhttps://github.com/zxlzr/DCL.\r2021-08-14\nFew-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models\nGuanqun Yang, Shay Dineen, Zhipeng Lin, Xueqing Liu\nabstract\rabstract: Public security vulnerability reports (e.g., CVE reports) play an importantrole in the maintenance of computer and network systems. Security companies andadministrators rely on information from these reports to prioritize tasks ondeveloping and deploying patches to their customers. Since these reports areunstructured texts, automatic information extraction (IE) can help scale up theprocessing by converting the unstructured reports to structured forms, e.g.,software names and versions and vulnerability types. Existing works onautomated IE for security vulnerability reports often rely on a large number oflabeled training samples. However, creating massive labeled training set isboth expensive and time consuming. In this work, for the first time, we proposeto investigate this problem where only a small number of labeled trainingsamples are available. In particular, we investigate the performance offine-tuning several state-of-the-art pre-trained language models on our smalltraining dataset. The results show that with pre-trained language models andcarefully tuned hyperparameters, we have reached or slightly outperformed thestate-of-the-art system on this task. Consistent with previous two-step processof first fine-tuning on main category and then transfer learning to others asin [7], if otherwise following our proposed approach, the number of requiredlabeled samples substantially decrease in both stages: 90% reduction infine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64labeled samples per category. Our experiments thus demonstrate theeffectiveness of few-sample learning on NER for security vulnerability report.This result opens up multiple research opportunities for few-sample learningfor security vulnerability reports, which is discussed in the paper. Code:https://github.com/guanqun-yang/FewVulnerability.\r2021-08-13\nAsteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection\nShouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, Zhiqiang Shi\nabstract\rabstract: Binary code similarity detection is a fundamental technique for many securityapplications such as vulnerability search, patch analysis, and malwaredetection. There is an increasing need to detect similar code for vulnerabilitysearch across architectures with the increase of critical vulnerabilities inIoT devices. The variety of IoT hardware architectures and software platformsrequires to capture semantic equivalence of code fragments in the similaritydetection. However, existing approaches are insufficient in capturing thesemantic similarity. We notice that the abstract syntax tree (AST) of afunction contains rich semantic information. Inspired by successfulapplications of natural language processing technologies in sentence semanticunderstanding, we propose a deep learning-based AST-encoding method, namedASTERIA, to measure the semantic equivalence of functions in differentplatforms. Our method leverages the Tree-LSTM network to learn the semanticrepresentation of a function from its AST. Then the similarity detection can beconducted efficiently and accurately by measuring the similarity between tworepresentation vectors. We have implemented an open-source prototype ofASTERIA. The Tree-LSTM model is trained on a dataset with 1,022,616 functionpairs and evaluated on a dataset with 95,078 function pairs. Evaluation resultsshow that our method outperforms the AST-based tool Diaphora andthe-state-of-art method Gemini by large margins with respect to the binarysimilarity detection. And our method is several orders of magnitude faster thanDiaphora and Gemini for the similarity calculation. In the application ofvulnerability search, our tool successfully identified 75 vulnerable functionsin 5,979 IoT firmware images.\r2021-08-05\nRobust Transfer Learning with Pretrained Language Models through Adapters\nWenjuan Han, Bo Pang, Yingnian Wu\nabstract\rabstract: Transfer learning with large pretrained transformer-based language modelslike BERT has become a dominating approach for most NLP tasks. Simplyfine-tuning those large language models on downstream tasks or combining itwith task-specific pretraining is often not robust. In particular, theperformance considerably varies as the random seed changes or the number ofpretraining and/or fine-tuning iterations varies, and the fine-tuned model isvulnerable to adversarial attack. We propose a simple yet effectiveadapter-based approach to mitigate these issues. Specifically, we insert smallbottleneck layers (i.e., adapter) within each layer of a pretrained model, thenfix the pretrained layers and train the adapter layers on the downstream taskdata, with (1) task-specific unsupervised pretraining and then (2)task-specific supervised training (e.g., classification, sequence labeling).Our experiments demonstrate that such a training scheme leads to improvedstability and adversarial robustness in transfer learning to various downstreamtasks.\r2021-08-04\nA Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python\nAmirreza Bagheri, Péter Hegedűs\nabstract\rabstract: In the age of big data and machine learning, at a time when the techniquesand methods of software development are evolving rapidly, a problem has arisen:programmers can no longer detect all the security flaws and vulnerabilities intheir code manually. To overcome this problem, developers can now rely onautomatic techniques, like machine learning based prediction models, to detectsuch issues. An inherent property of such approaches is that they work withnumeric vectors (i.e., feature vectors) as inputs. Therefore, one needs totransform the source code into such feature vectors, often referred to as codeembedding. A popular approach for code embedding is to adapt natural languageprocessing techniques, like text representation, to automatically derive thenecessary features from the source code. However, the suitability andcomparison of different text representation techniques for solving SoftwareEngineering (SE) problems is rarely studied systematically. In this paper, wepresent a comparative study on three popular text representation methods,word2vec, fastText, and BERT applied to the SE task of detectingvulnerabilities in Python code. Using a data mining approach, we collected alarge volume of Python source code in both vulnerable and fixed forms that weembedded with word2vec, fastText, and BERT to vectors and used a LongShort-Term Memory network to train on them. Using the same LSTM architecture,we could compare the efficiency of the different embeddings in derivingmeaningful feature vectors. Our findings show that all the text representationmethods are suitable for code representation in this particular task, but theBERT model is the most promising as it is the least time consuming and the LSTMmodel based on it achieved the best overall accuracy(93.8%) in predictingPython source code vulnerabilities.\r2021-06-30\nUnderstanding Adversarial Examples Through Deep Neural Network\u0026rsquo;s Response Surface and Uncertainty Regions\nJuan Shu, Bowei Xi, Charles Kamhoua\nabstract\rabstract: Deep neural network (DNN) is a popular model implemented in many systems tohandle complex tasks such as image classification, object recognition, naturallanguage processing etc. Consequently DNN structural vulnerabilities becomepart of the security vulnerabilities in those systems. In this paper we studythe root cause of DNN adversarial examples. We examine the DNN response surfaceto understand its classification boundary. Our study reveals the structuralproblem of DNN classification boundary that leads to the adversarial examples.Existing attack algorithms can generate from a handful to a few hundredadversarial examples given one clean image. We show there are infinitely manyadversarial images given one clean sample, all within a small neighborhood ofthe clean sample. We then define DNN uncertainty regions and showtransferability of adversarial examples is not universal. We also argue thatgeneralization error, the large sample theoretical guarantee established forDNN, cannot adequately capture the phenomenon of adversarial examples. We neednew theory to measure DNN robustness.\r2021-06-15\nExtracting Training Data from Large Language Models\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\nabstract\rabstract: It has become common to publish large (billion parameter) language modelsthat have been trained on private datasets. This paper demonstrates that insuch settings, an adversary can perform a training data extraction attack torecover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes ofthe public Internet, and are able to extract hundreds of verbatim textsequences from the model\u0026rsquo;s training data. These extracted examples include(public) personally identifiable information (names, phone numbers, and emailaddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possibleeven though each of the above sequences are included in just one document inthe training data. We comprehensively evaluate our extraction attack to understand the factorsthat contribute to its success. Worryingly, we find that larger models are morevulnerable than smaller models. We conclude by drawing lessons and discussingpossible safeguards for training large language models.\rNatural Language Adversarial Defense through Synonym Encoding\nXiaosen Wang, Hao Jin, Yichen Yang, Kun He\nabstract\rabstract: In the area of natural language processing, deep learning models are recentlyknown to be vulnerable to various types of adversarial perturbations, butrelatively few works are done on the defense side. Especially, there exists feweffective defense method against the successful synonym substitution basedattacks that preserve the syntactic structure and semantic information of theoriginal text while fooling the deep learning models. We contribute in thisdirection and propose a novel adversarial defense method called SynonymEncoding Method (SEM). Specifically, SEM inserts an encoder before the inputlayer of the target model to map each cluster of synonyms to a unique encodingand trains the model to eliminate possible adversarial perturbations withoutmodifying the network architecture or adding extra data. Extensive experimentsdemonstrate that SEM can effectively defend the current synonym substitutionbased attacks and block the transferability of adversarial examples. SEM isalso easy and efficient to scale to large models and big datasets.\r2021-05-29\nConstructing Flow Graphs from Procedural Cybersecurity Texts\nKuntal Kumar Pal, Kazuaki Kashihara, Pratyay Banerjee, Swaroop Mishra, Ruoyu Wang, Chitta Baral\nabstract\rabstract: Following procedural texts written in natural languages is challenging. Wemust read the whole text to identify the relevant information or identify theinstruction flows to complete a task, which is prone to failures. If such textsare structured, we can readily visualize instruction-flows, reason or infer aparticular step, or even build automated systems to help novice agents achievea goal. However, this structure recovery task is a challenge because of suchtexts\u0026rsquo; diverse nature. This paper proposes to identify relevant informationfrom such texts and generate information flows between sentences. We built alarge annotated procedural text dataset (CTFW) in the cybersecurity domain(3154 documents). This dataset contains valuable instructions regardingsoftware vulnerability analysis experiences. We performed extensive experimentson CTFW with our LM-GNN model variants in multiple settings. To show thegeneralizability of both this task and our method, we also experimented withprocedural texts from two other domains (Maintenance Manual and Cooking), whichare substantially different from cybersecurity. Our experiments show that GraphConvolution Network with BERT sentence embeddings outperforms BERT in all threedomains\r2021-03-22\nInfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective\nBoxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu\nabstract\rabstract: Large-scale language models such as BERT have achieved state-of-the-artperformance across a wide range of NLP tasks. Recent studies, however, showthat such BERT-based models are vulnerable facing the threats of textualadversarial attacks. We aim to address this problem from aninformation-theoretic perspective, and propose InfoBERT, a novel learningframework for robust fine-tuning of pre-trained language models. InfoBERTcontains two mutual-information-based regularizers for model training: (i) anInformation Bottleneck regularizer, which suppresses noisy mutual informationbetween the input and the feature representation; and (ii) a Robust Featureregularizer, which increases the mutual information between local robustfeatures and global features. We provide a principled way to theoreticallyanalyze and improve the robustness of representation learning for languagemodels in both standard and adversarial training. Extensive experimentsdemonstrate that InfoBERT achieves state-of-the-art robust accuracy overseveral adversarial datasets on Natural Language Inference (NLI) and QuestionAnswering (QA) tasks. Our code is available athttps://github.com/AI-secure/InfoBERT.\r2021-03-21\nAutomated Software Vulnerability Assessment with Concept Drift\nTriet H. M. Le, Bushra Sabir, M. Ali Babar\nabstract\rabstract: Software Engineering researchers are increasingly using Natural LanguageProcessing (NLP) techniques to automate Software Vulnerabilities (SVs)assessment using the descriptions in public repositories. However, the existingNLP-based approaches suffer from concept drift. This problem is caused by alack of proper treatment of new (out-of-vocabulary) terms for the evaluation ofunseen SVs over time. To perform automated SVs assessment with concept driftusing SVs\u0026rsquo; descriptions, we propose a systematic approach that combines bothcharacter and word features. The proposed approach is used to predict sevenVulnerability Characteristics (VCs). The optimal model of each VC is selectedusing our customized time-based cross-validation method from a list of eightNLP representations and six well-known Machine Learning models. We have usedthe proposed approach to conduct large-scale experiments on more than 100,000SVs in the National Vulnerability Database (NVD). The results show that ourapproach can effectively tackle the concept drift issue of the SVs\u0026rsquo;descriptions reported from 2000 to 2018 in NVD even without retraining themodel. In addition, our approach performs competitively compared to theexisting word-only method. We also investigate how to build compactconcept-drift-aware models with much fewer features and give somerecommendations on the choice of classifiers and NLP representations for SVsassessment.\r2021-02-16\nD2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis\nYunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti, Edward Epstein, Bo Yang, Jim Laredo, Alessandro Morari, Zhong Su\nabstract\rabstract: Static analysis tools are widely used for vulnerability detection as theyunderstand programs with complex behavior and millions of lines of code.Despite their popularity, static analysis tools are known to generate an excessof false positives. The recent ability of Machine Learning models to understandprogramming languages opens new possibilities when applied to static analysis.However, existing datasets to train models for vulnerability identificationsuffer from multiple limitations such as limited bug context, limited size, andsynthetic and unrealistic source code. We propose D2A, a differential analysisbased approach to label issues reported by static analysis tools. The D2Adataset is built by analyzing version pairs from multiple open source projects.From each project, we select bug fixing commits and we run static analysis onthe versions before and after such commits. If some issues detected in abefore-commit version disappear in the corresponding after-commit version, theyare very likely to be real bugs that got fixed by the commit. We use D2A togenerate a large labeled dataset to train models for vulnerabilityidentification. We show that the dataset can be used to build a classifier toidentify possible false alarms among the issues reported by static analysis,hence helping developers prioritize and investigate potential true positivesfirst.\r2020-11-07\nPrivacy in Deep Learning: A Survey\nFatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh\nabstract\rabstract: The ever-growing advances of deep learning in many areas including vision,recommendation systems, natural language processing, etc., have led to theadoption of Deep Neural Networks (DNNs) in production systems. The availabilityof large datasets and high computational power are the main contributors tothese advances. The datasets are usually crowdsourced and may contain sensitiveinformation. This poses serious privacy concerns as this data can be misused orleaked through various vulnerabilities. Even if the cloud provider and thecommunication link is trusted, there are still threats of inference attackswhere an attacker could speculate properties of the data used for training, orfind the underlying model architecture and parameters. In this survey, wereview the privacy concerns brought by deep learning, and the mitigatingtechniques introduced to tackle these issues. We also show that there is a gapin the literature regarding test-time inference privacy, and propose possiblefuture research directions.\r2020-10-08\nYou Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion\nRoei Schuster, Congzheng Song, Eran Tromer, Vitaly Shmatikov\nabstract\rabstract: Code autocompletion is an integral feature of modern code editors and IDEs.The latest generation of autocompleters uses neural language models, trained onpublic open-source code repositories, to suggest likely (not just staticallyfeasible) completions given the current context. We demonstrate that neural code autocompleters are vulnerable to poisoningattacks. By adding a few specially-crafted files to the autocompleter\u0026rsquo;straining corpus (data poisoning), or else by directly fine-tuning theautocompleter on these files (model poisoning), the attacker can influence itssuggestions for attacker-chosen contexts. For example, the attacker can \u0026ldquo;teach\u0026quot;the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3for the SSL/TLS protocol version, or a low iteration count for password-basedencryption. Moreover, we show that these attacks can be targeted: anautocompleter poisoned by a targeted attack is much more likely to suggest theinsecure completion for files from a specific repo or specific developer. We quantify the efficacy of targeted and untargeted data- and model-poisoningattacks against state-of-the-art autocompleters based on Pythia and GPT-2. Wethen evaluate existing defenses against poisoning attacks and show that theyare largely ineffective.\r2020-07-22\nOptimal policies for mitigating pandemic costs\nM. Serra, S. al-Mosleh, S. Ganga Prasath, V. Raju, S. Mantena, J. Chandra, S. Iams, L. Mahadevan\nabstract\rabstract: Several non-pharmaceutical interventions have been proposed to control thespread of the COVID-19 pandemic. On the large scale, these empirical solutions,often associated with extended and complete lockdowns, attempt to minimize thecosts associated with mortality, economic losses and social factors, whilebeing subject to constraints such as finite hospital capacity. Here we pose thequestion of how to mitigate pandemic costs subject to constraints by adoptingthe language of optimal control theory. This allows us to determine top-downpolicies for the nature and dynamics of social contact rates given anage-structured model for the dynamics of the disease. Depending on the relativeweights allocated to life and socioeconomic losses, we see that the optimalstrategies range from long-term social-distancing only for the most vulnerable,to partial lockdown to ensure not over-running hospitals, to alternating-shiftswith significant reduction in life and/or socioeconomic losses. Crucially,commonly used strategies that involve long periods of broad lockdown are almostnever optimal, as they are highly unstable to reopening and entail highsocioeconomic costs. Using parameter estimates from data available for Germanyand the USA, we quantify these policies and use sensitivity analysis in therelevant model parameters and initial conditions to determine the range ofrobustness of our policies. Finally we also discuss how bottom-up behavioralchanges can also change the dynamics of the pandemic and show how this intandem with top-down control policies can mitigate pandemic costs even moreeffectively.\r2020-06-20\nDefense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble\nYi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang\nabstract\rabstract: Despite neural networks have achieved prominent performance on many naturallanguage processing (NLP) tasks, they are vulnerable to adversarial examples.In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomizedsmoothing method for training a robust model to defense substitution-basedattacks. During training, DNE forms virtual sentences by sampling embeddingvectors for each word in an input sentence from a convex hull spanned by theword and its synonyms, and it augments them with the training data. In such away, the model is robust to adversarial attacks while maintaining theperformance on the original clean data. DNE is agnostic to the networkarchitectures and scales to large models for NLP applications. We demonstratethrough extensive experimentation that our method consistently outperformsrecently proposed defense methods by a significant margin across differentnetwork architectures and multiple data sets.\r2020-04-29\nAdversarial Training for Large Neural Language Models\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao\nabstract\rabstract: Generalization and robustness are both key desiderata for designing machinelearning methods. Adversarial training can enhance robustness, but past workoften finds it hurts generalization. In natural language processing (NLP),pre-training large neural language models such as BERT have demonstratedimpressive gain in generalization for a variety of tasks, with furtherimprovement from adversarial fine-tuning. However, these models are stillvulnerable to adversarial attacks. In this paper, we show that adversarialpre-training can improve both generalization and robustness. We propose ageneral algorithm ALUM (Adversarial training for large neural LangUage Models),which regularizes the training objective by applying perturbations in theembedding space that maximizes the adversarial loss. We present the firstcomprehensive study of adversarial training in all stages, includingpre-training from scratch, continual pre-training on a well-trained model, andtask-specific fine-tuning. ALUM obtains substantial gains over BERT on a widerange of NLP tasks, in both regular and adversarial scenarios. Even for modelsthat have been well trained on extremely large text corpora, such as RoBERTa,ALUM can still produce significant gains from continual pre-training, whereasconventional non-adversarial methods can not. ALUM can be further combined withtask-specific fine-tuning to attain additional gains. The ALUM code is publiclyavailable at https://github.com/namisan/mt-dnn.\r2020-04-07\nTowards Evaluating the Robustness of Chinese BERT Classifiers\nBoxin Wang, Boyuan Pan, Xin Li, Bo Li\nabstract\rabstract: Recent advances in large-scale language representation models such as BERThave improved the state-of-the-art performances in many NLP tasks. Meanwhile,character-level Chinese NLP models, including BERT for Chinese, have alsodemonstrated that they can outperform the existing models. In this paper, weshow that, however, such BERT-based models are vulnerable under character-leveladversarial attacks. We propose a novel Chinese char-level attack methodagainst BERT-based classifiers. Essentially, we generate \u0026ldquo;small\u0026rdquo; perturbationon the character level in the embedding space and guide the charactersubstitution procedure. Extensive experiments show that the classificationaccuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating lessthan 2 characters on average based on the proposed attack. Human evaluationsalso confirm that our generated Chinese adversarial examples barely affecthuman performance on these NLP tasks.\r2020-03-12\nÆGIS: Shielding Vulnerable Smart Contracts Against Attacks\nChristof Ferreira Torres, Mathis Baden, Robert Norvill, Beltran Borja Fiz Pontiveros, Hugo Jonker, Sjouke Mauw\nabstract\rabstract: In recent years, smart contracts have suffered major exploits, costingmillions of dollars. Unlike traditional programs, smart contracts are deployedon a blockchain. As such, they cannot be modified once deployed. Though varioustools have been proposed to detect vulnerable smart contracts, the majorityfails to protect vulnerable contracts that have already been deployed on theblockchain. Only very few solutions have been proposed so far to tackle theissue of post-deployment. However, these solutions suffer from low precisionand are not generic enough to prevent any type of attack. In this work, we introduce {\\AE}GIS, a dynamic analysis tool that protectssmart contracts from being exploited during runtime. Its capability ofdetecting new vulnerabilities can easily be extended through so-called attackpatterns. These patterns are written in a domain-specific language that istailored to the execution model of Ethereum smart contracts. The languageenables the description of malicious control and data flows. In addition, wepropose a novel mechanism to streamline and speed up the process of managingattack patterns. Patterns are voted upon and stored via a smart contract, thusleveraging the benefits of tamper-resistance and transparency provided by theblockchain. We compare {\\AE}GIS to current state-of-the-art tools anddemonstrate that our solution achieves higher precision in detecting attacks.Finally, we perform a large-scale analysis on the first 4.5 million blocks ofthe Ethereum blockchain, thereby confirming the occurrences of well reportedand yet unreported attacks in the wild.\r2019-11-18\nBuilding Fast Fuzzers\nRahul Gopinath, Andreas Zeller\nabstract\rabstract: Fuzzing is one of the key techniques for evaluating the robustness ofprograms against attacks. Fuzzing has to be effective in producing inputs thatcover functionality and find vulnerabilities. But it also has to be efficientin producing such inputs quickly. Random fuzzers are very efficient, as theycan quickly generate random inputs; but they are not very effective, as thelarge majority of inputs generated is syntactically invalid. Grammar-basedfuzzers make use of a grammar (or another model for the input language) toproduce syntactically correct inputs, and thus can quickly cover input spaceand associated functionality. Existing grammar-based fuzzers are surprisinglyinefficient, though: Even the fastest grammar fuzzer Dharma still producesinputs about a thousand times slower than the fastest random fuzzer. So far,one can have an effective or an efficient fuzzer, but not both. In this paper, we describe how to build fast grammar fuzzers from the groundup, treating the problem of fuzzing from a programming language implementationperspective. Starting with a Python textbook approach, we adopt and adaptoptimization techniques from functional programming and virtual machineimplementation techniques together with other novel domain-specificoptimizations in a step-by-step fashion. In our F1 prototype fuzzer, theseimprove production speed by a factor of 100\u0026ndash;300 over the fastest grammarfuzzer Dharma. As F1 is even 5\u0026ndash;8 times faster than a lexical random fuzzer, wecan find bugs faster and test with much larger valid inputs than previouslypossible.\r2019-10-12\nStatically Detecting Vulnerabilities by Processing Programming Languages as Natural Languages\nIbéria Medeiros, Nuno Neves, Miguel Correia\nabstract\rabstract: Web applications continue to be a favorite target for hackers due to acombination of wide adoption and rapid deployment cycles, which often lead tothe introduction of high impact vulnerabilities. Static analysis tools areimportant to search for bugs automatically in the program source code,supporting developers on their removal. However, building these tools requiresprogramming the knowledge on how to discover the vulnerabilities. This paperpresents an alternative approach in which tools learn to detect flawsautomatically by resorting to artificial intelligence concepts, more concretelyto natural language processing. The approach employs a sequence model to learnto characterize vulnerabilities based on an annotated corpus. Afterwards, themodel is utilized to discover and identify vulnerabilities in the source code.It was implemented in the DEKANT tool and evaluated experimentally with a largeset of PHP applications and WordPress plugins. Overall, we found severalhundred vulnerabilities belonging to 12 classes of input validationvulnerabilities, where 62 of them were zero-day.\r2018-06-25\nSAQL: A Stream-based Query System for Real-Time Abnormal System Behavior Detection\nPeng Gao, Xusheng Xiao, Ding Li, Zhichun Li, Kangkook Jee, Zhenyu Wu, Chung Hwan Kim, Sanjeev R. Kulkarni, Prateek Mittal\nabstract\rabstract: Recently, advanced cyber attacks, which consist of a sequence of steps thatinvolve many vulnerabilities and hosts, compromise the security of manywell-protected businesses. This has led to the solutions that ubiquitouslymonitor system activities in each host (big data) as a series of events, andsearch for anomalies (abnormal behaviors) for triaging risky events. Sincefighting against these attacks is a time-critical mission to prevent furtherdamage, these solutions face challenges in incorporating expert knowledge toperform timely anomaly detection over the large-scale provenance data. To address these challenges, we propose a novel stream-based query systemthat takes as input, a real-time event feed aggregated from multiple hosts inan enterprise, and provides an anomaly query engine that queries the event feedto identify abnormal behaviors based on the specified anomalies. To facilitatethe task of expressing anomalies based on expert knowledge, our system providesa domain-specific query language, SAQL, which allows analysts to express modelsfor (1) rule-based anomalies, (2) time-series anomalies, (3) invariant-basedanomalies, and (4) outlier-based anomalies. We deployed our system in NEC LabsAmerica comprising 150 hosts and evaluated it using 1.1TB of real systemmonitoring data (containing 3.3 billion events). Our evaluations on a broad setof attack behaviors and micro-benchmarks show that our system has a lowdetection latency (\u0026lt;2s) and a high system throughput (110,000 events/s;supporting ~4000 hosts), and is more efficient in memory utilization than theexisting stream-based complex event processing systems.\r"}]