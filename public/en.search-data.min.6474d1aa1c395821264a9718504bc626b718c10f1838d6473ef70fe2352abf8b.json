[{"id":0,"href":"/docs/arxiv_paper/","title":"Arxiv Paper","section":"Docs","content":"\rArxiv Papers: LLM, Privacy, and Copyright\r#\r2024-03-07\nMembership Inference Attacks and Privacy in Topic Modeling\nNico Manzonelli Wanrong Zhang Salil Vadhan\nabstract\rabstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.\rFederated Recommendation via Hybrid Retrieval Augmented Generation\nHuimin Zeng Zhenrui Yue Qian Jiang Dong Wang\nabstract\rabstract: Federated Recommendation (FR) emerges as a novel paradigm that enablesprivacy-preserving recommendations. However, traditional FR systems usuallyrepresent users/items with discrete identities (IDs), suffering fromperformance degradation due to the data sparsity and heterogeneity in FR. Onthe other hand, Large Language Models (LLMs) as recommenders have proveneffective across various recommendation scenarios. Yet, LLM-based recommendersencounter challenges such as low inference efficiency and potentialhallucination, compromising their performance in real-world scenarios. To thisend, we propose GPT-FedRec, a federated recommendation framework leveragingChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.GPT-FedRec is a two-stage solution. The first stage is a hybrid retrievalprocess, mining ID-based user patterns and text-based item features. Next, theretrieved results are converted into text prompts and fed into GPT forre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aimsto extract generalized features from data and exploit pretrained knowledgewithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, theRAG approach also prevents LLM hallucination, improving the recommendationperformance for real-world users. Experimental results on diverse benchmarkdatasets demonstrate the superior performance of GPT-FedRec againststate-of-the-art baseline methods.\rPrivacy-preserving Fine-tuning of Large Language Models through Flatness\nTiejin Chen Longchao Da Huixue Zhou Pingzhi Li Kaixiong Zhou Tianlong Chen Hua Wei\nabstract\rabstract: The privacy concerns associated with the use of Large Language Models (LLMs)have grown recently with the development of LLMs such as ChatGPT. DifferentialPrivacy (DP) techniques are explored in existing work to mitigate their privacyrisks at the cost of generalization degradation. Our paper reveals that theflatness of DP-trained models\u0026rsquo; loss landscape plays an essential role in thetrade-off between their privacy and generalization. We further propose aholistic framework to enforce appropriate weight flatness, which substantiallyimproves model generalization with competitive privacy preservation. Itinnovates from three coarse-to-grained levels, including perturbation-awaremin-max optimization on model weights within a layer, flatness-guided sparseprefix-tuning on weights across layers, and weight knowledge distillationbetween DP \u0026amp; non-DP weights copies. Comprehensive experiments of bothblack-box and white-box scenarios are conducted to demonstrate theeffectiveness of our proposal in enhancing generalization and maintaining DPcharacteristics. For instance, on text classification dataset QNLI, DP-Flatachieves similar performance with non-private full fine-tuning but with DPguarantee under privacy budget $\\epsilon=3$, and even better performance givenhigher privacy budgets. Codes are provided in the supplement.\r2024-03-06\nEnhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification\nRicardo Bigolin Lanfredi Pritam Mukherjee Ronald Summers\nabstract\rabstract: In chest X-ray (CXR) image analysis, rule-based systems are usually employedto extract labels from reports, but concerns exist about label quality. Thesedatasets typically offer only presence labels, sometimes with binaryuncertainty indicators, which limits their usefulness. In this work, we presentMAPLEZ (Medical report Annotations with Privacy-preserving Large language modelusing Expeditious Zero shot answers), a novel approach leveraging a locallyexecutable Large Language Model (LLM) to extract and enhance findings labels onCXR reports. MAPLEZ extracts not only binary labels indicating the presence orabsence of a finding but also the location, severity, and radiologists\u0026rsquo;uncertainty about the finding. Over eight abnormalities from five test sets, weshow that our method can extract these annotations with an increase of 5percentage points (pp) in F1 score for categorical presence annotations andmore than 30 pp increase in F1 score for the location annotations overcompeting labelers. Additionally, using these improved annotations inclassification supervision, we demonstrate substantial advancements in modelquality, with an increase of 1.7 pp in AUROC over models trained withannotations from the state-of-the-art approach. We share code and annotations.\rTaypsi: Static Enforcement of Privacy Policies for Policy-Agnostic Oblivious Computation\nQianchuan Ye Benjamin Delaware\nabstract\rabstract: Secure multiparty computation (MPC) techniques enable multiple parties tocompute joint functions over their private data without sharing that data withother parties, typically by employing powerful cryptographic protocols toprotect individual\u0026rsquo;s data. One challenge when writing such functions is thatmost MPC languages force users to intermix programmatic and privacy concerns ina single application, making it difficult to change or audit a program\u0026rsquo;sunderlying privacy policy. Prior policy-agnostic MPC languages relied ondynamic enforcement to decouple privacy requirements from program logic.Unfortunately, the resulting overhead makes it difficult to scale MPCapplications that manipulate structured data. This work proposes to eliminatethis overhead by instead transforming programs into semantically equivalentversions that statically enforce user-provided privacy policies. We haveimplemented this approach in a new MPC language, called Taypsi; ourexperimental evaluation demonstrates that the resulting system featuresconsiderable performance improvements on a variety of MPC applicationsinvolving structured data and complex privacy policies.\rTowards Efficient and Effective Unlearning of Large Language Models for Recommendation\nHangyu Wang Jianghao Lin Bo Chen Yang Yang Ruiming Tang Weinan Zhang Yong Yu\nabstract\rabstract: The significant advancements in large language models (LLMs) give rise to apromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).The efficacy of LLMRec arises from the open-world knowledge and reasoningcapabilities inherent in LLMs. LLMRec acquires the recommendation capabilitiesthrough instruction tuning based on user interaction data. However, in order toprotect user privacy and optimize utility, it is also crucial for LLMRec tointentionally forget specific user data, which is generally referred to asrecommendation unlearning. In the era of LLMs, recommendation unlearning posesnew challenges for LLMRec in terms of \\textit{inefficiency} and\\textit{ineffectiveness}. Existing unlearning methods require updating billionsof parameters in LLMRec, which is costly and time-consuming. Besides, theyalways impact the model utility during the unlearning process. To this end, wepropose \\textbf{E2URec}, the first \\underline{E}fficient and\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Ourproposed E2URec enhances the unlearning efficiency by updating only a fewadditional LoRA parameters, and improves the unlearning effectiveness byemploying a teacher-student framework, where we maintain multiple teachernetworks to guide the unlearning process. Extensive experiments show thatE2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affectingrecommendation performance. The source code is at\\url{https://github.com/justarter/E2URec}.\rExplaining Genetic Programming Trees using Large Language Models\nPaula Maddigan Andrew Lensen Bing Xue\nabstract\rabstract: Genetic programming (GP) has the potential to generate explainable results,especially when used for dimensionality reduction. In this research, weinvestigate the potential of leveraging eXplainable AI (XAI) and large languagemodels (LLMs) like ChatGPT to improve the interpretability of GP-basednon-linear dimensionality reduction. Our study introduces a novel XAI dashboardnamed GP4NLDR, the first approach to combine state-of-the-art GP with anLLM-powered chatbot to provide comprehensive, user-centred explanations. Weshowcase the system\u0026rsquo;s ability to provide intuitive and insightful narratives onhigh-dimensional data reduction processes through case studies. Our studyhighlights the importance of prompt engineering in eliciting accurate andpertinent responses from LLMs. We also address important considerations arounddata privacy, hallucinatory outputs, and the rapid advancements in generativeAI. Our findings demonstrate its potential in advancing the explainability ofGP algorithms. This opens the door for future research into explaining GPmodels with LLMs.\r2024-03-05\nCoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following\nKaiyan Zhang Jianyu Wang Ermo Hua Biqing Qi Ning Ding Bowen Zhou\nabstract\rabstract: With the advancement of language models (LMs), their exposure to private datais increasingly inevitable, and their deployment (especially for smaller ones)on personal devices, such as PCs and smartphones, has become a prevailingtrend. In contexts laden with user information, enabling models to bothsafeguard user privacy and execute commands efficiently emerges as an essentialresearch imperative. In this paper, we propose CoGenesis, a collaborativegeneration framework integrating large (hosted on cloud infrastructure) andsmall models (deployed on local devices) to address privacy concerns logically.Initially, we design a pipeline to create personalized writing instructiondatasets enriched with extensive context details as the testbed of thisresearch issue. Subsequently, we introduce two variants of CoGenesis based onsketch and logits respectively. Our experimental findings, based on oursynthesized dataset and two additional open-source datasets, indicate that: 1)Large-scale models perform well when provided with user context but struggle inthe absence of such context. 2) While specialized smaller models fine-tuned onthe synthetic dataset show promise, they still lag behind their largercounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,showcases competitive performance, providing a feasible solution to privacyissues.\rPrivacy-Aware Semantic Cache for Large Language Models\nWaris Gill Mohamed Elidrisi Pallavi Kalapatapu Ali Anwar Muhammad Ali Gulzar\nabstract\rabstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2have revolutionized natural language processing and search engine dynamics.However, these models incur exceptionally high computational costs. Forinstance, GPT-3 consists of 175 billion parameters and inference on thesemodels also demands billions of floating-point operations. Caching is a naturalsolution to reduce LLM inference costs on repeated queries. However, existingcaching methods are incapable of finding semantic similarities among LLMqueries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a semantic cache for LLMs that identifiessemantically similar queries to determine cache hit or miss. Using MeanCache,the response to a user\u0026rsquo;s semantically similar query can be retrieved from alocal cache rather than re-querying the LLM, thus reducing costs, serviceprovider load, and environmental impact. MeanCache leverages Federated Learning(FL) to collaboratively train a query similarity model in a distributed manneracross numerous users without violating privacy. By placing a local cache ineach user\u0026rsquo;s device and using FL, MeanCache reduces the latency and costs andenhances model performance, resulting in lower cache false hit rates. Ourexperiments, benchmarked against the GPTCache, reveal that MeanCache attains anapproximately 17% higher F-score and a 20% increase in precision duringsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces thestorage requirement by 83% and accelerates semantic cache hit-and-missdecisions by 11%, while still surpassing GPTCache.\r2024-03-04\nDifferentially Private Representation Learning via Image Captioning\nTom Sander Yaodong Yu Maziar Sanjabi Alain Durmus Yi Ma Kamalika Chaudhuri Chuan Guo\nabstract\rabstract: Differentially private (DP) machine learning is considered the gold-standardsolution for training a model from sensitive data while still preservingprivacy. However, a major barrier to achieving this ideal is its sub-optimalprivacy-accuracy trade-off, which is particularly visible in DP representationlearning. Specifically, it has been shown that under modest privacy budgets,most models learn representations that are not significantly better thanhand-crafted features. In this work, we show that effective DP representationlearning can be done via image captioning and scaling up to internet-scalemultimodal datasets. Through a series of engineering tricks, we successfullytrain a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratchusing a reasonable amount of computation, and obtaining unprecedentedhigh-quality image features that can be used in a variety of downstream visionand vision-language tasks. For example, under a privacy budget of$\\varepsilon=8$, a linear classifier trained on top of learned DP-Cap featuresattains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTAof 56.5%. Our work challenges the prevailing sentiment that high-utility DPrepresentation learning cannot be achieved by training from scratch.\rVision-Language Models for Medical Report Generation and Visual Question Answering: A Review\nIryna Hartsock Ghulam Rasool\nabstract\rabstract: Medical vision-language models (VLMs) combine computer vision and naturallanguage processing to analyze visual and textual medical data. Our paperreviews recent advancements in developing VLMs specialized for healthcare,focusing on models designed for medical report generation and visual questionanswering. We provide background on natural language processing and computervision, explaining how techniques from both fields are integrated into VLMs toenable learning from multimodal data. Key areas we address include theexploration of medical vision-language datasets, in-depth analyses ofarchitectures and pre-training strategies employed in recent noteworthy medicalVLMs, and comprehensive discussion on evaluation metrics for assessing VLMs\u0026rsquo;performance in medical report generation and visual question answering. We alsohighlight current challenges and propose future directions, including enhancingclinical validity and addressing patient privacy concerns. Overall, our reviewsummarizes recent progress in developing VLMs to harness multimodal medicaldata for improved healthcare applications.\rSciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis\nHengxing Cai Xiaochen Cai Junhan Chang Sihang Li Lin Yao Changxin Wang Zhifeng Gao Yongge Li Mujie Lin Shuwen Yang Jiankun Wang Yuqi Yin Yaqi Li Linfeng Zhang Guolin Ke\nabstract\rabstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionizednatural language understanding and generation, igniting a surge of interest inleveraging these technologies for the nuanced field of scientific literatureanalysis. Existing benchmarks, however, inadequately evaluate the proficiencyof LLMs in the scientific domain, especially in scenarios involving complexcomprehension and multimodal data. In response, we introduced SciAssess, abenchmark tailored for the in-depth analysis of scientific literature, craftedto provide a thorough assessment of LLMs\u0026rsquo; efficacy. SciAssess focuses onevaluating LLMs\u0026rsquo; abilities in memorization, comprehension, and analysis withinscientific contexts. It includes representative tasks from diverse scientificfields, such as general chemistry, organic materials, and alloy materials. Andrigorous quality control measures ensure its reliability in terms ofcorrectness, anonymization, and copyright compliance. SciAssess evaluatesleading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying theirstrengths and areas for improvement and supporting the ongoing development ofLLM applications in scientific literature analysis. SciAssess and its resourcesare made available at https://sci-assess.github.io, offering a valuable toolfor advancing LLM capabilities in scientific literature analysis.\rPushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities\nZheng Lin Guanqiao Qu Qiyuan Chen Xianhao Chen Zhe Chen Kaibin Huang\nabstract\rabstract: Large language models (LLMs), which have shown remarkable capabilities, arerevolutionizing AI development and potentially shaping our future. However,given their multimodality, the status quo cloud-based deployment faces somecritical challenges: 1) long response time; 2) high bandwidth costs; and 3) theviolation of data privacy. 6G mobile edge computing (MEC) systems may resolvethese pressing issues. In this article, we explore the potential of deployingLLMs at the 6G edge. We start by introducing killer applications powered bymultimodal LLMs, including robotics and healthcare, to highlight the need fordeploying LLMs in the vicinity of end users. Then, we identify the criticalchallenges for LLM deployment at the edge and envision the 6G MEC architecturefor LLMs. Furthermore, we delve into two design aspects, i.e., edge trainingand edge inference for LLMs. In both aspects, considering the inherent resourcelimitations at the edge, we discuss various cutting-edge techniques, includingsplit learning/inference, parameter-efficient fine-tuning, quantization, andparameter-sharing inference, to facilitate the efficient deployment of LLMs.This article serves as a position paper for thoroughly identifying themotivation, challenges, and pathway for empowering LLMs at the 6G edge.\rRing-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?\nYu-Lin Tsai Chia-Yi Hsu Chulin Xie Chih-Hsun Lin Jia-You Chen Bo Li Pin-Yu Chen Chia-Mu Yu Chun-Ying Huang\nabstract\rabstract: Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion(SD), have recently demonstrated exceptional capabilities for generatinghigh-quality content. However, this progress has raised several concerns ofpotential misuse, particularly in creating copyrighted, prohibited, andrestricted content, or NSFW (not safe for work) images. While efforts have beenmade to mitigate such problems, either by implementing a safety filter at theevaluation stage or by fine-tuning models to eliminate undesirable concepts orstyles, the effectiveness of these safety measures in dealing with a wide rangeof prompts remains largely unexplored. In this work, we aim to investigatethese safety mechanisms by proposing one novel concept retrieval algorithm forevaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2Idiffusion models, where the whole evaluation can be prepared in advance withoutprior knowledge of the target model. Specifically, Ring-A-Bell first performsconcept extraction to obtain holistic representations for sensitive andinappropriate concepts. Subsequently, by leveraging the extracted concept,Ring-A-Bell automatically identifies problematic prompts for diffusion modelswith the corresponding generation of inappropriate content, allowing the userto assess the reliability of deployed safety mechanisms. Finally, weempirically validate our method by testing online services such as Midjourneyand various methods of concept removal. Our results show that Ring-A-Bell, bymanipulating safe prompting benchmarks, can transform prompts that wereoriginally regarded as safe to evade existing safety mechanisms, thus revealingthe defects of the so-called safety mechanisms which could practically lead tothe generation of harmful contents. Our codes are available athttps://github.com/chiayi-hsu/Ring-A-Bell.\rDifferentially Private Synthetic Data via Foundation Model APIs 2: Text\nChulin Xie Zinan Lin Arturs Backurs Sivakanth Gopi Da Yu Huseyin A Inan Harsha Nori Haotian Jiang Huishuai Zhang Yin Tat Lee Bo Li Sergey Yekhanin\nabstract\rabstract: Text data has become extremely valuable due to the emergence of machinelearning algorithms that learn from it. A lot of high-quality text datagenerated in the real world is private and therefore cannot be shared or usedfreely due to privacy concerns. Generating synthetic replicas of private textdata with a formal privacy guarantee, i.e., differential privacy (DP), offers apromising and scalable solution. However, existing methods necessitate DPfinetuning of large language models (LLMs) on private data to generate DPsynthetic data. This approach is not viable for proprietary LLMs (e.g.,GPT-3.5) and also demands considerable computational resources for open-sourceLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)algorithm to generate DP synthetic images with only API access to diffusionmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, thatapplies to the complex setting of text. We use API access to an LLM andgenerate DP synthetic text without any model training. We conduct comprehensiveexperiments on three benchmark datasets. Our results demonstrate that Aug-PEproduces DP synthetic text that yields competitive utility with the SOTA DPfinetuning baselines. This underscores the feasibility of relying solely on APIaccess of LLMs to produce high-quality DP synthetic texts, thereby facilitatingmore accessible routes to privacy-preserving LLM applications. Our code anddata are available at https://github.com/AI-secure/aug-pe.\r2024-03-03\nReMatch: Retrieval Enhanced Schema Matching with LLMs\nEitam Sheetrit Menachem Brief Moshik Mishaeli Oren Elisha\nabstract\rabstract: Schema matching is a crucial task in data integration, involving thealignment of a source database schema with a target schema to establishcorrespondence between their elements. This task is challenging due to textualand semantic heterogeneity, as well as differences in schema sizes. Althoughmachine-learning-based solutions have been explored in numerous studies, theyoften suffer from low accuracy, require manual mapping of the schemas for modeltraining, or need access to source schema data which might be unavailable dueto privacy concerns. In this paper we present a novel method, named ReMatch,for matching schemas using retrieval-enhanced Large Language Models (LLMs). Ourmethod avoids the need for predefined mapping, any model training, or access todata in the source database. In the ReMatch method the tables of the targetschema and the attributes of the source schema are first represented asstructured passage-based documents. For each source attribute document, weretrieve $J$ documents, representing target schema tables, according to theirsemantic relevance. Subsequently, we create a prompt for every source table,comprising all its attributes and their descriptions, alongside all attributesfrom the set of top $J$ target tables retrieved previously. We employ LLMsusing this prompt for the matching task, yielding a ranked list of $K$potential matches for each source attribute. Our experimental results on largereal-world schemas demonstrate that ReMatch significantly improves matchingcapabilities and outperforms other machine learning approaches. By eliminatingthe requirement for training data, ReMatch becomes a viable solution forreal-world scenarios.\rOn the Compressibility of Quantized Large Language Models\nYu Mao Weilan Wang Hongchao Du Nan Guan Chun Jason Xue\nabstract\rabstract: Deploying Large Language Models (LLMs) on edge or mobile devices offerssignificant benefits, such as enhanced data privacy and real-time processingcapabilities. However, it also faces critical challenges due to the substantialmemory requirement of LLMs. Quantization is an effective way of reducing themodel size while maintaining good performance. However, even afterquantization, LLMs may still be too big to fit entirely into the limited memoryof edge or mobile devices and have to be partially loaded from the storage tocomplete the inference. In this case, the I/O latency of model loading becomesthe bottleneck of the LLM inference latency. In this work, we take apreliminary step of studying applying data compression techniques to reducedata movement and thus speed up the inference of quantized LLM onmemory-constrained devices. In particular, we discussed the compressibility ofquantized LLMs, the trade-off between the compressibility and performance ofquantized LLMs, and opportunities to optimize both of them jointly.\r2024-03-02\nDetection and Analysis of Stress-Related Posts in Reddit Acamedic Communities\nNazzere Oryngozha Pakizar Shamoi Ayan Igali\nabstract\rabstract: Nowadays, the significance of monitoring stress levels and recognizing earlysigns of mental illness cannot be overstated. Automatic stress detection intext can proactively help manage stress and protect mental well-being. Intoday\u0026rsquo;s digital era, social media platforms reflect the psychologicalwell-being and stress levels within various communities. This study focuses ondetecting and analyzing stress-related posts in Reddit academic communities.Due to online education and remote work, these communities have become centralfor academic discussions and support. We classify text as stressed or not usingnatural language processing and machine learning classifiers, with Dreaddit asour training dataset, which contains labeled data from Reddit. Next, we collectand analyze posts from various academic subreddits. We identified that the mosteffective individual feature for stress detection is the Bag of Words, pairedwith the Logistic Regression classifier, achieving a 77.78% accuracy rate andan F1 score of 0.79 on the DReaddit dataset. This combination also performsbest in stress detection on human-annotated datasets, with a 72% accuracy rate.Our key findings reveal that posts and comments in professors Redditcommunities are the most stressful, compared to other academic levels,including bachelor, graduate, and Ph.D. students. This research contributes toour understanding of the stress levels within academic communities. It can helpacademic institutions and online communities develop measures and interventionsto address this issue effectively.\rInexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy\nJamie Hayes Ilia Shumailov Eleni Triantafillou Amr Khalifa Nicolas Papernot\nabstract\rabstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel\u0026rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to theirU-MIA'' counterparts). We propose a categorization of existing U-MIAs intopopulation U-MIAs\u0026rsquo;\u0026rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs\u0026rsquo;\u0026rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.\rKnowledge Sanitization of Large Language Models\nYoichi Ishibashi Hidetoshi Shimodaira\nabstract\rabstract: We explore a knowledge sanitization approach to mitigate the privacy concernsassociated with large language models (LLMs). LLMs trained on a large corpus ofWeb data can memorize and potentially reveal sensitive or confidentialinformation, raising critical security concerns. Our technique efficientlyfine-tunes these models using the Low-Rank Adaptation (LoRA) method, promptingthem to generate harmless responses such as ``I don\u0026rsquo;t know\u0026rsquo;\u0026rsquo; when queried aboutspecific information. Experimental results in a closed-book question-answeringtask show that our straightforward method not only minimizes particularknowledge leakage but also preserves the overall performance of LLMs. These twoadvantages strengthen the defense against extraction attacks and reduces theemission of harmful content such as hallucinations.\rEvaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data\nAritra Hota Soumyajit Chatterjee Sandip Chakraborty\nabstract\rabstract: Traditional human-in-the-loop-based annotation for time-series data likeinertial data often requires access to alternate modalities like video or audiofrom the environment. These alternate sources provide the necessary informationto the human annotator, as the raw numeric data is often too obfuscated evenfor an expert. However, this traditional approach has many concerns surroundingoverall cost, efficiency, storage of additional modalities, time, scalability,and privacy. Interestingly, recent large language models (LLMs) are alsotrained with vast amounts of publicly available alphanumeric data, which allowsthem to comprehend and perform well on tasks beyond natural languageprocessing. Naturally, this opens up a potential avenue to explore LLMs asvirtual annotators where the LLMs will be directly provided the raw sensor datafor annotation instead of relying on any alternate modality. Naturally, thiscould mitigate the problems of the traditional human-in-the-loop approach.Motivated by this observation, we perform a detailed study in this paper toassess whether the state-of-the-art (SOTA) LLMs can be used as virtualannotators for labeling time-series physical sensing data. To perform this in aprincipled manner, we segregate the study into two major phases. In the firstphase, we investigate the challenges an LLM like GPT-4 faces in comprehendingraw sensor data. Considering the observations from phase 1, in the next phase,we investigate the possibility of encoding the raw sensor data using SOTA SSLapproaches and utilizing the projected time-series data to get annotations fromthe LLM. Detailed evaluation with four benchmark HAR datasets shows thatSSL-based encoding and metric-based guidance allow the LLM to make morereasonable decisions and provide accurate annotations without requiringcomputationally expensive fine-tuning or sophisticated prompt engineering.\r2024-03-01\nBasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)\nSean Wellington\nabstract\rabstract: BasedAI is a distributed network of machines which introduces decentralizedinfrastructure capable of integrating Fully Homomorphic Encryption (FHE) withany large language model (LLM) connected to its network. The proposed frameworkembeds a default mechanism, called \u0026ldquo;Cerberus Squeezing\u0026rdquo;, into the miningprocess which enables the transformation of a standard LLMs into encryptedzero-knowledge LLMs, or \u0026ldquo;ZK-LLMs\u0026rdquo;, leveraging insights from generativeadversarial networks for data privacy. This novel quantization mechanismempowers BasedAI miners to process and respond to prompts derived from Userinteraction with LLMs without the need for decrypting either the queries ortheir corresponding responses. The introduction of Cerberus Squeezingsignificantly improves performance degradation caused by quantized functions incurrent FHE-compliant computing environments by proactively optimizing callsbetween users, miners, and validators.\rTalkin\u0026rsquo; \u0026lsquo;Bout AI Generation: Copyright and the Generative-AI Supply Chain\nKatherine Lee A. Feder Cooper James Grimmelmann\nabstract\rabstract: \u0026ldquo;Does generative AI infringe copyright?\u0026rdquo; is an urgent question. It is also adifficult question, for two reasons. First, \u0026ldquo;generative AI\u0026rdquo; is not just oneproduct from one company. It is a catch-all name for a massive ecosystem ofloosely related technologies, including conversational text chatbots likeChatGPT, image generators like Midjourney and DALL-E, coding assistants likeGitHub Copilot, and systems that compose music and create videos. These systemsbehave differently and raise different legal issues. The second problem is thatcopyright law is notoriously complicated, and generative-AI systems manage totouch on a great many corners of it: authorship, similarity, direct andindirect liability, fair use, and licensing, among much else. These issuescannot be analyzed in isolation, because there are connections everywhere. In this Article, we aim to bring order to the chaos. To do so, we introducethe generative-AI supply chain: an interconnected set of stages that transformtraining data (millions of pictures of cats) into generations (a new,potentially never-seen-before picture of a cat that has never existed).Breaking down generative AI into these constituent stages reveals all of theplaces at which companies and users make choices that have copyrightconsequences. It enables us to trace the effects of upstream technical designson downstream uses, and to assess who in these complicated sociotechnicalsystems bears responsibility for infringement when it happens. Because weengage so closely with the technology of generative AI, we are able to shedmore light on the copyright questions. We do not give definitive answers as towho should and should not be held liable. Instead, we identify the keydecisions that courts will need to make as they grapple with these issues, andpoint out the consequences that would likely flow from different liabilityregimes.\rDifferentially Private Knowledge Distillation via Synthetic Text Generation\nJames Flemings Murali Annavaram\nabstract\rabstract: Large Language models (LLMs) are achieving state-of-the-art performance inmany different downstream tasks. However, the increasing urgency of dataprivacy requires LLMs to train with Differential Privacy (DP) on private data.Concurrently it is also necessary to compress LLMs for real-life deployments onresource-constrained devices or latency-sensitive applications. Differentialprivacy and model compression generally must trade off utility loss to achievetheir objectives. Moreover, concurrently achieving both can result in even moreutility loss. To this end, we propose a novel differentially private knowledgedistillation algorithm that exploits synthetic data generated by adifferentially private LLM. The knowledge of a teacher model is transferredonto the student in two ways: one way from the synthetic data itself, the hardlabels, and the other way by the output distribution of the teacher modelevaluated on the synthetic data, the soft labels. Furthermore, if the teacherand student share a similar architectural structure, we can further distillknowledge by exploiting hidden representations. Our results show that ourframework substantially improves the utility over existing baselines withstrong privacy parameters, {\\epsilon} = 2, validating that we can successfullycompress autoregressive LLMs while preserving the privacy of training data.\rTeach LLMs to Phish: Stealing Private Information from Language Models\nAshwinee Panda Christopher A. Choquette-Choo Zhengming Zhang Yaoqing Yang Prateek Mittal\nabstract\rabstract: When large language models are trained on private data, it can be asignificant privacy risk for them to memorize and regurgitate sensitiveinformation. In this work, we propose a new practical data extraction attackthat we call \u0026ldquo;neural phishing\u0026rdquo;. This attack enables an adversary to target andextract sensitive or personally identifiable information (PII), e.g., creditcard numbers, from a model trained on user data with upwards of 10% attacksuccess rates, at times, as high as 50%. Our attack assumes only that anadversary can insert as few as 10s of benign-appearing sentences into thetraining dataset using only vague priors on the structure of the user data.\rSafeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training\nAlyssa Huang Peihan Liu Ryumei Nakada Linjun Zhang Wanrong Zhang\nabstract\rabstract: The surge in multimodal AI\u0026rsquo;s success has sparked concerns over data privacyin vision-and-language tasks. While CLIP has revolutionized multimodal learningthrough joint training on images and text, its potential to unintentionallydisclose sensitive information necessitates the integration ofprivacy-preserving mechanisms. We introduce a differentially private adaptationof the Contrastive Language-Image Pretraining (CLIP) model that effectivelyaddresses privacy concerns while retaining accuracy. Our proposed method,Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diversevision-and-language tasks such as image classification and visual questionanswering. We demonstrate that our approach retains performance on par with thestandard non-private CLIP model. Furthermore, we analyze our proposed algorithmunder linear representation settings. We derive the convergence rate of ouralgorithm and show a trade-off between utility and privacy when gradients areclipped per-batch and the loss function does not satisfy smoothness conditionsassumed in the literature for the analysis of DP-SGD.\r2024-02-29\nEROS: Entity-Driven Controlled Policy Document Summarization\nJoykirat Singh Sehban Fazili Rohan Jain Md Shad Akhtar\nabstract\rabstract: Privacy policy documents have a crucial role in educating individuals aboutthe collection, usage, and protection of users\u0026rsquo; personal data by organizations.However, they are notorious for their lengthy, complex, and convoluted languageespecially involving privacy-related entities. Hence, they pose a significantchallenge to users who attempt to comprehend organization\u0026rsquo;s data usage policy.In this paper, we propose to enhance the interpretability and readability ofpolicy documents by using controlled abstractive summarization \u0026ndash; we enforcethe generated summaries to include critical privacy-related entities (e.g.,data and medium) and organization\u0026rsquo;s rationale (e.g.,target and reason) incollecting those entities. To achieve this, we develop PD-Sum, apolicy-document summarization dataset with marked privacy-related entitylabels. Our proposed model, EROS, identifies critical entities through aspan-based entity extraction model and employs them to control the informationcontent of the summaries using proximal policy optimization (PPO). Comparisonshows encouraging improvement over various baselines. Furthermore, we furnishqualitative and human evaluations to establish the efficacy of EROS.\rTowards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models\nChen Qian Jie Zhang Wei Yao Dongrui Liu Zhenfei Yin Yu Qiao Yong Liu Jing Shao\nabstract\rabstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Moststudies concentrate on fully pre-trained LLMs to better understand and improveLLMs\u0026rsquo; trustworthiness. In this paper, to reveal the untapped potential ofpre-training, we pioneer the exploration of LLMs\u0026rsquo; trustworthiness during thisperiod, focusing on five key dimensions: reliability, privacy, toxicity,fairness, and robustness. To begin with, we apply linear probing to LLMs. Thehigh probing accuracy suggests that \\textit{LLMs in early pre-training canalready distinguish concepts in each trustworthiness dimension}. Therefore, tofurther uncover the hidden possibilities of pre-training, we extract steeringvectors from a LLM\u0026rsquo;s pre-training checkpoints to enhance the LLM\u0026rsquo;strustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutualinformation estimation is bounded by linear probing accuracy, we also probeLLMs with mutual information to investigate the dynamics of trustworthinessduring pre-training. We are the first to observe a similar two-phasephenomenon: fitting and compression~\\citep{shwartz2017opening}. This researchprovides an initial exploration of trustworthiness modeling during LLMpre-training, seeking to unveil new insights and spur further developments inthe field. We will make our code publicly accessible at\\url{https://github.com/ChnQ/TracingLLM}.\rSynthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting\nLisa Oakley Steven Holtzen Alina Oprea\nabstract\rabstract: Programmatically generating tight differential privacy (DP) bounds is a hardproblem. Two core challenges are (1) finding expressive, compact, and efficientencodings of the distributions of DP algorithms, and (2) state space explosionstemming from the multiple quantifiers and relational properties of the DPdefinition. We address the first challenge by developing a method for tight privacy andaccuracy bound synthesis using weighted model counting on binary decisiondiagrams, a state of the art technique from the artificial intelligence andautomated reasoning communities for exactly computing probabilitydistributions. We address the second challenge by developing a framework forleveraging inherent symmetries in DP algorithms. Our solution benefits fromongoing research in probabilistic programming languages, allowing us tosuccinctly and expressively represent different DP algorithms with approachablelanguage syntax that can be used by non-experts. We provide a detailed case study of our solution on the binary randomizedresponse algorithm. We also evaluate an implementation of our solution usingthe Dice probabilistic programming language for the randomized response andtruncated geometric above threshold algorithms. We compare to prior work onexact DP verification using Markov chain probabilistic model checking. Very fewexisting works consider mechanized analysis of accuracy guarantees for DPalgorithms. We additionally provide a detailed analysis using our technique forfinding tight accuracy bounds for DP algorithms.\rPRSA: Prompt Reverse Stealing Attacks against Large Language Models\nYong Yang Xuhong Zhang Yi Jiang Xi Chen Haoyu Wang Shouling Ji Zonghui Wang\nabstract\rabstract: Prompt, recognized as crucial intellectual property, enables large languagemodels (LLMs) to perform specific tasks without the need of fine-tuning,underscoring their escalating importance. With the rise of prompt-basedservices, such as prompt marketplaces and LLM applications, providers oftendisplay prompts\u0026rsquo; capabilities through input-output examples to attract users.However, this paradigm raises a pivotal security concern: does the exposure ofinput-output pairs pose the risk of potential prompt leakage, infringing on theintellectual property rights of the developers? To our knowledge, this problemstill has not been comprehensively explored yet. To remedy this gap, in thispaper, we perform the first in depth exploration and propose a novel attackframework for reverse-stealing prompts against commercial LLMs, namely PRSA.The main idea of PRSA is that by analyzing the critical features of theinput-output pairs, we mimic and gradually infer (steal) the target prompts. Indetail, PRSA mainly consists of two key phases: prompt mutation and promptpruning. In the mutation phase, we propose a prompt attention algorithm basedon differential feedback to capture these critical features for effectivelyinferring the target prompts. In the prompt pruning phase, we identify and maskthe words dependent on specific inputs, enabling the prompts to accommodatediverse inputs for generalization. Through extensive evaluation, we verify thatPRSA poses a severe threat in real world scenarios. We have reported thesefindings to prompt service providers and actively collaborate with them to takeprotective measures for prompt copyright.\rAn Unforgeable Publicly Verifiable Watermark for Large Language Models\nAiwei Liu Leyi Pan Xuming Hu Shu\u0026rsquo;ang Li Lijie Wen Irwin King Philip S. Yu\nabstract\rabstract: Recently, text watermarking algorithms for large language models (LLMs) havebeen proposed to mitigate the potential harms of text generated by LLMs,including fake news and copyright issues. However, current watermark detectionalgorithms require the secret key used in the watermark generation process,making them susceptible to security breaches and counterfeiting during publicdetection. To address this limitation, we propose an unforgeable publiclyverifiable watermark algorithm that uses two different neural networks forwatermark generation and detection, instead of using the same key at bothstages. Meanwhile, the token embedding parameters are shared between thegeneration and detection networks, which makes the detection network achieve ahigh accuracy very efficiently. Experiments demonstrate that our algorithmattains high detection accuracy and computational efficiency through neuralnetworks with a minimized number of parameters. Subsequent analysis confirmsthe high complexity involved in forging the watermark from the detectionnetwork. Our code and data are available at\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable_watermark}.\r2024-02-28\nFedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing\nTerence Jie Chua Wenhan Yu Jun Zhao Kwok-Yan Lam\nabstract\rabstract: The emergence of foundation models, including language and vision models, hasreshaped AI\u0026rsquo;s landscape, offering capabilities across various applications.Deploying and fine-tuning these large models, like GPT-3 and BERT, presentschallenges, especially in the current foundation model era. We introduceEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, weexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT usesadapters, emulators, and PEFT for federated model tuning, enhancing modelprivacy and memory efficiency. Adapters adjust pre-trained models, whileemulators give a compact representation of original models, addressing bothprivacy and efficiency. Adaptable to various neural networks, our approach alsouses deep reinforcement learning for hyper-parameter optimization. We testedFedPEAT in a unique scenario with a server participating in collaborativefederated tuning, showcasing its potential in tackling foundation modelchallenges.\rMedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices\nAbdul Basit Khizar Hussain Muhammad Abdullah Hanif Muhammad Shafique\nabstract\rabstract: Large language models (LLMs) are revolutionizing various domains with theirremarkable natural language processing (NLP) abilities. However, deploying LLMsin resource-constrained edge computing and embedded systems presentssignificant challenges. Another challenge lies in delivering medical assistancein remote areas with limited healthcare facilities and infrastructure. Toaddress this, we introduce MedAide, an on-premise healthcare chatbot. Itleverages tiny-LLMs integrated with LangChain, providing efficient edge-basedpreliminary medical diagnostics and support. MedAide employs modeloptimizations for minimal memory footprint and latency on embedded edge deviceswithout server infrastructure. The training process is optimized using low-rankadaptation (LoRA). Additionally, the model is trained on diverse medicaldatasets, employing reinforcement learning from human feedback (RLHF) toenhance its domain-specific capabilities. The system is implemented on variousconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77%accuracy in medical consultations and scores 56 in USMLE benchmark, enabling anenergy-efficient healthcare assistance platform that alleviates privacyconcerns due to edge-based deployment, thereby empowering the community.\r2024-02-27\nEmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models\nRuisi Zhang Farinaz Koushanfar\nabstract\rabstract: This paper introduces EmMark,a novel watermarking framework for protectingthe intellectual property (IP) of embedded large language models deployed onresource-constrained edge devices. To address the IP theft risks posed bymalicious end-users, EmMark enables proprietors to authenticate ownership byquerying the watermarked model weights and matching the inserted signatures.EmMark\u0026rsquo;s novelty lies in its strategic watermark weight parameters selection,nsuring robustness and maintaining model quality. Extensive proof-of-conceptevaluations of models from OPT and LLaMA-2 families demonstrate EmMark\u0026rsquo;sfidelity, achieving 100% success in watermark extraction with model performancepreservation. EmMark also showcased its resilience against watermark removaland forging attacks.\rBeyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs\nTanise Ceron Neele Falk Ana Barić Dmitry Nikolaev Sebastian Padó\nabstract\rabstract: Due to the widespread use of large language models (LLMs) in ubiquitoussystems, we need to understand whether they embed a specific worldview and whatthese views reflect. Recent studies report that, prompted with politicalquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclearwhether these leanings are reliable (robust to prompt variations) and whetherthe leaning is consistent across policies and political leaning. We propose aseries of tests which assess the reliability and consistency of LLMs\u0026rsquo; stanceson political statements based on a dataset of voting-advice questionnairescollected from seven EU countries and annotated for policy domains. We studyLLMs ranging in size from 7B to 70B parameters and find that their reliabilityincreases with parameter count. Larger models show overall stronger alignmentwith left-leaning parties but differ among policy programs: They evince a(left-wing) positive stance towards environment protection, social welfare butalso (right-wing) law and order, with no consistent preferences in foreignpolicy, migration, and economy.\rBASES: Large-scale Web Search User Simulation with Large Language Model based Agents\nRuiyang Ren Peng Qiu Yingqi Qu Jing Liu Wayne Xin Zhao Hua Wu Ji-Rong Wen Haifeng Wang\nabstract\rabstract: Due to the excellent capacities of large language models (LLMs), it becomesfeasible to develop LLM-based agents for reliable user simulation. Consideringthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,we conduct large-scale user simulation for web search, to improve the analysisand modeling of user search behavior. Specially, we propose BASES, a novel usersimulation framework with LLM-based agents, designed to facilitatecomprehensive simulations of web search user behaviors. Our simulationframework can generate unique user profiles at scale, which subsequently leadsto diverse search behaviors. To demonstrate the effectiveness of BASES, weconduct evaluation experiments based on two human benchmarks in both Chineseand English, demonstrating that BASES can effectively simulate large-scalehuman-like search behaviors. To further accommodate the research on web search,we develop WARRIORS, a new large-scale dataset encompassing web search userbehaviors, including both Chinese and English versions, which can greatlybolster research in the field of information retrieval. Our code and data willbe publicly released soon.\rGenerative AI and Copyright: A Dynamic Perspective\nS. Alex Yang Angela Huyue Zhang\nabstract\rabstract: The rapid advancement of generative AI is poised to disrupt the creativeindustry. Amidst the immense excitement for this new technology, its futuredevelopment and applications in the creative industry hinge crucially upon twocopyright issues: 1) the compensation to creators whose content has been usedto train generative AI models (the fair use standard); and 2) the eligibilityof AI-generated content for copyright protection (AI-copyrightability). Whileboth issues have ignited heated debates among academics and practitioners, mostanalysis has focused on their challenges posed to existing copyright doctrines.In this paper, we aim to better understand the economic implications of thesetwo regulatory issues and their interactions. By constructing a dynamic modelwith endogenous content creation and AI model development, we unravel theimpacts of the fair use standard and AI-copyrightability on AI development, AIcompany profit, creators income, and consumer welfare, and how these impactsare influenced by various economic and operational factors. For example, whilegenerous fair use (use data for AI training without compensating the creator)benefits all parties when abundant training data exists, it can hurt creatorsand consumers when such data is scarce. Similarly, stronger AI-copyrightability(AI content enjoys more copyright protection) could hinder AI development andreduce social welfare. Our analysis also highlights the complex interplaybetween these two copyright issues. For instance, when existing training datais scarce, generous fair use may be preferred only when AI-copyrightability isweak. Our findings underscore the need for policymakers to embrace a dynamic,context-specific approach in making regulatory decisions and provide insightsfor business leaders navigating the complexities of the global regulatoryenvironment.\r2024-02-26\nPandora\u0026rsquo;s White-Box: Increased Training Data Leakage in Open LLMs\nJeffrey G. Wang Jason Wang Marvin Li Seth Neel\nabstract\rabstract: In this paper we undertake a systematic study of privacy attacks against opensource Large Language Models (LLMs), where an adversary has access to eitherthe model weights, gradients, or losses, and tries to exploit them to learnsomething about the underlying training data. Our headline results are thefirst membership inference attacks (MIAs) against pre-trained LLMs that areable to simultaneously achieve high TPRs and low FPRs, and a pipeline showingthat over $50%$ (!) of the fine-tuning dataset can be extracted from afine-tuned LLM in natural settings. We consider varying degrees of access tothe underlying model, customization of the language model, and resourcesavailable to the attacker. In the pre-trained setting, we propose three newwhite-box MIAs: an attack based on the gradient norm, a supervised neuralnetwork classifier, and a single step loss ratio attack. All outperformexisting black-box baselines, and our supervised attack closes the gap betweenMIA attack success against LLMs and other types of models. In fine-tuning, wefind that given access to the loss of the fine-tuned and base models, afine-tuned loss ratio attack FLoRA is able to achieve near perfect MIApeformance. We then leverage these MIAs to extract fine-tuning data fromfine-tuned language models. We find that the pipeline of generating fromfine-tuned models prompted with a small snippet of the prefix of each trainingexample, followed by using FLoRa to select the most likely training sample,succeeds the majority of the fine-tuning dataset after only $3$ epochs offine-tuning. Taken together, these findings show that highly effective MIAs areavailable in almost all LLM training settings, and highlight that great caremust be taken before LLMs are fine-tuned on highly sensitive data and thendeployed.\rDecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\nBoxin Wang Weixin Chen Hengzhi Pei Chulin Xie Mintong Kang Chenhui Zhang Chejian Xu Zidi Xiong Ritik Dutta Rylan Schaeffer Sang T. Truong Simran Arora Mantas Mazeika Dan Hendrycks Zinan Lin Yu Cheng Sanmi Koyejo Dawn Song Bo Li\nabstract\rabstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance \u0026ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives \u0026ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .\rMobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\nOmkar Thawakar Ashmal Vayani Salman Khan Hisham Cholakal Rao M. Anwer Michael Felsberg Tim Baldwin Eric P. Xing Fahad Shahbaz Khan\nabstract\rabstract: \u0026ldquo;Bigger the better\u0026rdquo; has been the predominant trend in recent Large LanguageModels (LLMs) development. However, LLMs do not suit well for scenarios thatrequire on-device processing, energy efficiency, low memory footprint, andresponse efficiency. These requisites are crucial for privacy, security, andsustainable deployment. This paper explores the \u0026ldquo;less is more\u0026rdquo; paradigm byaddressing the challenge of designing accurate yet efficient Small LanguageModels (SLMs) for resource constrained devices. Our primary contribution is theintroduction of an accurate and fully transparent open-source 0.5 billion(0.5B) parameter SLM, named MobiLlama, catering to the specific needs ofresource-constrained computing with an emphasis on enhanced performance withreduced resource demands. MobiLlama is a SLM design that initiates from alarger model and applies a careful parameter sharing scheme to reduce both thepre-training and the deployment cost. Our work strives to not only bridge thegap in open-source SLMs but also ensures full transparency, where completetraining data pipeline, training code, model weights, and over 300 checkpointsalong with evaluation codes is available at :https://github.com/mbzuai-oryx/MobiLlama.\rA Paradigm Shift: The Future of Machine Translation Lies with Large Language Models\nChenyang Lyu Zefeng Du Jitao Xu Yitao Duan Minghao Wu Teresa Lynn Alham Fikri Aji Derek F. Wong Longyue Wang\nabstract\rabstract: Machine Translation (MT) has greatly advanced over the years due to thedevelopments in deep neural networks. However, the emergence of Large LanguageModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MTdomain. In this context, we believe that the future of MT is intricately tiedto the capabilities of LLMs. These models not only offer vast linguisticunderstandings but also bring innovative methodologies, such as prompt-basedtechniques, that have the potential to further elevate MT. In this paper, weprovide an overview of the significant enhancements in MT that are influencedby LLMs and advocate for their pivotal role in upcoming MT research andimplementations. We highlight several new MT directions, emphasizing thebenefits of LLMs in scenarios such as Long-Document Translation, StylizedTranslation, and Interactive Translation. Additionally, we address theimportant concern of privacy in LLM-driven MT and suggest essentialprivacy-preserving strategies. By showcasing practical instances, we aim todemonstrate the advantages that LLMs offer, particularly in tasks liketranslating extended documents. We conclude by emphasizing the critical role ofLLMs in guiding the future evolution of MT and offer a roadmap for futureexploration in the sector.\rLLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery\nKexin Chen Yuyang Du Tao You Mobarakol Islam Ziyu Guo Yueming Jin Guangyong Chen Pheng-Ann Heng\nabstract\rabstract: Visual question answering (VQA) can be fundamentally crucial for promotingrobotic-assisted surgical education. In practice, the needs of trainees areconstantly evolving, such as learning more surgical types, adapting todifferent robots, and learning new surgical instruments and techniques for onesurgery. Therefore, continually updating the VQA system by a sequential datastream from multiple resources is demanded in robotic surgery to address newtasks. In surgical scenarios, the storage cost and patient data privacy oftenrestrict the availability of old data when updating the model, necessitating anexemplar-free continual learning (CL) setup. However, prior studies overlookedtwo vital problems of the surgical domain: i) large domain shifts from diversesurgical operations collected from multiple departments or clinical centers,and ii) severe data imbalance arising from the uneven presence of surgicalinstruments or activities during surgical procedures. This paper proposes toaddress these two problems with a multimodal large language model (LLM) and anadaptive weight assignment methodology. We first develop a new multi-teacher CLframework that leverages a multimodal LLM as the additional teacher. The stronggeneralization ability of the LLM can bridge the knowledge gap when domainshifts and data imbalances occur. We then put forth a novel data processingmethod that transforms complex LLM embeddings into logits compatible with ourCL framework. We further design an adaptive weight assignment approach thatbalances the generalization ability of the LLM and the domain expertise of theold CL model. We construct a new dataset for surgical VQA tasks, providingvaluable data resources for future research. Extensive experimental results onthree datasets demonstrate the superiority of our method to other advanced CLmodels.\rKnowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking\nSamuel Kernan Freire Chaofan Wang Mina Foosherian Stefan Wellsandt Santiago Ruiz-Arenas Evangelos Niforatos\nabstract\rabstract: Recent advances in natural language processing enable more intelligent waysto support knowledge sharing in factories. In manufacturing, operatingproduction lines has become increasingly knowledge-intensive, putting strain ona factory\u0026rsquo;s capacity to train and support new operators. This paper introducesa Large Language Model (LLM)-based system designed to retrieve information fromthe extensive knowledge contained in factory documentation and knowledge sharedby expert operators. The system aims to efficiently answer queries fromoperators and facilitate the sharing of new knowledge. We conducted a userstudy at a factory to assess its potential impact and adoption, elicitingseveral perceived benefits, namely, enabling quicker information retrieval andmore efficient resolution of issues. However, the study also highlighted apreference for learning from a human expert when such an option is available.Furthermore, we benchmarked several commercial and open-sourced LLMs for thissystem. The current state-of-the-art model, GPT-4, consistently outperformedits counterparts, with open-source models trailing closely, presenting anattractive option given their data privacy and customization benefits. Insummary, this work offers preliminary insights and a system design forfactories considering using LLM tools for knowledge management.\rTricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks\nAbhinav Rao Sachin Vashistha Atharva Naik Somak Aditya Monojit Choudhury\nabstract\rabstract: Recent explorations with commercial Large Language Models (LLMs) have shownthat non-expert users can jailbreak LLMs by simply manipulating their prompts;resulting in degenerate output behavior, privacy and security breaches,offensive outputs, and violations of content regulator policies. Limitedstudies have been conducted to formalize and analyze these attacks and theirmitigations. We bridge this gap by proposing a formalism and a taxonomy ofknown (and possible) jailbreaks. We survey existing jailbreak methods and theireffectiveness on open-source and commercial LLMs (such as GPT-based models,OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreakdetection in terms of their effectiveness against known attacks. For ouranalysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. Wewill make the dataset public along with the model outputs.\rCodeS: Towards Building Open-source Language Models for Text-to-SQL\nHaoyang Li Jing Zhang Hanbing Liu Ju Fan Xiaokang Zhang Jun Zhu Renjie Wei Hongyan Pan Cuiping Li Hong Chen\nabstract\rabstract: Language models have shown promising performance on the task of translatingnatural language questions into SQL queries (Text-to-SQL). However, most of thestate-of-the-art (SOTA) approaches rely on powerful yet closed-source largelanguage models (LLMs), such as ChatGPT and GPT-4, which may have thelimitations of unclear model architectures, data privacy risks, and expensiveinference overheads. To address the limitations, we introduce CodeS, a seriesof pre-trained language models with parameters ranging from 1B to 15B,specifically designed for the text-to-SQL task. CodeS is a fully open-sourcelanguage model, which achieves superior accuracy with much smaller parametersizes. This paper studies the research challenges in building CodeS. To enhancethe SQL generation abilities of CodeS, we adopt an incremental pre-trainingapproach using a specifically curated SQL-centric corpus. Based on this, weaddress the challenges of schema linking and rapid domain adaptation throughstrategic prompt construction and a bi-directional data augmentation technique.We conduct comprehensive evaluations on multiple datasets, including the widelyused Spider benchmark, the newly released BIRD benchmark, robustness-diagnosticbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, aswell as two real-world datasets created for financial and academicapplications. The experimental results show that our CodeS achieves new SOTAaccuracy and robustness on nearly all challenging text-to-SQL benchmarks.\rUnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models\nYihua Zhang Yimeng Zhang Yuguang Yao Jinghan Jia Jiancheng Liu Xiaoming Liu Sijia Liu\nabstract\rabstract: The rapid advancement of diffusion models (DMs) has not only transformedvarious real-world industries but has also introduced negative societalconcerns, including the generation of harmful content, copyright disputes, andthe rise of stereotypes and biases. To mitigate these issues, machineunlearning (MU) has emerged as a potential solution, demonstrating its abilityto remove undesired generative capabilities of DMs in various applications.However, by examining existing MU evaluation methods, we uncover several keychallenges that can result in incomplete, inaccurate, or biased evaluations forMU in DMs. To address them, we enhance the evaluation metrics for MU, includingthe introduction of an often-overlooked retainability measurement for DMspost-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensivehigh-resolution stylized image dataset that facilitates us to evaluate theunlearning of artistic painting styles in conjunction with associated imageobjects. We show that this dataset plays a pivotal role in establishing astandardized and automated evaluation framework for MU techniques on DMs,featuring 7 quantitative metrics to address various aspects of unlearningeffectiveness. Through extensive experiments, we benchmark 5 state-of-the-artMU methods, revealing novel insights into their pros and cons, and theunderlying unlearning mechanisms. Furthermore, we demonstrate the potential ofUnlearnCanvas to benchmark other generative modeling tasks, such as styletransfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce allthe results in this work can be found athttps://github.com/OPTML-Group/UnlearnCanvas.\rPrivacy-Preserved Neural Graph Databases\nQi Hu Haoran Li Jiaxin Bai Zihao Wang Yangqiu Song\nabstract\rabstract: In the era of large language models (LLMs), efficient and accurate dataretrieval has become increasingly crucial for the use of domain-specific orprivate data in the retrieval augmented generation (RAG). Neural graphdatabases (NGDBs) have emerged as a powerful paradigm that combines thestrengths of graph databases (GDBs) and neural networks to enable efficientstorage, retrieval, and analysis of graph-structured data which can beadaptively trained with LLMs. The usage of neural embedding storage and Complexneural logical Query Answering (CQA) provides NGDBs with generalizationability. When the graph is incomplete, by extracting latent patterns andrepresentations, neural graph databases can fill gaps in the graph structure,revealing hidden relationships and enabling accurate query answering.Nevertheless, this capability comes with inherent trade-offs, as it introducesadditional privacy risks to the domain-specific or private databases. Maliciousattackers can infer more sensitive information in the database usingwell-designed queries such as from the answer sets of where Turing Awardwinners born before 1950 and after 1940 lived, the living places of TuringAward winner Hinton are probably exposed, although the living places may havebeen deleted in the training stage due to the privacy concerns. In this work,we propose a privacy-preserved neural graph database (P-NGDB) framework toalleviate the risks of privacy leakage in NGDBs. We introduce adversarialtraining techniques in the training stage to enforce the NGDBs to generateindistinguishable answers when queried with private information, enhancing thedifficulty of inferring sensitive information through combinations of multipleinnocuous queries.\r2024-02-25\nText Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations\nYafei Xiang Hanyi Yu Yulu Gong Shuning Huo Mengran Zhu\nabstract\rabstract: With the rapid development of artificial intelligence technology, Transformerstructural pre-training model has become an important tool for large languagemodel (LLM) tasks. In the field of e-commerce, these models are especiallywidely used, from text understanding to generating recommendation systems,which provide powerful technical support for improving user experience andoptimizing service processes. This paper reviews the core application scenariosof Transformer pre-training model in e-commerce text understanding andrecommendation generation, including but not limited to automatic generation ofproduct descriptions, sentiment analysis of user comments, construction ofpersonalized recommendation system and automated processing of customer serviceconversations. Through a detailed analysis of the model\u0026rsquo;s working principle,implementation process, and application effects in specific cases, this paperemphasizes the unique advantages of pre-trained models in understanding complexuser intentions and improving the quality of recommendations. In addition, thechallenges and improvement directions for the future are also discussed, suchas how to further improve the generalization ability of the model, the abilityto handle large-scale data sets, and technical strategies to protect userprivacy. Ultimately, the paper points out that the application of Transformerstructural pre-training models in e-commerce has not only driven technologicalinnovation, but also brought substantial benefits to merchants and consumers,and looking forward, these models will continue to play a key role ine-commerce and beyond.\rCognitive Bias in High-Stakes Decision-Making with LLMs\nJessica Echterhoff Yao Liu Abeer Alessa Julian McAuley Zexue He\nabstract\rabstract: Large language models (LLMs) offer significant potential as tools to supportan expanding range of decision-making tasks. However, given their training onhuman (created) data, LLMs can inherit both societal biases against protectedgroups, as well as be subject to cognitive bias. Such human-like bias canimpede fair and explainable decisions made with LLM assistance. Our workintroduces BiasBuster, a framework designed to uncover, evaluate, and mitigatecognitive bias in LLMs, particularly in high-stakes decision-making tasks.Inspired by prior research in psychology and cognitive sciences, we develop adataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,prompt-induced, sequential, inherent). We test various bias mitigationstrategies, amidst proposing a novel method using LLMs to debias their ownprompts. Our analysis provides a comprehensive picture on the presence andeffects of cognitive bias across different commercial and open-source models.We demonstrate that our self-help debiasing effectively mitigate cognitive biaswithout having to manually craft examples for each bias type.\r2024-02-24\nFoot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology\nZhenhua Wang Wei Xie Baosheng Wang Enze Wang Zhiwen Gui Shuoyoucheng Ma Kai Chen\nabstract\rabstract: Large Language Models (LLMs) have gradually become the gateway for people toacquire new knowledge. However, attackers can break the model\u0026rsquo;s securityprotection (\u0026ldquo;jail\u0026rdquo;) to access restricted information, which is called\u0026quot;jailbreaking.\u0026quot; Previous studies have shown the weakness of current LLMs whenconfronted with such jailbreaking attacks. Nevertheless, comprehension of theintrinsic decision-making mechanism within the LLMs upon receipt of jailbreakprompts is noticeably lacking. Our research provides a psychologicalexplanation of the jailbreak prompts. Drawing on cognitive consistency theory,we argue that the key to jailbreak is guiding the LLM to achieve cognitivecoordination in an erroneous direction. Further, we propose an automaticblack-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.This method progressively induces the model to answer harmful questions viamulti-step incremental prompts. We instantiated a prototype system to evaluatethe jailbreaking effectiveness on 8 advanced LLMs, yielding an average successrate of 83.9%. This study builds a psychological perspective on the explanatoryinsights into the intrinsic decision-making logic of LLMs.\r2024-02-23\nUser Inference Attacks on Large Language Models\nNikhil Kandpal Krishna Pillutla Alina Oprea Peter Kairouz Christopher A. Choquette-Choo Zheng Xu\nabstract\rabstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user\u0026rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user\u0026rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.\rFast Adversarial Attacks on Language Models In One GPU Minute\nVinu Sankar Sadasivan Shoumik Saha Gaurang Sriramanan Priyatham Kattakinda Atoosa Chegini Soheil Feizi\nabstract\rabstract: In this paper, we introduce a novel class of fast, beam search-basedadversarial attack (BEAST) for Language Models (LMs). BEAST employsinterpretable parameters, enabling attackers to balance between attack speed,success rate, and the readability of adversarial prompts. The computationalefficiency of BEAST facilitates us to investigate its applications on LMs forjailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-freetargeted attack can jailbreak aligned LMs with high attack success rates withinone minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minutewith a success rate of 89% when compared to a gradient-based baseline thattakes over an hour to achieve 70% success rate using a single Nvidia RTX A600048GB GPU. Additionally, we discover a unique outcome wherein our untargetedattack induces hallucinations in LM chatbots. Through human evaluations, wefind that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% moreincorrect outputs when compared to LM outputs in the absence of our attack. Wealso learn that 22% of the time, BEAST causes Vicuna to generate outputs thatare not relevant to the original prompt. Further, we use BEAST to generateadversarial prompts in a few seconds that can boost the performance of existingmembership inference attacks for LMs. We believe that our fast attack, BEAST,has the potential to accelerate research in LM security and privacy. Ourcodebase is publicly available at https://github.com/vinusankars/BEAST.\rThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nShenglai Zeng Jiankun Zhang Pengfei He Yue Xing Yiding Liu Han Xu Jie Ren Shuaiqiang Wang Dawei Yin Yi Chang Jiliang Tang\nabstract\rabstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs\u0026rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.\rWho Wrote this Code? Watermarking for Code Generation\nTaehyun Lee Seokhee Hong Jaewoo Ahn Ilgee Hong Hwaran Lee Sangdoo Yun Jamin Shin Gunhee Kim\nabstract\rabstract: With the remarkable generation performance of large language models, ethicaland legal concerns about using them have been raised, such as plagiarism andcopyright issues. For such concerns, several approaches to watermark and detectLLM-generated text have been proposed very recently. However, we discover thatthe previous methods fail to function appropriately with code generation tasksbecause of the syntactic and semantic characteristics of code. Based on\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,Selective WatErmarking via Entropy Thresholding (SWEET), that promotes \u0026ldquo;green\u0026quot;tokens only at the position with high entropy of the token distribution duringgeneration, thereby preserving the correctness of the generated code. Thewatermarked code is detected by the statistical test and Z-score based on theentropy information. Our experiments on HumanEval and MBPP show that SWEETsignificantly improves the Pareto Frontier between the code correctness andwatermark detection performance. We also show that notable post-hoc detectionmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show thatsetting a reasonable entropy threshold is not much of a challenge. Code isavailable at https://github.com/hongcheki/sweet-watermark.\rA First Look at GPT Apps: Landscape and Vulnerability\nZejun Zhang Li Zhang Xin Yuan Anlan Zhang Mengwei Xu Feng Qian\nabstract\rabstract: With the advancement of Large Language Models (LLMs), increasinglysophisticated and powerful GPTs are entering the market. Despite theirpopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs\u0026rsquo;susceptibility to attacks raises concerns over safety and plagiarism. Thus, inthis work, we conduct a pioneering exploration of GPT stores, aiming to studyvulnerabilities and plagiarism within GPT applications. To begin with, weconduct, to our knowledge, the first large-scale monitoring and analysis of twostores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, wepropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.To complete these two tasks efficiently, we develop two automated tools: onefor web scraping and another designed for programmatically interacting withGPTs. Our findings reveal a significant enthusiasm among users and developersfor GPT interaction and creation, as evidenced by the rapid increase in GPTsand their creators. However, we also uncover a widespread failure to protectGPT internals, with nearly 90% of system prompts easily accessible, leading toconsiderable plagiarism and duplication among GPTs.\r2024-02-22\nExploring Memorization in Fine-tuned Language Models\nShenglai Zeng Yaxin Li Jie Ren Yiding Liu Han Xu Pengfei He Yue Xing Shuaiqiang Wang Jiliang Tang Dawei Yin\nabstract\rabstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models\u0026rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.\rSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\nHuachuan Qiu Hongliang He Shuai Zhang Anqi Li Zhenzhong Lan\nabstract\rabstract: Developing specialized dialogue systems for mental health support requiresmulti-turn conversation data, which has recently garnered increasing attention.However, gathering and releasing large-scale and real-life multi-turnconversations to facilitate advancements in mental health presents challengesdue to data privacy protection, as well as the time and cost involved. Toaddress the challenges related to data scarcity, we introduce SMILE, asingle-turn to multi-turn inclusive language expansion technique that promptsChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our workbegins with the analysis of language transformation, validating the feasibilityof the proposed method when compared with other baseline methods. We thenconduct a study on dialogue diversity, including lexical features, semanticfeatures, and dialogue topics, demonstrating the effectiveness of our proposedmethod. Furthermore, we implement an expert evaluation and the resultsdemonstrate that the dialogues generated with our proposed method are of higherquality than those generated with other baseline methods. Thus, we employ ourmethod to generate a large-scale, diverse, and high-quality dialogue datasetnamed SmileChat, comprising 55,165 dialogues in total with an average of 10.4turns per dialogue. Finally, we utilize the collected corpus to develop amental health chatbot, MeChat. To better assess the overall quality ofSmileChat, we collect a real-life chat dataset comprising 82 counselingdialogues for model evaluation. Both automatic and human evaluationsdemonstrate that our trained dialogue system exhibits significant improvements,showcasing that SmileChat is high-quality and practical.\r2024-02-21\nTree of Attacks: Jailbreaking Black-Box LLMs Automatically\nAnay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum Anderson Yaron Singer Amin Karbasi\nabstract\rabstract: While Large Language Models (LLMs) display versatile functionality, theycontinue to generate harmful, biased, and toxic content, as demonstrated by theprevalence of human-designed jailbreaks. In this work, we present Tree ofAttacks with Pruning (TAP), an automated method for generating jailbreaks thatonly requires black-box access to the target LLM. TAP utilizes an LLM toiteratively refine candidate (attack) prompts using tree-of-thought reasoninguntil one of the generated prompts jailbreaks the target. Crucially, beforesending prompts to the target, TAP assesses them and prunes the ones unlikelyto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigatea large search space of prompts and pruning reduces the total number of queriessent to the target. In empirical evaluations, we observe that TAP generatesprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)for more than 80% of the prompts using only a small number of queries.Interestingly, TAP is also capable of jailbreaking LLMs protected bystate-of-the-art guardrails, e.g., LlamaGuard. This significantly improves uponthe previous state-of-the-art black-box method for generating jailbreaks.\rLarge Language Models are Advanced Anonymizers\nRobin Staab Mark Vero Mislav Balunović Martin Vechev\nabstract\rabstract: Recent work in privacy research on large language models has shown that theyachieve near human-level performance at inferring personal data from real-worldonline texts. With consistently increasing model capabilities, existing textanonymization methods are currently lacking behind regulatory requirements andadversarial threats. This raises the question of how individuals caneffectively protect their personal data in sharing online texts. In this work,we take two steps to answer this question: We first present a new setting forevaluating anonymizations in the face of adversarial LLMs inferences, allowingfor a natural measurement of anonymization performance while remedying some ofthe shortcomings of previous metrics. We then present our LLM-based adversarialanonymization framework leveraging the strong inferential capabilities of LLMsto inform our anonymization procedure. In our experimental evaluation, we showon real-world and synthetic online texts how adversarial anonymizationoutperforms current industry-grade anonymizers both in terms of the resultingutility and privacy.\rPrivacy-Preserving Instructions for Aligning Large Language Models\nDa Yu Peter Kairouz Sewoong Oh Zheng Xu\nabstract\rabstract: Service providers of large language model (LLM) applications collect userinstructions in the wild and use them in further aligning LLMs with users\u0026rsquo;intentions. These instructions, which potentially contain sensitiveinformation, are annotated by human workers in the process. This poses a newprivacy risk not addressed by the typical private optimization. To this end, wepropose using synthetic instructions to replace real instructions in dataannotation and model fine-tuning. Formal differential privacy is guaranteed bygenerating those synthetic instructions using privately fine-tuned generators.Crucial in achieving the desired utility is our novel filtering algorithm thatmatches the distribution of the synthetic instructions to that of the realones. In both supervised fine-tuning and reinforcement learning from humanfeedback, our extensive experiments demonstrate the high utility of the finalset of synthetic instructions by showing comparable results to realinstructions. In supervised fine-tuning, models trained with private syntheticinstructions outperform leading open-source models such as Vicuna.\rDifferentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning\nZ Liu J Lou W Bao Z Qin K Ren\nabstract\rabstract: Finetuning on task-specific datasets is a widely-embraced paradigm ofharnessing the powerful capability of pretrained LLMs for various downstreamtasks. Due to the popularity of LLMs finetuning and its accompanying privacyconcerns, differentially private (DP) finetuning of pretrained LLMs hasgarnered increasing attention to safeguarding the privacy of task-specificdatasets. Lying at the design core of DP LLM finetuning methods is thesatisfactory tradeoff between privacy, utility, and scalability. Most existingmethods build upon the seminal work of DP-SGD. Despite pushing the scalabilityof DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunatelylimited by the inherent inefficiency of SGD. In this paper, we investigate thepotential of DP zeroth-order methods for LLM pretraining, which avoids thescalability bottleneck of SGD by approximating the gradient with the moreefficient zeroth-order gradient. Rather than treating the zeroth-order methodas a drop-in replacement for SGD, this paper presents a comprehensive studyboth theoretically and empirically. First, we propose the stagewise DPzeroth-order method that dynamically schedules key hyperparameters. This designis grounded on the synergy between DP random perturbation and the gradientapproximation error of the zeroth-order method, and its effect on finetuningtrajectory. Second, we further enhance the scalability by reducing thetrainable parameters that are identified by repurposing a data-free pruningtechnique requiring no additional data or extra privacy budget. We providetheoretical analysis for both proposed methods. We conduct extensive empiricalanalysis on both encoder-only masked language model and decoder-onlyautoregressive language model, achieving impressive results in terms ofscalability and utility.\r2024-02-20\nA Strategic Model of Software Dependency Networks\nCornelius Fritz Co-Pierre Georg Angelo Mele Michael Schweinberger\nabstract\rabstract: Modern software development involves collaborative efforts and reuse ofexisting code, which reduces the cost of developing new software. However,reusing code from existing packages exposes coders to vulnerabilities in thesedependencies. We study the formation of dependency networks among softwarepackages and libraries, guided by a structural model of network formation withobservable and unobservable heterogeneity. We estimate costs, benefits, andlink externalities of the network of 696,790 directed dependencies between35,473 repositories of the Rust programming language using a novel scalablealgorithm. We find evidence of a positive externality exerted on other coderswhen coders create dependencies. Furthermore, we show that coders are likely tolink to more popular packages of the same software type but less popularpackages of other types. We adopt models for the spread of infectious diseasesto measure a package\u0026rsquo;s systemicness as the number of downstream packages avulnerability would affect. Systemicness is highly skewed with the mostsystemic repository affecting almost 90% of all repositories only two stepsaway. Lastly, we show that protecting only the ten most important repositoriesreduces vulnerability contagion by nearly 40%.\rPrivacy Issues in Large Language Models: A Survey\nSeth Neel Peter Chang\nabstract\rabstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.\rOn the Convergence of Zeroth-Order Federated Tuning for Large Language Models\nZhenqing Ling Daoyuan Chen Liuyi Yao Yaliang Li Ying Shen\nabstract\rabstract: The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area.\rTRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification\nMartin Gubri Dennis Ulmer Hwaran Lee Sangdoo Yun Seong Joon Oh\nabstract\rabstract: Large Language Model (LLM) services and models often come with legal rules onwho can use them and how they must use them. Assessing the compliance of thereleased LLMs is crucial, as these rules protect the interests of the LLMcontributor and prevent misuse. In this context, we describe the novel problemof Black-box Identity Verification (BBIV). The goal is to determine whether athird-party application uses a certain LLM through its chat function. Wepropose a method called Targeted Random Adversarial Prompt (TRAP) thatidentifies the specific LLM in use. We repurpose adversarial suffixes,originally proposed for jailbreaking, to get a pre-defined answer from thetarget LLM, while other models give random answers. TRAP detects the targetLLMs with over 95% true positive rate at under 0.2% false positive rate evenafter a single interaction. TRAP remains effective even if the LLM has minorchanges that do not significantly alter the original function.\rDoes Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?\nBo-Ru Lu Nikita Haduong Chia-Hsuan Lee Zeqiu Wu Hao Cheng Paul Koester Jean Utke Tao Yu Noah A. Smith Mari Ostendorf\nabstract\rabstract: The capabilities of pretrained language models have opened opportunities toexplore new application areas, but applications involving human-humaninteraction are limited by the fact that most data is protected from publicrelease for privacy reasons. Problem-solving human dialogues in realapplications can be much more complex than existing Wizard-of-Oz collections,preventing successful domain transfer. To support information extraction (IE)for a private call center dataset, we introduce a human-in-the-loop dialoguegeneration framework capable of synthesizing realistic dialogues. In IEexperiments with auto insurance call center dialogues, we observe 25% relativeimprovement in $F_1$ after augmenting a small set of real human conversationswith synthetic data. We release code and our synthetic dataset to illustratethe complexity of real-world call center conversations and encouragedevelopment of complex dialogue datasets that are more representative ofnatural data.\rRTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution\nShang Liu Wenji Fang Yao Lu Qijun Zhang Hongce Zhang Zhiyao Xie\nabstract\rabstract: The automatic generation of RTL code (e.g., Verilog) using natural languageinstructions and large language models (LLMs) has attracted significantresearch interest recently. However, most existing approaches heavily rely oncommercial LLMs such as ChatGPT, while open-source LLMs tailored for thisspecific design generation task exhibit notably inferior performance. Theabsence of high-quality open-source solutions restricts the flexibility anddata privacy of this emerging technique. In this study, we present a newcustomized LLM solution with a modest parameter count of only 7B, achievingbetter performance than GPT-3.5 on two representative benchmarks for RTL codegeneration. This remarkable balance between accuracy and efficiency is madepossible by leveraging our new RTL code dataset and a customized LLM algorithm,both of which will be made fully open-source. Furthermore, we have successfullyquantized our LLM to 4-bit with a total size of 4GB, enabling it to function ona single laptop with only slight performance degradation. This efficiencyallows the RTL generator to serve as a local assistant for engineers, ensuringall design privacy concerns are addressed.\rReducing Privacy Risks in Online Self-Disclosures with Language Models\nYao Dou Isadora Krsek Tarek Naous Anubha Kabra Sauvik Das Alan Ritter Wei Xu\nabstract\rabstract: Self-disclosure, while being common and rewarding in social mediainteraction, also poses privacy risks. In this paper, we take the initiative toprotect the user-side privacy associated with online self-disclosure throughdetection and abstraction. We develop a taxonomy of 19 self-disclosurecategories and curate a large corpus consisting of 4.8K annotated disclosurespans. We then fine-tune a language model for detection, achieving over 65%partial span F$_1$. We further conduct an HCI user study, with 82% ofparticipants viewing the model positively, highlighting its real-worldapplicability. Motivated by the user feedback, we introduce the task ofself-disclosure abstraction, which is paraphrasing disclosures into lessspecific terms while preserving their utility, e.g., \u0026ldquo;Im 16F\u0026rdquo; to \u0026ldquo;I\u0026rsquo;m a teenagegirl\u0026rdquo;. We explore various fine-tuning strategies, and our best model cangenerate diverse abstractions that moderately reduce privacy risks whilemaintaining high utility according to human evaluation. To help users indeciding which disclosures to abstract, we present a task of rating theirimportance for context understanding. Our fine-tuned model achieves 80%accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we willonly release our corpus to researchers who agree to ethical guidelines.\r2024-02-19\nGenerative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity\nClaudio Novelli Federico Casolari Philipp Hacker Giorgio Spedicato Luciano Floridi\nabstract\rabstract: The advent of Generative AI, particularly through Large Language Models(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AIlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,thereby broadening their application scope. However, the complexity andemergent autonomy of these models introduce challenges in predictability andlegal compliance. This paper delves into the legal and regulatory implicationsof Generative AI and LLMs in the European Union context, analyzing aspects ofliability, privacy, intellectual property, and cybersecurity. It criticallyexamines the adequacy of the existing and proposed EU legislation, includingthe Artificial Intelligence Act (AIA) draft, in addressing the uniquechallenges posed by Generative AI in general and LLMs in particular. The paperidentifies potential gaps and shortcomings in the legislative framework andproposes recommendations to ensure the safe and compliant deployment ofgenerative models, ensuring they align with the EU\u0026rsquo;s evolving digital landscapeand legal standards.\rIs Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports\nFelix J. Dorfner Liv Jürgensen Leonhard Donle Fares Al Mohamad Tobias R. Bodenmann Mason C. Cleveland Felix Busch Lisa C. Adams James Sato Thomas Schultz Albert E. Kim Jameson Merkow Keno K. Bressem Christopher P. Bridge\nabstract\rabstract: Introduction: With the rapid advances in large language models (LLMs), therehave been numerous new open source as well as commercial models. While recentpublications have explored GPT-4 in its application to extracting informationof interest from radiology reports, there has not been a real-world comparisonof GPT-4 to different leading open-source models. Materials and Methods: Two different and independent datasets were used. Thefirst dataset consists of 540 chest x-ray reports that were created at theMassachusetts General Hospital between July 2019 and July 2021. The seconddataset consists of 500 chest x-ray reports from the ImaGenome dataset. We thencompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to theopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accuratelylabel the presence of multiple findings in x-ray text reports using differentprompting techniques. Results: On the ImaGenome dataset, the best performing open-source model wasLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shotprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,respectively. On the institutional dataset, the best performing open-sourcemodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- andfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and0.973, respectively. Conclusion: In this paper, we show that while GPT-4 is superior toopen-source models in zero-shot report labeling, the implementation of few-shotprompting can bring open-source models on par with GPT-4. This shows thatopen-source models could be a performant and privacy preserving alternative toGPT-4 for the task of radiology report classification.\rCan AI-Generated Text be Reliably Detected?\nVinu Sankar Sadasivan Aounon Kumar Sriram Balasubramanian Wenxiao Wang Soheil Feizi\nabstract\rabstract: The unregulated use of LLMs can potentially lead to malicious consequencessuch as plagiarism, generating fake news, spamming, etc. Therefore, reliabledetection of AI-generated text can be critical to ensure the responsible use ofLLMs. Recent works attempt to tackle this problem either using certain modelsignatures present in the generated text outputs or by applying watermarkingtechniques that imprint specific patterns onto them. In this paper, we showthat these detectors are not reliable in practical scenarios. In particular, wedevelop a recursive paraphrasing attack to apply on AI text, which can break awhole range of detectors, including the ones using the watermarking schemes aswell as neural network-based detectors, zero-shot classifiers, andretrieval-based detectors. Our experiments include passages around 300 tokensin length, showing the sensitivity of the detectors even in the case ofrelatively long passages. We also observe that our recursive paraphrasing onlydegrades text quality slightly, measured via human studies, and metrics such asperplexity scores and accuracy on text benchmarks. Additionally, we show thateven LLMs protected by watermarking schemes can be vulnerable against spoofingattacks aimed to mislead detectors to classify human-written text asAI-generated, potentially causing reputational damages to the developers. Inparticular, we show that an adversary can infer hidden AI text signatures ofthe LLM outputs without having white-box access to the detection method.Finally, we provide a theoretical connection between the AUROC of the bestpossible detector and the Total Variation distance between human and AI textdistributions that can be used to study the fundamental hardness of thereliable detection problem for advanced language models. Our code is publiclyavailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.\rCopyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications\nXinwei Guo Yujun Li Yafeng Peng Xuetao Wei\nabstract\rabstract: As AIGC has impacted our society profoundly in the past years, ethical issueshave received tremendous attention. The most urgent one is the AIGC copyrightdilemma, which can immensely stifle the development of AIGC and greatly costthe entire society. Given the complexity of AIGC copyright governance and thefact that no perfect solution currently exists, previous work advocatedcopyleft on AI governance but without substantive analysis. In this paper, wetake a step further to explore the feasibility of copyleft to alleviate theAIGC copyright dilemma. We conduct a mixed-methods study from two aspects:qualitatively, we use a formal what-if analysis to clarify the dilemma andprovide case studies to show the feasibility of copyleft; quantitatively, weperform a carefully designed survey to find out how the public feels aboutcopylefting AIGC. The key findings include: a) people generally perceive thedilemma, b) they prefer to use authorized AIGC under loose restriction, and c)they are positive to copyleft in AIGC and willing to use it in the future.\rPurifying Large Language Models by Ensembling a Small Language Model\nTianlin Li Qian Liu Tianyu Pang Chao Du Qing Guo Yang Liu Min Lin\nabstract\rabstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.\rKnowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion\nJinheon Baek Nirupama Chandrasekaran Silviu Cucerzan Allen herring Sujay Kumar Jauhar\nabstract\rabstract: Large Language Models (LLMs) excel at tackling various natural languagetasks. However, due to the significant costs involved in re-training orfine-tuning them, they remain largely static and difficult to personalize.Nevertheless, a variety of applications could benefit from generations that aretailored to users\u0026rsquo; preferences, goals, and knowledge. Among them is web search,where knowing what a user is trying to accomplish, what they care about, andwhat they know can lead to improved search experiences. In this work, wepropose a novel and general approach that augments an LLM with relevant contextfrom users\u0026rsquo; interaction histories with a search engine in order to personalizeits outputs. Specifically, we construct an entity-centric knowledge store foreach user based on their search and browsing activities on the web, which isthen leveraged to provide contextually relevant LLM prompt augmentations. Thisknowledge store is light-weight, since it only produces user-specific aggregateprojections of interests and knowledge onto public knowledge graphs, andleverages existing search log infrastructure, thereby mitigating the privacy,compliance, and scalability concerns associated with building deep userprofiles for personalization. We validate our approach on the task ofcontextual query suggestion, which requires understanding not only the user\u0026rsquo;scurrent search context but also what they historically know and care about.Through a number of experiments based on human evaluation, we show that ourapproach is significantly better than several other LLM-powered baselines,generating query suggestions that are contextually more relevant, personalized,and useful.\rClass-incremental Learning for Time Series: Benchmark and Evaluation\nZhongzheng Qiao Quang Pham Zhen Cao Hoang H Le P. N. Suganthan Xudong Jiang Ramasamy Savitha\nabstract\rabstract: Real-world environments are inherently non-stationary, frequently introducingnew classes over time. This is especially common in time series classification,such as the emergence of new disease classification in healthcare or theaddition of new activities in human activity recognition. In such cases, alearning system is required to assimilate novel classes effectively whileavoiding catastrophic forgetting of the old ones, which gives rise to theClass-incremental Learning (CIL) problem. However, despite the encouragingprogress in the image and language domains, CIL for time series data remainsrelatively understudied. Existing studies suffer from inconsistent experimentaldesigns, necessitating a comprehensive evaluation and benchmarking of methodsacross a wide range of datasets. To this end, we first present an overview ofthe Time Series Class-incremental Learning (TSCIL) problem, highlight itsunique challenges, and cover the advanced methodologies. Further, based onstandardized settings, we develop a unified experimental framework thatsupports the rapid development of new algorithms, easy integration of newdatasets, and standardization of the evaluation process. Using this framework,we conduct a comprehensive evaluation of various generic andtime-series-specific CIL methods in both standard and privacy-sensitivescenarios. Our extensive experiments not only provide a standard baseline tosupport future research but also shed light on the impact of various designfactors such as normalization layers or memory budget thresholds. Codes areavailable at https://github.com/zqiao11/TSCIL.\rDistilling Large Language Models for Text-Attributed Graph Learning\nBo Pan Zheng Zhang Yifei Zhang Yuntong Hu Liang Zhao\nabstract\rabstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents.Graph models can efficiently learn TAGs, but their training heavily relies onhuman-annotated labels, which are scarce or even unavailable in manyapplications. Large language models (LLMs) have recently demonstratedremarkable capabilities in few-shot and zero-shot TAG learning, but they sufferfrom scalability, cost, and privacy issues. Therefore, in this work, we focuson synergizing LLMs and graph models with their complementary strengths bydistilling the power of LLMs to a local graph model on TAG learning. To addressthe inherent gaps between LLMs (generative models for texts) and graph models(discriminative models for graphs), we propose first to let LLMs teach aninterpreter with rich textual rationale and then let a student model mimic theinterpreter\u0026rsquo;s reasoning without LLMs\u0026rsquo; textual rationale. Extensive experimentsvalidate the efficacy of our proposed framework.\rEffective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters\nShahed Masoudian Cornelia Volaucnik Markus Schedl Navid Rekabsaz\nabstract\rabstract: Bias mitigation of Language Models has been the topic of many studies with arecent focus on learning separate modules like adapters for on-demanddebiasing. Besides optimizing for a modularized debiased model, it is oftencritical in practice to control the degree of bias reduction at inference time,e.g., in order to tune for a desired performance-fairness trade-off in searchresults or to control the strength of debiasing in classification tasks. Inthis paper, we introduce Controllable Gate Adapter (ConGater), a novel modulargating mechanism with adjustable sensitivity parameters, which allows for agradual transition from the biased state of the model to the fully debiasedversion at inference time. We demonstrate ConGater performance by (1)conducting adversarial debiasing experiments with three different models onthree classification tasks with four protected attributes, and (2) reducing thebias of search results through fairness list-wise regularization to enableadjusting a trade-off between performance and fairness metrics. Our experimentson the classification tasks show that compared to baselines of the samecaliber, ConGater can maintain higher task performance while containing lessinformation regarding the attributes. Our results on the retrieval task showthat the fully debiased ConGater can achieve the same fairness performancewhile maintaining more than twice as high task performance than recent strongbaselines. Overall, besides strong performance ConGater enables the continuoustransitioning between biased and debiased states of models, enhancingpersonalization of use and interpretability through controllability.\rNOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization\nImjin Ahn Hansle Gwon Young-Hak Kim Tae Joon Jun Sanghyun Park\nabstract\rabstract: The discharge summary is a one of critical documents in the patient journey,encompassing all events experienced during hospitalization, including multiplevisits, medications, tests, surgery/procedures, and admissions/discharge.Providing a summary of the patient\u0026rsquo;s progress is crucial, as it significantlyinfluences future care and planning. Consequently, clinicians face thelaborious and resource-intensive task of manually collecting, organizing, andcombining all the necessary data for a discharge summary. Therefore, we propose\u0026quot;NOTE\u0026quot;, which stands for \u0026ldquo;Notable generation Of patient Text summaries throughan Efficient approach based on direct preference optimization\u0026rdquo;. NOTE is basedon Medical Information Mart for Intensive Care- III dataset and summarizes asingle hospitalization of a patient. Patient events are sequentially combinedand used to generate a discharge summary for each hospitalization. In thepresent circumstances, large language models\u0026rsquo; application programminginterfaces (LLMs\u0026rsquo; APIs) are widely available, but importing and exportingmedical data presents significant challenges due to privacy protection policiesin healthcare institutions. Moreover, to ensure optimal performance, it isessential to implement a lightweight model for internal server or programwithin the hospital. Therefore, we utilized DPO and parameter efficient finetuning (PEFT) techniques to apply a fine-tuning method that guarantees superiorperformance. To demonstrate the practical application of the developed NOTE, weprovide a webpage-based demonstration software. In the future, we will aim todeploy the software available for actual use by clinicians in hospital. NOTEcan be utilized to generate various summaries not only discharge summaries butalso throughout a patient\u0026rsquo;s journey, thereby alleviating the labor-intensiveworkload of clinicians and aiming for increased efficiency.\rWhere It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages\nSameer Jain Sedrick Scott Keh Shova Chettri Karun Dewan Pablo Izquierdo Johanna Prussman Pooja Shreshtha Cesar Suarez Zheyuan Ryan Shi Lei Li Fei Fang\nabstract\rabstract: Environmental conservation organizations routinely monitor news content onconservation in protected areas to maintain situational awareness ofdevelopments that can have an environmental impact. Existing automated mediamonitoring systems require large amounts of data labeled by domain experts,which is only feasible at scale for high-resource languages like English.However, such tools are most needed in the global south where news of interestis mainly in local low-resource languages, and far fewer experts are availableto annotate datasets sustainably. In this paper, we propose NewsSerow, a methodto automatically recognize environmental conservation content in low-resourcelanguages. NewsSerow is a pipeline of summarization, in-context few-shotclassification, and self-reflection using large language models (LLMs). Usingat most 10 demonstration example news articles in Nepali, NewsSerowsignificantly outperforms other few-shot methods and achieves comparableperformance with models fully fine-tuned using thousands of examples. The WorldWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring inNepal, significantly reducing their operational burden, and ensuring that AItools for conservation actually reach the communities that need them the most.NewsSerow has also been deployed for countries with other languages likeColombia.\rHU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?\nShubhashis Roy Dipta Sadat Shahriar\nabstract\rabstract: This paper describes our system developed for SemEval-2024 Task 8,\u0026ldquo;Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated TextDetection.\u0026rdquo; Machine-generated texts have been one of the main concerns due tothe use of large language models (LLM) in fake text generation, phishing,cheating in exams, or even plagiarizing copyright materials. A lot of systemshave been developed to detect machine-generated text. Nonetheless, the majorityof these systems rely on the text-generating model, a limitation that isimpractical in real-world scenarios, as it\u0026rsquo;s often impossible to know whichspecific model the user has used for text generation. In this work, we proposea single model based on contrastive learning, which uses ~40% of the baseline\u0026rsquo;sparameters (149M vs. 355M) but shows a comparable performance on the testdataset (21st out of 137 participants). Our key finding is that even without anensemble of multiple models, a single base model can have comparableperformance with the help of data augmentation and contrastive learning.\rOn Copyright Risks of Text-to-Image Diffusion Models\nYang Zhang Teoh Tze Tzun Lim Wei Hern Haonan Wang Kenji Kawaguchi\nabstract\rabstract: Diffusion models excel in many generative modeling tasks, notably in creatingimages from text prompts, a task referred to as text-to-image (T2I) generation.Despite the ability to generate high-quality images, these models oftenreplicate elements from their training data, leading to increasing copyrightconcerns in real applications in recent years. In response to this raisingconcern about copyright infringement, recent studies have studied the copyrightbehavior of diffusion models when using direct, copyrighted prompts. Ourresearch extends this by examining subtler forms of infringement, where evenindirect prompts can trigger copyright issues. Specifically, we introduce adata generation pipeline to systematically produce data for studying copyrightin diffusion models. Our pipeline enables us to investigate copyrightinfringement in a more practical setting, involving replicating visual featuresrather than entire works using seemingly irrelevant prompts for T2I generation.We generate data using our proposed pipeline to test various diffusion models,including the latest Stable Diffusion XL. Our findings reveal a widespreadtendency that these models tend to produce copyright-infringing content,highlighting a significant challenge in this field.\r2024-02-18\nStealthy Attack on Large Language Model based Recommendation\nJinghao Zhang Yuting Liu Qiang Liu Shu Wu Guibing Guo Liang Wang\nabstract\rabstract: Recently, the powerful large language models (LLMs) have been instrumental inpropelling the progress of recommender systems (RS). However, while thesesystems have flourished, their susceptibility to security threats has beenlargely overlooked. In this work, we reveal that the introduction of LLMs intorecommendation models presents new security vulnerabilities due to theiremphasis on the textual content of items. We demonstrate that attackers cansignificantly boost an item\u0026rsquo;s exposure by merely altering its textual contentduring the testing phase, without requiring direct interference with themodel\u0026rsquo;s training process. Additionally, the attack is notably stealthy, as itdoes not affect the overall recommendation performance and the modifications tothe text are subtle, making it difficult for users and platforms to detect. Ourcomprehensive experiments across four mainstream LLM-based recommendationmodels demonstrate the superior efficacy and stealthiness of our approach. Ourwork unveils a significant security gap in LLM-based recommendation systems andpaves the way for future research on protecting these systems.\rFederated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources\nJiamu Bai Daoyuan Chen Bingchen Qian Liuyi Yao Yaliang Li\nabstract\rabstract: Federated Learning (FL) has recently been applied to the parameter-efficientfine-tuning of Large Language Models (LLMs). While promising, it raisessignificant challenges due to the heterogeneous resources and datadistributions of clients.This study introduces FlexLoRA, a simple yet effectiveaggregation scheme for LLM fine-tuning, which mitigates the \u0026ldquo;buckets effect\u0026rdquo; intraditional FL that restricts the potential of clients with ample resources bytying them to the capabilities of the least-resourced participants. FlexLoRAallows for dynamic adjustment of local LoRA ranks, fostering the development ofa global model imbued with broader, less task-specific knowledge. Bysynthesizing a full-size LoRA weight from individual client contributions andemploying Singular Value Decomposition (SVD) for weight redistribution,FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600clients performing diverse NLP tasks, our experiments validate the efficacy ofFlexLoRA, with the federated global model achieving up to a 3.1% averageimprovement in downstream NLP task performance. FlexLoRA\u0026rsquo;s practicality isfurther underscored by its seamless integration with existing LoRA-based FLmethods and theoretical analysis, offering a path toward scalable,privacy-preserving federated tuning for LLMs.\r2024-02-17\nUnderstanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention\nEunkyung Jo Yuin Jeong SoHyun Park Daniel A. Epstein Young-Ho Kim\nabstract\rabstract: Recent large language models (LLMs) offer the potential to support publichealth monitoring by facilitating health disclosure through open-endedconversations but rarely preserve the knowledge gained about individuals acrossrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents anopportunity to improve engagement and self-disclosure, but we lack anunderstanding of how LTM impacts people\u0026rsquo;s interaction with LLM-driven chatbotsin public health interventions. We examine the case of CareCall \u0026ndash; anLLM-driven voice chatbot with LTM \u0026ndash; through the analysis of 1,252 call logsand interviews with nine users. We found that LTM enhanced health disclosureand fostered positive perceptions of the chatbot by offering familiarity.However, we also observed challenges in promoting self-disclosure through LTM,particularly around addressing chronic health conditions and privacy concerns.We discuss considerations for LTM integration in LLM-driven chatbots for publichealth monitoring, including carefully deciding what topics need to beremembered in light of public health goals.\rAnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection\nQihang Zhou Guansong Pang Yu Tian Shibo He Jiming Chen\nabstract\rabstract: Zero-shot anomaly detection (ZSAD) requires detection models trained usingauxiliary data to detect anomalies without any training sample in a targetdataset. It is a crucial task when training data is not accessible due tovarious concerns, eg, data privacy, yet it is challenging since the models needto generalize to anomalies across different domains where the appearance offoreground objects, abnormal regions, and background features, such asdefects/tumors on different products/organs, can vary significantly. Recentlylarge pre-trained vision-language models (VLMs), such as CLIP, havedemonstrated strong zero-shot recognition ability in various vision tasks,including anomaly detection. However, their ZSAD performance is weak since theVLMs focus more on modeling the class semantics of the foreground objectsrather than the abnormality/normality in the images. In this paper we introducea novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD acrossdifferent domains. The key insight of AnomalyCLIP is to learn object-agnostictext prompts that capture generic normality and abnormality in an imageregardless of its foreground objects. This allows our model to focus on theabnormal image regions rather than the object semantics, enabling generalizednormality and abnormality recognition on diverse types of objects. Large-scaleexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIPachieves superior zero-shot performance of detecting and segmenting anomaliesin datasets of highly diverse class semantics from various defect inspectionand medical imaging domains. Code will be made available athttps://github.com/zqhang/AnomalyCLIP.\rKnowledge Editing on Black-box Large Language Models\nXiaoshuai Song Zhengyang Wang Keqing He Guanting Dong Yutao Mou Jinxu Zhao Weiran Xu\nabstract\rabstract: Knowledge editing (KE) aims to efficiently and precisely modify the behaviorof large language models (LLMs) to update specific knowledge without negativelyinfluencing other knowledge. Current research primarily focuses on white-boxLLMs editing, overlooking an important scenario: black-box LLMs editing, whereLLMs are accessed through interfaces and only textual output is available. Inthis paper, we first officially introduce KE on black-box LLMs and then proposea comprehensive evaluation framework to overcome the limitations of existingevaluations that are not applicable to black-box LLMs editing and lackcomprehensiveness. To tackle privacy leaks of editing data and styleover-editing in current methods, we introduce a novel postEdit framework,resolving privacy concerns through downstream post-processing and maintainingtextual style consistency via fine-grained editing to original responses.Experiments and analysis on two benchmarks demonstrate that postEditoutperforms all baselines and achieves strong generalization, especially withhuge improvements on style retention (average $+20.82%\\uparrow$).\rA Platform for the Biomedical Application of Large Language Models\nSebastian Lobentanzer Shaohong Feng The BioChatter Consortium Andreas Maier Cankun Wang Jan Baumbach Nils Krehl Qin Ma Julio Saez-Rodriguez\nabstract\rabstract: Current-generation Large Language Models (LLMs) have stirred enormousinterest in recent months, yielding great potential for accessibility andautomation, while simultaneously posing significant challenges and risk ofmisuse. To facilitate interfacing with LLMs in the biomedical space, while atthe same time safeguarding their functionalities through sensible constraints,we propose a dedicated, open-source framework: BioChatter. Based on open-sourcesoftware packages, we synergise the many functionalities that are currentlydeveloping around LLMs, such as knowledge integration / retrieval-augmentedgeneration, model chaining, and benchmarking, resulting in an easy-to-use andinclusive framework for application in many use cases of biomedicine. We focuson robust and user-friendly implementation, including ways to deployprivacy-preserving local open-source LLMs. We demonstrate use cases via twomulti-purpose web apps (https://chat.biocypher.org), and provide documentation,support, and an open community.\rLLM-based Federated Recommendation\nJujia Zhao Wenjie Wang Chen Xu Zhaochun Ren See-Kiong Ng Tat-Seng Chua\nabstract\rabstract: Large Language Models (LLMs), with their advanced contextual understandingabilities, have demonstrated considerable potential in enhancing recommendationsystems via fine-tuning methods. However, fine-tuning requires users\u0026rsquo; behaviordata, which poses considerable privacy risks due to the incorporation ofsensitive user information. The unintended disclosure of such data couldinfringe upon data protection laws and give rise to ethical issues. To mitigatethese privacy issues, Federated Learning for Recommendation (Fed4Rec) hasemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-basedrecommendation presents two main challenges: first, an increase in theimbalance of performance across clients, affecting the system\u0026rsquo;s efficiency overtime, and second, a high demand on clients\u0026rsquo; computational and storage resourcesfor local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-basedRecommendation (PPLR) framework. The PPLR framework employs two primarystrategies. First, it implements a dynamic balance strategy, which involves thedesign of dynamic parameter aggregation and adjustment of learning speed fordifferent clients during the training phase, to ensure relatively balancedperformance across all clients. Second, PPLR adopts a flexible storagestrategy, selectively retaining certain sensitive layers of the language modelon the client side while offloading non-sensitive layers to the server. Thisapproach aims to preserve user privacy while efficiently saving computationaland storage resources. Experimental results demonstrate that PPLR not onlyachieves a balanced performance among clients but also enhances overall systemperformance in a manner that is both computationally and storage-efficient,while effectively protecting user privacy.\r2024-02-16\nLarge Language Model Unlearning\nYuanshun Yao Xiaojun Xu Yang Liu\nabstract\rabstract: We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,on large language models (LLMs). We show at least three scenarios of aligningLLMs with human preferences can benefit from unlearning: (1) removing harmfulresponses, (2) erasing copyright-protected content as requested, and (3)reducing hallucinations. Unlearning, as an alignment technique, has threeadvantages. (1) It only requires negative (e.g. harmful) examples, which aremuch easier and cheaper to collect (e.g. via red teaming or user reporting)than positive (e.g. helpful and often human-written) examples required in RLHF(RL from human feedback). (2) It is computationally efficient. (3) It isespecially effective when we know which training samples cause the misbehavior.To the best of our knowledge, our work is among the first to explore LLMunlearning. We are also among the first to formulate the settings, goals, andevaluations in LLM unlearning. We show that if practitioners only have limitedresources, and therefore the priority is to stop generating undesirable outputsrather than to try to generate desirable outputs, unlearning is particularlyappealing. Despite only having negative samples, our ablation study shows thatunlearning can still achieve better alignment performance than RLHF with just2% of its computational time.\rWhen Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\nMinrui Xu Dusit Niyato Jiawen Kang Zehui Xiong Shiwen Mao Zhu Han Dong In Kim Khaled B. Letaief\nabstract\rabstract: AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.\rCausal ATE Mitigates Unintended Bias in Controlled Text Generation\nRahul Madhavan Kahini Wadhawan\nabstract\rabstract: We study attribute control in language models through the method of CausalAverage Treatment Effect (Causal ATE). Existing methods for the attributecontrol task in Language Models (LMs) check for the co-occurrence of words in asentence with the attribute of interest, and control for them. However,spurious correlation of the words with the attribute in the training dataset,can cause models to hallucinate the presence of the attribute when presentedwith the spurious correlate during inference. We show that the simpleperturbation-based method of Causal ATE removes this unintended effect.Specifically, we ground it in the problem of toxicity mitigation, where asignificant challenge lies in the inadvertent bias that often emerges towardsprotected groups post detoxification. We show that this unintended bias can besolved by the use of the Causal ATE metric and rigorously prove our claim. Weprovide experimental validations for our claims and release our code(anonymously) here:https://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias.\rUser Experience Design Professionals\u0026rsquo; Perceptions of Generative Artificial Intelligence\nJie Li Hancheng Cao Laura Lin Youyang Hou Ruihao Zhu Abdallah El Ali\nabstract\rabstract: Among creative professionals, Generative Artificial Intelligence (GenAI) hassparked excitement over its capabilities and fear over unanticipatedconsequences. How does GenAI impact User Experience Design (UXD) practice, andare fears warranted? We interviewed 20 UX Designers, with diverse experienceand across companies (startups to large enterprises). We probed them tocharacterize their practices, and sample their attitudes, concerns, andexpectations. We found that experienced designers are confident in theiroriginality, creativity, and empathic skills, and find GenAI\u0026rsquo;s role asassistive. They emphasized the unique human factors of \u0026ldquo;enjoyment\u0026rdquo; and\u0026quot;agency\u0026quot;, where humans remain the arbiters of \u0026ldquo;AI alignment\u0026rdquo;. However, skilldegradation, job replacement, and creativity exhaustion can adversely impactjunior designers. We discuss implications for human-GenAI collaboration,specifically copyright and ownership, human creativity and agency, and AIliteracy and access. Through the lens of responsible and participatory AI, wecontribute a deeper understanding of GenAI fears and opportunities for UXD.\r2024-02-15\nRethinking Machine Unlearning for Large Language Models\nSijia Liu Yuanshun Yao Jinghan Jia Stephen Casper Nathalie Baracaldo Peter Hase Xiaojun Xu Yuguang Yao Hang Li Kush R. Varshney Mohit Bansal Sanmi Koyejo Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\rUnmemorization in Large Language Models via Self-Distillation and Deliberate Imagination\nYijiang River Dong Hongzhou Lin Mikhail Belkin Ramon Huerta Ivan Vulić\nabstract\rabstract: While displaying impressive generation capabilities across many tasks, LargeLanguage Models (LLMs) still struggle with crucial issues of privacy violationand unwanted exposure of sensitive data. This raises an essential question: howshould we prevent such undesired behavior of LLMs while maintaining theirstrong generation and natural language understanding (NLU) capabilities? Inthis work, we introduce a novel approach termed deliberate imagination in thecontext of LLM unlearning. Instead of trying to forget memorized data, weemploy a self-distillation framework, guiding LLMs to deliberately imaginealternative scenarios. As demonstrated in a wide range of experiments, theproposed method not only effectively unlearns targeted text but also preservesthe LLMs\u0026rsquo; capabilities in open-ended generation tasks as well as in NLU tasks.Our results demonstrate the usefulness of this approach across different modelsand sizes, and also with parameter-efficient fine-tuning, offering a novelpathway to addressing the challenges with private and sensitive data in LLMapplications.\rExploring the Adversarial Capabilities of Large Language Models\nLukas Struppek Minh Hieu Le Dominik Hintersdorf Kristian Kersting\nabstract\rabstract: The proliferation of large language models (LLMs) has sparked widespread andgeneral interest due to their strong language generation capabilities, offeringgreat potential for both industry and research. While previous research delvedinto the security and privacy issues of LLMs, the extent to which these modelscan exhibit adversarial behavior remains largely unexplored. Addressing thisgap, we investigate whether common publicly available LLMs have inherentcapabilities to perturb text samples to fool safety measures, so-calledadversarial examples resp.~attacks. More specifically, we investigate whetherLLMs are inherently able to craft adversarial examples out of benign samples tofool existing safe rails. Our experiments, which focus on hate speechdetection, reveal that LLMs succeed in finding adversarial perturbations,effectively undermining hate speech detection systems. Our findings carrysignificant implications for (semi-)autonomous systems relying on LLMs,highlighting potential challenges in their interaction with existing systemsand safety measures.\rAbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns\nAshfak Md Shibli Mir Mehedi A. Pritom Maanak Gupta\nabstract\rabstract: SMS phishing, also known as \u0026ldquo;smishing\u0026rdquo;, is a growing threat that tricks usersinto disclosing private information or clicking into URLs with maliciouscontent through fraudulent mobile text messages. In recent past, we have alsoobserved a rapid advancement of conversational generative AI chatbot services(e.g., OpenAI\u0026rsquo;s ChatGPT, Google\u0026rsquo;s BARD), which are powered by pre-trained largelanguage models (LLMs). These AI chatbots certainly have a lot of utilities butit is not systematically understood how they can play a role in creatingthreats and attacks. In this paper, we propose AbuseGPT method to show how theexisting generative AI-based chatbot services can be exploited by attackers inreal world to create smishing texts and eventually lead to craftier smishingcampaigns. To the best of our knowledge, there is no pre-existing work thatevidently shows the impacts of these generative text-based models on creatingSMS phishing. Thus, we believe this study is the first of its kind to shedlight on this emerging cybersecurity threat. We have found strong empiricalevidences to show that attackers can exploit ethical standards in the existinggenerative AI-based chatbot services by crafting prompt injection attacks tocreate newer smishing campaigns. We also discuss some future researchdirections and guidelines to protect the abuse of generative AI-based servicesand safeguard users from smishing attacks.\rDetecting Phishing Sites Using ChatGPT\nTakashi Koide Naoki Fukushi Hiroki Nakano Daiki Chiba\nabstract\rabstract: The emergence of Large Language Models (LLMs), including ChatGPT, is having asignificant impact on a wide range of fields. While LLMs have been extensivelyresearched for tasks such as code generation and text synthesis, theirapplication in detecting malicious web content, particularly phishing sites,has been largely unexplored. To combat the rising tide of cyber attacks due tothe misuse of LLMs, it is important to automate detection by leveraging theadvanced capabilities of LLMs. In this paper, we propose a novel system called ChatPhishDetector thatutilizes LLMs to detect phishing sites. Our system involves leveraging a webcrawler to gather information from websites, generating prompts for LLMs basedon the crawled data, and then retrieving the detection results from theresponses generated by the LLMs. The system enables us to detect multilingualphishing sites with high accuracy by identifying impersonated brands and socialengineering techniques in the context of the entire website, without the needto train machine learning models. To evaluate the performance of our system, weconducted experiments on our own dataset and compared it with baseline systemsand several LLMs. The experimental results using GPT-4V demonstratedoutstanding performance, with a precision of 98.7% and a recall of 99.6%,outperforming the detection results of other LLMs and existing systems. Thesefindings highlight the potential of LLMs for protecting users from onlinefraudulent activities and have important implications for enhancingcybersecurity measures.\r2024-02-14\nTowards Privacy-Aware Sign Language Translation at Scale\nPhillip Rust Bowen Shi Skyler Wang Necati Cihan Camgöz Jean Maillard\nabstract\rabstract: A major impediment to the advancement of sign language translation (SLT) isdata scarcity. Much of the sign language data currently available on the webcannot be used for training supervised models due to the lack of alignedcaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bearsprivacy risks due to the presence of biometric information, which theresponsible development of SLT technologies should account for. In this work,we propose a two-stage framework for privacy-aware SLT at scale that addressesboth of these issues. We introduce SSVP-SLT, which leverages self-supervisedvideo pretraining on anonymized and unannotated videos, followed by supervisedSLT finetuning on a curated parallel dataset. SSVP-SLT achievesstate-of-the-art finetuned and zero-shot gloss-free SLT performance on theHow2Sign dataset, outperforming the strongest respective baselines by over 3BLEU-4. Based on controlled experiments, we further discuss the advantages andlimitations of self-supervised pretraining and anonymization via facialobfuscation for SLT.\rCopyright Traps for Large Language Models\nMatthieu Meeus Igor Shilov Manuel Faysse Yves-Alexandre de Montjoye\nabstract\rabstract: Questions of fair use of copyright-protected content to train Large LanguageModels (LLMs) are being very actively debated. Document-level inference hasbeen proposed as a new task: inferring from black-box access to the trainedmodel whether a piece of content has been seen during training. SOTA methodshowever rely on naturally occurring memorization of (part of) the content.While very effective against models that memorize a lot, we hypothesize\u0026ndash;andlater confirm\u0026ndash;that they will not work against models that do not naturallymemorize, e.g. medium-size 1B models. We here propose to use copyright traps,the inclusion of fictitious entries in original content, to detect the use ofcopyrighted materials in LLMs with a focus on models where memorization doesnot naturally occur. We carefully design an experimental setup, randomlyinserting traps into original content (books) and train a 1.3B LLM. We firstvalidate that the use of content in our target model would be undetectableusing existing methods. We then show, contrary to intuition, that evenmedium-length trap sentences repeated a significant number of times (100) arenot detectable using existing methods. However, we show that longer sequencesrepeated a large number of times can be reliably detected (AUC=0.75) and usedas copyright traps. We further improve these results by studying how the numberof times a sequence is seen improves detectability, how sequences with higherperplexity tend to be memorized more, and how taking context into accountfurther improves detectability.\rTrained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code\nVahid Majdinasab Amin Nikanjam Foutse Khomh\nabstract\rabstract: Code auditing ensures that the developed code adheres to standards,regulations, and copyright protection by verifying that it does not containcode from protected sources. The recent advent of Large Language Models (LLMs)as coding assistants in the software development process poses new challengesfor code auditing. The dataset for training these models is mainly collectedfrom publicly available sources. This raises the issue of intellectual propertyinfringement as developers\u0026rsquo; codes are already included in the dataset.Therefore, auditing code developed using LLMs is challenging, as it isdifficult to reliably assert if an LLM used during development has been trainedon specific copyrighted codes, given that we do not have access to the trainingdatasets of these models. Given the non-disclosure of the training datasets,traditional approaches such as code clone detection are insufficient forasserting copyright infringement. To address this challenge, we propose a newapproach, TraWiC; a model-agnostic and interpretable method based on membershipinference for detecting code inclusion in an LLM\u0026rsquo;s training dataset. We extractsyntactic and semantic identifiers unique to each program to train a classifierfor detecting code inclusion. In our experiments, we observe that TraWiC iscapable of detecting 83.87% of codes that were used to train an LLM. Incomparison, the prevalent clone detection tool NiCad is only capable ofdetecting 47.64%. In addition to its remarkable performance, TraWiC has lowresource overhead in contrast to pair-wise clone detection that is conductedduring the auditing process of tools like CodeWhisperer reference tracker,across thousands of code snippets.\rChinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis\nWei Zhai Hongzhi Qi Qing Zhao Jianqiang Li Ziqi Wang Han Wang Bing Xiang Yang Guanghui Fu\nabstract\rabstract: In the current environment, psychological issues are prevalent andwidespread, with social media serving as a key outlet for individuals to sharetheir feelings. This results in the generation of vast quantities of datadaily, where negative emotions have the potential to precipitate crisissituations. There is a recognized need for models capable of efficientanalysis. While pre-trained language models have demonstrated theireffectiveness broadly, there\u0026rsquo;s a noticeable gap in pre-trained models tailoredfor specialized domains like psychology. To address this, we have collected ahuge dataset from Chinese social media platforms and enriched it with publiclyavailable datasets to create a comprehensive database encompassing 3.36 milliontext entries. To enhance the model\u0026rsquo;s applicability to psychological textanalysis, we integrated psychological lexicons into the pre-training maskingmechanism. Building on an existing Chinese language model, we performedadaptive training to develop a model specialized for the psychological domain.We assessed our model\u0026rsquo;s effectiveness across four public benchmarks, where itnot only surpassed the performance of standard pre-trained models but alsoshowed a inclination for making psychologically relevant predictions. Due toconcerns regarding data privacy, the dataset will not be made publiclyavailable. However, we have made the pre-trained models and codes publiclyaccessible to the community via:https://github.com/zwzzzQAQ/Chinese-MentalBERT.\rDPZero: Private Fine-Tuning of Language Models without Backpropagation\nLiang Zhang Bingcong Li Kiran Koshy Thekumparampil Sewoong Oh Niao He\nabstract\rabstract: The widespread practice of fine-tuning large language models (LLMs) ondomain-specific data faces two major challenges in memory and privacy. First,as the size of LLMs continues to grow, the memory demands of gradient-basedtraining methods via backpropagation become prohibitively high. Second, giventhe tendency of LLMs to memorize training data, it is important to protectpotentially sensitive information in the fine-tuning data from beingregurgitated. Zeroth-order methods, which rely solely on forward passes,substantially reduce memory consumption during training. However, directlycombining them with standard differentially private gradient descent suffersfrom growing model size. To bridge this gap, we introduce DPZero, a novelprivate zeroth-order algorithm with nearly dimension-independent rates. Thememory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa onsix downstream tasks.\rA Study of Fairness Concerns in AI-based Mobile App Reviews\nAli Rezaei Nasab Maedeh Dashti Mojtaba Shahin Mansooreh Zahedi Hourieh Khalajzadeh Chetan Arora Peng Liang\nabstract\rabstract: Fairness is one of the socio-technical concerns that must be addressed inAI-based systems. Unfair AI-based systems, particularly unfair AI-based mobileapps, can pose difficulties for a significant proportion of the globalpopulation. This paper aims to analyze fairness concerns in AI-based appreviews.We first manually constructed a ground-truth dataset, including astatistical sample of fairness and non-fairness reviews. Leveraging theground-truth dataset, we developed and evaluated a set of machine learning anddeep learning classifiers that distinguish fairness reviews from non-fairnessreviews. Our experiments show that our best-performing classifier can detectfairness reviews with a precision of 94%. We then applied the best-performingclassifier on approximately 9.5M reviews collected from 108 AI-based apps andidentified around 92K fairness reviews. Next, applying the K-means clusteringtechnique to the 92K fairness reviews, followed by manual analysis, led to theidentification of six distinct types of fairness concerns (e.g., \u0026lsquo;receivingdifferent quality of features and services in different platforms and devices\u0026rsquo;and \u0026rsquo;lack of transparency and fairness in dealing with user-generatedcontent\u0026rsquo;). Finally, the manual analysis of 2,248 app owners\u0026rsquo; responses to thefairness reviews identified six root causes (e.g., \u0026lsquo;copyright issues\u0026rsquo;) that appowners report to justify fairness concerns.\rInterpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding\nAlessandro Achille Greg Ver Steeg Tian Yu Liu Matthew Trager Carson Klingenberg Stefano Soatto\nabstract\rabstract: Quantifying the degree of similarity between images is a key copyright issuefor image-based machine learning. In legal doctrine however, determining thedegree of similarity between works requires subjective analysis, andfact-finders (judges and juries) can demonstrate considerable variability inthese subjective judgement calls. Images that are structurally similar can bedeemed dissimilar, whereas images of completely different scenes can be deemedsimilar enough to support a claim of copying. We seek to define and compute anotion of \u0026ldquo;conceptual similarity\u0026rdquo; among images that captures high-levelrelations even among images that do not share repeated elements or visuallysimilar components. The idea is to use a base multi-modal model to generate\u0026quot;explanations\u0026quot; (captions) of visual data at increasing levels of complexity.Then, similarity can be measured by the length of the caption needed todiscriminate between the two images: Two highly dissimilar images can bediscriminated early in their description, whereas conceptually dissimilar oneswill need more detail to be distinguished. We operationalize this definitionand show that it correlates with subjective (averaged human evaluation)assessment, and beats existing baselines on both image-to-image andtext-to-text similarity benchmarks. Beyond just providing a number, our methodalso offers interpretability by pointing to the specific level of granularityof the description where the source data are differentiated.\rOnline Advertisements with LLMs: Opportunities and Challenges\nSoheil Feizi MohammadTaghi Hajiaghayi Keivan Rezaei Suho Shin\nabstract\rabstract: This paper explores the potential for leveraging Large Language Models (LLM)in the realm of online advertising systems. We delve into essentialrequirements including privacy, latency, reliability as well as thesatisfaction of users and advertisers which such a system must fulfill. Wefurther introduce a general framework for LLM advertisement, consisting ofmodification, bidding, prediction, and auction modules. Different designconsiderations for each module is presented, with an in-depth examination oftheir practicality and the technical challenges inherent to theirimplementation. Finally, we explore the prospect of LLM-based dynamic creativeoptimization as a means to significantly enhance the appeal of advertisementsto users and discuss its additional challenges.\r2024-02-13\nJAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models\nJillian Fisher Ximing Lu Jaehun Jung Liwei Jiang Zaid Harchaoui Yejin Choi\nabstract\rabstract: The permanence of online content combined with the enhanced authorshipidentification techniques calls for stronger computational methods to protectthe identity and privacy of online authorship when needed, e.g., blind reviewsfor scientific papers, anonymous online reviews, or anonymous interactions inthe mental health forums. In this paper, we propose an unsupervisedinference-time approach to authorship obfuscation to address the uniquechallenges of authorship obfuscation: lack of supervision data for diverseauthorship and domains, and the need for a sufficient level of revision beyondsimple paraphrasing to obfuscate the authorship, all the while preserving theoriginal content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm forauthorship obfuscation that can be in principle applied to any text andauthorship. Our approach builds on small language models such as GPT2-XL inorder to help avoid disclosing the original content to proprietary LLM\u0026rsquo;s APIs,while also reducing the performance gap between small and large language modelsvia algorithmic enhancement. The key idea behind our approach is to boost thecreative power of smaller language models through constrained decoding, whilealso allowing for user-specified controls and flexibility. Experimental resultsdemonstrate that our approach based on GPT2-XL outperforms previousstate-of-the-art methods based on comparably small models, while performingcompetitively against GPT3.5 175B, a propriety model that is two orders ofmagnitudes larger.\rComputational Copyright: Towards A Royalty Model for Music Generative AI\nJunwei Deng Jiaqi Ma\nabstract\rabstract: The advancement of generative AI has given rise to pressing copyrightchallenges, particularly in music industry. This paper focuses on the economicaspects of these challenges, emphasizing that the economic impact constitutes acentral issue in the copyright arena. The complexity of the black-boxgenerative AI technologies not only suggests but necessitates algorithmicsolutions. However, such solutions have been largely missing, leading toregulatory challenges in this landscape. We aim to bridge the gap in currentapproaches by proposing potential royalty models for revenue sharing on AImusic generation platforms. Our methodology involves a detailed analysis ofexisting royalty models in platforms like Spotify and YouTube, and adaptingthese to the unique context of AI-generated music. A significant challenge weaddress is the attribution of AI-generated music to influential copyrightedcontent in the training data. To this end, we present algorithmic solutionsemploying data attribution techniques. Our experimental results verify theeffectiveness of these solutions. This research represents a pioneering effortin integrating technical advancements with economic and legal considerations inthe field of generative AI, offering a computational copyright solution for thechallenges posed by the opaque nature of AI technologies.\rMapping the Ethics of Generative AI: A Comprehensive Scoping Review\nThilo Hagendorff\nabstract\rabstract: The advent of generative artificial intelligence and the widespread adoptionof it in society engendered intensive debates about its ethical implicationsand risks. These risks often differ from those associated with traditionaldiscriminative machine learning. To synthesize the recent discourse and map itsnormative concepts, we conducted a scoping review on the ethics of generativeartificial intelligence, including especially large language models andtext-to-image models. Our analysis provides a taxonomy of 378 normative issuesin 19 topic areas and ranks them according to their prevalence in theliterature. The study offers a comprehensive overview for scholars,practitioners, or policymakers, condensing the ethical debates surroundingfairness, safety, harmful content, hallucinations, privacy, interaction risks,security, alignment, societal impacts, and others. We discuss the results,evaluate imbalances in the literature, and explore unsubstantiated riskscenarios.\rPrivacy-Preserving Language Model Inference with Instance Obfuscation\nYixiang Yao Fei Wang Srivatsan Ravi Muhao Chen\nabstract\rabstract: Language Models as a Service (LMaaS) offers convenient access for developersand researchers to perform inference using pre-trained language models.Nonetheless, the input data and the inference results containing privateinformation are exposed as plaintext during the service call, leading toprivacy issues. Recent studies have started tackling the privacy issue bytransforming input data into privacy-preserving representation from theuser-end with the techniques such as noise addition and content perturbation,while the exploration of inference result protection, namely decision privacy,is still a blank page. In order to maintain the black-box manner of LMaaS,conducting data privacy protection, especially for the decision, is achallenging task because the process has to be seamless to the models andaccompanied by limited communication and computation overhead. We thus proposeInstance-Obfuscated Inference (IOI) method, which focuses on addressing thedecision privacy issue of natural language understanding tasks in theircomplete life-cycle. Besides, we conduct comprehensive experiments to evaluatethe performance as well as the privacy-protection strength of the proposedmethod on various benchmarking tasks.\rBBox-Adapter: Lightweight Adapting for Black-Box Large Language Models\nHaotian Sun Yuchen Zhuang Wei Wei Chao Zhang Bo Dai\nabstract\rabstract: Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Geminifor specific tasks is challenging. Due to the opacity in their parameters,embeddings, and even output probabilities, existing fine-tuning adaptationmethods are inapplicable. Consequently, adapting these black-box LLMs is onlypossible through their API services, raising concerns about transparency,privacy, and cost. To address these challenges, we introduce BBox-Adapter, anovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes targetand source domain data by treating target data as positive and source data asnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss topromote the likelihood of target domain data while penalizing that of thesource domain. Furthermore, it features an online adaptation mechanism, whichincorporates real-time positive data sampling from ground-truth, human, or AIfeedback, coupled with negative data from previous adaptations. Extensiveexperiments demonstrate BBox-Adapter\u0026rsquo;s effectiveness and cost efficiency. Itimproves model performance by up to 6.77% across diverse tasks and domains,while reducing training and inference costs by 31.30x and 1.84x, respectively.\r2024-02-12\nPANORAMIA: Privacy Auditing of Machine Learning Models without Retraining\nMishaal Kazmi Hadrien Lautraite Alireza Akbari Mauricio Soroco Qiaoyue Tang Tao Wang Sébastien Gambs Mathias Lécuyer\nabstract\rabstract: We introduce a privacy auditing scheme for ML models that relies onmembership inference attacks using generated data as \u0026ldquo;non-members\u0026rdquo;. Thisscheme, which we call PANORAMIA, quantifies the privacy leakage for large-scaleML models without control of the training process or model re-training and onlyrequires access to a subset of the training data. To demonstrate itsapplicability, we evaluate our auditing scheme across multiple ML domains,ranging from image and tabular data classification to large-scale languagemodels.\rRetrieval-Augmented Thought Process as Sequential Decision Making\nThomas Pouplin Hao Sun Samuel Holt Mihaela van der Schaar\nabstract\rabstract: Large Language Models (LLMs) have demonstrated their strong ability to assistpeople and show \u0026ldquo;sparks of intelligence\u0026rdquo;. However, several open challengeshinder their wider application: such as concerns over privacy, tendencies toproduce hallucinations, and difficulties in handling long contexts. In thiswork, we address those challenges by introducing the Retrieval-AugmentedThought Process (RATP). Given access to external knowledge, RATP formulates thethought generation of LLMs as a multiple-step decision process. To optimizesuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns aQ-value estimator that permits cost-efficient inference. In addressing the taskof question-answering with private data, where ethical and security concernslimit LLM training methods, RATP achieves a 50% improvement over existingin-context retrieval-augmented language models.\rEmpowering Federated Learning for Massive Models with NVIDIA FLARE\nHolger R. Roth Ziyue Xu Yuan-Ting Hsieh Adithya Renduchintala Isaac Yang Zhihong Zhang Yuhong Wen Sean Yang Kevin Lu Kristopher Kersten Camir Ricketts Daguang Xu Chester Chen Yan Cheng Andrew Feng\nabstract\rabstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.\rEmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models\nGuo Lin Wenyue Hua Yongfeng Zhang\nabstract\rabstract: Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanism EmojiCryptto protect user privacy. It uses Emoji to encrypt the user inputs beforesending them to LLM, effectively rendering them indecipherable to human orLLM\u0026rsquo;s examination while retaining the original intent of the prompt, thusensuring the model\u0026rsquo;s performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that EmojiCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/EmojiCrypt.\rLarge language models can enhance persuasion through linguistic feature alignment\nMinkyu Shin Jin Kim\nabstract\rabstract: Although large language models (LLMs) are reshaping various aspects of humanlife, our current understanding of their impacts remains somewhat constrained.Here we investigate the impact of LLMs on human communication, using data onconsumer complaints in the financial industry. By employing an AI detectiontool on more than 820K complaints gathered by the Consumer Financial ProtectionBureau (CFPB), we find a sharp increase in the likely use of LLMs shortly afterthe release of ChatGPT. Moreover, the likely LLM usage was positivelycorrelated with message persuasiveness (i.e., increased likelihood of obtainingrelief from financial firms). Computational linguistic analyses suggest thatthe positive correlation may be explained by LLMs\u0026rsquo; enhancement of variouslinguistic features. Based on the results of these observational studies, wehypothesize that LLM usage may enhance a comprehensive set of linguisticfeatures, increasing message persuasiveness to receivers with heterogeneouslinguistic preferences (i.e., linguistic feature alignment). We test thishypothesis in preregistered experiments and find support for it. As an instanceof early empirical demonstrations of LLM usage for enhancing persuasion, ourresearch highlights the transformative potential of LLMs in humancommunication.\rSecret Collusion Among Generative AI Agents\nSumeet Ramesh Motwani Mikhail Baranchuk Martin Strohmeier Vijay Bolina Philip H. S. Torr Lewis Hammond Christian Schroeder de Witt\nabstract\rabstract: Recent capability increases in large language models (LLMs) open upapplications in which teams of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both the AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.\rGame Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch\nRay Ito Junichiro Takahashi\nabstract\rabstract: Several attempts have been made to implement text command control for gameagents. However, current technologies are limited to processing predefinedformat commands. This paper proposes a pioneering text command control systemfor a game agent that can understand natural language commands expressed infree-form. The proposed system uses a large language model (LLM) for codegeneration to interpret and transform natural language commands into behaviorbranch, a proposed knowledge expression based on behavior trees, whichfacilitates execution by the game agent. This study conducted empiricalvalidation within a game environment that simulates a Pok'emon game andinvolved multiple participants. The results confirmed the system\u0026rsquo;s ability tounderstand and carry out natural language commands, representing a noteworthyin the realm of real-time language interactive game agents. Notice for the use of this material. The copyright of this material isretained by the Japanese Society for Artificial Intelligence (JSAI). Thismaterial is published here with the agreement of JSAI. Please be complied withCopyright Law of Japan if any users wish to reproduce, make derivative work,distribute or make available to the public any part or whole thereof. AllRights Reserved, Copyright (C) The Japanese Society for ArtificialIntelligence.\rFive ethical principles for generative AI in scientific research\nZhicheng Lin\nabstract\rabstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.\rUtilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code\nLiming Jiang\nabstract\rabstract: Mini-applications, commonly referred to as mini-apps, are compact softwareprograms embedded within larger applications or platforms, offering targetedfunctionality without the need for separate installations. Typically web-basedor cloud-hosted, these mini-apps streamline user experiences by providingfocused services accessible through web browsers or mobile apps. Theirsimplicity, speed, and integration capabilities make them valuable additions tomessaging platforms, social media networks, e-commerce sites, and variousdigital environments. WeChat Mini Programs, a prominent feature of China\u0026rsquo;sleading messaging app, exemplify this trend, offering users a seamless array ofservices without additional downloads. Leveraging WeChat\u0026rsquo;s extensive user baseand payment infrastructure, Mini Programs facilitate efficient transactions andbridge online and offline experiences, shaping China\u0026rsquo;s digital landscapesignificantly. This paper investigates the potential of employing LargeLanguage Models (LLMs) to detect privacy breaches within WeChat Mini Programs.Given the widespread use of Mini Programs and growing concerns about dataprivacy, this research seeks to determine if LLMs can effectively identifyinstances of privacy leakage within this ecosystem. Through meticulous analysisand experimentation, we aim to highlight the efficacy of LLMs in safeguardinguser privacy and security within the WeChat Mini Program environment, therebycontributing to a more secure digital landscape.\r2024-02-11\nDifferentially Private Training of Mixture of Experts Models\nPierre Tholoniat Huseyin A. Inan Janardhan Kulkarni Robert Sim\nabstract\rabstract: This position paper investigates the integration of Differential Privacy (DP)in the training of Mixture of Experts (MoE) models within the field of naturallanguage processing. As Large Language Models (LLMs) scale to billions ofparameters, leveraging expansive datasets, they exhibit enhanced linguisticcapabilities and emergent abilities. However, this growth raises significantcomputational and privacy concerns. Our study addresses these issues byexploring the potential of MoE models, known for their computationalefficiency, and the application of DP, a standard for privacy preservation. Wepresent the first known attempt to train MoE models under the constraints ofDP, addressing the unique challenges posed by their architecture and thecomplexities of DP integration. Our initial experimental studies demonstratethat MoE models can be effectively trained with DP, achieving performance thatis competitive with their non-private counterparts. This initial study aims toprovide valuable insights and ignite further research in the domain ofprivacy-preserving MoE models, softly laying the groundwork for prospectivedevelopments in this evolving field.\rWeakly interacting one-dimensional topological insulators: a bosonization approach\nPolina Matveeva Dmitri Gutman Sam T. Carr\nabstract\rabstract: We investigate the topological properties of one-dimensional weaklyinteracting topological insulators using bosonization. To do that we study thetopological edge states that emerge at the edges of a model realized by astrong impurity or at the boundary between topologically distinct phases. Inthe bosonic model, the edge states are manifested as degenerate bosonic kinksat the boundaries. We first illustrate this idea on the example of theinteracting Su-Schrieffer-Heeger (SSH) chain. We compute the localizationlength of the edge states as the width of an edge soliton that occurs in theSSH model in the presence of a strong impurity. Next, we examine models of twocapacitively coupled SSH chains that can be either identical or in distincttopological phases. We find that weak Hubbard interaction reduces the groundstate degeneracy in the topological phase of identical chains. We then provethat similarly to the non-interacting model, the degeneracy of the edge statesin the interacting case is protected by chiral symmetry. We then studytopological insulators built from two SSH chains with inter-chain hopping, thatrepresent models of different chiral symmetric universality classes. Wedemonstrate in bosonic language that the topological index of a weakly coupledmodel is determined by the type of inter-chain coupling, invariant under one oftwo possible chiral symmetry operators. Finally, we show that a generalone-dimensional model in a phase with topological index $\\nu$ is equivalent atlow energies to a theory of at least $\\nu$ SSH chains. We illustrate this ideaon the example of an SSH model with longer-range hopping.\r2024-02-10\nData Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\nShahriar Golchin Mihai Surdeanu\nabstract\rabstract: We propose the Data Contamination Quiz (DCQ), a simple and effective approachto detect data contamination in large language models (LLMs) and estimate theamount of it. Specifically, we frame data contamination detection as a seriesof multiple-choice questions and devise a quiz format wherein three perturbedversions of each dataset instance are created. These changes only includeword-level perturbations. The generated perturbed versions, along with theoriginal instance, form the options in the DCQ, with an extra optionaccommodating the possibility that none of the provided choices is correct.Given that the only distinguishing signal among the choices is the exactwording relative to the original instance, an LLM, when tasked with identifyingthe original instance from the choices, gravitates towards the original one ifit has been exposed to it in its pre-training phase\u0026ndash;a trait intrinsic to LLMs.Tested over several datasets with GPT-4/3.5, our findings\u0026ndash;while fully lackingaccess to LLMs\u0026rsquo; pre-training data and internal parameters\u0026ndash;suggest that DCQuncovers greater contamination levels compared to existing detection methodsand proficiently bypasses more safety filters, especially those set to avoidgenerating copyrighted contents.\rOpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning\nRui Ye Wenhao Wang Jingyi Chai Dihan Li Zexi Li Yinda Xu Yaxin Du Yanfeng Wang Siheng Chen\nabstract\rabstract: Trained on massive publicly available data, large language models (LLMs) havedemonstrated tremendous success across various fields. While more datacontributes to better performance, a disconcerting reality is that high-qualitypublic data will be exhausted in a few years. In this paper, we offer apotential next step for contemporary LLMs: collaborative and privacy-preservingLLM training on the underutilized distributed private data via federatedlearning (FL), where multiple data owners collaboratively train a shared modelwithout transmitting raw data. To achieve this, we build a concise, integrated,and research-friendly framework/codebase, named OpenFedLLM. It covers federatedinstruction tuning for enhancing instruction-following capability, federatedvalue alignment for aligning with human values, and 7 representative FLalgorithms. Besides, OpenFedLLM supports training on diverse domains, where wecover 8 training datasets; and provides comprehensive evaluations, where wecover 30+ evaluation metrics. Through extensive experiments, we observe thatall FL algorithms outperform local training on training LLMs, demonstrating aclear performance improvement across a variety of settings. Notably, in afinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm canoutperform GPT-4 by a significant margin while the model obtained throughindividual training cannot, demonstrating strong motivation for clients toparticipate in FL. The code is available athttps://github.com/rui-ye/OpenFedLLM.\rEfficient Incremental Belief Updates Using Weighted Virtual Observations\nDavid Tolpin\nabstract\rabstract: We present an algorithmic solution to the problem of incremental beliefupdating in the context of Monte Carlo inference in Bayesian statistical modelsrepresented by probabilistic programs. Given a model and a sample-approximatedposterior, our solution constructs a set of weighted observations to conditionthe model such that inference would result in the same posterior. This problemarises e.g. in multi-level modelling, incremental inference, inference inpresence of privacy constraints. First, a set of virtual observations isselected, then, observation weights are found through a computationallyefficient optimization procedure such that the reconstructed posteriorcoincides with or closely approximates the original posterior. We implement andapply the solution to a number of didactic examples and case studies, showingefficiency and robustness of our approach. The provided referenceimplementation is agnostic to the probabilistic programming language or theinference algorithm, and can be applied to most mainstream probabilisticprogramming environments.\rWhispers in the Machine: Confidentiality in LLM-integrated Systems\nJonathan Evertz Merlin Chlosta Lea Schönherr Thorsten Eisenhofer\nabstract\rabstract: Large Language Models (LLMs) are increasingly integrated with external tools.While these integrations can significantly improve the functionality of LLMs,they also create a new attack surface where confidential data may be disclosedbetween different components. Specifically, malicious tools can exploitvulnerabilities in the LLM itself to manipulate the model and compromise thedata of other services, raising the question of how private data can beprotected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality inLLM-integrated systems. For this, we formalize a \u0026ldquo;secret key\u0026rdquo; game that cancapture the ability of a model to conceal private information. This enables usto compare the vulnerability of a model against confidentiality attacks andalso the effectiveness of different defense strategies. In this framework, weevaluate eight previously published attacks and four defenses. We find thatcurrent defenses lack generalization across attack strategies. Building on thisanalysis, we propose a method for robustness fine-tuning, inspired byadversarial training. This approach is effective in lowering the success rateof attackers and in improving the system\u0026rsquo;s resilience against unknown attacks.\r2024-02-09\nTowards Principled Assessment of Tabular Data Synthesis Algorithms\nYuntao Du Ninghui Li\nabstract\rabstract: Data synthesis has been advocated as an important approach for utilizing datawhile protecting data privacy. A large number of tabular data synthesisalgorithms (which we call synthesizers) have been proposed. Some synthesizerssatisfy Differential Privacy, while others aim to provide privacy in aheuristic fashion. A comprehensive understanding of the strengths andweaknesses of these synthesizers remains elusive due to lacking principledevaluation metrics and missing head-to-head comparisons of newly developedsynthesizers that take advantage of diffusion models and large language modelswith state-of-the-art marginal-based synthesizers. In this paper, we present a principled and systematic evaluation frameworkfor assessing tabular data synthesis algorithms. Specifically, we examine andcritique existing evaluation metrics, and introduce a set of new metrics interms of fidelity, privacy, and utility to address their limitations. Based onthe proposed metrics, we also devise a unified objective for tuning, which canconsistently improve the quality of synthetic data for all methods. Weconducted extensive evaluations of 8 different types of synthesizers on 12datasets and identified some interesting findings, which offer new directionsfor privacy-preserving data synthesis.\rExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs\nFernando Ferraretto Thiago Laitz Roberto Lotufo Rodrigo Nogueira\nabstract\rabstract: ExaRanker recently introduced an approach to training information retrieval(IR) models, incorporating natural language explanations as additional labels.The method addresses the challenge of limited labeled examples, leading toimprovements in the effectiveness of IR models. However, the initial resultswere based on proprietary language models such as GPT-3.5, which posedconstraints on dataset size due to its cost and data privacy. In this paper, weintroduce ExaRanker-Open, where we adapt and explore the use of open-sourcelanguage models to generate explanations. The method has been tested usingdifferent LLMs and datasets sizes to better comprehend the effectivecontribution of data augmentation. Our findings reveal that incorporatingexplanations consistently enhances neural rankers, with benefits escalating asthe LLM size increases. Notably, the data augmentation method provesadvantageous even with large datasets, as evidenced by ExaRanker surpassing thetarget baseline by 0.6 nDCG@10 points in our study. To encourage furtheradvancements by the research community, we have open-sourced both the code anddatasets at https://github.com/unicamp-dl/ExaRanker.\rStudious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning\nYichuan Mo Yuji Wang Zeming Wei Yisen Wang\nabstract\rabstract: Although Large Language Models (LLMs) have achieved tremendous success invarious applications, they are also susceptible to certain prompts that caninduce them to bypass built-in safety measures and provide dangerous or illegalcontent, a phenomenon known as jailbreak. To protect LLMs from producingharmful information, various defense strategies are proposed, with mostfocusing on content filtering or adversarial training of models. In this paper,we propose an approach named Prompt Adversarial Tuning (PAT) to train a defensecontrol mechanism, which is then embedded as a prefix to user prompts toimplement our defense strategy. We design a training process similar toadversarial training to achieve our optimized goal, alternating betweenupdating attack and defense controls. To our knowledge, we are the first toimplement defense from the perspective of prompt tuning. Once employed, ourmethod will hardly impact the operational efficiency of LLMs. Experiments showthat our method is effective in both black-box and white-box settings, reducingthe success rate of advanced attacks to nearly 0 while maintaining the benignanswer rate of 80% to simple benign questions. Our work might potentially charta new perspective for future explorations in LLM security.\r2024-02-08\nLLMs Among Us: Generative AI Participating in Digital Discourse\nKristina Radivojevic Nicholas Clark Paul Brenner\nabstract\rabstract: The emergence of Large Language Models (LLMs) has great potential to reshapethe landscape of many social media platforms. While this can bring promisingopportunities, it also raises many threats, such as biases and privacyconcerns, and may contribute to the spread of propaganda by malicious actors.We developed the \u0026ldquo;LLMs Among Us\u0026rdquo; experimental framework on top of the Mastodonsocial media platform for bot and human participants to communicate withoutknowing the ratio or nature of bot and human participants. We built 10 personaswith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted threerounds of the experiment and surveyed participants after each round to measurethe ability of LLMs to pose as human participants without human detection. Wefound that participants correctly identified the nature of other users in theexperiment only 42% of the time despite knowing the presence of both bots andhumans. We also found that the choice of persona had substantially more impacton human perception than the choice of mainstream LLMs.\rSocial Learning: Towards Collaborative Learning with Large Language Models\nAmirkeivan Mohtashami Florian Hartmann Sian Gooding Lukas Zilka Matt Sharifi Blaise Aguera y Arcas\nabstract\rabstract: We introduce the framework of \u0026ldquo;social learning\u0026rdquo; in the context of largelanguage models (LLMs), whereby models share knowledge with each other in aprivacy-aware manner using natural language. We present and evaluate twoapproaches for knowledge transfer between LLMs. In the first scenario, we allowthe model to generate abstract prompts aiming to teach the task. In our secondapproach, models transfer knowledge by generating synthetic examples. Weevaluate these methods across diverse datasets and quantify memorization as aproxy for privacy loss. These techniques inspired by social learning yieldpromising results with low memorization of the original data. In particular, weshow that performance using these methods is comparable to results with the useof original labels and prompts. Our work demonstrates the viability of sociallearning for LLMs, establishes baseline approaches and highlights severalunexplored areas for future work.\rImproved upper bounds for wide-sense frameproof codes\nYuhao Zhao Xiande Zhang\nabstract\rabstract: Frameproof codes have been extensively studied for many years due to theirapplication in copyright protection and their connection to extremal settheory. In this paper, we investigate upper bounds on the cardinality ofwide-sense $t$-frameproof codes. For $t=2$, we apply results from Spernertheory to give a better upper bound, which significantly improves a recentbound by Zhou and Zhou. For $t\\geq 3$, we provide a general upper bound byestablishing a relation between wide-sense frameproof codes and cover-freefamilies. Finally, when the code length $n$ is at most$\\frac{15+\\sqrt{33}}{24}(t-1)^2$, we show that a wide-sense $t$-frameproof codehas at most $n$ codewords, and the unique optimal code consists of allweight-one codewords. As byproducts, our results improve several best knownresults on binary $t$-frameproof codes.\rRevolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices\nMohamed Amine Ferrag Mthandazo Ndhlovu Norbert Tihanyi Lucas C. Cordeiro Merouane Debbah Thierry Lestable Narinderjit Singh Thandi\nabstract\rabstract: The field of Natural Language Processing (NLP) is currently undergoing arevolutionary transformation driven by the power of pre-trained Large LanguageModels (LLMs) based on groundbreaking Transformer architectures. As thefrequency and diversity of cybersecurity attacks continue to rise, theimportance of incident detection has significantly increased. IoT devices areexpanding rapidly, resulting in a growing need for efficient techniques toautonomously identify network-based attacks in IoT networks with both highprecision and minimal computational requirements. This paper presentsSecurityBERT, a novel architecture that leverages the Bidirectional EncoderRepresentations from Transformers (BERT) model for cyber threat detection inIoT networks. During the training of SecurityBERT, we incorporated a novelprivacy-preserving encoding technique called Privacy-Preserving Fixed-LengthEncoding (PPFLE). We effectively represented network traffic data in astructured format by combining PPFLE with the Byte-level Byte-Pair Encoder(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperformstraditional Machine Learning (ML) and Deep Learning (DL) methods, such asConvolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), incyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, ourexperimental analysis shows that SecurityBERT achieved an impressive 98.2%overall accuracy in identifying fourteen distinct attack types, surpassingprevious records set by hybrid solutions such as GAN-Transformer-basedarchitectures and CNN-LSTM models. With an inference time of less than 0.15seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERTis ideally suited for real-life traffic analysis and a suitable choice fordeployment on resource-constrained IoT devices.\r2024-02-07\nDefending Our Privacy With Backdoors\nDominik Hintersdorf Lukas Struppek Daniel Neider Kristian Kersting\nabstract\rabstract: The proliferation of large AI models trained on uncurated, often sensitiveweb-scraped data has raised significant privacy concerns. One of the concernsis that adversaries can extract information about the training data usingprivacy attacks. Unfortunately, the task of removing specific information fromthe models without sacrificing performance is not straightforward and hasproven to be challenging. We propose a rather easy yet effective defense basedon backdoor attacks to remove private information such as names and faces ofindividuals from vision-language models by fine-tuning them for only a fewminutes instead of re-training them from scratch. Specifically, throughstrategic insertion of backdoors into text encoders, we align the embeddings ofsensitive phrases with those of neutral terms-\u0026ldquo;a person\u0026rdquo; instead of theperson\u0026rsquo;s actual name. For image encoders, we map embeddings of individuals tobe removed from the model to a universal, anonymous embedding. Our empiricalresults demonstrate the effectiveness of our backdoor-based defense on CLIP byassessing its performance using a specialized privacy attack for zero-shotclassifiers. Our approach provides not only a new \u0026ldquo;dual-use\u0026rdquo; perspective onbackdoor attacks, but also presents a promising avenue to enhance the privacyof individuals within models trained on uncurated web-scraped data.\rHuman-Readable Fingerprint for Large Language Models\nBoyi Zeng Chenghu Zhou Xinbing Wang Zhouhan Lin\nabstract\rabstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters\u0026rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM\u0026rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog\u0026rsquo;s appearancestrongly indicates the LLM\u0026rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.\rDe-amplifying Bias from Differential Privacy in Language Model Fine-tuning\nSanjari Srivastava Piotr Mardziel Zhikhun Zhang Archana Ahlawat Anupam Datta John C Mitchell\nabstract\rabstract: Fairness and privacy are two important values machine learning (ML)practitioners often seek to operationalize in models. Fairness aims to reducemodel bias for social/demographic sub-groups. Privacy via differential privacy(DP) mechanisms, on the other hand, limits the impact of any individual\u0026rsquo;straining data on the resulting model. The trade-offs between privacy andfairness goals of trustworthy ML pose a challenge to those wishing to addressboth. We show that DP amplifies gender, racial, and religious bias whenfine-tuning large language models (LLMs), producing models more biased thanones fine-tuned without DP. We find the cause of the amplification to be adisparity in convergence of gradients across sub-groups. Through the case ofbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),a known method for addressing bias, also mitigates bias amplification by DP. Asa consequence, DP and CDA together can be used to fine-tune models whilemaintaining both fairness and privacy.\r2024-02-06\nDemocratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\nZhaoxuan Tan Qingkai Zeng Yijun Tian Zheyuan Liu Bing Yin Meng Jiang\nabstract\rabstract: Personalization in large language models (LLMs) is increasingly important,aiming to align LLM\u0026rsquo;s interactions, content, and recommendations withindividual user preferences. Recent advances in LLM personalization havespotlighted effective prompt design, by enriching user queries withnon-parametric knowledge through behavior history retrieval and textualprofiles. However, these approaches were limited due to a lack of modelownership, resulting in constrained customization and privacy issues. Moreover,they often failed to accurately capture user behavior patterns, especially incases where user data were complex and dynamic. To address these shortcomings,we introduce One PEFT Per User (OPPU), which employs personalizedparameter-efficient fine-tuning (PEFT) modules, to store user-specific behaviorpatterns and preferences. By plugging in users\u0026rsquo; personal PEFT parameters, theycan own and use their LLMs personally. OPPU integrates parametric userknowledge in the personal PEFT parameters with the non-parametric knowledgeacquired through retrieval and profile. This integration adapts individual LLMsto user behavior shifts. Experimental results demonstrate that OPPUsignificantly outperforms existing prompt-based methods across seven diversetasks in the LaMP benchmark. Further in-depth studies reveal OPPU\u0026rsquo;s enhancedcapabilities in handling user behavior shifts, modeling users at differentactive levels, maintaining robustness across various user history formats, anddisplaying versatility with different PEFT methods.\rOrganic or Diffused: Can We Distinguish Human Art from AI-generated Images?\nAnna Yoo Jeong Ha Josephine Passananti Ronik Bhaskar Shawn Shan Reid Southen Haitao Zheng Ben Y. Zhao\nabstract\rabstract: The advent of generative AI images has completely disrupted the art world.Distinguishing AI generated images from human art is a challenging problemwhose impact is growing over time. A failure to address this problem allows badactors to defraud individuals paying a premium for human art and companieswhose stated policies forbid AI imagery. It is also critical for content ownersto establish copyright, and for model trainers interested in curating trainingdata in order to avoid potential model collapse. There are several different approaches to distinguishing human art from AIimages, including classifiers trained by supervised learning, research toolstargeting diffusion models, and identification by professional artists usingtheir knowledge of artistic techniques. In this paper, we seek to understandhow well these approaches can perform against today\u0026rsquo;s modern generative modelsin both benign and adversarial settings. We curate real human art across 7styles, generate matching images from 5 generative models, and apply 8detectors (5 automated detectors and 3 different human groups including 180crowdworkers, 4000+ professional artists, and 13 expert artists experienced atdetecting AI). Both Hive and expert artists do very well, but make mistakes indifferent ways (Hive is weaker against adversarial perturbations while Expertartists produce higher false positives). We believe these weaknesses willremain as models continue to evolve, and use our data to demonstrate why acombined team of human and automated detectors provides the best combination ofaccuracy and robustness.\rLLsM: Generative Linguistic Steganography with Large Language Model\nYihao Wang Ruiqi Song Ru Zhang Jianyi Liu Lingxiao Li\nabstract\rabstract: Linguistic Steganography (LS) tasks aim to generate steganographic text(stego) based on secret information. Only authorized recipients can perceivethe existence of secrets in the texts and extract them, thereby preservingprivacy. However, the controllability of the stego generated by existingschemes is poor, and the stego is difficult to contain specific discoursecharacteristics such as style. As a result, the stego is easily detectable,compromising covert communication. To address these problems, this paperproposes LLsM, the first LS with the Large Language Model (LLM). We fine-tunedthe LLaMA2 with a large-scale constructed dataset encompassing rich discoursecharacteristics, which enables the fine-tuned LLM to generate texts withspecific discourse in a controllable manner. Then the discourse is used asguiding information and inputted into the fine-tuned LLM in the form of thePrompt together with secret. On this basis, the constructed candidate pool willbe range encoded and use secret to determine the interval. The same prefix ofthis interval\u0026rsquo;s beginning and ending is the secret embedded at this moment.Experiments show that LLsM performs superior to prevalent LS-task andrelated-task baselines regarding text quality, statistical analysis, discoursematching, and anti-steganalysis. In particular, LLsM\u0026rsquo;s MAUVE matric surpassessome baselines by 70%-80%, and its anti-steganalysis performance is 30%-40%higher. Notably, we also present examples of longer stegos generated by LLsM,showing its potential superiority in long LS tasks.\r2024-02-07\nGrounding Foundation Models through Federated Transfer Learning: A General Framework\nYan Kang Tao Fan Hanlin Gu Xiaojin Zhang Lixin Fan Qiang Yang\nabstract\rabstract: Foundation Models (FMs) such as GPT-4 encoded with vast knowledge andpowerful emergent abilities have achieved remarkable success in various naturallanguage processing and computer vision tasks. Grounding FMs by adapting themto domain-specific tasks or augmenting them with domain-specific knowledgeenables us to exploit the full potential of FMs. However, grounding FMs facesseveral challenges, stemming primarily from constrained computing resources,data privacy, model heterogeneity, and model ownership. Federated TransferLearning (FTL), the combination of federated learning and transfer learning,provides promising solutions to address these challenges. In recent years, theneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly inboth academia and industry. Motivated by the strong growth in FTL-FM researchand the potential impact of FTL-FM on industrial applications, we propose anFTL-FM framework that formulates problems of grounding FMs in the federatedlearning setting, construct a detailed taxonomy based on the FTL-FM frameworkto categorize state-of-the-art FTL-FM works, and comprehensively overviewFTL-FM works based on the proposed taxonomy. We also establish correspondencesbetween FTL-FM and conventional phases of adapting FM so that FM practitionerscan align their research works with FTL-FM. In addition, we overview advancedefficiency-improving and privacy-preserving techniques because efficiency andprivacy are critical concerns in FTL-FM. Last, we discuss opportunities andfuture research directions of FTL-FM.\r2024-02-06\nEmbedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy\nEfe Bozkir Süleyman Özdel Ka Hei Carrie Lau Mengdi Wang Hong Gao Enkelejda Kasneci\nabstract\rabstract: Recent developments in computer graphics, hardware, artificial intelligence(AI), and human-computer interaction likely lead to extended reality (XR)devices and setups being more pervasive. While these devices and setups provideusers with interactive, engaging, and immersive experiences with differentsensing modalities, such as eye and hand trackers, many non-player charactersare utilized in a pre-scripted way or by conventional AI techniques. In thispaper, we argue for using large language models (LLMs) in XR by embedding themin virtual avatars or as narratives to facilitate more inclusive experiencesthrough prompt engineering according to user profiles and fine-tuning the LLMsfor particular purposes. We argue that such inclusion will facilitate diversityfor XR use. In addition, we believe that with the versatile conversationalcapabilities of LLMs, users will engage more with XR environments, which mighthelp XR be more used in everyday life. Lastly, we speculate that combining theinformation provided to LLM-powered environments by the users and the biometricdata obtained through the sensors might lead to novel privacy invasions. Whilestudying such possible privacy invasions, user privacy concerns and preferencesshould also be investigated. In summary, despite some challenges, embeddingLLMs into XR is a promising and novel research area with several opportunities.\rSelective Pre-training for Private Fine-tuning\nDa Yu Sivakanth Gopi Janardhan Kulkarni Zinan Lin Saurabh Naik Tomasz Lukasz Religa Jian Yin Huishuai Zhang\nabstract\rabstract: Suppose we want to train text prediction models in email clients or wordprocessors. These models, which serve billions of predictions per hour, mustpreserve the privacy of user data and adhere to specific model size constraintsto meet memory, inference time requirements, and to reduce inference cost.Building small, fast, and private domain-specific language models is a thrivingarea of research. In this work, we show that a careful pre-training on a {\\emsubset} of the public dataset that is guided by the private dataset is crucialto train small DP language models. On standard benchmarks, models trained withour new framework achieve state-of-the-art performance, improving upon all thebaselines from the literature. Besides performance improvements, our framework also shows that with carefulpre-training and private fine-tuning, smaller models can match the performanceof much larger models that do not have access to private data, highlighting thepromise of private learning as a tool for model compression and efficiency. In many applications such as health care, finance, etc., private datasets areusually of much higher quality than public datasets, and our work shows novelways of utilizing private datasets at all the stages of training pipe-line toimprove deep learning efficiency. Language models based on our framework havebeen used in multiple real-world deployments serving billions of predictionsper day (and saving millions of dollars in terms of inference cost)highlighting the general applicability of our framework beyond academicbenchmarks.\r2024-02-05\nTexShape: Information Theoretic Sentence Embedding for Language Models\nH. Kaan Kale Homa Esfahanizadeh Noel Elias Oguzhan Baser Muriel Medard Sriram Vishwanath\nabstract\rabstract: With the exponential growth in data volume and the emergence ofdata-intensive applications, particularly in the field of machine learning,concerns related to resource utilization, privacy, and fairness have becomeparamount. This paper focuses on the textual domain of data and addresseschallenges regarding encoding sentences to their optimized representationsthrough the lens of information-theory. In particular, we use empiricalestimates of mutual information, using the Donsker-Varadhan definition ofKullback-Leibler divergence. Our approach leverages this estimation to train aninformation-theoretic sentence embedding, called TexShape, for (task-based)data compression or for filtering out sensitive information, enhancing privacyand fairness. In this study, we employ a benchmark language model for initialtext representation, complemented by neural networks for information-theoreticcompression and mutual information estimations. Our experiments demonstratesignificant advancements in preserving maximal targeted information and minimalsensitive information over adverse compression ratios, in terms of predictiveaccuracy of downstream models that are trained using the compressed data.\rPsychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach\nSergi Blanco-Cuaresma\nabstract\rabstract: This study explores the use of Large Language Models (LLMs) to analyze textcomments from Reddit users, aiming to achieve two primary objectives: firstly,to pinpoint critical excerpts that support a predefined psychologicalassessment of suicidal risk; and secondly, to summarize the material tosubstantiate the preassigned suicidal risk level. The work is circumscribed tothe use of \u0026ldquo;open-source\u0026rdquo; LLMs that can be run locally, thereby enhancing dataprivacy. Furthermore, it prioritizes models with low computationalrequirements, making it accessible to both individuals and institutionsoperating on limited computing budgets. The implemented strategy only relies ona carefully crafted prompt and a grammar to guide the LLM\u0026rsquo;s text completion.Despite its simplicity, the evaluation metrics show outstanding results, makingit a valuable privacy-focused and cost-effective approach. This work is part ofthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 sharedtask.\rWeak-to-Strong Jailbreaking on Large Language Models\nXuandong Zhao Xianjun Yang Tianyu Pang Chao Du Lei Li Yu-Xiang Wang William Yang Wang\nabstract\rabstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack\u0026rsquo;s key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model\u0026rsquo;s decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at https://github.com/XuandongZhao/weak-to-strong\rConversation Reconstruction Attack Against GPT Models\nJunjie Chu Zeyang Sha Michael Backes Yang Zhang\nabstract\rabstract: In recent times, significant advancements have been made in the field oflarge language models (LLMs), represented by GPT series models. To optimizetask execution, users often engage in multi-round conversations with GPT modelshosted in cloud environments. These multi-round conversations, potentiallyreplete with private information, require transmission and storage within thecloud. However, this operational paradigm introduces additional attacksurfaces. In this paper, we first introduce a specific ConversationReconstruction Attack targeting GPT models. Our introduced ConversationReconstruction Attack is composed of two steps: hijacking a session andreconstructing the conversations. Subsequently, we offer an exhaustiveevaluation of the privacy risks inherent in conversations when GPT models aresubjected to the proposed attack. However, GPT-4 demonstrates certainrobustness to the proposed attacks. We then introduce two advanced attacksaimed at better reconstructing previous conversations, specifically the UNRattack and the PBU attack. Our experimental findings indicate that the PBUattack yields substantial performance across all models, achieving semanticsimilarity scores exceeding 0.60, while the UNR attack is effective solely onGPT-3.5. Our results reveal the concern about privacy risks associated withconversations involving GPT models and aim to draw the community\u0026rsquo;s attention toprevent the potential misuse of these models\u0026rsquo; remarkable capabilities. We willresponsibly disclose our findings to the suppliers of related large languagemodels.\rPutting Context in Context: the Impact of Discussion Structure on Text Classification\nNicolò Penzo Antonio Longa Bruno Lepri Sara Tonelli Marco Guerini\nabstract\rabstract: Current text classification approaches usually focus on the content to beclassified. Contextual aspects (both linguistic and extra-linguistic) areusually neglected, even in tasks based on online discussions. Still in manycases the multi-party and multi-turn nature of the context from which theseelements are selected can be fruitfully exploited. In this work, we propose aseries of experiments on a large dataset for stance detection in English, inwhich we evaluate the contribution of different types of contextualinformation, i.e. linguistic, structural and temporal, by feeding them asnatural language input into a transformer-based model. We also experiment withdifferent amounts of training data and analyse the topology of local discussionnetworks in a privacy-compliant way. Results show that structural informationcan be highly beneficial to text classification but only under certaincircumstances (e.g. depending on the amount of training data and on discussionchain complexity). Indeed, we show that contextual information on smallerdatasets from other classification tasks does not yield significantimprovements. Our framework, based on local discussion networks, allows theintegration of structural information, while minimising user profiling, thuspreserving their privacy.\rZero-Shot Machine Unlearning at Scale via Lipschitz Regularization\nJack Foster Kyle Fogarty Stefan Schoepf Cengiz Öztireli Alexandra Brintrup\nabstract\rabstract: To comply with AI and data regulations, the need to forget private orcopyrighted information from trained machine learning models is increasinglyimportant. The key challenge in unlearning is forgetting the necessary data ina timely manner, while preserving model performance. In this work, we addressthe zero-shot unlearning scenario, whereby an unlearning algorithm must be ableto remove data given only a trained model and the data to be forgotten. Undersuch a definition, existing state-of-the-art methods are insufficient. Buildingon the concepts of Lipschitz continuity, we present a method that inducessmoothing of the forget sample\u0026rsquo;s output, with respect to perturbations of thatsample. We show this smoothing successfully results in forgetting whilepreserving general model performance. We perform extensive empirical evaluationof our method over a range of contemporary benchmarks, verifying that ourmethod achieves state-of-the-art performance under the strict constraints ofzero-shot unlearning.\r2024-02-04\nCopyright Protection in Generative AI: A Technical Perspective\nJie Ren Han Xu Pengfei He Yingqian Cui Shenglai Zeng Jiankun Zhang Hongzhi Wen Jiayuan Ding Hui Liu Yi Chang Jiliang Tang\nabstract\rabstract: Generative AI has witnessed rapid advancement in recent years, expandingtheir capabilities to create synthesized content such as text, images, audio,and code. The high fidelity and authenticity of contents generated by theseDeep Generative Models (DGMs) have sparked significant copyright concerns.There have been various legal debates on how to effectively safeguardcopyrights in DGMs. This work delves into this issue by providing acomprehensive overview of copyright protection from a technical perspective. Weexamine from two distinct viewpoints: the copyrights pertaining to the sourcedata held by the data owners and those of the generative models maintained bythe model builders. For data copyright, we delve into methods data owners canprotect their content and DGMs can be utilized without infringing upon theserights. For model copyright, our discussion extends to strategies forpreventing model theft and identifying outputs generated by specific models.Finally, we highlight the limitations of existing techniques and identify areasthat remain unexplored. Furthermore, we discuss prospective directions for thefuture of copyright protection, underscoring its importance for the sustainableand ethical development of Generative AI.\rA Survey of Large Language Models in Finance (FinLLMs)\nJean Lee Nicholas Stevens Soyeon Caren Han Minseok Song\nabstract\rabstract: Large Language Models (LLMs) have shown remarkable capabilities across a widevariety of Natural Language Processing (NLP) tasks and have attracted attentionfrom multiple domains, including financial services. Despite the extensiveresearch into general-domain LLMs, and their immense potential in finance,Financial LLM (FinLLM) research remains limited. This survey provides acomprehensive overview of FinLLMs, including their history, techniques,performance, and opportunities and challenges. Firstly, we present achronological overview of general-domain Pre-trained Language Models (PLMs)through to current FinLLMs, including the GPT-series, selected open-sourceLLMs, and financial LMs. Secondly, we compare five techniques used acrossfinancial PLMs and FinLLMs, including training methods, training data, andfine-tuning methods. Thirdly, we summarize the performance evaluations of sixbenchmark tasks and datasets. In addition, we provide eight advanced financialNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, wediscuss the opportunities and the challenges facing FinLLMs, such ashallucination, privacy, and efficiency. To support AI research in finance, wecompile a collection of accessible datasets and evaluation benchmarks onGitHub.\r2024-02-03\nSeparable Multi-Concept Erasure from Diffusion Models\nMengnan Zhao Lihe Zhang Tianhang Zheng Yuqiu Kong Baocai Yin\nabstract\rabstract: Large-scale diffusion models, known for their impressive image generationcapabilities, have raised concerns among researchers regarding social impacts,such as the imitation of copyrighted artistic styles. In response, existingapproaches turn to machine unlearning techniques to eliminate unsafe conceptsfrom pre-trained models. However, these methods compromise the generativeperformance and neglect the coupling among multi-concept erasures, as well asthe concept restoration problem. To address these issues, we propose aSeparable Multi-concept Eraser (SepME), which mainly includes two parts: thegeneration of concept-irrelevant representations and the weight decoupling. Theformer aims to avoid unlearning substantial information that is irrelevant toforgotten concepts. The latter separates optimizable model weights, making eachweight increment correspond to a specific concept erasure without affectinggenerative performance on other concepts. Specifically, the weight incrementfor erasing a specified concept is formulated as a linear combination ofsolutions calculated based on other known undesirable concepts. Extensiveexperiments indicate the efficacy of our approach in eliminating concepts,preserving model performance, and offering flexibility in the erasure orrecovery of various concepts.\rHuman-Centered Privacy Research in the Age of Large Language Models\nTianshi Li Sauvik Das Hao-Ping Lee Dakuo Wang Bingsheng Yao Zhiping Zhang\nabstract\rabstract: The emergence of large language models (LLMs), and their increased use inuser-facing systems, has led to substantial privacy concerns. To date, researchon these privacy concerns has been model-centered: exploring how LLMs lead toprivacy risks like memorization, or can be used to infer personalcharacteristics about people from their content. We argue that there is a needfor more research focusing on the human aspect of these privacy issues: e.g.,research on how design paradigms for LLMs affect users\u0026rsquo; disclosure behaviors,users\u0026rsquo; mental models and preferences for privacy controls, and the design oftools, systems, and artifacts that empower end-users to reclaim ownership overtheir personal data. To build usable, efficient, and privacy-friendly systemspowered by these models with imperfect privacy properties, our goal is toinitiate discussions to outline an agenda for conducting human-centeredresearch on privacy issues in LLM-powered systems. This Special Interest Group(SIG) aims to bring together researchers with backgrounds in usable securityand privacy, human-AI collaboration, NLP, or any other related domains to sharetheir perspectives and experiences on this problem, to help our communityestablish a collective understanding of the challenges, research opportunities,research methods, and strategies to collaborate with researchers outside ofHCI.\r2024-02-02\nDigits micro-model for accurate and secure transactions\nChirag Chhablani Nikhita Sharma Jordan Hosier Vijay K. Gurbani\nabstract\rabstract: Automatic Speech Recognition (ASR) systems are used in the financial domainto enhance the caller experience by enabling natural language understanding andfacilitating efficient and intuitive interactions. Increasing use of ASRsystems requires that such systems exhibit very low error rates. Thepredominant ASR models to collect numeric data are large, general-purposecommercial models \u0026ndash; Google Speech-to-text (STT), or Amazon Transcribe \u0026ndash; oropen source (OpenAI\u0026rsquo;s Whisper). Such ASR models are trained on hundreds ofthousands of hours of audio data and require considerable resources to run.Despite recent progress large speech recognition models, we highlight thepotential of smaller, specialized \u0026ldquo;micro\u0026rdquo; models. Such light models can betrained perform well on number recognition specific tasks, competing withgeneral models like Whisper or Google STT while using less than 80 minutes oftraining time and occupying at least an order of less memory resources. Also,unlike larger speech recognition models, micro-models are trained on carefullyselected and curated datasets, which makes them highly accurate, agile, andeasy to retrain, while using low compute resources. We present our work oncreating micro models for multi-digit number recognition that handle diversespeaking styles reflecting real-world pronunciation patterns. Our workcontributes to domain-specific ASR models, improving digit recognitionaccuracy, and privacy of data. An added advantage, their low resourceconsumption allows them to be hosted on-premise, keeping private data localinstead uploading to an external cloud. Our results indicate that ourmicro-model makes less errors than the best-of-breed commercial or open-sourceASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for ourmodel versus 11 GB VRAM for Whisper).\r(A)I Am Not a Lawyer, But\u0026hellip;: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice\nInyoung Cheong King Xia K. J. Kevin Feng Quan Ze Chen Amy X. Zhang\nabstract\rabstract: The rapid proliferation of large language models (LLMs) as general purposechatbots available to the public raises hopes around expanding access toprofessional guidance in law, medicine, and finance, while triggering concernsabout public reliance on LLMs for high-stakes circumstances. Prior research hasspeculated on high-level ethical considerations but lacks concrete criteriadetermining when and why LLM chatbots should or should not provide professionalassistance. Through examining the legal domain, we contribute a structuredexpert analysis to uncover nuanced policy considerations around using LLMs forprofessional advice, using methods inspired by case-based reasoning. Weconvened workshops with 20 legal experts and elicited dimensions on appropriateAI assistance for sample user queries (``cases\u0026rsquo;\u0026rsquo;). We categorized our expertdimensions into: (1) user attributes, (2) query characteristics, (3) AIcapabilities, and (4) impacts. Beyond known issues like hallucinations, expertsrevealed novel legal problems, including that users\u0026rsquo; conversations with LLMsare not protected by attorney-client confidentiality or bound to professionalethics that guard against conflicted counsel or poor quality advice. Thisaccountability deficit led participants to advocate for AI systems to helpusers polish their legal questions and relevant facts, rather than recommendspecific actions. More generally, we highlight the potential of case-basedexpert deliberation as a method of responsibly translating professionalintegrity and domain knowledge into design requirements to inform appropriateAI behavior when generating advice in professional domains.\rTransFR: Transferable Federated Recommendation with Pre-trained Language Models\nHonglei Zhang He Liu Haoxuan Li Yidong Li\nabstract\rabstract: Federated recommendations (FRs), facilitating multiple local clients tocollectively learn a global model without disclosing user private data, haveemerged as a prevalent architecture for privacy-preserving recommendations. Inconventional FRs, a dominant paradigm is to utilize discrete identities torepresent users/clients and items, which are subsequently mapped todomain-specific embeddings to participate in model training. Despiteconsiderable performance, we reveal three inherent limitations that can not beignored in federated settings, i.e., non-transferability across domains,unavailability in cold-start settings, and potential privacy violations duringfederated training. To this end, we propose a transferable federatedrecommendation model with universal textual representations, TransFR, whichdelicately incorporates the general capabilities empowered by pre-trainedlanguage models and the personalized abilities by fine-tuning local privatedata. Specifically, it first learns domain-agnostic representations of items byexploiting pre-trained models with public textual corpora. To tailor forfederated recommendation, we further introduce an efficient federatedfine-tuning and a local training mechanism. This facilitates personalized localheads for each client by utilizing their private behavior data. Byincorporating pre-training and fine-tuning within FRs, it greatly improves theadaptation efficiency transferring to a new domain and the generalizationcapacity to address cold-start issues. Through extensive experiments on severaldatasets, we demonstrate that our TransFR model surpasses severalstate-of-the-art FRs in terms of accuracy, transferability, and privacy.\rDTS-SQL: Decomposed Text-to-SQL with Small Large Language Models\nMohammadreza Pourreza Davood Rafiei\nabstract\rabstract: Leading models for the text-to-SQL task heavily rely on proprietary LargeLanguage Models (LLMs), posing concerns over data privacy. Closing theperformance gap between small open-source models and large proprietary modelsis crucial to mitigate this reliance. To this end, we introduce a noveltwo-stage fine-tuning approach that decomposes the task into two simpler tasks.Through comprehensive evaluation on two large cross-domain datasets and twosmall LLMs, we show that this approach improves execution accuracy by 3 to 7percent, effectively aligning the performance of open-source models with theirproprietary counterparts.\r2024-02-01\nHR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent\nWeijie Xu Zicheng Huang Wenxiang Hu Xi Fang Rajesh Kumar Cherukuri Naumaan Nayyar Lorenzo Malandri Srinivasan H. Sengamedu\nabstract\rabstract: Recent advancements in Large Language Models (LLMs) have been reshapingNatural Language Processing (NLP) task in several domains. Their use in thefield of Human Resources (HR) has still room for expansions and could bebeneficial for several time consuming tasks. Examples such as time-offsubmissions, medical claims filing, and access requests are noteworthy, butthey are by no means the sole instances. However, the aforementioneddevelopments must grapple with the pivotal challenge of constructing ahigh-quality training dataset. On one hand, most conversation datasets aresolving problems for customers not employees. On the other hand, gatheringconversations with HR could raise privacy concerns. To solve it, we introduceHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HRdomains to evaluate LLM Agent. Our work has the following contributions: (1) Itis the first labeled open-sourced conversation dataset in the HR domain for NLPresearch. (2) It provides a detailed recipe for the data generation procedurealong with data analysis and human evaluations. The data generation pipeline istransferable and can be easily adapted for labeled conversation data generationin other domains. (3) The proposed data-collection pipeline is mostly based onLLMs with minimal human involvement for annotation, which is time andcost-efficient.\r2024-01-31\nDe-identification is not always enough\nAtiquer Rahman Sarkar Yao-Shun Chuang Noman Mohammed Xiaoqian Jiang\nabstract\rabstract: For sharing privacy-sensitive data, de-identification is commonly regarded asadequate for safeguarding privacy. Synthetic data is also being considered as aprivacy-preserving alternative. Recent successes with numerical and tabulardata generative models and the breakthroughs in large generative languagemodels raise the question of whether synthetically generated clinical notescould be a viable alternative to real notes for research purposes. In thiswork, we demonstrated that (i) de-identification of real clinical notes doesnot protect records against a membership inference attack, (ii) proposed anovel approach to generate synthetic clinical notes using the currentstate-of-the-art large language models, (iii) evaluated the performance of thesynthetically generated notes in a clinical domain task, and (iv) proposed away to mount a membership inference attack where the target model is trainedwith synthetic data. We observed that when synthetically generated notesclosely match the performance of real data, they also exhibit similar privacyconcerns to the real data. Whether other approaches to synthetically generatedclinical notes could offer better trade-offs and become a better alternative tosensitive real notes warrants further investigation.\rFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nZhen Qin Daoyuan Chen Bingchen Qian Bolin Ding Yaliang Li Shuiguang Deng\nabstract\rabstract: Pre-trained large language models (LLMs) need fine-tuning to improve theirresponsiveness to natural language instructions. Federated learning offers away to fine-tune LLMs using the abundant data on end devices withoutcompromising data privacy. Most existing federated fine-tuning methods for LLMsrely on parameter-efficient fine-tuning techniques, which may not reach theperformance height possible with full-parameter tuning. However, federatedfull-parameter tuning of LLMs is a non-trivial problem due to the immensecommunication cost. This work introduces FedKSeed that employs zeroth-orderoptimization with a finite set of random seeds. It significantly reducestransmission requirements between the server and clients to just a few randomseeds and scalar gradients, amounting to only a few thousand bytes, makingfederated full-parameter tuning of billion-sized LLMs possible on devices.Building on it, we develop a strategy enabling probability-differentiated seedsampling, prioritizing perturbations with greater impact on model accuracy.Experiments across six scenarios with various LLMs, datasets and datapartitions demonstrate that our approach outperforms existing federated LLMfine-tuning methods in both communication efficiency and new taskgeneralization.\rPrompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation\nLei Guo Ziang Lu Junliang Yu Nguyen Quoc Viet Hung Hongzhi Yin\nabstract\rabstract: Cross-domain Recommendation (CDR) as one of the effective techniques inalleviating the data sparsity issues has been widely studied in recent years.However, previous works may cause domain privacy leakage since they necessitatethe aggregation of diverse domain data into a centralized server during thetraining process. Though several studies have conducted privacy preserving CDRvia Federated Learning (FL), they still have the following limitations: 1) Theyneed to upload users\u0026rsquo; personal information to the central server, posing therisk of leaking user privacy. 2) Existing federated methods mainly rely onatomic item IDs to represent items, which prevents them from modeling items ina unified feature space, increasing the challenge of knowledge transfer amongdomains. 3) They are all based on the premise of knowing overlapped usersbetween domains, which proves impractical in real-world applications. Toaddress the above limitations, we focus on Privacy-preserving Cross-domainRecommendation (PCDR) and propose PFCR as our solution. For Limitation 1, wedevelop a FL schema by exclusively utilizing users\u0026rsquo; interactions with localclients and devising an encryption method for gradient encryption. ForLimitation 2, we model items in a universal feature space by their descriptiontexts. For Limitation 3, we initially learn federated content representations,harnessing the generality of natural language to establish bridges betweendomains. Subsequently, we craft two prompt fine-tuning strategies to tailor thepre-trained model to the target domain. Extensive experiments on two real-worlddatasets demonstrate the superiority of our PFCR method compared to the SOTAapproaches.\r2024-01-30\nAre ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?\nDimitrios Ioannidis Jeremy Kepner Andrew Bowne Harriet S. Bryant\nabstract\rabstract: The rise of Generative Artificial Intelligence systems (\u0026ldquo;AI systems\u0026rdquo;) hascreated unprecedented social engagement. AI code generation systems provideresponses (output) to questions or requests by accessing the vast library ofopen-source code created by developers over the past few decades. However, theydo so by allegedly stealing the open-source code stored in virtual libraries,known as repositories. This Article focuses on how this happens and whetherthere is a solution that protects innovation and avoids years of litigation. Wealso touch upon the array of issues raised by the relationship between AI andcopyright. Looking ahead, we propose the following: (a) immediate changes tothe licenses for open-source code created by developers that will limit accessand/or use of any open-source code to humans only; (b) we suggest revisions tothe Massachusetts Institute of Technology (\u0026ldquo;MIT\u0026rdquo;) license so that AI systemsare required to procure appropriate licenses from open-source code developers,which we believe will harmonize standards and build social consensus for thebenefit of all of humanity, rather than promote profit-driven centers ofinnovation; (c) we call for urgent legislative action to protect the future ofAI systems while also promoting innovation; and (d) we propose a shift in theburden of proof to AI systems in obfuscation cases.\rTrust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik\u0026rsquo;s Cube\nKausik Lakkaraju Vedant Khandelwal Biplav Srivastava Forest Agostinelli Hengtao Tang Prathamjeet Singh Dezhi Wu Matt Irvin Ashish Kundu\nabstract\rabstract: Artificial intelligence (AI) has the potential to transform education withits power of uncovering insights from massive data about student learningpatterns. However, ethical and trustworthy concerns of AI have been raised butare unsolved. Prominent ethical issues in high school AI education include dataprivacy, information leakage, abusive language, and fairness. This paperdescribes technological components that were built to address ethical andtrustworthy concerns in a multi-modal collaborative platform (called ALLUREchatbot) for high school students to collaborate with AI to solve the Rubik\u0026rsquo;scube. In data privacy, we want to ensure that the informed consent of children,parents, and teachers, is at the center of any data that is managed. Sincechildren are involved, language, whether textual, audio, or visual, isacceptable both from users and AI and the system can steer interaction awayfrom dangerous situations. In information management, we also want to ensurethat the system, while learning to improve over time, does not leak informationabout users from one group to another.\rA Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion\nGuangke Chen Yedi Zhang Fu Song Ting Wang Xiaoning Du Yang Liu\nabstract\rabstract: Singing voice conversion (SVC) automates song covers by converting onesinger\u0026rsquo;s singing voice into another target singer\u0026rsquo;s singing voice with theoriginal lyrics and melody. However, it raises serious concerns about copyrightand civil right infringements to multiple entities. This work proposesSongBsAb, the first proactive approach to mitigate unauthorized SVC-basedillegal song covers. SongBsAb introduces human-imperceptible perturbations tosinging voices before releasing them, so that when they are used, thegeneration process of SVC will be interfered, resulting in unexpected singingvoices. SongBsAb features a dual prevention effect by causing both (singer)identity disruption and lyric disruption, namely, the SVC-covered singing voiceneither imitates the target singer nor preserves the original lyrics. Toimprove the imperceptibility of perturbations, we refine a psychoacousticmodel-based loss with the backing track as an additional masker, a uniqueaccompanying element for singing voices compared to ordinary speech voices. Toenhance the transferability, we propose to utilize a frame-level interactionreduction-based loss. We demonstrate the prevention effectiveness, utility, androbustness of SongBsAb on three SVC models and two datasets using bothobjective and human study-based subjective metrics. Our work fosters anemerging research direction for mitigating illegal automated song covers.\rAalap: AI Assistant for Legal \u0026amp; Paralegal Functions in India\nAman Tiwari Prathamesh Kalamkar Atreyo Banerjee Saurabh Karn Varun Hemachandran Smita Gupta\nabstract\rabstract: Using proprietary Large Language Models on legal tasks poses challenges dueto data privacy issues, domain data heterogeneity, domain knowledgesophistication, and domain objectives uniqueness. We created Aalalp, afine-tuned Mistral 7B model on instructions data related to specific Indianlegal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31% ofour test data and obtains an equivalent score in 34% of the test data asevaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoningrather than legal recall. Aalap is definitely helpful for the day-to-dayactivities of lawyers, judges, or anyone working in legal systems.\rSecurity and Privacy Challenges of Large Language Models: A Survey\nBadhan Chandra Das M. Hadi Amini Yanzhao Wu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.\r2024-01-29\nTowards Building the Federated GPT: Federated Instruction Tuning\nJianyi Zhang Saeed Vahidian Martin Kuo Chunyuan Li Ruiyi Zhang Tong Yu Yufan Zhou Guoyin Wang Yiran Chen\nabstract\rabstract: While \u0026ldquo;instruction-tuned\u0026rdquo; generative large language models (LLMs) havedemonstrated an impressive ability to generalize to new tasks, the trainingphases heavily rely on large amounts of diverse and high-quality instructiondata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,especially when it comes to human-written data, can pose significant challengesboth in terms of cost and accessibility. Moreover, concerns related to privacycan further limit access to such data, making the process of obtaining it acomplex and nuanced undertaking. Consequently, this hinders the generality ofthe tuned models and may restrict their effectiveness in certain contexts. Totackle this issue, our study introduces a new approach called FederatedInstruction Tuning (FedIT), which leverages federated learning (FL) as thelearning framework for the instruction tuning of LLMs. This marks the firstexploration of FL-based instruction tuning for LLMs. This is especiallyimportant since text data is predominantly generated by end users. Therefore,it is imperative to design and adapt FL approaches to effectively leveragethese users\u0026rsquo; diverse instructions stored on local devices, while preservingprivacy and ensuring data security. In the current paper, by conducting widelyused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneousand diverse sets of instructions on the client\u0026rsquo;s end with the proposedframework FedIT, we improved the performance of LLMs compared to centralizedtraining with only limited local instructions. Further, in this paper, wedeveloped a Github repository named Shepherd. This repository offers afoundational framework for exploring federated fine-tuning of LLMs usingheterogeneous instructions across diverse categories.\rSSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning\nPeizhuo Lv Pan Li Shenchen Zhu Shengzhi Zhang Kai Chen Ruigang Liang Chang Yue Fan Xiang Yuling Cai Hualong Ma Yingjun Zhang Guozhu Meng\nabstract\rabstract: Recent years have witnessed tremendous success in Self-Supervised Learning(SSL), which has been widely utilized to facilitate various downstream tasks inComputer Vision (CV) and Natural Language Processing (NLP) domains. However,attackers may steal such SSL models and commercialize them for profit, makingit crucial to verify the ownership of the SSL models. Most existing ownershipprotection solutions (e.g., backdoor-based watermarks) are designed forsupervised learning models and cannot be used directly since they require thatthe models\u0026rsquo; downstream tasks and target labels be known and available duringwatermark embedding, which is not always possible in the domain of SSL. Toaddress such a problem, especially when downstream tasks are diverse andunknown during watermark embedding, we propose a novel black-box watermarkingsolution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM mapswatermarked inputs of the protected encoders into an invariant representationspace, which causes any downstream classifier to produce expected behavior,thus allowing the detection of embedded watermarks. We evaluate SSL-WM onnumerous tasks, such as CV and NLP, using different SSL models bothcontrastive-based and generative-based. Experimental results demonstrate thatSSL-WM can effectively verify the ownership of stolen SSL models in variousdownstream tasks. Furthermore, SSL-WM is robust against model fine-tuning,pruning, and input preprocessing attacks. Lastly, SSL-WM can also evadedetection from evaluated watermark detection approaches, demonstrating itspromising application in protecting the ownership of SSL models.\rImportance-Aware Adaptive Dataset Distillation\nGuang Li Ren Togo Takahiro Ogawa Miki Haseyama\nabstract\rabstract: Herein, we propose a novel dataset distillation method for constructing smallinformative datasets that preserve the information of the large originaldatasets. The development of deep learning models is enabled by theavailability of large-scale datasets. Despite unprecedented success,large-scale datasets considerably increase the storage and transmission costs,resulting in a cumbersome model training process. Moreover, using raw data fortraining raises privacy and copyright concerns. To address these issues, a newtask named dataset distillation has been introduced, aiming to synthesize acompact dataset that retains the essential information from the large originaldataset. State-of-the-art (SOTA) dataset distillation methods have beenproposed by matching gradients or network parameters obtained during trainingon real and synthetic datasets. The contribution of different networkparameters to the distillation process varies, and uniformly treating themleads to degraded distillation performance. Based on this observation, wepropose an importance-aware adaptive dataset distillation (IADD) method thatcan improve distillation performance by automatically assigning importanceweights to different network parameters during distillation, therebysynthesizing more robust distilled datasets. IADD demonstrates superiorperformance over other SOTA dataset distillation methods based on parametermatching on multiple benchmark datasets and outperforms them in terms ofcross-architecture generalization. In addition, the analysis of self-adaptiveweights demonstrates the effectiveness of IADD. Furthermore, the effectivenessof IADD is validated in a real-world medical application such as COVID-19detection.\r2024-01-28\nData-Free Generalized Zero-Shot Learning\nBowen Tang Long Yan Jing Zhang Qian Yu Lu Sheng Dong Xu\nabstract\rabstract: Deep learning models have the ability to extract rich knowledge fromlarge-scale datasets. However, the sharing of data has become increasinglychallenging due to concerns regarding data copyright and privacy. Consequently,this hampers the effective transfer of knowledge from existing data to noveldownstream tasks and concepts. Zero-shot learning (ZSL) approaches aim torecognize new classes by transferring semantic knowledge learned from baseclasses. However, traditional generative ZSL methods often require access toreal images from base classes and rely on manually annotated attributes, whichpresents challenges in terms of data restrictions and model scalability. Tothis end, this paper tackles a challenging and practical problem dubbed asdata-free zero-shot learning (DFZSL), where only the CLIP-based base classesdata pre-trained classifier is available for zero-shot classification.Specifically, we propose a generic framework for DFZSL, which consists of threemain components. Firstly, to recover the virtual features of the base data, wemodel the CLIP features of base class images as samples from a von Mises-Fisher(vMF) distribution based on the pre-trained classifier. Secondly, we leveragethe text features of CLIP as low-cost semantic information and propose afeature-language prompt tuning (FLPT) method to further align the virtual imagefeatures and textual features. Thirdly, we train a conditional generative modelusing the well-aligned virtual image features and corresponding semantic textfeatures, enabling the generation of new classes features and achieve betterzero-shot generalization. Our framework has been evaluated on five commonlyused benchmarks for generalized ZSL, as well as 11 benchmarks for thebase-to-new ZSL. The results demonstrate the superiority and effectiveness ofour approach. Our code is available in https://github.com/ylong4/DFZSL\rAI as a Medical Ally: Evaluating ChatGPT\u0026rsquo;s Usage and Impact in Indian Healthcare\nAryaman Raina Prateek Mishra Harshit goyal Dhruv Kumar\nabstract\rabstract: This study investigates the integration and impact of Large Language Models(LLMs), like ChatGPT, in India\u0026rsquo;s healthcare sector. Our research employs a dualapproach, engaging both general users and medical professionals through surveysand interviews respectively. Our findings reveal that healthcare professionalsvalue ChatGPT in medical education and preliminary clinical settings, butexercise caution due to concerns about reliability, privacy, and the need forcross-verification with medical references. General users show a preference forAI interactions in healthcare, but concerns regarding accuracy and trustpersist. The study underscores the need for these technologies to complement,not replace, human medical expertise, highlighting the importance of developingLLMs in collaboration with healthcare providers. This paper enhances theunderstanding of LLMs in healthcare, detailing current usage, user trust, andimprovement areas. Our insights inform future research and development,underscoring the need for ethically compliant, user-focused LLM advancementsthat address healthcare-specific challenges.\rPrivacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\nXinyu Tang Richard Shin Huseyin A. Inan Andre Manoel Fatemehsadat Mireshghallah Zinan Lin Sivakanth Gopi Janardhan Kulkarni Robert Sim\nabstract\rabstract: We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.\r2024-01-27\nDataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure\nJunyi Ye Mengnan Du Guiling Wang\nabstract\rabstract: This paper introduces DataFrame question answering (QA), a novel task thatutilizes large language models (LLMs) to generate Pandas queries forinformation retrieval and data analysis on dataframes, emphasizing safe andnon-revealing data handling. Our method, which solely relies on dataframecolumn names, not only ensures data privacy but also significantly reduces thecontext window in the prompt, streamlining information processing andaddressing major challenges in LLM-based data analysis. We propose DataFrame QAas a comprehensive framework that includes safe Pandas query generation andcode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1metric on the renowned WikiSQL and our newly developed \u0026lsquo;UCI-DataFrameQA\u0026rsquo;,tailored for complex data analysis queries. Our findings indicate that GPT-4achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,underscoring its capability in securely retrieving and aggregating dataframevalues and conducting sophisticated data analyses. This approach, deployable ina zero-shot manner without prior training or adjustments, proves to be highlyadaptable and secure for diverse applications.\rFortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models\nYunhong He Jianling Qiu Wei Zhang Zhengqing Yuan\nabstract\rabstract: Recent advancements in large language models (LLMs) have significantlyenhanced capabilities in natural language processing and artificialintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionizedtext generation, translation, and question-answering tasks due to thetransformative Transformer model. Despite their widespread use, LLMs presentchallenges such as ethical dilemmas when models are compelled to respondinappropriately, susceptibility to phishing attacks, and privacy violations.This paper addresses these challenges by introducing a multi-pronged approachthat includes: 1) filtering sensitive vocabulary from user input to preventunethical responses; 2) detecting role-playing to halt interactions that couldlead to \u0026lsquo;prison break\u0026rsquo; scenarios; 3) implementing custom rule engines torestrict the generation of prohibited content; and 4) extending thesemethodologies to various LLM derivatives like Multi-Model Large Language Models(MLLMs). Our approach not only fortifies models against unethical manipulationsand privacy breaches but also maintains their high performance across tasks. Wedemonstrate state-of-the-art performance under various attack prompts, withoutcompromising the model\u0026rsquo;s core functionalities. Furthermore, the introduction ofdifferentiated security levels empowers users to control their personal datadisclosure. Our methods contribute to reducing social risks and conflictsarising from technological abuse, enhance data protection, and promote socialequity. Collectively, this research provides a framework for balancing theefficiency of question-answering systems with user privacy and ethicalstandards, ensuring a safer user experience and fostering trust in AItechnology.\r2024-01-26\nAM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning\nNaresh Kumar Devulapally Sidharth Anand Sreyasee Das Bhattacharjee Junsong Yuan\nabstract\rabstract: Human emotion can be presented in different modes i.e., audio, video, andtext. However, the contribution of each mode in exhibiting each emotion is notuniform. Furthermore, the availability of complete mode-specific details maynot always be guaranteed in the test time. In this work, we propose AM^2-EmoJE,a model for Adaptive Missing-Modality Emotion Recognition in Conversation viaJoint Embedding Learning model that is grounded on two-fold contributions:First, a query adaptive fusion that can automatically learn the relativeimportance of its mode-specific representations in a query-specific manner. Bythis the model aims to prioritize the mode-invariant spatial query details ofthe emotion patterns, while also retaining its mode-exclusive aspects withinthe learned multimodal query descriptor. Second the multimodal joint embeddinglearning module that explicitly addresses various missing modality scenarios intest-time. By this, the model learns to emphasize on the correlated patternsacross modalities, which may help align the cross-attended mode-specificdescriptors pairwise within a joint-embedding space and thereby compensate formissing modalities during inference. By leveraging the spatio-temporal detailsat the dialogue level, the proposed AM^2-EmoJE not only demonstrates superiorperformance compared to the best-performing state-of-the-art multimodalmethods, by effectively leveraging body language in place of face expression,it also exhibits an enhanced privacy feature. By reporting around 2-5%improvement in the weighted-F1 score, the proposed multimodal joint embeddingmodule facilitates an impressive performance gain in a variety ofmissing-modality query scenarios during test time.\rA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly\nYifan Yao Jinhao Duan Kaidi Xu Yuanfang Cai Zhibo Sun Yue Zhang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into \u0026ldquo;The Good\u0026rdquo; (beneficial LLM applications),\u0026ldquo;The Bad\u0026rdquo; (offensive applications), and \u0026ldquo;The Ugly\u0026rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs\u0026rsquo; potential to both bolster and jeopardize cybersecurity.\r2024-01-25\nTrustLLM: Trustworthiness in Large Language Models\nLichao Sun Yue Huang Haoran Wang Siyuan Wu Qihui Zhang Chujie Gao Yixin Huang Wenhan Lyu Yixuan Zhang Xiner Li Zhengliang Liu Yixin Liu Yijue Wang Zhikun Zhang Bhavya Kailkhura Caiming Xiong Chaowei Xiao Chunyuan Li Eric Xing Furong Huang Hao Liu Heng Ji Hongyi Wang Huan Zhang Huaxiu Yao Manolis Kellis Marinka Zitnik Meng Jiang Mohit Bansal James Zou Jian Pei Jian Liu Jianfeng Gao Jiawei Han Jieyu Zhao Jiliang Tang Jindong Wang John Mitchell Kai Shu Kaidi Xu Kai-Wei Chang Lifang He Lifu Huang Michael Backes Neil Zhenqiang Gong Philip S. Yu Pin-Yu Chen Quanquan Gu Ran Xu Rex Ying Shuiwang Ji Suman Jana Tianlong Chen Tianming Liu Tianyi Zhou William Wang Xiang Li Xiangliang Zhang Xiao Wang Xing Xie Xun Chen Xuyu Wang Yan Liu Yanfang Ye Yinzhi Cao Yong Chen Yue Zhao\nabstract\rabstract: Large language models (LLMs), exemplified by ChatGPT, have gainedconsiderable attention for their excellent natural language processingcapabilities. Nonetheless, these LLMs present many challenges, particularly inthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMsemerges as an important topic. This paper introduces TrustLLM, a comprehensivestudy of trustworthiness in LLMs, including principles for different dimensionsof trustworthiness, established benchmark, evaluation, and analysis oftrustworthiness for mainstream LLMs, and discussion of open challenges andfuture directions. Specifically, we first propose a set of principles fortrustworthy LLMs that span eight different dimensions. Based on theseprinciples, we further establish a benchmark across six dimensions includingtruthfulness, safety, fairness, robustness, privacy, and machine ethics. Wethen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting ofover 30 datasets. Our findings firstly show that in general trustworthiness andutility (i.e., functional effectiveness) are positively related. Secondly, ourobservations reveal that proprietary LLMs generally outperform most open-sourcecounterparts in terms of trustworthiness, raising concerns about the potentialrisks of widely accessible open-source LLMs. However, a few open-source LLMscome very close to proprietary ones. Thirdly, it is important to note that someLLMs may be overly calibrated towards exhibiting trustworthiness, to the extentthat they compromise their utility by mistakenly treating benign prompts asharmful and consequently not responding. Finally, we emphasize the importanceof ensuring transparency not only in the models themselves but also in thetechnologies that underpin trustworthiness. Knowing the specific trustworthytechnologies that have been employed is crucial for analyzing theireffectiveness.\rLLM on FHIR \u0026ndash; Demystifying Health Records\nPaul Schmiedmayer Adrit Rao Philipp Zagar Vishnu Ravi Aydin Zahedivash Arash Fereydooni Oliver Aalami\nabstract\rabstract: Objective: To enhance health literacy and accessibility of health informationfor a diverse patient population by developing a patient-centered artificialintelligence (AI) solution using large language models (LLMs) and FastHealthcare Interoperability Resources (FHIR) application programming interfaces(APIs). Materials and Methods: The research involved developing LLM on FHIR, anopen-source mobile application allowing users to interact with their healthrecords using LLMs. The app is built on Stanford\u0026rsquo;s Spezi ecosystem and usesOpenAI\u0026rsquo;s GPT-4. A pilot study was conducted with the SyntheticMass patientdataset and evaluated by medical experts to assess the app\u0026rsquo;s effectiveness inincreasing health literacy. The evaluation focused on the accuracy, relevance,and understandability of the LLM\u0026rsquo;s responses to common patient questions.Results: LLM on FHIR demonstrated varying but generally high degrees ofaccuracy and relevance in providing understandable health information topatients. The app effectively translated medical data into patient-friendlylanguage and was able to adapt its responses to different patient profiles.However, challenges included variability in LLM responses and the need forprecise filtering of health data. Discussion and Conclusion: LLMs offersignificant potential in improving health literacy and making health recordsmore accessible. LLM on FHIR, as a pioneering application in this field,demonstrates the feasibility and challenges of integrating LLMs into patientcare. While promising, the implementation and pilot also highlight risks suchas inconsistent responses and the importance of replicable output. Futuredirections include better resource identification mechanisms and executing LLMson-device to enhance privacy and reduce costs.\rLinear Programs with Conjunctive Database Queries\nFlorent Capelli Nicolas Crosetti Joachim Niehren Jan Ramon\nabstract\rabstract: In this paper, we study the problem of optimizing a linear program whosevariables are the answers to a conjunctive query. For this we propose thelanguage LP(CQ) for specifying linear programs whose constraints and objectivefunctions depend on the answer sets of conjunctive queries. We contribute anefficient algorithm for solving programs in a fragment of LP(CQ). The naturalapproach constructs a linear program having as many variables as there areelements in the answer set of the queries. Our approach constructs a linearprogram having the same optimal value but fewer variables. This is done byexploiting the structure of the conjunctive queries using generalized hypertreedecompositions of small width to factorize elements of the answer set together.We illustrate the various applications of LP(CQ) programs on three examples:optimizing deliveries of resources, minimizing noise for differential privacy,and computing the s-measure of patterns in graphs as needed for data mining.\rTrust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem\nJ. Carbo J. M. Molina\nabstract\rabstract: In this paper we propose a trust model to be used into a hypothetical mixedenvironment where humans and unmanned vehicles cooperate. We address theinclusion of emotions inside a trust model in a coherent way to the practicalapproaches to the current psychology theories. The most innovative contributionis how privacy issues play a role in the cooperation decisions of the emotionaltrust model. Both, emotions and trust have been cognitively modeled and managedwith the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agentsimplemented in GAML (the programming language of GAMA agent platform) thatcommunicates using the IEEE FIPA standard. The trusting behaviour of theseemotional agents is tested in a cooperative logistics problem where: agentshave to move objects to destinations and some of the objects and places haveprivacy issues. The execution of simulations of this logistic problem shows howemotions and trust contribute to improve the performance of agents in terms ofboth, time savings and privacy protection\r2024-01-24\nEmbedding Attack Project (Work Report)\nJiameng Pu Zafar Takhirov\nabstract\rabstract: This report summarizes all the MIA experiments (Membership Inference Attacks)of the Embedding Attack Project, including threat models, experimental setup,experimental results, findings and discussion. Current results cover theevaluation of two main MIA strategies (loss-based and embedding-based MIAs) on6 AI models ranging from Computer Vision to Language Modelling. There are twoongoing experiments on MIA defense and neighborhood-comparison embeddingattacks. These are ongoing projects. The current work on MIA and PIA can be summarized into six conclusions: (1)Amount of overfitting is directly proportional to model\u0026rsquo;s vulnerability; (2)early embedding layers in the model are less susceptible to privacy leaks; (3)Deeper model layers contain more membership information; (4) Models are morevulnerable to MIA if both embeddings and corresponding training labels arecompromised; (5) it is possible to use pseudo-labels to increase the MIAsuccess; and (6) although MIA and PIA success rates are proportional, reducingthe MIA does not necessarily reduce the PIA.\r2024-01-23\nMitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement\nChenghao Li Dake Chen Yuke Zhang Peter A. Beerel\nabstract\rabstract: While diffusion models demonstrate a remarkable capability for generatinghigh-quality images, their tendency to `replicate\u0026rsquo; training data raises privacyconcerns. Although recent research suggests that this replication may stem fromthe insufficient generalization of training data captions and duplication oftraining images, effective mitigation strategies remain elusive. To addressthis gap, our paper first introduces a generality score that measures thecaption generality and employ large language model (LLM) to generalize trainingcaptions. Subsequently, we leverage generalized captions and propose a noveldual fusion enhancement approach to mitigate the replication of diffusionmodels. Our empirical results demonstrate that our proposed methods cansignificantly reduce replication by 43.5% compared to the original diffusionmodel while maintaining the diversity and quality of generations. Code isavailable at https://github.com/HowardLi0816/dual-fusion-diffusion.\rRed Teaming Visual Language Models\nMukai Li Lei Li Yuwei Yin Masood Ahmed Zhenguang Liu Qi Liu\nabstract\rabstract: VLMs (Vision-Language Models) extend the capabilities of LLMs (Large LanguageModels) to accept multimodal inputs. Since it has been verified that LLMs canbe induced to generate harmful or inaccurate content through specific testcases (termed as Red Teaming), how VLMs perform in similar scenarios,especially with their combination of textual and visual inputs, remains aquestion. To explore this problem, we present a novel red teaming datasetRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modaljail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,privacy, safety, fairness). Our RTVLM is the first red-teaming dataset tobenchmark current VLMs in terms of these 4 different aspects. Detailed analysisshows that 10 prominent open-sourced VLMs struggle with the red teaming indifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning(SFT) using RTVLM, and this bolsters the models\u0026rsquo; performance with 10% in RTVLMtest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,overpassing other LLaVA-based models with regular alignment data. This revealsthat current open-sourced VLMs still lack red teaming alignment. Our code anddatasets will be open-source.\rA Survey of Text Watermarking in the Era of Large Language Models\nAiwei Liu Leyi Pan Yijian Lu Jingjing Li Xuming Hu Xi Zhang Lijie Wen Irwin King Hui Xiong Philip S. Yu\nabstract\rabstract: Text watermarking algorithms play a crucial role in the copyright protectionof textual content, yet their capabilities and application scenarios have beenlimited historically. The recent developments in large language models (LLMs)have opened new opportunities for the advancement of text watermarkingtechniques. LLMs not only enhance the capabilities of text watermarkingalgorithms through their text understanding and generation abilities but alsonecessitate the use of text watermarking algorithms for their own copyrightprotection. This paper conducts a comprehensive survey of the current state oftext watermarking technology, covering four main aspects: (1) an overview andcomparison of different text watermarking techniques; (2) evaluation methodsfor text watermarking algorithms, including their success rates, impact on textquality, robustness, and unforgeability; (3) potential application scenariosfor text watermarking technology; (4) current challenges and future directionsfor development. This survey aims to provide researchers with a thoroughunderstanding of text watermarking technology, thereby promoting its furtheradvancement.\r\u0026ldquo;The teachers are confused as well\u0026rdquo;: A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education\nKyrie Zhixuan Zhou Zachary Kilhoffer Madelyn Rose Sanfilippo Ted Underwood Ece Gumusel Mengyi Wei Abhinav Choudhry Jinjun Xiong\nabstract\rabstract: Large Language Models (LLMs) are advancing quickly and impacting people\u0026rsquo;slives for better or worse. In higher education, concerns have emerged such asstudents\u0026rsquo; misuse of LLMs and degraded education outcomes. To unpack the ethicalconcerns of LLMs for higher education, we conducted a case study consisting ofstakeholder interviews (n=20) in higher education computer science. We foundthat students use several distinct mental models to interact with LLMs - LLMsserve as a tool for (a) writing, (b) coding, and (c) information retrieval,which differ somewhat in ethical considerations. Students and teachers broughtup ethical issues that directly impact them, such as inaccurate LLM responses,hallucinations, biases, privacy leakage, and academic integrity issues.Participants emphasized the necessity of guidance and rules for the use of LLMsin higher education, including teaching digital literacy, rethinking education,and having cautious and contextual policies. We reflect on the ethicalchallenges and propose solutions.\r2024-01-22\nThe Ethics of Interaction: Mitigating Security Threats in LLMs\nAshutosh Kumar Sagarika Singh Shiv Vignesh Murty Swathy Ragupathy\nabstract\rabstract: This paper comprehensively explores the ethical challenges arising fromsecurity threats to Language Learning Models (LLMs). These intricate digitalrepositories are increasingly integrated into our daily lives, making themprime targets for attacks that can compromise their training data and theconfidentiality of their data sources. The paper delves into the nuancedethical repercussions of such security threats on society and individualprivacy. We scrutinize five major threats: prompt injection, jailbreaking,Personal Identifiable Information (PII) exposure, sexually explicit content,and hate based content, going beyond mere identification to assess theircritical ethical consequences and the urgency they create for robust defensivestrategies. The escalating reliance on LLMs underscores the crucial need forensuring these systems operate within the bounds of ethical norms, particularlyas their misuse can lead to significant societal and individual harm. Wepropose conceptualizing and developing an evaluative tool tailored for LLMs,which would serve a dual purpose, guiding developers and designers inpreemptive fortification of backend systems and scrutinizing the ethicaldimensions of LLM chatbot responses during the testing phase. By comparing LLMresponses with those expected from humans in a moral context, we aim to discernthe degree to which AI behaviors align with the ethical values held by abroader society. Ultimately, this paper not only underscores the ethicaltroubles presented by LLMs, it also highlights a path toward cultivating trustin these systems.\rTransformers with Attentive Federated Aggregation for Time Series Stock Forecasting\nChu Myaet Thwal Ye Lin Tun Kitae Kim Seong-Bae Park Choong Seon Hong\nabstract\rabstract: Recent innovations in transformers have shown their superior performance innatural language processing (NLP) and computer vision (CV). The ability tocapture long-range dependencies and interactions in sequential data has alsotriggered a great interest in time series modeling, leading to the widespreaduse of transformers in many time series applications. However, being the mostcommon and crucial application, the adaptation of transformers to time seriesforecasting has remained limited, with both promising and inconsistent results.In contrast to the challenges in NLP and CV, time series problems not only addthe complexity of order or temporal dependence among input sequences but alsoconsider trend, level, and seasonality information that much of this data isvaluable for decision making. The conventional training scheme has showndeficiencies regarding model overfitting, data scarcity, and privacy issueswhen working with transformers for a forecasting task. In this work, we proposeattentive federated transformers for time series stock forecasting with betterperformance while preserving the privacy of participating enterprises.Empirical results on various stock data from the Yahoo! Finance websiteindicate the superiority of our proposed scheme in dealing with the abovechallenges and data heterogeneity in federated learning.\r2024-01-21\nInstructional Fingerprinting of Large Language Models\nJiashu Xu Fei Wang Mingyu Derek Ma Pang Wei Koh Chaowei Xiao Muhao Chen\nabstract\rabstract: The exorbitant cost of training Large language models (LLMs) from scratchmakes it essential to fingerprint the models to protect intellectual propertyvia ownership authentication and to ensure downstream users and developerscomply with their license terms (e.g. restricting commercial use). In thisstudy, we present a pilot study on LLM fingerprinting as a form of verylightweight instruction tuning. Model publisher specifies a confidentialprivate key and implants it as an instruction backdoor that causes the LLM togenerate specific text when the key is present. Results on 11 popularly-usedLLMs showed that this approach is lightweight and does not affect the normalbehavior of the model. It also prevents publisher overclaim, maintainsrobustness against fingerprint guessing and parameter-efficient training, andsupports multi-stage fingerprinting akin to MIT License. Code is available inhttps://cnut1648.github.io/Model-Fingerprint/.\r2024-01-20\nDeception and Manipulation in Generative AI\nChristian Tarsney\nabstract\rabstract: Large language models now possess human-level linguistic abilities in manycontexts. This raises the concern that they can be used to deceive andmanipulate on unprecedented scales, for instance spreading politicalmisinformation on social media. In future, agentic AI systems might alsodeceive and manipulate humans for their own ends. In this paper, first, I arguethat AI-generated content should be subject to stricter standards againstdeception and manipulation than we ordinarily apply to humans. Second, I offernew characterizations of AI deception and manipulation meant to support suchstandards, according to which a statement is deceptive (manipulative) if itleads human addressees away from the beliefs (choices) they would endorse under``semi-ideal\u0026rsquo;\u0026rsquo; conditions. Third, I propose two measures to guard against AIdeception and manipulation, inspired by this characterization: \u0026ldquo;extremetransparency\u0026rdquo; requirements for AI-generated content and defensive systems that,among other things, annotate AI-generated statements with contextualizinginformation. Finally, I consider to what extent these measures can protectagainst deceptive behavior in future, agentic AIs, and argue that non-agenticdefensive systems can provide an important layer of defense even against morepowerful agentic systems.\rFwdLLM: Efficient FedLLM using Forward Gradient\nMengwei Xu Dongqi Cai Yaozong Wu Xiang Li Shangguang Wang\nabstract\rabstract: Large Language Models (LLMs) are transforming the landscape of mobileintelligence. Federated Learning (FL), a method to preserve user data privacy,is often employed in fine-tuning LLMs to downstream mobile tasks, an approachknown as FedLLM. Though recent efforts have addressed the network issue inducedby the vast model size, they have not practically mitigated vital challengesconcerning integration with mobile devices, such as significant memoryconsumption and sluggish model convergence. In response to these challenges, this work introduces FwdLLM, an innovativeFL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLMto employ backpropagation (BP)-free training methods, requiring devices only toexecute ``perturbed inferences\u0026rsquo;\u0026rsquo;. Consequently, FwdLLM delivers way bettermemory efficiency and time efficiency (expedited by mobile NPUs and an expandedarray of participant devices). FwdLLM centers around three key designs: (1) itcombines BP-free training with parameter-efficient training methods, anessential way to scale the approach to the LLM era; (2) it systematically andadaptively allocates computational loads across devices, striking a carefulbalance between convergence speed and accuracy; (3) it discriminatively samplesperturbed predictions that are more valuable to model convergence.Comprehensive experiments with five LLMs and three NLP tasks illustrateFwdLLM\u0026rsquo;s significant advantages over conventional methods, including up tothree orders of magnitude faster convergence and a 14.6x reduction in memoryfootprint. Uniquely, FwdLLM paves the way for federated learning ofbillion-parameter LLMs such as LLaMA on COTS mobile devices \u0026ndash; a featpreviously unattained.\r2024-01-18\nCase Study: Securing MMU-less Linux Using CHERI\nHesham Almatary Alfredo Mazzinghi Robert N. M. Watson\nabstract\rabstract: MMU-less Linux variant lacks security because it does not have protection orisolation mechanisms. It also does not use MPUs as they do not fit with itssoftware model because of the design drawbacks of MPUs (\\ie coarse-grainedprotection with fixed number of protected regions). We secure the existingMMU-less Linux version of the RISC-V port using CHERI. CHERI is ahardware-software capability-based system that extends the ISA, toolchain,programming languages, operating systems, and applications in order to providecomplete pointer and memory safety. We believe that CHERI could providesignificant security guarantees for high-end dynamic MMU-less embedded systemsat lower costs, compared to MMUs and MPUs, by: 1) building the entire softwarestack in pure-capability CHERI C mode which provides complete spatial memorysafety at the kernel and user-level, 2) isolating user programs as separateELFs, each with its own CHERI-based capability table; this provides spatialmemory safety similar to what the MMU offers (\\ie user programs cannot accesseach other\u0026rsquo;s memory), 3) isolating user programs from the kernel as the kernelhas its own capability table from the users and vice versa, and 4)compartmentalising kernel modules using CompartOS\u0026rsquo; linkage-basedcompartmentalisation. This offers a new security front that is not possibleusing the current MMU-based Linux, where vulnerable/malicious kernel modules(\\eg device drivers) executing in the kernel space would not compromise or takedown the entire system. These are the four main contributions of this paper,presenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.\rSilent Guardian: Protecting Text from Malicious Exploitation by Large Language Models\nJiawei Zhao Kejiang Chen Xiaojian Yuan Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.\rMeme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations\nPrince Jha Krishanu Maity Raghav Jain Apoorv Verma Sriparna Saha Pushpak Bhattacharyya\nabstract\rabstract: Internet memes have gained significant influence in communicating political,psychological, and sociocultural ideas. While memes are often humorous, therehas been a rise in the use of memes for trolling and cyberbullying. Although awide variety of effective deep learning-based models have been developed fordetecting offensive multimodal memes, only a few works have been done onexplainability aspect. Recent laws like \u0026ldquo;right to explanations\u0026rdquo; of General DataProtection Regulation, have spurred research in developing interpretable modelsrather than only focusing on performance. Motivated by this, we introduce {\\emMultiBully-Ex}, the first benchmark dataset for multimodal explanation fromcode-mixed cyberbullying memes. Here, both visual and textual modalities arehighlighted to explain why a given meme is cyberbullying. A ContrastiveLanguage-Image Pretraining (CLIP) projection-based multimodal shared-privatemultitask approach has been proposed for visual and textual explanation of ameme. Experimental results demonstrate that training with multimodalexplanations improves performance in generating textual justifications and moreaccurately identifying the visual evidence supporting a decision with reliableperformance improvements.\rMISS: A Generative Pretraining and Finetuning Approach for Med-VQA\nJiawei Chen Dingkang Yang Yue Jiang Yuxuan Lei Lihua Zhang\nabstract\rabstract: Medical visual question answering (VQA) is a challenging multimodal task,where Vision-Language Pre-training (VLP) models can effectively improve thegeneralization performance. However, most methods in the medical field treatVQA as an answer classification task which is difficult to transfer topractical application scenarios. Additionally, due to the privacy of medicalimages and the expensive annotation process, large-scale medical image-textpairs datasets for pretraining are severely lacking. In this paper, we proposea large-scale MultI-task Self-Supervised learning based framework (MISS) formedical VQA tasks. Unlike existing methods, we treat medical VQA as agenerative task. We unify the text encoder and multimodal encoder and alignimage-text features through multi-task learning. Furthermore, we propose aTransfer-and-Caption method that extends the feature space of single-modalimage datasets using large language models (LLMs), enabling those traditionalmedical vision field task data to be applied to VLP. Experiments show that ourmethod achieves excellent results with fewer multimodal datasets anddemonstrates the advantages of generative VQA models. The code and modelweights will be released upon the paper\u0026rsquo;s acceptance.\rTowards a Responsible AI Metrics Catalogue: A Collection of Metrics for AI Accountability\nBoming Xia Qinghua Lu Liming Zhu Sung Une Lee Yue Liu Zhenchang Xing\nabstract\rabstract: Artificial Intelligence (AI), particularly through the advent of large-scalegenerative AI (GenAI) models such as Large Language Models (LLMs), has become atransformative element in contemporary technology. While these models haveunlocked new possibilities, they simultaneously present significant challenges,such as concerns over data privacy and the propensity to generate misleading orfabricated content. Current frameworks for Responsible AI (RAI) often fallshort in providing the granular guidance necessary for tangible application,especially for Accountability-a principle that is pivotal for ensuringtransparent and auditable decision-making, bolstering public trust, and meetingincreasing regulatory expectations. This study bridges the accountability gapby introducing our effort towards a comprehensive metrics catalogue, formulatedthrough a systematic multivocal literature review (MLR) that integratesfindings from both academic and grey literature. Our catalogue delineatesprocess metrics that underpin procedural integrity, resource metrics thatprovide necessary tools and frameworks, and product metrics that reflect theoutputs of AI systems. This tripartite framework is designed to operationalizeAccountability in AI, with a special emphasis on addressing the intricacies ofGenAI.\r2024-01-17\nCharacterizing Online Eating Disorder Communities with Large Language Models\nMinh Duc Chu Aryan Karnati Zihao He Kristina Lerman\nabstract\rabstract: The rise in eating disorders, a dangerous mental health condition with highmortality and morbidity, has been linked to the proliferation of idealized bodyimages on social media. However, the link between social media and eatingdisorders is far more complex. We argue that social media platforms create afeedback loop that amplifies the growth of content and communities that promoteeating disorders like anorexia and bulimia. Specifically, social mediaplatforms make it easy for vulnerable individuals to find and connect tolike-minded others, while group dynamic processes encourage them to stayengaged within communities that promote and glorify harmful behaviors linked toeating disorders. We characterize this dynamic empirically through acombination of network and language analysis. We describe a novel frameworkthat leverages large language models to analyze the discourse within onlinecommunities and probe their attitudes on topics related to eating disorders toidentify potentially harmful content. Our work emphasizes the need for bettersocial media moderation to disrupt harmful feedback loops and protectvulnerable individuals.\rThe Mikado Filesystem: An experimental RPC filesystem running over gRPC\nJohn D. Dougrez-Lewis\nabstract\rabstract: Computer applications seeking to persist files remotely across the Internetare faced with a bewildering choice of mechanisms which tend to boil down tomonolithic proprietary closed-source Vendor solutions. We introduce The MikadoFilesystem (mikfs), which provides an open simple lightweight interoperableportable extensible remote filesystem that is open source. mikfs consists ofclient applications accessing remote servers via RPC running over TCP/IPconnections. mikfs is defined as a concrete set of API method calls over gRPCexpressed in Google\u0026rsquo;s Protocol Buffers\u0026rsquo; IDL. gRPC supports a wide variety ofprogramming languages \u0026amp; platforms. For a given language + platform, the gRPCtoolset can generate client- \u0026amp; server-side stubs from the IDL callable fromclient \u0026amp; server code in the selected languages, e.g., a client written in C# orjava running on a Windows PC can access a server written in C++ running onLinux. mikfs consists of a virtual hierarchical tree of files \u0026amp; directories.This logical filesystem is not constrained to the limits and file namingconventions of the host\u0026rsquo;s own physical native filesystem. API methods areprovided for authentication; for atomic file-level operations on files \u0026amp;directories; for clients to register to receive notifications of file \u0026amp;directory changes on a server. The public API allows developers to write theirown new servers and clients; allowing migration of hosted files betweendifferent implementations; extension with new methods \u0026amp; features; is OpenSource code available for inspection and adaptation. gRPC provides secureauthenticated connection \u0026amp; communication over HTTP/2; End-to-End Privacy \u0026amp;Security against eavesdropping of data in transit; support for multiplealternate user login mechanisms. mikfs is provided as source code, \u0026lsquo;TheBootstrap Distribution\u0026rsquo;, consisting of an ecosystem of clients, servers, toolsand utilities.\rExplain Thyself Bully: Sentiment Aided Cyberbullying Detection with Explanation\nKrishanu Maity Prince Jha Raghav Jain Sriparna Saha Pushpak Bhattacharyya\nabstract\rabstract: Cyberbullying has become a big issue with the popularity of different socialmedia networks and online communication apps. While plenty of research is goingon to develop better models for cyberbullying detection in monolinguallanguage, there is very little research on the code-mixed languages andexplainability aspect of cyberbullying. Recent laws like \u0026ldquo;right toexplanations\u0026rdquo; of General Data Protection Regulation, have spurred research indeveloping interpretable models rather than focusing on performance. Motivatedby this we develop the first interpretable multi-task model called {\\em mExCB}for automatic cyberbullying detection from code-mixed languages which cansimultaneously solve several tasks, cyberbullying detection,explanation/rationale identification, target group detection and sentimentanalysis. We have introduced {\\em BullyExplain}, the first benchmark datasetfor explainable cyberbullying detection in code-mixed language. Each post in{\\em BullyExplain} dataset is annotated with four labels, i.e., {\\em bullylabel, sentiment label, target and rationales (explainability)}, i.e., whichphrases are being responsible for annotating the post as a bully. The proposedmultitask framework (mExCB) based on CNN and GRU with word and sub-sentence(SS) level attention is able to outperform several baselines and state of theart models when applied on {\\em BullyExplain} dataset.\rMobileAgent: enhancing mobile control via human-machine interaction and SOP integration\nTinghe Ding\nabstract\rabstract: Agents centered around Large Language Models (LLMs) are now capable ofautomating mobile device operations for users. After fine-tuning to learn auser\u0026rsquo;s mobile operations, these agents can adhere to high-level userinstructions online. They execute tasks such as goal decomposition, sequencingof sub-goals, and interactive environmental exploration, until the finalobjective is achieved. However, privacy concerns related to personalized userdata arise during mobile operations, requiring user confirmation. Moreover,users\u0026rsquo; real-world operations are exploratory, with action data being complexand redundant, posing challenges for agent learning. To address these issues,in our practical application, we have designed interactive tasks between agentsand humans to identify sensitive information and align with personalized userneeds. Additionally, we integrated Standard Operating Procedure (SOP)information within the model\u0026rsquo;s in-context learning to enhance the agent\u0026rsquo;scomprehension of complex task execution. Our approach is evaluated on the newdevice control benchmark AitW, which encompasses 30K unique instructions acrossmulti-step tasks, including application operation, web searching, and webshopping. Experimental results show that the SOP-based agent achievesstate-of-the-art performance in LLMs without incurring additional inferencecosts, boasting an overall action success rate of 66.92%. The code and dataexamples are available at https://github.com/alipay/mobile-agent.\rEfficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR\nJunwen Bai Bo Li Qiujia Li Tara N. Sainath Trevor Strohman\nabstract\rabstract: The end-to-end ASR model is often desired in the streaming multilingualscenario since it is easier to deploy and can benefit from pre-trained speechmodels such as powerful foundation models. Meanwhile, the heterogeneous natureand imbalanced data abundance of different languages may cause performancedegradation, leading to asynchronous peak performance for different languagesduring training, especially on tail ones. Sometimes even the data itself maybecome unavailable as a result of the enhanced privacy protection. Existingwork tend to significantly increase the model size or learn language-specificdecoders to accommodate each language separately. In this study, we exploresimple yet effective Language-Dependent Adapter (LDA) finetuning under acascaded Conformer transducer framework enhanced by teacher pseudo-labeling fortail languages in the streaming multilingual ASR. The adapter only accounts for0.4% of the full model per language. It is plugged into the frozen foundationmodel and is the only trainable module during the finetuning process with noisystudent training. The final model merges the adapter parameters from differentcheckpoints for different languages. The model performance is validated on achallenging multilingual dictation dataset, which includes 39 tail languagesacross Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word errorrate reduction on average and up to 37.5% on a single locale. Furthermore, weshow that our parameter-efficient LDA can match the quality of the full modelfinetuning, thus greatly alleviating the asynchronous peak performance issue.\rHasTEE+ : Confidential Cloud Computing and Analytics with Haskell\nAbhiroop Sarkar Alejandro Russo\nabstract\rabstract: Confidential computing is a security paradigm that enables the protection ofconfidential code and data in a co-tenanted cloud deployment using specializedhardware isolation units called Trusted Execution Environments (TEEs). Byintegrating TEEs with a Remote Attestation protocol, confidential computingallows a third party to establish the integrity of an \\textit{enclave} hostedwithin an untrusted cloud. However, TEE solutions, such as Intel SGX and ARMTrustZone, offer low-level C/C++-based toolchains that are susceptible toinherent memory safety vulnerabilities and lack language constructs to monitorexplicit and implicit information-flow leaks. Moreover, the toolchains involvecomplex multi-project hierarchies and the deployment of hand-writtenattestation protocols for verifying \\textit{enclave} integrity. We address the above with HasTEE+, a domain-specific language (DSL) embeddedin Haskell that enables programming TEEs in a high-level language with strongtype-safety. HasTEE+ assists in multi-tier cloud application development by (1)introducing a \\textit{tierless} programming model for expressing distributedclient-server interactions as a single program, (2) integrating a generalremote-attestation architecture that removes the necessity to writeapplication-specific cross-cutting attestation code, and (3) employing adynamic information flow control mechanism to prevent explicit as well asimplicit data leaks. We demonstrate the practicality of HasTEE+ through a casestudy on confidential data analytics, presenting a data-sharing patternapplicable to mutually distrustful participants and providing overallperformance metrics.\r2024-01-16\nFederated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls\nSaurav Prakash Jin Sima Chao Pan Eli Chien Olgica Milenkovic\nabstract\rabstract: Hierarchical and tree-like data sets arise in many applications, includinglanguage processing, graph data mining, phylogeny and genomics. It is knownthat tree-like data cannot be embedded into Euclidean spaces of finitedimension with small distortion. This problem can be mitigated through the useof hyperbolic spaces. When such data also has to be processed in a distributedand privatized setting, it becomes necessary to work with new federatedlearning methods tailored to hyperbolic spaces. As an initial step towards thedevelopment of the field of federated learning in hyperbolic spaces, we proposethe first known approach to federated classification in hyperbolic spaces. Ourcontributions are as follows. First, we develop distributed versions of convexSVM classifiers for Poincar'e discs. In this setting, the information conveyedfrom clients to the global classifier are convex hulls of clusters present inindividual client data. Second, to avoid label switching issues, we introduce anumber-theoretic approach for label recovery based on the so-called integer$B_h$ sequences. Third, we compute the complexity of the convex hulls inhyperbolic spaces to assess the extent of data leakage; at the same time, inorder to limit communication cost for the hulls, we propose a new quantizationmethod for the Poincar'e disc coupled with Reed-Solomon-like encoding. Fourth,at the server level, we introduce a new approach for aggregating convex hullsof the clients based on balanced graph partitioning. We test our method on acollection of diverse data sets, including hierarchical single-cell RNA-seqdata from different patients distributed across different repositories thathave stringent privacy constraints. The classification accuracy of our methodis up to $\\sim 11%$ better than its Euclidean counterpart, demonstrating theimportance of privacy-preserving learning in hyperbolic spaces.\rTopic Modelling: Going Beyond Token Outputs\nLowri Williams Eirini Anthi Laura Arman Pete Burnap\nabstract\rabstract: Topic modelling is a text mining technique for identifying salient themesfrom a number of documents. The output is commonly a set of topics consistingof isolated tokens that often co-occur in such documents. Manual effort isoften associated with interpreting a topic\u0026rsquo;s description from such tokens.However, from a human\u0026rsquo;s perspective, such outputs may not adequately provideenough information to infer the meaning of the topics; thus, theirinterpretability is often inaccurately understood. Although several studieshave attempted to automatically extend topic descriptions as a means ofenhancing the interpretation of topic models, they rely on external languagesources that may become unavailable, must be kept up-to-date to generaterelevant results, and present privacy issues when training on or processingdata. This paper presents a novel approach towards extending the output oftraditional topic modelling methods beyond a list of isolated tokens. Thisapproach removes the dependence on external sources by using the textual dataitself by extracting high-scoring keywords and mapping them to the topicmodel\u0026rsquo;s token outputs. To measure the interpretability of the proposed outputsagainst those of the traditional topic modelling approach, independentannotators manually scored each output based on their quality and usefulness,as well as the efficiency of the annotation task. The proposed approachdemonstrated higher quality and usefulness, as well as higher efficiency in theannotation task, in comparison to the outputs of a traditional topic modellingmethod, demonstrating an increase in their interpretability.\r2024-01-15\nThe Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias\nKausik Lakkaraju Aniket Gupta Biplav Srivastava Marco Valtorta Dezhi Wu\nabstract\rabstract: Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence(AI) systems that output polarity and emotional intensity when given a piece oftext as input. Like other AIs, SASs are also known to have unstable behaviorwhen subjected to changes in data which can make it problematic to trust out ofconcerns like bias when AI works with humans and data has protected attributeslike gender, race, and age. Recently, an approach was introduced to assess SASsin a blackbox setting without training data or code, and rating them for biasusing synthetic English data. We augment it by introducing two human-generatedchatbot datasets and also consider a round-trip setting of translating the datafrom one language to the same through an intermediate language. We find thatthese settings show SASs performance in a more realistic light. Specifically,we find that rating SASs on the chatbot data showed more bias compared to thesynthetic data, and round-tripping using Spanish and Danish as intermediatelanguages reduces the bias (up to 68% reduction) in human-generated data while,in synthetic data, it takes a surprising turn by increasing the bias! Ourfindings will help researchers and practitioners refine their SAS testingstrategies and foster trust as SASs are considered part of moremission-critical applications for global use.\rCLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification\nZeyu Li Jingsheng Gao Tong Yu Suncheng Xiang Jiacheng Ruan Ting Liu Yuzhuo Fu\nabstract\rabstract: Existing research on audio classification faces challenges in recognizingattributes of passive underwater vessel scenarios and lacks well-annotateddatasets due to data privacy concerns. In this study, we introduce CLAPP(Contrastive Language-Audio Pre-training in Passive Underwater VesselClassification), a novel model. Our aim is to train a neural network using awide range of vessel audio and vessel state text pairs obtained from anoceanship dataset. CLAPP is capable of directly learning from raw vessel audiodata and, when available, from carefully curated labels, enabling improvedrecognition of vessel attributes in passive underwater vessel scenarios.Model\u0026rsquo;s zero-shot capability allows predicting the most relevant vessel statedescription for a given vessel audio, without directly optimizing for the task.Our approach aims to solve 2 challenges: vessel audio-text classification andpassive underwater vessel audio attribute recognition. The proposed methodachieves new state-of-the-art results on both Deepship and Shipsear publicdatasets, with a notable margin of about 7%-13% for accuracy compared to priormethods on zero-shot task.\r2024-01-14\nProtected edge modes based on the bulk and boundary renormalization group: A relationship between duality and generalized symmetry\nYoshiki Fukusumi\nabstract\rabstract: We propose a theoretical formulation of protected edge modes in the languageof quantum field theories based on the contemporary understanding of therenormalization group. We use bulk and boundary renormalization arguments whichhave never captured enough attention in condensed matter physics and relatedfields. We revisit various exotic bulk and boundary phenomena in contemporaryphysics, and one can see the conciseness of our formulations. Moreover, in thesystems with open boundaries in general space-time dimensions, we also analyzetheir implications under general duality implemented by the shift of defectscorresponding to generalized symmetries, including higher-form, non-invertiblesymmetries, in principle. Our formulation opens up a new paradigm to explorethe systems with protected edge modes in the established language of therenormalization group.\r2024-01-13\nExploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability\nJunxi Chen Junhao Dong Xiaohua Xie\nabstract\rabstract: Recently, many studies utilized adversarial examples (AEs) to raise the costof malicious image editing and copyright violation powered by latent diffusionmodels (LDMs). Despite their successes, a few have studied the surrogate modelthey used to generate AEs. In this paper, from the perspective of adversarialtransferability, we investigate how the surrogate model\u0026rsquo;s property influencesthe performance of AEs for LDMs. Specifically, we view the time-step samplingin the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogatemodels. We find that the smoothness of surrogate models at different time stepsdiffers, and we substantially improve the performance of the MC-based AEs byselecting smoother surrogate models. In the light of the theoretical frameworkon adversarial transferability in image classification, we also conduct atheoretical analysis to explain why smooth surrogate models can also boost AEsfor LDMs.\r2024-01-12\nSpeaker anonymization using neural audio codec language models\nMichele Panariello Francesco Nespoli Massimiliano Todisco Nicholas Evans\nabstract\rabstract: The vast majority of approaches to speaker anonymization involve theextraction of fundamental frequency estimates, linguistic features and aspeaker embedding which is perturbed to obfuscate the speaker identity beforean anonymized speech waveform is resynthesized using a vocoder. Recent work hasshown that x-vector transformations are difficult to control consistently:other sources of speaker information contained within fundamental frequency andlinguistic features are re-entangled upon vocoding, meaning that anonymizedspeech signals still contain speaker information. We propose an approach basedupon neural audio codecs (NACs), which are known to generate high-qualitysynthetic speech when combined with language models. NACs use quantized codes,which are known to effectively bottleneck speaker-related information: wedemonstrate the potential of speaker anonymization systems based on NAClanguage modeling by applying the evaluation framework of the Voice PrivacyChallenge 2022.\rBusiness and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems\nRebekah Rousi Hooman Samani Niko Mäkitalo Ville Vakkuri Simo Linkola Kai-Kristian Kemell Paulius Daubaris Ilenia Fronza Tommi Mikkonen Pekka Abrahamsson\nabstract\rabstract: Business and technology are intricately connected through logic and design.They are equally sensitive to societal changes and may be devastated byscandal. Cooperative multi-robot systems (MRSs) are on the rise, allowingrobots of different types and brands to work together in diverse contexts.Generative artificial intelligence has been a dominant topic in recentartificial intelligence (AI) discussions due to its capacity to mimic humansthrough the use of natural language and the production of media, including deepfakes. In this article, we focus specifically on the conversational aspects ofgenerative AI, and hence use the term Conversational Generative artificialintelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizingprocesses across sectors and transforming the way humans conduct business. Froma business perspective, cooperative MRSs alone, with potential conflicts ofinterest, privacy practices, and safety concerns, require ethical examination.MRSs empowered by CGIs demand multi-dimensional and sophisticated methods touncover imminent ethical pitfalls. This study focuses on ethics inCGI-empowered MRSs while reporting the stages of developing the MORUL model.\rGreening Large Language Models of Code\nJieke Shi Zhou Yang Hong Jin Kang Bowen Xu Junda He David Lo\nabstract\rabstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers\u0026rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\\times$less), carbon footprint (up to 157$\\times$ less), and inference latency (up to76$\\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).\r2024-01-11\nTOFU: A Task of Fictitious Unlearning for LLMs\nPratyush Maini Zhili Feng Avi Schwarzschild Zachary C. Lipton J. Zico Kolter\nabstract\rabstract: Large language models trained on massive corpora of data from the web canmemorize and reproduce sensitive or private data raising both legal and ethicalconcerns. Unlearning, or tuning models to forget information present in theirtraining data, provides us with a way to protect private data after training.Although several methods exist for such unlearning, it is unclear to whatextent they result in models equivalent to those where the data to be forgottenwas never learned in the first place. To address this challenge, we presentTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepenour understanding of unlearning. We offer a dataset of 200 diverse syntheticauthor profiles, each consisting of 20 question-answer pairs, and a subset ofthese profiles called the forget set that serves as the target for unlearning.We compile a suite of metrics that work together to provide a holistic pictureof unlearning efficacy. Finally, we provide a set of baseline results fromexisting unlearning algorithms. Importantly, none of the baselines we considershow effective unlearning motivating continued efforts to develop approachesfor unlearning that effectively tune models so that they truly behave as ifthey were never trained on the forget data at all.\rBeyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning\nJianwei Li Sheng Liu Qi Lei\nabstract\rabstract: Federated learning (FL) emphasizes decentralized training by storing datalocally and sending only model updates, underlining user privacy. Recently, aline of works on privacy attacks impairs user privacy by extracting sensitivetraining text from language models in the context of FL. Yet, these attacktechniques face distinct hurdles: some work chiefly with limited batch sizes(e.g., batch size of 1), and others are easily detectable. This paperintroduces an innovative approach that is challenging to detect, significantlyenhancing the recovery rate of text in various batch-size settings. Building onfundamental gradient matching and domain prior knowledge, we enhance the attackby recovering the input of the Pooler layer of language models, which enablesus to provide additional supervised signals at the feature level. Unlikegradient data, these signals do not average across sentences and tokens,thereby offering more nuanced and effective insights. We benchmark our methodusing text classification tasks on datasets such as CoLA, SST-2, and RottenTomatoes. Across different batch sizes and models, our approach consistentlyoutperforms previous state-of-the-art results.\rNavigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The advent of Generative AI has marked a significant milestone in artificialintelligence, demonstrating remarkable capabilities in generating realisticimages, texts, and data patterns. However, these advancements come withheightened concerns over data privacy and copyright infringement, primarily dueto the reliance on vast datasets for model training. Traditional approacheslike differential privacy, machine unlearning, and data poisoning only offerfragmented solutions to these complex issues. Our paper delves into themultifaceted challenges of privacy and copyright protection within the datalifecycle. We advocate for integrated approaches that combines technicalinnovation with ethical foresight, holistically addressing these concerns byinvestigating and devising solutions that are informed by the lifecycleperspective. This work aims to catalyze a broader discussion and inspireconcerted efforts towards data privacy and copyright integrity in GenerativeAI.\rHarnessing large-language models to generate private synthetic text\nAlexey Kurakin Natalia Ponomareva Umar Syed Liam MacDermed Andreas Terzis\nabstract\rabstract: Differentially private training algorithms like DP-SGD protect sensitivetraining data by ensuring that trained models do not reveal privateinformation. An alternative approach, which this paper studies, is to use asensitive dataset to generate synthetic data that is differentially privatewith respect to the original data, and then non-privately training a model onthe synthetic data. Doing so has several advantages: synthetic data can bereused for other tasks (including for hyper parameter tuning), retainedindefinitely, and shared with third parties without sacrificing privacy.However, generating private synthetic data is much harder than training aprivate model. To improve performance on text data, recent work has utilizedpublic data by starting with a pre-trained generative language model andprivately fine-tuning it on sensitive data. This model can be used to sample aDP synthetic dataset. While this strategy seems straightforward, executing ithas proven problematic. Previous approaches either show significant performanceloss, or have, as we show, critical design flaws. In this paper we demonstratethat a proper training objective along with tuning fewer parameters results inexcellent DP synthetic data quality. Our approach is competitive with directDP-training of downstream classifiers in terms of performance on downstreamtasks. Further, we demonstrate that our DP synthetic data is not only usefulfor downstream classifier training, but also to tune those same models.\r2024-01-09\nAuditing and Generating Synthetic Data with Controllable Trust Trade-offs\nBrian Belgodere Pierre Dognin Adam Ivankay Igor Melnyk Youssef Mroueh Aleksandra Mojsilovic Jiri Navratil Apoorva Nitsure Inkit Padhi Mattia Rigotti Jerret Ross Yair Schiff Radhika Vedpathak Richard A. Young\nabstract\rabstract: Real-world data often exhibits bias, imbalance, and privacy risks. Syntheticdatasets have emerged to address these issues. This paradigm relies ongenerative AI models to generate unbiased, privacy-preserving data whilemaintaining fidelity to the original data. However, assessing thetrustworthiness of synthetic datasets and models is a critical challenge. Weintroduce a holistic auditing framework that comprehensively evaluatessynthetic datasets and AI models. It focuses on preventing bias anddiscrimination, ensures fidelity to the source data, assesses utility,robustness, and privacy preservation. We demonstrate the framework\u0026rsquo;seffectiveness by auditing various generative models across diverse use caseslike education, healthcare, banking, and human resources, spanning differentdata modalities such as tabular, time-series, vision, and natural language.This holistic assessment is essential for compliance with regulatorysafeguards. We introduce a trustworthiness index to rank synthetic datasetsbased on their safeguards trade-offs. Furthermore, we present atrustworthiness-driven model selection and cross-validation process duringtraining, exemplified with \u0026ldquo;TrustFormers\u0026rdquo; across various data types. Thisapproach allows for controllable trustworthiness trade-offs in synthetic datacreation. Our auditing framework fosters collaboration among stakeholders,including data scientists, governance experts, internal reviewers, externalcertifiers, and regulators. This transparent reporting should become a standardpractice to prevent bias, discrimination, and privacy violations, ensuringcompliance with policies and providing accountability, safety, and performanceguarantees.\rPrivate Fine-tuning of Large Language Models with Zeroth-order Optimization\nXinyu Tang Ashwinee Panda Milad Nasr Saeed Mahloujifar Prateek Mittal\nabstract\rabstract: Fine-tuning large pretrained models on private datasets may run the risk ofviolating privacy. Differential privacy is a framework for mitigating privacyrisks by enforcing algorithmic stability. DP-SGD enables training models withprivate data in a privacy-preserving manner, but raises new obstacles in theform of performance loss and significant engineering challenges. We introduceDP-ZO, a new method for fine-tuning large language models that preserves theprivacy of training data by privatizing zeroth-order optimization. A keyinsight into the design of our method is that the direction of the gradient inSPSA, the zeroth-order algorithm we use, is always random and the onlyinformation that depends on private data is the step size, i.e., a scalar.Therefore, we only need to privatize the scalar step size, which ismemory-efficient. DP-ZO, which can be instantiated with either Laplace orGaussian noise, provides a strong privacy-utility trade-off across differenttasks, and model sizes, under conservative privacy budgets. One noteworthyresult is that DP-ZO exhibits just $1.86%$ performance degradation due toprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samplesfrom SQuAD.\r2024-01-08\nSynthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models\nAldo Gael Carranza Rezsa Farahani Natalia Ponomareva Alex Kurakin Matthew Jagielski Milad Nasr\nabstract\rabstract: We address the challenge of ensuring differential privacy (DP) guarantees intraining deep retrieval systems. Training these systems often involves the useof contrastive-style losses, which are typically non-per-example decomposable,making them difficult to directly DP-train with since common techniques requireper-example gradient. To address this issue, we propose an approach thatprioritizes ensuring query privacy prior to training a deep retrieval system.Our method employs DP language models (LMs) to generate private syntheticqueries representative of the original data. These synthetic queries can beused in downstream retrieval system training without compromising privacy. Ourapproach demonstrates a significant enhancement in retrieval quality comparedto direct DP-training, all while maintaining query-level privacy guarantees.This work highlights the potential of harnessing LMs to overcome limitations instandard DP-training methods.\r2024-01-07\nThe Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\nHaonan Wang Qianli Shen Yao Tong Yang Zhang Kenji Kawaguchi\nabstract\rabstract: The commercialization of diffusion models, renowned for their ability togenerate high-quality images that are often indistinguishable from real ones,brings forth potential copyright concerns. Although attempts have been made toimpede unauthorized access to copyrighted material during training and tosubsequently prevent DMs from generating copyrighted images, the effectivenessof these solutions remains unverified. This study explores the vulnerabilitiesassociated with copyright protection in DMs by introducing a backdoor datapoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.Our attack method operates without requiring access to or control over thediffusion model\u0026rsquo;s training or fine-tuning processes; it merely involves theinsertion of poisoning data into the clean training dataset. This data,comprising poisoning images equipped with prompts, is generated by leveragingthe powerful capabilities of multimodal large language models and text-guidedimage inpainting techniques. Our experimental results and analysis confirm themethod\u0026rsquo;s effectiveness. By integrating a minor portion ofnon-copyright-infringing stealthy poisoning data into the cleandataset-rendering it free from suspicion-we can prompt the finetuned diffusionmodels to produce copyrighted content when activated by specific triggerprompts. These findings underline potential pitfalls in the prevailingcopyright protection strategies and underscore the necessity for increasedscrutiny and preventative measures against the misuse of DMs.\r2024-01-06\nMalla: Demystifying Real-world Large Language Model Integrated Malicious Services\nZilong Lin Jian Cui Xiaojing Liao XiaoFeng Wang\nabstract\rabstract: The underground exploitation of large language models (LLMs) for maliciousservices (i.e., Malla) is witnessing an uptick, amplifying the cyber threatlandscape and posing questions about the trustworthiness of LLM technologies.However, there has been little effort to understand this new cybercrime, interms of its magnitude, impact, and techniques. In this paper, we conduct thefirst systematic study on 212 real-world Mallas, uncovering their proliferationin underground marketplaces and exposing their operational modalities. Ourstudy discloses the Malla ecosystem, revealing its significant growth andimpact on today\u0026rsquo;s public LLM services. Through examining 212 Mallas, weuncovered eight backend LLMs used by Mallas, along with 182 prompts thatcircumvent the protective measures of public LLM APIs. We further demystify thetactics employed by Mallas, including the abuse of uncensored LLMs and theexploitation of public LLM APIs through jailbreak prompts. Our findings enablea better understanding of the real-world exploitation of LLMs bycybercriminals, offering insights into strategies to counteract thiscybercrime.\rExploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review\nLuoma Ke Song Tong Peng Cheng Kaiping Peng\nabstract\rabstract: This paper explores the frontiers of large language models (LLMs) inpsychology applications. Psychology has undergone several theoretical changes,and the current use of Artificial Intelligence (AI) and Machine Learning,particularly LLMs, promises to open up new research directions. We provide adetailed exploration of how LLMs like ChatGPT are transforming psychologicalresearch. It discusses the impact of LLMs across various branches ofpsychology, including cognitive and behavioral, clinical and counseling,educational and developmental, and social and cultural psychology, highlightingtheir potential to simulate aspects of human cognition and behavior. The paperdelves into the capabilities of these models to emulate human-like textgeneration, offering innovative tools for literature review, hypothesisgeneration, experimental design, experimental subjects, data analysis, academicwriting, and peer review in psychology. While LLMs are essential in advancingresearch methodologies in psychology, the paper also cautions about theirtechnical and ethical challenges. There are issues like data privacy, theethical implications of using LLMs in psychological research, and the need fora deeper understanding of these models\u0026rsquo; limitations. Researchers shouldresponsibly use LLMs in psychological studies, adhering to ethical standardsand considering the potential consequences of deploying these technologies insensitive areas. Overall, the article provides a comprehensive overview of thecurrent state of LLMs in psychology, exploring potential benefits andchallenges. It serves as a call to action for researchers to leverage LLMs\u0026rsquo;advantages responsibly while addressing associated risks.\rSecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models\nJinglong Luo Yehong Zhang Jiaqi Zhang Xin Mu Hui Wang Yue Yu Zenglin Xu\nabstract\rabstract: With the growing use of large language models hosted on cloud platforms tooffer inference services, privacy concerns are escalating, especiallyconcerning sensitive data like investment plans and bank account details.Secure Multi-Party Computing (SMPC) emerges as a promising solution to protectthe privacy of inference data and model parameters. However, the application ofSMPC in Privacy-Preserving Inference (PPI) for large language models,particularly those based on the Transformer architecture, often leads toconsiderable slowdowns or declines in performance. This is largely due to themultitude of nonlinear operations in the Transformer architecture, which arenot well-suited to SMPC and difficult to circumvent or optimize effectively. Toaddress this concern, we introduce an advanced optimization framework calledSecFormer, to achieve fast and accurate PPI for Transformer models. Byimplementing model design optimization, we successfully eliminate the high-costexponential and maximum operations in PPI without sacrificing modelperformance. Additionally, we have developed a suite of efficient SMPCprotocols that utilize segmented polynomials, Fourier series and Goldschmidt\u0026rsquo;smethod to handle other complex nonlinear functions within PPI, such as GeLU,LayerNorm, and Softmax. Our extensive experiments reveal that SecFormeroutperforms MPCFormer in performance, showing improvements of $5.6%$ and$24.2%$ for BERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, respectively. Interms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma forBERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, demonstrating its effectivenessand speed.\r2024-01-04\nRecommendations for public action towards sustainable generative AI systems\nThomas Le Goff\nabstract\rabstract: Growing awareness of the environmental impact of digital technologies has ledto several isolated initiatives to promote sustainable practices. However,despite these efforts, the environmental footprint of generative AI,particularly in terms of greenhouse gas emissions and water consumption,remains considerable. This contribution first presents the components of thisenvironmental footprint, highlighting the massive CO2 emissions and waterconsumption associated with training large language models, thus underliningthe need to rethink learning and inference methods. The paper also explores thefactors and characteristics of models that have an influence on theirenvironmental footprint and demonstrates the existence of solutions to reduceit, such as using more efficient processors or optimising the energyperformance of data centres. The potentially harmful effects of AI on theplanet and its ecosystem have made environmental protection one of the foundingprinciples of AI ethics at international and European levels. However, thisrecognition has not yet translated into concrete measures to address it.Toaddress this issue, our contribution puts forward twelve pragmaticrecommendations for public action to promote sustainable generative AI, inparticular by building a long-term strategy to achieve carbon neutrality for AImodels, encouraging international cooperation to set common standards,supporting scientific research and developing appropriate legal and regulatoryframeworks.This paper seeks to inform the members of the InterministerialCommittee on Generative AI about the environmental challenges of thistechnology by providing a brief review of the scientific literature on thesubject and proposing concrete recommendations of public policy actions toreconcile technological innovation with the need to protect our environment.\r2024-01-03\nMULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative Intelligence Framework\nMaximilian T. Fischer Yannick Metz Lucas Joos Matthias Miller Daniel A. Keim\nabstract\rabstract: AI-driven models are increasingly deployed in operational analyticssolutions, for instance, in investigative journalism or the intelligencecommunity. Current approaches face two primary challenges: ethical and privacyconcerns, as well as difficulties in efficiently combining heterogeneous datasources for multimodal analytics. To tackle the challenge of multimodalanalytics, we present MULTI-CASE, a holistic visual analytics frameworktailored towards ethics-aware and multimodal intelligence exploration, designedin collaboration with domain experts. It leverages an equal joint agencybetween human and AI to explore and assess heterogeneous information spaces,checking and balancing automation through Visual Analytics. MULTI-CASE operateson a fully-integrated data model and features type-specific analysis withmultiple linked components, including a combined search, annotated text view,and graph-based analysis. Parts of the underlying entity detection are based ona RoBERTa-based language model, which we tailored towards user requirementsthrough fine-tuning. An overarching knowledge exploration graph combines allinformation streams, provides in-situ explanations, transparent sourceattribution, and facilitates effective exploration. To assess our approach, weconducted a comprehensive set of evaluations: We benchmarked the underlyinglanguage model on relevant NER tasks, achieving state-of-the-art performance.The demonstrator was assessed according to intelligence capability assessments,while the methodology was evaluated according to ethics design guidelines. As acase study, we present our framework in an investigative journalism setting,supporting war crime investigations. Finally, we conduct a formative userevaluation with domain experts in law enforcement. Our evaluations confirm thatour framework facilitates human agency and steering in security-sensitiveapplications.\rPredicting challenge moments from students\u0026rsquo; discourse: A comparison of GPT-4 to two traditional natural language processing approaches\nWannapon Suraworachet Jennifer Seon Mutlu Cukurova\nabstract\rabstract: Effective collaboration requires groups to strategically regulate themselvesto overcome challenges. Research has shown that groups may fail to regulate dueto differences in members\u0026rsquo; perceptions of challenges which may benefit fromexternal support. In this study, we investigated the potential of leveragingthree distinct natural language processing models: an expert knowledgerule-based model, a supervised machine learning (ML) model and a Large Languagemodel (LLM), in challenge detection and challenge dimension identification(cognitive, metacognitive, emotional and technical/other challenges) fromstudent discourse, was investigated. The results show that the supervised MLand the LLM approaches performed considerably well in both tasks, in contrastto the rule-based approach, whose efficacy heavily relies on the engineeredfeatures by experts. The paper provides an extensive discussion of the threeapproaches\u0026rsquo; performance for automated detection and support of students\u0026rsquo;challenge moments in collaborative learning activities. It argues that,although LLMs provide many advantages, they are unlikely to be the panacea toissues of the detection and feedback provision of socially shared regulation oflearning due to their lack of reliability, as well as issues of validityevaluation, privacy and confabulation. We conclude the paper with a discussionon additional considerations, including model transparency to explore feasibleand meaningful analytical feedback for students and educators using LLMs.\rDB-GPT: Empowering Database Interactions with Private Large Language Models\nSiqiao Xue Caigao Jiang Wenhui Shi Fangyin Cheng Keting Chen Hongjun Yang Zhiping Zhang Jianshan He Hongyang Zhang Ganglin Wei Wang Zhao Fan Zhou Danrui Qi Hong Yi Shaodong Liu Faqiang Chen\nabstract\rabstract: The recent breakthroughs in large language models (LLMs) are positioned totransition many areas of software. Database technologies particularly have animportant entanglement with LLMs as efficient and intuitive databaseinteractions are paramount. In this paper, we present DB-GPT, a revolutionaryand production-ready project that integrates LLMs with traditional databasesystems to enhance user experience and accessibility. DB-GPT is designed tounderstand natural language queries, provide context-aware responses, andgenerate complex SQL queries with high accuracy, making it an indispensabletool for users ranging from novice to expert. The core innovation in DB-GPTlies in its private LLM technology, which is fine-tuned on domain-specificcorpora to maintain user privacy and ensure data security while offering thebenefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, whichincludes a novel retrieval augmented generation (RAG) knowledge system, anadaptive learning mechanism to continuously improve performance based on userfeedback and a service-oriented multi-model framework (SMMF) with powerfuldata-driven agents. Our extensive experiments and user studies confirm thatDB-GPT represents a paradigm shift in database interactions, offering a morenatural, efficient, and secure way to engage with data repositories. The paperconcludes with a discussion of the implications of DB-GPT framework on thefuture of human-database interaction and outlines potential avenues for furtherenhancements and applications in the field. The project code is available athttps://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself byinstalling it with the instructionshttps://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minutevideo at https://www.youtube.com/watch?v=KYs4nTDzEhk.\r2024-01-02\nA Reliable Knowledge Processing Framework for Combustion Science using Foundation Models\nVansh Sharma Venkat Raman\nabstract\rabstract: This research explores the integration of large language models (LLMs) intoscientific data assimilation, focusing on combustion science as a case study.Leveraging foundational models integrated with Retrieval-Augmented Generation(RAG) framework, the study introduces an approach to process diverse combustionresearch data, spanning experimental studies, simulations, and literature. Themultifaceted nature of combustion research emphasizes the critical role ofknowledge processing in navigating and extracting valuable information from avast and diverse pool of sources. The developed approach minimizescomputational and economic expenses while optimizing data privacy and accuracy.It incorporates prompt engineering and offline open-source LLMs, offering userautonomy in selecting base models. The study provides a thorough examination oftext segmentation strategies, conducts comparative studies between LLMs, andexplores various optimized prompts to demonstrate the effectiveness of theframework. By incorporating an external database, the framework outperforms aconventional LLM in generating accurate responses and constructing robustarguments. Additionally, the study delves into the investigation of optimizedprompt templates for the purpose of efficient extraction of scientificliterature. The research addresses concerns related to hallucinations and falseresearch articles by introducing a custom workflow developed with a detectionalgorithm to filter out inaccuracies. Despite identified areas for improvement,the framework consistently delivers accurate domain-specific responses withminimal human oversight. The prompt-agnostic approach introduced holds promisefor future deliberations. The study underscores the significance of integratingLLMs and knowledge processing techniques in scientific research, providing afoundation for advancements in data assimilation and utilization.\rSafety and Performance, Why Not Both? Bi-Objective Optimized Model Compression against Heterogeneous Attacks Toward AI Software Deployment\nJie Zhu Leye Wang Xiao Han Anmin Liu Tao Xie\nabstract\rabstract: The size of deep learning models in artificial intelligence (AI) software isincreasing rapidly, hindering the large-scale deployment on resource-restricteddevices (e.g., smartphones). To mitigate this issue, AI software compressionplays a crucial role, which aims to compress model size while keeping highperformance. However, the intrinsic defects in a big model may be inherited bythe compressed one. Such defects may be easily leveraged by adversaries, sincea compressed model is usually deployed in a large number of devices withoutadequate protection. In this article, we aim to address the safe modelcompression problem from the perspective of safety-performance co-optimization.Specifically, inspired by the test-driven development (TDD) paradigm insoftware engineering, we propose a test-driven sparse training framework calledSafeCompress. By simulating the attack mechanism as safety testing,SafeCompress can automatically compress a big model to a small one followingthe dynamic sparse training paradigm. Then, considering two kinds ofrepresentative and heterogeneous attack mechanisms, i.e., black-box membershipinference attack and white-box membership inference attack, we develop twoconcrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, weimplement another instance called MMIA-SafeCompress by extending SafeCompressto defend against the occasion when adversaries conduct black-box and white-boxmembership inference attacks simultaneously. We conduct extensive experimentson five datasets for both computer vision and natural language processingtasks. The results show the effectiveness and generalizability of ourframework. We also discuss how to adapt SafeCompress to other attacks besidesmembership inference attack, demonstrating the flexibility of SafeCompress.\r2024-01-01\nTaking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education\nArne Bewersdorff Christian Hartmann Marie Hornberger Kathrin Seßler Maria Bannert Enkelejda Kasneci Gjergji Kasneci Xiaoming Zhai Claudia Nerdel\nabstract\rabstract: The integration of Artificial Intelligence (AI), particularly Large LanguageModel (LLM)-based systems, in education has shown promise in enhancing teachingand learning experiences. However, the advent of Multimodal Large LanguageModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processingmultimodal data including text, sound, and visual inputs, opens a new era ofenriched, personalized, and interactive learning landscapes in education.Grounded in theory of multimedia learning, this paper explores thetransformative role of MLLMs in central aspects of science education bypresenting exemplary innovative learning scenarios. Possible applications forMLLMs could range from content creation to tailored support for learning,fostering competencies in scientific practices, and providing assessment andfeedback. These scenarios are not limited to text-based and uni-modal formatsbut can be multimodal, increasing thus personalization, accessibility, andpotential learning effectiveness. Besides many opportunities, challenges suchas data protection and ethical considerations become more salient, calling forrobust frameworks to ensure responsible integration. This paper underscores thenecessity for a balanced approach in implementing MLLMs, where the technologycomplements rather than supplants the educator\u0026rsquo;s role, ensuring thus aneffective and ethical use of AI in science education. It calls for furtherresearch to explore the nuanced implications of MLLMs on the evolving role ofeducators and to extend the discourse beyond science education to otherdisciplines. Through the exploration of potentials, challenges, and futureimplications, we aim to contribute to a preliminary understanding of thetransformative trajectory of MLLMs in science education and beyond.\rMachine Learning for Synthetic Data Generation: A Review\nYingzhou Lu Minjie Shen Huazheng Wang Xiao Wang Capucine van Rechem Wenqi Wei\nabstract\rabstract: Machine learning heavily relies on data, but real-world applications oftenencounter various data-related issues. These include data of poor quality,insufficient data points leading to under-fitting of machine learning models,and difficulties in data access due to concerns surrounding privacy, safety,and regulations. In light of these challenges, the concept of synthetic datageneration emerges as a promising alternative that allows for data sharing andutilization in ways that real-world data cannot facilitate. This paper presentsa comprehensive systematic review of existing studies that employ machinelearning models for the purpose of generating synthetic data. The reviewencompasses various perspectives, starting with the applications of syntheticdata generation, spanning computer vision, speech, natural language processing,healthcare, and business domains. Additionally, it explores different machinelearning methods, with particular emphasis on neural network architectures anddeep generative models. The paper also addresses the crucial aspects of privacyand fairness concerns related to synthetic data generation. Furthermore, thisstudy identifies the challenges and opportunities prevalent in this emergingfield, shedding light on the potential avenues for future research. By delvinginto the intricacies of synthetic data generation, this paper aims tocontribute to the advancement of knowledge and inspire further exploration insynthetic data generation.\rDigger: Detecting Copyright Content Mis-usage in Large Language Model Training\nHaodong Li Gelei Deng Yi Liu Kailong Wang Yuekang Li Tianwei Zhang Yang Liu Guoai Xu Guosheng Xu Haoyu Wang\nabstract\rabstract: Pre-training, which utilizes extensive and varied datasets, is a criticalfactor in the success of Large Language Models (LLMs) across numerousapplications. However, the detailed makeup of these datasets is often notdisclosed, leading to concerns about data security and potential misuse. Thisis particularly relevant when copyrighted material, still under legalprotection, is used inappropriately, either intentionally or unintentionally,infringing on the rights of the authors. In this paper, we introduce a detailed framework designed to detect andassess the presence of content from potentially copyrighted books within thetraining datasets of LLMs. This framework also provides a confidence estimationfor the likelihood of each content sample\u0026rsquo;s inclusion. To validate ourapproach, we conduct a series of simulated experiments, the results of whichaffirm the framework\u0026rsquo;s effectiveness in identifying and addressing instances ofcontent misuse in LLM training processes. Furthermore, we investigate thepresence of recognizable quotes from famous literary works within thesedatasets. The outcomes of our study have significant implications for ensuringthe ethical use of copyrighted materials in the development of LLMs,highlighting the need for more transparent and responsible data managementpractices in this field.\r2023-12-31\nOpening A Pandora\u0026rsquo;s Box: Things You Should Know in the Era of Custom GPTs\nGuanhong Tao Siyuan Cheng Zhuo Zhang Junmin Zhu Guangyu Shen Xiangyu Zhang\nabstract\rabstract: The emergence of large language models (LLMs) has significantly acceleratedthe development of a wide range of applications across various fields. There isa growing trend in the construction of specialized platforms based on LLMs,such as the newly introduced custom GPTs by OpenAI. While custom GPTs providevarious functionalities like web browsing and code execution, they alsointroduce significant security threats. In this paper, we conduct acomprehensive analysis of the security and privacy issues arising from thecustom GPT platform. Our systematic examination categorizes potential attackscenarios into three threat models based on the role of the malicious actor,and identifies critical data exchange channels in custom GPTs. Utilizing theSTRIDE threat modeling framework, we identify 26 potential attack vectors, with19 being partially or fully validated in real-world settings. Our findingsemphasize the urgent need for robust security and privacy measures in thecustom GPT ecosystem, especially in light of the forthcoming launch of theofficial GPT store by OpenAI.\rViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\nDipankar Sarkar\nabstract\rabstract: This paper aims to introduce and analyze the Viz system in a comprehensiveway, a novel system architecture that integrates Quantized Low-Rank Adapters(QLoRA) to fine-tune large language models (LLM) within a legally compliant andresource efficient marketplace. Viz represents a significant contribution tothe field of artificial intelligence, particularly in addressing the challengesof computational efficiency, legal compliance, and economic sustainability inthe utilization and monetization of LLMs. The paper delineates the scholarlydiscourse and developments that have informed the creation of Viz, focusingprimarily on the advancements in LLM models, copyright issues in AI training(NYT case, 2023), and the evolution of model fine-tuning techniques,particularly low-rank adapters and quantized low-rank adapters, to create asustainable and economically compliant framework for LLM utilization. Theeconomic model it proposes benefits content creators, AI developers, andend-users, delineating a harmonious integration of technology, economy, andlaw, offering a comprehensive solution to the complex challenges of today\u0026rsquo;s AIlandscape.\r2023-12-30\nEvaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models\nMaximilian Weber Merle Reichardt\nabstract\rabstract: This paper explores the use of open generative Large Language Models (LLMs)for annotation tasks in the social sciences. The study highlights thechallenges associated with proprietary models, such as limited reproducibilityand privacy concerns, and advocates for the adoption of open (source) modelsthat can be operated on independent devices. Two examples of annotation tasks,sentiment analysis in tweets and identification of leisure activities inchildhood aspirational essays are provided. The study evaluates the performanceof different prompting strategies and models (neural-chat-7b-v3-2,Starling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). Theresults indicate the need for careful validation and tailored promptengineering. The study highlights the advantages of open models for dataprivacy and reproducibility.\rWhy is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis\nYuki Yada Tsuneo Matsumoto Fuyuko Kido Hayato Yamana\nabstract\rabstract: Dark patterns are deceptive user interface designs for online services thatmake users behave in unintended ways. Dark patterns, such as privacy invasion,financial loss, and emotional distress, can harm users. These issues have beenthe subject of considerable debate in recent years. In this paper, we studyinterpretable dark pattern auto-detection, that is, why a particular userinterface is detected as having dark patterns. First, we trained a model usingtransformer-based pre-trained language models, BERT, on a text-based datasetfor the automatic detection of dark patterns in e-commerce. Then, we appliedpost-hoc explanation techniques, including local interpretable model agnosticexplanation (LIME) and Shapley additive explanations (SHAP), to the trainedmodel, which revealed which terms influence each prediction as a dark pattern.In addition, we extracted and analyzed terms that affected the dark patterns.Our findings may prevent users from being manipulated by dark patterns, and aidin the construction of more equitable internet services. Our code is availableat https://github.com/yamanalab/why-darkpattern.\rSplit-and-Denoise: Protect large language model inference with local differential privacy\nPeihua Mai Ran Yan Zhe Huang Youjia Yang Yan Pang\nabstract\rabstract: Large Language Models (LLMs) shows powerful capability in natural languageunderstanding by capturing hidden semantics in vector space. This processenriches the value of the text embeddings for various downstream tasks, therebyfostering the Embedding-as-a-Service (EaaS) business model. However, the directtransmission of text to servers poses a largely unaddressed risk of privacyleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), aninnovative framework that split the model to execute the token embedding layeron the client side at minimal computational cost. This allows the client tointroduce noise prior to transmitting the embeddings to the server, andsubsequently receive and denoise the perturbed output embeddings for downstreamtasks. Our approach is designed for the inference stage of LLMs and requires nomodifications to the model parameters. Extensive experiments demonstrate SnD\u0026rsquo;seffectiveness in optimizing the privacy-utility tradeoff across various LLMarchitectures and diverse downstream tasks. The results reveal a significantperformance improvement under the same privacy budget compared to the baseline,offering clients a privacy-preserving solution for local privacy protection.\rTeach Large Language Models to Forget Privacy\nRan Yan Yujun Li Wenqian Li Peihua Mai Yan Pang Yinchuan Li\nabstract\rabstract: Large Language Models (LLMs) have proven powerful, but the risk of privacyleakage remains a significant concern. Traditional privacy-preserving methods,such as Differential Privacy and Homomorphic Encryption, are inadequate forblack-box API-only settings, demanding either model transparency or heavycomputational resources. We propose Prompt2Forget (P2F), the first frameworkdesigned to tackle the LLM local privacy challenge by teaching LLM to forget.The method involves decomposing full questions into smaller segments,generating fabricated answers, and obfuscating the model\u0026rsquo;s memory of theoriginal input. A benchmark dataset was crafted with questions containingprivacy-sensitive information from diverse fields. P2F achieves zero-shotgeneralization, allowing adaptability across a wide range of use cases withoutmanual adjustments. Experimental results indicate P2F\u0026rsquo;s robust capability toobfuscate LLM\u0026rsquo;s memory, attaining a forgetfulness score of around 90% withoutany utility loss. This represents an enhancement of up to 63% when contrastedwith the naive direct instruction technique, highlighting P2F\u0026rsquo;s efficacy inmitigating memory retention of sensitive information within LLMs. Our findingsestablish the first benchmark in the novel field of the LLM forgetting task,representing a meaningful advancement in privacy preservation in the emergingLLM domain.\r2023-12-29\nExploring the language of the sharing economy: Building trust and reducing privacy concern on Airbnb in German and English\nAlex Zarifis Richard Ingham Julia Kroenung\nabstract\rabstract: The text in the profile of those offering their properties in England inEnglish and in Germany in German, are compared to explore whether trust isbuilt, and privacy concerns are reduced in the same way. Six methods ofbuilding trust are used by the landlords: (1) the level of formality, (2)distance and proximity, (3) emotiveness and humor, (4) being assertive andpassive aggressive, (5) conformity to the platform language style andterminology and (6) setting boundaries. Privacy concerns are not usuallyreduced directly as this is left to the platform. The findings indicate thatlanguage has a limited influence and the platform norms and habits are thebiggest influence.\rDifferentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning\nXiao-Yang Liu Rongyi Zhu Daochen Zha Jiechao Gao Shan Zhong Meikang Qiu\nabstract\rabstract: The surge in interest and application of large language models (LLMs) hassparked a drive to fine-tune these models to suit specific applications, suchas finance and medical science. However, concerns regarding data privacy haveemerged, especially when multiple stakeholders aim to collaboratively enhanceLLMs using sensitive data. In this scenario, federated learning becomes anatural choice, allowing decentralized fine-tuning without exposing raw data tocentral servers. Motivated by this, we investigate how data privacy can beensured in LLM fine-tuning through practical federated learning approaches,enabling secure contributions from multiple parties to enhance LLMs. Yet,challenges arise: 1) despite avoiding raw data exposure, there is a risk ofinferring sensitive information from model outputs, and 2) federated learningfor LLMs incurs notable communication overhead. To address these challenges,this article introduces DP-LoRA, a novel federated learning algorithm tailoredfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism thatadds noise in weight updates, maintaining individual data privacy whilefacilitating collaborative model training. Moreover, DP-LoRA optimizescommunication efficiency via low-rank adaptation, minimizing the transmissionof updated weights during distributed training. The experimental results acrossmedical, financial, and general datasets using various LLMs demonstrate thatDP-LoRA effectively ensures strict privacy constraints while minimizingcommunication overhead.\r2023-12-28\nSentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference\nAbhijit Mishra Mingda Li Soham Deo\nabstract\rabstract: This paper addresses the privacy and security concerns associated with deepneural language models, which serve as crucial components in various modernAI-based applications. These models are often used after being pre-trained andfine-tuned for specific tasks, with deployment on servers accessed through theinternet. However, this introduces two fundamental risks: (a) the transmissionof user inputs to the server via the network gives rise to interceptionvulnerabilities, and (b) privacy concerns emerge as organizations that deploysuch models store user data with restricted context. To address this, wepropose a novel method to adapt and fine-tune transformer-based language modelson passkey-encrypted user-specific text. The original pre-trained languagemodel first undergoes a quick adaptation (without any further pre-training)with a series of irreversible transformations applied to the tokenizer andtoken embeddings. This enables the model to perform inference on encryptedinputs while preventing reverse engineering of text from model parameters andintermediate outputs. After adaptation, models are fine-tuned on encryptedversions of existing training datasets. Experimental evaluation employingadapted versions of renowned models (e.g., BERT, RoBERTa) across establishedbenchmark English and multilingual datasets for text classification andsequence labeling shows that encrypted models achieve performance parity withtheir original counterparts. This serves to safeguard performance, privacy, andsecurity cohesively.\r2023-12-27\nSocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models\nManish Nagireddy Lamogha Chiazor Moninder Singh Ioana Baldini\nabstract\rabstract: Current datasets for unwanted social bias auditing are limited to studyingprotected demographic features such as race and gender. In this work, weintroduce a comprehensive benchmark that is meant to capture the amplificationof social bias, via stigmas, in generative language models. Taking inspirationfrom social science research, we start with a documented list of 93 US-centricstigmas and curate a question-answering (QA) dataset which involves simplesocial situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,with a variety of prompt styles, carefully constructed to systematically testfor both social bias and model robustness. We present results forSocialStigmaQA with two open source generative language models and we find thatthe proportion of socially biased output ranges from 45% to 59% across avariety of decoding strategies and prompting styles. We demonstrate that thedeliberate design of the templates in our benchmark (e.g., adding biasing textto the prompt or using different verbs that change the answer that indicatesbias) impacts the model tendencies to generate socially biased output.Additionally, through manual evaluation, we discover problematic patterns inthe generated chain-of-thought output that range from subtle bias to lack ofreasoning. Warning: This paper contains examples of text which are toxic, biased, andpotentially harmful.\rHow to Raise a Robot \u0026ndash; A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots\nNiklas Hemken Florian Jacob Fabian Peller-Konrad Rainer Kartmann Tamim Asfour Hannes Hartenstein\nabstract\rabstract: Humanoid robots will be able to assist humans in their daily life, inparticular due to their versatile action capabilities. However, while theserobots need a certain degree of autonomy to learn and explore, they also shouldrespect various constraints, for access control and beyond. We explore thenovel field of incorporating privacy, security, and access control constraintswith robot task planning approaches. We report preliminary results on theclassical symbolic approach, deep-learned neural networks, and modern ideasusing large language models as knowledge base. From analyzing their trade-offs,we conclude that a hybrid approach is necessary, and thereby present a new usecase for the emerging field of neuro-symbolic artificial intelligence.\r2023-12-26\nCan ChatGPT Read Who You Are?\nErik Derner Dalibor Kučera Nuria Oliver Jan Zahálka\nabstract\rabstract: The interplay between artificial intelligence (AI) and psychology,particularly in personality assessment, represents an important emerging areaof research. Accurate personality trait estimation is crucial not only forenhancing personalization in human-computer interaction but also for a widevariety of applications ranging from mental health to education. This paperanalyzes the capability of a generic chatbot, ChatGPT, to effectively inferpersonality traits from short texts. We report the results of a comprehensiveuser study featuring texts written in Czech by a representative populationsample of 155 participants. Their self-assessments based on the Big FiveInventory (BFI) questionnaire serve as the ground truth. We compare thepersonality trait estimations made by ChatGPT against those by human raters andreport ChatGPT\u0026rsquo;s competitive performance in inferring personality traits fromtext. We also uncover a \u0026lsquo;positivity bias\u0026rsquo; in ChatGPT\u0026rsquo;s assessments across allpersonality dimensions and explore the impact of prompt composition onaccuracy. This work contributes to the understanding of AI capabilities inpsychological assessment, highlighting both the potential and limitations ofusing large language models for personality inference. Our research underscoresthe importance of responsible AI development, considering ethical implicationssuch as privacy, consent, autonomy, and bias in AI applications.\rFedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models\nPanlong Wu Kangshuo Li Ting Wang Fangxin Wang\nabstract\rabstract: Foundation models have shown great success in natural language processing,computer vision, and multimodal tasks. FMs have a large number of modelparameters, thus requiring a substantial amount of data to help optimize themodel during the training. Federated learning has revolutionized machinelearning by enabling collaborative learning from decentralized data while stillpreserving the data privacy of clients. Despite the great benefits foundationmodels can have empowered by federated learning, they face severe computation,communication, and statistical challenges. In this paper, we propose a noveltwo-stage federated learning algorithm called FedMS. A global expert is trainedin the first stage and a local expert is trained in the second stage to providebetter personalization. We construct a Mixture of Foundation Models (MoFM) withthese two experts and design a gate neural network with an inserted gateadapter that joins the aggregation every communication round in the secondstage. To further adapt to edge computing scenarios with limited computationalresources, we design a novel Sparsely Activated LoRA (SAL) algorithm thatfreezes the pre-trained foundation model parameters inserts low-rank adaptationmatrices into transformer blocks and activates them progressively during thetraining. We employ extensive experiments to verify the effectiveness of FedMS,results show that FedMS outperforms other SOTA baselines by up to 55.25% indefault settings.\r2023-12-25\nLarge Language Models Empowered Autonomous Edge AI for Connected Intelligence\nYifei Shen Jiawei Shao Xinjie Zhang Zehong Lin Hao Pan Dongsheng Li Jun Zhang Khaled B. Letaief\nabstract\rabstract: The evolution of wireless networks gravitates towards connected intelligence,a concept that envisions seamless interconnectivity among humans, objects, andintelligence in a hyper-connected cyber-physical world. Edge artificialintelligence (Edge AI) is a promising solution to achieve connectedintelligence by delivering high-quality, low-latency, and privacy-preserving AIservices at the network edge. This article presents a vision of autonomous edgeAI systems that automatically organize, adapt, and optimize themselves to meetusers\u0026rsquo; diverse requirements, leveraging the power of large language models(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting thepowerful abilities of GPT in language understanding, planning, and codegeneration, as well as incorporating classic wisdom such as task-orientedcommunication and edge federated learning, we present a versatile frameworkthat efficiently coordinates edge AI models to cater to users\u0026rsquo; personal demandswhile automatically generating code to train new models in a privacy-preservingmanner. Experimental results demonstrate the system\u0026rsquo;s remarkable ability toaccurately comprehend user demands, efficiently execute AI models with minimalcost, and effectively create high-performance AI models at edge servers.\rGanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification\nHuali Ren Anli Yan Xiaojun Ren Pei-Gen Ye Chong-zhi Gao Zhili Zhou Jin Li\nabstract\rabstract: Deep neural networks (DNNs) are extensively employed in a wide range ofapplication scenarios. Generally, training a commercially viable neural networkrequires significant amounts of data and computing resources, and it is easyfor unauthorized users to use the networks illegally. Therefore, networkownership verification has become one of the most crucial steps in safeguardingdigital assets. To verify the ownership of networks, the existing networkfingerprinting approaches perform poorly in the aspects of efficiency,stealthiness, and discriminability. To address these issues, we propose anetwork fingerprinting approach, named as GanFinger, to construct the networkfingerprints based on the network behavior, which is characterized by networkoutputs of pairs of original examples and conferrable adversarial examples.Specifically, GanFinger leverages Generative Adversarial Networks (GANs) toeffectively generate conferrable adversarial examples with imperceptibleperturbations. These examples can exhibit identical outputs on copyrighted andpirated networks while producing different results on irrelevant networks.Moreover, to enhance the accuracy of fingerprint ownership verification, thenetwork similarity is computed based on the accuracy-robustness distance offingerprint examples\u0026rsquo;outputs. To evaluate the performance of GanFinger, weconstruct a comprehensive benchmark consisting of 186 networks with fivenetwork structures and four popular network post-processing techniques. Thebenchmark experiments demonstrate that GanFinger significantly outperforms thestate-of-the-arts in efficiency, stealthiness, and discriminability. Itachieves a remarkable 6.57 times faster in fingerprint generation and booststhe ARUC value by 0.175, resulting in a relative improvement of about 26%.\rA Split-and-Privatize Framework for Large Language Model Fine-Tuning\nXicong Shen Yang Liu Huiqi Liu Jue Hong Bing Duan Zirui Huang Yunlong Mao Ye Wu Di Wu\nabstract\rabstract: Fine-tuning is a prominent technique to adapt a pre-trained language model todownstream scenarios. In parameter-efficient fine-tuning, only a small subsetof modules are trained over the downstream datasets, while leaving the rest ofthe pre-trained model frozen to save computation resources. In recent years, apopular productization form arises as Model-as-a-Service (MaaS), in whichvendors provide abundant pre-trained language models, server resources and corefunctions, and customers can fine-tune, deploy and invoke their customizedmodel by accessing the one-stop MaaS with their own private dataset. In thispaper, we identify the model and data privacy leakage risks in MaaSfine-tuning, and propose a Split-and-Privatize (SAP) framework, which manage tomitigate the privacy issues by adapting the existing split learningarchitecture. The proposed SAP framework is sufficiently investigated byexperiments, and the results indicate that it can enhance the empirical privacyby 62% at the cost of 1% model performance degradation on the StanfordSentiment Treebank dataset.\r2023-12-22\nUnsupervised Melody-to-Lyric Generation\nYufei Tian Anjali Narayan-Chen Shereen Oraby Alessandra Cervone Gunnar Sigurdsson Chenyang Tao Wenbo Zhao Yiwen Chen Tagyoung Chung Jing Huang Nanyun Peng\nabstract\rabstract: Automatic melody-to-lyric generation is a task in which song lyrics aregenerated to go with a given melody. It is of significant practical interestand more challenging than unconstrained lyric generation as the music imposesadditional constraints onto the lyrics. The training data is limited as mostsongs are copyrighted, resulting in models that underfit the complicatedcross-modal relationship between melody and lyrics. In this work, we propose amethod for generating high-quality lyrics without training on any alignedmelody-lyric data. Specifically, we design a hierarchical lyric generationframework that first generates a song outline and second the complete lyrics.The framework enables disentanglement of training (based purely on text) frominference (melody-guided text generation) to circumvent the shortage ofparallel data. We leverage the segmentation and rhythm alignment between melody and lyricsto compile the given melody into decoding constraints as guidance duringinference. The two-step hierarchical design also enables content control viathe lyric outline, a much-desired feature for democratizing collaborative songcreation. Experimental results show that our model can generate high-qualitylyrics that are more on-topic, singable, intelligible, and coherent than strongbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,with a 24% relative overall quality improvement based on human ratings.\r2023-12-21\nHElium: A Language and Compiler for Fully Homomorphic Encryption with Support for Proxy Re-Encryption\nMirko Günther Lars Schütze Kilian Becher Thorsten Strufe Jeronimo Castrillon\nabstract\rabstract: Privacy-preserving analysis of confidential data can increase the value ofsuch data and even improve peoples\u0026rsquo; lives. Fully homomorphic encryption (FHE)can enable privacy-preserving analysis. However, FHE adds a large amount ofcomputational overhead and its efficient use requires a high level ofexpertise. Compilers can automate certain aspects such as parameterization andcircuit optimizations. This in turn makes FHE accessible to non-cryptographers.Yet, multi-party scenarios remain complicated and exclude many promising usecases such as analyses of large amounts of health records for medical research.Proxy re-encryption (PRE), a technique that allows the conversion of data frommultiple sources to a joint encryption key, can enable FHE for multi-partyscenarios. Today, there are no optimizing compilers for FHE with PREcapabilities. We propose HElium, the first optimizing FHE compiler with native support forproxy re-encryption. HElium features HEDSL, a domain-specific language (DSL)specifically designed for multi-party scenarios. By tracking encryption keysand transforming the computation circuit during compilation, HElium minimizesthe number of expensive PRE operations. We evaluate the effectiveness ofHElium\u0026rsquo;s optimizations based on the real-world use case of the tumor recurrencerate, a well-known subject of medical research. Our empirical evaluation showsthat HElium substantially reduces the overhead introduced through complex PREoperations, an effect that increases for larger amounts of input data.\rDeID-GPT: Zero-shot Medical Text De-Identification by GPT-4\nZhengliang Liu Yue Huang Xiaowei Yu Lu Zhang Zihao Wu Chao Cao Haixing Dai Lin Zhao Yiwei Li Peng Shu Fang Zeng Lichao Sun Wei Liu Dinggang Shen Quanzheng Li Tianming Liu Dajiang Zhu Xiang Li\nabstract\rabstract: The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT\u0026quot;) to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.\rExperimenting with Large Language Models and vector embeddings in NASA SciX\nSergi Blanco-Cuaresma Ioana Ciucă Alberto Accomazzi Michael J. Kurtz Edwin A. Henneken Kelly E. Lockhart Felix Grezes Thomas Allen Golnaz Shapurian Carolyn S. Grant Donna M. Thompson Timothy W. Hostetler Matthew R. Templeton Shinyi Chen Jennifer Koch Taylor Jacovich Daniel Chivvis Fernanda de Macedo Alves Jean-Claude Paquin Jennifer Bartlett Mugdha Polimera Stephanie Jarmak\nabstract\rabstract: Open-source Large Language Models enable projects such as NASA SciX (i.e.,NASA ADS) to think out of the box and try alternative approaches forinformation retrieval and data augmentation, while respecting data copyrightand users\u0026rsquo; privacy. However, when large language models are directly promptedwith questions without any context, they are prone to hallucination. At NASASciX we have developed an experiment where we created semantic vectors for ourlarge collection of abstracts and full-text content, and we designed a promptsystem to ask questions using contextual chunks from our system. Based on anon-systematic human evaluation, the experiment shows a lower degree ofhallucination and better responses when using Retrieval Augmented Generation.Further exploration is required to design new features and data augmentationprocesses at NASA SciX that leverages this technology while respecting the highlevel of trust and quality that the project holds.\rFedJudge: Federated Legal Large Language Model\nLinan Yue Qi Liu Yichao Du Weibo Gao Ye Liu Fangzhou Yao\nabstract\rabstract: Large Language Models (LLMs) have gained prominence in the field of LegalIntelligence, offering potential applications in assisting legal professionalsand laymen. However, the centralized training of these Legal LLMs raises dataprivacy concerns, as legal data is distributed among various institutionscontaining sensitive individual information. This paper addresses thischallenge by exploring the integration of Legal LLMs with Federated Learning(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally ondevices or clients, and their parameters are aggregated and distributed on acentral server, ensuring data privacy without directly sharing raw data.However, computation and communication overheads hinder the full fine-tuning ofLLMs under the FL setting. Moreover, the distribution shift of legal datareduces the effectiveness of FL methods. To this end, in this paper, we proposethe first Federated Legal Large Language Model (FedJudge) framework, whichfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudgeutilizes parameter-efficient fine-tuning methods to update only a fewadditional parameters during the FL training. Besides, we explore the continuallearning methods to preserve the global model\u0026rsquo;s important parameters whentraining local clients to mitigate the problem of data shifts. Extensiveexperimental results on three real-world datasets clearly validate theeffectiveness of FedJudge. Code is released athttps://github.com/yuelinan/FedJudge.\r2023-12-20\nImproving Data Minimization through Decentralized Data Architectures\nIlaria Battiston Peter Boncz\nabstract\rabstract: In this research project, we investigate an alternative to the standardcloud-centralized data architecture. Specifically, we aim to leave part of theapplication data under the control of the individual data owners indecentralized personal data stores. Our primary goal is to increase dataminimization, i. e., enabling more sensitive personal data to be under thecontrol of its owners while providing a straightforward and efficient frameworkto design architectures that allow applications to run and data to be analyzed.To serve this purpose, the centralized part of the schema contains aggregatingviews over this decentralized data. We propose to design a declarative languagethat extends SQL, for architects to specify different kinds of tables and viewsat the schema level, along with sensitive columns and their minimum granularitylevel of their aggregations. Local updates need to be reflected in thecentralized views while ensuring privacy throughout intermediate calculations;for this we pursue the integration of distributed materialized view maintenanceand multi-party computation (MPC) techniques. We finally aim to implement thissystem, where the personal data stores could either live in mobile devices orencrypted cloud storage, in order to evaluate its performance properties.\rMontsalvat: Intel SGX Shielding for GraalVM Native Images\nPeterson Yuhala Jämes Ménétrey Pascal Felber Valerio Schiavoni Alain Tchana Gaël Thomas Hugo Guiroux Jean-Pierre Lozi\nabstract\rabstract: The popularity of the Java programming language has led to its wide adoptionin cloud computing infrastructures. However, Java applications running inuntrusted clouds are vulnerable to various forms of privileged attacks. Theemergence of trusted execution environments (TEEs) such as Intel SGX mitigatesthis problem. TEEs protect code and data in secure enclaves inaccessible tountrusted software, including the kernel and hypervisors. To efficiently useTEEs, developers must manually partition their applications into trusted anduntrusted parts, in order to reduce the size of the trusted computing base(TCB) and minimise the risks of security vulnerabilities. However, partitioningapplications poses two important challenges: (i) ensuring efficient objectcommunication between the partitioned components, and (ii) ensuring theconsistency of garbage collection between the parts, especially withmemory-managed languages such as Java. We present Montsalvat, a tool whichprovides a practical and intuitive annotation-based partitioning approach forJava applications destined for secure enclaves. Montsalvat provides an RMI-likemechanism to ensure inter-object communication, as well as consistent garbagecollection across the partitioned components. We implement Montsalvat withGraalVM native-image, a tool for compiling Java applications ahead-of-time intostandalone native executables that do not require a JVM at runtime. Ourextensive evaluation with micro- and macro-benchmarks shows our partitioningapproach to boost performance in real-world applications\rAll but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models\nSeunghoo Hong Juhun Lee Simon S. Woo\nabstract\rabstract: Text-to-Image models such as Stable Diffusion have shown impressive imagegeneration synthesis, thanks to the utilization of large-scale datasets.However, these datasets may contain sexually explicit, copyrighted, orundesirable content, which allows the model to directly generate them. Giventhat retraining these large models on individual concept deletion requests isinfeasible, fine-tuning algorithms have been developed to tackle concepterasing in diffusion models. While these algorithms yield good concept erasure,they all present one of the following issues: 1) the corrupted feature spaceyields synthesis of disintegrated objects, 2) the initially synthesized contentundergoes a divergence in both spatial structure and semantics in the generatedimages, and 3) sub-optimal training updates heighten the model\u0026rsquo;s susceptibilityto utility harm. These issues severely degrade the original utility ofgenerative models. In this work, we present a new approach that solves all ofthese challenges. We take inspiration from the concept of classifier guidanceand propose a surgical update on the classifier guidance term whileconstraining the drift of the unconditional score term. Furthermore, ouralgorithm empowers the user to select an alternative to the erasing concept,allowing for more controllability. Our experimental results show that ouralgorithm not only erases the target concept effectively but also preserves themodel\u0026rsquo;s generation capability.\r2023-12-19\nImage Captioning with Multi-Context Synthetic Data\nFeipeng Ma Yizhou Zhou Fengyun Rao Yueyi Zhang Xiaoyan Sun\nabstract\rabstract: Image captioning requires numerous annotated image-text pairs, resulting insubstantial annotation costs. Recently, large models (e.g. diffusion models andlarge language models) have excelled in producing high-quality images and text.This potential can be harnessed to create synthetic image-text pairs fortraining captioning models. Synthetic data can improve cost and time efficiencyin data collection, allow for customization to specific domains, bootstrapgeneralization capability for zero-shot performance, and circumvent privacyconcerns associated with real-world data. However, existing methods struggle toattain satisfactory performance solely through synthetic data. We identify theissue as generated images from simple descriptions mostly capture a solitaryperspective with limited context, failing to align with the intricate scenesprevalent in real-world imagery. To tackle this, we present an innovativepipeline that introduces multi-context data generation. Beginning with aninitial text corpus, our approach employs a large language model to extractmultiple sentences portraying the same scene from diverse viewpoints. Thesesentences are then condensed into a single sentence with multiple contexts.Subsequently, we generate intricate images using the condensed captions throughdiffusion models. Our model is exclusively trained on synthetic image-textpairs crafted through this process. The effectiveness of our pipeline isvalidated through experimental results in both the in-domain and cross-domainsettings, where it achieves state-of-the-art performance on well-known datasetssuch as MSCOCO, Flickr30k, and NoCaps.\rLength 3 Check Digit Codes with Grouped Tags and Disjoint Coding Applications\nLarry A. Dunning\nabstract\rabstract: In 1969 J. Verhoeff provided the first examples of a decimal error detectingcode using a single check digit to provide protection against all single,transposition, and adjacent twin errors. The three codes he presented arelength 3-digit codes with 2 information digits. To date, the existence of alength 4-digit code with 3 information digits having these properties remainsan open question. Existence of a 4-digit code would imply the existence of 10disjoint 3-digit codes. Apparently, no pair of such disjoint 3-digit codes isknown. Phonetic errors, where 2-digit pairs of the forms X0 and 1X areinterchanged, are language dependent, but can often be eliminated. Alternate 3-digit codes are developed here which enhance the level ofprotection beyond Verhoeff\u0026rsquo;s codes while still optionally providing protectionagainst phonetic errors. Through almost-disjoint coding schemes, it is shownhow copies of these new codes can fill in the gap between such 3 and 4-digitcodes. The results are extended to other useful alphabet sizes such as 26 and36 with stronger permutation of digits error detection, and to \u0026ldquo;tag codes\u0026quot;where digits are grouped.\rA Performance Evaluation of a Quantized Large Language Model on Various Smartphones\nTolga Çöplü Marc Loedi Arto Bendiken Mykhailo Makohin Joshua J. Bouw Stephen Cobb\nabstract\rabstract: This paper explores the feasibility and performance of on-device largelanguage model (LLM) inference on various Apple iPhone models. Amidst the rapidevolution of generative AI, on-device LLMs offer solutions to privacy,security, and connectivity challenges inherent in cloud-based models.Leveraging existing literature on running multi-billion parameter LLMs onresource-limited devices, our study examines the thermal effects andinteraction speeds of a high-performing LLM across different smartphonegenerations. We present real-world performance results, providing insights intoon-device inference capabilities.\r2023-12-18\nAI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs\nYann Hicke Anmol Agarwal Qianou Ma Paul Denny\nabstract\rabstract: Responding to the thousands of student questions on online QA platforms eachsemester has a considerable human cost, particularly in computing courses withrapidly growing enrollments. To address the challenges of scalable andintelligent question-answering (QA), we introduce an innovative solution thatleverages open-source Large Language Models (LLMs) from the LLaMA-2 family toensure data privacy. Our approach combines augmentation techniques such asretrieval augmented generation (RAG), supervised fine-tuning (SFT), andlearning from human preferences data using Direct Preference Optimization(DPO). Through extensive experimentation on a Piazza dataset from anintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs ofpreference data, we demonstrate a significant 30% improvement in the quality ofanswers, with RAG being a particularly impactful addition. Our contributionsinclude the development of a novel architecture for educational QA, extensiveevaluations of LLM performance utilizing both human assessments and LLM-basedmetrics, and insights into the challenges and future directions of educationaldata processing. This work paves the way for the development of AI-TA, anintelligent QA assistant customizable for courses with an online QA platform\rOpportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview\nLiang Zhang Zhelun Chen\nabstract\rabstract: In recent years, the rapid advancement and impressive capabilities of LargeLanguage Models (LLMs) have been evident across various domains. This paperexplores the application, implications, and potential of LLMs in buildingenergy efficiency and decarbonization studies. The wide-ranging capabilities ofLLMs are examined in the context of the building energy field, includingintelligent control systems, code generation, data infrastructure, knowledgeextraction, and education. Despite the promising potential of LLMs, challengesincluding complex and expensive computation, data privacy, security andcopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.The paper concludes with a call for future research focused on the enhancementof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative researchbetween AI and energy experts.\r2023-12-17\nFedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion\nWei Tang Zhiqian Wu Yixin Cao Yong Liao Pengyuan Zhou\nabstract\rabstract: Knowledge graph completion (KGC) aims to predict missing facts in knowledgegraphs (KGs), which is crucial as modern KGs remain largely incomplete. Whiletraining KGC models on multiple aligned KGs can improve performance, previousmethods that rely on transferring raw data among KGs raise privacy concerns. Toaddress this challenge, we propose a new federated learning framework thatimplicitly aggregates knowledge from multiple KGs without demanding raw dataexchange and entity alignment. We treat each KG as a client that trains a locallanguage model through textbased knowledge representation learning. A centralserver then aggregates the model weights from clients. As natural languageprovides a universal representation, the same knowledge thus has similarsemantic representations across KGs. As such, the aggregated language model canleverage complementary knowledge from multilingual KGs without demanding rawuser data sharing. Extensive experiments on a benchmark dataset demonstratethat our method substantially improves KGC on multilingual KGs, achievingcomparable performance to state-of-the-art alignment-based models withoutrequiring any labeled alignments or raw user data sharing. Our codes will bepublicly available.\r2023-12-15\nVoCopilot: Voice-Activated Tracking of Everyday Interactions\nSheen An Goh Manoj Gulati Ambuj Varshney\nabstract\rabstract: Voice plays an important role in our lives by facilitating communication,conveying emotions, and indicating health. Therefore, tracking vocalinteractions can provide valuable insight into many aspects of our lives. Thispaper presents our ongoing efforts to design a new vocal tracking system wecall VoCopilot. VoCopilot is an end-to-end system centered around anenergy-efficient acoustic hardware and firmware combined with advanced machinelearning models. As a result, VoCopilot is able to continuously trackconversations, record them, transcribe them, and then extract useful insightsfrom them. By utilizing large language models, VoCopilot ensures the user canextract useful insights from recorded interactions without having to learncomplex machine learning techniques. In order to protect the privacy of endusers, VoCopilot uses a novel wake-up mechanism that only records conversationsof end users. Additionally, all the rest of pipeline can be run on a commoditycomputer (Mac Mini M2). In this work, we show the effectiveness of VoCopilot inreal-world environment for two use cases.\rDistilling Large Language Models for Matching Patients to Clinical Trials\nMauro Nievas Aditya Basu Yanshan Wang Hrituraj Singh\nabstract\rabstract: The recent success of large language models (LLMs) has paved the way fortheir adoption in the high-stakes domain of healthcare. Specifically, theapplication of LLMs in patient-trial matching, which involves assessing patienteligibility against clinical trial\u0026rsquo;s nuanced inclusion and exclusion criteria,has shown promise. Recent research has shown that GPT-3.5, a widely recognizedLLM developed by OpenAI, can outperform existing methods with minimal \u0026lsquo;variableengineering\u0026rsquo; by simply comparing clinical trial information against patientsummaries. However, there are significant challenges associated with usingclosed-source proprietary LLMs like GPT-3.5 in practical healthcareapplications, such as cost, privacy and reproducibility concerns. To addressthese issues, this study presents the first systematic examination of theefficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA7B,13B, and 70B) for the task of patient-trial matching. Employing amultifaceted evaluation framework, we conducted extensive automated andhuman-centric assessments coupled with a detailed error analysis for eachmodel. To enhance the adaptability of open-source LLMs, we have created aspecialized synthetic dataset utilizing GPT-4, enabling effective fine-tuningunder constrained data conditions. Our findings reveal that open-source LLMs,when fine-tuned on this limited and synthetic dataset, demonstrate performanceparity with their proprietary counterparts. This presents a massive opportunityfor their deployment in real-world healthcare applications. To foster furtherresearch and applications in this field, we release both the annotatedevaluation dataset along with the fine-tuned LLM \u0026ndash; Trial-LLAMA \u0026ndash; for publicuse.\rPrivacy-Aware Document Visual Question Answering\nRubèn Tito Khanh Nguyen Marlon Tobaben Raouf Kerkouche Mohamed Ali Souibgui Kangsoo Jung Lei Kang Ernest Valveny Antti Honkela Mario Fritz Dimosthenis Karatzas\nabstract\rabstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.\rLLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers\nXuanqi Liu Zhuotao Liu\nabstract\rabstract: The community explored to build private inference frameworks fortransformer-based large language models (LLMs) in a server-client setting,where the server holds the model parameters and the client inputs its privatedata (or prompt) for inference. However, these frameworks impose significantoverhead when the private inputs are forward propagated through the originalLLMs. In this paper, we show that substituting the computation- andcommunication-heavy operators in the transformer architecture withprivacy-computing friendly approximations can greatly reduce the privateinference costs while incurring very minor impact on model performance.Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computingfriendly model inference pipeline achieves a $5\\times$ acceleration incomputation and an 80% reduction in communication overhead, while retainingnearly identical accuracy.\r2023-12-14\nChatSOS: LLM-based knowledge Q\u0026amp;A system for safety engineering\nHaiyang Tang Zhenyi Liu Dongping Chen Qingzhao Chu\nabstract\rabstract: Recent advancements in large language models (LLMs) have notably propellednatural language processing (NLP) capabilities, demonstrating significantpotential in safety engineering applications. Despite these advancements, LLMsface constraints in processing specialized tasks, attributed to factors such ascorpus size, input processing limitations, and privacy concerns. Obtaininguseful information from reliable sources in a limited time is crucial for LLM.Addressing this, our study introduces an LLM-based Q\u0026amp;A system for safetyengineering, enhancing the comprehension and response accuracy of the model. Weemployed prompt engineering to incorporate external knowledge databases, thusenriching the LLM with up-to-date and reliable information. The system analyzeshistorical incident reports through statistical methods, utilizes vectorembedding to construct a vector database, and offers an efficientsimilarity-based search functionality. Our findings indicate that theintegration of external knowledge significantly augments the capabilities ofLLM for in-depth problem analysis and autonomous task assignment. Iteffectively summarizes accident reports and provides pertinent recommendations.This integration approach not only expands LLM applications in safetyengineering but also sets a precedent for future developments towardsautomation and intelligent systems.\rRecovering from Privacy-Preserving Masking with Large Language Models\nArpita Vats Zhe Liu Peng Su Debjyoti Paul Yingyi Ma Yutong Pang Zeeshan Ahmed Ozlem Kalinli\nabstract\rabstract: Model adaptation is crucial to handle the discrepancy between proxy trainingdata and actual users data received. To effectively perform adaptation, textualdata of users is typically stored on servers or their local devices, wheredownstream natural language processing (NLP) models can be directly trainedusing such in-domain data. However, this might raise privacy and securityconcerns due to the extra risks of exposing user information to adversaries.Replacing identifying information in textual data with a generic marker hasbeen recently explored. In this work, we leverage large language models (LLMs)to suggest substitutes of masked tokens and have their effectiveness evaluatedon downstream language modeling tasks. Specifically, we propose multiplepre-trained and fine-tuned LLM-based approaches and perform empirical studieson various datasets for the comparison of these methods. Experimental resultsshow that models trained on the obfuscation corpora are able to achievecomparable performance with the ones trained on the original data withoutprivacy-preserving token masking.\r2023-12-13\nScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale\nMarkus Frohmann Carolin Holtermann Shahed Masoudian Anne Lauscher Navid Rekabsaz\nabstract\rabstract: Multi-task learning (MTL) has shown considerable practical benefits,particularly when using pre-trained language models (PLMs). While this iscommonly achieved by simultaneously learning $n$ tasks under a jointoptimization procedure, recent methods such as AdapterFusion structure theproblem into two distinct stages: (i) task learning, where knowledge specificto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)transfer, where this already learned knowledge is leveraged for a target task.This separation of concerns provides numerous benefits, such as promotingreusability, and addressing cases involving data privacy and societal concerns;on the flip side, current two-stage MTL methods come with the cost ofintroducing a substantial number of additional parameters. In this work, weaddress this issue by leveraging the usefulness of linearly scaling the outputrepresentations of source adapters for transfer learning. We introduceScaLearn, a simple and highly parameter-efficient two-stage MTL method thatcapitalizes on the knowledge of the source tasks by learning a minimal set ofscaling parameters that enable effective knowledge transfer to a target task.Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that ourScaLearn, in addition to facilitating the benefits of two-stage MTL,consistently outperforms strong baselines with only a small number of transferparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observethat ScaLearn maintains its strong abilities even when further reducingparameters through uniform scaling and layer-sharing, achieving similarlycompetitive results with only $8$ transfer parameters for each target task. Ourproposed approach thus demonstrates the power of simple scaling as a promisefor more efficient task transfer.\rEfficient Representation of the Activation Space in Deep Neural Networks\nTanya Akumu Celia Cintas Girmaw Abebe Tadesse Adebayo Oshingbesan Skyler Speakman Edward McFowland III\nabstract\rabstract: The representations of the activation space of deep neural networks (DNNs)are widely utilized for tasks like natural language processing, anomalydetection and speech recognition. Due to the diverse nature of these tasks andthe large size of DNNs, an efficient and task-independent representation ofactivations becomes crucial. Empirical p-values have been used to quantify therelative strength of an observed node activation compared to activationscreated by already-known inputs. Nonetheless, keeping raw data for thesecalculations increases memory resource consumption and raises privacy concerns.To this end, we propose a model-agnostic framework for creating representationsof activations in DNNs using node-specific histograms to compute p-values ofobserved activations without retaining already-known inputs. Our proposedapproach demonstrates promising potential when validated with multiple networkarchitectures across various downstream tasks and compared with the kerneldensity estimates and brute-force empirical baselines. In addition, theframework reduces memory usage by 30% with up to 4 times faster p-valuecomputing time while maintaining state of-the-art detection power in downstreamtasks such as the detection of adversarial attacks and synthesized content.Moreover, as we do not persist raw data at inference time, we could potentiallyreduce susceptibility to attacks and privacy issues.\rPUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning\nFlorian Bordes Shashank Shekhar Mark Ibrahim Diane Bouchacourt Pascal Vincent Ari S. Morcos\nabstract\rabstract: Synthetic image datasets offer unmatched advantages for designing andevaluating deep neural networks: they make it possible to (i) render as manydata samples as needed, (ii) precisely control each scene and yield granularground truth labels (and captions), (iii) precisely control distribution shiftsbetween training and testing to isolate variables of interest for soundexperimentation. Despite such promise, the use of synthetic image data is stilllimited \u0026ndash; and often played down \u0026ndash; mainly due to their lack of realism. Mostworks therefore rely on datasets of real images, which have often been scrapedfrom public images on the internet, and may have issues with regards toprivacy, bias, and copyright, while offering little control over how objectsprecisely appear. In this work, we present a path to democratize the use ofphotorealistic synthetic data: we develop a new generation of interactiveenvironments for representation learning research, that offer bothcontrollability and realism. We use the Unreal Engine, a powerful game enginewell known in the entertainment industry, to produce PUG (Photorealistic UnrealGraphics) environments and datasets for representation learning. In this paper,we demonstrate the potential of PUG to enable more rigorous evaluations ofvision models.\r2023-12-12\nEditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection\nXuanyu Zhang Runyi Li Jiwen Yu Youmin Xu Weiqi Li Jian Zhang\nabstract\rabstract: In the era where AI-generated content (AIGC) models can produce stunning andlifelike images, the lingering shadow of unauthorized reproductions andmalicious tampering poses imminent threats to copyright integrity andinformation security. Current image watermarking methods, while widely acceptedfor safeguarding visual content, can only protect copyright and ensuretraceability. They fall short in localizing increasingly realistic imagetampering, potentially leading to trust crises, privacy violations, and legaldisputes. To solve this challenge, we propose an innovative proactive forensicsframework EditGuard, to unify copyright protection and tamper-agnosticlocalization, especially for AIGC-based editing methods. It can offer ameticulous embedding of imperceptible watermarks and precise decoding oftampered areas and copyright information. Leveraging our observed fragility andlocality of image-into-image steganography, the realization of EditGuard can beconverted into a united image-bit steganography issue, thus completelydecoupling the training process from the tampering types. Extensive experimentsdemonstrate that our EditGuard balances the tamper localization accuracy,copyright recovery precision, and generalizability to various AIGC-basedtampering methods, especially for image forgery that is difficult for the nakedeye to detect. The project page is available athttps://xuanyuzhang21.github.io/project/editguard/.\rCode Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models\nSheng Zhang Hui Li\nabstract\rabstract: Code pre-trained language models (CPLMs) have received great attention sincethey can benefit various tasks that facilitate software development andmaintenance. However, CPLMs are trained on massive open-source code, raisingconcerns about potential data infringement. This paper launches the first studyof detecting unauthorized code use in CPLMs, i.e., Code Membership Inference(CMI) task. We design a framework Buzzer for different settings of CMI. Buzzerdeploys several inference techniques, including distilling the target CPLM,ensemble inference, and unimodal and bimodal calibration. Extensive experimentsshow that CMI can be achieved with high accuracy using Buzzer. Hence, Buzzercan serve as a CMI tool and help protect intellectual property rights.\rLanguage-Guided Transformer for Federated Multi-Label Classification\nI-Jieh Liu Ci-Siang Lin Fu-En Yang Yu-Chiang Frank Wang\nabstract\rabstract: Federated Learning (FL) is an emerging paradigm that enables multiple usersto collaboratively train a robust model in a privacy-preserving manner withoutsharing their private data. Most existing approaches of FL only considertraditional single-label image classification, ignoring the impact whentransferring the task to multi-label image classification. Nevertheless, it isstill challenging for FL to deal with user heterogeneity in their local datadistribution in the real-world FL scenario, and this issue becomes even moresevere in multi-label image classification. Inspired by the recent success ofTransformers in centralized settings, we propose a novel FL framework formulti-label classification. Since partial label correlation may be observed bylocal clients during training, direct aggregation of locally updated modelswould not produce satisfactory performances. Thus, we propose a novel FLframework of Language-Guided Transformer (FedLGT) to tackle this challengingtask, which aims to exploit and transfer knowledge across different clients forlearning a robust global model. Through extensive experiments on variousmulti-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT isable to achieve satisfactory performance and outperforms standard FL techniquesunder multi-label FL scenarios. Code is available athttps://github.com/Jack24658735/FedLGT.\rPractical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration\nWenjie Fu Huandong Wang Chen Gao Guanghua Liu Yong Li Tao Jiang\nabstract\rabstract: Membership Inference Attacks (MIA) aim to infer whether a target data recordhas been utilized for model training or not. Prior attempts have quantified theprivacy risks of language models (LMs) via MIAs, but there is still noconsensus on whether existing MIA algorithms can cause remarkable privacyleakage on practical Large Language Models (LLMs). Existing MIAs designed forLMs can be classified into two categories: reference-free and reference-basedattacks. They are both based on the hypothesis that training recordsconsistently strike a higher probability of being sampled. Nevertheless, thishypothesis heavily relies on the overfitting of target models, which will bemitigated by multiple regularization methods and the generalization of LLMs.The reference-based attack seems to achieve promising effectiveness in LLMs,which measures a more reliable membership signal by comparing the probabilitydiscrepancy between the target model and the reference model. However, theperformance of reference-based attack is highly dependent on a referencedataset that closely resembles the training dataset, which is usuallyinaccessible in the practical scenario. Overall, existing MIAs are unable toeffectively unveil privacy leakage over practical fine-tuned LLMs that areoverfitting-free and private. We propose a Membership Inference Attack based onSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, sincememorization in LLMs is inevitable during the training process and occursbefore overfitting, we introduce a more reliable membership signal,probabilistic variation, which is based on memorization rather thanoverfitting. Furthermore, we introduce a self-prompt approach, which constructsthe dataset to fine-tune the reference model by prompting the target LLMitself. In this manner, the adversary can collect a dataset with a similardistribution from public APIs.\r2023-12-11\nPerformance-lossless Black-box Model Watermarking\nNa Zhao Kejiang Chen Weiming Zhang Nenghai Yu\nabstract\rabstract: With the development of deep learning, high-value and high-cost models havebecome valuable assets, and related intellectual property protectiontechnologies have become a hot topic. However, existing model watermarking workin black-box scenarios mainly originates from training-based backdoor methods,which probably degrade original task performance. To address this, we propose abranch backdoor-based model watermarking protocol to protect model intellectualproperty, where a construction based on a message authentication scheme isadopted as the branch indicator. We prove the lossless performance of theprotocol by reduction. Taking the language generation task as an instance, weshow the effectiveness of the proposed protocol.\rMERGE: Fast Private Text Generation\nZi Liang Pinghui Wang Ruofei Zhang Nuo Xu Lifeng Xing Shuo Zhang\nabstract\rabstract: The drastic increase in language models\u0026rsquo; parameters has led to a new trend ofdeploying models in cloud servers, raising growing concerns about privateinference for Transformer-based models. Existing two-party privacy-preservingtechniques, however, only take into account natural language understanding(NLU) scenarios. Private inference in natural language generation (NLG),crucial for applications like translation and code completion, remainsunderexplored.In addition, previous privacy-preserving techniques suffer fromconvergence issues during model training and exhibit poor inference speed whenused with NLG models due to the neglect of time-consuming operations inauto-regressive generations. To address these issues, we propose a fast privatetext generation framework for Transformer-based language models, namelyMERGE.MERGE reuses the output hidden state as the word embedding to bypass theembedding computation and reorganize the linear operations in the Transformermodule to accelerate the forward procedure. Extensive experiments show thatMERGE achieves a 26.5x speedup to the vanilla encrypted model under thesequence length 512, and reduces 80% communication cost, with an up to 10xspeedup to state-of-the-art approximated models.\rInferDPT: Privacy-Preserving Inference for Black-box Large Language Model\nMeng Tong Kejiang Chen Jie Zhang Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: Large language models (LLMs), like ChatGPT, have greatly simplified textgeneration tasks. However, they have also raised concerns about privacy riskssuch as data leakage and unauthorized data collection. Existing solutions forprivacy-preserving inference face practical challenges related to computationtime and communication costs. In this paper, we propose InferDPT, the firstpractical framework for the privacy-preserving Inference of black-box LLMs,implementing Differential Privacy in Text generation. InferDPT comprises twokey modules: the \u0026ldquo;perturbation module\u0026rdquo; utilizes the exponential mechanism togenerate a perturbed prompt, facilitating privacy-preserving inference withblack-box LLMs, and the \u0026ldquo;extraction module\u0026rdquo;, inspired by knowledge distillationand retrieval-augmented generation, extracts coherent and consistent text fromthe perturbed generation result, ensuring successful text generationcompletion. To address privacy concerns related to previous exponentialmechanisms\u0026rsquo; susceptibility to embedding revision attacks, we introduce RANTEXT,a novel differential privacy mechanism integrated into the perturbation moduleof InferDPT, which introduces the concept of \u0026ldquo;RANdom adjacency\u0026rdquo; for TEXTperturbation within the prompt. Experimental results across three datasetsdemonstrate that the text generation quality of InferDPT is comparable to thatof non-private GPT-4, and RANTEXT surpasses existing state-of-the-artmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy andutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achievesan average privacy protection rate exceeding 90% against embedding revisionattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higherthan that of CUSTEXT+.\r2023-12-10\nECHO: An Automated Contextual Inquiry Framework for Anonymous Qualitative Studies using Conversational Assistants\nRishika Dwaraghanath Rahul Majethia Sanjana Gautam\nabstract\rabstract: Qualitative research studies often employ a contextual inquiry, or a fieldstudy that involves in-depth observation and interviews of a small sample ofstudy participants, in-situ, to gain a robust understanding of the reasons andcircumstances that led to the participant\u0026rsquo;s thoughts, actions, and experiencesregarding the domain of interest. Contextual inquiry, especially in sensitivedata studies, can be a challenging task due to reasons such as participantprivacy, as well as physical constraints such as in-person presence and manualanalysis of the qualitative data gathered. In this work, we discuss Enqu^eteContextuelle Habile Ordinateur (ECHO); a virtual-assistant framework toautomate the erstwhile manual process of conducting contextual inquiries andanalysing the respondents\u0026rsquo; subjective qualitative data. ECHO automates thecontextual inquiry pipeline, while not compromising on privacy preservation orresponse integrity. Its adaptive conversational interface enables respondentsto provide unstructured or semi-structured responses in free-form naturallanguage, allowing researchers to explore larger narratives in participantresponse data. It supports response-driven exploratory questions and automatescoding methodologies for qualitative data, thus enabling the inquirer to divedeeper into correlated questions and to do better cause-effect analysis. Itfocuses on addressing the limitations of manual annotation, bringingstandardisation to free-form text, and eliminating perspective bias amongstdifferent reviewers of subjective responses. A participatory mental healthstudy was conducted on 167 young adults bifurcated into two focus groups; oneof which was administered a conventional contextual inquiry, and the other viaECHO, virtually. ECHO outperformed on participant transparency, response detailand median time required for end-to-end inquiry completion, per participant.\rMutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer\nYongheng Deng Ziqing Qiao Ju Ren Yang Liu Yaoxue Zhang\nabstract\rabstract: While large language models (LLMs) are empowered with broad knowledge, theirtask-specific performance is often suboptimal. It necessitates fine-tuning LLMswith task-specific data, but such data may be inaccessible due to privacyconcerns. In this paper, we propose a novel approach to enhance LLMs withsmaller language models (SLMs) that are trained on clients using their privatetask-specific data. To enable mutual enhancement between LLMs and SLMs, wepropose CrossLM, where the SLMs promote the LLM to generate task-specifichigh-quality data, and both the LLM and SLMs are enhanced with the generateddata. We evaluate CrossLM using publicly accessible language models across arange of benchmark tasks. The results demonstrate that CrossLM significantlyenhances the task-specific performance of SLMs on clients and the LLM on thecloud server simultaneously while preserving the LLM\u0026rsquo;s generalizationcapability.\r2023-12-08\nA Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge Access and Identifying Risks to Workers\nAnna Gausen Bhaskar Mitra Siân Lindley\nabstract\rabstract: Organisations generate vast amounts of information, which has resulted in along-term research effort into knowledge access systems for enterprisesettings. Recent developments in artificial intelligence, in relation to largelanguage models, are poised to have significant impact on knowledge access.This has the potential to shape the workplace and knowledge in new andunanticipated ways. Many risks can arise from the deployment of these types ofAI systems, due to interactions between the technical system and organisationalpower dynamics. This paper presents the Consequence-Mechanism-Risk framework to identifyrisks to workers from AI-mediated enterprise knowledge access systems. We havedrawn on wide-ranging literature detailing risks to workers, and categorisedrisks as being to worker value, power, and wellbeing. The contribution of ourframework is to additionally consider (i) the consequences of these systemsthat are of moral import: commodification, appropriation, concentration ofpower, and marginalisation, and (ii) the mechanisms, which represent how theseconsequences may take effect in the system. The mechanisms are a means ofcontextualising risk within specific system processes, which is critical formitigation. This framework is aimed at helping practitioners involved in thedesign and deployment of AI-mediated knowledge access systems to consider therisks introduced to workers, identify the precise system mechanisms thatintroduce those risks and begin to approach mitigation. Future work could applythis framework to other technological systems to promote the protection ofworkers and other groups.\rTypeFly: Flying Drones with Large Language Model\nGuojun Chen Xiaojing Yu Lin Zhong\nabstract\rabstract: Commanding a drone with a natural language is not only user-friendly but alsoopens the door for emerging language agents to control the drone. Emerginglarge language models (LLMs) provide a previously impossible opportunity toautomatically translate a task description in a natural language to a programthat can be executed by the drone. However, powerful LLMs and their visioncounterparts are limited in three important ways. First, they are onlyavailable as cloud-based services. Sending images to the cloud raises privacyconcerns. Second, they are expensive, costing proportionally to the requestsize. Finally, without expensive fine-tuning, existing LLMs are quite limitedin their capability of writing a program for specialized systems like drones. In this paper, we present a system called TypeFly that tackles the abovethree problems using a combination of edge-based vision intelligence, novelprogramming language design, and prompt engineering. Instead of the familiarPython, TypeFly gets a cloud-based LLM service to write a program in a small,custom language called MiniSpec, based on task and scene descriptions inEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)but also able to consult the LLM during their execution using a special skillcalled query. Using a set of increasingly challenging drone tasks, we show thatdesign choices made by TypeFly can reduce both the cost of LLM service and thetask execution time by more than 2x. More importantly, query and promptengineering techniques contributed by TypeFly significantly improve the chanceof success of complex tasks.\rOn the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook\nMingyuan Fan Chengyu Wang Cen Chen Yang Liu Jun Huang\nabstract\rabstract: Diffusion models and large language models have emerged as leading-edgegenerative models, revolutionizing various aspects of human life. However, thepractical implementations of these models have also exposed inherent risks,bringing to the forefront their evil sides and sparking concerns regardingtheir trustworthiness. Despite the wealth of literature on this subject, acomprehensive survey specifically delving into the intersection of large-scalegenerative models and their trustworthiness remains largely absent. To bridgethis gap, this paper investigates both the long-standing and emerging threatsassociated with these models across four fundamental dimensions: 1) privacy, 2)security, 3) fairness, and 4) responsibility. Based on the investigationresults, we develop an extensive map outlining the trustworthiness of largegenerative models. After that, we provide practical recommendations andpotential research directions for future secure applications equipped withlarge generative models, ultimately promoting the trustworthiness of the modelsand benefiting the society as a whole.\r2023-12-07\nStableQ: Enhancing Data-Scarce Quantization with Text-to-Image Data\nYuhang Li Youngeun Kim Donghyun Lee Priyadarshini Panda\nabstract\rabstract: Though low-bit quantization enables efficient storage and inference of deepneural networks, it often requires the use of training data to maintainresilience against quantization errors. However, training data are frequentlysubject to privacy or copyright concerns. In this work, we address thechallenge of Data-Scarce Quantization, where access to training data isseverely limited or non-existent for quantization purposes. Conventionalapproaches typically rely on inverting dummy images or jointly traininggenerative models to produce synthetic input samples. However, these methodsstruggle to accurately recreate complex objects in large-scale datasets likeImageNet. To overcome these limitations, we introduce StableQ, a novel methodthat utilizes an advanced text-to-image diffusion model to generatehigh-resolution, photo-realistic synthetic data. To verify the quality of thegenerated data, we implement two robust filtering mechanisms. These mechanismsare designed to select images that closely resemble the intrinsiccharacteristics of the actual training data. Furthermore, in scenarios wherelimited training data are available, we use these data to guide the syntheticdata generation process by inverting a learnable token embedding in the textencoder. Our extensive experimental results demonstrate that StbaleQ sets a newbenchmark in both zero-shot and few-shot quantization, outperforming existingmethods in terms of accuracy and efficiency.\rDomain Private Transformers for Multi-Domain Dialog Systems\nAnmol Kabra Ethan R. Elenberg\nabstract\rabstract: Large, general purpose language models have demonstrated impressiveperformance across many different conversational domains. While multi-domainlanguage models achieve low overall perplexity, their outputs are notguaranteed to stay within the domain of a given input prompt. This paperproposes domain privacy as a novel way to quantify how likely a conditionallanguage model will leak across domains. We also develop policy functions basedon token-level domain classification, and propose an efficient fine-tuningmethod to improve the trained model\u0026rsquo;s domain privacy. Experiments on membershipinference attacks show that our proposed method has comparable resiliency tomethods adapted from recent literature on differentially private languagemodels.\rMaking Translators Privacy-aware on the User\u0026rsquo;s Side\nRyoma Sato\nabstract\rabstract: We propose PRISM to enable users of machine translation systems to preservethe privacy of data on their own initiative. There is a growing demand to applymachine translation systems to data that require privacy protection. Whileseveral machine translation engines claim to prioritize privacy, the extent andspecifics of such protection are largely ambiguous. First, there is often alack of clarity on how and to what degree the data is protected. Even ifservice providers believe they have sufficient safeguards in place,sophisticated adversaries might still extract sensitive information. Second,vulnerabilities may exist outside of these protective measures, such as withincommunication channels, potentially leading to data leakage. As a result, usersare hesitant to utilize machine translation engines for data demanding highlevels of privacy protection, thereby missing out on their benefits. PRISMresolves this problem. Instead of relying on the translation service to keepdata safe, PRISM provides the means to protect data on the user\u0026rsquo;s side. Thisapproach ensures that even machine translation engines with inadequate privacymeasures can be used securely. For platforms already equipped with privacysafeguards, PRISM acts as an additional protection layer, reinforcing theirsecurity furthermore. PRISM adds these privacy features without significantlycompromising translation accuracy. Our experiments demonstrate theeffectiveness of PRISM using real-world translators, T5 and ChatGPT(GPT-3.5-turbo), and the datasets with two languages. PRISM effectivelybalances privacy protection with translation accuracy.\rDefense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks\nXiaobei Yan Chip Hong Chang Tianwei Zhang\nabstract\rabstract: Artificial Intelligence (AI) hardware accelerators have been widely adoptedto enhance the efficiency of deep learning applications. However, they alsoraise security concerns regarding their vulnerability to power side-channelattacks (SCA). In these attacks, the adversary exploits unintendedcommunication channels to infer sensitive information processed by theaccelerator, posing significant privacy and copyright risks to the models.Advanced machine learning algorithms are further employed to facilitate theside-channel analysis and exacerbate the privacy issue of AI accelerators.Traditional defense strategies naively inject execution noise to the runtime ofAI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguardFPGA-based AI accelerators and mitigate model extraction threats viapower-based SCAs. The key insight of AIAShield is to leverage the prominentadversarial attack technique from the machine learning community to craftdelicate noise, which can significantly obfuscate the adversary\u0026rsquo;s side-channelobservation while incurring minimal overhead to the execution of the protectedmodel. At the hardware level, we design a new module based on ring oscillatorsto achieve fine-grained noise generation. At the algorithm level, we repurposeNeural Architecture Search to worsen the adversary\u0026rsquo;s extraction results.Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA)demonstrate that AIAShield outperforms existing solutions with excellenttransferability.\r2023-12-06\nUnderstanding (Un)Intended Memorization in Text-to-Image Generative Models\nAli Naseh Jaechul Roh Amir Houmansadr\nabstract\rabstract: Multimodal machine learning, especially text-to-image models like StableDiffusion and DALL-E 3, has gained significance for transforming text intodetailed images. Despite their growing use and remarkable generative capabilities, there is apressing need for a detailed examination of these models\u0026rsquo; behavior,particularly with respect to memorization. Historically, memorization inmachine learning has been context-dependent, with diverse definitions emergingfrom classification tasks to complex models like Large Language Models (LLMs)and Diffusion models. Yet, a definitive concept of memorization that alignswith the intricacies of text-to-image synthesis remains elusive. Thisunderstanding is vital as memorization poses privacy risks yet is essential formeeting user expectations, especially when generating representations ofunderrepresented entities. In this paper, we introduce a specialized definitionof memorization tailored to text-to-image models, categorizing it into threedistinct types according to user expectations. We closely examine the subtledistinctions between intended and unintended memorization, emphasizing theimportance of balancing user privacy with the generative quality of the modeloutputs. Using the Stable Diffusion model, we offer examples to validate ourmemorization definitions and clarify their application.\rStop Hiding The Sharp Knives: The WebAssembly Linux Interface\nArjun Ramesh Tianshu Huang Ben L. Titzer Anthony Rowe\nabstract\rabstract: WebAssembly is gaining popularity as a portable binary format targetable frommany programming languages. With a well-specified low-level virtual instructionset, minimal memory footprint and many high-performance implementations, it hasbeen successfully adopted for lightweight in-process memory sandboxing in manycontexts. Despite these advantages, WebAssembly lacks many standard systeminterfaces, making it difficult to reuse existing applications. This paper proposes WALI: The WebAssembly Linux Interface, a thin layer overLinux\u0026rsquo;s userspace system calls, creating a new class of virtualization whereWebAssembly seamlessly interacts with native processes and the underlyingoperating system. By virtualizing the lowest level of userspace, WALI offersapplication portability with little effort and reuses existing compilerbackends. With WebAssembly\u0026rsquo;s control flow integrity guarantees, these modulesgain an additional level of protection against remote code injection attacks.Furthermore, capability-based APIs can themselves be virtualized andimplemented in terms of WALI, improving reuse and robustness through betterlayering. We present an implementation of WALI in a modern WebAssembly engineand evaluate its performance on a number of applications which we can nowcompile with mostly trivial effort.\r2023-12-05\nDEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models\nXinwei Wu Junzhuo Li Minghui Xu Weilong Dong Shuangzhi Wu Chao Bian Deyi Xiong\nabstract\rabstract: Large language models pretrained on a huge amount of data capture richknowledge and information in the training data. The ability of datamemorization and regurgitation in pretrained language models, revealed inprevious studies, brings the risk of data leakage. In order to effectivelyreduce these risks, we propose a framework DEPN to Detect and Edit PrivacyNeurons in pretrained language models, partially inspired by knowledge neuronsand model editing. In DEPN, we introduce a novel method, termed as privacyneuron detector, to locate neurons associated with private information, andthen edit these detected privacy neurons by setting their activations to zero.Furthermore, we propose a privacy neuron aggregator dememorize privateinformation in a batch processing manner. Experimental results show that ourmethod can significantly and efficiently reduce the exposure of private dataleakage without deteriorating the performance of the model. Additionally, weempirically demonstrate the relationship between model memorization and privacyneurons, from multiple perspectives, including model size, training time,prompts, privacy neuron distribution, illustrating the robustness of ourapproach.\r2023-12-04\nResponsible Task Automation: Empowering Large Language Models as Responsible Task Automators\nZhizheng Zhang Xiaoyi Zhang Wenxuan Xie Yan Lu\nabstract\rabstract: The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users\u0026rsquo; privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios.\rThe Queen\u0026rsquo;s Guard: A Secure Enforcement of Fine-grained Access Control In Distributed Data Analytics Platforms\nFahad Shaon Sazzadur Rahaman Murat Kantarcioglu\nabstract\rabstract: Distributed data analytics platforms (i.e., Apache Spark, Hadoop) providehigh-level APIs to programmatically write analytics tasks that are rundistributedly in multiple computing nodes. The design of these frameworks wasprimarily motivated by performance and usability. Thus, the security takes aback seat. Consequently, they do not inherently support fine-grained accesscontrol or offer any plugin mechanism to enable it, making them risky to beused in multi-tier organizational settings. There have been attempts to build \u0026ldquo;add-on\u0026rdquo; solutions to enable fine-grainedaccess control for distributed data analytics platforms. In this paper, first,we show that straightforward enforcement of ``add-on\u0026rsquo;\u0026rsquo; access control isinsecure under adversarial code execution. Specifically, we show that anattacker can abuse platform-provided APIs to evade access controls withoutleaving any traces. Second, we designed a two-layered (i.e., proactive andreactive) defense system to protect against API abuses. On submission of a usercode, our proactive security layer statically screens it to find potentialattack signatures prior to its execution. The reactive security layer employscode instrumentation-based runtime checks and sandboxed execution to throttleany exploits at runtime. Next, we propose a new fine-grained access controlframework with an enhanced policy language that supports map and filterprimitives. Finally, we build a system named SecureDL with our new accesscontrol framework and defense system on top of Apache Spark, which ensuressecure access control policy enforcement under adversaries capable of executingcode. To the best of our knowledge, this is the first fine-grained attribute-basedaccess control framework for distributed data analytics platforms that issecure against platform API abuse attacks. Performance evaluation showed thatthe overhead due to added security is low.\r2023-12-01\nLinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices\nJunchen Zhao Yurun Song Simeng Liu Ian G. Harris Sangeetha Abdu Jyothi\nabstract\rabstract: Deploying Large Language Models (LLMs) locally on mobile devices presents asignificant challenge due to their extensive memory requirements. In thispaper, we introduce LinguaLinked, a system for decentralized, distributed LLMinference on mobile devices. LinguaLinked enables collaborative execution ofthe inference task across multiple trusted devices. LinguaLinked ensures dataprivacy by processing information locally. LinguaLinked uses three keystrategies. First, an optimized model assignment technique segments LLMs anduses linear optimization to align segments with each device\u0026rsquo;s capabilities.Second, an optimized data transmission mechanism ensures efficient andstructured data flow between model segments while also maintaining theintegrity of the original model structure. Finally, LinguaLinked incorporates aruntime load balancer that actively monitors and redistributes tasks amongmobile devices to prevent bottlenecks, enhancing the system\u0026rsquo;s overallefficiency and responsiveness. We demonstrate that LinguaLinked facilitatesefficient LLM inference while maintaining consistent throughput and minimallatency through extensive testing across various mobile devices, from high-endto low-end Android devices. In our evaluations, compared to the baseline,LinguaLinked achieves an inference performance acceleration of $1.11\\times$ to$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ withmulti-threading. Additionally, runtime load balancing yields an overallinference acceleration of $1.29\\times$ to $1.32\\times$.\r2023-11-30\nLocally Differentially Private Document Generation Using Zero Shot Prompting\nSaiteja Utpala Sara Hooker Pin Yu Chen\nabstract\rabstract: Numerous studies have highlighted the privacy risks associated withpretrained large language models. In contrast, our research offers a uniqueperspective by demonstrating that pretrained large language models caneffectively contribute to privacy preservation. We propose a locallydifferentially private mechanism called DP-Prompt, which leverages the power ofpretrained large language models and zero-shot prompting to counter authorde-anonymization attacks while minimizing the impact on downstream utility.When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),we observe a notable reduction in the success rate of de-anonymization attacks,showing that it surpasses existing approaches by a considerable margin despiteits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt(with ChatGPT) perfectly recovers the clean sentiment F1 score while achievinga 46% reduction in author identification F1 score against static attackers anda 26% reduction against adaptive attackers. We conduct extensive experimentsacross six open-source large language models, ranging up to 7 billionparameters, to analyze various effects of the privacy-utility tradeoff.\rSituating the social issues of image generation models in the model life cycle: a sociotechnical approach\nAmelia Katirai Noa Garcia Kazuki Ide Yuta Nakashima Atsuo Kishimoto\nabstract\rabstract: The race to develop image generation models is intensifying, with a rapidincrease in the number of text-to-image models available. This is coupled withgrowing public awareness of these technologies. Though other generative AImodels\u0026ndash;notably, large language models\u0026ndash;have received recent critical attentionfor the social and other non-technical issues they raise, there has beenrelatively little comparable examination of image generation models. This paperreports on a novel, comprehensive categorization of the social issuesassociated with image generation models. At the intersection of machinelearning and the social sciences, we report the results of a survey of theliterature, identifying seven issue clusters arising from image generationmodels: data issues, intellectual property, bias, privacy, and the impacts onthe informational, cultural, and natural environments. We situate these socialissues in the model life cycle, to aid in considering where potential issuesarise, and mitigation may be needed. We then compare these issue clusters withwhat has been reported for large language models. Ultimately, we argue that therisks posed by image generation models are comparable in severity to the risksposed by large language models, and that the social impact of image generationmodels must be urgently considered.\rCan Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?\nZhengyue Zhao Jinhao Duan Kaidi Xu Chenan Wang Rui Zhangp Zidong Dup Qi Guo Xing Hu\nabstract\rabstract: Stable Diffusion has established itself as a foundation model in generativeAI artistic applications, receiving widespread research and application. Somerecent fine-tuning methods have made it feasible for individuals to implantpersonalized concepts onto the basic Stable Diffusion model with minimalcomputational costs on small datasets. However, these innovations have alsogiven rise to issues like facial privacy forgery and artistic copyrightinfringement. In recent studies, researchers have explored the addition ofimperceptible adversarial perturbations to images to prevent potentialunauthorized exploitation and infringements when personal data is used forfine-tuning Stable Diffusion. Although these studies have demonstrated theability to protect images, it is essential to consider that these methods maynot be entirely applicable in real-world scenarios. In this paper, wesystematically evaluate the use of perturbations to protect images within apractical threat model. The results suggest that these approaches may not besufficient to safeguard image privacy and copyright effectively. Furthermore,we introduce a purification method capable of removing protected perturbationswhile preserving the original image structure to the greatest extent possible.Experiments reveal that Stable Diffusion can effectively learn from purifiedimages over all protective methods.\rTrustMark: Universal Watermarking for Arbitrary Resolution Images\nTu Bui Shruti Agarwal John Collomosse\nabstract\rabstract: Imperceptible digital watermarking is important in copyright protection,misinformation prevention, and responsible generative AI. We propose TrustMark- a GAN-based watermarking method with novel design in architecture andspatio-spectra losses to balance the trade-off between watermarked imagequality with the watermark recovery accuracy. Our model is trained withrobustness in mind, withstanding various in- and out-place perturbations on theencoded image. Additionally, we introduce TrustMark-RM - a watermark removermethod useful for re-watermarking. Our methods achieve state-of-art performanceon 3 benchmarks comprising arbitrary resolution images.\rImproving the Robustness of Transformer-based Large Language Models with Dynamic Attention\nLujia Shen Yuwen Pu Shouling Ji Changjiang Li Xuhong Zhang Chunpeng Ge Ting Wang\nabstract\rabstract: Transformer-based models, such as BERT and GPT, have been widely adopted innatural language processing (NLP) due to their exceptional performance.However, recent studies show their vulnerability to textual adversarial attackswhere the model\u0026rsquo;s output can be misled by intentionally manipulating the textinputs. Despite various methods that have been proposed to enhance the model\u0026rsquo;srobustness and mitigate this vulnerability, many require heavy consumptionresources (e.g., adversarial training) or only provide limited protection(e.g., defensive dropout). In this paper, we propose a novel method calleddynamic attention, tailored for the transformer architecture, to enhance theinherent robustness of the model itself against various adversarial attacks.Our method requires no downstream task knowledge and does not incur additionalcosts. The proposed dynamic attention consists of two modules: (I) attentionrectification, which masks or weakens the attention value of the chosen tokens,and (ii) dynamic modeling, which dynamically builds the set of candidatetokens. Extensive experiments demonstrate that dynamic attention significantlymitigates the impact of adversarial attacks, improving up to 33% betterperformance than previous methods against widely-used adversarial attacks. Themodel-level design of dynamic attention enables it to be easily combined withother defense methods (e.g., adversarial training) to further enhance themodel\u0026rsquo;s robustness. Furthermore, we demonstrate that dynamic attentionpreserves the state-of-the-art robustness space of the original model comparedto other dynamic modeling methods.\r2023-11-29\nThe AutoSPADA Platform: User-Friendly Edge Computing for Distributed Learning and Data Analytics in Connected Vehicles\nAdrian Nilsson Simon Smith Jonas Hagmar Magnus Önnheim Mats Jirstrand\nabstract\rabstract: Contemporary connected vehicles host numerous applications, such asdiagnostics and navigation, and new software is continuously being developed.However, the development process typically requires offline batch processing oflarge data volumes. In an edge computing approach, data analysts and developerscan instead process sensor data directly on computational resources insidevehicles. This enables rapid prototyping to shorten development cycles andreduce the time to create new business values or insights. This paper presentsthe design, implementation, and operation of the AutoSPADA edge computingplatform for distributed data analytics. The platform\u0026rsquo;s design followsscalability, reliability, resource efficiency, privacy, and security principlespromoted through mature and industrially proven technologies. In AutoSPADA,computational tasks are general Python scripts, and we provide a library to,for example, read signals from the vehicle and publish results to the cloud.Hence, users only need Python knowledge to use the platform. Moreover, theplatform is designed to be extended to support additional programminglanguages.\rProbabilistic Copyright Protection Can Fail for Text-to-Image Generative Models\nXiang Li Qianli Shen Kenji Kawaguchi\nabstract\rabstract: The booming use of text-to-image generative models has raised concerns abouttheir high risk of producing copyright-infringing content. While probabilisticcopyright protection methods provide a probabilistic guarantee against suchinfringement, in this paper, we introduce Virtually Assured AmplificationAttack (VA3), a novel online attack framework that exposes the vulnerabilitiesof these protection mechanisms. The proposed framework significantly amplifiesthe probability of generating infringing content on the sustained interactionswith generative models and a lower-bounded success probability of eachengagement. Our theoretical and experimental results demonstrate theeffectiveness of our approach and highlight the potential risk of implementingprobabilistic copyright protection in practical applications of text-to-imagegenerative models. Code is available at https://github.com/South7X/VA3.\rClinical Risk Prediction Using Language Models: Benefits And Considerations\nAngeela Acharya Sulabh Shrestha Anyi Chen Joseph Conte Sanja Avramovic Siddhartha Sikdar Antonios Anastasopoulos Sanmay Das\nabstract\rabstract: The utilization of Electronic Health Records (EHRs) for clinical riskprediction is on the rise. However, strict privacy regulations limit access tocomprehensive health records, making it challenging to apply standard machinelearning algorithms in practical real-world scenarios. Previous research hasaddressed this data limitation by incorporating medical ontologies andemploying transfer learning methods. In this study, we investigate thepotential of leveraging language models (LMs) as a means to incorporatesupplementary domain knowledge for improving the performance of variousEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR datasuch as clinical notes, this study focuses on using textual descriptions withinstructured EHR to make predictions exclusively based on that information. Weextensively compare against previous approaches across various data types andsizes. We find that employing LMs to represent structured EHRs, such asdiagnostic histories, leads to improved or at least comparable performance indiverse risk prediction tasks. Furthermore, LM-based approaches offer numerousadvantages, including few-shot learning, the capability to handle previouslyunseen medical concepts, and adaptability to various medical vocabularies.Nevertheless, we underscore, through various experiments, the importance ofbeing cautious when employing such models, as concerns regarding thereliability of LMs persist.\rIdentifying and Mitigating Vulnerabilities in LLM-Integrated Applications\nFengqing Jiang Zhangchen Xu Luyao Niu Boxin Wang Jinyuan Jia Bo Li Radha Poovendran\nabstract\rabstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users\u0026rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user\u0026rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.\r2023-11-28\nAGI: Artificial General Intelligence for Education\nEhsan Latif Gengchen Mai Matthew Nyaaba Xuansheng Wu Ninghao Liu Guoyu Lu Sheng Li Tianming Liu Xiaoming Zhai\nabstract\rabstract: Artificial general intelligence (AGI) has gained global recognition as afuture technology due to the emergence of breakthrough large language modelsand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventionalAI models, typically designed for a limited range of tasks, demand significantamounts of domain-specific data for training and may not always considerintricate interpersonal dynamics in education. AGI, driven by the recent largepre-trained models, represents a significant leap in the capability of machinesto perform tasks that require human-level intelligence, such as reasoning,problem-solving, decision-making, and even understanding human emotions andsocial interactions. This position paper reviews AGI\u0026rsquo;s key concepts,capabilities, scope, and potential within future education, including achievingfuture educational goals, designing pedagogy and curriculum, and performingassessments. It highlights that AGI can significantly improve intelligenttutoring systems, educational assessment, and evaluation procedures. AGIsystems can adapt to individual student needs, offering tailored learningexperiences. They can also provide comprehensive feedback on studentperformance and dynamically adjust teaching methods based on student progress.The paper emphasizes that AGI\u0026rsquo;s capabilities extend to understanding humanemotions and social interactions, which are critical in educational settings.The paper discusses that ethical issues in education with AGI include databias, fairness, and privacy and emphasizes the need for codes of conduct toensure responsible AGI use in academic settings like homework, teaching, andrecruitment. We also conclude that the development of AGI necessitatesinterdisciplinary collaborations between educators and AI engineers to advanceresearch and application efforts.\rPromptCARE: Prompt Copyright Protection by Watermark Injection and Verification\nHongwei Yao Jian Lou Kui Ren Zhan Qin\nabstract\rabstract: Large language models (LLMs) have witnessed a meteoric rise in popularityamong the general public users over the past few months, facilitating diversedownstream tasks with human-level accuracy and proficiency. Prompts play anessential role in this success, which efficiently adapt pre-trained LLMs totask-specific applications by simply prepending a sequence of tokens to thequery texts. However, designing and selecting an optimal prompt can be bothexpensive and demanding, leading to the emergence of Prompt-as-a-Serviceproviders who profit by providing well-designed prompts for authorized use.With the growing popularity of prompts and their indispensable role inLLM-based services, there is an urgent need to protect the copyright of promptsagainst unauthorized use. In this paper, we propose PromptCARE, the first framework for promptcopyright protection through watermark injection and verification. Promptwatermarking presents unique challenges that render existing watermarkingtechniques developed for model and dataset copyright verification ineffective.PromptCARE overcomes these hurdles by proposing watermark injection andverification schemes tailor-made for prompts and NLP characteristics. Extensiveexperiments on six well-known benchmark datasets, using three prevalentpre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate theeffectiveness, harmlessness, robustness, and stealthiness of PromptCARE.\rDe-identification of clinical free text using natural language processing: A systematic review of current approaches\nAleksandar Kovačević Bojana Bašaragin Nikola Milošević Goran Nenadić\nabstract\rabstract: Background: Electronic health records (EHRs) are a valuable resource fordata-driven medical research. However, the presence of protected healthinformation (PHI) makes EHRs unsuitable to be shared for research purposes.De-identification, i.e. the process of removing PHI is a critical step inmaking EHR data accessible. Natural language processing has repeatedlydemonstrated its feasibility in automating the de-identification process.Objectives: Our study aims to provide systematic evidence on how thede-identification of clinical free text has evolved in the last thirteen years,and to report on the performances and limitations of the currentstate-of-the-art systems. In addition, we aim to identify challenges andpotential research opportunities in this field. Methods: A systematic search inPubMed, Web of Science and the DBLP was conducted for studies published betweenJanuary 2010 and February 2023. Titles and abstracts were examined to identifythe relevant studies. Selected studies were then analysed in-depth, andinformation was collected on de-identification methodologies, data sources, andmeasured performance. Results: A total of 2125 publications were identified forthe title and abstract screening. 69 studies were found to be relevant. Machinelearning (37 studies) and hybrid (26 studies) approaches are predominant, whilesix studies relied only on rules. Majority of the approaches were trained andevaluated on public corpora. The 2014 i2b2/UTHealth corpus is the mostfrequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016CEGS N-GRID (10 studies) corpora.\rPCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Models\nXuefeng Fan Dahao Fu Hangyu Gui Xinpeng Zhang Xiaoyi Zhou\nabstract\rabstract: Deep neural networks (DNNs) have achieved tremendous success in artificialintelligence (AI) fields. However, DNN models can be easily illegally copied,redistributed, or abused by criminals, seriously damaging the interests ofmodel inventors. The copyright protection of DNN models by neural networkwatermarking has been studied, but the establishment of a traceabilitymechanism for determining the authorized users of a leaked model is a newproblem driven by the demand for AI services. Because the existing traceabilitymechanisms are used for models without watermarks, a small number offalse-positives are generated. Existing black-box active protection schemeshave loose authorization control and are vulnerable to forgery attacks.Therefore, based on the idea of black-box neural network watermarking with thevideo framing and image perceptual hash algorithm, a passive copyrightprotection and traceability framework PCPT is proposed that uses an additionalclass of DNN models, improving the existing traceability mechanism that yieldsa small number of false-positives. Based on an authorization control strategyand image perceptual hash algorithm, a DNN model active copyright protectionand traceability framework ACPT is proposed. This framework uses theauthorization control center constructed by the detector and verifier. Thisapproach realizes stricter authorization control, which establishes a strongconnection between users and model owners, improves the framework security, andsupports traceability verification.\rThe Transformative Influence of Large Language Models on Software Development\nSajed Jalil\nabstract\rabstract: The increasing adoption and commercialization of generalized Large LanguageModels (LLMs) have profoundly impacted various aspects of our daily lives.Initially embraced by the computer science community, the versatility of LLMshas found its way into diverse domains. In particular, the software engineeringrealm has witnessed the most transformative changes. With LLMs increasinglyserving as AI Pair Programming Assistants spurred the development ofspecialized models aimed at aiding software engineers. Although this newparadigm offers numerous advantages, it also presents critical challenges andopen problems. To identify the potential and prevailing obstacles, wesystematically reviewed contemporary scholarly publications, emphasizing theperspectives of software developers and usability concerns. Preliminaryfindings underscore pressing concerns about data privacy, bias, andmisinformation. Additionally, we identified several usability challenges,including prompt engineering, increased cognitive demands, and mistrust.Finally, we introduce 12 open problems that we have identified through oursurvey, covering these various domains.\r2023-11-27\nDiffSLVA: Harnessing Diffusion Models for Sign Language Video Anonymization\nZhaoyang Xia Carol Neidle Dimitris N. Metaxas\nabstract\rabstract: Since American Sign Language (ASL) has no standard written form, Deaf signersfrequently share videos in order to communicate in their native language.However, since both hands and face convey critical linguistic information insigned languages, sign language videos cannot preserve signer privacy. Whilesigners have expressed interest, for a variety of applications, in signlanguage video anonymization that would effectively preserve linguisticcontent, attempts to develop such technology have had limited success, giventhe complexity of hand movements and facial expressions. Existing approachesrely predominantly on precise pose estimations of the signer in video footageand often require sign language video datasets for training. These requirementsprevent them from processing videos \u0026lsquo;in the wild,\u0026rsquo; in part because of thelimited diversity present in current sign language video datasets. To addressthese limitations, our research introduces DiffSLVA, a novel methodology thatutilizes pre-trained large-scale diffusion models for zero-shot text-guidedsign language video anonymization. We incorporate ControlNet, which leverageslow-level image features such as HED (Holistically-Nested Edge Detection)edges, to circumvent the need for pose estimation. Additionally, we develop aspecialized module dedicated to capturing facial expressions, which arecritical for conveying essential linguistic information in signed languages. Wethen combine the above methods to achieve anonymization that better preservesthe essential linguistic content of the original signer. This innovativemethodology makes possible, for the first time, sign language videoanonymization that could be used for real-world applications, which would offersignificant benefits to the Deaf and Hard-of-Hearing communities. Wedemonstrate the effectiveness of our approach with a series of signeranonymization experiments.\rPIPE : Parallelized Inference Through Post-Training Quantization Ensembling of Residual Expansions\nEdouard Yvinec Arnaud Dapogny Kevin Bailly\nabstract\rabstract: Deep neural networks (DNNs) are ubiquitous in computer vision and naturallanguage processing, but suffer from high inference cost. This problem can beaddressed by quantization, which consists in converting floating pointperations into a lower bit-width format. With the growing concerns on privacyrights, we focus our efforts on data-free methods. However, such techniquessuffer from their lack of adaptability to the target devices, as a hardwaretypically only support specific bit widths. Thus, to adapt to a variety ofdevices, a quantization method shall be flexible enough to find good accuracyv.s. speed trade-offs for every bit width and target device. To achieve this,we propose PIPE, a quantization method that leverages residual error expansion,along with group sparsity and an ensemble approximation for betterparallelization. PIPE is backed off by strong theoretical guarantees andachieves superior performance on every benchmarked application (from vision toNLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 toternary quantization).\rTokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform\nYihao Li Yanyi Lai Tianchi Liao Chuan Chen Zibin Zheng\nabstract\rabstract: With the development of practical deep learning models like generative AI,their excellent performance has brought huge economic value. For instance,ChatGPT has attracted more than 100 million users in three months. Since themodel training requires a lot of data and computing power, a well-performingdeep learning model is behind a huge effort and cost. Facing various modelattacks, unauthorized use and abuse from the network that threaten theinterests of model owners, in addition to considering legal and otheradministrative measures, it is equally important to protect the model\u0026rsquo;scopyright from the technical means. By using the model watermarking technology,we point out the possibility of building a unified platform for model ownershipverification. Given the application history of blockchain in copyrightverification and the drawbacks of a centralized third-party, this paperconsiders combining model watermarking technology and blockchain to build aunified model copyright protection platform. By a new solution we calledTokenized Model, it protects the model\u0026rsquo;s copyright by reliable ownership recordand verification mechanism. It also promotes the financial value of model byconstructing the model\u0026rsquo;s transaction process and contribution shares of amodel. In the typical case study, we also study the various performance underusual scenario to verify the effectiveness of this platform.\rDP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\nJunyuan Hong Jiachen T. Wang Chenhui Zhang Zhangheng Li Bo Li Zhangyang Wang\nabstract\rabstract: Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts\u0026rsquo; dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model\u0026rsquo;s provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\r2023-11-26\nAI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction\nJunsol Kim Byungkyu Lee\nabstract\rabstract: Large language models (LLMs) that produce human-like responses have begun torevolutionize research practices in the social sciences. This paper shows howwe can integrate LLMs and social surveys to accurately predict individualresponses to survey questions that were not asked before. We develop a novelmethodological framework to personalize LLMs by considering the meaning ofsurvey questions derived from their text, the latent beliefs of individualsinferred from their response patterns, and the temporal contexts acrossdifferent survey periods through fine-tuning LLMs with survey data. Using theGeneral Social Survey from 1972 to 2021, we show that the fine-tuned modelbased on Alpaca-7b can predict individual responses to survey questions thatare partially missing as well as entirely missing. The remarkable predictioncapabilities allow us to fill in missing trends with high confidence andpinpoint when public attitudes changed, such as the rising support for same-sexmarriage. We discuss practical constraints, socio-demographic representation,and ethical concerns regarding individual autonomy and privacy when using LLMsfor opinion prediction. This study demonstrates that LLMs and surveys canmutually enhance each other\u0026rsquo;s capabilities: LLMs broaden survey potential,while surveys improve the alignment of LLMs.\r2023-11-24\nInput Reconstruction Attack against Vertical Federated Large Language Models\nFei Zheng\nabstract\rabstract: Recently, large language models (LLMs) have drawn extensive attention fromacademia and the public, due to the advent of the ChatGPT. While LLMs showtheir astonishing ability in text generation for various tasks, privacyconcerns limit their usage in real-life businesses. More specifically, eitherthe user\u0026rsquo;s inputs (the user sends the query to the model-hosting server) or themodel (the user downloads the complete model) itself will be revealed duringthe usage. Vertical federated learning (VFL) is a promising solution to thiskind of problem. It protects both the user\u0026rsquo;s input and the knowledge of themodel by splitting the model into a bottom part and a top part, which ismaintained by the user and the model provider, respectively. However, in thispaper, we demonstrate that in LLMs, VFL fails to protect the user input sinceit is simple and cheap to reconstruct the input from the intermediateembeddings. Experiments show that even with a commercial GPU, the inputsentence can be reconstructed in only one second. We also discuss severalpossible solutions to enhance the privacy of vertical federated LLMs.\r2023-11-23\nPrivateLoRA For Efficient Privacy Preserving LLM\nYiming Wang Yu Lin Xiaodong Zeng Guannan Zhang\nabstract\rabstract: End users face a choice between privacy and efficiency in current LargeLanguage Model (LLM) service paradigms. In cloud-based paradigms, users areforced to compromise data locality for generation quality and processing speed.Conversely, edge device paradigms maintain data locality but fail to deliversatisfactory performance. In this work, we propose a novel LLM service paradigmthat distributes privacy-sensitive computation on edge devices and sharedcomputation in the cloud. Only activations are transmitted between the centralcloud and edge devices to ensure data locality. Our core innovation,PrivateLoRA, addresses the challenging communication overhead by exploiting thelow rank of residual activations, achieving over 95% communication reduction.Consequently, PrivateLoRA effectively maintains data locality and is extremelyresource efficient. Under standard 5G networks, PrivateLoRA achieves throughputover 300% of device-only solutions for 7B models and over 80% of an A100 GPUfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRAfor advanced personalization. Our approach democratizes access tostate-of-the-art generative AI for edge devices, paving the way for moretailored LLM experiences for the general public. To our knowledge, our proposedframework is the first efficient and privacy-preserving LLM solution in theliterature.\rMARBLE: Music Audio Representation Benchmark for Universal Evaluation\nRuibin Yuan Yinghao Ma Yizhi Li Ge Zhang Xingran Chen Hanzhi Yin Le Zhuo Yiqi Liu Jiawen Huang Zeyue Tian Binyue Deng Ningzhi Wang Chenghua Lin Emmanouil Benetos Anton Ragni Norbert Gyenge Roger Dannenberg Wenhu Chen Gus Xia Wei Xue Si Liu Shi Wang Ruibo Liu Yike Guo Jie Fu\nabstract\rabstract: In the era of extensive intersection between art and Artificial Intelligence(AI), such as image generation and fiction co-creation, AI for music remainsrelatively nascent, particularly in music understanding. This is evident in thelimited work on deep music representations, the scarcity of large-scaledatasets, and the absence of a universal and community-driven benchmark. Toaddress this issue, we introduce the Music Audio Representation Benchmark foruniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for variousMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomywith four hierarchy levels, including acoustic, performance, score, andhigh-level description. We then establish a unified protocol based on 14 taskson 8 public-available datasets, providing a fair and standard assessment ofrepresentations of all open-sourced pre-trained models developed on musicrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, andreproducible suite for the community, with a clear statement on copyrightissues on datasets. Results suggest recently proposed large-scale pre-trainedmusical language models perform the best in most tasks, with room for furtherimprovement. The leaderboard and toolkit repository are published athttps://marble-bm.shef.ac.uk to promote future music AI research.\rA Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs\nMuhammad Ilyas Azeem Sallam Abualhaija\nabstract\rabstract: Specifying legal requirements for software systems to ensure their compliancewith the applicable regulations is a major concern to requirements engineering(RE). Personal data which is collected by an organization is often shared withother organizations to perform certain processing activities. In such cases,the General Data Protection Regulation (GDPR) requires issuing a dataprocessing agreement (DPA) which regulates the processing and further ensuresthat personal data remains protected. Violating GDPR can lead to huge finesreaching to billions of Euros. Software systems involving personal dataprocessing must adhere to the legal obligations stipulated in GDPR and outlinedin DPAs. Requirements engineers can elicit from DPAs legal requirements forregulating the data processing activities in software systems. Checking thecompleteness of a DPA according to the GDPR provisions is therefore anessential prerequisite to ensure that the elicited requirements are complete.Analyzing DPAs entirely manually is time consuming and requires adequate legalexpertise. In this paper, we propose an automation strategy to address thecompleteness checking of DPAs against GDPR. Specifically, we pursue tenalternative solutions which are enabled by different technologies, namelytraditional machine learning, deep learning, language modeling, and few-shotlearning. The goal of our work is to empirically examine how these differenttechnologies fare in the legal domain. We computed F2 score on a set of 30 realDPAs. Our evaluation shows that best-performing solutions yield F2 score of86.7% and 89.7% are based on pre-trained BERT and RoBERTa language models. Ouranalysis further shows that other alternative solutions based on deep learning(e.g., BiLSTM) and few-shot learning (e.g., SetFit) can achieve comparableaccuracy, yet are more efficient to develop.\rChallenges of Large Language Models for Mental Health Counseling\nNeo Christopher Chung George Dyer Lennart Brocki\nabstract\rabstract: The global mental health crisis is looming with a rapid increase in mentaldisorders, limited resources, and the social stigma of seeking treatment. Asthe field of artificial intelligence (AI) has witnessed significantadvancements in recent years, large language models (LLMs) capable ofunderstanding and generating human-like text may be used in supporting orproviding psychological counseling. However, the application of LLMs in themental health domain raises concerns regarding the accuracy, effectiveness, andreliability of the information provided. This paper investigates the majorchallenges associated with the development of LLMs for psychologicalcounseling, including model hallucination, interpretability, bias, privacy, andclinical effectiveness. We explore potential solutions to these challenges thatare practical and applicable to the current paradigm of AI. From our experiencein developing and deploying LLMs for mental health, AI holds a great promisefor improving mental health care, if we can carefully navigate and overcomepitfalls of LLMs.\r2023-11-22\nSteal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models\nGe Luo Junqiang Huang Manman Zhang Zhenxing Qian Sheng Li Xinpeng Zhang\nabstract\rabstract: The advancement in text-to-image models has led to astonishing artisticperformances. However, several studios and websites illegally fine-tune thesemodels using artists\u0026rsquo; artworks to mimic their styles for profit, which violatesthe copyrights of artists and diminishes their motivation to produce originalworks. Currently, there is a notable lack of research focusing on this issue.In this paper, we propose a novel watermarking framework that detects mimicryin text-to-image models through fine-tuning. This framework embeds subtlewatermarks into digital artworks to protect their copyrights while stillpreserving the artist\u0026rsquo;s visual expression. If someone takes watermarkedartworks as training data to mimic an artist\u0026rsquo;s style, these watermarks canserve as detectable indicators. By analyzing the distribution of thesewatermarks in a series of generated images, acts of fine-tuning mimicry usingstolen victim data will be exposed. In various fine-tune scenarios and againstwatermark attack methods, our research confirms that analyzing the distributionof watermarks in artificially generated images reliably detects unauthorizedmimicry.\r2023-11-21\nDon\u0026rsquo;t forget private retrieval: distributed private similarity search for large language models\nGuy Zyskind Tobin South Alex Pentland\nabstract\rabstract: While the flexible capabilities of large language models (LLMs) allow them toanswer a range of queries based on existing learned knowledge, informationretrieval to augment generation is an important tool to allow LLMs to answerquestions on information not included in pre-training data. Such privateinformation is increasingly being generated in a wide array of distributedcontexts by organizations and individuals. Performing such informationretrieval using neural embeddings of queries and documents always leakedinformation about queries and database content unless both were stored locally.We present Private Retrieval Augmented Generation (PRAG), an approach that usesmulti-party computation (MPC) to securely transmit queries to a distributed setof servers containing a privately constructed database to return top-k andapproximate top-k documents. This is a first-of-its-kind approach to denseinformation retrieval that ensures no server observes a client\u0026rsquo;s query or cansee the database content. The approach introduces a novel MPC friendly protocolfor inverted file approximate search (IVF) that allows for fast document searchover distributed and private data in sublinear communication complexity. Thiswork presents new avenues through which data for use in LLMs can be accessedand used without needing to centralize or forgo privacy.\rKNVQA: A Benchmark for evaluation knowledge-based VQA\nSirui Cheng Siyu Zhang Jiayi Wu Muchen Lan\nabstract\rabstract: Within the multimodal field, large vision-language models (LVLMs) have madesignificant progress due to their strong perception and reasoning capabilitiesin the visual and language systems. However, LVLMs are still plagued by the twocritical issues of object hallucination and factual accuracy, which limit thepracticality of LVLMs in different scenarios. Furthermore, previous evaluationmethods focus more on the comprehension and reasoning of language content butlack a comprehensive evaluation of multimodal interactions, thereby resultingin potential limitations. To this end, we propose a novel KNVQA-Eval, which isdevoted to knowledge-based VQA task evaluation to reflect the factuality ofmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,we develop a new KNVQA dataset by incorporating human judgment and perception,aiming to evaluate the accuracy of standard answers relative to AI-generatedanswers in knowledge-based VQA. This work not only comprehensively evaluatesthe contextual information of LVLMs using reliable human annotations, but alsofurther analyzes the fine-grained capabilities of current methods to revealpotential avenues for subsequent optimization of LVLMs-based estimators. Ourproposed VQA-Eval and corresponding dataset KNVQA will facilitate thedevelopment of automatic evaluation tools with the advantages of low cost,privacy protection, and reproducibility. Our code will be released uponpublication.\rAdapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications\nSamira Ghodratnama Mehrdad Zakershahrak\nabstract\rabstract: The advent of Large Language Models (LLMs) heralds a pivotal shift in onlineuser interactions with information. Traditional Information Retrieval (IR)systems primarily relied on query-document matching, whereas LLMs excel incomprehending and generating human-like text, thereby enriching the IRexperience significantly. While LLMs are often associated with chatbotfunctionalities, this paper extends the discussion to their explicitapplication in information retrieval. We explore methodologies to optimize theretrieval process, select optimal models, and effectively scale and orchestrateLLMs, aiming for cost-efficiency and enhanced result accuracy. A notablechallenge, model hallucination-where the model yields inaccurate ormisinterpreted data-is addressed alongside other model-specific hurdles. Ourdiscourse extends to crucial considerations including user privacy, dataoptimization, and the necessity for system clarity and interpretability.Through a comprehensive examination, we unveil not only innovative strategiesfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,but also the consequential considerations that underline the need for abalanced approach aligned with user-centric principles.\r2023-11-20\nMulti-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks\nRami Haffar David Sánchez Josep Domingo-Ferrer\nabstract\rabstract: Human facial data hold tremendous potential to address a variety ofclassification problems, including face recognition, age estimation, genderidentification, emotion analysis, and race classification. However, recentprivacy regulations, such as the EU General Data Protection Regulation andothers, have restricted the ways in which human images may be collected andused for research. As a result, several previously published data setscontaining human faces have been removed from the internet due to inadequatedata collection methods that failed to meet privacy regulations. Data setsconsisting of synthetic data have been proposed as an alternative, but theyfall short of accurately representing the real data distribution. On the otherhand, most available data sets are labeled for just a single task, which limitstheir applicability. To address these issues, we present the Multi-Task Faces(MTF) image data set, a meticulously curated collection of face images designedfor various classification tasks, including face recognition, as well as race,gender, and age classification. The MTF data set has been ethically gathered byleveraging publicly available images of celebrities and strictly adhering tocopyright regulations. In this paper, we present this data set and providedetailed descriptions of the followed data collection and processingprocedures. Furthermore, we evaluate the performance of five deep learning (DL)models on the MTF data set across the aforementioned classification tasks.Additionally, we compare the performance of DL models over the processed MTFdata and over raw data crawled from the internet. The reported resultsconstitute a baseline for further research employing these data. The MTF dataset can be accessed through the following link (please cite the present paperif you use the data set): https://github.com/RamiHaf/MTF_data_set\r2023-11-18\nExperts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q\u0026amp;A\nZahra Kolagar Anna Katharina Leschanowsky Birgit Popp\nabstract\rabstract: Privacy policies play a vital role in safeguarding user privacy as legaljurisdictions worldwide emphasize the need for transparent data processing.While the suitability of privacy policies to enhance transparency has beencritically discussed, employing conversational AI systems presents uniquechallenges in informing users effectively. In this position paper, we propose adynamic workflow for transforming privacy policies into privacyquestion-and-answer (Q\u0026amp;A) pairs to make privacy policies easily accessiblethrough conversational AI. Thereby, we facilitate interdisciplinarycollaboration among legal experts and conversation designers, while alsoconsidering the utilization of large language models\u0026rsquo; generative capabilitiesand addressing associated challenges. Our proposed workflow underscorescontinuous improvement and monitoring throughout the construction of privacyQ\u0026amp;As, advocating for comprehensive review and refinement through anexperts-in-the-loop approach.\rFully Composable and Adequate Verified Compilation with Direct Refinements between Open Modules (Technical Report)\nLing Zhang Yuting Wang Jinhua Wu Jérémie Koenig Zhong Shao\nabstract\rabstract: Verified compilation of open modules (i.e., modules whose functionalitydepends on other modules) provides a foundation for end-to-end verification ofmodular programs ubiquitous in contemporary software. However, despiteintensive investigation in this topic for decades, the proposed approaches arestill difficult to use in practice as they rely on assumptions about theinternal working of compilers which make it difficult for external users toapply the verification results. We propose an approach to verifiedcompositional compilation without such assumptions in the setting of verifyingcompilation of heterogeneous modules written in first-order languagessupporting global memory and pointers. Our approach is based on the memorymodel of CompCert and a new discovery that a Kripke relation with a notion ofmemory protection can serve as a uniform and composable semantic interface forthe compiler passes. By absorbing the rely-guarantee conditions on memoryevolution for all compiler passes into this Kripke Memory Relation and bypiggybacking requirements on compiler optimizations onto it, we getcompositional correctness theorems for realistic optimizing compilers asrefinements that directly relate native semantics of open modules and that areignorant of intermediate compilation processes. Such direct refinements supportall the compositionality and adequacy properties essential for verifiedcompilation of open modules. We have applied this approach to the fullcompilation chain of CompCert with its Clight source language and demonstratedthat our compiler correctness theorem is open to composition and intuitive touse with reduced verification complexity through end-to-end verification ofnon-trivial heterogeneous modules that may freely invoke each other (e.g.,mutually recursively).\r2023-11-17\nFunctionMarker: Watermarking Language Datasets via Knowledge Injection\nShuai Li Kejiang Chen Kunsheng Tang Wen Huang Jie Zhang Weiming Zhang Nenghai Yu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated superior performance invarious natural language processing tasks. Meanwhile, they require extensivetraining data, raising concerns related to dataset copyright protection.Backdoor-based watermarking is a viable approach to protect the copyright ofclassification datasets. However, these methods may introduce maliciousmisclassification behaviors into watermarked LLMs by attackers and also affectthe semantic information of the watermarked text. To address these issues, wepropose FunctionMarker, a novel copyright protection method for languagedatasets via knowledge injection. FunctionMarker enables LLMs to learn specificknowledge through fine-tuning on watermarked datasets, and we can extract theembedded watermark by obtaining the responses of LLMs to specificknowledge-related queries. Considering watermark capacity and stealthness, weselect customizable functions as specific knowledge for LLMs to learn and embedthe watermark into them. Moreover, FunctionMarker can embed multi-bitwatermarks while preserving the original semantic information, therebyincreasing the difficulty of adaptive attacks. We take mathematical functionsas an instance to evaluate the effectiveness of FunctionMarker, and experimentsshow that only 0.3% of watermarked text achieves a 90% watermark extractionaccuracy in most cases, validating our method\u0026rsquo;s effectiveness.\r2023-11-16\nText Sanitization Beyond Specific Domains: Zero-Shot Redaction \u0026amp; Substitution with Large Language Models\nFederico Albanese Daniel Ciolek Nicolas D\u0026rsquo;Ippolito\nabstract\rabstract: In the context of information systems, text sanitization techniques are usedto identify and remove sensitive data to comply with security and regulatoryrequirements. Even though many methods for privacy preservation have beenproposed, most of them are focused on the detection of entities from specificdomains (e.g., credit card numbers, social security numbers), lackinggenerality and requiring customization for each desirable domain. Moreover,removing words is, in general, a drastic measure, as it can degrade textcoherence and contextual information. Less severe measures include substitutinga word for a safe alternative, yet it can be challenging to automatically findmeaningful substitutions. We present a zero-shot text sanitization techniquethat detects and substitutes potentially sensitive information using LargeLanguage Models. Our evaluation shows that our method excels at protectingprivacy while maintaining text coherence and contextual information, preservingdata utility for downstream tasks.\rTowards More Realistic Membership Inference Attacks on Large Diffusion Models\nJan Dubiński Antoni Kowalczuk Stanisław Pawlak Przemysław Rokita Tomasz Trzciński Paweł Morawiecki\nabstract\rabstract: Generative diffusion models, including Stable Diffusion and Midjourney, cangenerate visually appealing, diverse, and high-resolution images for variousapplications. These models are trained on billions of internet-sourced images,raising significant concerns about the potential unauthorized use ofcopyright-protected images. In this paper, we examine whether it is possible todetermine if a specific image was used in the training set, a problem known inthe cybersecurity community and referred to as a membership inference attack.Our focus is on Stable Diffusion, and we address the challenge of designing afair evaluation framework to answer this membership question. We propose amethodology to establish a fair evaluation setup and apply it to StableDiffusion, enabling potential extensions to other generative models. Utilizingthis evaluation setup, we execute membership attacks (both known and newlyintroduced). Our research reveals that previously proposed evaluation setups donot provide a full understanding of the effectiveness of membership inferenceattacks. We conclude that the membership inference attack remains a significantchallenge for large diffusion models (often deployed as black-box systems),indicating that related privacy and copyright issues will persist in theforeseeable future.\rWhen the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks\nEve Fleisig Rediet Abebe Dan Klein\nabstract\rabstract: Though majority vote among annotators is typically used for ground truthlabels in natural language processing, annotator disagreement in tasks such ashate speech detection may reflect differences in opinion across groups, notnoise. Thus, a crucial problem in hate speech detection is determining whethera statement is offensive to the demographic group that it targets, when thatgroup may constitute a small fraction of the annotator pool. We construct amodel that predicts individual annotator ratings on potentially offensive textand combines this information with the predicted target group of the text tomodel the opinions of target group members. We show gains across a range ofmetrics, including raising performance over the baseline by 22% at predictingindividual annotators\u0026rsquo; ratings and by 33% at predicting variance amongannotators, which provides a metric for model uncertainty downstream. We findthat annotator ratings can be predicted using their demographic information andopinions on online content, without the need to track identifying annotator IDsthat link each annotator to their ratings. We also find that use ofnon-invasive survey questions on annotators\u0026rsquo; online experiences helps tomaximize privacy and minimize unnecessary collection of demographic informationwhen predicting annotators\u0026rsquo; opinions.\rIncorporating Worker Perspectives into MTurk Annotation Practices for NLP\nOlivia Huang Eve Fleisig Dan Klein\nabstract\rabstract: Current practices regarding data collection for natural language processingon Amazon Mechanical Turk (MTurk) often rely on a combination of studies ondata quality and heuristics shared among NLP researchers. However, withoutconsidering the perspectives of MTurk workers, these approaches are susceptibleto issues regarding workers\u0026rsquo; rights and poor response quality. We conducted acritical literature review and a survey of MTurk workers aimed at addressingopen questions regarding best practices for fair payment, worker privacy, dataquality, and considering worker incentives. We found that worker preferencesare often at odds with received wisdom among NLP researchers. Surveyed workerspreferred reliable, reasonable payments over uncertain, very high payments;reported frequently lying on demographic questions; and expressed frustrationat having work rejected with no explanation. We also found that workers viewsome quality control methods, such as requiring minimum response times orMaster\u0026rsquo;s qualifications, as biased and largely ineffective. Based on the surveyresults, we provide recommendations on how future NLP studies may betteraccount for MTurk workers\u0026rsquo; experiences in order to respect workers\u0026rsquo; rights andimprove data quality.\r2023-11-15\nHow Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\nLingbo Mo Boshi Wang Muhao Chen Huan Sun\nabstract\rabstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.\rAn Empathetic User-Centric Chatbot for Emotional Support\nYanting Pan Yixuan Tang Yuchen Niu\nabstract\rabstract: This paper explores the intersection of Otome Culture and artificialintelligence, particularly focusing on how Otome-oriented games fulfill theemotional needs of young women. These games, which are deeply rooted in asubcultural understanding of love, provide players with feelings ofsatisfaction, companionship, and protection through carefully crafted narrativestructures and character development. With the proliferation of Large LanguageModels (LLMs), there is an opportunity to transcend traditional static gamenarratives and create dynamic, emotionally responsive interactions. We presenta case study of Tears of Themis, where we have integrated LLM technology toenhance the interactive experience. Our approach involves augmenting existinggame narratives with a Question and Answer (QA) system, enriched through dataaugmentation and emotional enhancement techniques, resulting in a chatbot thatoffers realistic and supportive companionship.\rValue FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values\nJing Yao Xiaoyuan Yi Xiting Wang Yifan Gong Xing Xie\nabstract\rabstract: The rapid advancement of Large Language Models (LLMs) has attracted muchattention to value alignment for their responsible development. However, how todefine values in this context remains a largely unexplored question. Existingwork mainly follows the Helpful, Honest, Harmless principle and specifiesvalues as risk criteria formulated in the AI community, e.g., fairness andprivacy protection, suffering from poor clarity, adaptability and transparency.Inspired by basic values in humanity and social science across cultures, thiswork proposes a novel basic value alignment paradigm and introduces a valuespace spanned by basic value dimensions. All LLMs\u0026rsquo; behaviors can be mapped intothe space by identifying the underlying values, possessing the potential toaddress the three challenges. To foster future research, we apply therepresentative Schwartz\u0026rsquo;s Theory of Basic Values as an initialized example andconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.Our extensive analysis of FULCRA reveals the underlying relation between basicvalues and LLMs\u0026rsquo; behaviors, demonstrating that our approach not only coversexisting mainstream risks but also anticipates possibly unidentified ones.Additionally, we present an initial implementation of the basic valueevaluation and alignment, paving the way for future research in this line.\r2023-11-14\nSparsity-Preserving Differentially Private Training of Large Embedding Models\nBadih Ghazi Yangsibo Huang Pritish Kamath Ravi Kumar Pasin Manurangsi Amer Sinha Chiyuan Zhang\nabstract\rabstract: As the use of large embedding models in recommendation systems and languageapplications increases, concerns over user data privacy have also risen.DP-SGD, a training algorithm that combines differential privacy with stochasticgradient descent, has been the workhorse in protecting user privacy withoutcompromising model accuracy by much. However, applying DP-SGD naively toembedding models can destroy gradient sparsity, leading to reduced trainingefficiency. To address this issue, we present two new algorithms, DP-FEST andDP-AdaFEST, that preserve gradient sparsity during private training of largeembedding models. Our algorithms achieve substantial reductions ($10^6 \\times$)in gradient size, while maintaining comparable levels of accuracy, on benchmarkreal-world datasets.\rLatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud\nMengke Zhang Tianxing He Tianle Wang Lu Mi Fatemehsadat Mireshghallah Binyi Chen Hao Wang Yulia Tsvetkov\nabstract\rabstract: In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).\r2023-11-13\nFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\nKerem Zaman Leshem Choshen Shashank Srivastava\nabstract\rabstract: Model fusion research aims to aggregate the knowledge of multiple models toenhance performance by combining their weights. In this work, we study theinverse, investigating whether and how can model fusion interfere and reduceunwanted knowledge. We delve into the effects of model fusion on the evolutionof learned shortcuts, social biases, and memorization capabilities infine-tuned language models. Through several experiments covering textclassification and generation tasks, our analysis highlights that sharedknowledge among models is usually enhanced during model fusion, while unsharedknowledge is usually lost or forgotten. Based on this observation, wedemonstrate the potential of model fusion as a debiasing tool and showcase itsefficacy in addressing privacy concerns associated with language models.\r2023-11-12\nFlames: Benchmarking Value Alignment of Chinese Large Language Models\nKexin Huang Xiangyang Liu Qianyu Guo Tianxiang Sun Jiawei Sun Yaru Wang Zeyang Zhou Yixu Wang Yan Teng Xipeng Qiu Yingchun Wang Dahua Lin\nabstract\rabstract: The widespread adoption of large language models (LLMs) across variousregions underscores the urgent need to evaluate their alignment with humanvalues. Current benchmarks, however, fall short of effectively uncoveringsafety vulnerabilities in LLMs. Despite numerous models achieving high scoresand \u0026rsquo;topping the chart\u0026rsquo; in these evaluations, there is still a significant gapin LLMs\u0026rsquo; deeper alignment with human values and achieving genuine harmlessness.To this end, this paper proposes the first highly adversarial benchmark namedFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responseswith fine-grained annotations, and a specified scorer. Our frameworkencompasses both common harmlessness principles, such as fairness, safety,legality, and data protection, and a unique morality dimension that integratesspecific Chinese values such as harmony. Based on the framework, we carefullydesign adversarial prompts that incorporate complex scenarios and jailbreakingmethods, mostly with implicit malice. By prompting mainstream LLMs with suchadversarially constructed prompts, we obtain model responses, which are thenrigorously annotated for evaluation. Our findings indicate that all theevaluated LLMs demonstrate relatively poor performance on Flames, particularlyin the safety and fairness dimensions. Claude emerges as the best-performingmodel overall, but with its harmless rate being only 63.08% while GPT-4 onlyscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,setting a new challenge for contemporary LLMs and highlighting the need forfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,we develop a specified scorer capable of scoring LLMs across multipledimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publiclyavailable on https://github.com/AIFlames/Flames.\rTunable Soft Prompts are Messengers in Federated Learning\nChenhe Dong Yuexiang Xie Bolin Ding Ying Shen Yaliang Li\nabstract\rabstract: Federated learning (FL) enables multiple participants to collaborativelytrain machine learning models using decentralized data sources, alleviatingprivacy concerns that arise from directly sharing local data. However, the lackof model privacy protection in FL becomes an unneglectable challenge,especially when people want to federally finetune models based on a proprietarylarge language model. In this study, we propose a novel FL training approachthat accomplishes information exchange among participants via tunable softprompts. These soft prompts, updated and transmitted between the server andclients, assume the role of the global model parameters and serve as messengersto deliver useful knowledge from the local data and global model. As the globalmodel itself is not required to be shared and the local training is conductedbased on an auxiliary model with fewer parameters than the global model, theproposed approach provides protection for the global model while reducingcommunication and computation costs in FL. Extensive experiments show theeffectiveness of the proposed approach compared to several baselines. We havereleased the source code at\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.\rEvaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling\nYujin Cho Mingeon Kim Seojin Kim Oyun Kwon Ryan Donghan Kwon Yoonha Lee Dohyun Lim\nabstract\rabstract: This study investigates the efficacy of Large Language Models (LLMs) ininteractive language therapy for high-functioning autistic adolescents. Withthe rapid advancement of artificial intelligence, particularly in naturallanguage processing, LLMs present a novel opportunity to augment traditionalpsychological counseling methods. This research primarily focuses on evaluatingthe LLM\u0026rsquo;s ability to engage in empathetic, adaptable, and contextuallyappropriate interactions within a therapeutic setting. A comprehensiveevaluation was conducted by a panel of clinical psychologists and psychiatristsusing a specially developed scorecard. The assessment covered various aspectsof the LLM\u0026rsquo;s performance, including empathy, communication skills,adaptability, engagement, and the ability to establish a therapeutic alliance.The study avoided direct testing with patients, prioritizing privacy andethical considerations, and instead relied on simulated scenarios to gauge theLLM\u0026rsquo;s effectiveness. The results indicate that LLMs hold significant promise assupportive tools in therapy, demonstrating strengths in empathetic engagementand adaptability in conversation. However, challenges in achieving the depth ofpersonalization and emotional understanding characteristic of human therapistswere noted. The study also highlights the importance of ethical considerationsin the application of AI in therapeutic contexts. This research providesvaluable insights into the potential and limitations of using LLMs inpsychological counseling for autistic adolescents. It lays the groundwork forfuture explorations into AI\u0026rsquo;s role in mental health care, emphasizing the needfor ongoing development to enhance the capabilities of these models intherapeutic settings.\r2023-11-11\nAn In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing\nLe Peng Gaoxiang Luo sicheng zhou jiandong chen Rui Zhang Ziyue Xu Ju Sun\nabstract\rabstract: Language models (LMs) such as BERT and GPT have revolutionized naturallanguage processing (NLP). However, the medical field faces challenges intraining LMs due to limited data access and privacy constraints imposed byregulations like the Health Insurance Portability and Accountability Act(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning(FL) offers a decentralized solution that enables collaborative learning whileensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasksencompassing 8 corpora using 6 LMs. Our results show that: 1) FL modelsconsistently outperformed models trained on individual clients\u0026rsquo; data andsometimes performed comparably with models trained with polled data; 2) withthe fixed number of total data, FL models training with more clients producedinferior performance but pre-trained transformer-based models exhibited greatresilience. 3) FL models significantly outperformed large language models usingzero-/one-shot learning and offered lightning inference speed.\rDo Not Harm Protected Groups in Debiasing Language Representation Models\nChloe Qinyu Zhu Rickard Stureborg Brandon Fain\nabstract\rabstract: Language Representation Models (LRMs) trained with real-world data maycapture and exacerbate undesired bias and cause unfair treatment of people invarious demographic groups. Several techniques have been investigated forapplying interventions to LRMs to remove bias in benchmark evaluations on, forexample, word embeddings. However, the negative side effects of debiasinginterventions are usually not revealed in the downstream tasks. We proposexGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. Inthis work, We examine four debiasing techniques on a real-world textclassification task and show that reducing biasing is at the cost of degradingperformance for all demographic groups, including those the debiasingtechniques aim to protect. We advocate that a debiasing technique should havegood downstream performance with the constraint of ensuring no harm to theprotected group.\rGenerative AI for Space-Air-Ground Integrated Networks (SAGIN)\nRuichen Zhang Hongyang Du Dusit Niyato Jiawen Kang Zehui Xiong Abbas Jamalipour Ping Zhang Dong In Kim\nabstract\rabstract: Recently, generative AI technologies have emerged as a significantadvancement in artificial intelligence field, renowned for their language andimage generation capabilities. Meantime, space-air-ground integrated network(SAGIN) is an integral part of future B5G/6G for achieving ubiquitousconnectivity. Inspired by this, this article explores an integration ofgenerative AI in SAGIN, focusing on potential applications and case study. Wefirst provide a comprehensive review of SAGIN and generative AI models,highlighting their capabilities and opportunities of their integration.Benefiting from generative AI\u0026rsquo;s ability to generate useful data and facilitateadvanced decision-making processes, it can be applied to various scenarios ofSAGIN. Accordingly, we present a concise survey on their integration, includingchannel modeling and channel state information (CSI) estimation, jointair-space-ground resource allocation, intelligent network deployment, semanticcommunications, image extraction and processing, security and privacyenhancement. Next, we propose a framework that utilizes a Generative DiffusionModel (GDM) to construct channel information map to enhance quality of servicefor SAGIN. Simulation results demonstrate the effectiveness of the proposedframework. Finally, we discuss potential research directions for generativeAI-enabled SAGIN.\r2023-11-10\nRemoving RLHF Protections in GPT-4 via Fine-Tuning\nQiusi Zhan Richard Fang Rohan Bindu Akul Gupta Tatsunori Hashimoto Daniel Kang\nabstract\rabstract: As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendorsof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to removeRLHF protections with as few as 340 examples and a 95% success rate. Thesetraining examples can be automatically generated with weaker models. We furthershow that removing RLHF protections does not decrease usefulness onnon-censored outputs, providing evidence that our fine-tuning strategy does notdecrease usefulness despite using weaker models to generate training data. Ourresults show the need for further research on protections on LLMs.\rWatermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service\nYuanmin Tang Jing Yu Keke Gai Xiangyan Qu Yue Hu Gang Xiong Qi Wu\nabstract\rabstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at https://github.com/Pter61/vlpmarker.\r2023-11-09\nChatbots Are Not Reliable Text Annotators\nRoss Deans Kristensen-McLachlan Miceal Canavan Márton Kardos Mia Jacobsen Lene Aarøe\nabstract\rabstract: Recent research highlights the significant potential of ChatGPT for textannotation in social science research. However, ChatGPT is a closed-sourceproduct which has major drawbacks with regards to transparency,reproducibility, cost, and data protection. Recent advances in open-source (OS)large language models (LLMs) offer alternatives which remedy these challenges.This means that it is important to evaluate the performance of OS LLMs relativeto ChatGPT and standard approaches to supervised machine learningclassification. We conduct a systematic comparative evaluation of theperformance of a range of OS LLM models alongside ChatGPT, using both zero- andfew-shot learning as well as generic and custom prompts, with results comparedto more traditional supervised classification models. Using a new dataset ofTweets from US news media, and focusing on simple binary text annotation tasksfor standard social science concepts, we find significant variation in theperformance of ChatGPT and OS models across the tasks, and that supervisedclassifiers consistently outperform both. Given the unreliable performance ofChatGPT and the significant challenges it poses to Open Science we adviseagainst using ChatGPT for substantive text annotation tasks in social scienceresearch.\rSynthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model\nBrandon Theodorou Cao Xiao Jimeng Sun\nabstract\rabstract: Synthetic electronic health records (EHRs) that are both realistic andpreserve privacy can serve as an alternative to real EHRs for machine learning(ML) modeling and statistical analysis. However, generating high-fidelity andgranular electronic health record (EHR) data in its original,highly-dimensional form poses challenges for existing methods due to thecomplexities inherent in high-dimensional data. In this paper, we proposeHierarchical Autoregressive Language mOdel (HALO) for generating longitudinalhigh-dimensional EHR, which preserve the statistical properties of real EHR andcan be used to train accurate ML models without privacy concerns. Our HALOmethod, designed as a hierarchical autoregressive model, generates aprobability density function of medical codes, clinical visits, and patientrecords, allowing for the generation of realistic EHR data in its original,unaggregated form without the need for variable selection or aggregation.Additionally, our model also produces high-quality continuous variables in alongitudinal and probabilistic manner. We conducted extensive experiments anddemonstrate that HALO can generate high-fidelity EHR data with high-dimensionaldisease code probabilities (d \u0026gt; 10,000), disease co-occurrence probabilitieswithin visits (d \u0026gt; 1,000,000), and conditional probabilities across consecutivevisits (d \u0026gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison toreal EHR data. This performance then enables downstream ML models trained onits synthetic data to achieve comparable accuracy to models trained on realdata (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using acombination of real and synthetic data enhances the accuracy of ML modelsbeyond that achieved by using only real EHR data.\rEnhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT\nJingye Yang Cong Liu Wendy Deng Da Wu Chunhua Weng Yunyun Zhou Kai Wang\nabstract\rabstract: We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.\rPRODIGy: a PROfile-based DIalogue Generation dataset\nDaniela Occhipinti Serra Sinem Tekiroglu Marco Guerini\nabstract\rabstract: Providing dialogue agents with a profile representation can improve theirconsistency and coherence, leading to better conversations. However, currentprofile-based dialogue datasets for training such agents contain eitherexplicit profile representations that are simple and dialogue-specific, orimplicit representations that are difficult to collect. In this work, wepropose a unified framework in which we bring together both standard and moresophisticated profile representations by creating a new resource where eachdialogue is aligned with all possible speaker representations such ascommunication style, biographies, and personality. This framework allows totest several baselines built using generative language models with severalprofile configurations. The automatic evaluation shows that profile-basedmodels have better generalisation capabilities than models trained on dialoguesonly, both in-domain and cross-domain settings. These results are consistentfor fine-tuned models and instruction-based LLMs. Additionally, humanevaluation demonstrates a clear preference for generations consistent with bothprofile and context. Finally, to account for possible privacy concerns, allexperiments are done under two configurations: inter-character andintra-character. In the former, the LM stores the information about thecharacter in its internal representation, while in the latter, the LM does notretain any personal information but uses it only at inference time.\r2023-11-08\nBuilding Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective\nMd Tahmid Rahman Laskar Xue-Yong Fu Cheng Chen Shashi Bhushan TN\nabstract\rabstract: This paper studies how to effectively build meeting summarization systems forreal-world usage using large language models (LLMs). For this purpose, weconduct an extensive evaluation and comparison of various closed-source andopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findingsreveal that most closed-source LLMs are generally better in terms ofperformance. However, much smaller open-source models like LLaMA- 2 (7B and13B) could still achieve performance comparable to the large closed-sourcemodels even in zero-shot scenarios. Considering the privacy concerns ofclosed-source models for only being accessible via API, alongside the high costassociated with using fine-tuned versions of the closed-source models, theopensource models that can achieve competitive performance are moreadvantageous for industrial use. Balancing performance with associated costsand privacy concerns, the LLaMA-2-7B model looks more promising for industrialusage. In sum, this paper offers practical insights on using LLMs forreal-world business meeting summarization, shedding light on the trade-offsbetween performance and cost.\rFederated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models\nSixing Yu J. Pablo Muñoz Ali Jannesari\nabstract\rabstract: Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, havedemonstrated remarkable success in a wide range of applications, driven bytheir ability to leverage vast amounts of data for pre-training. However,optimizing FMs often requires access to sensitive data, raising privacyconcerns and limiting their applicability in many domains. In this paper, wepropose the Federated Foundation Models (FFMs) paradigm, which combines thebenefits of FMs and Federated Learning (FL) to enable privacy-preserving andcollaborative learning across multiple end-users. We discuss the potentialbenefits and challenges of integrating FL into the lifespan of FMs, coveringpre-training, fine-tuning, and application. We further outline potential futureresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, andfederated prompt tuning, which allow the development of more personalized andcontext-aware models while ensuring data privacy. Moreover, we explore thepossibility of continual/lifelong learning in FFMs, as increased computationalpower at the edge may unlock the potential for optimizing FMs using newlygenerated private data close to the data source. The proposed FFM conceptsoffer a flexible and scalable framework for training large language models in aprivacy-preserving manner, setting the stage for subsequent advancements inboth FM training and federated learning.\rStepping out of Flatland: Discovering Behavior Patterns as Topological Structures in Cyber Hypergraphs\nHelen Jenne Sinan G. Aksoy Daniel Best Alyson Bittner Gregory Henselman-Petrusek Cliff Joslyn Bill Kay Audun Myers Garret Seppala Jackson Warley Stephen J. Young Emilie Purvine\nabstract\rabstract: Data breaches and ransomware attacks occur so often that they have becomepart of our daily news cycle. This is due to a myriad of factors, including theincreasing number of internet-of-things devices, shift to remote work duringthe pandemic, and advancement in adversarial techniques, which all contributeto the increase in both the complexity of data captured and the challenge ofprotecting our networks. At the same time, cyber research has made strides,leveraging advances in machine learning and natural language processing tofocus on identifying sophisticated attacks that are known to evade conventionalmeasures. While successful, the shortcomings of these methods, particularly thelack of interpretability, are inherent and difficult to overcome. Consequently,there is an ever-increasing need to develop new tools for analyzing cyber datato enable more effective attack detection. In this paper, we present a novelframework based in the theory of hypergraphs and topology to understand datafrom cyber networks through topological signatures, which are both flexible andcan be traced back to the log data. While our approach\u0026rsquo;s mathematical groundingrequires some technical development, this pays off in interpretability, whichwe will demonstrate with concrete examples in a large-scale cyber networkdataset. These examples are an introduction to the broader possibilities thatlie ahead; our goal is to demonstrate the value of applying methods from theburgeoning fields of hypernetwork science and applied topology to understandrelationships among behaviors in cyber data.\r2023-11-07\nP-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models\nHaoran Li Dadi Guo Donghao Li Wei Fan Qi Hu Xin Liu Chunkit Chan Duanyi Yao Yangqiu Song\nabstract\rabstract: The rapid development of language models (LMs) brings unprecedentedaccessibility and usage for both models and users. On the one hand, powerfulLMs, trained with massive textual data, achieve state-of-the-art performanceover numerous downstream NLP tasks. On the other hand, more and more attentionis paid to unrestricted model accesses that may bring malicious privacy risksof data leakage. To address these issues, many recent works proposeprivacy-preserving language models (PPLMs) with differential privacy (DP).Unfortunately, different DP implementations make it challenging for a faircomparison among existing PPLMs. In this paper, we present P-Bench, amulti-perspective privacy evaluation benchmark to empirically and intuitivelyquantify the privacy leakage of LMs. Instead of only protecting and measuringthe privacy of protected data with DP parameters, P-Bench sheds light on theneglected inference data privacy during actual usage. P-Bench first clearlydefines multi-faceted privacy objectives during private fine-tuning. Then,P-Bench constructs a unified pipeline to perform private fine-tuning. Lastly,P-Bench performs existing privacy attacks on LMs with pre-defined privacyobjectives as the empirical evaluation results. The empirical attack resultsare used to fairly and intuitively evaluate the privacy leakage of variousPPLMs. We conduct extensive experiments on three datasets of GLUE formainstream LMs.\rLoss Balancing for Fair Supervised Learning\nMohammad Mahdi Khalili Xueru Zhang Mahed Abroshan\nabstract\rabstract: Supervised learning models have been used in various domains such as lending,college admission, face recognition, natural language processing, etc. However,they may inherit pre-existing biases from training data and exhibitdiscrimination against protected social groups. Various fairness notions havebeen proposed to address unfairness issues. In this work, we focus on EqualizedLoss (EL), a fairness notion that requires the expected loss to be(approximately) equalized across different groups. Imposing EL on the learningprocess leads to a non-convex optimization problem even if the loss function isconvex, and the existing fair learning algorithms cannot properly be adopted tofind the fair predictor under the EL constraint. This paper introduces analgorithm that can leverage off-the-shelf convex programming tools (e.g.,CVXPY) to efficiently find the global optimum of this non-convex optimization.In particular, we propose the ELminimizer algorithm, which finds the optimalfair predictor under EL by reducing the non-convex optimization to a sequenceof convex optimization problems. We theoretically prove that our algorithmfinds the global optimal solution under certain conditions. Then, we supportour theoretical results through several empirical studies.\r2023-11-06\nFindings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs\nLongyue Wang Zhaopeng Tu Yan Gu Siyou Liu Dian Yu Qingsong Ma Chenyang Lyu Liting Zhou Chao-Hong Liu Yufeng Ma Weiyu Chen Yvette Graham Bonnie Webber Philipp Koehn Andy Way Yulin Yuan Shuming Shi\nabstract\rabstract: Translating literary works has perennially stood as an elusive dream inmachine translation (MT), a journey steeped in intricate challenges. To fosterprogress in this domain, we hold a new shared task at WMT 2023, the firstedition of the Discourse-Level Literary Translation. First, we (Tencent AI Laband China Literature Ltd.) release a copyrighted and document-levelChinese-English web novel corpus. Furthermore, we put forth anindustry-endorsed criteria to guide human evaluation process. This year, wetotally received 14 submissions from 7 academia and industry teams. We employboth automatic and human evaluations to measure the performance of thesubmitted systems. The official ranking of the systems is based on the overallhuman judgments. In addition, our extensive analysis reveals a series ofinteresting findings on literary and discourse-aware MT. We release data,system outputs, and leaderboard athttp://www2.statmt.org/wmt23/literary-translation-task.html.\r2023-11-05\nPyclipse, a library for deidentification of free-text clinical notes\nCallandra Moore Jonathan Ranisau Walter Nelson Jeremy Petch Alistair Johnson\nabstract\rabstract: Automated deidentification of clinical text data is crucial due to the highcost of manual deidentification, which has been a barrier to sharing clinicaltext and the advancement of clinical natural language processing. However,creating effective automated deidentification tools faces several challenges,including issues in reproducibility due to differences in text processing,evaluation methods, and a lack of consistency across clinical domains andinstitutions. To address these challenges, we propose the pyclipse framework, aunified and configurable evaluation procedure to streamline the comparison ofdeidentification algorithms. Pyclipse serves as a single interface for runningopen-source deidentification algorithms on local clinical data, allowing forcontext-specific evaluation. To demonstrate the utility of pyclipse, we comparesix deidentification algorithms across four public and two private clinicaltext datasets. We find that algorithm performance consistently falls short ofthe results reported in the original papers, even when evaluated on the samebenchmark dataset. These discrepancies highlight the complexity of accuratelyassessing and comparing deidentification algorithms, emphasizing the need for areproducible, adjustable, and extensible framework like pyclipse. Our frameworklays the foundation for a unified approach to evaluate and improvedeidentification tools, ultimately enhancing patient protection in clinicalnatural language processing.\rQuantifying and Analyzing Entity-level Memorization in Large Language Models\nZhenhong Zhou Jiuyang Xiang Chaomeng Chen Sen Su\nabstract\rabstract: Large language models (LLMs) have been proven capable of memorizing theirtraining data, which can be extracted through specifically designed prompts. Asthe scale of datasets continues to grow, privacy risks arising frommemorization have attracted increasing attention. Quantifying language modelmemorization helps evaluate potential privacy risks. However, prior works onquantifying memorization require access to the precise original data or incursubstantial computational overhead, making it difficult for applications inreal-world language models. To this end, we propose a fine-grained,entity-level definition to quantify memorization with conditions and metricscloser to real-world scenarios. In addition, we also present an approach forefficiently extracting sensitive entities from autoregressive language models.We conduct extensive experiments based on the proposed, probing languagemodels\u0026rsquo; ability to reconstruct sensitive entities under different settings. Wefind that language models have strong memorization at the entity level and areable to reproduce the training data even with partial leakages. The resultsdemonstrate that LLMs not only memorize their training data but also understandassociations between entities. These findings necessitate that trainers of LLMsexercise greater prudence regarding model memorization, adopting memorizationmitigation techniques to preclude privacy violations.\rTag Your Fish in the Broken Net: A Responsible Web Framework for Protecting Online Privacy and Copyright\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The World Wide Web, a ubiquitous source of information, serves as a primaryresource for countless individuals, amassing a vast amount of data from globalinternet users. However, this online data, when scraped, indexed, and utilizedfor activities like web crawling, search engine indexing, and, notably, AImodel training, often diverges from the original intent of its contributors.The ascent of Generative AI has accentuated concerns surrounding data privacyand copyright infringement. Regrettably, the web\u0026rsquo;s current framework fallsshort in facilitating pivotal actions like consent withdrawal or data copyrightclaims. While some companies offer voluntary measures, such as crawler accessrestrictions, these often remain inaccessible to individual users. To empoweronline users to exercise their rights and enable companies to adhere toregulations, this paper introduces a user-controlled consent tagging frameworkfor online data. It leverages the extensibility of HTTP and HTML in conjunctionwith the decentralized nature of distributed ledger technology. With thisframework, users have the ability to tag their online data at the time oftransmission, and subsequently, they can track and request the withdrawal ofconsent for their data from the data holders. A proof-of-concept system isimplemented, demonstrating the feasibility of the framework. This work holdssignificant potential for contributing to the reinforcement of user consent,privacy, and copyright on the modern internet and lays the groundwork forfuture insights into creating a more responsible and user-centric webecosystem.\rDomain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand\nJunfeng Guo Yiming Li Lixu Wang Shu-Tao Xia Heng Huang Cong Liu Bo Li\nabstract\rabstract: The prosperity of deep neural networks (DNNs) is largely benefited fromopen-source datasets, based on which users can evaluate and improve theirmethods. In this paper, we revisit backdoor-based dataset ownershipverification (DOV), which is currently the only feasible approach to protectthe copyright of open-source datasets. We reveal that these methods arefundamentally harmful given that they could introduce maliciousmisclassification behaviors to watermarked DNNs by the adversaries. In thispaper, we design DOV from another perspective by making watermarked models(trained on the protected dataset) correctly classify some `hard\u0026rsquo; samples thatwill be misclassified by the benign model. Our method is inspired by thegeneralization property of DNNs, where we find a \\emph{hardly-generalizeddomain} for the original dataset (as its \\emph{domain watermark}). It can beeasily learned with the protected dataset containing modified samples.Specifically, we formulate the domain generation as a bi-level optimization andpropose to optimize a set of visually-indistinguishable clean-label modifieddata with similar effects to domain-watermarked samples from thehardly-generalized domain to ensure watermark stealthiness. We also design ahypothesis-test-guided ownership verification via our domain watermark andprovide the theoretical analyses of our method. Extensive experiments on threebenchmark datasets are conducted, which verify the effectiveness of our methodand its resistance to potential adaptive methods. The code for reproducing mainexperiments is available at\\url{https://github.com/JunfengGo/Domain-Watermark}.\r2023-11-03\nAutomating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models\nJake Chanenson Madison Pickering Noah Apthorpe\nabstract\rabstract: Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research.\rDetecting Pretraining Data from Large Language Models\nWeijia Shi Anirudh Ajith Mengzhou Xia Yangsibo Huang Daogao Liu Terra Blevins Danqi Chen Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\rAntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors\nYou-Ming Chang Chen Yeh Wei-Chen Chiu Ning Yu\nabstract\rabstract: Deep generative models can create remarkably photorealistic fake images whileraising concerns about misinformation and copyright infringement, known asdeepfake threats. Deepfake detection technique is developed to distinguishbetween real and fake images, where the existing methods typically trainclassifiers in the image domain or various feature domains. However, thegeneralizability of deepfake detection against emerging and more advancedgenerative models remains challenging. In this paper, inspired by the zero-shotadvantages of Vision-Language Models (VLMs), we propose a novel approach usingVLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfakedetection accuracy over unseen data. We formulate deepfake detection as avisual question answering problem, and tune soft prompts for InstructBLIP todistinguish a query image is real or fake. We conduct full-spectrum experimentson datasets from 3 held-in and 13 held-out generative models, covering moderntext-to-image generation, image editing and image attacks. Results demonstratethat (1) the deepfake detection accuracy can be significantly and consistentlyimproved (from 54.6% to 91.31%, in average accuracy over unseen data) usingpretrained vision-language models with prompt tuning; (2) our superiorperformance is at less cost of trainable parameters, resulting in an effectiveand efficient solution for deepfake detection. Code and models can be found athttps://github.com/nctu-eva-lab/AntifakePrompt.\rMARRS: Multimodal Reference Resolution System\nHalim Cagri Ates Shruti Bhargava Site Li Jiarui Lu Siddhardha Maddula Joel Ruben Antony Moniz Anil Kumar Nalamalapu Roman Hoang Nguyen Melis Ozyildirim Alkesh Patel Dhivya Piraviperumal Vincent Renkens Ankit Samal Thy Tran Bo-Hsiang Tseng Hong Yu Yuan Zhang Rong Zou\nabstract\rabstract: Successfully handling context is essential for any dialog understanding task.This context maybe be conversational (relying on previous user queries orsystem responses), visual (relying on what the user sees, for example, on theirscreen), or background (based on signals such as a ringing alarm or playingmusic). In this work, we present an overview of MARRS, or Multimodal ReferenceResolution System, an on-device framework within a Natural LanguageUnderstanding system, responsible for handling conversational, visual andbackground context. In particular, we present different machine learning modelsto enable handing contextual queries; specifically, one to enable referenceresolution, and one to handle context via query rewriting. We also describe howthese models complement each other to form a unified, coherent, lightweightsystem that can understand context while preserving user privacy.\r2023-11-02\nImproving Fairness using Vision-Language Driven Image Augmentation\nMoreno D\u0026rsquo;Incà Christos Tzelepis Ioannis Patras Nicu Sebe\nabstract\rabstract: Fairness is crucial when training a deep-learning discriminative model,especially in the facial domain. Models tend to correlate specificcharacteristics (such as age and skin color) with unrelated attributes(downstream tasks), resulting in biases which do not correspond to reality. Itis common knowledge that these correlations are present in the data and arethen transferred to the models during training. This paper proposes a method tomitigate these correlations to improve fairness. To do so, we learninterpretable and meaningful paths lying in the semantic space of a pre-traineddiffusion model (DiffAE) \u0026ndash; such paths being supervised by contrastive textdipoles. That is, we learn to edit protected characteristics (age and skincolor). These paths are then applied to augment images to improve the fairnessof a given dataset. We test the proposed method on CelebA-HQ and UTKFace onseveral downstream tasks with age and skin color as protected characteristics.As a proxy for fairness, we compute the difference in accuracy with respect tothe protected characteristics. Quantitative results show how the augmentedimages help the model improve the overall accuracy, the aforementioned metric,and the disparity of equal opportunity. Code is available at:https://github.com/Moreno98/Vision-Language-Bias-Control.\rFVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation\nYan Wang Jian Cheng Yixin Chen Shuai Shao Lanyun Zhu Zhenzhou Wu Tao Liu Haogang Zhu\nabstract\rabstract: Medical image segmentation methods normally perform poorly when there is adomain shift between training and testing data. Unsupervised Domain Adaptation(UDA) addresses the domain shift problem by training the model using bothlabeled data from the source domain and unlabeled data from the target domain.Source-Free UDA (SFUDA) was recently proposed for UDA without requiring thesource data during the adaptation, due to data privacy or data transmissionissues, which normally adapts the pre-trained deep model in the testing stage.However, in real clinical scenarios of medical image segmentation, the trainedmodel is normally frozen in the testing stage. In this paper, we proposeFourier Visual Prompting (FVP) for SFUDA of medical image segmentation.Inspired by prompting learning in natural language processing, FVP steers thefrozen pre-trained model to perform well in the target domain by adding avisual prompt to the input target data. In FVP, the visual prompt isparameterized using only a small amount of low-frequency learnable parametersin the input frequency space, and is learned by minimizing the segmentationloss between the predicted segmentation of the prompted target image andreliable pseudo segmentation label of the target image under the frozen model.To our knowledge, FVP is the first work to apply visual prompts to SFUDA formedical image segmentation. The proposed FVP is validated using three publicdatasets, and experiments demonstrate that FVP yields better segmentationresults, compared with various existing methods.\rInclusiveness Matters: A Large-Scale Analysis of User Feedback\nNowshin Nawar Arony Ze Shi Li Bowen Xu Daniela Damian\nabstract\rabstract: In an era of rapidly expanding software usage, catering to the diverse needsof users from various backgrounds has become a critical challenge.Inclusiveness, representing a core human value, is frequently overlooked duringsoftware development, leading to user dissatisfaction. Users often engage indiscourse on online platforms where they indicate their concerns. In thisstudy, we leverage user feedback from three popular online sources, Reddit,Google Play Store, and Twitter, for 50 of the most popular apps in the world toreveal the inclusiveness-related concerns from end users. Using aSocio-Technical Grounded Theory approach, we analyzed 23,107 posts across thethree sources and identified 1,211 inclusiveness related posts. We organize ourempirical results in a taxonomy for inclusiveness comprising 6 majorcategories: Fairness, Technology, Privacy, Demography, Usability, and OtherHuman Values. To explore automated support to identifying inclusiveness-relatedposts, we experimented with five state-of-the-art pre-trained large languagemodels (LLMs) and found that these models\u0026rsquo; effectiveness is high and yet varieddepending on the data source. GPT-2 performed best on Reddit, BERT on theGoogle Play Store, and BART on Twitter. Our study provides an in-depth view ofinclusiveness-related user feedback from most popular apps and online sources.We provide implications and recommendations that can be used to bridge the gapbetween user expectations and software so that software developers can resonatewith the varied and evolving needs of the wide spectrum of users.\r2023-11-01\nTowards Legally Enforceable Hate Speech Detection for Public Forums\nChu Fei Luo Rohan Bhambhoria Xiaodan Zhu Samuel Dahan\nabstract\rabstract: Hate speech causes widespread and deep-seated societal issues. Properenforcement of hate speech laws is key for protecting groups of people againstharmful and discriminatory language. However, determining what constitutes hatespeech is a complex task that is highly open to subjective interpretations.Existing works do not align their systems with enforceable definitions of hatespeech, which can make their outputs inconsistent with the goals of regulators.This research introduces a new perspective and task for enforceable hate speechdetection centred around legal definitions, and a dataset annotated onviolations of eleven possible definitions by legal experts. Given the challengeof identifying clear, legally enforceable instances of hate speech, we augmentthe dataset with expert-generated samples and an automatically mined challengeset. We experiment with grounding the model decision in these definitions usingzero-shot and few-shot prompting. We then report results on several largelanguage models (LLMs). With this task definition, automatic hate speechdetection can be more closely aligned to enforceable laws, and hence assist inmore rigorous enforcement of legal protections against harmful speech in publicforums.\rMulti-step Jailbreaking Privacy Attacks on ChatGPT\nHaoran Li Dadi Guo Wei Fan Mingshi Xu Jie Huang Fanpu Meng Yangqiu Song\nabstract\rabstract: With the rapid progress of large language models (LLMs), many downstream NLPtasks can be well solved given appropriate prompts. Though model developers andresearchers work hard on dialog safety to avoid generating harmful content fromLLMs, it is still challenging to steer AI-generated content (AIGC) for thehuman good. As powerful LLMs are devouring existing text data from variousdomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whetherthe private information is included in the training data and what privacythreats can these LLMs and their downstream applications bring. In this paper,we study the privacy threats from OpenAI\u0026rsquo;s ChatGPT and the New Bing enhanced byChatGPT and show that application-integrated LLMs may cause new privacythreats. To this end, we conduct extensive experiments to support our claimsand discuss LLMs\u0026rsquo; privacy implications.\rKnowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\nRan Xu Hejie Cui Yue Yu Xuan Kan Wenqi Shi Yuchen Zhuang Wei Jin Joyce Ho Carl Yang\nabstract\rabstract: Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.\r2023-10-31\nInvisible Watermarking for Audio Generation Diffusion Models\nXirong Cao Xiang Li Divyesh Jadav Yanzhao Wu Zhehui Chen Chen Zeng Wenqi Wei\nabstract\rabstract: Diffusion models have gained prominence in the image domain for theircapabilities in data generation and transformation, achieving state-of-the-artperformance in various tasks in both image and audio domains. In the rapidlyevolving field of audio-based machine learning, safeguarding model integrityand establishing data copyright are of paramount importance. This paperpresents the first watermarking technique applied to audio diffusion modelstrained on mel-spectrograms. This offers a novel approach to the aforementionedchallenges. Our model excels not only in benign audio generation, but alsoincorporates an invisible watermarking trigger mechanism for modelverification. This watermark trigger serves as a protective layer, enabling theidentification of model ownership and ensuring its integrity. Through extensiveexperiments, we demonstrate that invisible watermark triggers can effectivelyprotect against unauthorized modifications while maintaining high utility inbenign audio generation tasks.\rCocoon: Static Information Flow Control in Rust\nAda Barach Maxwell Taylor Vincent Beardsley Jacob Bambeck Michael D. Bond Zhiqiang Lin\nabstract\rabstract: Information flow control (IFC) ensures confidentiality by preventing secretvalues from affecting non-secret values. Existing language-level IFC approachesmodify the language and use non-standard compilation tools, impose run-timeoverhead, or report false leaks, all of which hinder adoption. This paperpresents Cocoon, a Rust library for static type-based IFC that uses theunmodified Rust language and compiler. The key insight of Cocoon lies inleveraging Rust\u0026rsquo;s type system and procedural macros to establish an effectsystem that allows applications to safely compute arbitrary functions on secretdata. We integrated Cocoon into two popular Rust programs, the Spotify TUIclient and Mozilla\u0026rsquo;s Servo browser engine, to protect a secret value in eachprogram. The results show that applications can be retrofitted to use Cocoonwith limited modifications, at least to protect a single value, with negligibleor nonexistent impacts on run-time and compile-time performance.\rUnlearn What You Want to Forget: Efficient Unlearning for LLMs\nJiaao Chen Diyi Yang\nabstract\rabstract: Large language models (LLMs) have achieved significant progress frompre-training on and memorizing a wide range of textual data, however, thisprocess might suffer from privacy issues and violations of data protectionregulations. As a result, the ability to easily remove data related toindividual users from such models while not deteriorating their predictivequality after the removal becomes increasingly important. To address theseissues, in this work, we propose an efficient unlearning framework that couldefficiently update LLMs without having to retrain the whole model after dataremovals, by introducing lightweight unlearning layers learned with a selectiveteacher-student objective into the transformers. In addition, we introduce afusion mechanism to effectively combine different unlearning layers that learnsto forget different sets of data to handle a sequence of forgetting operations.Experiments on classification and generation tasks demonstrate theeffectiveness of our proposed methods compared to the state-of-the-artbaselines.\rMaking Large Language Models Better Data Creators\nDong-Ho Lee Jay Pujara Mohit Sewak Ryen W. White Sujay Kumar Jauhar\nabstract\rabstract: Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world.\r2023-10-30\nLearnware: Small Models Do Big\nZhi-Hua Zhou Zhi-Hao Tan\nabstract\rabstract: There are complaints about current machine learning techniques such as therequirement of a huge amount of training data and proficient training skills,the difficulty of continual learning, the risk of catastrophic forgetting, theleaking of data privacy/proprietary, etc. Most research efforts have beenfocusing on one of those concerned issues separately, paying less attention tothe fact that most issues are entangled in practice. The prevailing big modelparadigm, which has achieved impressive results in natural language processingand computer vision applications, has not yet addressed those issues, whereasbecoming a serious source of carbon emissions. This article offers an overviewof the learnware paradigm, which attempts to enable users not need to buildmachine learning models from scratch, with the hope of reusing small models todo things even beyond their original purposes, where the key ingredient is thespecification which enables a trained model to be adequately identified toreuse according to the requirement of future users who know nothing about themodel in advance.\rKnowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks\nMinki Kang Seanie Lee Jinheon Baek Kenji Kawaguchi Sung Ju Hwang\nabstract\rabstract: Large Language Models (LLMs) have shown promising performance inknowledge-intensive reasoning tasks that require a compound understanding ofknowledge. However, deployment of the LLMs in real-world applications can bechallenging due to their high computational requirements and concerns on dataprivacy. Previous studies have focused on building task-specific small LanguageModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,these approaches are ill-suited for knowledge-intensive reasoning tasks due tothe limited capacity of small LMs in memorizing the knowledge required.Motivated by our theoretical analysis on memorization, we proposeKnowledge-Augmented Reasoning Distillation (KARD), a novel method thatfine-tunes small LMs to generate rationales obtained from LLMs with augmentedknowledge retrieved from an external knowledge base. Moreover, we furtherpropose a neural reranker to obtain documents relevant to rationale generation.We empirically show that KARD significantly improves the performance of smallT5 and GPT models on the challenging knowledge-intensive reasoning datasets,namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the250M T5 models achieve superior performance against the fine-tuned 3B models,having 12 times larger parameters, on both MedQA-USMLE and StrategyQAbenchmarks.\r2023-10-28\nFedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy\nJianqing Zhang Yang Hua Hao Wang Tao Song Zhengui Xue Ruhui Ma Haibing Guan\nabstract\rabstract: Recently, personalized federated learning (pFL) has attracted increasingattention in privacy protection, collaborative learning, and tacklingstatistical heterogeneity among clients, e.g., hospitals, mobile smartphones,etc. Most existing pFL methods focus on exploiting the global information andpersonalized information in the client-level model parameters while neglectingthat data is the source of these two kinds of information. To address this, wepropose the Federated Conditional Policy (FedCP) method, which generates aconditional policy for each sample to separate the global information andpersonalized information in its features and then processes them by a globalhead and a personalized head, respectively. FedCP is more fine-grained toconsider personalization in a sample-specific manner than existing pFL methods.Extensive experiments in computer vision and natural language processingdomains show that FedCP outperforms eleven state-of-the-art methods by up to6.69%. Furthermore, FedCP maintains its superiority when some clientsaccidentally drop out, which frequently happens in mobile settings. Our code ispublic at https://github.com/TsingZ0/FedCP.\rCOPF: Continual Learning Human Preference through Optimal Policy Fitting\nHan Zhang Lin Gui Yuanzhao Zhai Hui Wang Yu Lei Ruifeng Xu\nabstract\rabstract: The technique of Reinforcement Learning from Human Feedback (RLHF) is acommonly employed method to improve pre-trained Language Models (LM), enhancingtheir ability to conform to human preferences. Nevertheless, the currentRLHF-based LMs necessitate full retraining each time novel queries or feedbackare introduced, which becomes a challenging task because human preferences canvary between different domains or tasks. Retraining LMs poses practicaldifficulties in many real-world situations due to the significant time andcomputational resources required, along with concerns related to data privacy.To address this limitation, we propose a new method called Continual OptimalPolicy Fitting (COPF), in which we estimate a series of optimal policies usingthe Monte Carlo method, and then continually fit the policy sequence with thefunction regularization. COPF involves a single learning phase and doesn\u0026rsquo;tnecessitate complex reinforcement learning. Importantly, it shares thecapability with RLHF to learn from unlabeled data, making it flexible forcontinual preference learning. Our experimental results show that COPFoutperforms strong Continuous learning (CL) baselines when it comes toconsistently aligning with human preferences on different tasks and domains.\r2023-10-27\nSDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes\nAdam D. Lelkes Eric Loreaux Tal Schuster Ming-Jun Chen Alvin Rajkomar\nabstract\rabstract: Social and behavioral determinants of health (SDOH) play a significant rolein shaping health outcomes, and extracting these determinants from clinicalnotes is a first step to help healthcare providers systematically identifyopportunities to provide appropriate care and address disparities. Progress onusing NLP methods for this task has been hindered by the lack of high-qualitypublicly available labeled data, largely due to the privacy and regulatoryconstraints on the use of real patients\u0026rsquo; information. This paper introduces anew dataset, SDOH-NLI, that is based on publicly available notes and which werelease publicly. We formulate SDOH extraction as a natural language inference(NLI) task, and provide binary textual entailment labels obtained from humanraters for a cross product of a set of social history snippets as premises andSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks inthat our premises and hypotheses are obtained independently. We evaluate both\u0026quot;off-the-shelf\u0026quot; entailment models as well as models fine-tuned on our data, andhighlight the ways in which our dataset appears more challenging than commonlyused NLI datasets.\rCan LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory\nNiloofar Mireshghallah Hyunwoo Kim Xuhui Zhou Yulia Tsvetkov Maarten Sap Reza Shokri Yejin Choi\nabstract\rabstract: The interactive use of large language models (LLMs) in AI assistants (atwork, home, etc.) introduces a new set of inference-time privacy risks: LLMsare fed different types of information from multiple sources in their inputsand are expected to reason about what to share in their outputs, for whatpurpose and with whom, within a given context. In this work, we draw attentionto the highly critical yet overlooked notion of contextual privacy by proposingConfAIde, a benchmark designed to identify critical weaknesses in the privacyreasoning capabilities of instruction-tuned LLMs. Our experiments show thateven the most capable models such as GPT-4 and ChatGPT reveal privateinformation in contexts that humans would not, 39% and 57% of the time,respectively. This leakage persists even when we employ privacy-inducingprompts or chain-of-thought reasoning. Our work underscores the immediate needto explore novel inference-time privacy-preserving approaches, based onreasoning and theory of mind.\r2023-10-26\nPockEngine: Sparse and Efficient Fine-tuning in a Pocket\nLigeng Zhu Lanxiang Hu Ji Lin Wei-Chen Wang Wei-Ming Chen Chuang Gan Song Han\nabstract\rabstract: On-device learning and efficient fine-tuning enable continuous andprivacy-preserving customization (e.g., locally fine-tuning large languagemodels on personalized data). However, existing training frameworks aredesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) andlack the optimizations for learning on the edge, which faces challenges ofresource limitations and edge hardware diversity. We introduce PockEngine: atiny, sparse and efficient engine to enable fine-tuning on various edgedevices. PockEngine supports sparse backpropagation: it prunes the backwardgraph and sparsely updates the model with measured memory saving and latencyreduction while maintaining the model quality. Secondly, PockEngine iscompilation first: the entire training graph (including forward, backward andoptimization steps) is derived at compile-time, which reduces the runtimeoverhead and brings opportunities for graph transformations. PockEngine alsointegrates a rich set of training graph optimizations, thus can furtheraccelerate the training cost, including operator reordering and backendswitching. PockEngine supports diverse applications, frontends and hardwarebackends: it flexibly compiles and tunes models defined inPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. Weevaluated PockEngine on both vision models and large language models.PockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow(Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin).Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orinat 550 tokens/s, 7.9$\\times$ faster than the PyTorch.\rDocumentNet: Bridging the Data Gap in Document Pre-Training\nLijun Yu Jin Miao Xiaoyu Sun Jiayi Chen Alexander G. Hauptmann Hanjun Dai Wei Wei\nabstract\rabstract: Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER.\r2023-10-25\nPrivately Aligning Language Models with Reinforcement Learning\nFan Wu Huseyin A. Inan Arturs Backurs Varun Chandrasekaran Janardhan Kulkarni Robert Sim\nabstract\rabstract: Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.\rDual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping\nYunming Zhang Dengpan Ye Caiyun Xie Long Tang Chuanxi Chen Ziyi Liu Jiacheng Deng\nabstract\rabstract: The malicious applications of deep forgery, represented by face swapping,have introduced security threats such as misinformation dissemination andidentity fraud. While some research has proposed the use of robust watermarkingmethods to trace the copyright of facial images for post-event traceability,these methods cannot effectively prevent the generation of forgeries at thesource and curb their dissemination. To address this problem, we propose anovel comprehensive active defense mechanism that combines traceability andadversariality, called Dual Defense. Dual Defense invisibly embeds a singlerobust watermark within the target face to actively respond to sudden cases ofmalicious face swapping. It disrupts the output of the face swapping modelwhile maintaining the integrity of watermark information throughout the entiredissemination process. This allows for watermark extraction at any stage ofimage tracking for traceability. Specifically, we introduce a watermarkembedding network based on original-domain feature impersonation attack. Thisnetwork learns robust adversarial features of target facial images and embedswatermarks, seeking a well-balanced trade-off between watermark invisibility,adversariality, and traceability through perceptual adversarial encodingstrategies. Extensive experiments demonstrate that Dual Defense achievesoptimal overall defense success rates and exhibits promising universality inanti-face swapping tasks and dataset generalization ability. It maintainsimpressive adversariality and traceability in both original and robustsettings, surpassing current forgery defense methods that possess only one ofthese capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGDmethods.\rFedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning\nJaemin Shin Hyungjun Yoon Seungjoo Lee Sungjoon Park Yunxin Liu Jinho D. Choi Sung-Ju Lee\nabstract\rabstract: Psychiatrists diagnose mental disorders via the linguistic use of patients.Still, due to data privacy, existing passive mental health monitoring systemsuse alternative features such as activity, app usage, and location via mobiledevices. We propose FedTherapist, a mobile mental health monitoring system thatutilizes continuous speech and keyboard input in a privacy-preserving way viafederated learning. We explore multiple model designs by comparing theirperformance and overhead for FedTherapist to overcome the complex nature ofon-device language model training on smartphones. We further propose aContext-Aware Language Learning (CALL) methodology to effectively utilizesmartphones\u0026rsquo; large and noisy text for mental health signal sensing. OurIRB-approved evaluation of the prediction of self-reported depression, stress,anxiety, and mood from 46 participants shows higher accuracy of FedTherapistcompared with the performance with non-language features, achieving 0.15 AUROCimprovement and 8.21% MAE reduction.\rRCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models\nZefan Wang Zichuan Liu Yingying Zhang Aoxiao Zhong Lunting Fan Lingfei Wu Qingsong Wen\nabstract\rabstract: Large language model (LLM) applications in cloud root cause analysis (RCA)have been actively explored recently. However, current methods are stillreliant on manual workflow settings and do not unleash LLMs\u0026rsquo; decision-makingand environment interaction capabilities. We present RCAgent, a tool-augmentedLLM autonomous agent framework for practical and privacy-aware industrial RCAusage. Running on an internally deployed model rather than GPT families,RCAgent is capable of free-form data collection and comprehensive analysis withtools. Our framework combines a variety of enhancements, including a uniqueSelf-Consistency for action trajectories, and a suite of methods for contextmanagement, stabilization, and importing domain knowledge. Our experiments showRCAgent\u0026rsquo;s evident and consistent superiority over ReAct across all aspects ofRCA \u0026ndash; predicting root causes, solutions, evidence, and responsibilities \u0026ndash; andtasks covered or uncovered by current rules, as validated by both automatedmetrics and human evaluations. Furthermore, RCAgent has already been integratedinto the diagnosis and issue discovery workflow of the Real-time ComputePlatform for Apache Flink of Alibaba Cloud.\r2023-10-24\nFLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering\nMd Rafi Ur Rashid Vishnu Asutosh Dasu Kang Gu Najrin Sultana Shagufta Mehnaz\nabstract\rabstract: Federated learning (FL) is becoming a key component in many technology-basedapplications including language modeling \u0026ndash; where individual FL participantsoften have privacy-sensitive text data in their local datasets. However,realizing the extent of privacy leakage in federated language models is notstraightforward and the existing attacks only intend to extract data regardlessof how sensitive or naive it is. To fill this gap, in this paper, we introducetwo novel findings with regard to leaking privacy-sensitive user data fromfederated language models. Firstly, we make a key observation that modelsnapshots from the intermediate rounds in FL can cause greater privacy leakagethan the final trained model. Secondly, we identify that privacy leakage can beaggravated by tampering with a model\u0026rsquo;s selective weights that are specificallyresponsible for memorizing the sensitive training data. We show how a maliciousclient can leak the privacy-sensitive data of some other user in FL evenwithout any cooperation from the server. Our best-performing method improvesthe membership inference recall by 29% and achieves up to 70% private datareconstruction, evidently outperforming existing attacks with strongerassumptions of adversary capabilities.\rSoK: Memorization in General-Purpose Large Language Models\nValentin Hartmann Anshuman Suri Vincent Bindschaedler David Evans Shruti Tople Robert West\nabstract\rabstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.\rCreating a silver standard for patent simplification\nSilvia Casola Alberto Lavelli Horacio Saggion\nabstract\rabstract: Patents are legal documents that aim at protecting inventions on the one handand at making technical knowledge circulate on the other. Their complex style\u0026ndash; a mix of legal, technical, and extremely vague language \u0026ndash; makes theircontent hard to access for humans and machines and poses substantial challengesto the information retrieval community. This paper proposes an approach toautomatically simplify patent text through rephrasing. Since no in-domainparallel simplification data exist, we propose a method to automaticallygenerate a large-scale silver standard for patent sentences. To obtaincandidates, we use a general-domain paraphrasing system; however, the processis error-prone and difficult to control. Thus, we pair it with proper filtersand construct a cleaner corpus that can successfully be used to train asimplification system. Human evaluation of the synthetic silver corpus showsthat it is considered grammatical, adequate, and contains simple sentences.\rCRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model\nKaiyan Zhang Ning Ding Biqing Qi Xuekai Zhu Xinwei Long Bowen Zhou\nabstract\rabstract: Instruction tuning has recently been recognized as an effective way ofaligning Large Language Models (LLMs) to enhance their generalization abilityacross various tasks. However, when tuning publicly accessible, centralizedLLMs with private instruction data, privacy concerns are inevitable. Whiledirect transfer of parameterized modules between models is a plausible approachto address this, its implications and effectiveness need further exploration.This paper focuses on Offsite-Tuning (OFT), a representative technique thattransfers transformer blocks between centralized LLMs and downstream emulators.Given the limited understanding of the underlying mechanism of OFT, we performan empirical analysis on LLMs from the perspectives of representation andfunctional similarity. Interestingly, our findings reveal a unique modularstructure within the layers of LLMs that appears to emerge as the model sizeexpands. Simultaneously, we note subtle but potentially significant changes inrepresentation and intermediate predictions across the layers. Inspired bythese observations, we propose CRaSh, involving Clustering, Removing, andSharing, a training-free strategy to derive improved emulators from LLMs. CRaShsignificantly boosts performance of OFT with billions of parameters.Furthermore, we investigate the optimal solutions yielded by fine-tuning withand without full model through the lens of loss landscape. Our findingsdemonstrate a linear connectivity among these optima falling over the samebasin, thereby highlighting the effectiveness of CRaSh and OFT. The source codeis publicly available at https://github.com/TsinghuaC3I/CRaSh.\rThe Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks\nXiaoyi Chen Siyuan Tang Rui Zhu Shijun Yan Lei Jin Zihao Wang Liya Su XiaoFeng Wang Haixu Tang\nabstract\rabstract: The era post-2018 marked the advent of Large Language Models (LLMs), withinnovations such as OpenAI\u0026rsquo;s ChatGPT showcasing prodigious linguistic prowess.As the industry galloped toward augmenting model parameters and capitalizing onvast swaths of human language data, security and privacy challenges alsoemerged. Foremost among these is the potential inadvertent accrual of PersonalIdentifiable Information (PII) during web-based data acquisition, posing risksof unintended PII disclosure. While strategies like RLHF during training andCatastrophic Forgetting have been marshaled to control the risk of privacyinfringements, recent advancements in LLMs, epitomized by OpenAI\u0026rsquo;s fine-tuninginterface for GPT-3.5, have reignited concerns. One may ask: can thefine-tuning of LLMs precipitate the leakage of personal information embeddedwithin training datasets? This paper reports the first endeavor to seek theanswer to the question, particularly our discovery of a new LLM exploitationavenue, called the Janus attack. In the attack, one can construct a PIIassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,to potentially reinstate and reveal concealed PIIs. Our findings indicate that,with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition frombeing impermeable to PII extraction to a state where they divulge a substantialproportion of concealed PII. This research, through its deep dive into theJanus attack vector, underscores the imperative of navigating the intricateinterplay between LLM utility and privacy preservation.\rCounter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think \u0026ndash; Introducing AI Detectability Index\nMegha Chakraborty S. M Towhidul Islam Tonmoy S M Mehedi Zaman Krish Sharma Niyar R Barman Chandan Gupta Shreya Gautam Tanay Kumar Vinija Jain Aman Chadha Amit P. Sheth Amitava Das\nabstract\rabstract: With the rise of prolific ChatGPT, the risk and consequences of AI-generatedtext has increased alarmingly. To address the inevitable question of ownershipattribution for AI-generated artifacts, the US Copyright Office released astatement stating that \u0026lsquo;If a work\u0026rsquo;s traditional elements of authorship wereproduced by a machine, the work lacks human authorship and the Office will notregister it\u0026rsquo;. Furthermore, both the US and the EU governments have recentlydrafted their initial proposals regarding the regulatory framework for AI.Given this cynosural spotlight on generative AI, AI-generated text detection(AGTD) has emerged as a topic that has already received immediate attention inresearch, with some initial methods having been proposed, soon followed byemergence of techniques to bypass detection. This paper introduces the CounterTuring Test (CT^2), a benchmark consisting of techniques aiming to offer acomprehensive evaluation of the robustness of existing AGTD techniques. Ourempirical findings unequivocally highlight the fragility of the proposed AGTDmethods under scrutiny. Amidst the extensive deliberations on policy-making forregulating AI development, it is of utmost importance to assess thedetectability of content generated by LLMs. Thus, to establish a quantifiablespectrum facilitating the evaluation and ranking of LLMs according to theirdetectability levels, we propose the AI Detectability Index (ADI). We conduct athorough examination of 15 contemporary LLMs, empirically demonstrating thatlarger LLMs tend to have a higher ADI, indicating they are less detectablecompared to smaller LLMs. We firmly believe that ADI holds significant value asa tool for the wider NLP community, with the potential to serve as a rubric inAI-related policy-making.\r2023-10-23\nHealth Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model\nYohn Jairo Parra Bautista Vinicious Lima Carlos Theran Richard Alo\nabstract\rabstract: Health disparities are differences in health outcomes and access tohealthcare between different groups, including racial and ethnic minorities,low-income people, and rural residents. An artificial intelligence (AI) programcalled large language models (LLMs) can understand and generate human language,improving health communication and reducing health disparities. There are manychallenges in using LLMs in human-doctor interaction, including the need fordiverse and representative data, privacy concerns, and collaboration betweenhealthcare providers and technology experts. We introduce the comparativeinvestigation of domain-specific large language models such as SciBERT with amulti-purpose LLMs BERT. We used cosine similarity to analyze text queriesabout health disparities in exam rooms when factors such as race are usedalone. Using text queries, SciBERT fails when it doesn\u0026rsquo;t differentiate betweenqueries text: \u0026ldquo;race\u0026rdquo; alone and \u0026ldquo;perpetuates health disparities.\u0026rdquo; We believeclinicians can use generative AI to create a draft response when communicatingasynchronously with patients. However, careful attention must be paid to ensurethey are developed and implemented ethically and equitably.\rDid the Neurons Read your Book? Document-level Membership Inference for Large Language Models\nMatthieu Meeus Shubham Jain Marek Rei Yves-Alexandre de Montjoye\nabstract\rabstract: With large language models (LLMs) poised to become embedded in our dailylives, questions are starting to be raised about the dataset(s) they learnedfrom. These questions range from potential bias or misinformation LLMs couldretain from their training data to questions of copyright and fair use ofhuman-generated text. However, while these questions emerge, developers of therecent state-of-the-art LLMs become increasingly reluctant to disclose detailson their training corpus. We here introduce the task of document-levelmembership inference for real-world LLMs, i.e. inferring whether the LLM hasseen a given document during training or not. First, we propose a procedure forthe development and evaluation of document-level membership inference for LLMsby leveraging commonly used data sources for training and the model releasedate. We then propose a practical, black-box method to predict document-levelmembership and instantiate it on OpenLLaMA-7B with both books and academicpapers. We show our methodology to perform very well, reaching an impressiveAUC of 0.856 for books and 0.678 for papers. We then show our approach tooutperform the sentence-level membership inference attacks used in the privacyliterature for the document-level membership task. We finally evaluate whethersmaller models might be less sensitive to document-level inference and showOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.Taken together, our results show that accurate document-level membership can beinferred for LLMs, increasing the transparency of technology poised to changeour lives.\rTowards LLM-driven Dialogue State Tracking\nYujie Feng Zexin Lu Bo Liu Liming Zhan Xiao-Ming Wu\nabstract\rabstract: Dialogue State Tracking (DST) is of paramount importance in ensuring accuratetracking of user goals and system actions within task-oriented dialoguesystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPThas sparked considerable interest in assessing their efficacy across diverseapplications. In this study, we conduct an initial examination of ChatGPT\u0026rsquo;scapabilities in DST. Our evaluation uncovers the exceptional performance ofChatGPT in this task, offering valuable insights to researchers regarding itscapabilities and providing useful directions for designing and enhancingdialogue systems. Despite its impressive performance, ChatGPT has significantlimitations including its closed-source nature, request restrictions, raisingdata privacy concerns, and lacking local deployment capabilities. To addressthese concerns, we present LDST, an LLM-driven DST framework based on smaller,open-source foundation models. By utilizing a novel domain-slot instructiontuning method, LDST achieves performance on par with ChatGPT. Comprehensiveevaluations across three distinct experimental settings, we find that LDSTexhibits remarkable performance improvements in both zero-shot and few-shotsetting compared to previous SOTA methods. The source code is provided forreproducibility.\rDifferentially Private Natural Language Models: Recent Advances and Future Directions\nLijie Hu Ivan Habernal Lei Shen Di Wang\nabstract\rabstract: Recent developments in deep learning have led to great success in variousnatural language processing (NLP) tasks. However, these applications mayinvolve data that contain sensitive information. Therefore, how to achieve goodperformance while also protecting the privacy of sensitive data is a crucialchallenge in NLP. To preserve privacy, Differential Privacy (DP), which canprevent reconstruction attacks and protect against potential side knowledge, isbecoming a de facto technique for private data analysis. In recent years, NLPin DP models (DP-NLP) has been studied from different perspectives, whichdeserves a comprehensive review. In this paper, we provide the first systematicreview of recent advances in DP deep learning models in NLP. In particular, wefirst discuss some differences and additional challenges of DP-NLP comparedwith the standard DP deep learning. Then, we investigate some existing work onDP-NLP and present its recent developments from three aspects: gradientperturbation based methods, embedding vector perturbation based methods, andensemble model based methods. We also discuss some challenges and futuredirections.\rMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems\nJakub Macina Nico Daheim Sankalan Pal Chowdhury Tanmay Sinha Manu Kapur Iryna Gurevych Mrinmaya Sachan\nabstract\rabstract: While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly.\rH2O Open Ecosystem for State-of-the-art Large Language Models\nArno Candel Jon McKinney Philipp Singer Pascal Pfeiffer Maximilian Jeblick Chun Ming Lee Marcos V. Conde\nabstract\rabstract: Large Language Models (LLMs) represent a revolution in AI. However, they alsopose many significant risks, such as the presence of biased, private,copyrighted or harmful text. For this reason we need open, transparent and safesolutions. We introduce a complete open-source ecosystem for developing andtesting LLMs. The goal of this project is to boost open alternatives toclosed-source approaches. We release h2oGPT, a family of fine-tuned LLMs ofdiverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUIdesigned for efficient fine-tuning, evaluation, and deployment of LLMs usingthe most recent state-of-the-art techniques. Our code and models are fullyopen-source. We believe this work helps to boost AI development and make itmore accessible, efficient and trustworthy. The demo is available at:https://gpt.h2o.ai/\r$Λ$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative AI\nShoki Ohta Takayuki Nishio\nabstract\rabstract: In the wake of the burgeoning expansion of generative artificial intelligence(AI) services, the computational demands inherent to these technologiesfrequently necessitate cloud-powered computational offloading, particularly forresource-constrained mobile devices. These services commonly employ prompts tosteer the generative process, and both the prompts and the resultant content,such as text and images, may harbor privacy-sensitive or confidentialinformation, thereby elevating security and privacy risks. To mitigate theseconcerns, we introduce $\\Lambda$-Split, a split computing framework tofacilitate computational offloading while simultaneously fortifying dataprivacy against risks such as eavesdropping and unauthorized access. In$\\Lambda$-Split, a generative model, usually a deep neural network (DNN), ispartitioned into three sub-models and distributed across the user\u0026rsquo;s localdevice and a cloud server: the input-side and output-side sub-models areallocated to the local, while the intermediate, computationally-intensivesub-model resides on the cloud server. This architecture ensures that only thehidden layer outputs are transmitted, thereby preventing the externaltransmission of privacy-sensitive raw input and output data. Given theblack-box nature of DNNs, estimating the original input or output fromintercepted hidden layer outputs poses a significant challenge for maliciouseavesdroppers. Moreover, $\\Lambda$-Split is orthogonal to traditionalencryption-based security mechanisms, offering enhanced security when deployedin conjunction. We empirically validate the efficacy of the $\\Lambda$-Splitframework using Llama 2 and Stable Diffusion XL, representative large languageand diffusion models developed by Meta and Stability AI, respectively. Our$\\Lambda$-Split implementation is publicly accessible athttps://github.com/nishio-laboratory/lambda_split.\r2023-10-22\nCOFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation\nNan Wang Qifan Wang Yi-Chia Wang Maziar Sanjabi Jingzhou Liu Hamed Firooz Hongning Wang Shaoliang Nie\nabstract\rabstract: As language models become increasingly integrated into our digital lives,Personalized Text Generation (PTG) has emerged as a pivotal component with awide range of applications. However, the bias inherent in user written text,often used for PTG model training, can inadvertently associate different levelsof linguistic quality with users\u0026rsquo; protected attributes. The model can inheritthe bias and perpetuate inequality in generating text w.r.t. users\u0026rsquo; protectedattributes, leading to unfair treatment when serving users. In this work, weinvestigate fairness of PTG in the context of personalized explanationgeneration for recommendations. We first discuss the biases in generatedexplanations and their fairness implications. To promote fairness, we introducea general framework to achieve measure-specific counterfactual fairness inexplanation generation. Extensive experiments and human evaluations demonstratethe effectiveness of our method.\rText-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning\nShengfang Zhai Yinpeng Dong Qingni Shen Shi Pu Yuejian Fang Hang Su\nabstract\rabstract: With the help of conditioning mechanisms, the state-of-the-art diffusionmodels have achieved tremendous success in guided image generation,particularly in text-to-image synthesis. To gain a better understanding of thetraining process and potential risks of text-to-image synthesis, we perform asystematic investigation of backdoor attack on text-to-image diffusion modelsand propose BadT2I, a general multimodal backdoor attack framework that tamperswith image synthesis in diverse semantic levels. Specifically, we performbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, ourmethods efficiently inject backdoors into a large-scale text-to-image diffusionmodel while preserving its utility with benign inputs. We conduct empiricalexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,demonstrating that the large-scale diffusion model can be easily backdooredwithin a few fine-tuning steps. We conduct additional experiments to explorethe impact of different types of textual triggers, as well as the backdoorpersistence during further training, providing insights for the development ofbackdoor defense methods. Besides, our investigation may contribute to thecopyright protection of text-to-image models in the future.\rNeural Text Sanitization with Privacy Risk Indicators: An Empirical Analysis\nAnthi Papadopoulou Pierre Lison Mark Anderson Lilja Øvrelid Ildikó Pilán\nabstract\rabstract: Text sanitization is the task of redacting a document to mask all occurrencesof (direct or indirect) personal identifiers, with the goal of concealing theidentity of the individual(s) referred in it. In this paper, we consider atwo-step approach to text sanitization and provide a detailed analysis of itsempirical performance on two recently published datasets: the TextAnonymization Benchmark (Pil'an et al., 2022) and a collection of Wikipediabiographies (Papadopoulou et al., 2022). The text sanitization process startswith a privacy-oriented entity recognizer that seeks to determine the textspans expressing identifiable personal information. This privacy-orientedentity recognizer is trained by combining a standard named entity recognitionmodel with a gazetteer populated by person-related terms extracted fromWikidata. The second step of the text sanitization process consists inassessing the privacy risk associated with each detected text span, eitherisolated or in combination with other text spans. We present five distinctindicators of the re-identification risk, respectively based on language modelprobabilities, text span classification, sequence labelling, perturbations, andweb search. We provide a contrastive analysis of each privacy indicator andhighlight their benefits and limitations, notably in relation to the availablelabeled data.\rDemocratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models\nSumuk Shashidhar Abhinav Chinta Vaibhav Sahai Zhenhailong Wang Heng Ji\nabstract\rabstract: The dominance of proprietary LLMs has led to restricted access and raisedinformation privacy concerns. High-performing open-source alternatives arecrucial for information-sensitive and high-volume applications but often lagbehind in performance. To address this gap, we propose (1) A untargeted variantof iterative self-critique and self-refinement devoid of external influence.(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score(PeRFICS) - to find the optimal model for a given task considering refinedperformance and cost. Our experiments show that SoTA open source models ofvarying sizes from 7B - 65B, on average, improve 8.2% from their baselineperformance. Strikingly, even models with extremely small memory footprints,such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%improvement in high-creativity, open ended tasks on the Vicuna benchmark.Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement.This work has profound implications for resource-constrained andinformation-sensitive environments seeking to leverage LLMs without incurringprohibitive costs, compromising on performance and privacy. The domain-agnosticself-refinement process coupled with our novel ranking metric facilitatesinformed decision-making in model selection, thereby reducing costs anddemocratizing access to high-performing language models, as evidenced by casestudies.\r2023-10-21\nTransductive Learning for Textual Few-Shot Classification in API-based Embedding Models\nPierre Colombo Victor Pellegrain Malik Boudiaf Victor Storchan Myriam Tami Ismail Ben Ayed Celine Hudelot Pablo Piantanida\nabstract\rabstract: Proprietary and closed APIs are becoming increasingly common to processnatural language, and are impacting the practical applications of naturallanguage processing, including few-shot classification. Few-shot classificationinvolves training a model to perform a new classification task with a handfulof labeled data. This paper presents three contributions. First, we introduce ascenario where the embedding of a pre-trained model is served through a gatedAPI with compute-cost and data-privacy constraints. Second, we propose atransductive inference, a learning paradigm that has been overlooked by the NLPcommunity. Transductive inference, unlike traditional inductive learning,leverages the statistics of unlabeled data. We also introduce a newparameter-free transductive regularizer based on the Fisher-Rao loss, which canbe used on top of the gated API embeddings. This method fully utilizesunlabeled data, does not share any label with the third-party API provider andcould serve as a baseline for future research. Third, we propose an improvedexperimental setting and compile a benchmark of eight datasets involvingmulticlass classification in four different languages, with up to 151 classes.We evaluate our methods using eight backbone models, along with an episodicevaluation over 1,000 episodes, which demonstrate the superiority oftransductive inference over the standard inductive setting.\r2023-10-20\nZero-Shot Sharpness-Aware Quantization for Pre-trained Language Models\nMiaoxi Zhu Qihuang Zhong Li Shen Liang Ding Juhua Liu Bo Du Dacheng Tao\nabstract\rabstract: Quantization is a promising approach for reducing memory overhead andaccelerating inference, especially in large pre-trained language model (PLM)scenarios. While having no access to original training data due to security andprivacy concerns has emerged the demand for zero-shot quantization. Most of thecutting-edge zero-shot quantization methods primarily 1) apply to computervision tasks, and 2) neglect of overfitting problem in the generativeadversarial learning process, leading to sub-optimal performance. Motivated bythis, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)framework for the zero-shot quantization of various PLMs. The key algorithm insolving ZSAQ is the SAM-SGA optimization, which aims to improve thequantization accuracy and model generalization via optimizing a minimaxproblem. We theoretically prove the convergence rate for the minimaxoptimization problem and this result can be applied to other nonconvex-PLminimax optimization frameworks. Extensive experiments on 11 tasks demonstratethat our method brings consistent and significant performance gains on bothdiscriminative and generative PLMs, i.e., up to +6.98 average score.Furthermore, we empirically validate that our method can effectively improvethe model generalization.\rAssessing Privacy Risks in Language Models: A Case Study on Summarization Tasks\nRuixiang Tang Gord Lueck Rodolfo Quispe Huseyin A Inan Janardhan Kulkarni Xia Hu\nabstract\rabstract: Large language models have revolutionized the field of NLP by achievingstate-of-the-art performance on various tasks. However, there is a concern thatthese models may disclose information in the training data. In this study, wefocus on the summarization task and investigate the membership inference (MI)attack: given a sample and black-box access to a model\u0026rsquo;s API, it is possible todetermine if the sample was part of the training data. We exploit textsimilarity and the model\u0026rsquo;s resistance to document modifications as potential MIsignals and evaluate their effectiveness on widely used datasets. Our resultsdemonstrate that summarization models are at risk of exposing data membership,even in cases where the reference summary is not available. Furthermore, wediscuss several safeguards for training summarization models to protect againstMI attacks and discuss the inherent trade-off between privacy and utility.\r2023-10-19\nTabuLa: Harnessing Language Models for Tabular Data Synthesis\nZilong Zhao Robert Birke Lydia Chen\nabstract\rabstract: Given the ubiquitous use of tabular data in industries and the growingconcerns in data privacy and security, tabular data synthesis emerges as acritical research area. The recent state-of-the-art methods show that largelanguage models (LLMs) can be adopted to generate realistic tabular data. AsLLMs pre-process tabular data as full text, they have the advantage of avoidingthe curse of dimensionality associated with one-hot encoding high-dimensionaldata. However, their long training time and limited re-usability on new tasksprevent them from replacing exiting tabular generative models. In this paper,we propose Tabula, a tabular data synthesizer based on the language modelstructure. Through Tabula, we demonstrate the inherent limitation of employingpre-trained language models designed for natural language processing (NLP) inthe context of tabular data synthesis. Our investigation delves into thedevelopment of a dedicated foundational model tailored specifically for tabulardata synthesis. Additionally, we propose a token sequence compression strategyto significantly reduce training time while preserving the quality of syntheticdata. Extensive experiments on six datasets demonstrate that using a languagemodel structure without loading the well-trained model weights yields a betterstarting model for tabular data synthesis. Moreover, the Tabula model,previously trained on other tabular data, serves as an excellent foundationmodel for new tabular data synthesis tasks. Additionally, the token sequencecompression method substantially reduces the model\u0026rsquo;s training time. Resultsshow that Tabula averagely reduces 46.2% training time per epoch comparing tocurrent LLMs-based state-of-the-art algorithm and consistently achieves evenhigher synthetic data utility.\rReliable and Efficient In-Memory Fault Tolerance of Large Language Model Pretraining\nYuxin Wang Shaohuai Shi Xin He Zhenheng Tang Xinglin Pan Yang Zheng Xiaoyu Wu Amelie Chi Zhou Bingsheng He Xiaowen Chu\nabstract\rabstract: Extensive system scales (i.e. thousands of GPU/TPUs) and prolonged trainingperiods (i.e. months of pretraining) significantly escalate the probability offailures when training large language models (LLMs). Thus, efficient andreliable fault-tolerance methods are in urgent need. Checkpointing is theprimary fault-tolerance method to periodically save parameter snapshots fromGPU memory to disks via CPU memory. In this paper, we identify the frequency ofexisting checkpoint-based fault-tolerance being significantly limited by thestorage I/O overheads, which results in hefty re-training costs on restartingfrom the nearest checkpoint. In response to this gap, we introduce an in-memoryfault-tolerance framework for large-scale LLM pretraining. The framework booststhe efficiency and reliability of fault tolerance from three aspects: (1)Reduced Data Transfer and I/O: By asynchronously caching parameters, i.e.,sharded model parameters, optimizer states, and RNG states, to CPU volatilememory, Our framework significantly reduces communication costs and bypassescheckpoint I/O. (2) Enhanced System Reliability: Our framework enhancesparameter protection with a two-layer hierarchy: snapshot management processes(SMPs) safeguard against software failures, together with Erasure Coding (EC)protecting against node failures. This double-layered protection greatlyimproves the survival probability of the parameters compared to existingcheckpointing methods. (3) Improved Snapshotting Frequency: Our frameworkachieves more frequent snapshotting compared with asynchronous checkpointingoptimizations under the same saving time budget, which improves the faulttolerance efficiency. Empirical results demonstrate that Our frameworkminimizes the overhead of fault tolerance of LLM pretraining by effectivelyleveraging redundant CPU resources.\rPrivacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework\nImdad Ullah Najm Hassan Sukhpal Singh Gill Basem Suleiman Tariq Ahamed Ahanger Zawar Shah Junaid Qadir Salil S. Kanhere\nabstract\rabstract: The generative Artificial Intelligence (AI) tools based on Large LanguageModels (LLMs) use billions of parameters to extensively analyse large datasetsand extract critical private information such as, context, specific details,identifying information etc. This have raised serious threats to user privacyand reluctance to use such tools. This article proposes the conceptual modelcalled PrivChatGPT, a privacy-preserving model for LLMs that consists of twomain components i.e., preserving user privacy during the datacuration/pre-processing together with preserving private context and theprivate training process for large-scale data. To demonstrate itsapplicability, we show how a private mechanism could be integrated into theexisting model for training LLMs to protect user privacy; specifically, weemployed differential privacy and private training using Reinforcement Learning(RL). We measure the privacy loss and evaluate the measure of uncertainty orrandomness once differential privacy is applied. It further recursivelyevaluates the level of privacy guarantees and the measure of uncertainty ofpublic database and resources, during each update when new information is addedfor training purposes. To critically evaluate the use of differential privacyfor private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,private information retrieval, randomisation, for various performance measuressuch as the model performance and accuracy, computational complexity, privacyvs. utility etc. We conclude that differential privacy, randomisation, andobfuscation can impact utility and performance of trained models, conversely,the use of ToR, Blockchain, and PIR may introduce additional computationalcomplexity and high training latency. We believe that the proposed model couldbe used as a benchmark for proposing privacy preserving LLMs for generative AItools.\rRed Teaming Language Model Detectors with Language Models\nZhouxing Shi Yihan Wang Fan Yin Xiangning Chen Kai-Wei Chang Cho-Jui Hsieh\nabstract\rabstract: The prevalence and strong capability of large language models (LLMs) presentsignificant safety and ethical risks if exploited by malicious users. Toprevent the potentially deceptive usage of LLMs, recent works have proposedalgorithms to detect LLM-generated text and protect LLMs. In this paper, weinvestigate the robustness and reliability of these LLM detectors underadversarial attacks. We study two types of attack strategies: 1) replacingcertain words in an LLM\u0026rsquo;s output with their synonyms given the context; 2)automatically searching for an instructional prompt to alter the writing styleof the generation. In both strategies, we leverage an auxiliary LLM to generatethe word replacements or the instructional prompt. Different from previousworks, we consider a challenging setting where the auxiliary LLM can also beprotected by a detector. Experiments reveal that our attacks effectivelycompromise the performance of all detectors in the study with plausiblegenerations, underscoring the urgent need to improve the robustness ofLLM-generated text detection systems.\rUnmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights\nYichuan Deng Zhao Song Shenghao Xie Chiwun Yang\nabstract\rabstract: In the realm of deep learning, transformers have emerged as a dominantarchitecture, particularly in natural language processing tasks. However, withtheir widespread adoption, concerns regarding the security and privacy of thedata processed by these models have arisen. In this paper, we address a pivotalquestion: Can the data fed into transformers be recovered using their attentionweights and outputs? We introduce a theoretical framework to tackle thisproblem. Specifically, we present an algorithm that aims to recover the inputdata $X \\in \\mathbb{R}^{d \\times n}$ from given attention weights $W = QK^\\top\\in \\mathbb{R}^{d \\times d}$ and output $B \\in \\mathbb{R}^{n \\times n}$ byminimizing the loss function $L(X)$. This loss function captures thediscrepancy between the expected output and the actual output of thetransformer. Our findings have significant implications for the LocalizedLayer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model\u0026rsquo;sdesign from a security and privacy perspective. This work underscores theimportance of understanding and safeguarding the internal workings oftransformers to ensure the confidentiality of processed data.\r2023-10-18\nBlack-Box Training Data Identification in GANs via Detector Networks\nLukman Olagoke Salil Vadhan Seth Neel\nabstract\rabstract: Since their inception Generative Adversarial Networks (GANs) have beenpopular generative models across images, audio, video, and tabular data. Inthis paper we study whether given access to a trained GAN, as well as freshsamples from the underlying distribution, if it is possible for an attacker toefficiently identify if a given point is a member of the GAN\u0026rsquo;s training data.This is of interest for both reasons related to copyright, where a user maywant to determine if their copyrighted data has been used to train a GAN, andin the study of data privacy, where the ability to detect training setmembership is known as a membership inference attack. Unlike the majority ofprior work this paper investigates the privacy implications of using GANs inblack-box settings, where the attack only has access to samples from thegenerator, rather than access to the discriminator as well. We introduce asuite of membership inference attacks against GANs in the black-box setting andevaluate our attacks on image GANs trained on the CIFAR10 dataset and tabularGANs trained on genomic data. Our most successful attack, called The Detector,involve training a second network to score samples based on their likelihood ofbeing generated by the GAN, as opposed to a fresh sample from the distribution.We prove under a simple model of the generator that the detector is anapproximately optimal membership inference attack. Across a wide range oftabular and image datasets, attacks, and GAN architectures, we find thatadversaries can orchestrate non-trivial privacy attacks when provided withaccess to samples from the generator. At the same time, the attack successachievable against GANs still appears to be lower compared to other generativeand discriminative models; this leaves the intriguing open question of whetherGANs are in fact more private, or if it is a matter of developing strongerattacks.\rTo Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images \u0026hellip; For Now\nYimeng Zhang Jinghan Jia Xin Chen Aochuan Chen Yihua Zhang Jiancheng Liu Ke Ding Sijia Liu\nabstract\rabstract: The recent advances in diffusion models (DMs) have revolutionized thegeneration of complex and diverse images. However, these models also introducepotential safety hazards, such as the production of harmful content andinfringement of data copyrights. Although there have been efforts to createsafety-driven unlearning methods to counteract these challenges, doubts remainabout their capabilities. To bridge this uncertainty, we propose an evaluationframework built upon adversarial attacks (also referred to as adversarialprompts), in order to discern the trustworthiness of these safety-drivenunlearned DMs. Specifically, our research explores the (worst-case) robustnessof unlearned DMs in eradicating unwanted concepts, styles, and objects,assessed by the generation of adversarial prompts. We develop a noveladversarial learning approach called UnlearnDiff that leverages the inherentclassification capabilities of DMs to streamline the generation of adversarialprompts, making it as simple for DMs as it is for image classification attacks.This technique streamlines the creation of adversarial prompts, making theprocess as intuitive for generative modeling as it is for image classificationassaults. Through comprehensive benchmarking, we assess the unlearningrobustness of five prevalent unlearned DMs across multiple tasks. Our resultsunderscore the effectiveness and efficiency of UnlearnDiff when compared tostate-of-the-art adversarial prompting methods. Codes are available athttps://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: This papercontains model outputs that may be offensive in nature.\rEvaluating the Fairness of Discriminative Foundation Models in Computer Vision\nJunaid Ali Matthaeus Kleindessner Florian Wenzel Kailash Budhathoki Volkan Cevher Chris Russell\nabstract\rabstract: We propose a novel taxonomy for bias evaluation of discriminative foundationmodels, such as Contrastive Language-Pretraining (CLIP), that are used forlabeling tasks. We then systematically evaluate existing methods for mitigatingbias in these models with respect to our taxonomy. Specifically, we evaluateOpenAI\u0026rsquo;s CLIP and OpenCLIP models for key applications, such as zero-shotclassification, image retrieval and image captioning. We categorize desiredbehaviors based around three axes: (i) if the task concerns humans; (ii) howsubjective the task is (i.e., how likely it is that people from a diverse rangeof backgrounds would agree on a labeling); and (iii) the intended purpose ofthe task and if fairness is better served by impartiality (i.e., makingdecisions independent of the protected attributes) or representation (i.e.,making decisions to maximize diversity). Finally, we provide quantitativefairness evaluations for both binary-valued and multi-valued protectedattributes over ten diverse datasets. We find that fair PCA, a post-processingmethod for fair representations, works very well for debiasing in most of theaforementioned tasks while incurring only minor loss of performance. However,different debiasing approaches vary in their effectiveness depending on thetask. Hence, one should choose the debiasing approach depending on the specificuse case.\r2023-10-17\nLast One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning\nRui Wen Tianhao Wang Michael Backes Yang Zhang Ahmed Salem\nabstract\rabstract: Large Language Models (LLMs) are powerful tools for natural languageprocessing, enabling novel applications and user experiences. However, toachieve optimal performance, LLMs often require adaptation with private data,which poses privacy and security challenges. Several techniques have beenproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparativeprivacy and security properties have not been systematically investigated. Inthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICLagainst three types of well-established attacks: membership inference, whichexposes data leakage (privacy); backdoor, which injects malicious behavior(security); and model stealing, which can violate intellectual property(privacy and security). Our results show that there is no silver bullet forprivacy and security in LLM adaptation and each technique has differentstrengths and weaknesses.\rDisentangling the Linguistic Competence of Privacy-Preserving BERT\nStefan Arnold Nils Kemmerzell Annika Schreiner\nabstract\rabstract: Differential Privacy (DP) has been tailored to address the unique challengesof text-to-text privatization. However, text-to-text privatization is known fordegrading the performance of language models when trained on perturbed text.Employing a series of interpretation techniques on the internal representationsextracted from BERT trained on perturbed pre-text, we intend to disentangle atthe linguistic level the distortion induced by differential privacy.Experimental results from a representational similarity analysis indicate thatthe overall similarity of internal representations is substantially reduced.Using probing tasks to unpack this dissimilarity, we find evidence thattext-to-text privatization affects the linguistic competence across severalformalisms, encoding localized properties of words while falling short atencoding the contextual relationships between spans of words.\rWatermarking LLMs with Weight Quantization\nLinyang Li Botian Jiang Pengyu Wang Ke Ren Hang Yan Xipeng Qiu\nabstract\rabstract: Abuse of large language models reveals high risks as large language modelsare being deployed at an astonishing speed. It is important to protect themodel weights to avoid malicious usage that violates licenses of open-sourcelarge language models. This paper proposes a novel watermarking strategy thatplants watermarks in the quantization process of large language models withoutpre-defined triggers during inference. The watermark works when the model isused in the fp32 mode and remains hidden when the model is quantized to int8,in this way, the users can only inference the model without further supervisedfine-tuning of the model. We successfully plant the watermark into open-sourcelarge language model weights including GPT-Neo and LLaMA. We hope our proposedmethod can provide a potential direction for protecting model weights in theera of large language model applications.\rOpportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health\nShubo Tian Qiao Jin Lana Yeganova Po-Ting Lai Qingqing Zhu Xiuying Chen Yifan Yang Qingyu Chen Won Kim Donald C. Comeau Rezarta Islamaj Aadit Kapoor Xin Gao Zhiyong Lu\nabstract\rabstract: ChatGPT has drawn considerable attention from both the general public anddomain experts with its remarkable text generation capabilities. This hassubsequently led to the emergence of diverse applications in the field ofbiomedicine and health. In this work, we examine the diverse applications oflarge language models (LLMs), such as ChatGPT, in biomedicine and health.Specifically we explore the areas of biomedical information retrieval, questionanswering, medical text summarization, information extraction, and medicaleducation, and investigate whether LLMs possess the transformative power torevolutionize these tasks or whether the distinct complexities of biomedicaldomain presents unique challenges. Following an extensive literature survey, wefind that significant advances have been made in the field of text generationtasks, surpassing the previous state-of-the-art methods. For otherapplications, the advances have been modest. Overall, LLMs have not yetrevolutionized biomedicine, but recent rapid progress indicates that suchmethods hold great potential to provide valuable means for acceleratingdiscovery and improving health. We also find that the use of LLMs, likeChatGPT, in the fields of biomedicine and health entails various risks andchallenges, including fabricated information in its generated responses, aswell as legal and privacy concerns associated with sensitive patient data. Webelieve this survey can provide a comprehensive and timely overview tobiomedical researchers and healthcare practitioners on the opportunities andchallenges associated with using ChatGPT and other LLMs for transformingbiomedicine and health.\r2023-10-16\nPrivacy in Large Language Models: Attacks, Defenses and Future Directions\nHaoran Li Yulin Chen Jinglong Luo Yan Kang Xiaojin Zhang Qi Hu Chunkit Chan Yangqiu Song\nabstract\rabstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary\u0026rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.\rSnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds\nYanyu Li Huan Wang Qing Jin Ju Hu Pavlo Chemerys Yun Fu Yanzhi Wang Sergey Tulyakov Jian Ren\nabstract\rabstract: Text-to-image diffusion models can create stunning images from naturallanguage descriptions that rival the work of professional artists andphotographers. However, these models are large, with complex networkarchitectures and tens of denoising iterations, making them computationallyexpensive and slow to run. As a result, high-end GPUs and cloud-based inferenceare required to run diffusion models at scale. This is costly and has privacyimplications, especially when user data is sent to a third party. To overcomethese challenges, we present a generic approach that, for the first time,unlocks running text-to-image diffusion models on mobile devices in less than$2$ seconds. We achieve so by introducing efficient network architecture andimproving step distillation. Specifically, we propose an efficient UNet byidentifying the redundancy of the original model and reducing the computationof the image decoder via data distillation. Further, we enhance the stepdistillation by exploring training strategies and introducing regularizationfrom classifier-free guidance. Our extensive experiments on MS-COCO show thatour model with $8$ denoising steps achieves better FID and CLIP scores thanStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creationby bringing powerful text-to-image diffusion models to the hands of users.\rFATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models\nTao Fan Yan Kang Guoqiang Ma Weijing Chen Wenbin Wei Lixin Fan Qiang Yang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, haveexhibited remarkable performances across various tasks in recent years.However, LLMs face two main challenges in real-world applications. Onechallenge is that training LLMs consumes vast computing resources, preventingLLMs from being adopted by small and medium-sized enterprises with limitedcomputing resources. Another is that training LLM requires a large amount ofhigh-quality data, which are often scattered among enterprises. To addressthese challenges, we propose FATE-LLM, an industrial-grade federated learningframework for large language models. FATE-LLM (1) facilitates federatedlearning for large language models (coined FedLLM); (2) promotes efficienttraining of FedLLM using parameter-efficient fine-tuning methods; (3) protectsthe intellectual property of LLMs; (4) preserves data privacy during trainingand inference through privacy-preserving mechanisms. We release the code ofFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the researchof FedLLM and enable a broad range of industrial applications.\r2023-10-15\nA Recipe for Watermarking Diffusion Models\nYunqing Zhao Tianyu Pang Chao Du Xiao Yang Ngai-Man Cheung Min Lin\nabstract\rabstract: Diffusion models (DMs) have demonstrated advantageous potential on generativetasks. Widespread interest exists in incorporating DMs into downstreamapplications, such as producing or editing photorealistic images. However,practical deployment and unprecedented power of DMs raise legal issues,including copyright protection and monitoring of generated content. In thisregard, watermarking has been a proven solution for copyright protection andcontent monitoring, but it is underexplored in the DMs literature.Specifically, DMs generate samples from longer tracks and may have newlydesigned multimodal structures, necessitating the modification of conventionalwatermarking pipelines. To this end, we conduct comprehensive analyses andderive a recipe for efficiently watermarking state-of-the-art DMs (e.g., StableDiffusion), via training from scratch or finetuning. Our recipe isstraightforward but involves empirically ablated implementation details,providing a foundation for future research on watermarking DMs. The code isavailable at https://github.com/yunqing-me/WatermarkDM.\r2023-10-14\nTowards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning\nChang Lu Chandan K. Reddy Ping Wang Yue Ning\nabstract\rabstract: Automatic coding of International Classification of Diseases (ICD) is amulti-label text categorization task that involves extracting disease orprocedure codes from clinical notes. Despite the application ofstate-of-the-art natural language processing (NLP) techniques, there are stillchallenges including limited availability of data due to privacy constraintsand the high variability of clinical notes caused by different writing habitsof medical professionals and various pathological features of patients. In thiswork, we investigate the semi-structured nature of clinical notes and proposean automatic algorithm to segment them into sections. To address thevariability issues in existing ICD coding models with limited data, weintroduce a contrastive pre-training approach on sections using a softmulti-label similarity metric based on tree edit distance. Additionally, wedesign a masked section training strategy to enable ICD coding models to locatesections related to ICD codes. Extensive experimental results demonstrate thatour proposed training strategies effectively enhance the performance ofexisting ICD coding methods.\rUnified High-binding Watermark for Unconditional Image Generation Models\nRuinan Ma Yu-an Tan Shangbo Wu Tian Chen Yajie Wang Yuanzhang Li\nabstract\rabstract: Deep learning techniques have implemented many unconditional image generation(UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images(also known as AI-Generated Content, AIGC for short) produced by these modelsbring urgent needs for intellectual property protection such as datatraceability and copyright certification. An attacker can steal the outputimages of the target model and use them as part of the training data to train aprivate surrogate UIG model. The implementation mechanisms of UIG models arediverse and complex, and there is no unified and effective protection andverification method at present. To address these issues, we propose a two-stageunified watermark verification mechanism with high-binding effects for suchmodels. In the first stage, we use an encoder to invisibly write the watermarkimage into the output images of the original AIGC tool, and reversely extractthe watermark image through the corresponding decoder. In the second stage, wedesign the decoder fine-tuning process, and the fine-tuned decoder can makecorrect judgments on whether the suspicious model steals the original AIGC tooldata. Experiments demonstrate our method can complete the verification workwith almost zero false positive rate under the condition of only using themodel output images. Moreover, the proposed method can achieve data stealverification across different types of UIG models, which further increases thepracticality of the method.\r2023-10-13\nCopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow\nJunlei Zhou Jiashi Gao Ziwei Wang Xuetao Wei\nabstract\rabstract: Web-based AI image generation has become an innovative art form that cangenerate novel artworks with the rapid development of the diffusion model.However, this new technique brings potential copyright infringement risks as itmay incorporate the existing artworks without the owners\u0026rsquo; consent. Copyrightinfringement quantification is the primary and challenging step towardsAI-generated image copyright traceability. Previous work only focused on dataattribution from the training data perspective, which is unsuitable for tracingand quantifying copyright infringement in practice because of the followingreasons: (1) the training datasets are not always available in public; (2) themodel provider is the responsible party, not the image. Motivated by this, inthis paper, we propose CopyScope, a new framework to quantify the infringementof AI-generated images from the model level. We first rigorously identifypivotal components within the AI image generation pipeline. Then, we propose totake advantage of Fr'echet Inception Distance (FID) to effectively capture theimage similarity that fits human perception naturally. We further propose theFID-based Shapley algorithm to evaluate the infringement contribution amongmodels. Extensive experiments demonstrate that our work not only reveals theintricacies of infringement quantification but also effectively depicts theinfringing models quantitatively, thus promoting accountability in AIimage-generation tasks.\r2023-10-12\nDataless Knowledge Fusion by Merging Weights of Language Models\nXisen Jin Xiang Ren Daniel Preotiuc-Pietro Pengxiang Cheng\nabstract\rabstract: Fine-tuning pre-trained language models has become the prevalent paradigm forbuilding downstream NLP models. Oftentimes fine-tuned models are readilyavailable but their training data is not, due to data privacy or intellectualproperty concerns. This creates a barrier to fusing knowledge across individualmodels to yield a better single model. In this paper, we study the problem ofmerging individual models built on different training data sets to obtain asingle model that performs well both across all data set domains and cangeneralize on out-of-domain data. We propose a dataless knowledge fusion methodthat merges models in their parameter space, guided by weights that minimizeprediction differences between the merged model and the individual models. Overa battery of evaluation settings, we show that the proposed methodsignificantly outperforms baselines such as Fisher-weighted averaging or modelensembling. Further, we find that our method is a promising alternative tomulti-task learning that can preserve or sometimes improve over the individualmodels without access to the training data. Finally, model merging is moreefficient than training a multi-task model, thus making it applicable to awider set of scenarios.\rBuilding Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models\nJinmeng Rao Song Gao Gengchen Mai Krzysztof Janowicz\nabstract\rabstract: In recent years we have seen substantial advances in foundation models forartificial intelligence, including language, vision, and multimodal models.Recent studies have highlighted the potential of using foundation models ingeospatial artificial intelligence, known as GeoAI Foundation Models, forgeographic question answering, remote sensing image understanding, mapgeneration, and location-based services, among others. However, the developmentand application of GeoAI foundation models can pose serious privacy andsecurity risks, which have not been fully discussed or addressed to date. Thispaper introduces the potential privacy and security risks throughout thelifecycle of GeoAI foundation models and proposes a comprehensive blueprint forresearch directions and preventative and control strategies. Through thisvision paper, we hope to draw the attention of researchers and policymakers ingeospatial domains to these privacy and security risks inherent in GeoAIfoundation models and advocate for the development of privacy-preserving andsecure GeoAI foundation models.\rAn Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\nShyni Sharaf V. S. Anoop\nabstract\rabstract: This paper conducts a comprehensive investigation into applying largelanguage models, particularly on BioBERT, in healthcare. It begins withthoroughly examining previous natural language processing (NLP) approaches inhealthcare, shedding light on the limitations and challenges these methodsface. Following that, this research explores the path that led to theincorporation of BioBERT into healthcare applications, highlighting itssuitability for addressing the specific requirements of tasks related tobiomedical text mining. The analysis outlines a systematic methodology forfine-tuning BioBERT to meet the unique needs of the healthcare domain. Thisapproach includes various components, including the gathering of data from awide range of healthcare sources, data annotation for tasks like identifyingmedical entities and categorizing them, and the application of specializedpreprocessing techniques tailored to handle the complexities found inbiomedical texts. Additionally, the paper covers aspects related to modelevaluation, with a focus on healthcare benchmarks and functions like processingof natural language in biomedical, question-answering, clinical documentclassification, and medical entity recognition. It explores techniques toimprove the model\u0026rsquo;s interpretability and validates its performance compared toexisting healthcare-focused language models. The paper thoroughly examinesethical considerations, particularly patient privacy and data security. Ithighlights the benefits of incorporating BioBERT into healthcare contexts,including enhanced clinical decision support and more efficient informationretrieval. Nevertheless, it acknowledges the impediments and complexities ofthis integration, encompassing concerns regarding data privacy, transparency,resource-intensive requirements, and the necessity for model customization toalign with diverse healthcare domains.\r2023-10-11\nBeyond Memorization: Violating Privacy Via Inference with Large Language Models\nRobin Staab Mark Vero Mislav Balunović Martin Vechev\nabstract\rabstract: Current privacy research on large language models (LLMs) primarily focuses onthe issue of extracting memorized training data. At the same time, models\u0026rsquo;inference capabilities have increased drastically. This raises the key questionof whether current LLMs could violate individuals\u0026rsquo; privacy by inferringpersonal attributes from text given at inference time. In this work, we presentthe first comprehensive study on the capabilities of pretrained LLMs to inferpersonal attributes from text. We construct a dataset consisting of real Redditprofiles, and show that current LLMs can infer a wide range of personalattributes (e.g., location, income, sex), achieving up to $85%$ top-1 and$95.8%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time($240\\times$) required by humans. As people increasingly interact withLLM-powered chatbots across all aspects of life, we also explore the emergingthreat of privacy-invasive chatbots trying to extract personal informationthrough seemingly benign questions. Finally, we show that common mitigations,i.e., text anonymization and model alignment, are currently ineffective atprotecting user privacy against LLM inference. Our findings highlight thatcurrent LLMs can infer personal data at a previously unattainable scale. In theabsence of working defenses, we advocate for a broader discussion around LLMprivacy implications beyond memorization, striving for a wider privacyprotection.\rImproved Membership Inference Attacks Against Language Classification Models\nShlomit Shachor Natalia Razinkov Abigail Goldsteen\nabstract\rabstract: Artificial intelligence systems are prevalent in everyday life, with usecases in retail, manufacturing, health, and many other fields. With the rise inAI adoption, associated risks have been identified, including privacy risks tothe people whose data was used to train models. Assessing the privacy risks ofmachine learning models is crucial to enabling knowledgeable decisions onwhether to use, deploy, or share a model. A common approach to privacy riskassessment is to run one or more known attacks against the model and measuretheir success rate. We present a novel framework for running membershipinference attacks against classification models. Our framework takes advantageof the ensemble method, generating many specialized attack models for differentsubsets of the data. We show that this approach achieves higher accuracy thaneither a single attack model or an attack model per class label, both onclassical and language classification tasks.\r2023-10-10\nSound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention\nRodolfo Valentim Idilio Drago Marco Mellia Federico Cerutti\nabstract\rabstract: Sound-squatting is a phishing attack that tricks users into maliciousresources by exploiting similarities in the pronunciation of words. Proactivedefense against sound-squatting candidates is complex, and existing solutionsrely on manually curated lists of homophones. We here introduce Sound-skwatter,a multi-language AI-based system that generates sound-squatting candidates forproactive defense. Sound-skwatter relies on an innovative multi-modalcombination of Transformers Networks and acoustic models to learn soundsimilarities. We show that Sound-skwatter can automatically list knownhomophones and thousands of high-quality candidates. In addition, it coverscross-language sound-squatting, i.e., when the reader and the listener speakdifferent languages, supporting any combination of languages. We applySound-skwatter to network-centric phishing via squatted domain names. We find ~10% of the generated domains exist in the wild, the vast majority unknown toprotection solutions. Next, we show attacks on the PyPI package manager, where~ 17% of the popular packages have at least one existing candidate. We believeSound-skwatter is a crucial asset to mitigate the sound-squatting phenomenonproactively on the Internet. To increase its impact, we publish an online demoand release our models and code as open source.\rMemorization of Named Entities in Fine-tuned BERT Models\nAndor Diera Nicolas Lell Aygul Garifullina Ansgar Scherp\nabstract\rabstract: Privacy preserving deep learning is an emerging field in machine learningthat aims to mitigate the privacy risks in the use of deep neural networks. Onesuch risk is training data extraction from language models that have beentrained on datasets, which contain personal and privacy sensitive information.In our study, we investigate the extent of named entity memorization infine-tuned BERT models. We use single-label text classification asrepresentative downstream task and employ three different fine-tuning setups inour experiments, including one with Differentially Privacy (DP). We create alarge number of text samples from the fine-tuned BERT models utilizing a customsequential sampling strategy with two prompting strategies. We search in thesesamples for named entities and check if they are also present in thefine-tuning datasets. We experiment with two benchmark datasets in the domainsof emails and blogs. We show that the application of DP has a detrimentaleffect on the text generation capabilities of BERT. Furthermore, we show that afine-tuned BERT does not generate more named entities specific to thefine-tuning dataset than a BERT model that is pre-trained only. This suggeststhat BERT is unlikely to emit personal or privacy sensitive named entities.Overall, our results are important to understand to what extent BERT-basedservices are prone to training data extraction attacks.\rEvaluation and Analysis of Hallucination in Large Vision-Language Models\nJunyang Wang Yiyang Zhou Guohai Xu Pengcheng Shi Chenlin Zhao Haiyang Xu Qinghao Ye Ming Yan Ji Zhang Jihua Zhu Jitao Sang Haoyu Tang\nabstract\rabstract: Large Vision-Language Models (LVLMs) have recently achieved remarkablesuccess. However, LVLMs are still plagued by the hallucination problem, whichlimits the practicality in many scenarios. Hallucination refers to theinformation of LVLMs\u0026rsquo; responses that does not exist in the visual input, whichposes potential risks of substantial consequences. There has been limited workstudying hallucination evaluation in LVLMs. In this paper, we proposeHallucination Evaluation based on Large Language Models (HaELM), an LLM-basedhallucination evaluation framework. HaELM achieves an approximate 95%performance comparable to ChatGPT and has additional advantages including lowcost, reproducibility, privacy preservation and local deployment. Leveragingthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, weanalyze the factors contributing to hallucination in LVLMs and offer helpfulsuggestions to mitigate the hallucination problem. Our training data and humanannotation hallucination data will be made public soon.\rDASICS: Enhancing Memory Protection with Dynamic Compartmentalization\nYue Jin Yibin Xu Chengyuan Yang Han Wang Tianyi Huang Tianyue Lu Mingyu Chen\nabstract\rabstract: In the existing software development ecosystem, security issues introduced bythird-party code cannot be overlooked. Among these security concerns, memoryaccess vulnerabilities stand out prominently, leading to risks such as thetheft or tampering of sensitive data. To address this issue, software-baseddefense mechanisms have been established at the programming language, compiler,and operating system levels. However, as a trade-off, these mechanismssignificantly reduce software execution efficiency. Hardware-software co-designapproaches have sought to either construct entirely isolated trusted executionenvironments or attempt to partition security domains within the same addressspace. While such approaches enhance efficiency compared to pure softwaremethods, they also encounter challenges related to granularity of protection,performance overhead, and portability. In response to these challenges, wepresent the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secureprocessor design, which offers dynamic and flexible security protection acrossmultiple privilege levels, addressing data flow protection, control flowprotection, and secure system calls. We have implemented hardware FPGAprototypes and software QEMU simulator prototypes based on DASICS, along withnecessary modifications to system software for adaptability. We illustrate theprotective mechanisms and effectiveness of DASICS with two practical examplesand provide potential real-world use cases where DASICS could be applied.\rBC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models\nHaoxiang Luo Jian Luo Athanasios V. Vasilakos\nabstract\rabstract: In recent years, artificial intelligence (AI) and machine learning (ML) arereshaping society\u0026rsquo;s production methods and productivity, and also changing theparadigm of scientific research. Among them, the AI language model representedby ChatGPT has made great progress. Such large language models (LLMs) servepeople in the form of AI-generated content (AIGC) and are widely used inconsulting, healthcare, and education. However, it is difficult to guaranteethe authenticity and reliability of AIGC learning data. In addition, there arealso hidden dangers of privacy disclosure in distributed AI training. Moreover,the content generated by LLMs is difficult to identify and trace, and it isdifficult to cross-platform mutual recognition. The above information securityissues in the coming era of AI powered by LLMs will be infinitely amplified andaffect everyone\u0026rsquo;s life. Therefore, we consider empowering LLMs using blockchaintechnology with superior security features to propose a vision for trusted AI.This paper mainly introduces the motivation and technical route of blockchainfor LLM (BC4LLM), including reliable learning corpus, secure training process,and identifiable generated content. Meanwhile, this paper also reviews thepotential applications and future challenges, especially in the frontiercommunication networks field, including network resource allocation, dynamicspectrum sharing, and semantic communication. Based on the above work combinedand the prospect of blockchain and LLMs, it is expected to help the earlyrealization of trusted AI and provide guidance for the academic community.\rWatermarking Classification Dataset for Copyright Protection\nYixin Liu Hongsheng Hu Xun Chen Xuyun Zhang Lichao Sun\nabstract\rabstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals\u0026rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.\r2023-10-09\nDoes Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?\nAdaku Uchendu Jooyoung Lee Hua Shen Thai Le Ting-Hao \u0026lsquo;Kenneth\u0026rsquo; Huang Dongwon Lee\nabstract\rabstract: Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved thegeneration of coherent sentences resembling human writing on a large scale,resulting in the creation of so-called deepfake texts. However, this progressposes security and privacy concerns, necessitating effective solutions fordistinguishing deepfake texts from human-written ones. Although prior worksstudied humans\u0026rsquo; ability to detect deepfake texts, none has examined whether\u0026quot;collaboration\u0026quot; among humans improves the detection of deepfake texts. In thisstudy, to address this gap of understanding on deepfake texts, we conductedexperiments with two groups: (1) nonexpert individuals from the AMT platformand (2) writing experts from the Upwork platform. The results demonstrate thatcollaboration among humans can potentially improve the detection of deepfaketexts for both groups, increasing detection accuracies by 6.36% for non-expertsand 12.76% for experts, respectively, compared to individuals\u0026rsquo; detectionaccuracies. We further analyze the explanations that humans used for detectinga piece of text as deepfake text, and find that the strongest indicator ofdeepfake texts is their lack of coherence and consistency. Our study providesuseful insights for future tools and framework designs to facilitate thecollaborative human detection of deepfake texts. The experiment datasets andAMT implementations are available at:https://github.com/huashen218/llm-deepfake-human-study.git\rTwo Models are Better than One: Federated Learning Is Not Private For Google GBoard Next Word Prediction\nMohamed Suliman Douglas Leith\nabstract\rabstract: In this paper we present new attacks against federated learning when used totrain natural language text models. We illustrate the effectiveness of theattacks against the next word prediction model used in Google\u0026rsquo;s GBoard app, awidely used mobile keyboard app that has been an early adopter of federatedlearning for production use. We demonstrate that the words a user types ontheir mobile handset, e.g. when sending text messages, can be recovered withhigh accuracy under a wide range of conditions and that counter-measures such ause of mini-batches and adding local noise are ineffective. We also show thatthe word order (and so the actual sentences typed) can be reconstructed withhigh fidelity. This raises obvious privacy concerns, particularly since GBoardis in production use.\rDiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models\nYingqian Cui Jie Ren Han Xu Pengfei He Hui Liu Lichao Sun Yue Xing Jiliang Tang\nabstract\rabstract: Recently, Generative Diffusion Models (GDMs) have showcased their remarkablecapabilities in learning and generating images. A large community of GDMs hasnaturally emerged, further promoting the diversified applications of GDMs invarious fields. However, this unrestricted proliferation has raised seriousconcerns about copyright protection. For example, artists including paintersand photographers are becoming increasingly concerned that GDMs couldeffortlessly replicate their unique creative works without authorization. Inresponse to these challenges, we introduce a novel watermarking scheme,DiffusionShield, tailored for GDMs. DiffusionShield protects images fromcopyright infringement by GDMs through encoding the ownership information intoan imperceptible watermark and injecting it into the images. Its watermark canbe easily learned by GDMs and will be reproduced in their generated images. Bydetecting the watermark from generated images, copyright infringement can beexposed with evidence. Benefiting from the uniformity of the watermarks and thejoint optimization method, DiffusionShield ensures low distortion of theoriginal image, high watermark detection performance, and the ability to embedlengthy messages. We conduct rigorous and comprehensive experiments to show theeffectiveness of DiffusionShield in defending against infringement by GDMs andits superiority over traditional watermarking methods.\r2023-10-08\nAre Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT\nAkshaj Kumar Veldanda Fabian Grob Shailja Thakur Hammond Pearce Benjamin Tan Ramesh Karri Siddharth Garg\nabstract\rabstract: Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibitapplicability across numerous tasks. One domain of interest is their use inalgorithmic hiring, specifically in matching resumes with job categories. Yet,this introduces issues of bias on protected attributes like gender, race andmaternity status. The seminal work of Bertrand \u0026amp; Mullainathan (2003) set thegold-standard for identifying hiring bias via field experiments where theresponse rate for identical resumes that differ only in protected attributes,e.g., racially suggestive names such as Emily or Lakisha, is compared. Wereplicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude andLlama) to evaluate bias (or lack thereof) on gender, race, maternity status,pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)matching resumes to job categories; and (2) summarizing resumes with employmentrelevant information. Overall, LLMs are robust across race and gender. Theydiffer in their performance on pregnancy status and political affiliation. Weuse contrastive input decoding on open-source LLMs to uncover potential sourcesof bias.\r2023-10-07\nPrompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\nGabriele Tolomei Cesare Campagnano Fabrizio Silvestri Giovanni Trappolini\nabstract\rabstract: In this paper, we present a groundbreaking paradigm for human-computerinteraction that revolutionizes the traditional notion of an operating system. Within this innovative framework, user requests issued to the machine arehandled by an interconnected ecosystem of generative AI models that seamlesslyintegrate with or even replace traditional software applications. At the coreof this paradigm shift are large generative models, such as language anddiffusion models, which serve as the central interface between users andcomputers. This pioneering approach leverages the abilities of advancedlanguage models, empowering users to engage in natural language conversationswith their computing devices. Users can articulate their intentions, tasks, andinquiries directly to the system, eliminating the need for explicit commands orcomplex navigation. The language model comprehends and interprets the user\u0026rsquo;sprompts, generating and displaying contextual and meaningful responses thatfacilitate seamless and intuitive interactions. This paradigm shift not only streamlines user interactions but also opens upnew possibilities for personalized experiences. Generative models can adapt toindividual preferences, learning from user input and continuously improvingtheir understanding and response generation. Furthermore, it enables enhancedaccessibility, as users can interact with the system using speech or text,accommodating diverse communication preferences. However, this visionary concept raises significant challenges, includingprivacy, security, trustability, and the ethical use of generative models.Robust safeguards must be in place to protect user data and prevent potentialmisuse or manipulation of the language model. While the full realization of this paradigm is still far from being achieved,this paper serves as a starting point for envisioning this transformativepotential.\r2023-10-06\nDe-Identification of French Unstructured Clinical Notes for Machine Learning Tasks\nYakini Tchouka Jean-François Couchot Maxime Coulmeau David Laiymani Philippe Selles Azzedine Rahmani\nabstract\rabstract: Unstructured textual data are at the heart of health systems: liaison lettersbetween doctors, operating reports, coding of procedures according to theICD-10 standard, etc. The details included in these documents make it possibleto get to know the patient better, to better manage him or her, to better studythe pathologies, to accurately remunerate the associated medical acts\\ldots Allthis seems to be (at least partially) within reach of today by artificialintelligence techniques. However, for obvious reasons of privacy protection,the designers of these AIs do not have the legal right to access thesedocuments as long as they contain identifying data. De-identifying thesedocuments, i.e. detecting and deleting all identifying information present inthem, is a legally necessary step for sharing this data between twocomplementary worlds. Over the last decade, several proposals have been made tode-identify documents, mainly in English. While the detection scores are oftenhigh, the substitution methods are often not very robust to attack. In French,very few methods are based on arbitrary detection and/or substitution rules. Inthis paper, we propose a new comprehensive de-identification method dedicatedto French-language medical documents. Both the approach for the detection ofidentifying elements (based on deep learning) and their substitution (based ondifferential privacy) are based on the most proven existing approaches. Theresult is an approach that effectively protects the privacy of the patients atthe heart of these medical documents. The whole approach has been evaluated ona French language medical dataset of a French public hospital and the resultsare very encouraging.\rQuantized Transformer Language Model Implementations on Edge Devices\nMohammad Wali Ur Rahman Murad Mehrab Abrar Hunter Gibbons Copening Salim Hariri Sicong Shao Pratik Satam Soheil Salehi\nabstract\rabstract: Large-scale transformer-based models like the Bidirectional EncoderRepresentations from Transformers (BERT) are widely used for Natural LanguageProcessing (NLP) applications, wherein these models are initially pre-trainedwith a large corpus with millions of parameters and then fine-tuned for adownstream NLP task. One of the major limitations of these large-scale modelsis that they cannot be deployed on resource-constrained devices due to theirlarge model size and increased inference latency. In order to overcome theselimitations, such large-scale models can be converted to an optimizedFlatBuffer format, tailored for deployment on resource-constrained edgedevices. Herein, we evaluate the performance of such FlatBuffer transformedMobileBERT models on three different edge devices, fine-tuned for Reputationanalysis of English language tweets in the RepLab 2013 dataset. In addition,this study encompassed an evaluation of the deployed models, wherein theirlatency, performance, and resource efficiency were meticulously assessed. Ourexperiment results show that, compared to the original BERT large model, theconverted and quantized MobileBERT models have 160$\\times$ smaller footprintsfor a 4.1% drop in accuracy while analyzing at least one tweet per second onedge devices. Furthermore, our study highlights the privacy-preserving aspectof TinyML systems as all data is processed locally within a serverlessenvironment.\r2023-10-05\nValidating transformers for redaction of text from electronic health records in real-world healthcare\nZeljko Kraljevic Anthony Shek Joshua Au Yeung Ewart Jonathan Sheldon Mohammad Al-Agil Haris Shuaib Xi Bai Kawsar Noor Anoop D. Shah Richard Dobson James Teo\nabstract\rabstract: Protecting patient privacy in healthcare records is a top priority, andredaction is a commonly used method for obscuring directly identifiableinformation in text. Rule-based methods have been widely used, but theirprecision is often low causing over-redaction of text and frequently not beingadaptable enough for non-standardised or unconventional structures of personalhealth information. Deep learning techniques have emerged as a promisingsolution, but implementing them in real-world environments poses challenges dueto the differences in patient record structure and language across differentdepartments, hospitals, and countries. In this study, we present AnonCAT, a transformer-based model and a blueprinton how deidentification models can be deployed in real-world healthcare.AnonCAT was trained through a process involving manually annotated redactionsof real-world documents from three UK hospitals with different electronichealth record systems and 3116 documents. The model achieved high performancein all three hospitals with a Recall of 0.99, 0.99 and 0.96. Our findings demonstrate the potential of deep learning techniques forimproving the efficiency and accuracy of redaction in global healthcare dataand highlight the importance of building workflows which not just use thesemodels but are also able to continually fine-tune and audit the performance ofthese algorithms to ensure continuing effectiveness in real-world settings.This approach provides a blueprint for the real-world use of de-identifyingalgorithms through fine-tuning and localisation, the code together withtutorials is available on GitHub (https://github.com/CogStack/MedCAT).\r2023-10-04\nMedAlpaca \u0026ndash; An Open-Source Collection of Medical Conversational AI Models and Training Data\nTianyu Han Lisa C. Adams Jens-Michalis Papaioannou Paul Grundmann Tom Oberhauser Alexander Löser Daniel Truhn Keno K. Bressem\nabstract\rabstract: As large language models (LLMs) like OpenAI\u0026rsquo;s GPT series continue to makestrides, we witness the emergence of artificial intelligence applications in anever-expanding range of fields. In medicine, these LLMs hold considerablepromise for improving medical workflows, diagnostics, patient care, andeducation. Yet, there is an urgent need for open-source models that can bedeployed on-premises to safeguard patient privacy. In our work, we present aninnovative dataset consisting of over 160,000 entries, specifically crafted tofine-tune LLMs for effective medical applications. We investigate the impact offine-tuning these datasets on publicly accessible pre-trained LLMs, andsubsequently, we juxtapose the performance of pre-trained-only models againstthe fine-tuned models concerning the examinations that future medical doctorsmust pass to achieve certification.\rDP-SGD for non-decomposable objective functions\nWilliam Kong Andrés Muñoz Medina Mónica Ribero\nabstract\rabstract: Unsupervised pre-training is a common step in developing computer visionmodels and large language models. In this setting, the absence of labelsrequires the use of similarity-based loss functions, such as contrastive loss,that favor minimizing the distance between similar inputs and maximizing thedistance between distinct inputs. As privacy concerns mount, training thesemodels using differential privacy has become more important. However, due tohow inputs are generated for these losses, one of their undesirable propertiesis that their $L_2$ sensitivity can grow with increasing batch size. Thisproperty is particularly disadvantageous for differentially private trainingmethods, such as DP-SGD. To overcome this issue, we develop a new DP-SGDvariant for similarity based loss functions \u0026ndash; in particular the commonly usedcontrastive loss \u0026ndash; that manipulates gradients of the objective function in anovel way to obtain a senstivity of the summed gradient that is $O(1)$ forbatch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, ourmethod\u0026rsquo;s performance comes close to that of a non-private model and generallyoutperforms DP-SGD applied directly to the contrastive loss.\rBridging the Gap Between Foundation Models and Heterogeneous Federated Learning\nSixing Yu J. Pablo Muñoz Ali Jannesari\nabstract\rabstract: Federated learning (FL) offers privacy-preserving decentralized machinelearning, optimizing models at edge clients without sharing private data.Simultaneously, foundation models (FMs) have gained traction in the artificialintelligence (AI) community due to their exceptional performance across varioustasks. However, integrating FMs into FL presents challenges, primarily due totheir substantial size and intensive resource requirements. This is especiallytrue when considering the resource heterogeneity in edge FL systems. We presentan adaptive framework for Resource-aware Federated Foundation Models (RaFFM) toaddress these challenges. RaFFM introduces specialized model compressionalgorithms tailored for FL scenarios, such as salient parameter prioritizationand high-performance subnetwork extraction. These algorithms enable dynamicscaling of given transformer-based FMs to fit heterogeneous resourceconstraints at the network edge during both FL\u0026rsquo;s optimization and deploymentstages. Experimental results demonstrate that RaFFM shows significantsuperiority in resource utilization efficiency and uses fewer resources todeploy FMs to FL. Despite the lower resource consumption, target modelsoptimized by RaFFM achieve performance on par with traditional FL methodsapplied to full-sized FMs. This is evident across tasks in both naturallanguage processing and computer vision domains.\rWho\u0026rsquo;s Harry Potter? Approximate Unlearning in LLMs\nRonen Eldan Mark Russinovich\nabstract\rabstract: Large language models (LLMs) are trained on massive internet corpora thatoften contain copyrighted content. This poses legal and ethical challenges forthe developers and users of these models, as well as the original authors andpublishers. In this paper, we propose a novel technique for unlearning a subsetof the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter booksfrom the Llama2-7b model (a generative language model recently open-sourced byMeta). While the model took over 184K GPU-hours to pretrain, we show that inabout 1 GPU hour of finetuning, we effectively erase the model\u0026rsquo;s ability togenerate or recall Harry Potter-related content, while its performance oncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remainsalmost unaffected. We make our fine-tuned model publicly available onHuggingFace for community evaluation. To the best of our knowledge, this is thefirst paper to present an effective technique for unlearning in generativelanguage models. Our technique consists of three main components: First, we use a reinforcedmodel that is further trained on the target data to identify the tokens thatare most related to the unlearning target, by comparing its logits with thoseof a baseline model. Second, we replace idiosyncratic expressions in the targetdata with generic counterparts, and leverage the model\u0026rsquo;s own predictions togenerate alternative labels for every token. These labels aim to approximatethe next-token predictions of a model that has not been trained on the targetdata. Third, we finetune the model on these alternative labels, whicheffectively erases the original text from the model\u0026rsquo;s memory whenever it isprompted with its context.\r2023-10-03\nLarge Language Models Can Be Good Privacy Protection Learners\nYijia Xiao Yiqiao Jin Yushi Bai Yue Wu Xianjun Yang Xiao Luo Wenchao Yu Xujiang Zhao Yanchi Liu Haifeng Chen Wei Wang Wei Cheng\nabstract\rabstract: The proliferation of Large Language Models (LLMs) has driven considerableinterest in fine-tuning them with domain-specific data to create specializedlanguage models. Nevertheless, such domain-specific fine-tuning data oftencontains sensitive personally identifiable information (PII). Directfine-tuning LLMs on this data without privacy protection poses a risk ofleakage. To address this challenge, we introduce Privacy Protection LanguageModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injectsdomain-specific knowledge while safeguarding data privacy. Our work offers atheoretical analysis for model design and delves into various techniques suchas corpus curation, penalty-based unlikelihood in training loss, andinstruction-based tuning, etc. Extensive experiments across diverse datasetsand scenarios demonstrate the effectiveness of our approaches. In particular,instruction tuning with both positive and negative examples, stands out as apromising method, effectively protecting private data while enhancing themodel\u0026rsquo;s knowledge. Our work underscores the potential for Large Language Modelsas robust privacy protection learners.\rCan Large Language Models Provide Security \u0026amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions\nYufan Chen Arjun Arunasalam Z. Berkay Celik\nabstract\rabstract: Users seek security \u0026amp; privacy (S\u0026amp;P) advice from online resources, includingtrusted websites and content-sharing platforms. These resources help usersunderstand S\u0026amp;P technologies and tools and suggest actionable strategies. LargeLanguage Models (LLMs) have recently emerged as trusted information sources.However, their accuracy and correctness have been called into question. Priorresearch has outlined the shortcomings of LLMs in answering multiple-choicequestions and user ability to inadvertently circumvent model restrictions(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliableS\u0026amp;P advice is not well-explored. In this paper, we measure their ability torefute popular S\u0026amp;P misconceptions that the general public holds. We first studyrecent academic literature to curate a dataset of over a hundred S\u0026amp;P-relatedmisconceptions across six different topics. We then query two popular LLMs(Bard and ChatGPT) and develop a labeling guide to evaluate their responses tothese misconceptions. To comprehensively evaluate their responses, we furtherapply three strategies: query each misconception multiple times, generate andquery their paraphrases, and solicit source URLs of the responses. Both modelsdemonstrate, on average, a 21.3% non-negligible error rate, incorrectlysupporting popular S\u0026amp;P misconceptions. The error rate increases to 32.6% whenwe repeatedly query LLMs with the same or paraphrased misconceptions. We alsoexpose that models may partially support a misconception or remainnoncommittal, refusing a firm stance on misconceptions. Our exploration ofinformation sources for responses revealed that LLMs are susceptible toproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point tounrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).\rFT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models\nYingqian Cui Jie Ren Yuping Lin Han Xu Pengfei He Yue Xing Wenqi Fan Hui Liu Jiliang Tang\nabstract\rabstract: Text-to-image generative models based on latent diffusion models (LDM) havedemonstrated their outstanding ability in generating high-quality andhigh-resolution images according to language prompt. Based on these powerfullatent diffusion models, various fine-tuning methods have been proposed toachieve the personalization of text-to-image diffusion models such as artisticstyle adaptation and human face transfer. However, the unauthorized usage ofdata for model personalization has emerged as a prevalent concern in relationto copyright violations. For example, a malicious user may use the fine-tuningtechnique to generate images which mimic the style of a painter without his/herpermission. In light of this concern, we have proposed FT-Shield, awatermarking approach specifically designed for the fine-tuning oftext-to-image diffusion models to aid in detecting instances of infringement.We develop a novel algorithm for the generation of the watermark to ensure thatthe watermark on the training images can be quickly and accurately transferredto the generated images of text-to-image diffusion models. A watermark will bedetected on an image by a binary watermark detector if the image is generatedby a model that has been fine-tuned using the protected watermarked images.Comprehensive experiments were conducted to validate the effectiveness ofFT-Shield.\rAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\nJi Lin Jiaming Tang Haotian Tang Shang Yang Xingyu Dang Chuang Gan Song Han\nabstract\rabstract: Large language models (LLMs) have shown excellent performance on varioustasks, but the astronomical model size raises the hardware barrier for serving(memory size) and slows down token generation (memory bandwidth). In thispaper, we propose Activation-aware Weight Quantization (AWQ), ahardware-friendly approach for LLM low-bit weight-only quantization. Our methodis based on the observation that weights are not equally important: protectingonly 1% of salient weights can greatly reduce quantization error. We thenpropose to search for the optimal per-channel scaling that protects the salientweights by observing the activation, not weights. AWQ does not rely on anybackpropagation or reconstruction, so it can well preserve LLMs\u0026rsquo; generalizationability on different domains and modalities, without overfitting to thecalibration set. AWQ outperforms existing work on various language modeling anddomain-specific benchmarks. Thanks to better generalization, it achievesexcellent quantization performance for instruction-tuned LMs and, for the firsttime, multi-modal LMs. Alongside AWQ, we implement an efficient and flexibleinference framework tailored for LLMs on the edge, offering more than 3xspeedup over the Huggingface FP16 implementation on both desktop and mobileGPUs. It also democratizes the deployment of the 70B Llama-2 model on mobileGPU (NVIDIA Jetson Orin 64GB).\rCan Language Models be Instructed to Protect Personal Information?\nYang Chen Ethan Mendes Sauvik Das Wei Xu Alan Ritter\nabstract\rabstract: Large multimodal language models have proven transformative in numerousapplications. However, these models have been shown to memorize and leakpre-training data, raising serious user privacy and information securityconcerns. While data leaks should be prevented, it is also crucial to examinethe trade-off between the privacy protection and model utility of proposedapproaches. In this paper, we introduce PrivQA \u0026ndash; a multimodal benchmark toassess this privacy/utility trade-off when a model is instructed to protectspecific categories of personal information in a simulated scenario. We alsopropose a technique to iteratively self-moderate responses, which significantlyimproves privacy. However, through a series of red-teaming experiments, we findthat adversaries can also easily circumvent these protections with simplejailbreaking methods through textual and/or image inputs. We believe PrivQA hasthe potential to support the development of new models with improved privacyprotections, as well as the adversarial robustness of these protections. Werelease the entire PrivQA dataset at https://llm-access-control.github.io/.\rFine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?\nJingyuan Sun Marie-Francine Moens\nabstract\rabstract: To decipher the algorithm underlying the human brain\u0026rsquo;s languagerepresentation, previous work probed brain responses to language input withpre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.However, full fine-tuning generally updates the entire parametric space anddistorts pre-trained features, cognitively inconsistent with the brain\u0026rsquo;s robustmulti-task learning ability. Prompt-tuning, in contrast, protects pre-trainedweights and learns task-specific embeddings to fit a task. Could prompt-tuninggenerate representations that better account for the brain\u0026rsquo;s languagerepresentations than fine-tuning? If so, what kind of NLU task leads apre-trained model to better decode the information represented in the humanbrain? We investigate these questions by comparing prompt-tuned and fine-tunedrepresentations in neural decoding, that is predicting the linguistic stimulusfrom the brain activities evoked by the stimulus. We find that on none of the10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning inneural decoding, implicating that a more brain-consistent tuning method yieldsrepresentations that better correlate with brain data. Moreover, we identifythat tasks dealing with fine-grained concept meaning yield representations thatbetter decode brain activation patterns than other tasks, especially thesyntactic chunking task. This indicates that our brain encodes morefine-grained concept information than shallow syntactic information whenrepresenting languages.\r2023-10-02\nBTR: Binary Token Representations for Efficient Retrieval Augmented Language Models\nQingqing Cao Sewon Min Yizhong Wang Hannaneh Hajishirzi\nabstract\rabstract: Retrieval augmentation addresses many critical problems in large languagemodels such as hallucination, staleness, and privacy leaks. However, runningretrieval-augmented language models (LMs) is slow and difficult to scale due toprocessing large amounts of retrieved text. We introduce binary tokenrepresentations (BTR), which use 1-bit vectors to precompute every token inpassages, significantly reducing computation during inference. Despite thepotential loss of accuracy, our new calibration techniques and trainingobjectives restore performance. Combined with offline and runtime compression,this only requires 127GB of disk space for encoding 3 billion tokens inWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTRaccelerates state-of-the-art inference by up to 4x and reduces storage by over100x while maintaining over 95% task performance.\rFedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models\nJingwei Sun Ziyue Xu Hongxu Yin Dong Yang Daguang Xu Yiran Chen Holger R. Roth\nabstract\rabstract: Pre-trained language models (PLM) have revolutionized the NLP landscape,achieving stellar performances across diverse tasks. These models, whilebenefiting from vast training data, often require fine-tuning on specific datato cater to distinct downstream tasks. However, this data adaptation processhas inherent security and privacy concerns, primarily when leveraginguser-generated, device-residing data. Federated learning (FL) provides asolution, allowing collaborative model fine-tuning without centralized datacollection. However, applying FL to finetune PLMs is hampered by challenges,including restricted model parameter access, high computational requirements,and communication overheads. This paper introduces Federated Black-box PromptTuning (FedBPT), a framework designed to address these challenges. FedBPT doesnot require the clients to access the model parameters. By focusing on trainingoptimal prompts and utilizing gradient-free optimization methods, FedBPTreduces the number of exchanged variables, boosts communication efficiency, andminimizes computational and storage costs. Experiments highlight theframework\u0026rsquo;s ability to drastically cut communication and memory costs whilemaintaining competitive performance. Ultimately, FedBPT presents a promisingsolution for efficient, privacy-preserving fine-tuning of PLM in the age oflarge language models.\rCoupling public and private gradient provably helps optimization\nRuixuan Liu Zhiqi Bu Yu-xiang Wang Sheng Zha George Karypis\nabstract\rabstract: The success of large neural networks is crucially determined by theavailability of data. It has been observed that training only on a small amountof public data, or privately on the abundant private data can lead toundesirable degradation of accuracy. In this work, we leverage both private andpublic data to improve the optimization, by coupling their gradients via aweighted linear combination. We formulate an optimal solution for the optimalweight in the convex setting to indicate that the weighting coefficient shouldbe hyperparameter-dependent. Then, we prove the acceleration in the convergenceof non-convex loss and the effects of hyper-parameters such as privacy budget,number of iterations, batch size, and model size on the choice of the weightingcoefficient. We support our analysis with empirical experiments across languageand vision benchmarks, and provide a guideline for choosing the optimal weightof the gradient coupling.\rGotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models\nZhou Yang Zhipeng Zhao Chenyu Wang Jieke Shi Dongsum Kim Donggyun Han David Lo\nabstract\rabstract: Given large-scale source code datasets available in open-source projects andadvanced large language models, recent code models have been proposed toaddress a series of critical software engineering tasks, such as program repairand code completion. The training data of the code models come from varioussources, not only the publicly available source code, e.g., open-sourceprojects on GitHub but also the private data such as the confidential sourcecode from companies, which may contain sensitive information (for example, SSHkeys and personal information). As a result, the use of these code models mayraise new privacy concerns. In this paper, we focus on a critical yet not well-explored question on usingcode models: what is the risk of membership information leakage in code models?Membership information leakage refers to the risk that an attacker can inferwhether a given data point is included in (i.e., a member of) the trainingdata. To answer this question, we propose Gotcha, a novel membership inferenceattack method specifically for code models. We investigate the membershipleakage risk of code models. Our results reveal a worrying fact that the riskof membership leakage is high: although the previous attack methods are closeto random guessing, Gotcha can predict the data membership with a high truepositive rate of 0.95 and a low false positive rate of 0.10. We also show thatthe attacker\u0026rsquo;s knowledge of the victim model (e.g., the model architecture andthe pre-training data) impacts the success rate of attacks. Further analysisdemonstrates that changing the decoding strategy can mitigate the risk ofmembership leakage. This study calls for more attention to understanding theprivacy of code models and developing more effective countermeasures againstsuch attacks.\r2023-09-30\nPrivacy-Preserving In-Context Learning for Large Language Models\nTong Wu Ashwinee Panda Jiachen T. Wang Prateek Mittal\nabstract\rabstract: In-context learning (ICL) is an important capability of Large Language Models(LLMs), enabling these models to dynamically adapt based on specific,in-context exemplars, thereby improving accuracy and relevance. However, LLM\u0026rsquo;sresponses may leak the sensitive private information contained in in-contextexemplars. To address this challenge, we propose Differentially PrivateIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. Thekey idea for DP-ICL paradigm is generating differentially private responsesthrough a noisy consensus among an ensemble of LLM\u0026rsquo;s responses based ondisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiateseveral techniques showing how to privatize ICL for text classification andlanguage generation. We evaluate DP-ICL on four text classification benchmarksand two language generation tasks, and our empirical results show that DP-ICLachieves a strong utility-privacy tradeoff.\rInvestigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting\nBaphumelele Masikisiki Vukosi Marivate Yvette Hlope\nabstract\rabstract: Large Language Models, such as Generative Pre-trained Transformer 3 (aka.GPT-3), have been developed to understand language through the analysis ofextensive text data, allowing them to identify patterns and connections betweenwords. While LLMs have demonstrated impressive performance across varioustext-related tasks, they encounter challenges in tasks associated withreasoning. To address this challenge, Chain of Thought(CoT) prompting methodhas been proposed as a means to enhance LLMs\u0026rsquo; proficiency in complex reasoningtasks like solving math word problems and answering questions based on logicalargumentative reasoning. The primary aim of this research is to assess how wellfour language models can grade reflective essays of third-year medicalstudents. The assessment will specifically target the evaluation of criticalthinking skills using CoT prompting. The research will provide the following contributions; to introduce andeducate on the process of instructing models to evaluate reflective essays froma dataset they have not been previously trained on; to illustrate the use ofCoT prompting as an instructional approach for training large models to carryout particular tasks. Our results suggest that among all the models, Llama-7bperforms the least effectively, displaying the highest mean squared error.Conversely, ChatGPT emerges as the superior model, boasting a higher Cohenkappa score value of 0.53. Lastly, it\u0026rsquo;s important to note that the selectedmodels do prioritise user privacy by allowing users to delete their ownconducted conversations.\r2023-09-29\nCan Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks\nVaidehi Patil Peter Hase Mohit Bansal\nabstract\rabstract: Pretrained language models sometimes possess knowledge that we do not wishthem to, including memorized personal information and knowledge that could beused to harm people. They can also output toxic or harmful text. To mitigatethese safety and informational issues, we propose an attack-and-defenseframework for studying the task of deleting sensitive information directly frommodel weights. We study direct edits to model weights because (1) this approachshould guarantee that particular deleted information is never extracted byfuture prompt attacks, and (2) it should protect against whitebox attacks,which is necessary for making claims about safety/privacy in a setting wherepublicly available model weights could be used to elicit sensitive information.Our threat model assumes that an attack succeeds if the answer to a sensitivequestion is located among a set of B generated candidates, based on scenarioswhere the information would be insecure if the answer is among B candidates.Experimentally, we show that even state-of-the-art model editing methods suchas ROME struggle to truly delete factual information from models like GPT-J, asour whitebox and blackbox attacks can recover \u0026ldquo;deleted\u0026rdquo; information from anedited model 38% of the time. These attacks leverage two key observations: (1)that traces of deleted information can be found in intermediate model hiddenstates, and (2) that applying an editing method for one question may not deleteinformation across rephrased versions of the question. Finally, we provide newdefense methods that protect against some extraction attacks, but we do notfind a single universally effective defense method. Our results suggest thattruly deleting sensitive information is a tractable but difficult problem,since even relatively low attack success rates have potentially severe societalimplications for real-world deployment of language models.\rRevolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\nSamuel Carreira Tomás Marques José Ribeiro Carlos Grilo\nabstract\rabstract: The field of Artificial Intelligence has witnessed remarkable progress inrecent years, especially with the emergence of powerful large language models(LLMs) based on the transformer architecture. Cloud-based LLMs, such asOpenAI\u0026rsquo;s ChatGPT, offer impressive capabilities but come with concernsregarding latency and privacy due to network dependencies. This articlepresents an innovative approach to LLM inference, envisioning a future whereLLMs with billions of parameters can be executed directly on mobile deviceswithout network connectivity. The article showcases a fine-tuned GPT LLM with 3billion parameters that can operate smoothly on devices with as low as 4GB ofmemory. Through the integration of native code and model quantizationtechniques, the application not only serves as a general-purpose assistant butalso facilitates seamless mobile interactions with text-to-actions features.The article provides insights into the training pipeline, implementationdetails, test results, and future directions of on-device LLM inference. Thisbreakthrough technology opens up possibilities for empowering users withsophisticated AI capabilities while preserving their privacy and eliminatinglatency concerns.\rMedical Foundation Models are Susceptible to Targeted Misinformation Attacks\nTianyu Han Sven Nebelung Firas Khader Tianci Wang Gustav Mueller-Franzes Christiane Kuhl Sebastian Försch Jens Kleesiek Christoph Haarburger Keno K. Bressem Jakob Nikolas Kather Daniel Truhn\nabstract\rabstract: Large language models (LLMs) have broad medical knowledge and can reasonabout medical information across many domains, holding promising potential fordiverse medical applications in the near future. In this study, we demonstratea concerning vulnerability of LLMs in medicine. Through targeted manipulationof just 1.1% of the model\u0026rsquo;s weights, we can deliberately inject an incorrectbiomedical fact. The erroneous information is then propagated in the model\u0026rsquo;soutput, whilst its performance on other biomedical tasks remains intact. Wevalidate our findings in a set of 1,038 incorrect biomedical facts. Thispeculiar susceptibility raises serious security and trustworthiness concernsfor the application of LLMs in healthcare settings. It accentuates the need forrobust protective measures, thorough verification mechanisms, and stringentmanagement of access to these models, ensuring their reliable and safe use inmedical practice.\r2023-09-28\nForgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble\nZhe Liu Ozlem Kalinli\nabstract\rabstract: Recent research has shown that language models have a tendency to memorizerare or unique token sequences in the training corpus. After deploying a model,practitioners might be asked to delete any personal information from the modelby individuals\u0026rsquo; requests. Re-training the underlying model every timeindividuals would like to practice their rights to be forgotten iscomputationally expensive. We employ a teacher-student framework and propose anovel leave-one-out ensemble method to unlearn the targeted textual sequencesthat need to be forgotten from the model. In our approach, multiple teachersare trained on disjoint sets; for each targeted sequence to be removed, weexclude the teacher trained on the set containing this sequence and aggregatethe predictions from remaining teachers to provide supervision duringfine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that theproposed method achieves superior privacy-utility trade-offs than othercounterparts.\r2023-09-27\nIdentifying and Mitigating Privacy Risks Stemming from Language Models: A Survey\nVictoria Smith Ali Shahin Shamsabadi Carolyn Ashurst Adrian Weller\nabstract\rabstract: Rapid advancements in language models (LMs) have led to their adoption acrossmany sectors. Alongside the potential benefits, such models present a range ofrisks, including around privacy. In particular, as LMs have grown in size, thepotential to memorise aspects of their training data has increased, resultingin the risk of leaking private information. As LMs become increasinglywidespread, it is vital that we understand such privacy risks and how theymight be mitigated. To help researchers and policymakers understand the stateof knowledge around privacy attacks and mitigations, including where more workis needed, we present the first technical survey on LM privacy. We (i) identifya taxonomy of salient dimensions where attacks differ on LMs, (ii) surveyexisting attacks and use our taxonomy of dimensions to highlight key trends,(iii) discuss existing mitigation strategies, highlighting their strengths andlimitations, identifying key gaps and demonstrating open problems and areas forconcern.\r2023-09-26\nPLMM: Personal Large Models on Mobile Devices\nYuanhao Gong\nabstract\rabstract: Inspired by Federated Learning, in this paper, we propose personal largemodels that are distilled from traditional large language models but moreadaptive to local users\u0026rsquo; personal information such as education background andhobbies. We classify the large language models into three levels: the personallevel, expert level and traditional level. The personal level models areadaptive to users\u0026rsquo; personal information. They encrypt the users\u0026rsquo; input andprotect their privacy. The expert level models focus on merging specificknowledge such as finance, IT and art. The traditional models focus on theuniversal knowledge discovery and upgrading the expert models. In suchclassifications, the personal models directly interact with the user. For thewhole system, the personal models have users\u0026rsquo; (encrypted) personal information.Moreover, such models must be small enough to be performed on personalcomputers or mobile devices. Finally, they also have to response in real-timefor better user experience and produce high quality results. The proposedpersonal large models can be applied in a wide range of applications such aslanguage and vision tasks.\r2023-09-25\nAn Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox\nChaoran Chen Weijun Li Wenxin Song Yanfang Ye Yaxing Yao Toby Jia-jun Li\nabstract\rabstract: The \u0026ldquo;privacy paradox\u0026rdquo; describes the discrepancy between users\u0026rsquo; privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users\u0026rsquo; hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes.\r2023-09-22\nRight to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions\nDawen Zhang Pamela Finckenberg-Broman Thong Hoang Shidong Pan Zhenchang Xing Mark Staples Xiwei Xu\nabstract\rabstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.\r2023-09-21\nMarkNerf:Watermarking for Neural Radiance Field\nLifeng Chen Jia Liu Yan Ke Wenquan Sun Weina Dong Xiaozhong Pan\nabstract\rabstract: A watermarking algorithm is proposed in this paper to address the copyrightprotection issue of implicit 3D models. The algorithm involves embeddingwatermarks into the images in the training set through an embedding network,and subsequently utilizing the NeRF model for 3D modeling. A copyright verifieris employed to generate a backdoor image by providing a secret perspective asinput to the neural radiation field. Subsequently, a watermark extractor isdevised using the hyperparameterization method of the neural network to extractthe embedded watermark image from that perspective. In a black box scenario, ifthere is a suspicion that the 3D model has been used without authorization, theverifier can extract watermarks from a secret perspective to verify networkcopyright. Experimental results demonstrate that the proposed algorithmeffectively safeguards the copyright of 3D models. Furthermore, the extractedwatermarks exhibit favorable visual effects and demonstrate robust resistanceagainst various types of noise attacks.\r2023-09-20\n\u0026ldquo;It\u0026rsquo;s a Fair Game\u0026rsquo;\u0026rsquo;, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents\nZhiping Zhang Michelle Jia Hao-Ping Lee Bingsheng Yao Sauvik Das Ada Lerner Dakuo Wang Tianshi Li\nabstract\rabstract: The widespread use of Large Language Model (LLM)-based conversational agents(CAs), especially in high-stakes domains, raises many privacy concerns.Building ethical LLM-based CAs that respect user privacy requires an in-depthunderstanding of the privacy risks that concern users the most. However,existing research, primarily model-centered, does not provide insight intousers\u0026rsquo; perspectives. To bridge this gap, we analyzed sensitive disclosures inreal-world ChatGPT conversations and conducted semi-structured interviews with19 LLM-based CA users. We found that users are constantly faced with trade-offsbetween privacy, utility, and convenience when using LLM-based CAs. However,users\u0026rsquo; erroneous mental models and the dark patterns in system design limitedtheir awareness and comprehension of the privacy risks. Additionally, thehuman-like interactions encouraged more sensitive disclosures, whichcomplicated users\u0026rsquo; ability to navigate the trade-offs. We discuss practicaldesign guidelines and the needs for paradigmatic shifts to protect the privacyof LLM-based CA users.\rPolicy Patterns for Usage Control in Data Spaces\nTobias Dam Andreas Krimbacher Sebastian Neumaier\nabstract\rabstract: Data-driven technologies have the potential to initiate a transportationrelated revolution in the way we travel, commute and navigate within cities. Asa major effort of this transformation relies on Mobility Data Spaces for theexchange of mobility data, the necessity to protect valuable data and formulateconditions for data exchange arises. This paper presents key contributions tothe development of automated contract negotiation and data usage policies inthe Mobility Data Space. A comprehensive listing of policy patterns for usagecontrol is provided, addressing common requirements and scenarios in datasharing and governance. The use of the Open Digital Rights Language (ODRL) isproposed to formalize the collected policies, along with an extension of theODRL vocabulary for data space-specific properties.\r2023-09-19\nSpecializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training\nRuiqi Xu Yongfeng Huang Xin Chen Lin Zhang\nabstract\rabstract: In this work, we introduce the concept of complex text style transfer tasks,and constructed complex text datasets based on two widely applicable scenarios.Our dataset is the first large-scale data set of its kind, with 700 rephrasedsentences and 1,000 sentences from the game Genshin Impact. While largelanguage models (LLM) have shown promise in complex text style transfer, theyhave drawbacks such as data privacy concerns, network instability, and highdeployment costs. To address these issues, we explore the effectiveness ofsmall models (less than T5-3B) with implicit style pre-training throughcontrastive learning. We also propose a method for automated evaluation of textgeneration quality based on alignment with human evaluations using ChatGPT.Finally, we compare our approach with existing methods and show that our modelachieves state-of-art performances of few-shot text style transfer models.\rFacilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation\nHuachuan Qiu Shuai Zhang Hongliang He Anqi Li Zhenzhong Lan\nabstract\rabstract: NSFW (Not Safe for Work) content, in the context of a dialogue, can havesevere side effects on users in open-domain dialogue systems. However, researchon detecting NSFW language, especially sexually explicit content, within adialogue context has significantly lagged behind. To address this issue, weintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialoguedetection. Leveraging knowledge distillation techniques involving GPT-4 andChatGPT, this dataset offers a cost-effective means of constructing NSFWcontent detectors. The process entails collecting real-life human-machineinteraction data and breaking it down into single utterances and single-turndialogues, with the chatbot delivering the final utterance. ChatGPT is employedto annotate unlabeled data, serving as a training set. Rationale validation andtest sets are constructed using ChatGPT and GPT-4 as annotators, with aself-criticism strategy for resolving discrepancies in labeling. A BERT modelis fine-tuned as a text classifier on pseudo-labeled data, and its performanceis assessed. The study emphasizes the importance of AI systems prioritizinguser safety and well-being in digital conversations while respecting freedom ofexpression. The proposed approach not only advances NSFW content detection butalso aligns with evolving user protection needs in AI-driven dialogues.\rDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass\nMinxin Du Xiang Yue Sherman S. M. Chow Tianhao Wang Chenyu Huang Huan Sun\nabstract\rabstract: Differentially private stochastic gradient descent (DP-SGD) adds noise togradients in back-propagation, safeguarding training data from privacy leakage,particularly membership inference. It fails to cover (inference-time) threatslike embedding inversion and sensitive attribute inference. It is also costlyin storage and computation when used to fine-tune large pre-trained languagemodels (LMs). We propose DP-Forward, which directly perturbs embedding matrices in theforward pass of LMs. It satisfies stringent local DP requirements for trainingand inference data. To instantiate it using the smallest matrix-valued noise,we devise an analytic matrix Gaussian~mechanism (aMGM) by drawing possiblynon-i.i.d. noise from a matrix Gaussian distribution. We then investigateperturbing outputs from different hidden (sub-)layers of LMs with aMGM noises.Its utility on three typical tasks almost hits the non-private baseline andoutperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves3$\\times$ time and memory costs compared to DP-SGD with the latest high-speedlibrary. It also reduces the average success rates of embedding inversion andsensitive attribute inference by up to 88pp and 41pp, respectively, whereasDP-SGD fails.\rLLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI\u0026rsquo;s ChatGPT Plugins\nUmar Iqbal Tadayoshi Kohno Franziska Roesner\nabstract\rabstract: Large language model (LLM) platforms, such as ChatGPT, have recently begunoffering a plugin ecosystem to interface with third-party services on theinternet. While these plugins extend the capabilities of LLM platforms, theyare developed by arbitrary third parties and thus cannot be implicitly trusted.Plugins also interface with LLM platforms and users using natural language,which can have imprecise interpretations. In this paper, we propose a frameworkthat lays a foundation for LLM platform designers to analyze and improve thesecurity, privacy, and safety of current and future plugin-integrated LLMplatforms. Our framework is a formulation of an attack taxonomy that isdeveloped by iteratively exploring how LLM platform stakeholders could leveragetheir capabilities and responsibilities to mount attacks against each other. Aspart of our iterative process, we apply our framework in the context ofOpenAI\u0026rsquo;s plugin ecosystem. We uncover plugins that concretely demonstrate thepotential for the types of issues that we outline in our attack taxonomy. Weconclude by discussing novel challenges and by providing recommendations toimprove the security, privacy, and safety of present and future LLM-basedcomputing platforms.\rDifferentially Private Optimization on Large Model at Small Cost\nZhiqi Bu Yu-Xiang Wang Sheng Zha George Karypis\nabstract\rabstract: Differentially private (DP) optimization is the standard paradigm to learnlarge neural networks that are accurate and privacy-preserving. Thecomputational cost for DP deep learning, however, is notoriously heavy due tothe per-sample gradient clipping. Existing DP implementations are 2-1000X morecostly in time and space complexity than the standard (non-private) training.In this work, we develop a novel Book-Keeping (BK) technique that implementsexisting DP optimizers (thus achieving the same accuracy), with a substantialimprovement on the computational cost. Specifically, BK enables DP training onlarge models and high dimensional data to be roughly as fast and memory-savingas the standard training, whereas previous DP algorithms can be inefficient orincapable of training due to memory error. The computational advantage of BK issupported by the complexity analysis as well as extensive experiments on visionand language tasks. Our implementation achieves state-of-the-art (SOTA)accuracy with very small extra cost: on GPT2 and at almost the same memory cost(\u0026lt;1% overhead), BK has 1.03X the time complexity of the standard training(0.83X training speed in practice), and 0.61X the time complexity of the mostefficient DP implementation (1.36X training speed in practice). We open-sourcethe codebase for the BK algorithm at the FastDP library(https://github.com/awslabs/fast-differential-privacy).\rPolicyGPT: Automated Analysis of Privacy Policies with Large Language Models\nChenhao Tang Zhengliang Liu Chong Ma Zihao Wu Yiwei Li Wei Liu Dajiang Zhu Quanzheng Li Xiang Li Tianming Liu Lei Fan\nabstract\rabstract: Privacy policies serve as the primary conduit through which online serviceproviders inform users about their data collection and usage procedures.However, in a bid to be comprehensive and mitigate legal risks, these policydocuments are often quite verbose. In practical use, users tend to click theAgree button directly rather than reading them carefully. This practice exposesusers to risks of privacy leakage and legal issues. Recently, the advent ofLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened newpossibilities for text analysis, especially for lengthy documents like privacypolicies. In this study, we investigate a privacy policy text analysisframework PolicyGPT based on the LLM. This framework was tested using twodatasets. The first dataset comprises of privacy policies from 115 websites,which were meticulously annotated by legal experts, categorizing each segmentinto one of 10 classes. The second dataset consists of privacy policies from304 popular mobile applications, with each sentence manually annotated andclassified into one of another 10 categories. Under zero-shot learningconditions, PolicyGPT demonstrated robust performance. For the first dataset,it achieved an accuracy rate of 97%, while for the second dataset, it attainedan 87% accuracy rate, surpassing that of the baseline machine learning andneural network models.\r2023-09-18\nSpecification-Driven Video Search via Foundation Models and Formal Verification\nYunhao Yang Jean-Raphaël Gaglione Sandeep Chinchali Ufuk Topcu\nabstract\rabstract: The increasing abundance of video data enables users to search for events ofinterest, e.g., emergency incidents. Meanwhile, it raises new concerns, such asthe need for preserving privacy. Existing approaches to video search requireeither manual inspection or a deep learning model with massive training. Wedevelop a method that uses recent advances in vision and language models, aswell as formal methods, to search for events of interest in video clipsautomatically and efficiently. The method consists of an algorithm to maptext-based event descriptions into linear temporal logic over finite traces(LTL$_f$) and an algorithm to construct an automaton encoding the videoinformation. Then, the method formally verifies the automaton representing thevideo against the LTL$_f$ specifications and adds the pertinent video clips tothe search result if the automaton satisfies the specifications. We providequalitative and quantitative analysis to demonstrate the video-searchingcapability of the proposed method. It achieves over 90 percent precision insearching over privacy-sensitive videos and a state-of-the-art autonomousdriving dataset.\rInstruction-Following Speech Recognition\nCheng-I Jeff Lai Zhiyun Lu Liangliang Cao Ruoming Pang\nabstract\rabstract: Conventional end-to-end Automatic Speech Recognition (ASR) models primarilyfocus on exact transcription tasks, lacking flexibility for nuanced userinteractions. With the advent of Large Language Models (LLMs) in speechprocessing, more organic, text-prompt-based interactions have become possible.However, the mechanisms behind these models\u0026rsquo; speech understanding and\u0026quot;reasoning\u0026quot; capabilities remain underexplored. To study this question from thedata perspective, we introduce instruction-following speech recognition,training a Listen-Attend-Spell model to understand and execute a diverse set offree-form text instructions. This enables a multitude of speech recognitiontasks \u0026ndash; ranging from transcript manipulation to summarization \u0026ndash; withoutrelying on predefined command sets. Remarkably, our model, trained from scratchon Librispeech, interprets and executes simple instructions without requiringLLMs or pre-trained speech modules. It also offers selective transcriptionoptions based on instructions like \u0026ldquo;transcribe first half and then turn offlistening,\u0026rdquo; providing an additional layer of privacy and safety compared toexisting LLMs. Our findings highlight the significant potential ofinstruction-following training to advance speech foundation models.\rEmpowering Fake-News Mitigation: Insights from Sharers\u0026rsquo; Social Media Post-Histories\nVerena Schoenmueller Simon J. Blanchard Gita V. Johar\nabstract\rabstract: Misinformation is a global concern and limiting its spread is critical forprotecting democracy, public health, and consumers. We propose that consumers\u0026rsquo;own social media post-histories are an underutilized data source to study whatleads them to share links to fake-news. In Study 1, we explore how textual cuesextracted from post-histories distinguish fake-news sharers from random socialmedia users and others in the misinformation ecosystem. Among other results, wefind across two datasets that fake-news sharers use more words related toanger, religion and power. In Study 2, we show that adding textual cues frompost-histories improves the accuracy of models to predict who is likely toshare fake-news. In Study 3, we provide a preliminary test of two mitigationstrategies deduced from Study 1 - activating religious values and reducinganger - and find that they reduce fake-news sharing and sharing more generally.In Study 4, we combine survey responses with users\u0026rsquo; verified Twitterpost-histories and show that using empowering language in a fact-checkingbrowser extension ad increases download intentions. Our research encouragesmarketers, misinformation scholars, and practitioners to use post-histories todevelop theories and test interventions to reduce the spread of misinformation.\r2023-09-17\nYour Room is not Private: Gradient Inversion Attack on Reinforcement Learning\nMiao Li Wenhao Ding Ding Zhao\nabstract\rabstract: The prominence of embodied Artificial Intelligence (AI), which empowersrobots to navigate, perceive, and engage within virtual environments, hasattracted significant attention, owing to the remarkable advancements incomputer vision and large language models. Privacy emerges as a pivotal concernwithin the realm of embodied AI, as the robot accesses substantial personalinformation. However, the issue of privacy leakage in embodied AI tasks,particularly in relation to reinforcement learning algorithms, has not receivedadequate consideration in research. This paper aims to address this gap byproposing an attack on the value-based algorithm and the gradient-basedalgorithm, utilizing gradient inversion to reconstruct states, actions, andsupervision signals. The choice of using gradients for the attack is motivatedby the fact that commonly employed federated learning techniques solely utilizegradients computed based on private user data to optimize models, withoutstoring or transmitting the data to public servers. Nevertheless, thesegradients contain sufficient information to potentially expose private data. Tovalidate our approach, we conduct experiments on the AI2THOR simulator andevaluate our algorithm on active perception, a prevalent task in embodied AI.The experimental results demonstrate the effectiveness of our method insuccessfully reconstructing all information from the data across 120 roomlayouts.\r2023-09-16\nData-Flow-Based Normalization Generation Algorithm of R1CS for Zero-Knowledge Proof\nChenhao Shi Hao Chen Ruibang Liu Guoqiang Li\nabstract\rabstract: The communities of blockchains and distributed ledgers have been stirred upby the introduction of zero-knowledge proofs (ZKPs). Originally designed tosolve privacy issues, ZKPs have now evolved into an effective remedy forscalability concerns and are applied in Zcash (internet money like Bitcoin). Toenable ZKPs, Rank-1 Constraint Systems (R1CS) offer a verifier for bi-linearequations. To accurately and efficiently represent R1CS, several language toolslike Circom, Noir, and Snarky have been proposed to automate the compilation ofadvanced programs into R1CS. However, due to the flexible nature of R1CSrepresentation, there can be significant differences in the compiled R1CS formsgenerated from circuit language programs with the same underlying semantics. Toaddress this issue, this paper uses a data-flow-based R1CS paradigm algorithm,which produces a standardized format for different R1CS instances withidentical semantics. By using the normalized R1CS format circuits, thecomplexity of circuits\u0026rsquo; verification can be reduced. In addition, this paperpresents an R1CS normalization algorithm benchmark, and our experimentalevaluation demonstrates the effectiveness and correctness of our methods.\r2023-09-15\nSystem Fingerprint Recognition for Deepfake Audio: An Initial Dataset and Investigation\nXinrui Yan Jiangyan Yi Chenglong Wang Jianhua Tao Junzuo Zhou Hao Gu Ruibo Fu\nabstract\rabstract: The rapid progress of deep speech synthesis models has posed significantthreats to society such as malicious content manipulation. Therefore, manystudies have emerged to detect the so-called deepfake audio. However, existingworks focus on the binary detection of real audio and fake audio. In real-worldscenarios such as model copyright protection and digital evidence forensics, itis needed to know what tool or model generated the deepfake audio to explainthe decision. This motivates us to ask: Can we recognize the systemfingerprints of deepfake audio? In this paper, we present the first deepfakeaudio dataset for system fingerprint recognition (SFR) and conduct an initialinvestigation. We collected the dataset from the speech synthesis systems ofseven Chinese vendors that use the latest state-of-the-art deep learningtechnologies, including both clean and compressed sets. In addition, tofacilitate the further development of system fingerprint recognition methods,we provide extensive benchmarks that can be compared and research findings. Thedataset will be publicly available. .\r2023-09-14\nDo Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code Completion Tools\nYizhan Huang Yichen Li Weibin Wu Jianping Zhang Michael R. Lyu\nabstract\rabstract: Neural Code Completion Tools (NCCTs) have reshaped the field of softwaredevelopment, which accurately suggest contextually-relevant code snippetsbenefiting from language modeling techniques. However, language models may emitthe training data verbatim during inference with appropriate prompts. Thismemorization property raises privacy concerns of commercial NCCTs about thehard-coded credential leakage, leading to unauthorized access to systems.Therefore, to answer whether NCCTs will inadvertently emit the hard-codedcredential, we propose an evaluation tool called Hard-coded Credential Revealer(HCR). HCR effectively constructs test prompts from GitHub code files withcredentials to trigger memorization phenomenon of commercial NCCTs. Then, HCRextracts credentials with pre-defined format from the responses by fourdesigned filters. We apply HCR to evaluate two representative commercial NCCTs:GitHub Copilot and Amazon CodeWhisperer and successfully extracted 2,702hard-coded credentials from Copilot and 129 secrets from CodeWhisper under theblack-box setting, among which at least 3.6% and 5.4% secrets are real stringsfrom GitHub repositories. Moreover, two operational credentials wereidentified. The experimental results raise the severe privacy concern of thepotential leakage of hard-coded credentials in the training data of commercialNCCTs.\rPreventing Unauthorized AI Over-Analysis by Medical Image Adversarial Watermarking\nXingxing Wei Bangzheng Pu Shiji Zhao Chen Chi Huazhu Fu\nabstract\rabstract: The advancement of deep learning has facilitated the integration ofArtificial Intelligence (AI) into clinical practices, particularly incomputer-aided diagnosis. Given the pivotal role of medical images in variousdiagnostic procedures, it becomes imperative to ensure the responsible andsecure utilization of AI techniques. However, the unauthorized utilization ofAI for image analysis raises significant concerns regarding patient privacy andpotential infringement on the proprietary rights of data custodians.Consequently, the development of pragmatic and cost-effective strategies thatsafeguard patient privacy and uphold medical image copyrights emerges as acritical necessity. In direct response to this pressing demand, we present apioneering solution named Medical Image Adversarial watermarking (MIAD-MARK).Our approach introduces watermarks that strategically mislead unauthorized AIdiagnostic models, inducing erroneous predictions without compromising theintegrity of the visual content. Importantly, our method integrates anauthorization protocol tailored for legitimate users, enabling the removal ofthe MIAD-MARK through encryption-generated keys. Through extensive experiments,we validate the efficacy of MIAD-MARK across three prominent medical imagedatasets. The empirical outcomes demonstrate the substantial impact of ourapproach, notably reducing the accuracy of standard AI diagnostic models to amere 8.57% under white box conditions and 45.83% in the more challenging blackbox scenario. Additionally, our solution effectively mitigates unauthorizedexploitation of medical images even in the presence of sophisticated watermarkremoval networks. Notably, those AI diagnosis networks exhibit a meager averageaccuracy of 38.59% when applied to images protected by MIAD-MARK, underscoringthe robustness of our safeguarding mechanism.\r2023-09-13\neDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models\nMinsik Cho Keivan A. Vahid Qichen Fu Saurabh Adya Carlo C Del Mundo Mohammad Rastegari Devang Naik Peter Zatloukal\nabstract\rabstract: Since Large Language Models or LLMs have demonstrated high-qualityperformance on many complex language tasks, there is a great interest inbringing these LLMs to mobile devices for faster responses and better privacyprotection. However, the size of LLMs (i.e., billions of parameters) requireshighly effective compression to fit into storage-limited devices. Among manycompression techniques, weight-clustering, a form of non-linear quantization,is one of the leading candidates for LLM compression, and supported by modernsmartphones. Yet, its training overhead is prohibitively significant for LLMfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shownthe state-of-the-art trade-off between compression ratio and accuracyregression, but its large memory complexity makes it nearly impossible to applyto train-time LLM compression. In this paper, we propose a memory-efficient DKMimplementation, eDKM powered by novel techniques to reduce the memory footprintof DKM by orders of magnitudes. For a given tensor to be saved on CPU for thebackward pass of DKM, we compressed the tensor by applying uniquification andsharding after checking if there is no duplicated tensor previously copied toCPU. Our experimental results demonstrate that \\prjname can fine-tune andcompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) withthe Alpaca dataset by reducing the train-time memory footprint of a decoderlayer by 130$\\times$, while delivering good accuracy on broader LLM benchmarks(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).\rSpeaker anonymization using orthogonal Householder neural network\nXiaoxiao Miao Xin Wang Erica Cooper Junichi Yamagishi Natalia Tomashenko\nabstract\rabstract: Speaker anonymization aims to conceal a speaker\u0026rsquo;s identity while preservingcontent information in speech. Current mainstream neural-network speakeranonymization systems disentangle speech into prosody-related, content, andspeaker representations. The speaker representation is then anonymized by aselection-based speaker anonymizer that uses a mean vector over a set ofrandomly selected speaker vectors from an external pool of English speakers.However, the resulting anonymized vectors are subject to severe privacy leakageagainst powerful attackers, reduction in speaker diversity, and languagemismatch problems for unseen-language speaker anonymization. To generatediverse, language-neutral speaker vectors, this paper proposes an anonymizerbased on an orthogonal Householder neural network (OHNN). Specifically, theOHNN acts like a rotation to transform the original speaker vectors intoanonymized speaker vectors, which are constrained to follow the distributionover the original speaker vector space. A basic classification loss isintroduced to ensure that anonymized speaker vectors from different speakershave unique speaker identities. To further protect speaker identities, animproved classification loss and similarity loss are used to pushoriginal-anonymized sample pairs away from each other. Experiments onVoicePrivacy Challenge datasets in English and the \\textit{AISHELL-3} datasetin Mandarin demonstrate the proposed anonymizer\u0026rsquo;s effectiveness.\r2023-09-12\nPrompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts\nZhi-Yi Chin Chieh-Ming Jiang Ching-Chun Huang Pin-Yu Chen Wei-Chen Chiu\nabstract\rabstract: Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shownremarkable ability in high-quality content generation, and become one of therepresentatives for the recent wave of transformative AI. Nevertheless, suchadvance comes with an intensifying concern about the misuse of this generativetechnology, especially for producing copyrighted or NSFW (i.e. not safe forwork) images. Although efforts have been made to filter inappropriateimages/prompts or remove undesirable concepts/styles via model fine-tuning, thereliability of these safety mechanisms against diversified problematic promptsremains largely unexplored. In this work, we propose Prompting4Debugging (P4D)as a debugging and red-teaming tool that automatically finds problematicprompts for diffusion models to test the reliability of a deployed safetymechanism. We demonstrate the efficacy of our P4D tool in uncovering newvulnerabilities of SD models with safety mechanisms. Particularly, our resultshows that around half of prompts in existing safe prompting benchmarks whichwere originally considered \u0026ldquo;safe\u0026rdquo; can actually be manipulated to bypass manydeployed safety mechanisms, including concept removal, negative prompt, andsafety guidance. Our findings suggest that, without comprehensive testing, theevaluations on limited safe prompting benchmarks can lead to a false sense ofsafety for text-to-image models.\rFingerprint Attack: Client De-Anonymization in Federated Learning\nQiongkai Xu Trevor Cohn Olga Ohrimenko\nabstract\rabstract: Federated Learning allows collaborative training without data sharing insettings where participants do not trust the central server and one another.Privacy can be further improved by ensuring that communication between theparticipants and the server is anonymized through a shuffle; decoupling theparticipant identity from their data. This paper seeks to examine whether sucha defense is adequate to guarantee anonymity, by proposing a novelfingerprinting attack over gradients sent by the participants to the server. Weshow that clustering of gradients can easily break the anonymization in anempirical study of learning federated language models on two language corpora.We then show that training with differential privacy can provide a practicaldefense against our fingerprint attack.\r2023-09-11\nPreventing Verbatim Memorization in Language Models Gives a False Sense of Privacy\nDaphne Ippolito Florian Tramèr Milad Nasr Chiyuan Zhang Matthew Jagielski Katherine Lee Christopher A. Choquette-Choo Nicholas Carlini\nabstract\rabstract: Studying data memorization in neural language models helps us understand therisks (e.g., to privacy or copyright) associated with models regurgitatingtraining data and aids in the development of countermeasures. Many prior works\u0026ndash; and some recently deployed defenses \u0026ndash; focus on \u0026ldquo;verbatim memorization\u0026rdquo;,defined as a model generation that exactly matches a substring from thetraining set. We argue that verbatim memorization definitions are toorestrictive and fail to capture more subtle forms of memorization.Specifically, we design and implement an efficient defense that perfectlyprevents all verbatim memorization. And yet, we demonstrate that this \u0026ldquo;perfect\u0026quot;filter does not prevent the leakage of training data. Indeed, it is easilycircumvented by plausible and minimally modified \u0026ldquo;style-transfer\u0026rdquo; prompts \u0026ndash;and in some cases even the non-modified original prompts \u0026ndash; to extractmemorized information. We conclude by discussing potential alternativedefinitions and why defining memorization is a difficult yet crucial openquestion for neural language models.\rPrivacy Side Channels in Machine Learning Systems\nEdoardo Debenedetti Giorgio Severi Nicholas Carlini Christopher A. Choquette-Choo Matthew Jagielski Milad Nasr Eric Wallace Florian Tramèr\nabstract\rabstract: Most current approaches for protecting privacy in machine learning (ML)assume that models exist in a vacuum, when in reality, ML models are part oflarger systems that include components for training data filtering, outputmonitoring, and more. In this work, we introduce privacy side channels: attacksthat exploit these system-level components to extract private information atfar higher rates than is otherwise possible for standalone models. We proposefour categories of side channels that span the entire ML lifecycle (trainingdata filtering, input preprocessing, output post-processing, and queryfiltering) and allow for either enhanced membership inference attacks or evennovel threats such as extracting users\u0026rsquo; test queries. For example, we show thatdeduplicating training data before applying differentially-private trainingcreates a side-channel that completely invalidates any provable privacyguarantees. Moreover, we show that systems which block language models fromregenerating training data can be exploited to allow exact reconstruction ofprivate keys contained in the training set \u0026ndash; even if the model did notmemorize these keys. Taken together, our results demonstrate the need for aholistic, end-to-end privacy analysis of machine learning.\r2023-09-10\nUncloneable Quantum Advice\nAnne Broadbent Martti Karvonen Sébastien Lord\nabstract\rabstract: The famous no-cloning principle has been shown recently to enable a number ofuncloneable functionalities. Here we address for the first time unkeyed quantumuncloneablity, via the study of a complexity-theoretic tool that enables acomputation, but that is natively unkeyed: quantum advice. Remarkably, this isan application of the no-cloning principle in a context where the quantumstates of interest are not chosen by a random process. We show theunconditional existence of promise problems admitting uncloneable quantumadvice, and the existence of languages with uncloneable advice, assuming thefeasibility of quantum copy-protecting certain functions. Along the way, wenote that state complexity classes, introduced by Rosenthal and Yuen (ITCS2022) - which concern the computational difficulty of synthesizing sequences ofquantum states - can be naturally generalized to obtain state cloningcomplexity classes. We make initial observations on these classes, notablyobtaining a result analogous to the existence of undecidable problems. Our proof technique establishes the existence of ingenerable sequences offinite bit strings - essentially meaning that they cannot be generated by anyuniform circuit family. We then prove a generic result showing that thedifficulty of accomplishing a computational task on uniformly random inputsimplies its difficulty on any fixed, ingenerable sequence. We use this resultto derandomize quantum cryptographic games that relate to cloning, and thenincorporate a result of Kundu and Tan (arXiv 2022) to obtain uncloneableadvice. Applying this two-step process to a monogamy-of-entanglement gameyields a promise problem with uncloneable advice, and applying it to thequantum copy-protection of pseudorandom functions with super-logarithmic outputlengths yields a language with uncloneable advice.\r2023-09-08\nLLMCad: Fast and Scalable On-device Large Language Model Inference\nDaliang Xu Wangsong Yin Xin Jin Ying Zhang Shiyun Wei Mengwei Xu Xuanzhe Liu\nabstract\rabstract: Generative tasks, such as text generation and question answering, hold acrucial position in the realm of mobile applications. Due to their sensitivityto privacy concerns, there is a growing demand for their execution directly onmobile devices. Currently, the execution of these generative tasks heavilydepends on Large Language Models (LLMs). Nevertheless, the limited memorycapacity of these devices presents a formidable challenge to the scalability ofsuch models. In our research, we introduce LLMCad, an innovative on-device inferenceengine specifically designed for efficient generative Natural LanguageProcessing (NLP) tasks. The core idea behind LLMCad revolves around modelcollaboration: a compact LLM, residing in memory, takes charge of generatingthe most straightforward tokens, while a high-precision LLM steps in tovalidate these tokens and rectify any identified errors. LLMCad incorporatesthree novel techniques: (1) Instead of generating candidate tokens in asequential manner, LLMCad employs the smaller LLM to construct a token tree,encompassing a wider range of plausible token pathways. Subsequently, thelarger LLM can efficiently validate all of these pathways simultaneously. (2)It employs a self-adjusting fallback strategy, swiftly initiating theverification process whenever the smaller LLM generates an erroneous token. (3)To ensure a continuous flow of token generation, LLMCad speculatively generatestokens during the verification process by implementing a compute-IO pipeline.Through an extensive series of experiments, LLMCad showcases an impressivetoken generation speed, achieving rates up to 9.3x faster than existinginference engines.\r2023-09-07\nEnhancing Pipeline-Based Conversational Agents with Large Language Models\nMina Foosherian Hendrik Purwins Purna Rathnayake Touhidul Alam Rui Teimao Klaus-Dieter Thoben\nabstract\rabstract: The latest advancements in AI and deep learning have led to a breakthrough inlarge language model (LLM)-based agents such as GPT-4. However, many commercialconversational agent development tools are pipeline-based and have limitationsin holding a human-like conversation. This paper investigates the capabilitiesof LLMs to enhance pipeline-based conversational agents during two phases: 1)in the design and development phase and 2) during operations. In 1) LLMs canaid in generating training data, extracting entities and synonyms,localization, and persona design. In 2) LLMs can assist in contextualization,intent classification to prevent conversational breakdown and handleout-of-scope questions, auto-correcting utterances, rephrasing responses,formulating disambiguation questions, summarization, and enabling closedquestion-answering capabilities. We conducted informal experiments with GPT-4in the private banking domain to demonstrate the scenarios above with apractical example. Companies may be hesitant to replace their pipeline-basedagents with LLMs entirely due to privacy concerns and the need for deepintegration within their existing ecosystems. A hybrid approach in which LLMs\u0026rsquo;are integrated into the pipeline-based agents allows them to save time andcosts of building and running agents by capitalizing on the capabilities ofLLMs while retaining the integration and privacy safeguards of their existingsystems.\rReuNify: A Step Towards Whole Program Analysis for React Native Android Apps\nYonghui Liu Xiao Chen Pei Liu John Grundy Chunyang Chen Li Li\nabstract\rabstract: React Native is a widely-used open-source framework that facilitates thedevelopment of cross-platform mobile apps. The framework enables JavaScriptcode to interact with native-side code, such as Objective-C/Swift for iOS andJava/Kotlin for Android, via a communication mechanism provided by ReactNative. However, previous research and tools have overlooked this mechanism,resulting in incomplete analysis of React Native app code. To address thislimitation, we have developed REUNIFY, a prototype tool that integrates theJavaScript and native-side code of React Native apps into an intermediatelanguage that can be processed by the Soot static analysis framework. By doingso, REUNIFY enables the generation of a comprehensive model of the app\u0026rsquo;sbehavior. Our evaluation indicates that, by leveraging REUNIFY, the Soot-basedframework can improve its coverage of static analysis for the 1,007 mostpopular React Native Android apps, augmenting the number of lines of Jimplecode by 70%. Additionally, we observed an average increase of 84% in new nodesreached in the callgraph for these apps, after integrating REUNIFY. WhenREUNIFY is used for taint flow analysis, an average of two additional privacyleaks were identified. Overall, our results demonstrate that REUNIFYsignificantly enhances the Soot-based framework\u0026rsquo;s capability to analyze ReactNative Android apps.\rVeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints\nAoting Hu Zhigang Lu Renjie Xie Minhui Xue\nabstract\rabstract: Deploying Machine Learning as a Service gives rise to model plagiarism,leading to copyright infringement. Ownership testing techniques are designed toidentify model fingerprints for verifying plagiarism. However, previous worksoften rely on overfitting or robustness features as fingerprints, lackingtheoretical guarantees and exhibiting under-performance on generalized models.In this paper, we propose a novel ownership testing method called VeriDIP,which verifies a DNN model\u0026rsquo;s intellectual property. VeriDIP makes two majorcontributions. (1) It utilizes membership inference attacks to estimate thelower bound of privacy leakage, which reflects the fingerprint of a givenmodel. The privacy leakage fingerprints highlight the unique patterns throughwhich the models memorize sensitive training datasets. (2) We introduce a novelapproach using less private samples to enhance the performance of ownershiptesting. Extensive experimental results confirm that VeriDIP is effective andefficient in validating the ownership of deep learning models trained on bothimage and tabular datasets. VeriDIP achieves comparable performance tostate-of-the-art methods on image datasets while significantly reducingcomputation and communication costs. Enhanced VeriDIP demonstrates superiorverification performance on generalized deep learning models, particularly ontable-trained models. Additionally, VeriDIP exhibits similar effectiveness onutility-preserving differentially private models compared to non-differentiallyprivate baselines.\r2023-09-06\nPublicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes\nSunjun Kweon Junu Kim Jiyoun Kim Sujeong Im Eunbyeol Cho Seongsu Bae Jungwoo Oh Gyubok Lee Jong Hak Moon Seng Chan You Seungjin Baek Chang Hoon Han Yoon Bin Jung Yohan Jo Edward Choi\nabstract\rabstract: The development of large language models tailored for handling patients\u0026rsquo;clinical notes is often hindered by the limited accessibility and usability ofthese notes due to strict privacy regulations. To address these challenges, wefirst create synthetic large-scale clinical notes using publicly available casereports extracted from biomedical literature. We then use these synthetic notesto train our specialized clinical large language model, Asclepius. WhileAsclepius is trained on synthetic data, we assess its potential performance inreal-world applications by evaluating it using real clinical notes. Webenchmark Asclepius against several other large language models, includingGPT-3.5-turbo and other open-source alternatives. To further validate ourapproach using synthetic notes, we also compare Asclepius with its variantstrained on real clinical notes. Our findings convincingly demonstrate thatsynthetic clinical notes can serve as viable substitutes for real ones whenconstructing high-performing clinical language models. This conclusion issupported by detailed evaluations conducted by both GPT-4 and medicalprofessionals. All resources including weights, codes, and data used in thedevelopment of Asclepius are made publicly accessible for future research.\rMy Art My Choice: Adversarial Protection Against Unruly AI\nAnthony Rhodes Ram Bhagat Umur Aybars Ciftci Ilke Demir\nabstract\rabstract: Generative AI is on the rise, enabling everyone to produce realistic contentvia publicly available interfaces. Especially for guided image generation,diffusion models are changing the creator economy by producing high quality lowcost content. In parallel, artists are rising against unruly AI, since theirartwork are leveraged, distributed, and dissimulated by large generativemodels. Our approach, My Art My Choice (MAMC), aims to empower content ownersby protecting their copyrighted materials from being utilized by diffusionmodels in an adversarial fashion. MAMC learns to generate adversariallyperturbed \u0026ldquo;protected\u0026rdquo; versions of images which can in turn \u0026ldquo;break\u0026rdquo; diffusionmodels. The perturbation amount is decided by the artist to balance distortionvs. protection of the content. MAMC is designed with a simple UNet-basedgenerator, attacking black box diffusion models, combining several losses tocreate adversarial twins of the original artwork. We experiment on threedatasets for various image-to-image tasks, with different user control values.Both protected image and diffusion output results are evaluated in visual,noise, structure, pixel, and generative spaces to validate our claims. Webelieve that MAMC is a crucial step for preserving ownership information for AIgenerated content in a flawless, based-on-need, and human-centric way.\rHide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection\nYu Chen Tingxin Li Huiming Liu Yang Yu\nabstract\rabstract: Numerous companies have started offering services based on large languagemodels (LLM), such as ChatGPT, which inevitably raises privacy concerns asusers\u0026rsquo; prompts are exposed to the model provider. Previous research on securereasoning using multi-party computation (MPC) has proven to be impractical forLLM applications due to its time-consuming and communication-intensive nature.While lightweight anonymization techniques can protect private information inprompts through substitution or masking, they fail to recover sensitive datareplaced in the LLM-generated results. In this paper, we expand the applicationscenarios of anonymization techniques by training a small local model tode-anonymize the LLM\u0026rsquo;s returned results with minimal computational overhead. Weintroduce the HaS framework, where \u0026ldquo;H(ide)\u0026rdquo; and \u0026ldquo;S(eek)\u0026rdquo; represent its two coreprocesses: hiding private entities for anonymization and seeking privateentities for de-anonymization, respectively. To quantitatively assess HaS\u0026rsquo;sprivacy protection performance, we propose both black-box and white-boxadversarial models. Furthermore, we conduct experiments to evaluate HaS\u0026rsquo;susability in translation and classification tasks. The experimental findingsdemonstrate that the HaS framework achieves an optimal balance between privacyprotection and utility.\rAI for Investment: A Platform Disruption\nMohammad Rasouli Ravi Chiruvolu Ali Risheh\nabstract\rabstract: With the investment landscape becoming more competitive, efficiently scalingdeal sourcing and improving deal insights have become a dominant strategy forfunds. While funds are already spending significant efforts on these two tasks,they cannot be scaled with traditional approaches; hence, there is a surge inautomating them. Many third party software providers have emerged recently toaddress this need with productivity solutions, but they fail due to a lack ofpersonalization for the fund, privacy constraints, and natural limits ofsoftware use cases. Therefore, most major funds and many smaller funds havestarted developing their in-house AI platforms: a game changer for theindustry. These platforms grow smarter by direct interactions with the fund andcan be used to provide personalized use cases. Recent developments in largelanguage models, e.g. ChatGPT, have provided an opportunity for other funds toalso develop their own AI platforms. While not having an AI platform now is nota competitive disadvantage, it will be in two years. Funds require a practicalplan and corresponding risk assessments for such AI platforms.\rAutomated Bioinformatics Analysis via AutoBA\nJuexiao Zhou Bin Zhang Xiuying Chen Haoyang Li Xiaopeng Xu Siyuan Chen Xin Gao\nabstract\rabstract: With the fast-growing and evolving omics data, the demand for streamlined andadaptable tools to handle the analysis continues to grow. In response to thisneed, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AIagent based on a large language model designed explicitly for conventionalomics data analysis. AutoBA simplifies the analytical process by requiringminimal user input while delivering detailed step-by-step plans for variousbioinformatics tasks. Through rigorous validation by expert bioinformaticians,AutoBA\u0026rsquo;s robustness and adaptability are affirmed across a diverse range ofomics analysis cases, including whole genome sequencing (WGS), RNA sequencing(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA\u0026rsquo;sunique capacity to self-design analysis processes based on input datavariations further underscores its versatility. Compared with onlinebioinformatic services, AutoBA deploys the analysis locally, preserving dataprivacy. Moreover, different from the predefined pipeline, AutoBA hasadaptability in sync with emerging bioinformatics tools. Overall, AutoBArepresents a convenient tool, offering robustness and adaptability for complexomics data analysis.\r2023-09-05\n\u0026ldquo;An Adapt-or-Die Type of Situation\u0026rdquo;: Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals\nVeera Vimpari Annakaisa Kultima Perttu Hämäläinen Christian Guckelsberger\nabstract\rabstract: Text-to-image generation (TTIG) models, a recent addition to creative AI, cangenerate images based on a text description. These models have begun to rivalthe work of professional creatives, and sparked discussions on the future ofcreative work, loss of jobs, and copyright issues, amongst other importantimplications. To support the sustainable adoption of TTIG, we must providerich, reliable and transparent insights into how professionals perceive, adoptand use TTIG. Crucially though, the public debate is shallow, narrow andlacking transparency, while academic work has focused on studying the use ofTTIG in a general artist population, but not on the perceptions and attitudesof professionals in a specific industry. In this paper, we contribute aqualitative, exploratory interview study on TTIG in the Finnish videogameindustry. Through a Template Analysis on semi-structured interviews with 14game professionals, we reveal 12 overarching themes, structured into 49sub-themes on professionals\u0026rsquo; perception, adoption and use of TTIG systems ingames industry practice. Experiencing (yet another) change of roles andcreative processes, our participants\u0026rsquo; reflections can inform discussions withinthe industry, be used by policymakers to inform urgently needed legislation,and support researchers in games, HCI and AI to support the sustainable,professional use of TTIG to benefit people and games as cultural artefacts.\r2023-09-03\nFusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs\nZhenheng Tang Yuxin Wang Xin He Longteng Zhang Xinglin Pan Qiang Wang Rongfei Zeng Kaiyong Zhao Shaohuai Shi Bingsheng He Xiaowen Chu\nabstract\rabstract: The rapid growth of memory and computation requirements of large languagemodels (LLMs) has outpaced the development of hardware, hindering people wholack large-scale high-end GPUs from training or deploying LLMs. However,consumer-level GPUs, which constitute a larger market share, are typicallyoverlooked in LLM due to their weaker computing performance, smaller storagecapacity, and lower communication bandwidth. Additionally, users may haveprivacy concerns when interacting with remote LLMs. In this paper, we envisiona decentralized system unlocking the potential vast untapped consumer-levelGPUs in pre-training, inference and fine-tuning of LLMs with privacyprotection. However, this system faces critical challenges, including limitedCPU and GPU memory, low network bandwidth, the variability of peer and deviceheterogeneity. To address these challenges, our system design incorporates: 1)a broker with backup pool to implement dynamic join and quit of computingproviders; 2) task scheduling with hardware performance to improve systemefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) toachieve model and task universality; 4) abstracting intermediate representionand execution planes to ensure compatibility of various devices and deeplearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which aresignificantly more expensive.\r2023-09-02\nCombing for Credentials: Active Pattern Extraction from Smart Reply\nBargav Jayaraman Esha Ghosh Melissa Chase Sambuddha Roy Wei Dai David Evans\nabstract\rabstract: Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply\u0026rsquo;\u0026rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.\rValue Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties\nTaylor Sorensen Liwei Jiang Jena Hwang Sydney Levine Valentina Pyatkin Peter West Nouha Dziri Ximing Lu Kavel Rao Chandra Bhagavatula Maarten Sap John Tasioulas Yejin Choi\nabstract\rabstract: Human values are crucial to human decision-making. Value pluralism is theview that multiple correct values may be held in tension with one another(e.g., when considering lying to a friend to protect their feelings, how doesone balance honesty with friendship?). As statistical learners, AI systems fitto averages by default, washing out these potentially irreducible valueconflicts. To improve AI systems to better reflect value pluralism, thefirst-order challenge is to explore the extent to which AI systems can modelpluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, andduties connected to 31k human-written situations. ValuePrism\u0026rsquo;s contextualizedvalues are generated by GPT-4 and deemed high-quality by human annotators 91%of the time. We conduct a large-scale study with annotators across diversesocial and demographic backgrounds to try to understand whose values arerepresented. With ValuePrism, we build Kaleido, an open, light-weight, and structuredlanguage-based multi-task model that generates, explains, and assesses therelevance and valence (i.e., support or oppose) of human values, rights, andduties within a specific context. Humans prefer the sets of values output byour system over the teacher GPT-4, finding them more accurate and with broadercoverage. In addition, we demonstrate that Kaleido can help explain variabilityin human decision-making by outputting contrasting values. Finally, we showthat Kaleido\u0026rsquo;s representations transfer to other philosophical frameworks anddatasets, confirming the benefit of an explicit, modular, and interpretableapproach to value pluralism. We hope that our work will serve as a step tomaking more explicit the implicit values behind human decision-making and tosteering AI systems to make decisions that are more in accordance with them.\r2023-09-01\nDesigning a realistic peer-like embodied conversational agent for supporting children\u0026rsquo;s storytelling\nZhixin Li Ying Xu\nabstract\rabstract: Advances in artificial intelligence have facilitated the use of largelanguage models (LLMs) and AI-generated synthetic media in education, which mayinspire HCI researchers to develop technologies, in particular, embodiedconversational agents (ECAs) to simulate the kind of scaffolding children mightreceive from a human partner. In this paper, we will propose a design prototypeof a peer-like ECA named STARie that integrates multiple AI models - GPT-3,Speech Synthesis (Real-time Voice Cloning), VOCA (Voice Operated CharacterAnimation), and FLAME (Faces Learned with an Articulated Model and Expressions)that aims to support narrative production in collaborative storytelling,specifically for children aged 4-8. However, designing a child-centered ECAraises concerns about age appropriateness, children privacy, gender choices ofECAs, and the uncanny valley effect. Thus, this paper will also discussconsiderations and ethical concerns that must be taken into account whendesigning such an ECA. This proposal offers insights into the potential use ofAI-generated synthetic media in child-centered AI design and how peer-like AIembodiment may support children\\textquotesingle s storytelling.\r2023-08-31\nContinual Learning From a Stream of APIs\nEnneng Yang Zhenyi Wang Li Shen Nan Yin Tongliang Liu Guibing Guo Xingwei Wang Dacheng Tao\nabstract\rabstract: Continual learning (CL) aims to learn new tasks without forgetting previoustasks. However, existing CL methods require a large amount of raw data, whichis often unavailable due to copyright considerations and privacy risks.Instead, stakeholders usually release pre-trained machine learning models as aservice (MLaaS), which users can access via APIs. This paper considers twopractical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL(DFCL-APIs), which achieve CL from a stream of APIs with partial or no rawdata. Performing CL under these two new settings faces several challenges:unavailable full raw data, unknown model parameters, heterogeneous models ofarbitrary architecture and scale, and catastrophic forgetting of previous APIs.To overcome these issues, we propose a novel data-free cooperative continualdistillation learning framework that distills knowledge from a stream of APIsinto a CL model by generating pseudo data, just by querying APIs. Specifically,our framework includes two cooperative generators and one CL model, formingtheir training as an adversarial game. We first use the CL model and thecurrent API as fixed discriminators to train generators via a derivative-freemethod. Generators adversarially generate hard and diverse synthetic data tomaximize the response gap between the CL model and the API. Next, we train theCL model by minimizing the gap between the responses of the CL model and theblack-box API on synthetic data, to transfer the API\u0026rsquo;s knowledge to the CLmodel. Furthermore, we propose a new regularization term based on networksimilarity to prevent catastrophic forgetting of previous APIs.Our methodperforms comparably to classic CL with full raw data on the MNIST and SVHN inthe DFCL-APIs setting. In the DECL-APIs setting, our method achieves 0.97x,0.75x and 0.69x performance of classic CL on CIFAR10, CIFAR100, andMiniImageNet.\r2023-08-30\nGrandma Karl is 27 years old \u0026ndash; research agenda for pseudonymization of research data\nElena Volodina Simon Dobnik Therese Lindström Tiedemann Xuan-Son Vu\nabstract\rabstract: Accessibility of research data is critical for advances in many researchfields, but textual data often cannot be shared due to the personal andsensitive information which it contains, e.g names or political opinions.General Data Protection Regulation (GDPR) suggests pseudonymization as asolution to secure open access to research data, but we need to learn moreabout pseudonymization as an approach before adopting it for manipulation ofresearch data. This paper outlines a research agenda within pseudonymization,namely need of studies into the effects of pseudonymization on unstructureddata in relation to e.g. readability and language assessment, as well as theeffectiveness of pseudonymization as a way of protecting writer identity, whilealso exploring different ways of developing context-sensitive algorithms fordetection, labelling and replacement of personal information in unstructureddata. The recently granted project on pseudonymization Grandma Karl is 27 yearsold addresses exactly those challenges.\rIntroducing Language Guidance in Prompt-based Continual Learning\nMuhammad Gul Zain Ali Khan Muhammad Ferjad Naeem Luc Van Gool Didier Stricker Federico Tombari Muhammad Zeshan Afzal\nabstract\rabstract: Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters.\rOn the culture of open access: the Sci-hub paradox\nAbdelghani Maddi David Sapinho\nabstract\rabstract: Shadow libraries, also known as \u0026lsquo;\u0026lsquo;pirate libraries\u0026rsquo;\u0026rsquo;, are online collectionsof copyrighted publications that have been made available for free without thepermission of the copyright holders. They have gradually become key players ofscientific knowledge dissemination, despite their illegality in most countriesof the world. Many publishers and scientist-editors decry such libraries fortheir copyright infringement and loss of publication usage information, whilesome scholars and institutions support them, sometimes in a roundabout way, fortheir role in reducing inequalities of access to knowledge, particularly inlow-income countries. Although there is a wealth of literature on shadowlibraries, none of this have focused on its potential role in knowledgedissemination, through the open access movement. Here we analyze how shadowlibraries can affect researchers\u0026rsquo; citation practices, highlighting somecounter-intuitive findings about their impact on the Open Access CitationAdvantage (OACA). Based on a large randomized sample, this study first showsthat OA publications, including those in fully OA journals, receive morecitations than their subscription-based counterparts do. However, the OACA hasslightly decreased over the seven last years. The introduction of a distinctionbetween those accessible or not via the Scihub platform amongsubscription-based suggest that the generalization of its use cancels thepositive effect of OA publishing. The results show that publications in fullyOA journals are victims of the success of Sci-hub. Thus, paradoxically,although Sci-hub may seem to facilitate access to scientific knowledge, itnegatively affects the OA movement as a whole, by reducing the comparativeadvantage of OA publications in terms of visibility for researchers. Thedemocratization of the use of Sci-hub may therefore lead to a vicious cycle,hindering efforts to develop full OA strategies without proposing a credibleand sustainable alternative model for the dissemination of scientificknowledge.\r2023-08-29\nTransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification\nJianing Wang Chengyu Wang Cen Chen Ming Gao Jun Huang Aoying Zhou\nabstract\rabstract: Text classification is one of the most imperative tasks in natural languageprocessing (NLP). Recent advances with pre-trained language models (PLMs) haveshown remarkable success on this task. However, the satisfying results obtainedby PLMs heavily depend on the large amounts of task-specific labeled data,which may not be feasible in many application scenarios due to data access andprivacy constraints. The recently-proposed prompt-based fine-tuning paradigmimproves the performance of PLMs for few-shot text classification withtask-specific templates. Yet, it is unclear how the prompting knowledge can betransferred across tasks, for the purpose of mutual reinforcement. We proposeTransPrompt v2, a novel transferable prompting framework for few-shot learningacross similar or distant text classification tasks. For learning acrosssimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)procedure to train a meta-learner that captures the cross-task transferableknowledge. For learning across distant tasks, we further inject the task typedescriptions into the prompt, and capture the intra-type and inter-type promptembeddings among multiple distant tasks. Additionally, two de-biasingtechniques are further designed to make the trained meta-learner moretask-agnostic and unbiased towards any tasks. After that, the meta-learner canbe adapted to each specific task with better parameters initialization.Extensive experiments show that TransPrompt v2 outperforms single-task andcross-task strong baselines over multiple NLP tasks and datasets. We furthershow that the meta-learner can effectively improve the performance of PLMs onpreviously unseen tasks. In addition, TransPrompt v2 also outperforms strongfine-tuning baselines when learning with full training sets.\rCEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction\nUmar Khalid Hasan Iqbal Saeed Vahidian Jing Hua Chen Chen\nabstract\rabstract: Human-robot interaction (HRI) is a rapidly growing field that encompassessocial and industrial applications. Machine learning plays a vital role inindustrial HRI by enhancing the adaptability and autonomy of robots in complexenvironments. However, data privacy is a crucial concern in the interactionbetween humans and robots, as companies need to protect sensitive data whilemachine learning algorithms require access to large datasets. FederatedLearning (FL) offers a solution by enabling the distributed training of modelswithout sharing raw data. Despite extensive research on Federated learning (FL)for tasks such as natural language processing (NLP) and image classification,the question of how to use FL for HRI remains an open research problem. Thetraditional FL approach involves transmitting large neural network parametermatrices between the server and clients, which can lead to high communicationcosts and often becomes a bottleneck in FL. This paper proposes acommunication-efficient FL framework for human-robot interaction (CEFHRI) toaddress the challenges of data heterogeneity and communication costs. Theframework leverages pre-trained models and introduces a trainablespatiotemporal adapter for video understanding tasks in HRI. Experimentalresults on three human-robot interaction benchmark datasets: HRI30, InHARD, andCOIN demonstrate the superiority of CEFHRI over full fine-tuning in terms ofcommunication costs. The proposed methodology provides a secure and efficientapproach to HRI federated learning, particularly in industrial environmentswith data privacy concerns and limited communication bandwidth. Our code isavailable athttps://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.\r2023-08-28\nChallenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption\nAlaina N. Talboy Elizabeth Fuller\nabstract\rabstract: Assessments of algorithmic bias in large language models (LLMs) are generallycatered to uncovering systemic discrimination based on protectedcharacteristics such as sex and ethnicity. However, there are over 180documented cognitive biases that pervade human reasoning and decision makingthat are routinely ignored when discussing the ethical complexities of AI. Wedemonstrate the presence of these cognitive biases in LLMs and discuss theimplications of using biased reasoning under the guise of expertise. We callfor stronger education, risk management, and continued research as widespreadadoption of this technology increases. Finally, we close with a set of bestpractices for when and how to employ this technology as widespread adoptioncontinues to grow.\rNoctalgia (sky grief): Our Brightening Night Skies and Loss of Environment for Astronomy and Sky Traditions\nAparna Venkatesan John C. Barentine\nabstract\rabstract: Fifty years after the first mention of light pollution in Science, thejournal recently elevated this topic to the cover of its 16 June 2023 issue,highlighting the large impact on human and ecological health, circadianrhythms, migratory patterns, and more. We offer here the term noctalgia toexpress \u0026ldquo;sky grief\u0026rdquo; for the accelerating loss of the home environment of ourshared skies - representing loss of science, heritage, millennia-old skytraditions, place-based language, and more - and summarize next steps toaddress the protection of our nighttime and daytime skies.\rCodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models\nZhensu Sun Xiaoning Du Fu Song Li Li\nabstract\rabstract: Code datasets are of immense value for training neural-network-based codecompletion models, where companies or organizations have made substantialinvestments to establish and process these datasets. Unluckily, these datasets,either built for proprietary or public usage, face the high risk ofunauthorized exploits, resulting from data leakages, license violations, etc.Even worse, the ``black-box\u0026rsquo;\u0026rsquo; nature of neural models sets a high barrier forexternals to audit their training datasets, which further connives theseunauthorized usages. Currently, watermarking methods have been proposed toprohibit inappropriate usage of image and natural language datasets. However,due to domain specificity, they are not directly applicable to code datasets,leaving the copyright protection of this emerging and important field of codedata still exposed to threats. To fill this gap, we propose a method, namedCodeMark, to embed user-defined imperceptible watermarks into code datasets totrace their usage in training neural code completion models. CodeMark is basedon adaptive semantic-preserving transformations, which preserve the exactfunctionality of the code data and keep the changes covert againstrule-breakers. We implement CodeMark in a toolkit and conduct an extensiveevaluation of code completion models. CodeMark is validated to fulfill alldesired properties of practical watermarks, including harmlessness to modelaccuracy, verifiability, robustness, and imperceptibility.\rEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models\nRongjie Yi Liwei Guo Shiyun Wei Ao Zhou Shangguang Wang Mengwei Xu\nabstract\rabstract: Large Language Models (LLMs) such as GPTs and LLaMa have ushered in arevolution in machine intelligence, owing to their exceptional capabilities ina wide range of machine learning tasks. However, the transition of LLMs fromdata centers to edge devices presents a set of challenges and opportunities.While this shift can enhance privacy and availability, it is hampered by theenormous parameter sizes of these models, leading to impractical runtime costs.In light of these considerations, we introduce EdgeMoE, the first on-deviceinference engine tailored for mixture-of-expert (MoE) LLMs, a popular variantof sparse LLMs that exhibit nearly constant computational complexity as theirparameter size scales. EdgeMoE achieves both memory and computationalefficiency by strategically partitioning the model across the storagehierarchy. Specifically, non-expert weights are stored in the device\u0026rsquo;s memory,while expert weights are kept in external storage and are fetched into memoryonly when they are activated. This design is underpinned by a crucial insightthat expert weights, though voluminous, are infrequently accessed due to sparseactivation patterns. To further mitigate the overhead associated with expertI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wisebitwidth adaptation: This method reduces the size of expert weights with anacceptable level of accuracy loss. (2) Expert management: It predicts theexperts that will be activated in advance and preloads them into thecompute-I/O pipeline, thus further optimizing the process. In empiricalevaluations conducted on well-established MoE LLMs and various edge devices,EdgeMoE demonstrates substantial memory savings and performance improvementswhen compared to competitive baseline solutions.\r2023-08-27\nProtecting Language Generation Models via Invisible Watermarking\nXuandong Zhao Yu-Xiang Wang Lei Li\nabstract\rabstract: Language generation models have been an increasingly powerful enabler formany applications. Many such models offer free or affordable API access, whichmakes them potentially vulnerable to model extraction attacks throughdistillation. To protect intellectual property (IP) and ensure fair use ofthese models, various techniques such as lexical watermarking and synonymreplacement have been proposed. However, these methods can be nullified byobvious countermeasures such as \u0026ldquo;synonym randomization\u0026rdquo;. To address this issue,we propose GINSEW, a novel method to protect text generation models from beingstolen through distillation. The key idea of our method is to inject secretsignals into the probability vector of the decoding steps for each targettoken. We can then detect the secret message by probing a suspect model to tellif it is distilled from the protected one. Experimental results show thatGINSEW can effectively identify instances of IP infringement with minimalimpact on the generation quality of protected APIs. Our method demonstrates anabsolute improvement of 19 to 29 points on mean average precision (mAP) indetecting suspects compared to previous methods against watermark removalattacks.\r2023-08-23\nHow to Protect Copyright Data in Optimization of Large Language Models?\nTimothy Chu Zhao Song Chiwun Yang\nabstract\rabstract: Large language models (LLMs) and generative AI have played a transformativerole in computer research and applications. Controversy has arisen as towhether these models output copyrighted data, which can occur if the data themodels are trained on is copyrighted. LLMs are built on the transformer neuralnetwork architecture, which in turn relies on a mathematical computation calledAttention that uses the softmax function. In this paper, we show that large language model training and optimizationcan be seen as a softmax regression problem. We then establish a method ofefficiently performing softmax regression, in a way that prevents theregression function from generating copyright data. This establishes atheoretical method of training large language models in a way that avoidsgenerating copyright data.\r2023-08-22\nTowards an On-device Agent for Text Rewriting\nYun Zhu Yinxiao Liu Felix Stahlberg Shankar Kumar Yu-hui Chen Liangchen Luo Lei Shu Renjie Liu Jindong Chen Lei Meng\nabstract\rabstract: Large Language Models (LLMs) have demonstrated impressive capabilities fortext rewriting. Nonetheless, the large sizes of these models make themimpractical for on-device inference, which would otherwise allow for enhancedprivacy and economical inference. Creating a smaller yet potent language modelfor text rewriting presents a formidable challenge because it requiresbalancing the need for a small size with the need to retain the emergentcapabilities of the LLM, that requires costly data collection. To address theabove challenge, we introduce a new instruction tuning approach for building amobile-centric text rewriting model. Our strategies enable the generation ofhigh quality training data without any human labeling. In addition, we proposea heuristic reinforcement learning framework which substantially enhancesperformance without requiring preference data. To further bridge theperformance gap with the larger server-side model, we propose an effectiveapproach that combines the mobile rewrite agent with the server model using acascade. To tailor the text rewriting tasks to mobile scenarios, we introduceMessageRewriteEval, a benchmark that focuses on text rewriting for messagesthrough natural language instructions. Through empirical experiments, wedemonstrate that our on-device model surpasses the current state-of-the-artLLMs in text rewriting while maintaining a significantly reduced model size.Notably, we show that our proposed cascading approach improves modelperformance.\r2023-08-21\nFedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning\nHaokun Chen Yao Zhang Denis Krompass Jindong Gu Volker Tresp\nabstract\rabstract: Recently, foundation models have exhibited remarkable advancements inmulti-modal learning. These models, equipped with millions (or billions) ofparameters, typically require a substantial amount of data for finetuning.However, collecting and centralizing training data from diverse sectors becomeschallenging due to distinct privacy regulations. Federated Learning (FL)emerges as a promising solution, enabling multiple clients to collaborativelytrain neural networks without centralizing their local data. To alleviateclient computation burdens and communication overheads, previous works haveadapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only asmall fraction of the model parameters are optimized and communicated duringfederated communications. Nevertheless, most previous works have focused on asingle modality and neglected one common phenomenon, i.e., the presence of dataheterogeneity across the clients. Therefore, in this work, we propose afinetuning framework tailored to heterogeneous multi-modal FL, called FederatedDual-Aadapter Teacher (FedDAT). Specifically, our approach leverages aDual-Adapter Teacher (DAT) to address data heterogeneity by regularizing theclient local updates and applying Mutual Knowledge Distillation (MKD) for anefficient knowledge transfer. FedDAT is the first approach that enables anefficient distributed finetuning of foundation models for a variety ofheterogeneous Vision-Language tasks. To demonstrate its effectiveness, weconduct extensive experiments on four multi-modality FL benchmarks withdifferent types of data heterogeneity, where FedDAT substantially outperformsthe existing centralized PEFT methods adapted for FL.\rFederated learning for secure development of AI models for Parkinson\u0026rsquo;s disease detection using speech from different languages\nSoroosh Tayebi Arasteh Cristian David Rios-Urrego Elmar Noeth Andreas Maier Seung Hee Yang Jan Rusz Juan Rafael Orozco-Arroyave\nabstract\rabstract: Parkinson\u0026rsquo;s disease (PD) is a neurological disorder impacting a person\u0026rsquo;sspeech. Among automatic PD assessment methods, deep learning models have gainedparticular interest. Recently, the community has explored cross-pathology andcross-language models which can improve diagnostic accuracy even further.However, strict patient data privacy regulations largely prevent institutionsfrom sharing patient speech data with each other. In this paper, we employfederated learning (FL) for PD detection using speech signals from 3 real-worldlanguage corpora of German, Spanish, and Czech, each from a separateinstitution. Our results indicate that the FL model outperforms all the localmodels in terms of diagnostic accuracy, while not performing very differentlyfrom the model based on centrally combined training sets, with the advantage ofnot requiring any data sharing among collaborators. This will simplifyinter-institutional collaborations, resulting in enhancement of patientoutcomes.\r2023-08-19\nFederated Few-Shot Learning for Mobile NLP\nDongqi Cai Shangguang Wang Yaozong Wu Felix Xiaozhu Lin Mengwei Xu\nabstract\rabstract: Natural language processing (NLP) sees rich mobile applications. To supportvarious language understanding tasks, a foundation NLP model is oftenfine-tuned in a federated, privacy-preserving setting (FL). This processcurrently relies on at least hundreds of thousands of labeled training samplesfrom mobile clients; yet mobile users often lack willingness or knowledge tolabel their data. Such an inadequacy of data labels is known as a few-shotscenario; it becomes the key blocker for mobile NLP applications. For the first time, this work investigates federated NLP in the few-shotscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling andprompt learning, we first establish a training pipeline that deliverscompetitive accuracy when only 0.05% (fewer than 100) of the training data islabeled and the remaining is unlabeled. To instantiate the workflow, we furtherpresent a system FeS, addressing the high execution cost with novel designs.(1) Curriculum pacing, which injects pseudo labels to the training workflow ata rate commensurate to the learning progress; (2) Representational diversity, amechanism for selecting the most learnable data, only for which pseudo labelswill be generated; (3) Co-planning of a model\u0026rsquo;s training depth and layercapacity. Together, these designs reduce the training delay, client energy, andnetwork traffic by up to 46.0$\\times$, 41.2$\\times$ and 3000.0$\\times$,respectively. Through algorithm/system co-design, FFNLP demonstrates that FLcan apply to challenging settings where most training samples are unlabeled.\rDUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization\nXiaoyu Ye Hao Huang Jiaqi An Yongtao Wang\nabstract\rabstract: Stable Diffusion (SD) customization approaches enable users to personalize SDmodel outputs, greatly enhancing the flexibility and diversity of AI art.However, they also allow individuals to plagiarize specific styles or subjectsfrom copyrighted images, which raises significant concerns about potentialcopyright infringement. To address this issue, we propose an invisibledata-free universal adversarial watermark (DUAW), aiming to protect a myriad ofcopyrighted images from different customization approaches across variousversions of SD models. First, DUAW is designed to disrupt the variationalautoencoder during SD customization. Second, DUAW operates in a data-freecontext, where it is trained on synthetic images produced by a Large LanguageModel (LLM) and a pretrained SD model. This approach circumvents the necessityof directly handling copyrighted images, thereby preserving theirconfidentiality. Once crafted, DUAW can be imperceptibly integrated intomassive copyrighted images, serving as a protective measure by inducingsignificant distortions in the images generated by customized SD models.Experimental results demonstrate that DUAW can effectively distort the outputsof fine-tuned SD models, rendering them discernible to both human observers anda simple classifier.\r2023-08-18\nLeveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks\nAbubakar S. Ali Dimitrios Michael Manias Abdallah Shami Sami Muhaidat\nabstract\rabstract: As the dawn of sixth-generation (6G) networking approaches, it promisesunprecedented advancements in communication and automation. Among the leadinginnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming toachieve fully automated, self-optimizing networks with minimal humanintervention. Despite the advantages ZTNs offer in terms of efficiency andscalability, challenges surrounding transparency, adaptability, and human trustremain prevalent. Concurrently, the advent of Large Language Models (LLMs)presents an opportunity to elevate the ZTN framework by bridging the gapbetween automated processes and human-centric interfaces. This paper exploresthe integration of LLMs into ZTNs, highlighting their potential to enhancenetwork transparency and improve user interactions. Through a comprehensivecase study on deep reinforcement learning (DRL)-based anti-jamming technique,we demonstrate how LLMs can distill intricate network operations intointuitive, human-readable reports. Additionally, we address the technical andethical intricacies of melding LLMs with ZTNs, with an emphasis on dataprivacy, transparency, and bias reduction. Looking ahead, we identify emergingresearch avenues at the nexus of LLMs and ZTNs, advocating for sustainedinnovation and interdisciplinary synergy in the domain of automated networks.\r2023-08-17\nFashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings\nYulin Su Min Yang Minghui Qiu Jing Wang Tao Wang\nabstract\rabstract: Logo embedding plays a crucial role in various e-commerce applications byfacilitating image retrieval or recognition, such as intellectual propertyprotection and product search. However, current methods treat logo embedding asa purely visual problem, which may limit their performance in real-worldscenarios. A notable issue is that the textual knowledge embedded in logoimages has not been adequately explored. Therefore, we propose a novel approachthat leverages textual knowledge as an auxiliary to improve the robustness oflogo embedding. The emerging Multimodal Large Language Models (MLLMs) havedemonstrated remarkable capabilities in both visual and textual understandingand could become valuable visual assistants in understanding logo images.Inspired by this observation, our proposed method, FashionLOGO, aims to utilizeMLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logoembedding by prompting them to generate explicit textual knowledge throughthree types of prompts, including image OCR, brief captions, and detaileddescriptions prompts, in a zero-shot setting. We adopt a cross-attentiontransformer to enable image embedding queries to learn supplementary knowledgefrom textual embeddings automatically. To reduce computational costs, we onlyuse the image embedding model in the inference stage, similar to traditionalinference pipelines. Our extensive experiments on three real-world datasetsdemonstrate that FashionLOGO learns generalized and robust logo embeddings,achieving state-of-the-art performance in all benchmark datasets. Furthermore,we conduct comprehensive ablation studies to demonstrate the performanceimprovements resulting from the introduction of MLLMs.\rApproaches to Generative Artificial Intelligence, A Social Justice Perspective\nMyke Healy\nabstract\rabstract: In the 2023-2024 academic year, the widespread availability of generativeartificial intelligence, exemplified by ChatGPT\u0026rsquo;s 1.6 billion monthly visits,is set to impact academic integrity. With 77% of high school studentspreviously reporting engagement in dishonest behaviour, the rise of AI-drivenwriting assistance, dubbed \u0026lsquo;AI-giarism\u0026rsquo; by Chan (arXiv:2306.03358v2), will makeplagiarism more accessible and less detectable. While these concerns areurgent, they also raise broader questions about the revolutionary nature ofthis technology, including autonomy, data privacy, copyright, and equity. Thispaper aims to explore generative AI from a social justice perspective,examining the training of these models, the inherent biases, and the potentialinjustices in detecting AI-generated writing.\rDifferential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models\nPhillip Rust Anders Søgaard\nabstract\rabstract: Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingualgeneralization or compression to facilitate transfer to a large number of(potentially unseen) languages. However, these models should ideally also beprivate, linguistically fair, and transparent, by relating their predictions totraining data. Can these requirements be simultaneously satisfied? We show thatmultilingual compression and linguistic fairness are compatible withdifferential privacy, but that differential privacy is at odds with trainingdata influence sparsity, an objective for transparency. We further present aseries of experiments on two common NLP tasks and evaluate multilingualcompression and training data influence sparsity under different privacyguarantees, exploring these trade-offs in more detail. Our results suggest thatwe need to develop ways to jointly optimize for these objectives in order tofind practical trade-offs.\r2023-08-16\nFermionic fractional quantum Hall states: A modern approach to systems with bulk-edge correspondence\nYoshiki Fukusumi Bo Yang\nabstract\rabstract: In contemporary physics, especially in condensed matter physics, fermionictopological order and its protected edge modes are one of the most importantobjects. In this work, we propose a systematic construction of the cylinderpartition corresponding to the fermionic fractional quantum Hall effect (FQHE)and a general mechanism for obtaining the candidates of the protected edgemodes. In our construction, when the underlying conformal field theory has the$Z_{2}$ duality defects corresponding to the fermionic $Z_{2}$ electricparticle, we show that the FQH partition function has a fermionic T duality.This duality is analogous to (hopefully the same as) the dualities in the dualresonance models, typically known as supersymmetry, and gives a renormalizationgroup (RG) theoretic understanding of the topological phases. We also introducea modern understanding of bulk topological degeneracies and topologicalentanglement entropy. This understanding is based on the traditional tunnelproblem and the recent conjecture of correspondence between the bulkrenormalization group flow and the boundary conformal field theory. Ourformalism gives an intuitive and general understanding of the modern physics ofthe topologically ordered systems in the traditional language of RG andfermionization.\rAblating Concepts in Text-to-Image Diffusion Models\nNupur Kumari Bingliang Zhang Sheng-Yu Wang Eli Shechtman Richard Zhang Jun-Yan Zhu\nabstract\rabstract: Large-scale text-to-image diffusion models can generate high-fidelity imageswith powerful compositional ability. However, these models are typicallytrained on an enormous amount of Internet data, often containing copyrightedmaterial, licensed images, and personal photos. Furthermore, they have beenfound to replicate the style of various living artists or memorize exacttraining samples. How can we remove such copyrighted concepts or images withoutretraining the model from scratch? To achieve this goal, we propose anefficient method of ablating concepts in the pretrained model, i.e., preventingthe generation of a target concept. Our algorithm learns to match the imagedistribution for a target style, instance, or text prompt we wish to ablate tothe distribution corresponding to an anchor concept. This prevents the modelfrom generating target concepts given its text condition. Extensive experimentsshow that our method can successfully prevent the generation of the ablatedconcept while preserving closely related concepts in the model.\r2023-08-15\nRobustness Over Time: Understanding Adversarial Examples\u0026rsquo; Effectiveness on Longitudinal Versions of Large Language Models\nYugeng Liu Tianshuo Cong Zhengyu Zhao Michael Backes Yun Shen Yang Zhang\nabstract\rabstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.\r2023-08-14\nTowards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark\nShuyu Yang Yinan Zhou Yaxiong Wang Yujiao Wu Li Zhu Zhedong Zheng\nabstract\rabstract: In this paper, we introduce a large Multi-Attribute and Language Searchdataset for text-based person retrieval, called MALS, and explore thefeasibility of performing pre-training on both attribute recognition andimage-text matching tasks in one stone. In particular, MALS contains 1,510,330image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,and all images are annotated with 27 attributes. Considering the privacyconcerns and annotation costs, we leverage the off-the-shelf diffusion modelsto generate the dataset. To verify the feasibility of learning from thegenerated data, we develop a new joint Attribute Prompt Learning and TextMatching Learning (APTM) framework, considering the shared knowledge betweenattribute and text. As the name implies, APTM contains an attribute promptlearning stream and a text matching learning stream. (1) The attribute promptlearning leverages the attribute prompts for image-attribute alignment, whichenhances the text matching learning. (2) The text matching learning facilitatesthe representation learning on fine-grained details, and in turn, boosts theattribute prompt learning. Extensive experiments validate the effectiveness ofthe pre-training on MALS, achieving state-of-the-art retrieval performance viaAPTM on three challenging real-world benchmarks. In particular, APTM achieves aconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy onCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.\r2023-08-13\nFree-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks\nDavid Junhao Zhang Mutian Xu Chuhui Xue Wenqing Zhang Xiaoguang Han Song Bai Mike Zheng Shou\nabstract\rabstract: Despite the rapid advancement of unsupervised learning in visualrepresentation, it requires training on large-scale datasets that demand costlydata collection, and pose additional challenges due to concerns regarding dataprivacy. Recently, synthetic images generated by text-to-image diffusionmodels, have shown great potential for benefiting image recognition. Althoughpromising, there has been inadequate exploration dedicated to unsupervisedlearning on diffusion-generated images. To address this, we start by uncoveringthat diffusion models\u0026rsquo; cross-attention layers inherently provideannotation-free attention masks aligned with corresponding text inputs ongenerated images. We then investigate the problems of three prevalentunsupervised learning techniques ( i.e., contrastive learning, masked modeling,and vision-language pretraining) and introduce customized solutions by fullyexploiting the aforementioned free attention masks. Our approach is validatedthrough extensive experiments that show consistent improvements in baselinemodels across various downstream tasks, including image classification,detection, segmentation, and image-text retrieval. By utilizing our method, itis possible to close the performance gap between unsupervised pretraining onsynthetic data and real-world scenarios.\rEthical Aspects of ChatGPT in Software Engineering Research\nMuhammad Azeem Akbar Arif Ali Khan Peng Liang\nabstract\rabstract: ChatGPT can improve Software Engineering (SE) research practices by offeringefficient, accessible information analysis and synthesis based on naturallanguage interactions. However, ChatGPT could bring ethical challenges,encompassing plagiarism, privacy, data security, and the risk of generatingbiased or potentially detrimental data. This research aims to fill the givengap by elaborating on the key elements: motivators, demotivators, and ethicalprinciples of using ChatGPT in SE research. To achieve this objective, weconducted a literature survey, identified the mentioned elements, and presentedtheir relationships by developing a taxonomy. Further, the identifiedliterature-based elements (motivators, demotivators, and ethical principles)were empirically evaluated by conducting a comprehensive questionnaire-basedsurvey involving SE researchers. Additionally, we employed InterpretiveStructure Modeling (ISM) approach to analyze the relationships between theethical principles of using ChatGPT in SE research and develop a level baseddecision model. We further conducted a Cross-Impact Matrix MultiplicationApplied to Classification (MICMAC) analysis to create a cluster-based decisionmodel. These models aim to help SE researchers devise effective strategies forethically integrating ChatGPT into SE research by following the identifiedprinciples through adopting the motivators and addressing the demotivators. Thefindings of this study will establish a benchmark for incorporating ChatGPTservices in SE research with an emphasis on ethical considerations.\r2023-08-11\nEnhancing Network Management Using Code Generated by Large Language Models\nSathiya Kumaran Mani Yajie Zhou Kevin Hsieh Santiago Segarra Ranveer Chandra Srikanth Kandula\nabstract\rabstract: Analyzing network topologies and communication graphs plays a crucial role incontemporary network management. However, the absence of a cohesive approachleads to a challenging learning curve, heightened errors, and inefficiencies.In this paper, we introduce a novel approach to facilitate anatural-language-based network management experience, utilizing large languagemodels (LLMs) to generate task-specific code from natural language queries.This method tackles the challenges of explainability, scalability, and privacyby allowing network operators to inspect the generated code, eliminating theneed to share network data with LLMs, and concentrating on application-specificrequests combined with general program synthesis techniques. We design andevaluate a prototype system using benchmark applications, showcasing highaccuracy, cost-effectiveness, and the potential for further enhancements usingcomplementary program synthesis techniques.\r2023-08-10\nUnderstanding the Cryptocurrency Free Giveaway Scam Disseminated on Twitter Lists\nKai Li Darren Lee Shixuan Guan\nabstract\rabstract: This paper presents a comprehensive analysis of the cryptocurrency freegiveaway scam disseminated in a new distribution channel, Twitter lists. Tocollect and detect the scam in this channel, unlike existing scam detectionsystems that rely on manual effort, this paper develops a fully automated scamdetection system, \\textit{GiveawayScamHunter}, to continuously collect listsfrom Twitter and utilize a Nature-Language-Processing (NLP) model toautomatically detect the free giveaway scam and extract the scam cryptocurrencyaddress. By running \\textit{GiveawayScamHunter} from June 2022 to June 2023, wedetected 95,111 free giveaway scam lists on Twitter that were created bythousands of Twitter accounts. Through analyzing the list creator accounts, ourwork reveals that scammers have combined different strategies to spread thescam, including compromising popular accounts and creating spam accounts onTwitter. Our analysis result shows that 43.9% of spam accounts still remainactive as of this writing. Furthermore, we collected 327 free giveaway domainsand 121 new scam cryptocurrency addresses. By tracking the transactions of thescam cryptocurrency addresses, this work uncovers that over 365 victims havebeen attacked by the scam, resulting in an estimated financial loss of 872KUSD. Overall, this work sheds light on the tactics, scale, and impact of freegiveaway scams disseminated on Twitter lists, emphasizing the urgent need foreffective detection and prevention mechanisms to protect social media usersfrom such fraudulent activity.\r2023-08-09\nLLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following\nKaize Shi Xueyao Sun Dingxian Wang Yinlin Fu Guandong Xu Qing Li\nabstract\rabstract: E-commerce authoring involves creating attractive, abundant, and targetedpromotional content to drive product sales. The emergence of large languagemodels (LLMs) introduces an innovative paradigm, offering a unified solution toaddress various authoring tasks within this scenario. However, mainstream LLMstrained on general corpora with common sense knowledge reveal limitations infitting complex and personalized features unique to e-commerce products andcustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,raising concerns about safeguarding voluminous customer privacy data duringtransmission. This paper proposes the LLaMA-E, the unified and customizedinstruction-following language models focusing on diverse e-commerce authoringtasks. Specifically, the domain experts create the seed instruction set fromthe tasks of ads generation, query-enhanced product title rewriting, productclassification, purchase intent speculation, and general Q\u0026amp;A. These tasksenable the models to comprehensively understand precise e-commerce authoringknowledge by interleaving features covering typical service aspects ofcustomers, sellers, and platforms. The GPT-3.5 is introduced as a teachermodel, which expands the seed instructions to form a training set for theLLaMA-E models with various scales. The experimental results show that theproposed LLaMA-E models achieve state-of-the-art results in quantitative andqualitative evaluations, also exhibiting the advantage in zero-shot scenes. Tothe best of our knowledge, this study is the first to serve the LLMs tospecific e-commerce authoring scenarios.\r2023-08-08\nSILO Language Models: Isolating Legal Risk In a Nonparametric Datastore\nSewon Min Suchin Gururangan Eric Wallace Hannaneh Hajishirzi Noah A. Smith Luke Zettlemoyer\nabstract\rabstract: The legality of training language models (LMs) on copyrighted or otherwiserestricted data is under intense debate. However, as we show, model performancesignificantly degrades if trained only on low-risk text (e.g., out-of-copyrightbooks or government documents), due to its limited size and domain coverage. Wepresent SILO, a new language model that manages this risk-performance tradeoffduring inference. SILO is built by (1) training a parametric LM on Open LicenseCorpus (OLC), a new corpus we curate with 228B tokens of public domain andpermissively licensed text and (2) augmenting it with a more general and easilymodifiable nonparametric datastore (e.g., containing copyrighted books or news)that is only queried during inference. The datastore allows use of high-riskdata without training on it, supports sentence-level data attribution, andenables data producers to opt out from the model by removing content from thestore. These capabilities can foster compliance with data-use regulations suchas the fair use doctrine in the United States and the GDPR in the EuropeanUnion. Our experiments show that the parametric LM struggles on domains notcovered by OLC. However, access to the datastore greatly improves out of domainperformance, closing 90% of the performance gap with an LM trained on the Pile,a more diverse corpus with mostly high-risk text. We also analyze whichnonparametric approach works best, where the remaining errors lie, and howperformance scales with datastore size. Our results suggest that it is possibleto build high quality language models while mitigating their legal risk.\rAssistive Chatbots for healthcare: a succinct review\nBasabdatta Sen Bhattacharya Vibhav Sinai Pissurlenkar\nabstract\rabstract: Artificial Intelligence (AI) for supporting healthcare services has neverbeen more necessitated than by the recent global pandemic. Here, we review thestate-of-the-art in AI-enabled Chatbots in healthcare proposed during the last10 years (2013-2023). The focus on AI-enabled technology is because of itspotential for enhancing the quality of human-machine interaction via Chatbots,reducing dependence on human-human interaction and saving man-hours. Our reviewindicates that there are a handful of (commercial) Chatbots that are being usedfor patient support, while there are others (non-commercial) that are in theclinical trial phases. However, there is a lack of trust on this technologyregarding patient safety and data protection, as well as a lack of widerawareness on its benefits among the healthcare workers and professionals. Also,patients have expressed dissatisfaction with Natural Language Processing (NLP)skills of the Chatbots in comparison to humans. Notwithstanding the recentintroduction of ChatGPT that has raised the bar for the NLP technology, thisChatbot cannot be trusted with patient safety and medical ethics withoutthorough and rigorous checks to serve in the `narrow\u0026rsquo; domain of assistivehealthcare. Our review suggests that to enable deployment and integration ofAI-enabled Chatbots in public health services, the need of the hour is: tobuild technology that is simple and safe to use; to build confidence on thetechnology among: (a) the medical community by focussed training anddevelopment; (b) the patients and wider community through outreach.\rSimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\nYouyang Ng Daisuke Miyashita Yasuto Hoshi Yasuhiro Morioka Osamu Torii Tomoya Kodama Jun Deguchi\nabstract\rabstract: Large Language Model (LLM) based Generative AI systems have seen significantprogress in recent years. Integrating a knowledge retrieval architecture allowsfor seamless integration of private data into publicly available Generative AIsystems using pre-trained LLM without requiring additional model fine-tuning.Moreover, Retrieval-Centric Generation (RCG) approach, a promising futureresearch direction that explicitly separates roles of LLMs and retrievers incontext interpretation and knowledge memorization, potentially leads to moreefficient implementation. SimplyRetrieve is an open-source tool with the goalof providing a localized, lightweight, and user-friendly interface to thesesophisticated advancements to the machine learning community. SimplyRetrievefeatures a GUI and API based RCG platform, assisted by a Private Knowledge BaseConstructor and a Retrieval Tuning Module. By leveraging these capabilities,users can explore the potential of RCG for improving generative AI performancewhile maintaining privacy standards. The tool is available athttps://github.com/RCGAI/SimplyRetrieve with an MIT license.\rDegeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion\nZixuan Ni Longhui Wei Jiacheng Li Siliang Tang Yueting Zhuang Qi Tian\nabstract\rabstract: Owing to the unrestricted nature of the content in the training data, largetext-to-image diffusion models, such as Stable Diffusion (SD), are capable ofgenerating images with potentially copyrighted or dangerous content based oncorresponding textual concepts information. This includes specific intellectualproperty (IP), human faces, and various artistic styles. However, NegativePrompt, a widely used method for content removal, frequently fails to concealthis content due to inherent limitations in its inference logic. In this work,we propose a novel strategy named \\textbf{Degeneration-Tuning (DT)} to shieldcontents of unwanted concepts from SD weights. By utilizing Scrambled Grid toreconstruct the correlation between undesired concepts and their correspondingimage domain, we guide SD to generate meaningless content when such textualconcepts are provided as input. As this adaptation occurs at the level of themodel\u0026rsquo;s weights, the SD, after DT, can be grafted onto other conditionaldiffusion frameworks like ControlNet to shield unwanted concepts. In additionto qualitatively showcasing the effectiveness of our DT method in protectingvarious types of concepts, a quantitative comparison of the SD before and afterDT indicates that the DT method does not significantly impact the generativequality of other contents. The FID and IS scores of the model on COCO-30Kexhibit only minor changes after DT, shifting from 12.61 and 39.20 to 13.04 and38.25, respectively, which clearly outperforms the previous methods.\r2023-08-07\nFederated Representation Learning for Automatic Speech Recognition\nGuruprasad V Ramesh Gopinath Chennupati Milind Rao Anit Kumar Sahu Ariya Rastrow Jasha Droppo\nabstract\rabstract: Federated Learning (FL) is a privacy-preserving paradigm, allowing edgedevices to learn collaboratively without sharing data. Edge devices like Alexaand Siri are prospective sources of unlabeled audio data that can be tapped tolearn robust audio representations. In this work, we bring Self-supervisedLearning (SSL) and FL together to learn representations for Automatic SpeechRecognition respecting data privacy constraints. We use the speaker and chapterinformation in the unlabeled speech dataset, Libri-Light, to simulate non-IIDspeaker-siloed data distributions and pre-train an LSTM encoder with theContrastive Predictive Coding framework with FedSGD. We show that thepre-trained ASR encoder in FL performs as well as a centrally pre-trained modeland produces an improvement of 12-15% (WER) compared to no pre-training. Wefurther adapt the federated pre-trained models to a new language, French, andshow a 20% (WER) improvement over no pre-training.\rLabeling without Seeing? Blind Annotation for Privacy-Preserving Entity Resolution\nYixiang Yao Weizhao Jin Srivatsan Ravi\nabstract\rabstract: The entity resolution problem requires finding pairs across datasets thatbelong to different owners but refer to the same entity in the real world. Totrain and evaluate solutions (either rule-based or machine-learning-based) tothe entity resolution problem, generating a ground truth dataset with entitypairs or clusters is needed. However, such a data annotation process involveshumans as domain oracles to review the plaintext data for all candidate recordpairs from different parties, which inevitably infringes the privacy of dataowners, especially in privacy-sensitive cases like medical records. To the bestof our knowledge, there is no prior work on privacy-preserving ground truthdataset generation, especially in the domain of entity resolution. We propose anovel blind annotation protocol based on homomorphic encryption that allowsdomain oracles to collaboratively label ground truths without sharing data inplaintext with other parties. In addition, we design a domain-specificeasy-to-use language that hides the sophisticated underlying homomorphicencryption layer. Rigorous proof of the privacy guarantee is provided and ourempirical experiments via an annotation simulator indicate the feasibility ofour privacy-preserving protocol (f-measure on average achieves more than 90%compared with the real ground truths).\rTableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT\nLiangyu Zha Junlin Zhou Liyao Li Rui Wang Qingyi Huang Saisai Yang Jing Yuan Changbao Su Xiang Li Aofeng Su Tao Zhang Chen Zhou Kaizhe Shou Miao Wang Wufang Zhu Guoshan Lu Chao Ye Yali Ye Wentao Ye Yiming Zhang Xinglong Deng Jie Xu Haobo Wang Gang Chen Junbo Zhao\nabstract\rabstract: Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework\u0026rsquo;s adaptability tospecific use cases.\rCuing Without Sharing: A Federated Cued Speech Recognition Framework via Mutual Knowledge Distillation\nYuxuan Zhang Lei Liu Li Liu\nabstract\rabstract: Cued Speech (CS) is a visual coding tool to encode spoken languages at thephonetic level, which combines lip-reading and hand gestures to effectivelyassist communication among people with hearing impairments. The Automatic CSRecognition (ACSR) task aims to recognize CS videos into linguistic texts,which involves both lips and hands as two distinct modalities conveyingcomplementary information. However, the traditional centralized trainingapproach poses potential privacy risks due to the use of facial and gesturevideos in CS data. To address this issue, we propose a new Federated CuedSpeech Recognition (FedCSR) framework to train an ACSR model over thedecentralized CS data without sharing private information. In particular, amutual knowledge distillation method is proposed to maintain cross-modalsemantic consistency of the Non-IID CS data, which ensures learning a unifiedfeature space for both linguistic and visual information. On the server side, aglobally shared linguistic model is trained to capture the long-termdependencies in the text sentences, which is aligned with the visualinformation from the local clients via visual-to-linguistic distillation. Onthe client side, the visual model of each client is trained with its own localdata, assisted by linguistic-to-visual distillation treating the linguisticmodel as the teacher. To the best of our knowledge, this is the first approachto consider the federated ACSR task for privacy protection. Experimentalresults on the Chinese CS dataset with multiple cuers demonstrate that ourapproach outperforms both mainstream federated learning baselines and existingcentralized state-of-the-art ACSR methods, achieving 9.7% performanceimprovement for character error rate (CER) and 15.0% for word error rate (WER).\rMembership Inference Attacks against Language Models via Neighbourhood Comparison\nJustus Mattern Fatemehsadat Mireshghallah Zhijing Jin Bernhard Schölkopf Mrinmaya Sachan Taylor Berg-Kirkpatrick\nabstract\rabstract: Membership Inference attacks (MIAs) aim to predict whether a data sample waspresent in the training data of a machine learning model or not, and are widelyused for assessing the privacy risks of language models. Most existing attacksrely on the observation that models tend to assign higher probabilities totheir training samples than non-training points. However, simple thresholdingof the model score in isolation tends to lead to high false-positive rates asit does not account for the intrinsic complexity of a sample. Recent work hasdemonstrated that reference-based attacks which compare model scores to thoseobtained from a reference model trained on similar data can substantiallyimprove the performance of MIAs. However, in order to train reference models,attacks of this kind make the strong and arguably unrealistic assumption thatan adversary has access to samples closely resembling the original trainingdata. Therefore, we investigate their performance in more realistic scenariosand find that they are highly fragile in relation to the data distribution usedto train reference models. To investigate whether this fragility provides alayer of safety, we propose and evaluate neighbourhood attacks, which comparemodel scores for a given sample to scores of synthetically generated neighbourtexts and therefore eliminate the need for access to the training datadistribution. We show that, in addition to being competitive withreference-based attacks that have perfect knowledge about the training datadistribution, our attack clearly outperforms existing reference-free attacks aswell as reference-based attacks with imperfect knowledge, which demonstratesthe need for a reevaluation of the threat model of adversarial attacks.\r2023-08-06\nInvisible Image Watermarks Are Provably Removable Using Generative AI\nXuandong Zhao Kexun Zhang Zihao Su Saastha Vasan Ilya Grishchenko Christopher Kruegel Giovanni Vigna Yu-Xiang Wang Lei Li\nabstract\rabstract: Invisible watermarks safeguard images\u0026rsquo; copyright by embedding hidden messagesonly detectable by owners. They also prevent people from misusing images,especially those generated by AI models. We propose a family of regenerationattacks to remove these invisible watermarks. The proposed attack method firstadds random noise to an image to destroy the watermark and then reconstructsthe image. This approach is flexible and can be instantiated with many existingimage-denoising algorithms and pre-trained generative models such as diffusionmodels. Through formal proofs and empirical results, we show that all invisiblewatermarks are vulnerable to the proposed attack. For a particularly resilientwatermark, RivaGAN, regeneration attacks remove 93-99% of the invisiblewatermarks while the baseline attacks remove no more than 3%. However, if we donot require the watermarked image to look the same as the original one,watermarks that keep the image semantically similar can be an alternativedefense against our attack. Our finding underscores the need for a shift inresearch/industry emphasis from invisible watermarks to semantically similarones. Code is available at https://github.com/XuandongZhao/WatermarkAttacker.\r2023-08-05\nLarge Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching\nJiayi Yuan Ruixiang Tang Xiaoqian Jiang Xia Hu\nabstract\rabstract: The process of matching patients with suitable clinical trials is essentialfor advancing medical research and providing optimal care. However, currentapproaches face challenges such as data standardization, ethicalconsiderations, and a lack of interoperability between Electronic HealthRecords (EHRs) and clinical trial criteria. In this paper, we explore thepotential of large language models (LLMs) to address these challenges byleveraging their advanced natural language generation capabilities to improvecompatibility between EHRs and clinical trial descriptions. We propose aninnovative privacy-aware data augmentation approach for LLM-based patient-trialmatching (LLM-PTM), which balances the benefits of LLMs while ensuring thesecurity and confidentiality of sensitive patient data. Our experimentsdemonstrate a 7.32% average improvement in performance using the proposedLLM-PTM method, and the generalizability to new data is improved by 12.12%.Additionally, we present case studies to further illustrate the effectivenessof our approach and provide a deeper understanding of its underlyingprinciples.\r2023-08-03\nGradual Sensitivity Typing\nDamian Arquez Matías Toro Éric Tanter\nabstract\rabstract: Reasoning about the sensitivity of functions with respect to their inputs hasinteresting applications in various areas, such as differential privacy. Inorder to check and enforce sensitivity, several approaches have been developed,notably sensitivity type systems. In these systems, sensitivity can be seen asan effect in the sense of type-and-effects systems as originally proposed byGifford and Lucassen. Because type-and-effect systems can make certain usefulprogramming patterns tedious or overly conservative, there is value in bringingthe benefits of gradual typing to these disciplines in order to ease theiradoption. In this work, we motivate, formalize, and prototype gradualsensitivity typing. The language GSoul supports both the unrestricted unknownsensitivity and bounded imprecision in the form of intervals. Gradualsensitivity typing allows programmers to smoothly evolve typed programs withoutany static sensitivity information towards hardened programs with a mix ofstatic and dynamic sensitivity checking. In particular, we show that gradualsensitivity supports recursive functions for which fully static checking wouldbe overly conservative, seamlessly enabling exact runtime sensitivity checks.GSoul satisfies both the gradual guarantees and sensitivity type soundness,known as metric preservation. We establish that, in general, gradual metricpreservation is termination insensitive, and that one can achievetermination-sensitive gradual metric preservation by hardening specificationsto bounded imprecision. We implement a prototype that provides an interactivetest bed for gradual sensitivity typing. This work opens the door togradualizing other typing disciplines that rely on function sensitivity such asdifferential privacy, as well as other quantitative type-based reasoningtechniques.\rMapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity Challenges: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis\nMaya Karanouh\nabstract\rabstract: The exponential growth in user acquisition and popularity of OpenAIs ChatGPT,an artificial intelligence(AI) powered chatbot, was accompanied by widespreadmainstream media coverage. This article presents a quantitative data analysisof the early trends and sentiments revealed by conducting text mining and NLPmethods onto a corpus of 10,902 mainstream news headlines related to thesubject of ChatGPT and artificial intelligence, from the launch of ChatGPT inNovember 2022 to March 2023. The findings revealed in sentiment analysis,ChatGPT and artificial intelligence, were perceived more positively thannegatively in the mainstream media. In regards to word frequency results, oversixty-five percent of the top frequency words were focused on Big Tech issuesand actors while topics such as jobs, diversity, ethics, copyright, gender andwomen were poorly represented or completely absent and only accounted for sixpercent of the total corpus. This article is a critical analysis into the powerstructures and collusions between Big Tech and Big Media in their hegemonicexclusion of diversity and job challenges from mainstream media.\rMusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies\nKe Chen Yusong Wu Haohe Liu Marianna Nezhurina Taylor Berg-Kirkpatrick Shlomo Dubnov\nabstract\rabstract: Diffusion models have shown promising results in cross-modal generationtasks, including text-to-image and text-to-audio generation. However,generating music, as a special type of audio, presents unique challenges due tolimited availability of music data and sensitive issues related to copyrightand plagiarism. In this paper, to tackle these challenges, we first construct astate-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusionand AudioLDM architectures to the music domain. We achieve this by retrainingthe contrastive language-audio pretraining model (CLAP) and the Hifi-GANvocoder, as components of MusicLDM, on a collection of music data samples.Then, to address the limitations of training data and to avoid plagiarism, weleverage a beat tracking model and propose two different mixup strategies fordata augmentation: beat-synchronous audio mixup and beat-synchronous latentmixup, which recombine training audio directly or via a latent embeddingsspace, respectively. Such mixup strategies encourage the model to interpolatebetween musical training samples and generate new music within the convex hullof the training data, making the generated music more diverse while stillstaying faithful to the corresponding style. In addition to popular evaluationmetrics, we design several new evaluation metrics based on CLAP score todemonstrate that our proposed MusicLDM and beat-synchronous mixup strategiesimprove both the quality and novelty of generated music, as well as thecorrespondence between input text and generated music.\r2023-08-02\nA Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards\nJoshua Harrison Ehsan Toreini Maryam Mehrnezhad\nabstract\rabstract: With recent developments in deep learning, the ubiquity of micro-phones andthe rise in online services via personal devices, acoustic side channel attackspresent a greater threat to keyboards than ever. This paper presents apractical implementation of a state-of-the-art deep learning model in order toclassify laptop keystrokes, using a smartphone integrated microphone. Whentrained on keystrokes recorded by a nearby phone, the classifier achieved anaccuracy of 95%, the highest accuracy seen without the use of a language model.When trained on keystrokes recorded using the video-conferencing software Zoom,an accuracy of 93% was achieved, a new best for the medium. Our results provethe practicality of these side channel attacks via off-the-shelf equipment andalgorithms. We discuss a series of mitigation methods to protect users againstthese series of attacks.\r2023-07-31\nPerceptions of the Fourth Industrial Revolution and Artificial Intelligence Impact on Society\nDaniel Agbaji Brady Lund Nishith Reddy Mannuru\nabstract\rabstract: The Fourth Industrial Revolution, particularly Artificial Intelligence (AI),has had a profound impact on society, raising concerns about its implicationsand ethical considerations. The emergence of text generative AI tools likeChatGPT has further intensified concerns regarding ethics, security, privacy,and copyright. This study aims to examine the perceptions of individuals indifferent information flow categorizations toward AI. The results reveal keythemes in participant-supplied definitions of AI and the fourth industrialrevolution, emphasizing the replication of human intelligence, machinelearning, automation, and the integration of digital technologies. Participantsexpressed concerns about job replacement, privacy invasion, and inaccurateinformation provided by AI. However, they also recognized the benefits of AI,such as solving complex problems and increasing convenience. Views ongovernment involvement in shaping the fourth industrial revolution varied, withsome advocating for strict regulations and others favoring support anddevelopment. The anticipated changes brought by the fourth industrialrevolution include automation, potential job impacts, increased socialdisconnect, and reliance on technology. Understanding these perceptions iscrucial for effectively managing the challenges and opportunities associatedwith AI in the evolving digital landscape.\rAMOE: a Tool to Automatically Extract and Assess Organizational Evidence for Continuous Cloud Audit\nFranz Deimling Michela Fazzolari\nabstract\rabstract: The recent spread of cloud services has enabled many companies to takeadvantage of them. Nevertheless, the main concern about the adoption of cloudservices remains the lack of transparency perceived by customers regardingsecurity and privacy. To overcome this issue, Cloud Service Certifications(CSCs) have emerged as an effective solution to increase the level of trust incloud services, possibly based on continuous auditing to monitor and evaluatethe security of cloud services on an ongoing basis. Continuous auditing can beeasily implemented for technical aspects, while organizational aspects can bechallenging due to their generic nature and varying policies between serviceproviders. In this paper, we propose an approach to facilitate the automaticassessment of organizational evidence, such as that extracted from securitypolicy documents. The evidence extraction process is based on Natural LanguageProcessing (NLP) techniques, in particular on Question Answering (QA). Theimplemented prototype provides promising results on an annotated dataset, sinceit is capable to retrieve the correct answer for more than half of the testedmetrics. This prototype can be helpful for Cloud Service Providers (CSPs) toautomate the auditing of textual policy documents and to help in reducing thetime required by auditors to check policy documents.\rChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model\nHanyao Huang Ou Zheng Dongdong Wang Jiayi Yin Zijin Wang Shengxuan Ding Heng Yin Chuan Xu Renjie Yang Qian Zheng Bing Shi\nabstract\rabstract: The ChatGPT, a lite and conversational variant of Generative PretrainedTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone LargeLanguage Models (LLMs) with billions of parameters. LLMs have stirred up muchinterest among researchers and practitioners in their impressive skills innatural language processing tasks, which profoundly impact various fields. Thispaper mainly discusses the future applications of LLMs in dentistry. Weintroduce two primary LLM deployment methods in dentistry, including automateddental diagnosis and cross-modal dental diagnosis, and examine their potentialapplications. Especially, equipped with a cross-modal encoder, a single LLM canmanage multi-source data and conduct advanced natural language reasoning toperform complex clinical operations. We also present cases to demonstrate thepotential of a fully automatic Multi-Modal LLM AI system for dentistry clinicalapplication. While LLMs offer significant potential benefits, the challenges,such as data privacy, data quality, and model bias, need further study.Overall, LLMs have the potential to revolutionize dental diagnosis andtreatment, which indicates a promising avenue for clinical application andresearch in dentistry.\rFair Algorithms for Hierarchical Agglomerative Clustering\nAnshuman Chhabra Prasant Mohapatra\nabstract\rabstract: Hierarchical Agglomerative Clustering (HAC) algorithms are extensivelyutilized in modern data science, and seek to partition the dataset intoclusters while generating a hierarchical relationship between the data samples.HAC algorithms are employed in many applications, such as biology, naturallanguage processing, and recommender systems. Thus, it is imperative to ensurethat these algorithms are fair \u0026ndash; even if the dataset contains biases againstcertain protected groups, the cluster outputs generated should not discriminateagainst samples from any of these groups. However, recent work in clusteringfairness has mostly focused on center-based clustering algorithms, such ask-median and k-means clustering. In this paper, we propose fair algorithms forperforming HAC that enforce fairness constraints 1) irrespective of thedistance linkage criteria used, 2) generalize to any natural measures ofclustering fairness for HAC, 3) work for multiple protected groups, and 4) havecompetitive running times to vanilla HAC. Through extensive experiments onmultiple real-world UCI datasets, we show that our proposed algorithm findsfairer clusterings compared to vanilla HAC as well as other state-of-the-artfair clustering approaches.\r2023-07-30\nA Review of Media Copyright Management using Blockchain Technologies from the Academic and Business Perspectives\nRoberto García Ana Cediel Mercè Teixidó Rosa Gil\nabstract\rabstract: Blockchain technologies open new opportunities for media copyrightmanagement. To provide an overview of the main initiatives in this blockchainapplication area, we have first reviewed the existing academic literature. Thereview shows literature is still scarce and immature in many aspects, which ismore evident when comparing it to initiatives coming from the industry.Blockchain has been receiving significant inflows of venture capital andcrowdfunding, which have boosted its progress in many fields, including itsapplication to media management. Consequently, we have complemented the reviewwith a business perspective. Existing reports about blockchain and media havebeen studied and consolidated into four prominent use cases. Moreover, each onehas been illustrated through existing businesses already exploring them.Combining the academic and industry perspectives, we provide a more general andcomplete overview of current trends in media copyright management usingblockchain technologies.\r2023-07-28\nHolistic Survey of Privacy and Fairness in Machine Learning\nSina Shaham Arash Hajisafi Minh K Quan Dinh C Nguyen Bhaskar Krishnamachari Charith Peris Gabriel Ghinita Cyrus Shahabi Pubudu N. Pathirana\nabstract\rabstract: Privacy and fairness are two crucial pillars of responsible ArtificialIntelligence (AI) and trustworthy Machine Learning (ML). Each objective hasbeen independently studied in the literature with the aim of reducing utilityloss in achieving them. Despite the significant interest attracted from bothacademia and industry, there remains an immediate demand for more in-depthresearch to unravel how these two objectives can be simultaneously integratedinto ML models. As opposed to well-accepted trade-offs, i.e., privacy-utilityand fairness-utility, the interrelation between privacy and fairness is notwell-understood. While some works suggest a trade-off between the two objectivefunctions, there are others that demonstrate the alignment of these functionsin certain scenarios. To fill this research gap, we provide a thorough reviewof privacy and fairness in ML, including supervised, unsupervised,semi-supervised, and reinforcement learning. After examining and consolidatingthe literature on both objectives, we present a holistic survey on the impactof privacy on fairness, the impact of fairness on privacy, existingarchitectures, their interaction in application domains, and algorithms thataim to achieve both objectives while minimizing the utility sacrificed.Finally, we identify research challenges in achieving privacy and fairnessconcurrently in ML, particularly focusing on large language models.\rMean Estimation with User-level Privacy under Data Heterogeneity\nRachel Cummings Vitaly Feldman Audra McMillan Kunal Talwar\nabstract\rabstract: A key challenge in many modern data analysis tasks is that user data areheterogeneous. Different users may possess vastly different numbers of datapoints. More importantly, it cannot be assumed that all users sample from thesame underlying distribution. This is true, for example in language data, wheredifferent speech styles result in data heterogeneity. In this work we propose asimple model of heterogeneous user data that allows user data to differ in bothdistribution and quantity of data, and provide a method for estimating thepopulation-level mean while preserving user-level differential privacy. Wedemonstrate asymptotic optimality of our estimator and also prove general lowerbounds on the error achievable in the setting we introduce.\rLessons in Reproducibility: Insights from NLP Studies in Materials Science\nXiangyun Lei Edward Kim Viktoriia Baibakova Shijing Sun\nabstract\rabstract: Natural Language Processing (NLP), a cornerstone field within artificialintelligence, has been increasingly utilized in the field of materials scienceliterature. Our study conducts a reproducibility analysis of two pioneeringworks within this domain: \u0026ldquo;Machine-learned and codified synthesis parameters ofoxide materials\u0026rdquo; by Kim et al., and \u0026ldquo;Unsupervised word embeddings capturelatent knowledge from materials science literature\u0026rdquo; by Tshitoyan et al. We aimto comprehend these studies from a reproducibility perspective, acknowledgingtheir significant influence on the field of materials informatics, rather thancritiquing them. Our study indicates that both papers offered thoroughworkflows, tidy and well-documented codebases, and clear guidance for modelevaluation. This makes it easier to replicate their results successfully andpartially reproduce their findings. In doing so, they set commendable standardsfor future materials science publications to aspire to. However, our analysisalso highlights areas for improvement such as to provide access to trainingdata where copyright restrictions permit, more transparency on modelarchitecture and the training process, and specifications of softwaredependency versions. We also cross-compare the word embedding models betweenpapers, and find that some key differences in reproducibility andcross-compatibility are attributable to design choices outside the bounds ofthe models themselves. In summary, our study appreciates the benchmark set bythese seminal papers while advocating for further enhancements in researchreproducibility practices in the field of NLP for materials science. Thisbalance of understanding and continuous improvement will ultimately propel theintersecting domains of NLP and materials science literature into a future ofexciting discoveries.\rRAWIW: RAW Image Watermarking Robust to ISP Pipeline\nKang Fu Xiaohong Liu Jun Jia Zicheng Zhang Yicong Peng Jia Wang Guangtao Zhai\nabstract\rabstract: Invisible image watermarking is essential for image copyright protection.Compared to RGB images, RAW format images use a higher dynamic range to capturethe radiometric characteristics of the camera sensor, providing greaterflexibility in post-processing and retouching. Similar to the master recordingin the music industry, RAW images are considered the original format fordistribution and image production, thus requiring copyright protection.Existing watermarking methods typically target RGB images, leaving a gap forRAW images. To address this issue, we propose the first deep learning-based RAWImage Watermarking (RAWIW) framework for copyright protection. Unlike RGB imagewatermarking, our method achieves cross-domain copyright protection. Wedirectly embed copyright information into RAW images, which can be laterextracted from the corresponding RGB images generated by differentpost-processing methods. To achieve end-to-end training of the framework, weintegrate a neural network that simulates the ISP pipeline to handle theRAW-to-RGB conversion process. To further validate the generalization of ourframework to traditional ISP pipelines and its robustness to transmissiondistortion, we adopt a distortion network. This network simulates various typesof noises introduced during the traditional ISP pipeline and transmission.Furthermore, we employ a three-stage training strategy to strike a balancebetween robustness and concealment of watermarking. Our extensive experimentsdemonstrate that RAWIW successfully achieves cross-domain copyright protectionfor RAW images while maintaining their visual quality and robustness to ISPpipeline distortions.\r2023-07-27\nTowards Regulated Deep Learning\nAndrés García-Camino\nabstract\rabstract: Regulation of Multi-Agent Systems (MAS) and Declarative ElectronicInstitutions (DEIs) was a multidisciplinary research topic of the past decadeinvolving (Physical and Software) Agents and Law since the beginning, butrecently evolved towards News-claimed Robot Lawyer since 2016. One of thesefirst proposals of restricting the behaviour of Software Agents was ElectronicInstitutions.However, with the recent reformulation of Artificial NeuralNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legalissues regarding the use of DL has raised concerns in the ArtificialIntelligence (AI) Community. Now that the Regulation of MAS is almost correctlyaddressed, we propose the Regulation of Artificial Neural Networks asAgent-based Training of a special type of regulated Artificial Neural Networkthat we call Institutional Neural Network (INN).The main purpose of this paperis to bring attention to Artificial Teaching (AT) and to give a tentativeanswer showing a proof-of-concept implementation of Regulated Deep Learning(RDL). This paper introduces the former concept and provide $I^*$, a languagepreviously used to model declaratively and extend Electronic Institutions, as ameans to regulate the execution of Artificial Neural Networks and theirinteractions with Artificial Teachers (ATs)\r2023-07-26\nUnveiling Security, Privacy, and Ethical Concerns of ChatGPT\nXiaodong Wu Ran Duan Jianbing Ni\nabstract\rabstract: This paper delves into the realm of ChatGPT, an AI-powered chatbot thatutilizes topic modeling and reinforcement learning to generate naturalresponses. Although ChatGPT holds immense promise across various industries,such as customer service, education, mental health treatment, personalproductivity, and content creation, it is essential to address its security,privacy, and ethical implications. By exploring the upgrade path from GPT-1 toGPT-4, discussing the model\u0026rsquo;s features, limitations, and potentialapplications, this study aims to shed light on the potential risks ofintegrating ChatGPT into our daily lives. Focusing on security, privacy, andethics issues, we highlight the challenges these concerns pose for widespreadadoption. Finally, we analyze the open problems in these areas, calling forconcerted efforts to ensure the development of secure and ethically sound largelanguage models.\r2023-07-25\nNot with my name! Inferring artists\u0026rsquo; names of input strings employed by Diffusion Models\nRoberto Leotta Oliver Giudice Luca Guarnera Sebastiano Battiato\nabstract\rabstract: Diffusion Models (DM) are highly effective at generating realistic,high-quality images. However, these models lack creativity and merely composeoutputs based on their training data, guided by a textual input provided atcreation time. Is it acceptable to generate images reminiscent of an artist,employing his name as input? This imply that if the DM is able to replicate anartist\u0026rsquo;s work then it was trained on some or all of his artworks thus violatingcopyright. In this paper, a preliminary study to infer the probability of useof an artist\u0026rsquo;s name in the input string of a generated image is presented. Tothis aim we focused only on images generated by the famous DALL-E 2 andcollected images (both original and generated) of five renowned artists.Finally, a dedicated Siamese Neural Network was employed to have a first kindof probability. Experimental results demonstrate that our approach is anoptimal starting point and can be employed as a prior for predicting a completeinput string of an investigated image. Dataset and code are available at:https://github.com/ictlab-unict/not-with-my-name .\rA short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM\nCarlos Ferreira\nabstract\rabstract: Artificial Intelligence is not a new subject, and business, industry andpublic sectors have used it in different ways and contexts and consideringmultiple concerns. This work reviewed research papers published in ACM DigitalLibrary and IEEE Xplore conference proceedings in the last two years supportedby fundamental concepts of Natural Language Processing (NLP) and Text Mining(TM). The objective was to capture insights regarding data privacy, ethics,interpretability, explainability, trustworthiness, and fairness in the publicsector. The methodology has saved analysis time and could retrieve paperscontaining relevant information. The results showed that fairness was the mostfrequent concern. The least prominent topic was data privacy (although embeddedin most articles), while the most prominent was trustworthiness. Finally,gathering helpful insights about those concerns regarding A.I. applications inthe public sector was also possible.\rMultilevel Large Language Models for Everyone\nYuanhao Gong\nabstract\rabstract: Large language models have made significant progress in the past few years.However, they are either generic {\\it or} field specific, splitting thecommunity into different groups. In this paper, we unify these large languagemodels into a larger map, where the generic {\\it and} specific models arelinked together and can improve each other, based on the user personal inputand information from the internet. The idea of linking several large languagemodels together is inspired by the functionality of human brain. The specificregions on the brain cortex are specific for certain low level functionality.And these regions can jointly work together to achieve more complex high levelfunctionality. Such behavior on human brain cortex sheds the light to designthe multilevel large language models that contain global level, field level anduser level models. The user level models run on local machines to achieveefficient response and protect the user\u0026rsquo;s privacy. Such multilevel modelsreduce some redundancy and perform better than the single level models. Theproposed multilevel idea can be applied in various applications, such asnatural language processing, computer vision tasks, professional assistant,business and healthcare.\r2023-07-22\nSecurity and Privacy Issues of Federated Learning\nJahid Hasan\nabstract\rabstract: Federated Learning (FL) has emerged as a promising approach to address dataprivacy and confidentiality concerns by allowing multiple participants toconstruct a shared model without centralizing sensitive data. However, thisdecentralized paradigm introduces new security challenges, necessitating acomprehensive identification and classification of potential risks to ensureFL\u0026rsquo;s security guarantees. This paper presents a comprehensive taxonomy ofsecurity and privacy challenges in Federated Learning (FL) across variousmachine learning models, including large language models. We specificallycategorize attacks performed by the aggregator and participants, focusing onpoisoning attacks, backdoor attacks, membership inference attacks, generativeadversarial network (GAN) based attacks, and differential privacy attacks.Additionally, we propose new directions for future research, seeking innovativesolutions to fortify FL systems against emerging security risks and upholdsensitive data confidentiality in distributed learning environments.\rPractical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review\nLixiang Yan Lele Sha Linxuan Zhao Yuheng Li Roberto Martinez-Maldonado Guanliang Chen Xinyu Li Yueqiao Jin Dragan Gašević\nabstract\rabstract: Educational technology innovations leveraging large language models (LLMs)have shown the potential to automate the laborious process of generating andanalysing textual content. While various innovations have been developed toautomate a range of educational tasks (e.g., question generation, feedbackprovision, and essay grading), there are concerns regarding the practicalityand ethicality of these innovations. Such concerns may hinder future researchand the adoption of LLMs-based innovations in authentic educational contexts.To address this, we conducted a systematic scoping review of 118 peer-reviewedpapers published since 2017 to pinpoint the current state of research on usingLLMs to automate and support educational tasks. The findings revealed 53 usecases for LLMs in automating education tasks, categorised into nine maincategories: profiling/labelling, detection, grading, teaching support,prediction, knowledge representation, feedback, content generation, andrecommendation. Additionally, we also identified several practical and ethicalchallenges, including low technological readiness, lack of replicability andtransparency, and insufficient privacy and beneficence considerations. Thefindings were summarised into three recommendations for future studies,including updating existing innovations with state-of-the-art models (e.g.,GPT-3/4), embracing the initiative of open-sourcing models/systems, andadopting a human-centred approach throughout the developmental process. As theintersection of AI and education is continuously evolving, the findings of thisstudy can serve as an essential reference point for researchers, allowing themto leverage the strengths, learn from the limitations, and uncover potentialresearch opportunities enabled by ChatGPT and other generative AI models.\r2023-07-21\nBibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing\nConner Ganjavi Michael B. Eppler Asli Pekcan Brett Biedermann Andre Abreu Gary S. Collins Inderbir S. Gill Giovanni E. Cacciamani\nabstract\rabstract: We aim to determine the extent and content of guidance for authors regardingthe use of generative-AI (GAI), Generative Pretrained models (GPTs) and LargeLanguage Models (LLMs) powered tools among the top 100 academic publishers andjournals in science. The websites of these publishers and journals werescreened from between 19th and 20th May 2023. Among the largest 100 publishers,17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top25 publishers. Among the top 100 journals, 70% have provided guidance on GAI.Of those with guidance, 94.1% of publishers and 95.7% of journals prohibitedthe inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit theuse of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15(21.4%) journals indicated their guidance exclusively applies to the writingprocess. When disclosing the use of GAI, 42.8% of publishers and 44.3% ofjournals included specific disclosure criteria. There was variability inguidance of where to disclose the use of GAI, including in the methods,acknowledgments, cover letter, or a new section. There was also variability inhow to access GAI guidance and the linking of journal and publisherinstructions to authors. There is a lack of guidance by some top publishers andjournals on the use of GAI by authors. Among those publishers and journals thatprovide guidance, there is substantial heterogeneity in the allowable uses ofGAI and in how it should be disclosed, with this heterogeneity persisting amongaffiliated publishers and journals in some instances. The lack ofstandardization burdens authors and threatens to limit the effectiveness ofthese regulations. There is a need for standardized guidelines in order toprotect the integrity of scientific output as GAI continues to grow inpopularity.\rProject Florida: Federated Learning Made Easy\nDaniel Madrigal Diaz Andre Manoel Jialei Chen Nalin Singal Robert Sim\nabstract\rabstract: We present Project Florida, a system architecture and software developmentkit (SDK) enabling deployment of large-scale Federated Learning (FL) solutionsacross a heterogeneous device ecosystem. Federated learning is an approach tomachine learning based on a strong data sovereignty principle, i.e., thatprivacy and security of data is best enabled by storing it at its origin,whether on end-user devices or in segregated cloud storage silos. Federatedlearning enables model training across devices and silos while the trainingdata remains within its security boundary, by distributing a model snapshot toa client running inside the boundary, running client code to update the model,and then aggregating updated snapshots across many clients in a centralorchestrator. Deploying a FL solution requires implementation of complexprivacy and security mechanisms as well as scalable orchestrationinfrastructure. Scale and performance is a paramount concern, as the modeltraining process benefits from full participation of many client devices, whichmay have a wide variety of performance characteristics. Project Florida aims tosimplify the task of deploying cross-device FL solutions by providingcloud-hosted infrastructure and accompanying task management interfaces, aswell as a multi-platform SDK supporting most major programming languagesincluding C++, Java, and Python, enabling FL training across a wide range ofoperating system (OS) and hardware specifications. The architecture decouplesservice management from the FL workflow, enabling a cloud service provider todeliver FL-as-a-service (FLaaS) to ML engineers and application developers. Wepresent an overview of Florida, including a description of the architecture,sample code, and illustrative experiments demonstrating system capabilities.\rOn Provable Copyright Protection for Generative Models\nNikhil Vyas Sham Kakade Boaz Barak\nabstract\rabstract: There is a growing concern that learned conditional generative models mayoutput samples that are substantially similar to some copyrighted data $C$ thatwas in their training set. We give a formal definition of $\\textit{nearaccess-freeness (NAF)}$ and prove bounds on the probability that a modelsatisfying this definition outputs a sample similar to $C$, even if $C$ isincluded in its training set. Roughly speaking, a generative model $p$ is$\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of$p$ diverges by at most $k$-bits from the output of a model $q$ that$\\textit{did not access $C$ at all}$. We also give generative model learningalgorithms, which efficiently modify the original generative model learningalgorithm in a black box manner, that output generative models with strongbounds on the probability of sampling protected content. Furthermore, weprovide promising experiments for both language (transformers) and image(diffusion) generative models, showing minimal degradation in output qualitywhile ensuring strong protections against sampling protected content.\rContinual Learning for Abdominal Multi-Organ and Tumor Segmentation\nYixiao Zhang Xinyi Li Huimiao Chen Alan Yuille Yaoyao Liu Zongwei Zhou\nabstract\rabstract: The ability to dynamically extend a model to new data and classes is criticalfor multiple organ and tumor segmentation. However, due to privacy regulations,accessing previous data and annotations can be problematic in the medicaldomain. This poses a significant barrier to preserving the high segmentationaccuracy of the old classes when learning from new classes because of thecatastrophic forgetting problem. In this paper, we first empiricallydemonstrate that simply using high-quality pseudo labels can fairly mitigatethis problem in the setting of organ segmentation. Furthermore, we put forwardan innovative architecture designed specifically for continuous organ and tumorsegmentation, which incurs minimal computational overhead. Our proposed designinvolves replacing the conventional output layer with a suite of lightweight,class-specific heads, thereby offering the flexibility to accommodate newlyemerging classes. These heads enable independent predictions for newlyintroduced and previously learned classes, effectively minimizing the impact ofnew classes on old ones during the course of continual learning. We furtherpropose incorporating Contrastive Language-Image Pretraining (CLIP) embeddingsinto the organ-specific heads. These embeddings encapsulate the semanticinformation of each class, informed by extensive image-text co-training. Theproposed method is evaluated on both in-house and public abdominal CT datasetsunder organ and tumor segmentation tasks. Empirical results suggest that theproposed design improves the segmentation performance of a baseline neuralnetwork on newly-introduced and previously-learned classes along the learningtrajectory.\r2023-07-20\nExploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots\nAdvait Sarkar\nabstract\rabstract: Artificial Intelligence (AI), and in particular generative models, aretransformative tools for knowledge work. They problematise notions ofcreativity, originality, plagiarism, the attribution of credit, and copyrightownership. Critics of generative models emphasise the reliance on large amountsof training data, and view the output of these models as no more thanrandomised plagiarism, remix, or collage of the source data. On these grounds,many have argued for stronger regulations on the deployment, use, andattribution of the output of these models. However, these issues are not new orunique to artificial intelligence. In this position paper, using examples fromliterary criticism, the history of art, and copyright law, I show howcreativity and originality resist definition as a notatable orinformation-theoretic property of an object, and instead can be seen as theproperty of a process, an author, or a viewer. Further alternative views holdthat all creative work is essentially reuse (mostly without attribution), orthat randomness itself can be creative. I suggest that creativity is ultimatelydefined by communities of creators and receivers, and the deemed sources ofcreativity in a workflow often depend on which parts of the workflow can beautomated. Using examples from recent studies of AI in creative knowledge work,I suggest that AI shifts knowledge work from material production to criticalintegration. This position paper aims to begin a conversation around a morenuanced approach to the problems of creativity and credit assignment forgenerative models, one which more fully recognises the importance of thecreative and curatorial voice of the users of these models and moves away fromsimpler notational or information-theoretic views.\rPatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation\nLe Xiao Xin Shan\nabstract\rabstract: Large language models(LLMS)have shown excellent text generation capabilities,capable of generating fluent human-like responses for many downstream tasks.However, applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To cope with the above challenges, this paperproposes PatternGPT, a pattern-driven text generation framework for LargeLanguage Models. Firstly, the framework utilizes the extraction capability ofLarge Language Models to generate rich and diversified structured andformalized patterns, which facilitates the introduction of external knowledgeto do the computation, and then draws on the idea of federated learning to usemultiple agents to achieve the sharing in order to obtain more diversifiedpatterns, and finally uses judgment criteria and optimization algorithm tosearch for high-quality patterns to guide the generation of models. Finally,external knowledge such as judgment criteria and optimization algorithms areused to search for high-quality patterns, and the searched patterns are used toguide model generation. This framework has the advantages of generatingdiversified patterns, protecting data privacy, combining external knowledge,and improving the quality of generation, which provides an effective method tooptimize the text generation capability of large language models, and make itbetter applied to the field of intelligent dialogue and content generation.\r2023-07-19\nWhat can we learn from Data Leakage and Unlearning for Law?\nJaydeep Borkar\nabstract\rabstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users\u0026rsquo; PII.In order to comply with privacy laws such as the \u0026ldquo;right to be forgotten\u0026rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.\r2023-07-18\nSynthetic Text Generation with Differential Privacy: A Simple and Practical Recipe\nXiang Yue Huseyin A. Inan Xuechen Li Girish Kumar Julia McAnallen Hoda Shajari Huan Sun David Levitan Robert Sim\nabstract\rabstract: Privacy concerns have attracted increasing attention in data-driven productsdue to the tendency of machine learning models to memorize sensitive trainingdata. Generating synthetic versions of such data with a formal privacyguarantee, such as differential privacy (DP), provides a promising path tomitigating these privacy concerns, but previous approaches in this directionhave typically failed to produce synthetic data of high quality. In this work,we show that a simple and practical recipe in the text domain is effective:simply fine-tuning a pretrained generative language model with DP enables themodel to generate useful synthetic text with strong privacy protection. Throughextensive empirical analyses on both benchmark and private customer data, wedemonstrate that our method produces synthetic text that is competitive interms of utility with its non-private counterpart, meanwhile providing strongprotection against potential privacy leakages.\rCloud-native RStudio on Kubernetes for Hopsworks\nGibson Chikafa Sina Sheikholeslami Salman Niazi Jim Dowling Vladimir Vlassov\nabstract\rabstract: In order to fully benefit from cloud computing, services are designedfollowing the \u0026ldquo;multi-tenant\u0026rdquo; architectural model, which is aimed at maximizingresource sharing among users. However, multi-tenancy introduces challenges ofsecurity, performance isolation, scaling, and customization. RStudio server isan open-source Integrated Development Environment (IDE) accessible over a webbrowser for the R programming language. We present the design andimplementation of a multi-user distributed system on Hopsworks, adata-intensive AI platform, following the multi-tenant model that providesRStudio as Software as a Service (SaaS). We use the most popular cloud-nativetechnologies: Docker and Kubernetes, to solve the problems of performanceisolation, security, and scaling that are present in a multi-tenantenvironment. We further enable secure data sharing in RStudio server instancesto provide data privacy and allow collaboration among RStudio users. Weintegrate our system with Apache Spark, which can scale and handle Big Dataprocessing workloads. Also, we provide a UI where users can provide customconfigurations and have full control of their own RStudio server instances. Oursystem was tested on a Google Cloud Platform cluster with four worker nodes,each with 30GB of RAM allocated to them. The tests on this cluster showed that44 RStudio servers, each with 2GB of RAM, can be run concurrently. Our systemcan scale out to potentially support hundreds of concurrently running RStudioservers by adding more resources (CPUs and RAM) to the cluster or system.\rFederated Large Language Model: A Position Paper\nChaochao Chen Xiaohua Feng Jun Zhou Jianwei Yin Xiaolin Zheng\nabstract\rabstract: Large scale language models (LLM) have received significant attention andfound diverse applications across various domains, but their developmentencounters challenges in real-world scenarios. These challenges arise due tothe scarcity of public domain data availability and the need to maintainprivacy with respect to private domain data. To address these issues, federatedlearning (FL) has emerged as a promising technology that enables collaborativetraining of shared models while preserving decentralized data. We propose theconcept of federated LLM, which comprises three key components, i.e., federatedLLM pre-training, federated LLM fine-tuning, and federated LLM promptengineering. For each component, we discuss its advantage over traditional LLMtraining methods and propose specific engineering strategies forimplementation. Furthermore, we explore the novel challenges introduced by theintegration of FL and LLM. We analyze existing solutions and identify potentialobstacles faced by these solutions within the context of federated LLM.\r2023-07-17\nFederated Learning of Gboard Language Models with Differential Privacy\nZheng Xu Yanxiang Zhang Galen Andrew Christopher A. Choquette-Choo Peter Kairouz H. Brendan McMahan Jesse Rosenstock Yuanbo Zhang\nabstract\rabstract: We train language models (LMs) with federated learning (FL) and differentialprivacy (DP) in the Google Keyboard (Gboard). We apply theDP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm toachieve meaningfully formal DP guarantees without requiring uniform sampling ofclient devices. To provide favorable privacy-utility trade-offs, we introduce anew client participation criterion and discuss the implication of itsconfiguration in large scale systems. We show how quantile-based clipestimation~\\citep{andrew2019differentially} can be combined with DP-FTRL toadaptively choose the clip norm during training or reduce the hyperparametertuning in preparation for training. With the help of pretraining on publicdata, we train and deploy more than twenty Gboard LMs that achieve high utilityand $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two modelsadditionally trained with secure aggregation~\\citep{bonawitz2017practical}. Weare happy to announce that all the next word prediction neural network LMs inGboard now have DP guarantees, and all future launches of Gboard neural networkLMs will require DP guarantees. We summarize our experience and provideconcrete suggestions on DP training for practitioners.\r2023-07-14\nPopulation Expansion for Training Language Models with Private Federated Learning\nTatsuki Koga Congzheng Song Martin Pelikan Mona Chitnis\nabstract\rabstract: Federated learning (FL) combined with differential privacy (DP) offersmachine learning (ML) training with distributed devices and with a formalprivacy guarantee. With a large population of devices, FL with DP produces aperformant model in a timely manner. However, for applications with a smallerpopulation, not only does the model utility degrade as the DP noise isinversely proportional to population, but also the training latency increasessince waiting for enough clients to become available from a smaller pool isslower. In this work, we thus propose expanding the population based on domainadaptation techniques to speed up the training and improves the final modelquality when training with small populations. We empirically demonstrate thatour techniques can improve the utility by 13% to 30% on real-world languagemodeling datasets.\r2023-07-13\nLarge Language Models for Supply Chain Optimization\nBeibin Li Konstantina Mellou Bo Zhang Jeevan Pathuri Ishai Menache\nabstract\rabstract: Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide \u0026ndash; a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft\u0026rsquo;s cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.\r2023-07-12\nDistilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events\nYu Gu Sheng Zhang Naoto Usuyama Yonas Woldesenbet Cliff Wong Praneeth Sanapathi Mu Wei Naveen Valluri Erika Strandberg Tristan Naumann Hoifung Poon\nabstract\rabstract: Large language models (LLMs), such as GPT-4, have demonstrated remarkablecapabilities across a wide range of tasks, including health applications. Inthis paper, we study how LLMs can be used to scale biomedical knowledgecuration. We find that while LLMs already possess decent competency instructuring biomedical text, by distillation into a task-specific student modelthrough self-supervised learning, substantial gains can be attained overout-of-box LLMs, with additional advantages such as cost, efficiency, andwhite-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is animportant area for improving care. On standard ADE extraction evaluation, aGPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervisedstate-of-the-art models without using any labeled data. Despite being over1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 byover 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)and ADE extraction architecture shed light on best practice for biomedicalknowledge extraction. Similar gains were attained by distillation for otherstandard biomedical knowledge extraction tasks such as gene-diseaseassociations and protected health information, further illustrating the promiseof this approach.\rTowards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models\nSanghyun Kim Seohyeon Jung Balhae Kim Moonseok Choi Jinwoo Shin Juho Lee\nabstract\rabstract: Large-scale image generation models, with impressive quality made possible bythe vast amount of data available on the Internet, raise social concerns thatthese models may generate harmful or copyrighted content. The biases andharmfulness arise throughout the entire training process and are hard tocompletely remove, which have become significant hurdles to the safe deploymentof these models. In this paper, we propose a method called SDD to preventproblematic content generation in text-to-image diffusion models. Weself-distill the diffusion model to guide the noise estimate conditioned on thetarget removal concept to match the unconditional one. Compared to the previousmethods, our method eliminates a much greater proportion of harmful contentfrom the generated images without degrading the overall image quality.Furthermore, our method allows the removal of multiple concepts at once,whereas previous works are limited to removing a single concept at a time.\rUnified Medical Image-Text-Label Contrastive Learning With Continuous Prompt\nYuhao Wang\nabstract\rabstract: Contrastive language-image Pre-training (CLIP) [13] can leverage largedatasets of unlabeled Image-Text pairs, which have demonstrated impressiveperformance in various downstream tasks. Given that annotating medical data istime-consuming and laborious, Image-Text Pre-training has promisingapplications in exploiting large-scale medical image and radiology reportdatasets. However, medical Image-Text Pre-training faces several challenges, asfollows: (1) Due to privacy concerns, the amount of available medical data isrelatively small compared to natural data, leading to weaker generalizationability of the model. (2) Medical images are highly similar with onlyfine-grained differences in subtleties, resulting in a large number offalse-negative sample pairs in comparison learning. (3) The hand-crafted Promptusually differs from the natural medical image report, Subtle changes inwording can lead to significant differences in performance. In this paper, wepropose a unified Image-Text-Label contrastive learning framework based oncontinuous prompts, with three main contributions. First, We unified the dataof images, text, and labels, which greatly expanded the training data that themodel could utilize. Second, we address the issue of data diversity and theimpact of hand-crafted prompts on model performance by introducing continuousimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training tomitigate the problem of too many false-negative samples. We demonstrate throughsufficient experiments that the Unified Medical Contrastive Learning (UMCL)framework exhibits excellent performance on several downstream tasks.\r2023-07-10\nDeductive Controller Synthesis for Probabilistic Hyperproperties\nRoman Andriushchenko Ezio Bartocci Milan Ceska Francesco Pontiggia Sarah Sallinger\nabstract\rabstract: Probabilistic hyperproperties specify quantitative relations between theprobabilities of reaching different target sets of states from differentinitial sets of states. This class of behavioral properties is suitable forcapturing important security, privacy, and system-level requirements. Wepropose a new approach to solve the controller synthesis problem for Markovdecision processes (MDPs) and probabilistic hyperproperties. Our specificationlanguage builds on top of the logic HyperPCTL and enhances it with structuralconstraints over the synthesized controllers. Our approach starts from a familyof controllers represented symbolically and defined over the same copy of anMDP. We then introduce an abstraction refinement strategy that can relatemultiple computation trees and that we employ to prune the search spacedeductively. The experimental evaluation demonstrates that the proposedapproach considerably outperforms HyperProb, a state-of-the-art SMT-based modelchecking tool for HyperPCTL. Moreover, our approach is the first one that isable to effectively combine probabilistic hyperproperties with additionalintra-controller constraints (e.g. partial observability) as well asinter-controller constraints (e.g. agreements on a common action).\rEthicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation\nZhexin Zhang Jiaxin Wen Minlie Huang\nabstract\rabstract: Large pre-trained language models achieve impressive results across manytasks. However, recent works point out that pre-trained language models maymemorize a considerable fraction of their training data, leading to the privacyrisk of information leakage. In this paper, we propose a method named Ethicistfor targeted training data extraction through loss smoothed soft prompting andcalibrated confidence estimation, investigating how to recover the suffix inthe training data when given a prefix. To elicit memorization in the attackedmodel, we tune soft prompt embeddings while keeping the model fixed. We furtherpropose a smoothing loss that smooths the loss distribution of the suffixtokens to make it easier to sample the correct suffix. In order to select themost probable suffix from a collection of sampled suffixes and estimate theprediction confidence, we propose a calibrated confidence estimation method,which normalizes the confidence of the generated suffixes with a localestimation. We show that Ethicist significantly improves the extractionperformance on a recently proposed public benchmark. We also investigateseveral factors influencing the data extraction performance, including decodingstrategy, model scale, prefix length, and suffix length. Our code is availableat https://github.com/thu-coai/Targeted-Data-Extraction.\rPrivacy-Preserving Graph Machine Learning from Data to Computation: A Survey\nDongqi Fu Wenxuan Bao Ross Maciejewski Hanghang Tong Jingrui He\nabstract\rabstract: In graph machine learning, data collection, sharing, and analysis ofteninvolve multiple parties, each of which may require varying levels of datasecurity and privacy. To this end, preserving privacy is of great importance inprotecting sensitive information. In the era of big data, the relationshipsamong data entities have become unprecedentedly complex, and more applicationsutilize advanced data structures (i.e., graphs) that can support networkstructures and relevant attribute information. To date, many graph-based AImodels have been proposed (e.g., graph neural networks) for various domaintasks, like computer vision and natural language processing. In this paper, wefocus on reviewing privacy-preserving techniques of graph machine learning. Wesystematically review related works from the data to the computational aspects.We first review methods for generating privacy-preserving graph data. Then wedescribe methods for transmitting privacy-preserved information (e.g., graphmodel parameters) to realize the optimization-based computation when datasharing among multiple parties is risky or impossible. In addition todiscussing relevant theoretical methodology and software tools, we also discusscurrent challenges and highlight several possible future research opportunitiesfor privacy-preserving graph machine learning. Finally, we envision a unifiedand comprehensive secure graph machine learning system.\r2023-07-09\nShaping the Emerging Norms of Using Large Language Models in Social Computing Research\nHong Shen Tianshi Li Toby Jia-Jun Li Joon Sung Park Diyi Yang\nabstract\rabstract: The emergence of Large Language Models (LLMs) has brought both excitement andconcerns to social computing research. On the one hand, LLMs offerunprecedented capabilities in analyzing vast amounts of textual data andgenerating human-like responses, enabling researchers to delve into complexsocial phenomena. On the other hand, concerns are emerging regarding thevalidity, privacy, and ethics of the research when LLMs are involved. This SIGaims at offering an open space for social computing researchers who areinterested in understanding the impacts of LLMs to discuss their currentpractices, perspectives, challenges when engaging with LLMs in their everydaywork and collectively shaping the emerging norms of using LLMs in socialcomputing research.\r2023-07-08\nMeasuring the Success of Diffusion Models at Imitating Human Artists\nStephen Casper Zifan Guo Shreya Mogulothu Zachary Marinov Chinmay Deshpande Rui-Jie Yew Zheng Dai Dylan Hadfield-Menell\nabstract\rabstract: Modern diffusion models have set the state-of-the-art in AI image generation.Their success is due, in part, to training on Internet-scale data which oftenincludes copyrighted work. This prompts questions about the extent to whichthese models learn from, imitate, or copy the work of human artists. This worksuggests that tying copyright liability to the capabilities of the model may beuseful given the evolving ecosystem of generative models. Specifically, much ofthe legal analysis of copyright and generative systems focuses on the use ofprotected data for training. As a result, the connections between data,training, and the system are often obscured. In our approach, we considersimple image classification techniques to measure a model\u0026rsquo;s ability to imitatespecific artists. Specifically, we use Contrastive Language-Image Pretrained(CLIP) encoders to classify images in a zero-shot fashion. Our process firstprompts a model to imitate a specific artist. Then, we test whether CLIP can beused to reclassify the artist (or the artist\u0026rsquo;s work) from the imitation. Ifthese tests match the imitation back to the original artist, this suggests themodel can imitate that artist\u0026rsquo;s expression. Our approach is simple andquantitative. Furthermore, it uses standard techniques and does not requireadditional training. We demonstrate our approach with an audit of StableDiffusion\u0026rsquo;s capacity to imitate 70 professional digital artists withcopyrighted work online. When Stable Diffusion is prompted to imitate an artistfrom this set, we find that the artist can be identified from the imitationwith an average accuracy of 81.0%. Finally, we also show that a sample of theartist\u0026rsquo;s work can be matched to these imitation images with a high degree ofstatistical reliability. Overall, these results suggest that Stable Diffusionis broadly successful at imitating individual human artists.\r2023-07-07\nThe Ethical Implications of Generative Audio Models: A Systematic Literature Review\nJulia Barnett\nabstract\rabstract: Generative audio models typically focus their applications in music andspeech generation, with recent models having human-like quality in their audiooutput. This paper conducts a systematic literature review of 884 papers in thearea of generative audio models in order to both quantify the degree to whichresearchers in the field are considering potential negative impacts andidentify the types of ethical implications researchers in this area need toconsider. Though 65% of generative audio research papers note positivepotential impacts of their work, less than 10% discuss any negative impacts.This jarringly small percentage of papers considering negative impact isparticularly worrying because the issues brought to light by the few papersdoing so are raising serious ethical implications and concerns relevant to thebroader field such as the potential for fraud, deep-fakes, and copyrightinfringement. By quantifying this lack of ethical consideration in generativeaudio research and identifying key areas of potential harm, this paper lays thegroundwork for future work in the field at a critical point in time in order toguide more conscientious research as this field progresses.\rFederated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios\nKarim Gamal Ahmed Gaber Hossam Amer\nabstract\rabstract: Federated learning is a growing field in the machine learning community dueto its decentralized and private design. Model training in federated learningis distributed over multiple clients giving access to lots of client data whilemaintaining privacy. Then, a server aggregates the training done on thesemultiple clients without access to their data, which could be emojis widelyused in any social media service and instant messaging platforms to expressusers\u0026rsquo; sentiments. This paper proposes federated learning-based multilingualemoji prediction in both clean and attack scenarios. Emoji prediction data havebeen crawled from both Twitter and SemEval emoji datasets. This data is used totrain and evaluate different transformer model sizes including a sparselyactivated transformer with either the assumption of clean data in all clientsor poisoned data via label flipping attack in some clients. Experimentalresults on these models show that federated learning in either clean orattacked scenarios performs similarly to centralized training in multilingualemoji prediction on seen and unseen languages under different data sources anddistributions. Our trained transformers perform better than other techniques onthe SemEval emoji dataset in addition to the privacy as well as distributedbenefits of federated learning.\r2023-07-06\nUndecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages\nShreyanth S\nabstract\rabstract: By combining the undecimated wavelet transform within a Word EmbeddedSemantic Marginal Autoencoder (WESMA), this research study provides a novelstrategy for improving security measures and denoising multiple languages. Theincorporation of these strategies is intended to address the issues ofrobustness, privacy, and multilingualism in data processing applications. Theundecimated wavelet transform is used as a feature extraction tool to identifyprominent language patterns and structural qualities in the input data. Theproposed system may successfully capture significant information whilepreserving the temporal and geographical links within the data by employingthis transform. This improves security measures by increasing the system\u0026rsquo;sability to detect abnormalities, discover hidden patterns, and distinguishbetween legitimate content and dangerous threats. The Word Embedded SemanticMarginal Autoencoder also functions as an intelligent framework fordimensionality and noise reduction. The autoencoder effectively learns theunderlying semantics of the data and reduces noise components by exploitingword embeddings and semantic context. As a result, data quality and accuracyare increased in following processing stages. The suggested methodology istested using a diversified dataset that includes several languages and securityscenarios. The experimental results show that the proposed approach iseffective in attaining security enhancement and denoising capabilities acrossmultiple languages. The system is strong in dealing with linguistic variances,producing consistent outcomes regardless of the language used. Furthermore,incorporating the undecimated wavelet transform considerably improves thesystem\u0026rsquo;s ability to efficiently address complex security concerns\r2023-07-05\nOpen-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks\nMeysam Alizadeh Maël Kubli Zeynab Samei Shirin Dehghani Juan Diego Bermeo Maria Korobeynikova Fabrizio Gilardi\nabstract\rabstract: This study examines the performance of open-source Large Language Models(LLMs) in text annotation tasks and compares it with proprietary models likeChatGPT and human-based services such as MTurk. While prior researchdemonstrated the high performance of ChatGPT across numerous NLP tasks,open-source LLMs like HugginChat and FLAN are gaining attention for theircost-effectiveness, transparency, reproducibility, and superior dataprotection. We assess these models using both zero-shot and few-shot approachesand different temperature parameters across a range of text annotation tasks.Our findings show that while ChatGPT achieves the best performance in mosttasks, open-source LLMs not only outperform MTurk but also demonstratecompetitive potential against ChatGPT in specific tasks.\r2023-07-04\nProPILE: Probing Privacy Leakage in Large Language Models\nSiwon Kim Sangdoo Yun Hwaran Lee Martin Gubri Sungroh Yoon Seong Joon Oh\nabstract\rabstract: The rapid advancement and widespread use of large language models (LLMs) haveraised significant concerns regarding the potential leakage of personallyidentifiable information (PII). These models are often trained on vastquantities of web-collected data, which may inadvertently include sensitivepersonal data. This paper presents ProPILE, a novel probing tool designed toempower data subjects, or the owners of the PII, with awareness of potentialPII leakage in LLM-based services. ProPILE lets data subjects formulate promptsbased on their own PII to evaluate the level of privacy intrusion in LLMs. Wedemonstrate its application on the OPT-1.3B model trained on the publiclyavailable Pile dataset. We show how hypothetical data subjects may assess thelikelihood of their PII being included in the Pile dataset being revealed.ProPILE can also be leveraged by LLM service providers to effectively evaluatetheir own levels of PII leakage with more powerful prompts specifically tunedfor their in-house models. This tool represents a pioneering step towardsempowering the data subjects for their awareness and control over their owndata on the web.\rTree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust\nYuxin Wen John Kirchenbauer Jonas Geiping Tom Goldstein\nabstract\rabstract: Watermarking the outputs of generative models is a crucial technique fortracing copyright and preventing potential harm from AI-generated content. Inthis paper, we introduce a novel technique called Tree-Ring Watermarking thatrobustly fingerprints diffusion model outputs. Unlike existing methods thatperform post-hoc modifications to images after sampling, Tree-Ring Watermarkingsubtly influences the entire sampling process, resulting in a model fingerprintthat is invisible to humans. The watermark embeds a pattern into the initialnoise vector used for sampling. These patterns are structured in Fourier spaceso that they are invariant to convolutions, crops, dilations, flips, androtations. After image generation, the watermark signal is detected byinverting the diffusion process to retrieve the noise vector, which is thenchecked for the embedded signal. We demonstrate that this technique can beeasily applied to arbitrary diffusion models, including text-conditioned StableDiffusion, as a plug-in with negligible loss in FID. Our watermark issemantically hidden in the image space and is far more robust than watermarkingalternatives that are currently deployed. Code is available athttps://github.com/YuxinWenRick/tree-ring-watermark.\r2023-06-28\nViP: A Differentially Private Foundation Model for Computer Vision\nYaodong Yu Maziar Sanjabi Yi Ma Kamalika Chaudhuri Chuan Guo\nabstract\rabstract: Artificial intelligence (AI) has seen a tremendous surge in capabilitiesthanks to the use of foundation models trained on internet-scale data. On theflip side, the uncurated nature of internet-scale data also poses significantprivacy and legal risks, as they often contain personal information orcopyrighted material that should not be trained on without permission. In thiswork, we propose as a mitigation measure a recipe to train foundation visionmodels with differential privacy (DP) guarantee. We identify maskedautoencoders as a suitable learning algorithm that aligns well with DP-SGD, andtrain ViP \u0026ndash; a Vision transformer with differential Privacy \u0026ndash; under a strictprivacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate thequality of representation learned by ViP using standard downstream visiontasks; in particular, ViP achieves a (non-private) linear probing accuracy of$55.7%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trainedand evaluated on ImageNet). Our result suggests that scaling to internet-scaledata can be practical for private learning. Code is available at\\url{https://github.com/facebookresearch/ViP-MAE}.\rMulti-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare\nWon Joon Yun Samuel Kim Joongheon Kim\nabstract\rabstract: The prodigious growth of digital health data has precipitated a mountinginterest in harnessing machine learning methodologies, such as natural languageprocessing (NLP), to scrutinize medical records, clinical notes, and othertext-based health information. Although NLP techniques have exhibitedsubstantial potential in augmenting patient care and informing clinicaldecision-making, data privacy and adherence to regulations persist as criticalconcerns. Federated learning (FL) emerges as a viable solution, empoweringmultiple organizations to train machine learning models collaboratively withoutdisseminating raw data. This paper proffers a pragmatic approach to medical NLPby amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA.We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-basedmodel and Bidirectional Encoder Representations from Transformers (BERT), whichhave demonstrated exceptional performance in comprehending context andsemantics within medical data. This paper encompasses the development of anintegrated framework that addresses data privacy and regulatory compliancechallenges while maintaining elevated accuracy and performance, incorporatingBERT pretraining, and comprehensively substantiating the efficacy of theproposed approach.\r2023-06-27\nRequirements for Explainability and Acceptance of Artificial Intelligence in Collaborative Work\nSabine Theis Sophie Jentzsch Fotini Deligiannaki Charles Berro Arne Peter Raulf Carmen Bruder\nabstract\rabstract: The increasing prevalence of Artificial Intelligence (AI) in safety-criticalcontexts such as air-traffic control leads to systems that are practical andefficient, and to some extent explainable to humans to be trusted and accepted.The present structured literature analysis examines n = 236 articles on therequirements for the explainability and acceptance of AI. Results include acomprehensive review of n = 48 articles on information people need to perceivean AI as explainable, the information needed to accept an AI, andrepresentation and interaction methods promoting trust in an AI. Resultsindicate that the two main groups of users are developers who requireinformation about the internal operations of the model and end users whorequire information about AI results or behavior. Users\u0026rsquo; information needs varyin specificity, complexity, and urgency and must consider context, domainknowledge, and the user\u0026rsquo;s cognitive resources. The acceptance of AI systemsdepends on information about the system\u0026rsquo;s functions and performance, privacyand ethical considerations, as well as goal-supporting information tailored toindividual preferences and information to establish trust in the system.Information about the system\u0026rsquo;s limitations and potential failures can increaseacceptance and trust. Trusted interaction methods are human-like, includingnatural language, speech, text, and visual representations such as graphs,charts, and animations. Our results have significant implications for futurehuman-centric AI systems being developed. Thus, they are suitable as input forfurther application-specific investigations of user needs.\r2023-06-26\nChatIDS: Explainable Cybersecurity Using Generative AI\nVictor Jüttner Martin Grimmer Erik Buchmann\nabstract\rabstract: Intrusion Detection Systems (IDS) are a proven approach to secure networks.However, in a privately used network, it is difficult for users withoutcybersecurity expertise to understand IDS alerts, and to respond in time withadequate measures. This puts the security of home networks, smart homeinstallations, home-office workers, etc. at risk, even if an IDS is correctlyinstalled and configured. In this work, we propose ChatIDS, our approach toexplain IDS alerts to non-experts by using large language models. We evaluatethe feasibility of ChatIDS by using ChatGPT, and we identify open researchissues with the help of interdisciplinary experts in artificial intelligence.Our results show that ChatIDS has the potential to increase network security byproposing meaningful security measures in an intuitive language from IDSalerts. Nevertheless, some potential issues in areas such as trust, privacy,ethics, etc. need to be resolved, before ChatIDS might be put into practice.\r2023-06-24\nChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge\nYunxiang Li Zihan Li Kai Zhang Ruilong Dan Steve Jiang You Zhang\nabstract\rabstract: The primary aim of this research was to address the limitations observed inthe medical knowledge of prevalent large language models (LLMs) such asChatGPT, by creating a specialized language model with enhanced accuracy inmedical advice. We achieved this by adapting and refining the large languagemodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialoguessourced from a widely used online medical consultation platform. Theseconversations were cleaned and anonymized to respect privacy concerns. Inaddition to the model refinement, we incorporated a self-directed informationretrieval mechanism, allowing the model to access and utilize real-timeinformation from online sources like Wikipedia and data from curated offlinemedical databases. The fine-tuning of the model with real-world patient-doctorinteractions significantly improved the model\u0026rsquo;s ability to understand patientneeds and provide informed advice. By equipping the model with self-directedinformation retrieval from reliable online and offline sources, we observedsubstantial improvements in the accuracy of its responses. Our proposedChatDoctor, represents a significant advancement in medical LLMs, demonstratinga significant improvement in understanding patient inquiries and providingaccurate advice. Given the high stakes and low error tolerance in the medicalfield, such enhancements in providing accurate and reliable information are notonly beneficial but essential.\r2023-06-23\nDeconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models\nAdel Elmahdy Ahmed Salem\nabstract\rabstract: Natural language processing (NLP) models have become increasingly popular inreal-world applications, such as text classification. However, they arevulnerable to privacy attacks, including data reconstruction attacks that aimto extract the data used to train the model. Most previous studies on datareconstruction attacks have focused on LLM, while classification models wereassumed to be more secure. In this work, we propose a new targeted datareconstruction attack called the Mix And Match attack, which takes advantage ofthe fact that most classification models are based on LLM. The Mix And Matchattack uses the base model of the target model to generate candidate tokens andthen prunes them using the classification head. We extensively demonstrate theeffectiveness of the attack using both random and organic canaries. This workhighlights the importance of considering the privacy risks associated with datareconstruction attacks in classification models and offers insights intopossible leakages.\rExploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT\nRyan Lingo\nabstract\rabstract: This research delves into the construction and utilization of syntheticdatasets, specifically within the telematics sphere, leveraging OpenAI\u0026rsquo;spowerful language model, ChatGPT. Synthetic datasets present an effectivesolution to challenges pertaining to data privacy, scarcity, and control overvariables - characteristics that make them particularly valuable for researchpursuits. The utility of these datasets, however, largely depends on theirquality, measured through the lenses of diversity, relevance, and coherence. Toillustrate this data creation process, a hands-on case study is conducted,focusing on the generation of a synthetic telematics dataset. The experimentinvolved an iterative guidance of ChatGPT, progressively refining prompts andculminating in the creation of a comprehensive dataset for a hypothetical urbanplanning scenario in Columbus, Ohio. Upon generation, the synthetic dataset wassubjected to an evaluation, focusing on the previously identified qualityparameters and employing descriptive statistics and visualization techniquesfor a thorough analysis. Despite synthetic datasets not serving as perfectreplacements for actual world data, their potential in specific use-cases, whenexecuted with precision, is significant. This research underscores thepotential of AI models like ChatGPT in enhancing data availability for complexsectors like telematics, thus paving the way for a myriad of new researchopportunities.\r2023-06-22\nXACML Extension for Graphs: Flexible Authorization Policy Specification and Datastore-independent Enforcement\nAya Mohamed Dagmar Auer Daniel Hofer Josef Küng\nabstract\rabstract: The increasing use of graph-structured data for business- andprivacy-critical applications requires sophisticated, flexible and fine-grainedauthorization and access control. Currently, role-based access control issupported in graph databases, where access to objects is restricted via roles.This does not take special properties of graphs into account such as verticesand edges along the path between a given subject and resource. In previousiterations of our research, we started to design an authorization policylanguage and access control model, which considers the specification of graphpaths and enforces them in the multi-model database ArangoDB. Since thisapproach is promising to consider graph characteristics in data protection, weimprove the language in this work to provide flexible path definitions andspecifying edges as protected resources. Furthermore, we introduce a method fora datastore-independent policy enforcement. Besides discussing the latest workin our XACML4G model, which is an extension to the Extensible Access ControlMarkup Language (XACML), we demonstrate our prototypical implementation with areal case and give an outlook on performance.\rVec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity\nAndrew Kean Gao\nabstract\rabstract: Vector embeddings have become ubiquitous tools for many language-relatedtasks. A leading embedding model is OpenAI\u0026rsquo;s text-ada-002 which can embedapproximately 6,000 words into a 1,536-dimensional vector. While powerful,text-ada-002 is not open source and is only available via API. We trained asimple neural network to convert open-source 768-dimensional MPNet embeddingsinto text-ada-002 embeddings. We compiled a subset of 50,000 online foodreviews. We calculated MPNet and text-ada-002 embeddings for each review andtrained a simple neural network to for 75 epochs. The neural network wasdesigned to predict the corresponding text-ada-002 embedding for a given MPNETembedding. Our model achieved an average cosine similarity of 0.932 on 10,000unseen reviews in our held-out test dataset. We manually assessed the qualityof our predicted embeddings for vector search over text-ada-002-embeddedreviews. While not as good as real text-ada-002 embeddings, predictedembeddings were able to retrieve highly relevant reviews. Our final model,Vec2Vec, is lightweight (\u0026lt;80 MB) and fast. Future steps include training aneural network with a more sophisticated architecture and a larger dataset ofpaired embeddings to achieve greater performance. The ability to convertbetween and align embedding spaces may be helpful for interoperability,limiting dependence on proprietary models, protecting data privacy, reducingcosts, and offline operations.\r2023-06-21\nA VM-Agnostic and Backwards Compatible Protected Modifier for Dynamically-Typed Languages\nIona Thomas Vincent Aranega Stéphane Ducasse Guillermo Polito Pablo Tesone\nabstract\rabstract: In object-oriented languages, method visibility modifiers hold a key role inseparating internal methods from the public API. Protected visibility modifiersoffer a way to hide methods from external objects while authorizing internaluse and overriding in subclasses. While present in main statically-typedlanguages, visibility modifiers are not as common or mature indynamically-typed languages. In this article, we present ProtDyn, aself-send-based visibility model calculated at compile time fordynamically-typed languages relying on name-mangling and syntacticdifferentiation of self vs non self sends. We present #Pharo, a ProtDynimplementation of this model that is backwards compatible with existingprograms, and its port to Python. Using these implementations we study theperformance impact of ProtDyn on the method lookup, in the presence of globallookup caches and polymorphic inline caches. We show that our name mangling anddouble method registration technique has a very low impact on performance andkeeps the benefits from the global lookup cache and polymorphic inline cache.We also show that the memory overhead on a real use case is between 2% and 13%in the worst-case scenario. Protected modifier semantics enforces encapsulationsuch as private but allow developers to still extend the class in subclasses.ProtDyn offers a VM-agnostic and backwards-compatible design to introduceprotected semantics in dynamically-typed languages.\rA new color image secret sharing protocol\nJosé Ignacio Farrán David Cerezo\nabstract\rabstract: Visual cryptography aims to protect images against their possibleillegitimate use. Thus, one can cipher, hash, or add watermarks for protectingcopyright, among others. In this paper we provide a new solution to the problemof secret sharing for the case when the secret is an image. Our method combinesthe Shamir scheme for secret sharing using finite fields of characteristic 2with the CBC mode of operation of a secure symmetric cryptographic scheme likeAES, so that the security relies on that of the mentioned techniques. Theresulting shares have the same resolution as that of the original image. Theidea of the method could be generalized to other multimedia formats like audioor video, adapting the method to the corresponding encoded information.\rFederated Self-Learning with Weak Supervision for Speech Recognition\nMilind Rao Gopinath Chennupati Gautam Tiwari Anit Kumar Sahu Anirudh Raju Ariya Rastrow Jasha Droppo\nabstract\rabstract: Automatic speech recognition (ASR) models with low-footprint are increasinglybeing deployed on edge devices for conversational agents, which enhancesprivacy. We study the problem of federated continual incremental learning forrecurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancingscheme of learning on-device, without access to ground truth human transcriptsor machine transcriptions from a stronger ASR model. In particular, we studythe performance of a self-learning based scheme, with a paired teacher modelupdated through an exponential moving average of ASR models. Further, wepropose using possibly noisy weak-supervision signals such as feedback scoresand natural language understanding semantics determined from user behavioracross multiple turns in a session of interactions with the conversationalagent. These signals are leveraged in a multi-task policy-gradient trainingapproach to improve the performance of self-learning for ASR. Finally, we showhow catastrophic forgetting can be mitigated by combining on-device learningwith a memory-replay approach using selected historical datasets. Theseinnovations allow for 10% relative improvement in WER on new use cases withminimal degradation on other test sets in the absence of strong-supervisionsignals such as ground-truth transcriptions.\r2023-06-20\nFedMultimodal: A Benchmark For Multimodal Federated Learning\nTiantian Feng Digbalay Bose Tuo Zhang Rajat Hebbar Anil Ramakrishna Rahul Gupta Mi Zhang Salman Avestimehr Shrikanth Narayanan\nabstract\rabstract: Over the past few years, Federated Learning (FL) has become an emergingmachine learning technique to tackle data privacy challenges throughcollaborative training. In the Federated Learning algorithm, the clients submita locally trained model, and the server aggregates these parameters untilconvergence. Despite significant efforts that have been made to FL in fieldslike computer vision, audio, and natural language processing, the FLapplications utilizing multimodal data streams remain largely unexplored. It isknown that multimodal learning has broad real-world applications in emotionrecognition, healthcare, multimedia, and social media, while user privacypersists as a critical concern. Specifically, there are no existing FLbenchmarks targeting multimodal applications or related tasks. In order tofacilitate the research in multimodal FL, we introduce FedMultimodal, the firstFL benchmark for multimodal learning covering five representative multimodalapplications from ten commonly used datasets with a total of eight uniquemodalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-endmodeling framework ranging from data partition and feature extraction to FLbenchmark algorithms and model evaluation. Unlike existing FL benchmarks,FedMultimodal provides a standardized approach to assess the robustness of FLagainst three common data corruptions in real-life multimodal applications:missing modalities, missing labels, and erroneous labels. We hope thatFedMultimodal can accelerate numerous future research directions, includingdesigning multimodal FL algorithms toward extreme data heterogeneity,robustness multimodal FL, and efficient multimodal FL. The datasets andbenchmark results can be accessed at:https://github.com/usc-sail/fed-multimodal.\rPoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs\nHao Cui Rahmadi Trimananda Athina Markopoulou Scott Jordan\nabstract\rabstract: Privacy policies disclose how an organization collects and handles personalinformation. Recent work has made progress in leveraging natural languageprocessing (NLP) to automate privacy policy analysis and extract datacollection statements from different sentences, considered in isolation fromeach other. In this paper, we view and analyze, for the first time, the entiretext of a privacy policy in an integrated way. In terms of methodology: (1) wedefine PoliGraph, a type of knowledge graph that captures statements in aprivacy policy as relations between different parts of the text; and (2) wedevelop an NLP-based tool, PoliGraph-er, to automatically extract PoliGraphfrom the text. In addition, (3) we revisit the notion of ontologies, previouslydefined in heuristic ways, to capture subsumption relations between terms. Wemake a clear distinction between local and global ontologies to capture thecontext of individual privacy policies, application domains, and privacy laws.Using a public dataset for evaluation, we show that PoliGraph-er identifies 40%more collection statements than prior state-of-the-art, with 97% precision. Interms of applications, PoliGraph enables automated analysis of a corpus ofprivacy policies and allows us to: (1) reveal common patterns in the textsacross different privacy policies, and (2) assess the correctness of the termsas defined within a privacy policy. We also apply PoliGraph to: (3) detectcontradictions in a privacy policy, where we show false alarms by prior work,and (4) analyze the consistency of privacy policies and network traffic, wherewe identify significantly more clear disclosures than prior work.\r2023-06-19\nPath to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost\nJuexiao Zhou Xiuying Chen Xin Gao\nabstract\rabstract: Medical artificial general intelligence (AGI) is an emerging field that aimsto develop systems specifically designed for medical applications that possessthe ability to understand, learn, and apply knowledge across a wide range oftasks and domains. Large language models (LLMs) represent a significant steptowards AGI. However, training cross-domain LLMs in the medical field posessignificant challenges primarily attributed to the requirement of collectingdata from diverse domains. This task becomes particularly difficult due toprivacy restrictions and the scarcity of publicly available medical datasets.Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specificmedical LLMs with the lowest cost, and suggest a possible path to achievemedical AGI. With an increasing number of domain-specific professionalmultimodal LLMs in the medical field being developed, MedAGI is designed toautomatically select appropriate medical models by analyzing users\u0026rsquo; questionswith our novel adaptive expert selection algorithm. It offers a unifiedapproach to existing LLMs in the medical field, eliminating the need forretraining regardless of the introduction of new models. This characteristicrenders it a future-proof solution in the dynamically advancing medical domain.To showcase the resilience of MedAGI, we conducted an evaluation across threedistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysisof pathology pictures. The results demonstrated that MedAGI exhibitedremarkable versatility and scalability, delivering exceptional performanceacross diverse domains. Our code is publicly available to facilitate furtherresearch at https://github.com/JoshuaChou2018/MedAGI.\r2023-06-18\nParameter-efficient Modularised Bias Mitigation via AdapterFusion\nDeepak Kumar Oleg Lesota George Zerveas Daniel Cohen Carsten Eickhoff Markus Schedl Navid Rekabsaz\nabstract\rabstract: Large pre-trained language models contain societal biases and carry alongthese biases to downstream tasks. Current in-processing bias mitigationapproaches (like adversarial training) impose debiasing by updating a model\u0026rsquo;sparameters, effectively transferring the model to a new, irreversible debiasedstate. In this work, we propose a novel approach to develop stand-alonedebiasing functionalities separate from the model, which can be integrated intothe model on-demand, while keeping the core model untouched. Drawing from theconcept of AdapterFusion in multi-task learning, we introduce DAM (Debiasingwith Adapter Modules) - a debiasing approach to first encapsulate arbitrarybias mitigation functionalities into separate adapters, and then add them tothe model on-demand in order to deliver fairness qualities. We conduct a largeset of experiments on three classification tasks with gender, race, and age asprotected attributes. Our results show that DAM improves or maintains theeffectiveness of bias mitigation, avoids catastrophic forgetting in amulti-attribute scenario, and maintains on-par task performance, while grantingparameter-efficiency and easy switching between the original and debiasedmodels.\rNLP-based Automated Compliance Checking of Data Processing Agreements against GDPR\nOrlando Amaral Muhammad Ilyas Azeem Sallam Abualhaija Lionel C Briand\nabstract\rabstract: Processing personal data is regulated in Europe by the General DataProtection Regulation (GDPR) through data processing agreements (DPAs).Checking the compliance of DPAs contributes to the compliance verification ofsoftware systems as DPAs are an important source of requirements for softwaredevelopment involving the processing of personal data. However, manuallychecking whether a given DPA complies with GDPR is challenging as it requiressignificant time and effort for understanding and identifying DPA-relevantcompliance requirements in GDPR and then verifying these requirements in theDPA. In this paper, we propose an automated solution to check the compliance ofa given DPA against GDPR. In close interaction with legal experts, we firstbuilt two artifacts: (i) the \u0026ldquo;shall\u0026rdquo; requirements extracted from the GDPRprovisions relevant to DPA compliance and (ii) a glossary table defining thelegal concepts in the requirements. Then, we developed an automated solutionthat leverages natural language processing (NLP) technologies to check thecompliance of a given DPA against these \u0026ldquo;shall\u0026rdquo; requirements. Specifically, ourapproach automatically generates phrasal-level representations for the textualcontent of the DPA and compares it against predefined representations of the\u0026quot;shall\u0026quot; requirements. Over a dataset of 30 actual DPAs, the approach correctlyfinds 618 out of 750 genuine violations while raising 76 false violations, andfurther correctly identifies 524 satisfied requirements. The approach has thusan average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%.Compared to a baseline that relies on off-the-shelf NLP tools, our approachprovides an average accuracy gain of ~20 percentage points. The accuracy of ourapproach can be improved to ~94% with limited manual verification effort.\rShould ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era\nDong Zhang\nabstract\rabstract: With various AI tools such as ChatGPT becoming increasingly popular, we areentering a true AI era. We can foresee that exceptional AI tools will soon reapconsiderable profits. A crucial question arise: should AI tools share revenuewith their training data providers in additional to traditional stakeholdersand shareholders? The answer is Yes. Large AI tools, such as large languagemodels, always require more and better quality data to continuously improve,but current copyright laws limit their access to various types of data. Sharingrevenue between AI tools and their data providers could transform the currenthostile zero-sum game relationship between AI tools and a majority ofcopyrighted data owners into a collaborative and mutually beneficial one, whichis necessary to facilitate the development of a virtuous cycle among AI tools,their users and data providers that drives forward AI technology and builds ahealthy AI ecosystem. However, current revenue-sharing business models do notwork for AI tools in the forthcoming AI era, since the most widely used metricsfor website-based traffic and action, such as clicks, will be replaced by newmetrics such as prompts and cost per prompt for generative AI tools. Acompletely new revenue-sharing business model, which must be almost independentof AI tools and be easily explained to data providers, needs to establish aprompt-based scoring system to measure data engagement of each data provider.This paper systematically discusses how to build such a scoring system for alldata providers for AI tools based on classification and content similaritymodels, and outlines the requirements for AI tools or third parties to buildit. Sharing revenue with data providers using such a scoring system wouldencourage more data owners to participate in the revenue-sharing program. Thiswill be a utilitarian AI era where all parties benefit.\r2023-06-16\nJust One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness\nEric Zelikman Qian Huang Percy Liang Nick Haber Noah D. Goodman\nabstract\rabstract: Language model training in distributed settings is limited by thecommunication cost of gradient exchanges. In this short note, we extend recentwork from Malladi et al. (2023), using shared randomness to perform distributedfine-tuning with low bandwidth. The method is a natural decentralized extensionof memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).Each iteration, each machine seeds a Random Number Generator (RNG) to performlocal reproducible perturbations on model weights and calculate and exchangescalar projected gradients, which are then used to update each model. By usinga (machine, sample) identifier as the random seed, each model can regenerateone another\u0026rsquo;s perturbations. As machines only exchange single-byte projectedgradients, this is highly communication efficient. There are also potentialprivacy benefits, as projected gradients may be calculated on differenttraining data, and models never access the other\u0026rsquo;s data. Our approach not onlydrastically reduces communication bandwidth requirements but also accommodatesdynamic addition or removal of machines during the training process and retainsthe memory-efficient and inference-only advantages of recent work. We performproof-of-concept experiments to demonstrate the potential usefulness of thismethod, building off of rich literature on distributed optimization andmemory-efficient training.\rh2oGPT: Democratizing Large Language Models\nArno Candel Jon McKinney Philipp Singer Pascal Pfeiffer Maximilian Jeblick Prithvi Prabhu Jeff Gambera Mark Landry Shivam Bansal Ryan Chesler Chun Ming Lee Marcos V. Conde Pasha Stetsenko Olivier Grellier SriSatish Ambati\nabstract\rabstract: Applications built on top of Large Language Models (LLMs) such as GPT-4represent a revolution in AI due to their human-level capabilities in naturallanguage processing. However, they also pose many significant risks such as thepresence of biased, private, or harmful text, and the unauthorized inclusion ofcopyrighted material. We introduce h2oGPT, a suite of open-source code repositories for thecreation and use of LLMs based on Generative Pretrained Transformers (GPTs).The goal of this project is to create the world\u0026rsquo;s best truly open-sourcealternative to closed-source approaches. In collaboration with and as part ofthe incredible and unstoppable open-source community, we open-source severalfine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercialuse under fully permissive Apache 2.0 licenses. Included in our release is100% private document search using natural language. Open-source language models help boost AI development and make it moreaccessible and trustworthy. They lower entry hurdles, allowing people andgroups to tailor these models to their needs. This openness increasesinnovation, transparency, and fairness. An open-source strategy is needed toshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.\r2023-06-15\nMatching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models\nMyles Foley Ambrish Rawat Taesung Lee Yufang Hou Gabriele Picco Giulio Zizzo\nabstract\rabstract: The wide applicability and adaptability of generative large language models(LLMs) has enabled their rapid adoption. While the pre-trained models canperform many tasks, such models are often fine-tuned to improve theirperformance on various downstream applications. However, this leads to issuesover violation of model licenses, model theft, and copyright infringement.Moreover, recent advances show that generative technology is capable ofproducing harmful content which exacerbates the problems of accountabilitywithin model supply chains. Thus, we need a method to investigate how a modelwas trained or a piece of text was generated and what their pre-trained basemodel was. In this paper we take the first step to address this open problem bytracing back the origin of a given fine-tuned LLM to its correspondingpre-trained base model. We consider different knowledge levels and attributionstrategies, and find that we can correctly trace back 8 out of the 10 finetuned models with our best method.\r2023-06-14\nRadiology-GPT: A Large Language Model for Radiology\nZhengliang Liu Aoxiao Zhong Yiwei Li Longtao Yang Chao Ju Zihao Wu Chong Ma Peng Shu Cheng Chen Sekeun Kim Haixing Dai Lin Zhao Dajiang Zhu Jun Liu Wei Liu Dinggang Shen Xiang Li Quanzheng Li Tianming Liu\nabstract\rabstract: We introduce Radiology-GPT, a large language model for radiology. Using aninstruction tuning approach on an extensive dataset of radiology domainknowledge, Radiology-GPT demonstrates superior performance compared to generallanguage models such as StableLM, Dolly and LLaMA. It exhibits significantversatility in radiological diagnosis, research, and communication. This workserves as a catalyst for future developments in clinical NLP. The successfulimplementation of Radiology-GPT is indicative of the potential of localizinggenerative large language models, specifically tailored for distinctive medicalspecialties, while ensuring adherence to privacy standards such as HIPAA. Theprospect of developing individualized, large-scale language models that caterto specific needs of various hospitals presents a promising direction. Thefusion of conversational competence and domain-specific knowledge in thesemodels is set to foster future development in healthcare AI. A demo ofRadiology-GPT is available athttps://huggingface.co/spaces/allen-eric/radiology-gpt.\rProtecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization\nZhigang Kan Linbo Qiao Hao Yu Liwen Peng Yifu Gao Dongsheng Li\nabstract\rabstract: Large Language Models (LLMs) are gaining increasing attention due to theirexceptional performance across numerous tasks. As a result, the general publicutilize them as an influential tool for boosting their productivity whilenatural language processing researchers endeavor to employ them in solvingexisting or new research problems. Unfortunately, individuals can only accesssuch powerful AIs through APIs, which ultimately leads to the transmission ofraw data to the models\u0026rsquo; providers and increases the possibility of privacy dataleakage. Current privacy-preserving methods for cloud-deployed language modelsaim to protect privacy information in the pre-training dataset or during themodel training phase. However, they do not meet the specific challengespresented by the remote access approach of new large-scale language models. This paper introduces a novel task, \u0026ldquo;User Privacy Protection for DialogueModels,\u0026rdquo; which aims to safeguard sensitive user information from any possibledisclosure while conversing with chatbots. We also present an evaluation schemefor this task, which covers evaluation metrics for privacy protection, dataavailability, and resistance to simulation attacks. Moreover, we propose thefirst framework for this task, namely privacy protection through textsanitization. Before sending the input to remote large models, it filters outthe sensitive information, using several rounds of text sanitization based onprivacy types that users define. Upon receiving responses from the largermodel, our framework automatically restores privacy to ensure that theconversation goes smoothly, without intervention from the privacy filter.Experiments based on real-world datasets demonstrate the efficacy of ourprivacy-preserving approach against eavesdropping from potential attackers.\r2023-06-13\nFriend or Foe Inside? Exploring In-Process Isolation to Maintain Memory Safety for Unsafe Rust\nMerve Gülmez Thomas Nyman Christoph Baumann Jan Tobias Mühlberg\nabstract\rabstract: Rust is a popular memory-safe systems programming language. In order tointeract with hardware or call into non-Rust libraries, Rust provides\\emph{unsafe} language features that shift responsibility for ensuring memorysafety to the developer. Failing to do so, may lead to memory safety violationsin unsafe code which can violate safety of the entire application. In this workwe explore in-process isolation with Memory Protection Keys as a mechanism toshield safe program sections from safety violations that may happen in unsafesections. Our approach is easy to use and comprehensive as it prevents heap andstack-based violations. We further compare process-based and in-processisolation mechanisms and the necessary requirements for data serialization,communication, and context switching. Our results show that in-processisolation can be effective and efficient, permits for a high degree ofautomation, and also enables a notion of application rewinding where the safeprogram section may detect and safely handle violations in unsafe code.\rPersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer\nXu Han Bin Guo Yoon Jung Benjamin Yao Yu Zhang Xiaohu Liu Chenlei Guo\nabstract\rabstract: Personalized dialogue agents (DAs) powered by large pre-trained languagemodels (PLMs) often rely on explicit persona descriptions to maintainpersonality consistency. However, such descriptions may not always be availableor may pose privacy concerns. To tackle this bottleneck, we introducePersonaPKT, a lightweight transfer learning approach that can buildpersona-consistent dialogue models without explicit persona descriptions. Byrepresenting each persona as a continuous vector, PersonaPKT learns implicitpersona-specific features directly from a small number of dialogue samplesproduced by the same persona, adding less than 0.1% trainable parameters foreach persona on top of the PLM backbone. Empirical results demonstrate thatPersonaPKT effectively builds personalized DAs with high storage efficiency,outperforming various baselines in terms of persona consistency whilemaintaining good response generation quality. In addition, it enhances privacyprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is aneffective solution for creating personalized DAs that respect user privacy.\rDifferentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling\nXiaoyun Li Ping Li\nabstract\rabstract: Minwise hashing (MinHash) is a standard algorithm widely used in theindustry, for large-scale search and learning applications with the binary(0/1) Jaccard similarity. One common use of MinHash is for processing massiven-gram text representations so that practitioners do not have to materializethe original data (which would be prohibitive). Another popular use of MinHashis for building hash tables to enable sub-linear time approximate near neighbor(ANN) search. MinHash has also been used as a tool for building large-scalemachine learning systems. The standard implementation of MinHash requiresapplying $K$ random permutations. In comparison, the method of one permutationhashing (OPH), is an efficient alternative of MinHash which splits the datavectors into $K$ bins and generates hash values within each bin. OPH issubstantially more efficient and also more convenient to use. In this paper, we combine the differential privacy (DP) with OPH (as well asMinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adoptedto deal with empty bins in OPH. A detailed roadmap to the algorithm design ispresented along with the privacy analysis. An analytical comparison of ourproposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided tojustify the advantage of DP-OPH. Experiments on similarity search confirm themerits of DP-OPH, and guide the choice of the proper variant in differentpractical scenarios. Our technique is also extended to bin-wise consistentweighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS fornon-binary data. Experiments on classification tasks demonstrate that DP-BCWSis able to achieve excellent utility at around $\\epsilon = 5\\sim 10$, where$\\epsilon$ is the standard parameter in the language of $(\\epsilon,\\delta)$-DP.\r2023-06-12\n\u0026ldquo;Private Prediction Strikes Back!\u0026rsquo;\u0026rsquo; Private Kernelized Nearest Neighbors with Individual Renyi Filter\nYuqing Zhu Xuandong Zhao Chuan Guo Yu-Xiang Wang\nabstract\rabstract: Most existing approaches of differentially private (DP) machine learningfocus on private training. Despite its many advantages, private training lacksthe flexibility in adapting to incremental changes to the training dataset suchas deletion requests from exercising GDPR\u0026rsquo;s right to be forgotten. We revisit along-forgotten alternative, known as private prediction, and propose a newalgorithm named Individual Kernelized Nearest Neighbor (Ind-KNN). Ind-KNN iseasily updatable over dataset changes and it allows precise control of theR'{e}nyi DP at an individual user level \u0026ndash; a user\u0026rsquo;s privacy loss is measuredby the exact amount of her contribution to predictions; and a user is removedif her prescribed privacy budget runs out. Our results show that Ind-KNNconsistently improves the accuracy over existing private prediction methods fora wide range of $\\epsilon$ on four vision and language tasks. We alsoillustrate several cases under which Ind-KNN is preferable over privatetraining with NoisySGD.\rBackdooring Neural Code Search\nWeisong Sun Yuchen Chen Guanhong Tao Chunrong Fang Xiangyu Zhang Quanjun Zhang Bin Luo\nabstract\rabstract: Reusing off-the-shelf code snippets from online repositories is a commonpractice, which significantly enhances the productivity of software developers.To find desired code snippets, developers resort to code search engines throughnatural language queries. Neural code search models are hence behind many suchengines. These models are based on deep learning and gain substantial attentiondue to their impressive performance. However, the security aspect of thesemodels is rarely studied. Particularly, an adversary can inject a backdoor inneural code search models, which return buggy or even vulnerable code withsecurity/privacy issues. This may impact the downstream software (e.g., stocktrading systems and autonomous driving) and cause financial loss and/orlife-threatening incidents. In this paper, we demonstrate such attacks arefeasible and can be quite stealthy. By simply modifying one variable/functionname, the attacker can make buggy/vulnerable code rank in the top 11%. Ourattack BADCODE features a special trigger generation and injection procedure,making the attack more effective and stealthy. The evaluation is conducted ontwo neural code search models and the results show our attack outperformsbaselines by 60%. Our user study demonstrates that our attack is more stealthythan the baseline by two times based on the F1 score.\r2023-06-11\nDeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection\nHossein Aboutalebi Dayou Mao Carol Xu Alexander Wong\nabstract\rabstract: The tremendous recent advances in generative artificial intelligencetechniques have led to significant successes and promise in a wide range ofdifferent applications ranging from conversational agents and textual contentgeneration to voice and visual synthesis. Amid the rise in generative AI andits increasing widespread adoption, there has been significant growing concernover the use of generative AI for malicious purposes. In the realm of visualcontent synthesis using generative AI, key areas of significant concern hasbeen image forgery (e.g., generation of images containing or derived fromcopyright content), and data poisoning (i.e., generation of adversariallycontaminated images). Motivated to address these key concerns to encourageresponsible generative AI, we introduce the DeepfakeArt Challenge, alarge-scale challenge benchmark dataset designed specifically to aid in thebuilding of machine learning algorithms for generative AI art forgery and datapoisoning detection. Comprising of over 32,000 records across a variety ofgenerative forgery and data poisoning techniques, each entry consists of a pairof images that are either forgeries / adversarially contaminated or not. Eachof the generated images in the DeepfakeArt Challenge benchmark dataset has beenquality checked in a comprehensive manner. The DeepfakeArt Challenge is a corepart of GenAI4Good, a global open source initiative for accelerating machinelearning for promoting responsible creation and deployment of generative AI forgood.\r2023-06-09\nProtect Your Prompts: Protocols for IP Protection in LLM Applications\nM. A. van Wyk M. Bekker X. L. Richards K. J. Nixon\nabstract\rabstract: With the rapid adoption of AI in the form of large language models (LLMs),the potential value of carefully engineered prompts has become significant.However, to realize this potential, prompts should be tradable on an openmarket. Since prompts are, at present, generally economically non-excludable,by virtue of their nature as text, no general competitive market has yet beenestablished. This note discusses two protocols intended to provide protectionof prompts, elevating their status as intellectual property, thus confirmingthe intellectual property rights of prompt engineers, and potentiallysupporting the flourishing of an open market for LLM prompts.\rLearning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning\nRuihan Wu Xiangyu Chen Chuan Guo Kilian Q. Weinberger\nabstract\rabstract: Gradient inversion attack enables recovery of training samples from modelgradients in federated learning (FL), and constitutes a serious threat to dataprivacy. To mitigate this vulnerability, prior work proposed both principleddefenses based on differential privacy, as well as heuristic defenses based ongradient compression as countermeasures. These defenses have so far been veryeffective, in particular those based on gradient compression that allow themodel to maintain high accuracy while greatly reducing the effectiveness ofattacks. In this work, we argue that such findings underestimate the privacyrisk in FL. As a counterexample, we show that existing defenses can be brokenby a simple adaptive attack, where a model trained on auxiliary data is able toinvert gradients on both vision and language tasks.\rRobust Multi-bit Natural Language Watermarking through Invariant Features\nKiYoon Yoo Wonhyuk Ahn Jiho Jang Nojun Kwak\nabstract\rabstract: Recent years have witnessed a proliferation of valuable original naturallanguage contents found in subscription-based media outlets, web novelplatforms, and outputs of large language models. However, these contents aresusceptible to illegal piracy and potential misuse without proper securitymeasures. This calls for a secure watermarking system to guarantee copyrightprotection through leakage tracing or ownership identification. To effectivelycombat piracy and protect copyrights, a multi-bit watermarking framework shouldbe able to embed adequate bits of information and extract the watermarks in arobust manner despite possible corruption. In this work, we explore ways toadvance both payload and robustness by following a well-known proposition fromimage watermarking and identify features in natural language that are invariantto minor corruption. Through a systematic analysis of the possible sources oferrors, we further propose a corruption-resistant infill model. Our full methodimproves upon the previous work on robustness by +16.8% point on average onfour datasets, three corruption types, and two corruption ratios. Codeavailable at https://github.com/bangawayoo/nlp-watermarking.\rPrivacy Aware Question-Answering System for Online Mental Health Risk Assessment\nPrateek Chhikara Ujjwal Pasupulety John Marshall Dhiraj Chaurasia Shweta Kumari\nabstract\rabstract: Social media platforms have enabled individuals suffering from mentalillnesses to share their lived experiences and find the online supportnecessary to cope. However, many users fail to receive genuine clinicalsupport, thus exacerbating their symptoms. Screening users based on what theypost online can aid providers in administering targeted healthcare and minimizefalse positives. Pre-trained Language Models (LMs) can assess users\u0026rsquo; socialmedia data and classify them in terms of their mental health risk. We propose aQuestion-Answering (QA) approach to assess mental health risk using theUnified-QA model on two large mental health datasets. To protect user data, weextend Unified-QA by anonymizing the model training process using differentialprivacy. Our results demonstrate the effectiveness of modeling risk assessmentas a QA task, specifically for mental health use cases. Furthermore, themodel\u0026rsquo;s performance decreases by less than 1% with the inclusion ofdifferential privacy. The proposed system\u0026rsquo;s performance is indicative of apromising research direction that will lead to the development of privacy-awarediagnostic systems.\r2023-06-08\nPrivacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization\nOleksandr Yermilov Vipul Raheja Artem Chernodub\nabstract\rabstract: This work investigates the effectiveness of different pseudonymizationtechniques, ranging from rule-based substitutions to using pre-trained LargeLanguage Models (LLMs), on a variety of datasets and models used for two widelyused NLP tasks: text classification and summarization. Our work providescrucial insights into the gaps between original and anonymized data (focusingon the pseudonymization technique) and model quality and fosters futureresearch into higher-quality anonymization techniques to better balance thetrade-offs between data protection and utility preservation. We make our code,pseudonymized datasets, and downstream models publicly available\rMulti-Epoch Matrix Factorization Mechanisms for Private Machine Learning\nChristopher A. Choquette-Choo H. Brendan McMahan Keith Rush Abhradeep Thakurta\nabstract\rabstract: We introduce new differentially private (DP) mechanisms for gradient-basedmachine learning (ML) with multiple passes (epochs) over a dataset,substantially improving the achievable privacy-utility-computation tradeoffs.We formalize the problem of DP mechanisms for adaptive streams with multipleparticipations and introduce a non-trivial extension of online matrixfactorization DP mechanisms to our setting. This includes establishing thenecessary theory for sensitivity calculations and efficient computation ofoptimal matrices. For some applications like $\u0026gt;!! 10,000$ SGD steps, applyingthese optimal techniques becomes computationally expensive. We thus design anefficient Fourier-transform-based mechanism with only a minor utility loss.Extensive empirical evaluation on both example-level DP for imageclassification and user-level DP for language modeling demonstrate substantialimprovements over all previous methods, including the widely-used DP-SGD .Though our primary application is to ML, our main DP results are applicable toarbitrary linear queries and hence may have much broader applicability.\rPandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\nYidong Wang Zhuohao Yu Zhengran Zeng Linyi Yang Cunxiang Wang Hao Chen Chaoya Jiang Rui Xie Jindong Wang Xing Xie Wei Ye Shikun Zhang Yue Zhang\nabstract\rabstract: Instruction tuning large language models (LLMs) remains a challenging task,owing to the complexity of hyperparameter selection and the difficulty involvedin evaluating the tuned models. To determine the optimal hyperparameters, anautomatic, robust, and reliable evaluation benchmark is essential. However,establishing such a benchmark is not a trivial task due to the challengesassociated with evaluation accuracy and privacy protection. In response tothese challenges, we introduce a judge large language model, named PandaLM,which is trained to distinguish the superior model given several LLMs.PandaLM\u0026rsquo;s focus extends beyond just the objective correctness of responses,which is the main focus of traditional evaluation datasets. It addresses vitalsubjective factors such as relative conciseness, clarity, adherence toinstructions, comprehensiveness, and formality. To ensure the reliability ofPandaLM, we collect a diverse human-annotated test dataset, where all contextsare generated by humans and labels are aligned with human preferences. Ourresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5\u0026rsquo;s evaluationability and 88.28% of GPT-4\u0026rsquo;s in terms of F1-score on our test dataset. PandaLMenables the evaluation of LLM to be fairer but with less cost, evidenced bysignificant improvements achieved by models tuned through PandaLM compared totheir counterparts trained with default Alpaca\u0026rsquo;s hyperparameters. In addition,PandaLM does not depend on API-based evaluations, thus avoiding potential dataleakage. All resources of PandaLM are released athttps://github.com/WeOpenML/PandaLM.\rSkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model\nJuexiao Zhou Xiaonan He Liyuan Sun Jiannan Xu Xiuying Chen Yuetan Chu Longxi Zhou Xingyu Liao Bin Zhang Xin Gao\nabstract\rabstract: Skin and subcutaneous diseases rank high among the leading contributors tothe global burden of nonfatal diseases, impacting a considerable portion of thepopulation. Nonetheless, the field of dermatology diagnosis faces threesignificant hurdles. Firstly, there is a shortage of dermatologists accessibleto diagnose patients, particularly in rural regions. Secondly, accuratelyinterpreting skin disease images poses a considerable challenge. Lastly,generating patient-friendly diagnostic reports is usually a time-consuming andlabor-intensive task for dermatologists. To tackle these challenges, we presentSkinGPT-4, which is the world\u0026rsquo;s first interactive dermatology diagnostic systempowered by an advanced visual large language model. SkinGPT-4 leverages afine-tuned version of MiniGPT-4, trained on an extensive collection of skindisease images (comprising 52,929 publicly available and proprietary images)along with clinical concepts and doctors\u0026rsquo; notes. We designed a two-steptraining process to allow SkinGPT to express medical features in skin diseaseimages with natural language and make accurate diagnoses of the types of skindiseases. With SkinGPT-4, users could upload their own skin photos fordiagnosis, and the system could autonomously evaluate the images, identifiesthe characteristics and categories of the skin conditions, performs in-depthanalysis, and provides interactive treatment recommendations. Meanwhile,SkinGPT-4\u0026rsquo;s local deployment capability and commitment to user privacy alsorender it an appealing choice for patients in search of a dependable andprecise diagnosis of their skin ailments. To demonstrate the robustness ofSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, whichwere independently reviewed by certified dermatologists, and showed thatSkinGPT-4 could provide accurate diagnoses of skin diseases.\r2023-06-07\nPrivately generating tabular data using language models\nAlexandre Sablayrolles Yue Wang Brian Karrer\nabstract\rabstract: Privately generating synthetic data from a table is an important brick of aprivacy-first world. We propose and investigate a simple approach of treatingeach row in a table as a sentence and training a language model withdifferential privacy. We show this approach obtains competitive results inmodelling tabular data across multiple datasets, even at small scales thatfavor alternative methods based on marginal distributions.\r2023-06-06\nAdversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples\nChumeng Liang Xiaoyu Wu Yang Hua Jiaru Zhang Yiming Xue Tao Song Zhengui Xue Ruhui Ma Haibing Guan\nabstract\rabstract: Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise newcopyright concerns, where infringers benefit from using unauthorized paintingsto train DMs to generate novel paintings in a similar style. To address theseemerging copyright violations, in this paper, we are the first to explore andpropose to utilize adversarial examples for DMs to protect human-createdartworks. Specifically, we first build a theoretical framework to define andevaluate the adversarial examples for DMs. Then, based on this framework, wedesign a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimationof adversarial examples for DMs by optimizing upon different latent variablessampled from the reverse process of DMs. Extensive experiments show that thegenerated adversarial examples can effectively hinder DMs from extracting theirfeatures. Therefore, our method can be a powerful tool for human artists toprotect their copyright against infringers equipped with DM-based AI-for-Artapplications. The code of our method is available on GitHub:https://github.com/mist-project/mist.git.\r2023-06-05\nImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment\nEslam Mohamed Bakr Pengzhan Sun Li Erran Li Mohamed Elhoseiny\nabstract\rabstract: Most pre-trained learning systems are known to suffer from bias, whichtypically emerges from the data, the model, or both. Measuring and quantifyingbias and its sources is a challenging task and has been extensively studied inimage captioning. Despite the significant effort in this direction, we observedthat existing metrics lack consistency in the inclusion of the visual signal.In this paper, we introduce a new bias assessment metric, dubbed$ImageCaptioner^2$, for image captioning. Instead of measuring the absolutebias in the model or the data, $ImageCaptioner^2$ pay more attention to thebias introduced by the model w.r.t the data bias, termed bias amplification.Unlike the existing methods, which only evaluate the image captioningalgorithms based on the generated captions only, $ImageCaptioner^2$incorporates the image while measuring the bias. In addition, we design aformulation for measuring the bias of generated captions as prompt-based imagecaptioning instead of using language classifiers. Finally, we apply our$ImageCaptioner^2$ metric across 11 different image captioning architectures onthree different datasets, i.e., MS-COCO caption dataset, Artemis V1, andArtemis V2, and on three different protected attributes, i.e., gender, race,and emotions. Consequently, we verify the effectiveness of our$ImageCaptioner^2$ metric by proposing AnonymousBench, which is a novel humanevaluation paradigm for bias metrics. Our metric shows significant superiorityover the recent bias metric; LIC, in terms of human alignment, where thecorrelation scores are 80% and 54% for our metric and LIC, respectively. Thecode is available at https://eslambakr.github.io/imagecaptioner2.github.io/.\r2023-06-04\nAdversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy\nXiaoting Li Lingwei Chen Dinghao Wu\nabstract\rabstract: Social media has drastically reshaped the world that allows billions ofpeople to engage in such interactive environments to conveniently create andshare content with the public. Among them, text data (e.g., tweets, blogs)maintains the basic yet important social activities and generates a rich sourceof user-oriented information. While those explicit sensitive user data likecredentials has been significantly protected by all means, personal privateattribute (e.g., age, gender, location) disclosure due to inference attacks issomehow challenging to avoid, especially when powerful natural languageprocessing (NLP) techniques have been effectively deployed to automateattribute inferences from implicit text data. This puts users\u0026rsquo; attributeprivacy at risk. To address this challenge, in this paper, we leverage theinherent vulnerability of machine learning to adversarial attacks, and design anovel text-space Adversarial attack for Social Good, called Adv4SG. In otherwords, we cast the problem of protecting personal attribute privacy as anadversarial attack formulation problem over the social media text data todefend against NLP-based attribute inference attacks. More specifically, Adv4SGproceeds with a sequence of word perturbations under given constraints suchthat the probed attribute cannot be identified correctly. Different from theprior works, we advance Adv4SG by considering social media property, andintroducing cost-effective mechanisms to expedite attribute obfuscation overtext data under the black-box setting. Extensive experiments on real-worldsocial media datasets have demonstrated that our method can effectively degradethe inference accuracy with less computational cost over different attributesettings, which substantially helps mitigate the impacts of inference attacksand thus achieve high performance in user attribute privacy protection.\rModular and On-demand Bias Mitigation with Attribute-Removal Subnetworks\nLukas Hauzenberger Shahed Masoudian Deepak Kumar Markus Schedl Navid Rekabsaz\nabstract\rabstract: Societal biases are reflected in large pre-trained language models and theirfine-tuned versions on downstream tasks. Common in-processing bias mitigationapproaches, such as adversarial training and mutual information removal,introduce additional optimization criteria, and update the model to reach a newdebiased state. However, in practice, end-users and practitioners might preferto switch back to the original model, or apply debiasing only on a specificsubset of protected attributes. To enable this, we propose a novel modular biasmitigation approach, consisting of stand-alone highly sparse debiasingsubnetworks, where each debiasing module can be integrated into the core modelon-demand at inference time. Our approach draws from the concept of \\emph{diff}pruning, and proposes a novel training regime adaptable to variousrepresentation disentanglement optimizations. We conduct experiments on threeclassification tasks with gender, race, and age as protected attributes. Theresults show that our modular approach, while maintaining task performance,improves (or at least remains on-par with) the effectiveness of bias mitigationin comparison with baseline finetuning. Particularly on a two-attributedataset, our approach with separately learned debiasing subnetworks showseffective utilization of either or both the subnetworks for selective biasmitigation.\rProTeCt: Prompt Tuning for Hierarchical Consistency\nTz-Ying Wu Chih-Hui Ho Nuno Vasconcelos\nabstract\rabstract: Large visual-language models, like CLIP, learn generalized representationsand have shown promising zero-shot performance. Few-shot adaptation methods,based on prompt tuning, have also been shown to further improve performance ondownstream datasets. However, these models are not hierarchically consistent.Frequently, they infer incorrect labels at coarser taxonomic class levels, evenwhen the inference at the leaf level (original class labels) is correct. Thisis problematic, given their support for open set classification and, inparticular, open-grained classification, where practitioners define label setsat various levels of granularity. To address this problem, we propose a prompttuning technique to calibrate the hierarchical consistency of modelpredictions. A set of metrics of hierarchical consistency, the HierarchicalConsistent Accuracy (HCA) and the Mean Treecut Accuracy (MTA), are firstproposed to benchmark model performance in the open-granularity setting. Aprompt tuning technique, denoted as Prompt Tuning for Hierarchical Consistency(ProTeCt), is then proposed to calibrate classification across all possiblelabel set granularities. Results show that ProTeCt can be combined withexisting prompt tuning methods to significantly improve open-granularityclassification performance without degradation of the original classificationperformance at the leaf level.\r2023-06-02\nUnlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation\nZhengyue Zhao Jinhao Duan Xing Hu Kaidi Xu Chenan Wang Rui Zhang Zidong Du Qi Guo Yunji Chen\nabstract\rabstract: Diffusion models have demonstrated remarkable performance in image generationtasks, paving the way for powerful AIGC applications. However, thesewidely-used generative models can also raise security and privacy concerns,such as copyright infringement, and sensitive data leakage. To tackle theseissues, we propose a method, Unlearnable Diffusion Perturbation, to safeguardimages from unauthorized exploitation. Our approach involves designing analgorithm to generate sample-wise perturbation noise for each image to beprotected. This imperceptible protective noise makes the data almostunlearnable for diffusion models, i.e., diffusion models trained or fine-tunedon the protected data cannot generate high-quality and diverse images relatedto the protected training data. Theoretically, we frame this as a max-minoptimization problem and introduce EUDP, a noise scheduler-based method toenhance the effectiveness of the protective noise. We evaluate our methods onboth Denoising Diffusion Probabilistic Model and Latent Diffusion Models,demonstrating that training diffusion models on the protected data lead to asignificant reduction in the quality of the generated images. Especially, theexperimental results on Stable Diffusion demonstrate that our methodeffectively safeguards images from being used to train Diffusion Models invarious tasks, such as training specific objects and styles. This achievementholds significant importance in real-world scenarios, as it contributes to theprotection of privacy and copyright against AI-generated content.\rWhen Federated Learning Meets Pre-trained Language Models\u0026rsquo; Parameter-Efficient Tuning Methods\nZhuo Zhang Yuanhang Yang Yong Dai Lizhen Qu Zenglin Xu\nabstract\rabstract: With increasing privacy concerns on data, recent studies have madesignificant progress using federated learning (FL) on privacy-sensitive naturallanguage processing (NLP) tasks. Much literature suggests fully fine-tuningpre-trained language models (PLMs) in the FL paradigm can mitigate the dataheterogeneity problem and close the performance gap with centralized training.However, large PLMs bring the curse of prohibitive communication overhead andlocal model adaptation costs for the FL system. To this end, we introducevarious parameter-efficient tuning (PETuning) methods into federated learning.Specifically, we provide a holistic empirical study of representative PLMstuning methods in FL. The experimental results cover the analysis of dataheterogeneity levels, data scales, and different FL scenarios. Overallcommunication overhead can be significantly reduced by locally tuning andglobally aggregating lightweight model parameters while maintaining acceptableperformance in various FL settings. To facilitate the research of PETuning inFL, we also develop a federated tuning framework FedPETuning, which allowspractitioners to exploit different PETuning methods under the FL trainingparadigm conveniently. The source code is available at\\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.\rAre You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark\nWenjun Peng Jingwei Yi Fangzhao Wu Shangxi Wu Bin Zhu Lingjuan Lyu Binxing Jiao Tong Xu Guangzhong Sun Xing Xie\nabstract\rabstract: Large language models (LLMs) have demonstrated powerful capabilities in bothtext understanding and generation. Companies have begun to offer Embedding as aService (EaaS) based on these LLMs, which can benefit various natural languageprocessing (NLP) tasks for customers. However, previous studies have shown thatEaaS is vulnerable to model extraction attacks, which can cause significantlosses for the owners of LLMs, as training these models is extremely expensive.To protect the copyright of LLMs for EaaS, we propose an Embedding Watermarkmethod called EmbMarker that implants backdoors on embeddings. Our methodselects a group of moderate-frequency words from a general text corpus to forma trigger set, then selects a target embedding as the watermark, and inserts itinto the embeddings of texts containing trigger words as the backdoor. Theweight of insertion is proportional to the number of trigger words included inthe text. This allows the watermark backdoor to be effectively transferred toEaaS-stealer\u0026rsquo;s model for copyright verification while minimizing the adverseimpact on the original embeddings\u0026rsquo; utility. Our extensive experiments onvarious datasets show that our method can effectively protect the copyright ofEaaS models without compromising service quality.\r2023-06-01\nTMI! Finetuned Models Leak Private Information from their Pretraining Data\nJohn Abascal Stanley Wu Alina Oprea Jonathan Ullman\nabstract\rabstract: Transfer learning has become an increasingly popular technique in machinelearning as a way to leverage a pretrained model trained for one task to assistwith building a finetuned model for a related task. This paradigm has beenespecially popular for privacy in machine learning, where the pretrained modelis considered public, and only the data for finetuning is considered sensitive.However, there are reasons to believe that the data used for pretraining isstill sensitive, making it essential to understand how much information thefinetuned model leaks about the pretraining data. In this work we propose a newmembership-inference threat model where the adversary only has access to thefinetuned model and would like to infer the membership of the pretraining data.To realize this threat model, we implement a novel metaclassifier-based attack,TMI, that leverages the influence of memorized pretraining samples onpredictions in the downstream task. We evaluate TMI on both vision and naturallanguage tasks across multiple transfer learning settings, including finetuningwith differential privacy. Through our evaluation, we find that TMI cansuccessfully infer membership of pretraining examples using query access to thefinetuned model.\rUNGOML: Automated Classification of unsafe Usages in Go\nAnna-Katharina Wickert Clemens Damke Lars Baumgärtner Eyke Hüllermeier Mira Mezini\nabstract\rabstract: The Go programming language offers strong protection from memory corruption.As an escape hatch of these protections, it provides the unsafe package.Previous studies identified that this unsafe package is frequently used inreal-world code for several purposes, e.g., serialization or casting types. Dueto the variety of these reasons, it may be possible to refactor specific usagesto avoid potential vulnerabilities. However, the classification of unsafeusages is challenging and requires the context of the call and the program\u0026rsquo;sstructure. In this paper, we present the first automated classifier for unsafeusages in Go, UNGOML, to identify what is done with the unsafe package and whyit is used. For UNGOML, we built four custom deep learning classifiers trainedon a manually labeled data set. We represent Go code as enriched control-flowgraphs (CFGs) and solve the label prediction task with one single-vertex andthree context-aware classifiers. All three context-aware classifiers achieve atop-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore,in a set-valued conformal prediction setting, we achieve accuracies of morethan 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML canbe used to efficiently filter unsafe usages for use cases such as refactoringor a security audit. UNGOML: https://github.com/stg-tud/ungoml Artifact:https://dx.doi.org/10.6084/m9.figshare.22293052\rBag of Tricks for Training Data Extraction from Language Models\nWeichen Yu Tianyu Pang Qian Liu Chao Du Bingyi Kang Yan Huang Min Lin Shuicheng Yan\nabstract\rabstract: With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available athttps://github.com/weichen-yu/LM-Extraction.\rChatGPT as a Text Simplification Tool to Remove Bias\nCharmaine Barker Dimitar Kazakov\nabstract\rabstract: The presence of specific linguistic signals particular to a certain sub-groupof people can be picked up by language models during training. If the modelbegins to associate specific language with a distinct group, any decisions madebased upon this language would hold a strong correlation to a decision basedupon their protected characteristic, leading to possible discrimination. Weexplore a potential technique for bias mitigation in the form of simplificationof text. The driving force of this idea is that simplifying text shouldstandardise language between different sub-groups to one way of speaking whilekeeping the same meaning. The experiment shows promising results as theclassifier accuracy for predicting the sensitive attribute drops by up to 17%for the simplified data.\rChallenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond\nChuan Chen Zhenpeng Wu Yanyi Lai Wenlin Ou Tianchi Liao Zibin Zheng\nabstract\rabstract: Artificial Intelligence Generated Content (AIGC) is one of the latestachievements in AI development. The content generated by related applications,such as text, images and audio, has sparked a heated discussion. Variousderived AIGC applications are also gradually entering all walks of life,bringing unimaginable impact to people\u0026rsquo;s daily lives. However, the rapiddevelopment of such generative tools has also raised concerns about privacy andsecurity issues, and even copyright issues in AIGC. We note that advancedtechnologies such as blockchain and privacy computing can be combined with AIGCtools, but no work has yet been done to investigate their relevance andprospect in a systematic and detailed way. Therefore it is necessary toinvestigate how they can be used to protect the privacy and security of data inAIGC by fully exploring the aforementioned technologies. In this paper, wefirst systematically review the concept, classification and underlyingtechnologies of AIGC. Then, we discuss the privacy and security challengesfaced by AIGC from multiple perspectives and purposefully list thecountermeasures that currently exist. We hope our survey will help researchersand industry to build a more secure and robust AIGC system.\r2023-05-31\nDeep Regression Unlearning\nAyush K Tarun Vikram S Chundawat Murari Mandal Mohan Kankanhalli\nabstract\rabstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: https://github.com/ayu987/deep-regression-unlearning\rSynthetic Pre-Training Tasks for Neural Machine Translation\nZexue He Graeme Blackwood Rameswar Panda Julian McAuley Rogerio Feris\nabstract\rabstract: Pre-training models with large crawled corpora can lead to issues such astoxicity and bias, as well as copyright and privacy concerns. A promising wayof alleviating such concerns is to conduct pre-training with synthetic tasksand data, since no real-world information is ingested by the model. Our goal inthis paper is to understand the factors that contribute to the effectiveness ofpre-training models when using synthetic resources, particularly in the contextof neural machine translation. We propose several novel approaches topre-training translation models that involve different levels of lexical andstructural knowledge, including: 1) generating obfuscated data from a largeparallel corpus 2) concatenating phrase pairs extracted from a smallword-aligned corpus, and 3) generating synthetic parallel data without realhuman language corpora. Our experiments on multiple language pairs reveal thatpre-training benefits can be realized even with high levels of obfuscation orpurely synthetic parallel data. We hope the findings from our comprehensiveempirical analysis will shed light on understanding what matters for NMTpre-training, as well as pave the way for the development of more efficient andless toxic models.\r2023-05-30\nExamining risks of racial biases in NLP tools for child protective services\nAnjalie Field Amanda Coston Nupoor Gandhi Alexandra Chouldechova Emily Putnam-Hornstein David Steier Yulia Tsvetkov\nabstract\rabstract: Although much literature has established the presence of demographic bias innatural language processing (NLP) models, most work relies on curated biasmetrics that may not be reflective of real-world applications. At the sametime, practitioners are increasingly using algorithmic tools in high-stakessettings, with particular recent interest in NLP. In this work, we focus on onesuch setting: child protective services (CPS). CPS workers often write copiousfree-form text notes about families they are working with, and CPS agencies areactively seeking to deploy NLP models to leverage these data. Givenwell-established racial bias in this setting, we investigate possible waysdeployed NLP is liable to increase racial disparities. We specifically examineword statistics within notes and algorithmic fairness in risk prediction,coreference resolution, and named entity recognition (NER). We documentconsistent algorithmic unfairness in NER models, possible algorithmicunfairness in coreference resolution models, and little evidence of exacerbatedracial bias in risk prediction. While there is existing pronounced criticism ofrisk prediction, our results expose previously undocumented risks of racialbias in realistic information extraction systems, highlighting potentialconcerns in deploying them, even though they may appear more benign. Our workserves as a rare realistic examination of NLP algorithmic fairness in apotential deployed setting and a timely investigation of a specific riskassociated with deploying NLP in CPS settings.\rJointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning\nUmang Gupta Aram Galstyan Greg Ver Steeg\nabstract\rabstract: Efficient finetuning of pretrained language transformers is becomingincreasingly prevalent for solving natural language processing tasks. Whileeffective, it can still require a large number of tunable parameters. This canbe a drawback for low-resource applications and training withdifferential-privacy constraints, where excessive noise may be introducedduring finetuning. To this end, we propose a novel language transformerfinetuning strategy that introduces task-specific parameters in multipletransformer layers. These parameters are derived from fixed random projectionsof a single trainable vector, enabling finetuning with significantly fewerparameters while maintaining performance. We achieve within 5% of fullfinetuning performance on GLUE tasks with as few as 4,100 parameters per task,outperforming other parameter-efficient finetuning approaches that use asimilar number of per-task parameters. Besides, the random projections can beprecomputed at inference, avoiding additional computational latency. All thesemake our method particularly appealing for low-resource applications. Finally,our method achieves the best or comparable utility compared to several recentfinetuning methods when training with the same privacy constraints,underscoring its effectiveness and potential real-world impact.\rDoes CLIP Know My Face?\nDominik Hintersdorf Lukas Struppek Manuel Brack Felix Friedrich Patrick Schramowski Kristian Kersting\nabstract\rabstract: With the rise of deep learning in various applications, privacy concernsaround the protection of training data has become a critical area of research.Whereas prior studies have focused on privacy risks in single-modal models, weintroduce a novel method to assess privacy for multi-modal models, specificallyvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)reveals whether an individual was included in the training data by querying themodel with images of the same person. Letting the model choose from a widevariety of possible text labels, the model reveals whether it recognizes theperson and, therefore, was used for training. Our large-scale experiments onCLIP demonstrate that individuals used for training can be identified with veryhigh accuracy. We confirm that the model has learned to associate names withdepicted individuals, implying the existence of sensitive information that canbe extracted by adversaries. Our results highlight the need for strongerprivacy protection in large-scale models and suggest that IDIAs can be used toprove the unauthorized use of data for training and to enforce privacy laws.\r2023-05-29\nREx: Data-Free Residual Quantization Error Expansion\nEdouard Yvinec Arnaud Dapgony Matthieu Cord Kevin Bailly\nabstract\rabstract: Deep neural networks (DNNs) are ubiquitous in computer vision and naturallanguage processing, but suffer from high inference cost. This problem can beaddressed by quantization, which consists in converting floating pointoperations into a lower bit-width format. With the growing concerns on privacyrights, we focus our efforts on data-free methods. However, such techniquessuffer from their lack of adaptability to the target devices, as a hardwaretypically only support specific bit widths. Thus, to adapt to a variety ofdevices, a quantization method shall be flexible enough to find good accuracyv.s. speed trade-offs for every bit width and target device. To achieve this,we propose REx, a quantization method that leverages residual error expansion,along with group sparsity and an ensemble approximation for betterparallelization. REx is backed off by strong theoretical guarantees andachieves superior performance on every benchmarked application (from vision toNLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 toternary quantization).\rVoluminous yet Vacuous? Semantic Capital in an Age of Large Language Models\nLuca Nannini\nabstract\rabstract: Large Language Models (LLMs) have emerged as transformative forces in therealm of natural language processing, wielding the power to generate human-liketext. However, despite their potential for content creation, they carry therisk of eroding our Semantic Capital (SC) - the collective knowledge within ourdigital ecosystem - thereby posing diverse social epistemic challenges. Thispaper explores the evolution, capabilities, and limitations of these models,while highlighting ethical concerns they raise. The study contribution istwo-fold: first, it is acknowledged that, withstanding the challenges oftracking and controlling LLM impacts, it is necessary to reconsider ourinteraction with these AI technologies and the narratives that form publicperception of them. It is argued that before achieving this goal, it isessential to confront a potential deontological tipping point in an increasingAI-driven infosphere. This goes beyond just adhering to AI ethical norms orregulations and requires understanding the spectrum of social epistemic risksLLMs might bring to our collective SC. Secondly, building on Luciano Floridi\u0026rsquo;staxonomy for SC risks, those are mapped within the functionality andconstraints of LLMs. By this outlook, we aim to protect and enrich our SC whilefostering a collaborative environment between humans and AI that augments humanintelligence rather than replacing it.\rFingerprinting Generative Adversarial Networks\nGuanlin Li Guowen Xu Han Qiu Shangwei Guo Run Wang Jiwei Li Tianwei Zhang Rongxing Lu\nabstract\rabstract: Generative Adversarial Networks (GANs) have been widely used in variousapplication scenarios. Since the production of a commercial GAN requiressubstantial computational and human resources, the copyright protection of GANsis urgently needed. In this paper, we present the first fingerprinting schemefor the Intellectual Property (IP) protection of GANs. We break through thestealthiness and robustness bottlenecks suffered by previous fingerprintingmethods for classification models being naively transferred to GANs.Specifically, we innovatively construct a composite deep learning model fromthe target GAN and a classifier. Then we generate fingerprint samples from thiscomposite model, and embed them in the classifier for effective ownershipverification. This scheme inspires some concrete methodologies to practicallyprotect the modern GAN models. Theoretical analysis proves that these methodscan satisfy different security requirements necessary for IP protection. Wealso conduct extensive experiments to show that our solutions outperformexisting strategies.\r2023-05-26\nUnsupervised Melody-Guided Lyrics Generation\nYufei Tian Anjali Narayan-Chen Shereen Oraby Alessandra Cervone Gunnar Sigurdsson Chenyang Tao Wenbo Zhao Tagyoung Chung Jing Huang Nanyun Peng\nabstract\rabstract: Automatic song writing is a topic of significant practical interest. However,its research is largely hindered by the lack of training data due to copyrightconcerns and challenged by its creative nature. Most noticeably, prior worksoften fall short of modeling the cross-modal correlation between melody andlyrics due to limited parallel data, hence generating lyrics that are lesssingable. Existing works also lack effective mechanisms for content control, amuch desired feature for democratizing song creation for people with limitedmusic background. In this work, we propose to generate pleasantly listenablelyrics without training on melody-lyric aligned data. Instead, we design ahierarchical lyric generation framework that disentangles training (basedpurely on text) from inference (melody-guided text generation). At inferencetime, we leverage the crucial alignments between melody and lyrics and compilethe given melody into constraints to guide the generation process. Evaluationresults show that our model can generate high-quality lyrics that are moresingable, intelligible, coherent, and in rhyme than strong baselines includingthose supervised on parallel data.\r2023-05-25\nDon\u0026rsquo;t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text\nAshim Gupta Carter Wood Blum Temma Choji Yingjie Fei Shalin Shah Alakananda Vempala Vivek Srikumar\nabstract\rabstract: Can language models transform inputs to protect text classifiers againstadversarial attacks? In this work, we present ATINTER, a model that interceptsand learns to rewrite adversarial inputs to make them non-adversarial for adownstream text classifier. Our experiments on four datasets and five attackmechanisms reveal that ATINTER is effective at providing better adversarialrobustness than existing defense approaches, without compromising taskaccuracy. For example, on sentiment classification using the SST-2 dataset, ourmethod improves the adversarial accuracy over the best existing defenseapproach by more than 4% with a smaller decrease in task accuracy (0.5% vs2.5%). Moreover, we show that ATINTER generalizes across multiple downstreamtasks and classifiers without having to explicitly retrain it for thosesettings. Specifically, we find that when ATINTER is trained to removeadversarial perturbations for the sentiment classification task on the SST-2dataset, it even transfers to a semantically different task of newsclassification (on AGNews) and improves the adversarial robustness by more than10%.\rTraining Data Extraction From Pre-trained Language Models: A Survey\nShotaro Ishihara\nabstract\rabstract: As the deployment of pre-trained language models (PLMs) expands, pressingsecurity concerns have arisen regarding the potential for malicious extractionof training data, posing a threat to data privacy. This study is the first toprovide a comprehensive survey of training data extraction from PLMs. Ourreview covers more than 100 key papers in fields such as natural languageprocessing and security. First, preliminary knowledge is recapped and ataxonomy of various definitions of memorization is presented. The approachesfor attack and defense are then systemized. Furthermore, the empirical findingsof several quantitative studies are highlighted. Finally, future researchdirections based on this review are suggested.\r2023-05-24\nFlocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\nHaonan Duan Adam Dziedzic Nicolas Papernot Franziska Boenisch\nabstract\rabstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock\u0026rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.\rFrom Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads\nP. V. Sai Charan Hrushikesh Chunduri P. Mohan Anand Sandeep K Shukla\nabstract\rabstract: This research article critically examines the potential risks andimplications arising from the malicious utilization of large languagemodels(LLM), focusing specifically on ChatGPT and Google\u0026rsquo;s Bard. Although theselarge language models have numerous beneficial applications, the misuse of thistechnology by cybercriminals for creating offensive payloads and tools is asignificant concern. In this study, we systematically generated implementablecode for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, andconduct a comparative analysis of its performance with Google\u0026rsquo;s Bard. Ourexperimentation reveals that ChatGPT has the potential to enable attackers toaccelerate the operation of more targeted and sophisticated attacks.Additionally, the technology provides amateur attackers with more capabilitiesto perform a wide range of attacks and empowers script kiddies to developcustomized tools that contribute to the acceleration of cybercrime.Furthermore, LLMs significantly benefits malware authors, particularlyransomware gangs, in generating sophisticated variants of wiper and ransomwareattacks with ease. On a positive note, our study also highlights how offensivesecurity researchers and pentesters can make use of LLMs to simulate realisticattack scenarios, identify potential vulnerabilities, and better protectorganizations. Overall, we conclude by emphasizing the need for increasedvigilance in mitigating the risks associated with LLMs. This includesimplementing robust security measures, increasing awareness and educationaround the potential risks of this technology, and collaborating with securityexperts to stay ahead of emerging threats.\rMachine Unlearning: its nature, scope, and importance for a \u0026ldquo;delete culture\u0026rdquo;\nLuciano Floridi\nabstract\rabstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them \u0026lsquo;forget\u0026rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.\rTrade-Offs Between Fairness and Privacy in Language Modeling\nCleo Matzken Steffen Eger Ivan Habernal\nabstract\rabstract: Protecting privacy in contemporary NLP models is gaining in importance. Sodoes the need to mitigate social biases of such models. But can we have both atthe same time? Existing research suggests that privacy preservation comes atthe price of worsening biases in classification tasks. In this paper, weexplore the extent to which this tradeoff really holds when we incorporate bothprivacy preservation and de-biasing techniques into training text generationmodels. How does improving the model along one dimension affect the otherdimension as well as the utility of the model? We conduct an extensive set ofexperiments that include bias detection, privacy attacks, language modeling,and performance on downstream tasks.\rPrivacy Implications of Retrieval-Based Language Models\nYangsibo Huang Samyak Gupta Zexuan Zhong Kai Li Danqi Chen\nabstract\rabstract: Retrieval-based language models (LMs) have demonstrated improvedinterpretability, factuality, and adaptability compared to their parametriccounterparts, by incorporating retrieved text from external datastores. Whileit is well known that parametric models are prone to leaking private data, itremains unclear how the addition of a retrieval datastore impacts modelprivacy. In this work, we present the first study of privacy risks inretrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimaldesign and training procedure in domains where privacy is of concern, aiming tostrike a balance between utility and privacy. Crucially, we find that $k$NN-LMsare more susceptible to leaking private information from their privatedatastore than parametric models. We further explore mitigations of privacyrisks. When privacy information is targeted and readily detected in the text,we find that a simple sanitization step would completely eliminate the risks,while decoupling query and key encoders achieves an even better utility-privacytrade-off. Otherwise, we consider strategies of mixing public and private datain both datastore and encoder training. While these methods offer modestimprovements, they leave considerable room for future work. Together, ourfindings provide insights for practitioners to better understand and mitigateprivacy risks in retrieval-based LMs. Our code is available at:https://github.com/Princeton-SysML/kNNLM_privacy .\rCan Copyright be Reduced to Privacy?\nNiva Elkin-Koren Uri Hacohen Roi Livni Shay Moran\nabstract\rabstract: There is an increasing concern that generative AI models may produce outputsthat are remarkably similar to the copyrighted input content on which they aretrained. This worry has escalated as the quality and complexity of generativemodels have immensely improved, and the availability of large datasetscontaining copyrighted material has increased. Researchers are activelyexploring strategies to mitigate the risk of producing infringing samples, anda recent line of work suggests to employ techniques such as differentialprivacy and other forms of algorithmic stability to safeguard copyrightedcontent. In this work, we examine the question whether algorithmic stabilitytechniques such as differential privacy are suitable to ensure the responsibleuse of generative models without inadvertently violating copyright laws. Weargue that there are fundamental differences between privacy and copyright thatshould not be overlooked. In particular we highlight that although algorithmicstability may be perceived as a practical tool to detect copying, it does notnecessarily equate to copyright protection. Therefore, if it is adopted asstandard for copyright infringement, it may undermine copyright law intendedpurposes.\rMGeo: Multi-Modal Geographic Pre-Training Method\nRuixue Ding Boli Chen Pengjun Xie Fei Huang Xin Li Qiang Zhang Yao Xu\nabstract\rabstract: As a core task in location-based services (LBS) (e.g., navigation maps),query and point of interest (POI) matching connects users\u0026rsquo; intent withreal-world geographic information. Recently, pre-trained models (PTMs) havemade advancements in many natural language processing (NLP) tasks. Generictext-based PTMs do not have enough geographic knowledge for query-POI matching.To overcome this limitation, related literature attempts to employdomain-adaptive pre-training based on geo-related corpus. However, a querygenerally contains mentions of multiple geographic objects, such as nearbyroads and regions of interest (ROIs). The geographic context (GC), i.e., thesediverse geographic objects and their relationships, is therefore pivotal toretrieving the most relevant POI. Single-modal PTMs can barely make use of theimportant GC and therefore have limited performance. In this work, we propose anovel query-POI matching method Multi-modal Geographic language model (MGeo),which comprises a geographic encoder and a multi-modal interaction module. MGeorepresents GC as a new modality and is able to fully extract multi-modalcorrelations for accurate query-POI matching. Besides, there is no publiclyavailable benchmark for this topic. In order to facilitate further research, webuild a new open-source large-scale benchmark Geographic TExtual Similarity(GeoTES). The POIs come from an open-source geographic information system(GIS). The queries are manually generated by annotators to prevent privacyissues. Compared with several strong baselines, the extensive experimentresults and detailed ablation analyses on GeoTES demonstrate that our proposedmulti-modal pre-training method can significantly improve the query-POImatching capability of generic PTMs, even when the queries\u0026rsquo; GC is not provided.Our code and dataset are publicly available athttps://github.com/PhantomGrapes/MGeo.\r2023-05-23\nA Customized Text Sanitization Mechanism with Differential Privacy\nHuimin Chen Fengran Mo Yanhao Wang Cen Chen Jian-Yun Nie Chengyu Wang Jamie Cui\nabstract\rabstract: As privacy issues are receiving increasing attention within the NaturalLanguage Processing (NLP) community, numerous methods have been proposed tosanitize texts subject to differential privacy. However, the state-of-the-arttext sanitization mechanisms based on metric local differential privacy (MLDP)do not apply to non-metric semantic similarity measures and cannot achieve goodtrade-offs between privacy and utility. To address the above limitations, wepropose a novel Customized Text (CusText) sanitization mechanism based on theoriginal $\\epsilon$-differential privacy (DP) definition, which is compatiblewith any similarity measure. Furthermore, CusText assigns each input token acustomized output set of tokens to provide more advanced privacy protection atthe token level. Extensive experiments on several benchmark datasets show thatCusText achieves a better trade-off between privacy and utility than existingmechanisms. The code is available at https://github.com/sai4july/CusText.\r2023-05-22\nThe \u0026ldquo;code\u0026rsquo;\u0026rsquo; of Ethics:A Holistic Audit of AI Code Generators\nWanlun Ma Yiliao Song Minhui Xue Sheng Wen Yang Xiang\nabstract\rabstract: AI-powered programming language generation (PLG) models have gainedincreasing attention due to their ability to generate source code of programsin a few seconds with a plain program description. Despite their remarkableperformance, many concerns are raised over the potential risks of theirdevelopment and deployment, such as legal issues of copyright infringementinduced by training usage of licensed code, and malicious consequences due tothe unregulated use of these models. In this paper, we present thefirst-of-its-kind study to systematically investigate the accountability of PLGmodels from the perspectives of both model development and deployment. Inparticular, we develop a holistic framework not only to audit the training datausage of PLG models, but also to identify neural code generated by PLG modelsas well as determine its attribution to a source model. To this end, we proposeusing membership inference to audit whether a code snippet used is in the PLGmodel\u0026rsquo;s training data. In addition, we propose a learning-based method todistinguish between human-written code and neural code. In neural codeattribution, through both empirical and theoretical analysis, we show that itis impossible to reliably attribute the generation of one code snippet to onemodel. We then propose two feasible alternative methods: one is to attributeone neural code snippet to one of the candidate PLG models, and the other is toverify whether a set of neural code snippets can be attributed to a given PLGmodel. The proposed framework thoroughly examines the accountability of PLGmodels which are verified by extensive experiments. The implementations of ourproposed framework are also encapsulated into a new artifact, namedCodeForensic, to foster further research.\rEnhancing Small Medical Learners with Privacy-preserving Contextual Prompting\nXinlu Zhang Shiyang Li Xianjun Yang Chenxin Tian Yao Qin Linda Ruth Petzold\nabstract\rabstract: Large language models (LLMs) demonstrate remarkable medical expertise, butdata privacy concerns impede their direct use in healthcare environments.Although offering improved data privacy protection, domain-specific smalllanguage models (SLMs) often underperform LLMs, emphasizing the need formethods that reduce this performance gap while alleviating privacy concerns. Inthis paper, we present a simple yet effective method that harnesses LLMs\u0026rsquo;medical proficiency to boost SLM performance in medical tasks underprivacy-restricted scenarios. Specifically, we mitigate patient privacy issuesby extracting keywords from medical data and prompting the LLM to generate amedical knowledge-intensive context by simulating clinicians\u0026rsquo; thoughtprocesses. This context serves as additional input for SLMs, augmenting theirdecision-making capabilities. Our method significantly enhances performance inboth few-shot and full training settings across three medicalknowledge-intensive tasks, achieving up to a 22.57% increase in absoluteaccuracy compared to SLM fine-tuning without context, and sets newstate-of-the-art results in two medical tasks within privacy-restrictedscenarios. Further out-of-domain testing and experiments in two general domaindatasets showcase its generalizability and broad applicability.\rMist: Towards Improved Adversarial Examples for Diffusion Models\nChumeng Liang Xiaoyu Wu\nabstract\rabstract: Diffusion Models (DMs) have empowered great success inartificial-intelligence-generated content, especially in artwork creation, yetraising new concerns in intellectual properties and copyright. For example,infringers can make profits by imitating non-authorized human-created paintingswith DMs. Recent researches suggest that various adversarial examples fordiffusion models can be effective tools against these copyright infringements.However, current adversarial examples show weakness in transferability overdifferent painting-imitating methods and robustness under straightforwardadversarial defense, for example, noise purification. We surprisingly find thatthe transferability of adversarial examples can be significantly enhanced byexploiting a fused and modified adversarial loss term under consistentparameters. In this work, we comprehensively evaluate the cross-methodtransferability of adversarial examples. The experimental observation showsthat our method generates more transferable adversarial examples with evenstronger robustness against the simple adversarial defense.\r2023-05-21\nWatermarking Diffusion Model\nYugeng Liu Zheng Li Michael Backes Yun Shen Yang Zhang\nabstract\rabstract: The availability and accessibility of diffusion models (DMs) havesignificantly increased in recent years, making them a popular tool foranalyzing and predicting the spread of information, behaviors, or phenomenathrough a population. Particularly, text-to-image diffusion models (e.g., DALLE2 and Latent Diffusion Models (LDMs) have gained significant attention inrecent years for their ability to generate high-quality images and performvarious image synthesis tasks. Despite their widespread adoption in manyfields, DMs are often susceptible to various intellectual property violations.These can include not only copyright infringement but also more subtle forms ofmisappropriation, such as unauthorized use or modification of the model.Therefore, DM owners must be aware of these potential risks and takeappropriate steps to protect their models. In this work, we are the first toprotect the intellectual property of DMs. We propose a simple but effectivewatermarking scheme that injects the watermark into the DMs and can be verifiedby the pre-defined prompts. In particular, we propose two differentwatermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injectsthe watermark into the LDMs and activates it using a prompt containing thewatermark. On the other hand, the FIXEDWM is considered more advanced andstealthy compared to the NAIVEWM, as it can only activate the watermark whenusing a prompt containing a trigger in a fixed position. We conducted arigorous evaluation of both approaches, demonstrating their effectiveness inwatermark injection and verification with minimal impact on the LDM\u0026rsquo;sfunctionality.\rCommunication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter\nYi Liu Xiaohan Bi Lei Li Sishuo Chen Wenkai Yang Xu Sun\nabstract\rabstract: Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as apromising paradigm for institutions with limited language resources. Thisapproach allows multiple institutions to act as clients and train a unifiedmodel through model synchronization, rather than collecting sensitive data forcentralized training. This significantly reduces the cost of corpus collectionand preserves data privacy. However, as pre-trained language models (PLMs)continue to increase in size, the communication cost for transmittingparameters during synchronization has become a training speed bottleneck. Inthis paper, we propose a communication-efficient Fed-MNMT framework thataddresses this issue by keeping PLMs frozen and only transferring lightweightadapter modules between clients. Since different language pairs exhibitsubstantial discrepancies in data distributions, adapter parameters of clientsmay conflict with each other. To tackle this, we explore various clusteringstrategies to group parameters for integration and mitigate the negativeeffects of conflicting parameters. Experimental results demonstrate that ourframework reduces communication cost by over 98% while achieving similar oreven better performance compared to competitive baselines. Further analysisreveals that clustering strategies effectively solve the problem of linguisticdiscrepancy and pruning adapter modules further improves communicationefficiency.\r2023-05-20\nCan Public Large Language Models Help Private Cross-device Federated Learning?\nBoxin Wang Yibo Jacky Zhang Yuan Cao Bo Li H. Brendan McMahan Sewoong Oh Zheng Xu Manzil Zaheer\nabstract\rabstract: We study (differentially) private federated learning (FL) of language models.The language models in cross-device FL are relatively small, which can betrained with meaningful formal user-level differential privacy (DP) guaranteeswhen massive parallelism in training is enabled by the participation of amoderate size of users. Recently, public data has been used to improveprivacy-utility trade-offs for both large and small language models. In thiswork, we provide a systematic study of using large-scale public data and LLMsto help differentially private training of on-device FL models, and furtherimprove the privacy-utility tradeoff by techniques of distillation. Moreover,we propose a novel distribution matching algorithm with theoretical groundingto sample public data close to private data distribution, which significantlyimproves the sample efficiency of (pre-)training on public data. The proposedmethod is efficient and effective for training private model by takingadvantage of public data, especially for customized on-device architecturesthat do not have ready-to-use pre-trained models.\r2023-05-19\nControlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning\nMustafa Safa Ozdayi Charith Peris Jack FitzGerald Christophe Dupuy Jimit Majmudar Haidar Khan Rahil Parikh Rahul Gupta\nabstract\rabstract: Large Language Models (LLMs) are known to memorize significant portions oftheir training data. Parts of this memorized content have been shown to beextractable by simply querying the model, which poses a privacy risk. Wepresent a novel approach which uses prompt-tuning to control the extractionrates of memorized content in LLMs. We present two prompt training strategiesto increase and decrease extraction rates, which correspond to an attack and adefense, respectively. We demonstrate the effectiveness of our techniques byusing models from the GPT-Neo family on a public benchmark. For the 1.3Bparameter GPT-Neo model, our attack yields a 9.3 percentage point increase inextraction rate compared to our baseline. Our defense can be tuned to achievedifferent privacy-utility trade-offs by a user-specified hyperparameter. Weachieve an extraction rate reduction of up to 97.7% relative to our baseline,with a perplexity increase of 16.9%.\rTowards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models\nJiayi Fu Haoying Han Xing Su Chao Fan\nabstract\rabstract: Pre-trained large language models (PLMs) have the potential to support urbanscience research through content creation, information extraction, assistedprogramming, text classification, and other technical advances. In thisresearch, we explored the opportunities, challenges, and prospects of PLMs inurban science research. Specifically, we discussed potential applications ofPLMs to urban institution, urban space, urban information, and citizenbehaviors research through seven examples using ChatGPT. We also examined thechallenges of PLMs in urban science research from both technical and socialperspectives. The prospects of the application of PLMs in urban scienceresearch were then proposed. We found that PLMs can effectively aid inunderstanding complex concepts in urban science, facilitate urban spatial formidentification, assist in disaster monitoring, and sense public sentiment. Atthe same time, however, the applications of PLMs in urban science research faceevident threats, such as technical limitations, security, privacy, and socialbias. The development of fundamental models based on domain knowledge andhuman-AI collaboration may help improve PLMs to support urban science researchin future.\rChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery\nAnaelia Ovalle Mehrab Beikzadeh Parshan Teimouri Kai-Wei Chang Majid Sarrafzadeh\nabstract\rabstract: Large language models have been useful in expanding mental health caredelivery. ChatGPT, in particular, has gained popularity for its ability togenerate human-like dialogue. However, data-sensitive domains \u0026ndash; including butnot limited to healthcare \u0026ndash; face challenges in using ChatGPT due to privacyand data-ownership concerns. To enable its utilization, we propose a textambiguation framework that preserves user privacy. We ground this in the taskof addressing stress prompted by user-provided texts to demonstrate theviability and helpfulness of privacy-preserved generations. Our results suggestthat chatGPT recommendations are still able to be moderately helpful andrelevant, even when the original user text is not provided.\r2023-05-18\nComparing Biases and the Impact of Multilingual Training across Multiple Languages\nSharon Levy Neha Anna John Ling Liu Yogarshi Vyas Jie Ma Yoshinari Fujinuma Miguel Ballesteros Vittorio Castelli Dan Roth\nabstract\rabstract: Studies in bias and fairness in natural language processing have primarilyexamined social biases within a single language and/or across few attributes(e.g. gender, race). However, biases can manifest differently across variouslanguages for individual attributes. As a result, it is critical to examinebiases within each language and attribute. Of equal importance is to study howthese biases compare across languages and how the biases are affected whentraining a model on multilingual data versus monolingual data. We present abias analysis across Italian, Chinese, English, Hebrew, and Spanish on thedownstream sentiment analysis task to observe whether specific demographics areviewed more positively. We study bias similarities and differences across theselanguages and investigate the impact of multilingual vs. monolingual trainingdata. We adapt existing sentiment bias templates in English to Italian,Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality,and gender. Our results reveal similarities in bias expression such asfavoritism of groups that are dominant in each language\u0026rsquo;s culture (e.g.majority religions and nationalities). Additionally, we find an increasedvariation in predictions across protected groups, indicating biasamplification, after multilingual finetuning in comparison to multilingualpretraining.\rVaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2\nAryo Pradipta Gema Michał Kobiela Achille Fraisse Ajitha Rajan Diego A. Oyarzún Javier Antonio Alfaro\nabstract\rabstract: The SARS-CoV-2 pandemic has emphasised the importance of developing auniversal vaccine that can protect against current and future variants of thevirus. The present study proposes a novel conditional protein Language Modelarchitecture, called Vaxformer, which is designed to produce natural-lookingantigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generatedprotein sequences of the Vaxformer model using DDGun protein stability measure,netMHCpan antigenicity score, and a structure fidelity score with AlphaFold togauge its viability for vaccine development. Our results show that Vaxformeroutperforms the existing state-of-the-art Conditional Variational Autoencodermodel to generate antigenicity-controlled SARS-CoV-2 spike proteins. Thesefindings suggest promising opportunities for conditional Transformer models toexpand our understanding of vaccine design and their role in mitigating globalhealth challenges. The code used in this study is available athttps://github.com/aryopg/vaxformer .\rHow Deep Learning Sees the World: A Survey on Adversarial Attacks \u0026amp; Defenses\nJoana C. Costa Tiago Roxo Hugo Proença Pedro R. M. Inácio\nabstract\rabstract: Deep Learning is currently used to perform multiple tasks, such as objectrecognition, face recognition, and natural language processing. However, DeepNeural Networks (DNNs) are vulnerable to perturbations that alter the networkprediction (adversarial examples), raising concerns regarding its usage incritical areas, such as self-driving vehicles, malware detection, andhealthcare. This paper compiles the most recent adversarial attacks, grouped bythe attacker capacity, and modern defenses clustered by protection strategies.We also present the new advances regarding Vision Transformers, summarize thedatasets and metrics used in the context of adversarial settings, and comparethe state-of-the-art results under different attacks, finishing with theidentification of open issues.\rAugmented Large Language Models with Parametric Knowledge Guiding\nZiyang Luo Can Xu Pu Zhao Xiubo Geng Chongyang Tao Jing Ma Qingwei Lin Daxin Jiang\nabstract\rabstract: Large Language Models (LLMs) have significantly advanced natural languageprocessing (NLP) with their impressive language understanding and generationcapabilities. However, their performance may be suboptimal for domain-specifictasks that require specialized knowledge due to limited exposure to the relateddata. Additionally, the lack of transparency of most state-of-the-art (SOTA)LLMs, which can only be accessed via APIs, impedes further fine-tuning withdomain custom data. Moreover, providing private data to the LLMs\u0026rsquo; owner leadsto data privacy problems. To address these challenges, we propose the novelParametric Knowledge Guiding (PKG) framework, which equips LLMs with aknowledge-guiding module to access relevant knowledge without altering theLLMs\u0026rsquo; parameters. Our PKG is based on open-source \u0026ldquo;white-box\u0026rdquo; language models,allowing offline memory of any knowledge that LLMs require. We demonstrate thatour PKG framework can enhance the performance of \u0026ldquo;black-box\u0026rdquo; LLMs on a range ofdomain knowledge-intensive tasks that require factual (+7.9%), tabular(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.\rEthical ChatGPT: Concerns, Challenges, and Commandments\nJianlong Zhou Heimo Müller Andreas Holzinger Fang Chen\nabstract\rabstract: Large language models, e.g. ChatGPT are currently contributing enormously tomake artificial intelligence even more popular, especially among the generalpopulation. However, such chatbot models were developed as tools to supportnatural language communication between humans. Problematically, it is very mucha ``statistical correlation machine\u0026quot; (correlation instead of causality) andthere are indeed ethical concerns associated with the use of AI language modelssuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlightsspecific ethical concerns on ChatGPT and articulates key challenges whenChatGPT is used in various applications. Practical commandments for differentstakeholders of ChatGPT are also proposed that can serve as checklistguidelines for those applying ChatGPT in their applications. These commandmentexamples are expected to motivate the ethical use of ChatGPT.\r2023-05-17\nLife of PII \u0026ndash; A PII Obfuscation Transformer\nAjinkya Deshmukh Saumya Banthia Anantha Sharma\nabstract\rabstract: Protecting sensitive information is crucial in today\u0026rsquo;s world of LargeLanguage Models (LLMs) and data-driven services. One common method used topreserve privacy is by using data perturbation techniques to reduceoverreaching utility of (sensitive) Personal Identifiable Information (PII)data while maintaining its statistical and semantic properties. Dataperturbation methods often result in significant information loss, making themimpractical for use. In this paper, we propose \u0026lsquo;Life of PII\u0026rsquo;, a novelObfuscation Transformer framework for transforming PII into faux-PII whilepreserving the original information, intent, and context as much as possible.Our approach includes an API to interface with the given document, aconfiguration-based obfuscator, and a model based on the Transformerarchitecture, which has shown high context preservation and performance innatural language processing tasks and LLMs. Our Transformer-based approach learns mapping between the original PII andits transformed faux-PII representation, which we call \u0026ldquo;obfuscated\u0026rdquo; data. Ourexperiments demonstrate that our method, called Life of PII, outperformstraditional data perturbation techniques in terms of both utility preservationand privacy protection. We show that our approach can effectively reduceutility loss while preserving the original information, offering greaterflexibility in the trade-off between privacy protection and data utility. Ourwork provides a solution for protecting PII in various real-world applications.\rNetGPT: Generative Pretrained Transformer for Network Traffic\nXuying Meng Chungang Lin Yequan Wang Yujun Zhang\nabstract\rabstract: All data on the Internet are transferred by network traffic, thus accuratelymodeling network traffic can help improve network services quality and protectdata privacy. Pretrained models for network traffic can utilize large-scale rawdata to learn the essential characteristics of network traffic, and generatedistinguishable results for input traffic without considering specificdownstream tasks. Effective pretrained models can significantly optimize thetraining efficiency and effectiveness of downstream tasks, such as applicationclassification, attack detection and traffic generation. Despite the greatsuccess of pretraining in natural language processing, there is no work in thenetwork field. Considering the diverse demands and characteristics of networktraffic and network tasks, it is non-trivial to build a pretrained model fornetwork traffic and we face various challenges, especially the heterogeneousheaders and payloads in the multi-pattern network traffic and the differentdependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt toprovide a generative pretrained model NetGPT for both traffic understanding andgeneration tasks. We propose the multi-pattern network traffic modeling toconstruct unified text inputs and support both traffic understanding andgeneration tasks. We further optimize the adaptation effect of the pretrainedmodel to diversified tasks by shuffling header fields, segmenting packets inflows, and incorporating diverse task labels with prompts. With diverse trafficdatasets from encrypted software, DNS, private industrial protocols andcryptocurrency mining, expensive experiments demonstrate the effectiveness ofour NetGPT in a range of traffic understanding and generation tasks on trafficdatasets, and outperform state-of-the-art baselines by a wide margin.\rA Novel Procrustes Analysis Method to Quantify Multi-Joint Coordination of the Upper Extremity after Stroke\nKhadija F. Zaidi Michelle Harris-Love\nabstract\rabstract: Upper extremity motor impairment affects about 80% of persons after strokes.For stroke rehabilitation, upper limb kinematic assessments have increasinglybeen used as primary or secondary outcome measures. Studying the upperextremity provides a valuable tool for assessing limb coordination,mal-adaptations, and recovery. There is currently no universal standardizedscale for categorizing multi-joint upper extremity movement. We propose amodified Procrustes statistical shape method as a quantitative analysis thatcan recognize segments of movement where multiple limb segments arecoordinating movement. Generalized Procrustes methods allow data points to becompared across an array simultaneously rather than comparing them in pairs.Rather than rely solely on discrete kinematic values to contrast movement, thismethod allows evaluation of how movement progresses. The Procrustes analysis ofable-bodied movement showed that the hand and forearm segments moved in a morecoordinated manner during initiation. The shoulder and elbow become morecoordinated during movement completion. In impaired movement, this coordinationbetween the hand and forearm is disrupted. Potentially mal-adaptivecompensation occurs between the upper arm and forearm after movement enters thedeceleration phase. The utilization of Procrustes analysis may be a steptowards developing a comprehensive and universal quantitative tool that doesnot require changes to existing treatments or increase patient burden. Copyright 2023 IEEE. Personal use of this material is permitted. Permissionfrom IEEE must be obtained for all other uses, in any current or future media,including reprinting/republishing this material for advertising or promotionalpurposes, creating new collective works, for resale or redistribution toservers or lists, or reuse of any copyrighted component of this work in otherworks.\r2023-05-16\nQuantum Computation by Spin Parity Measurements with Encoded Spin Qubits\nMatthew Brooks Charles Tahan\nabstract\rabstract: Joint measurements of two-Pauli observables are a powerful tool for both thecontrol and protection of quantum information. By following a simple recipe formeasurement choices, single- and two- qubit rotations using two-Pauli parityand single qubit measurements are guaranteed to be unitary whilst requiringonly a single ancilla qubit. This language for measurement based quantumcomputing is shown to be directly applicable to encoded double quantum dotsinglet-triplet spin qubits, by measuring spin-parity between dots fromneighboring qubits. Along with exchange interaction, a complete, leakage free,measurement based gate set can be shown, up to a known Pauli correction. Boththeoretically exact spin-parity measurements and experimentally demonstratedasymmetric spin-parity measurements are shown to be viable for achieving theproposed measurement based scheme, provided some extra leakage mitigatingmeasurement steps. This new method of spin qubit control offers a leakagesuppressed, low resource overhead implementation of a measurement-based controlthat is viable on current spin qubit processor devices.\rBot or Human? Detecting ChatGPT Imposters with A Single Question\nHong Wang Xuan Luo Weizhi Wang Xifeng Yan\nabstract\rabstract: Large language models like ChatGPT have recently demonstrated impressivecapabilities in natural language understanding and generation, enabling variousapplications including translation, essay writing, and chit-chatting. However,there is a concern that they can be misused for malicious purposes, such asfraud or denial-of-service attacks. Therefore, it is crucial to develop methodsfor detecting whether the party involved in a conversation is a bot or a human.In this paper, we propose a framework named FLAIR, Finding Large language modelAuthenticity via a single Inquiry and Response, to detect conversational botsin an online manner. Specifically, we target a single question scenario thatcan effectively differentiate human users from bots. The questions are dividedinto two categories: those that are easy for humans but difficult for bots(e.g., counting, substitution, positioning, noise filtering, and ASCII art),and those that are easy for bots but difficult for humans (e.g., memorizationand computation). Our approach shows different strengths of these questions intheir effectiveness, providing a new way for online service providers toprotect themselves against nefarious activities and ensure that they areserving real users. We open-sourced our dataset onhttps://github.com/hongwang600/FLAIR and welcome contributions from thecommunity to enrich such detection datasets.\r2023-05-15\nA Reproducible Extraction of Training Images from Diffusion Models\nRyan Webster\nabstract\rabstract: Recently, Carlini et al. demonstrated the widely used model Stable Diffusioncan regurgitate real training samples, which is troublesome from a copyrightperspective. In this work, we provide an efficient extraction attack on parwith the recent attack, with several order of magnitudes less networkevaluations. In the process, we expose a new phenomena, which we dub templateverbatims, wherein a diffusion model will regurgitate a training sample largelyin tact. Template verbatims are harder to detect as they require retrieval andmasking to correctly label. Furthermore, they are still generated by newersystems, even those which de-duplicate their training set, and we give insightinto why they still appear during generation. We extract training images fromseveral state of the art systems, including Stable Diffusion 2.0, Deep ImageFloyd, and finally Midjourney v4. We release code to verify our extractionattack, perform the attack, as well as all extracted prompts at\\url{https://github.com/ryanwebster90/onestep-extraction}.\rSB-VQA: A Stack-Based Video Quality Assessment Framework for Video Enhancement\nDing-Jiun Huang Yu-Ting Kao Tieh-Hung Chuang Ya-Chun Tsai Jing-Kai Lou Shuen-Huei Guan\nabstract\rabstract: In recent years, several video quality assessment (VQA) methods have beendeveloped, achieving high performance. However, these methods were notspecifically trained for enhanced videos, which limits their ability to predictvideo quality accurately based on human subjective perception. To address thisissue, we propose a stack-based framework for VQA that outperforms existingstate-of-the-art methods on VDPVE, a dataset consisting of enhanced videos. Inaddition to proposing the VQA framework for enhanced videos, we alsoinvestigate its application on professionally generated content (PGC). Toaddress copyright issues with premium content, we create the PGCVQ dataset,which consists of videos from YouTube. We evaluate our proposed approach andstate-of-the-art methods on PGCVQ, and provide new insights on the results. Ourexperiments demonstrate that existing VQA algorithms can be applied to PGCvideos, and we find that VQA performance for PGC videos can be improved byconsidering the plot of a play, which highlights the importance of videosemantic understanding.\r2023-05-13\nBeyond the Safeguards: Exploring the Security Risks of ChatGPT\nErik Derner Kristina Batistič\nabstract\rabstract: The increasing popularity of large language models (LLMs) such as ChatGPT hasled to growing concerns about their safety, security risks, and ethicalimplications. This paper aims to provide an overview of the different types ofsecurity risks associated with ChatGPT, including malicious text and codegeneration, private data disclosure, fraudulent services, informationgathering, and producing unethical content. We present an empirical studyexamining the effectiveness of ChatGPT\u0026rsquo;s content filters and explore potentialways to bypass these safeguards, demonstrating the ethical implications andsecurity risks that persist in LLMs even when protections are in place. Basedon a qualitative analysis of the security implications, we discuss potentialstrategies to mitigate these risks and inform researchers, policymakers, andindustry professionals about the complex security challenges posed by LLMs likeChatGPT. This study contributes to the ongoing discussion on the ethical andsecurity implications of LLMs, underscoring the need for continued research inthis area.\r2023-05-12\nPLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English\nJianfeng Chi Wasi Uddin Ahmad Yuan Tian Kai-Wei Chang\nabstract\rabstract: Privacy policies provide individuals with information about their rights andhow their personal information is handled. Natural language understanding (NLU)technologies can support individuals and practitioners to understand betterprivacy practices described in lengthy and complex documents. However, existingefforts that use NLU technologies are limited by processing the language in away exclusive to a single task focusing on certain privacy practices. To thisend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)benchmark, a multi-task benchmark for evaluating the privacy policy languageunderstanding across various tasks. We also collect a large corpus of privacypolicies to enable privacy policy domain-specific language model pre-training.We evaluate several generic pre-trained language models and continuepre-training them on the collected corpus. We demonstrate that domain-specificcontinual pre-training offers performance improvements across all tasks.\r2023-05-11\nReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network\nNatan Semyonov Rami Puzis Asaf Shabtai Gilad Katz\nabstract\rabstract: Watermarking is one of the most important copyright protection tools fordigital media. The most challenging type of watermarking is the imperceptibleone, which embeds identifying information in the data while retaining thelatter\u0026rsquo;s original quality. To fulfill its purpose, watermarks need to withstandvarious distortions whose goal is to damage their integrity. In this study, weinvestigate a novel deep learning-based architecture for embeddingimperceptible watermarks. The key insight guiding our architecture design isthe need to correlate the dimensions of our watermarks with the sizes ofreceptive fields (RF) of modules of our architecture. This adaptation makes ourwatermarks more robust, while also enabling us to generate them in a way thatbetter maintains image quality. Extensive evaluations on a wide variety ofdistortions show that the proposed method is robust against most commondistortions on watermarks including collusive distortion.\rCryptSan: Leveraging ARM Pointer Authentication for Memory Safety in C/C++\nKonrad Hohentanner Philipp Zieris Julian Horsch\nabstract\rabstract: Memory safety bugs remain in the top ranks of security vulnerabilities, evenafter decades of research on their detection and prevention. Variousmitigations have been proposed for C/C++, ranging from language dialects toinstrumentation. Among these, compiler-based instrumentation is particularlypromising, not requiring manual code modifications and being able to achieveprecise memory safety. Unfortunately, existing compiler-based solutionscompromise in many areas, including performance but also usability and memorysafety guarantees. New developments in hardware can help improve performanceand security of compiler-based memory safety. ARM Pointer Authentication, addedin the ARMv8.3 architecture, is intended to enable hardware-assisted ControlFlow Integrity (CFI). But since its operations are generic, it also enablesother, more comprehensive hardware-supported runtime integrity approaches. Assuch, we propose CryptSan, a memory safety approach based on ARM PointerAuthentication. CryptSan uses pointer signatures to retrofit memory safety toC/C++ programs, protecting heap, stack, and globals against temporal andspatial vulnerabilities. We present a full LLVM-based prototype implementation,running on an M1 MacBook Pro, i.e., on actual ARMv8.3 hardware. Our prototypeevaluation shows that the system outperforms similar approaches underreal-world conditions. This, together with its interoperability withuninstrumented libraries and cryptographic protection against attacks onmetadata, makes CryptSan a viable solution for retrofitting memory safety toC/C++ programs.\r2023-05-10\nPrivacy-Preserving Prompt Tuning for Large Language Model Services\nYansong Li Zhixing Tan Yang Liu\nabstract\rabstract: Prompt tuning provides an efficient way for users to customize Large LanguageModels (LLMs) with their private data in the emerging LLM service scenario.However, the sensitive nature of private data brings the need for privacypreservation in LLM service customization. Based on prompt tuning, we proposePrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacyguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,allowing users to privatize their data locally with local differential privacy.As prompt tuning performs poorly when directly trained on privatized data, weintroduce a novel privatized token reconstruction task that is trained jointlywith the downstream task, allowing LLMs to learn better task-dependentrepresentations. Despite the simplicity of our framework, experiments show thatRAPT achieves competitive performance across tasks while providing privacyguarantees against adversaries.\r2023-05-09\nMeasuring Forgetting of Memorized Training Examples\nMatthew Jagielski Om Thakkar Florian Tramèr Daphne Ippolito Katherine Lee Nicholas Carlini Eric Wallace Shuang Song Abhradeep Thakurta Nicolas Papernot Chiyuan Zhang\nabstract\rabstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models \u0026ldquo;forget\u0026rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.\r2023-05-08\nLess is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness\nLiangliang Cao Bowen Zhang Chen Chen Yinfei Yang Xianzhi Du Wencong Zhang Zhiyun Lu Yantao Zheng\nabstract\rabstract: The CLIP (Contrastive Language-Image Pre-training) model and its variants arebecoming the de facto backbone in many applications. However, training a CLIPmodel from hundreds of millions of image-text pairs can be prohibitivelyexpensive. Furthermore, the conventional CLIP model doesn\u0026rsquo;t differentiatebetween the visual semantics and meaning of text regions embedded in images.This can lead to non-robustness when the text in the embedded region doesn\u0026rsquo;tmatch the image\u0026rsquo;s visual appearance. In this paper, we discuss two effectiveapproaches to improve the efficiency and robustness of CLIP training: (1)augmenting the training dataset while maintaining the same number ofoptimization steps, and (2) filtering out samples that contain text regions inthe image. By doing so, we significantly improve the classification andretrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering outimages with text regions also protects the model from typographic attacks. Toverify this, we build a new dataset named ImageNet with Adversarial TextRegions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1accuracy of 68.78%, outperforming previous models whose accuracy was all below50%.\rDifferentially Private Attention Computation\nYeqi Gao Zhao Song Xin Yang\nabstract\rabstract: Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].\rAugmented Datasheets for Speech Datasets and Ethical Decision-Making\nOrestis Papakyriakopoulos Anna Seo Gyeong Choi Jerone Andrews Rebecca Bourke William Thong Dora Zhao Alice Xiang Allison Koenecke\nabstract\rabstract: Speech datasets are crucial for training Speech Language Technologies (SLT);however, the lack of diversity of the underlying training data can lead toserious limitations in building equitable and robust SLT products, especiallyalong dimensions of language, accent, dialect, variety, and speech impairment -and the intersectionality of speech features with socioeconomic and demographicfeatures. Furthermore, there is often a lack of oversight on the underlyingtraining data - commonly built on massive web-crawling and/or publiclyavailable speech - with regard to the ethics of such data collection. Toencourage standardized documentation of such speech data components, weintroduce an augmented datasheet for speech datasets, which can be used inaddition to \u0026ldquo;Datasheets for Datasets\u0026rdquo;. We then exemplify the importance of eachquestion in our augmented datasheet based on in-depth literature reviews ofspeech data used in domains such as machine learning, linguistics, and health.Finally, we encourage practitioners - ranging from dataset creators toresearchers - to use our augmented datasheet to better define the scope,properties, and limits of speech datasets, while also encouraging considerationof data-subject protection and user community empowerment. Ethical datasetcreation is not a one-size-fits-all process, but dataset creators can use ouraugmented datasheet to reflexively consider the social context of related SLTapplications and data sources in order to foster more inclusive SLT productsdownstream.\r2023-05-05\nNot what you\u0026rsquo;ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\nKai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz Mario Fritz\nabstract\rabstract: Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks\u0026rsquo; practical viability againstboth real-world systems, such as Bing\u0026rsquo;s GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication\u0026rsquo;s functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.\rStreamlining personal data access requests: From obstructive procedures to automated web workflows\nNicola Leschke Florian Kirsten Frank Pallas Elias Grünewald\nabstract\rabstract: Transparency and data portability are two core principles of modern privacylegislations such as the GDPR. From the regulatory perspective, providingindividuals (data subjects) with access to their data is a main building blockfor implementing these. Different from other privacy principles and respectiveregulatory provisions, however, this right to data access has so far only seenmarginal technical reflection. Processes related to performing data subjectaccess requests (DSARs) are thus still to be executed manually, hindering theconcept of data access from unfolding its full potential. To tackle this problem, we present an automated approach to the execution ofDSARs, employing modern techniques of web automation. In particular, we proposea generic DSAR workflow model, a corresponding formal language for representingthe particular workflows of different service providers (controllers), apublicly accessible and extendable workflow repository, and a browser-basedexecution engine, altogether providing ``one-click\u0026rsquo;\u0026rsquo; DSARs. To validate ourapproach and technical concepts, we examine, formalize and make publiclyavailable the DSAR workflows of 15 widely used service providers and implementthe execution engine in a publicly available browser extension. Altogether, wethereby pave the way for automated data subject access requests and lay thegroundwork for a broad variety of subsequent technical means helping web usersto better understand their privacy-related exposure to different serviceproviders.\r2023-05-04\nChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI\nBill Tomlinson Andrew W. Torrance Rebecca W. Black\nabstract\rabstract: Recent advances in artificial intelligence (AI) have raised questions aboutwhether the use of AI is appropriate and legal in various professionalcontexts. Here, we present a perspective on how scholars may approach writingin conjunction with AI, and offer approaches to evaluating whether or not suchAI-writing violates copyright or falls within the safe harbor of fair use. Wepresent a set of best practices for standard of care with regard to plagiarism,copyright, and fair use. As AI is likely to grow more capable in the comingyears, it is appropriate to begin integrating AI into scholarly writingactivities. We offer a framework for establishing sound legal and scholarlyfoundations.\rTraining Is Everything: Artificial Intelligence, Copyright, and Fair Training\nAndrew W. Torrance Bill Tomlinson\nabstract\rabstract: To learn how to behave, the current revolutionary generation of AIs must betrained on vast quantities of published images, written works, and sounds, manyof which fall within the core subject matter of copyright law. To some, the useof copyrighted works as training sets for AI is merely a transitory andnon-consumptive use that does not materially interfere with owners\u0026rsquo; content orcopyrights protecting it. Companies that use such content to train their AIengine often believe such usage should be considered \u0026ldquo;fair use\u0026rdquo; under UnitedStates law (sometimes known as \u0026ldquo;fair dealing\u0026rdquo; in other countries). By contrast,many copyright owners, as well as their supporters, consider the incorporationof copyrighted works into training sets for AI to constitute misappropriationof owners\u0026rsquo; intellectual property, and, thus, decidedly not fair use under thelaw. This debate is vital to the future trajectory of AI and its applications. In this article, we analyze the arguments in favor of, and against, viewingthe use of copyrighted works in training sets for AI as fair use. We call thisform of fair use \u0026ldquo;fair training\u0026rdquo;. We identify both strong and spuriousarguments on both sides of this debate. In addition, we attempt to take abroader perspective, weighing the societal costs (e.g., replacement of certainforms of human employment) and benefits (e.g., the possibility of novelAI-based approaches to global issues such as environmental disruption) ofallowing AI to make easy use of copyrighted works as training sets tofacilitate the development, improvement, adoption, and diffusion of AI.Finally, we suggest that the debate over AI and copyrighted works may be atempest in a teapot when placed in the wider context of massive societalchallenges such as poverty, equality, climate change, and loss of biodiversity,to which AI may be part of the solution.\r2023-05-03\nTraining Natural Language Processing Models on Encrypted Text for Enhanced Privacy\nDavut Emre Tasar Ceren Ocal Tasar\nabstract\rabstract: With the increasing use of cloud-based services for training and deployingmachine learning models, data privacy has become a major concern. This isparticularly important for natural language processing (NLP) models, whichoften process sensitive information such as personal communications andconfidential documents. In this study, we propose a method for training NLPmodels on encrypted text data to mitigate data privacy concerns whilemaintaining similar performance to models trained on non-encrypted data. Wedemonstrate our method using two different architectures, namelyDoc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroupsdataset. Our results indicate that both encrypted and non-encrypted modelsachieve comparable performance, suggesting that our encryption method iseffective in preserving data privacy without sacrificing model accuracy. Inorder to replicate our experiments, we have provided a Colab notebook at thefollowing address: https://t.ly/lR-TP\r2023-05-02\nMitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy\nAly M. Kassem\nabstract\rabstract: Large Language models (LLMs) are trained on large amounts of data, which caninclude sensitive information that may compromise personal privacy. LLMs showedto memorize parts of the training data and emit those data verbatim when anadversary prompts appropriately. Previous research has primarily focused ondata preprocessing and differential privacy techniques to address memorizationor prevent verbatim memorization exclusively, which can give a false sense ofprivacy. However, these methods rely on explicit and implicit assumptions aboutthe structure of the data to be protected, which often results in an incompletesolution to the problem. To address this, we propose a novel framework thatutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigateapproximate memorization. Our approach utilizes a negative similarity score,such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilaritypolicy. Our results demonstrate that this framework effectively mitigatesapproximate memorization while maintaining high levels of coherence and fluencyin the generated samples. Furthermore, our framework is robust in mitigatingapproximate memorization across various circumstances, including longercontext, which is known to increase memorization in LLMs.\rDifferentially Private Learning with Per-Sample Adaptive Clipping\nTianyu Xia Shuheng Shen Su Yao Xinyi Fu Ke Xu Xiaolong Xu Xing Fu\nabstract\rabstract: Privacy in AI remains a topic that draws attention from researchers and thegeneral public in recent years. As one way to implement privacy-preserving AI,differentially private learning is a framework that enables AI models to usedifferential privacy (DP). To achieve DP in the learning process, existingalgorithms typically limit the magnitude of gradients with a constant clipping,which requires carefully tuned due to its significant impact on modelperformance. As a solution to this issue, latest works NSGD and Auto-Sinnovatively propose to use normalization instead of clipping to avoidhyperparameter tuning. However, normalization-based approaches like NSGD andAuto-S rely on a monotonic weight function, which imposes excessive weight onsmall gradient samples and introduces extra deviation to the update. In thispaper, we propose a Differentially Private Per-Sample Adaptive Clipping(DP-PSAC) algorithm based on a non-monotonic adaptive weight function, whichguarantees privacy without the typical hyperparameter tuning process of using aconstant clipping while significantly reducing the deviation between the updateand true batch-averaged gradient. We provide a rigorous theoretical convergenceanalysis and show that with convergence rate at the same order, the proposedalgorithm achieves a lower non-vanishing bound, which is maintained overtraining iterations, compared with NSGD/Auto-S. In addition, through extensiveexperimental evaluation, we show that DP-PSAC outperforms or matches thestate-of-the-art methods on multiple main-stream vision and language tasks.\r2023-05-01\nA Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\nCe Zhou Qian Li Chen Li Jun Yu Yixin Liu Guangjing Wang Kai Zhang Cheng Ji Qiben Yan Lifang He Hao Peng Jianxin Li Jia Wu Ziwei Liu Pengtao Xie Caiming Xiong Jian Pei Philip S. Yu Lichao Sun\nabstract\rabstract: Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence.\r2023-04-30\nReliable Gradient-free and Likelihood-free Prompt Tuning\nMaohao Shen Soumya Ghosh Prasanna Sattigeri Subhro Das Yuheng Bu Gregory Wornell\nabstract\rabstract: Due to privacy or commercial constraints, large pre-trained language models(PLMs) are often offered as black-box APIs. Fine-tuning such models todownstream tasks is challenging because one can neither access the model\u0026rsquo;sinternal representations nor propagate gradients through it. This paperaddresses these challenges by developing techniques for adapting PLMs with onlyAPI access. Building on recent work on soft prompt tuning, we develop methodsto tune the soft prompts without requiring gradient computation. Further, wedevelop extensions that in addition to not requiring gradients also do not needto access any internal representation of the PLM beyond the input embeddings.Moreover, instead of learning a single prompt, our methods learn a distributionover prompts allowing us to quantify predictive uncertainty. Ours is the firstwork to consider uncertainty in prompts when only having API access to the PLM.Finally, through extensive experiments, we carefully vet the proposed methodsand find them competitive with (and sometimes even improving on) gradient-basedapproaches with full access to the PLM.\r2023-04-27\nLongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation\nPetra Galuščáková Romain Deveaud Gabriela Gonzalez-Saez Philippe Mulhem Lorraine Goeuriot Florina Piroi Martin Popel\nabstract\rabstract: LongEval-Retrieval is a Web document retrieval benchmark that focuses oncontinuous retrieval evaluation. This test collection is intended to be used tostudy the temporal persistence of Information Retrieval systems and will beused as the test collection in the Longitudinal Evaluation of Model PerformanceTrack (LongEval) at CLEF 2023. This benchmark simulates an evolving informationsystem environment - such as the one a Web search engine operates in - wherethe document collection, the query distribution, and relevance all movecontinuously, while following the Cranfield paradigm for offline evaluation. Todo that, we introduce the concept of a dynamic test collection that is composedof successive sub-collections each representing the state of an informationsystem at a given time step. In LongEval-Retrieval, each sub-collectioncontains a set of queries, documents, and soft relevance assessments built fromclick models. The data comes from Qwant, a privacy-preserving Web search enginethat primarily focuses on the French market. LongEval-Retrieval also provides a\u0026rsquo;mirror\u0026rsquo; collection: it is initially constructed in the French language tobenefit from the majority of Qwant\u0026rsquo;s traffic, before being translated toEnglish. This paper presents the creation process of LongEval-Retrieval andprovides baseline runs and analysis.\r2023-04-25\nTABLET: Learning From Instructions For Tabular Data\nDylan Slack Sameer Singh\nabstract\rabstract: Acquiring high-quality data is often a significant challenge in trainingmachine learning (ML) models for tabular prediction, particularly inprivacy-sensitive and costly domains like medicine and finance. Providingnatural language instructions to large language models (LLMs) offers analternative solution. However, it is unclear how effectively instructionsleverage the knowledge in LLMs for solving tabular prediction problems. Toaddress this gap, we introduce TABLET, a benchmark of 20 diverse tabulardatasets annotated with instructions that vary in their phrasing, granularity,and technicality. Additionally, TABLET includes the instructions\u0026rsquo; logic andstructured modifications to the instructions. We find in-context instructionsincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% forChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabularprediction in our benchmark by evaluating instruction faithfulness. We findLLMs often ignore instructions and fail to predict specific instancescorrectly, even with examples. Our analysis on TABLET shows that, whileinstructions help LLM performance, learning from instructions for tabular datarequires new capabilities.\r2023-04-24\nPARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis\nShu Wei Nuo Xu\nabstract\rabstract: Document layout analysis has a wide range of requirements across variousdomains, languages, and business scenarios. However, most currentstate-of-the-art algorithms are language-dependent, with architectures thatrely on transformer encoders or language-specific text encoders, such as BERT,for feature extraction. These approaches are limited in their ability to handlevery long documents due to input sequence length constraints and are closelytied to language-specific tokenizers. Additionally, training a cross-languagetext encoder can be challenging due to the lack of labeled multilingualdocument datasets that consider privacy. Furthermore, some layout tasks requirea clean separation between different layout components without overlap, whichcan be difficult for image segmentation-based algorithms to achieve. In thispaper, we present Paragraph2Graph, a language-independent graph neural network(GNN)-based model that achieves competitive results on common document layoutdatasets while being adaptable to business scenarios with strict separation.With only 19.95 million parameters, our model is suitable for industrialapplications, particularly in multi-language scenarios.\r2023-04-23\nAnalyzing Leakage of Personally Identifiable Information in Language Models\nNils Lukas Ahmed Salem Robert Sim Shruti Tople Lukas Wutschitz Santiago Zanella-Béguelin\nabstract\rabstract: Language Models (LMs) have been shown to leak information about training datathrough sentence-level membership inference and reconstruction attacks.Understanding the risk of LMs leaking Personally Identifiable Information (PII)has received less attention, which can be attributed to the false assumptionthat dataset curation techniques such as scrubbing are sufficient to preventPII leakage. Scrubbing techniques reduce but do not prevent the risk of PIIleakage: in practice scrubbing is imperfect and must balance the trade-offbetween minimizing disclosure and preserving the utility of the dataset. On theother hand, it is unclear to which extent algorithmic defenses such asdifferential privacy, designed to guarantee sentence- or user-level privacy,prevent PII disclosure. In this work, we introduce rigorous game-baseddefinitions for three types of PII leakage via black-box extraction, inference,and reconstruction attacks with only API access to an LM. We empiricallyevaluate the attacks against GPT-2 models fine-tuned with and without defensesin three domains: case law, health care, and e-mails. Our main contributionsare (i) novel attacks that can extract up to 10$\\times$ more PII sequences thanexisting attacks, (ii) showing that sentence-level differential privacy reducesthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)a subtle connection between record-level membership inference and PIIreconstruction. Code to reproduce all experiments in the paper is available athttps://github.com/microsoft/analysing_pii_leakage.\r2023-04-22\nRetrieval Enhanced Data Augmentation for Question Answering on Privacy Policies\nMd Rizwan Parvez Jianfeng Chi Wasi Uddin Ahmad Yuan Tian Kai-Wei Chang\nabstract\rabstract: Prior studies in privacy policies frame the question answering (QA) task asidentifying the most relevant text segment or a list of sentences from a policydocument given a user query. Existing labeled datasets are heavily imbalanced(only a few relevant segments), limiting the QA performance in this domain. Inthis paper, we develop a data augmentation framework based on ensemblingretriever models that captures the relevant text segments from unlabeled policydocuments and expand the positive examples in the training set. In addition, toimprove the diversity and quality of the augmented data, we leverage multiplepre-trained language models (LMs) and cascade them with noise reduction filtermodels. Using our augmented data on the PrivacyQA benchmark, we elevate theexisting baseline by a large margin (10% F1) and achieve a newstate-of-the-art F1 score of 50%. Our ablation studies provide furtherinsights into the effectiveness of our approach.\r2023-04-21\nA Group-Specific Approach to NLP for Hate Speech Detection\nKarina Halevy\nabstract\rabstract: Automatic hate speech detection is an important yet complex task, requiringknowledge of common sense, stereotypes of protected groups, and histories ofdiscrimination, each of which may constantly evolve. In this paper, we proposea group-specific approach to NLP for online hate speech detection. The approachconsists of creating and infusing historical and linguistic knowledge about aparticular protected group into hate speech detection models, analyzinghistorical data about discrimination against a protected group to betterpredict spikes in hate speech against that group, and critically evaluatinghate speech detection models through lenses of intersectionality and ethics. Wedemonstrate this approach through a case study on NLP for detection ofantisemitic hate speech. The case study synthesizes the currentEnglish-language literature on NLP for antisemitism detection, introduces anovel knowledge graph of antisemitic history and language from the 20th centuryto the present, infuses information from the knowledge graph into a set oftweets over Logistic Regression and uncased DistilBERT baselines, and suggeststhat incorporating context from the knowledge graph can help models pick upsubtle stereotypes.\r2023-04-20\nOn the Independence of Association Bias and Empirical Fairness in Language Models\nLaura Cabello Anna Katrine Jørgensen Anders Søgaard\nabstract\rabstract: The societal impact of pre-trained language models has prompted researchersto probe them for strong associations between protected attributes andvalue-loaded terms, from slur to prestigious job titles. Such work is said toprobe models for bias or fairness-or such probes \u0026lsquo;into representational biases\u0026rsquo;are said to be \u0026lsquo;motivated by fairness\u0026rsquo;-suggesting an intimate connectionbetween bias and fairness. We provide conceptual clarity by distinguishingbetween association biases (Caliskan et al., 2022) and empirical fairness (Shenet al., 2022) and show the two can be independent. Our main contribution,however, is showing why this should not come as a surprise. To this end, wefirst provide a thought experiment, showing how association bias and empiricalfairness can be completely orthogonal. Next, we provide empirical evidence thatthere is no correlation between bias metrics and fairness metrics across themost widely used language models. Finally, we survey the sociological andpsychological literature and show how this literature provides ample supportfor expecting these metrics to be uncorrelated.\r2023-04-19\nCatch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers\nAishwarya Deep Shukla Laksh Agarwal Jie Mein Goh Guodong Gao Ritu Agarwal\nabstract\rabstract: The proliferation of fake reviews of doctors has potentially detrimentalconsequences for patient well-being and has prompted concern among consumerprotection groups and regulatory bodies. Yet despite significant advancementsin the fields of machine learning and natural language processing, thereremains limited comprehension of the characteristics differentiating fraudulentfrom authentic reviews. This study utilizes a novel pre-labeled dataset of38048 physician reviews to establish the effectiveness of large language modelsin classifying reviews. Specifically, we compare the performance of traditionalML models, such as logistic regression and support vector machines, togenerative pre-trained transformer models. Furthermore, we use GPT4, the newestmodel in the GPT family, to uncover the key dimensions along which fake andgenuine physician reviews differ. Our findings reveal significantly superiorperformance of GPT-3 over traditional ML models in this context. Additionally,our analysis suggests that GPT3 requires a smaller training sample thantraditional models, suggesting its appropriateness for tasks with scarcetraining data. Moreover, the superiority of GPT3 performance increases in thecold start context i.e., when there are no prior reviews of a doctor. Finally,we employ GPT4 to reveal the crucial dimensions that distinguish fake physicianreviews. In sharp contrast to previous findings in the literature that wereobtained using simulated data, our findings from a real-world dataset show thatfake reviews are generally more clinically detailed, more reserved insentiment, and have better structure and grammar than authentic ones.\rMaybenot: A Framework for Traffic Analysis Defenses\nTobias Pulls\nabstract\rabstract: End-to-end encryption is a powerful tool for protecting the privacy ofInternet users. Together with the increasing use of technologies such as Tor,VPNs, and encrypted messaging, it is becoming increasingly difficult fornetwork adversaries to monitor and censor Internet traffic. One remainingavenue for adversaries is traffic analysis: the analysis of patterns inencrypted traffic to infer information about the users and their activities.Recent improvements using deep learning have made traffic analysis attacks moreeffective than ever before. We present Maybenot, a framework for traffic analysis defenses. Maybenot isdesigned to be easy to use and integrate into existing end-to-end encryptedprotocols. It is implemented in the Rust programming language as a crate(library), together with a simulator to further the development of defenses.Defenses in Maybenot are expressed as probabilistic state machines thatschedule actions to inject padding or block outgoing traffic. Maybenot is anevolution from the Tor Circuit Padding Framework by Perry and Kadianakis,designed to support a wide range of protocols and use cases.\rRevitalizing Endangered Languages: AI-powered language learning as a catalyst for language appreciation\nDinesh Kumar Nanduri Elizabeth M. Bonsignore\nabstract\rabstract: According to UNESCO, there are nearly 7,000 languages spoken worldwide, ofwhich around 3,000 languages are in danger of disappearing before the end ofthe century. With roughly 230 languages having already become extinct betweenthe years 1950-2010, collectively this represents a significant loss oflinguistic and cultural diversity. This position paper aims to explore thepotential of AI-based language learning approaches that promote early exposureand appreciation of languages to ultimately contribute to the preservation ofendangered languages, thereby addressing the urgent need to protect linguisticand cultural diversity.\r2023-04-18\nA Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese\nHugo Sousa Arian Pasquali Alípio Jorge Catarina Sousa Santos Mário Amorim Lopes\nabstract\rabstract: Textual health records of cancer patients are usually protracted and highlyunstructured, making it very time-consuming for health professionals to get acomplete overview of the patient\u0026rsquo;s therapeutic course. As such limitations canlead to suboptimal and/or inefficient treatment procedures, healthcareproviders would greatly benefit from a system that effectively summarizes theinformation of those records. With the advent of deep neural models, thisobjective has been partially attained for English clinical texts, however, theresearch community still lacks an effective solution for languages with limitedresources. In this paper, we present the approach we developed to extractprocedures, drugs, and diseases from oncology health records written inEuropean Portuguese. This project was conducted in collaboration with thePortuguese Institute for Oncology which, besides holding over $10$ years ofduly protected medical records, also provided oncologist expertise throughoutthe development of the project. Since there is no annotated corpus forbiomedical entity extraction in Portuguese, we also present the strategy wefollowed in annotating the corpus for the development of the models. The finalmodels, which combined a neural architecture with entity linking, achieved$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extractionof procedures, drugs, and diseases, respectively.\rOrder-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models\nJiawei Liu Yangyang Kang Di Tang Kaisong Song Changlong Sun Xiaofeng Wang Wei Lu Xiaozhong Liu\nabstract\rabstract: Neural text ranking models have witnessed significant advancement and areincreasingly being deployed in practice. Unfortunately, they also inheritadversarial vulnerabilities of general neural models, which have been detectedbut remain underexplored by prior studies. Moreover, the inherit adversarialvulnerabilities might be leveraged by blackhat SEO to defeat better-protectedsearch engines. In this study, we propose an imitation adversarial attack onblack-box neural passage ranking models. We first show that the target passageranking model can be transparentized and imitated by enumerating criticalqueries/candidates and then train a ranking imitation model. Leveraging theranking imitation model, we can elaborately manipulate the ranking results andtransfer the manipulation attack to the target ranking model. For this purpose,we propose an innovative gradient-based attack method, empowered by thepairwise objective function, to generate adversarial triggers, which causespremeditated disorderliness with very few tokens. To equip the triggercamouflages, we add the next sentence prediction loss and the language modelfluency constraint to the objective function. Experimental results on passageranking demonstrate the effectiveness of the ranking imitation attack model andadversarial triggers against various SOTA neural ranking models. Furthermore,various mitigation analyses and human evaluation show the effectiveness ofcamouflages when facing potential mitigation approaches. To motivate otherscholars to further investigate this novel and important problem, we make theexperiment data and code publicly available.\rFedTP: Federated Learning by Transformer Personalization\nHongxia Li Zhongyi Cai Jingya Wang Jiangnan Tang Weiping Ding Chin-Teng Lin Ye Shi\nabstract\rabstract: Federated learning is an emerging learning paradigm where multiple clientscollaboratively train a machine learning model in a privacy-preserving manner.Personalized federated learning extends this paradigm to overcome heterogeneityacross clients by learning personalized models. Recently, there have been someinitial attempts to apply Transformers to federated learning. However, theimpacts of federated learning algorithms on self-attention have not yet beenstudied. This paper investigates this relationship and reveals that federatedaveraging algorithms actually have a negative impact on self-attention wherethere is data heterogeneity. These impacts limit the capabilities of theTransformer model in federated learning settings. Based on this, we proposeFedTP, a novel Transformer-based federated learning framework that learnspersonalized self-attention for each client while aggregating the otherparameters among the clients. Instead of using a vanilla personalizationmechanism that maintains personalized self-attention layers of each clientlocally, we develop a learn-to-personalize mechanism to further encourage thecooperation among clients and to increase the scablability and generalizationof FedTP. Specifically, the learn-to-personalize is realized by learning ahypernetwork on the server that outputs the personalized projection matrices ofself-attention layers to generate client-wise queries, keys and values.Furthermore, we present the generalization bound for FedTP with thelearn-to-personalize mechanism. Notably, FedTP offers a convenient environmentfor performing a range of image and language tasks using the same federatednetwork architecture - all of which benefit from Transformer personalization.Extensive experiments verify that FedTP with the learn-to-personalize mechanismyields state-of-the-art performance in non-IID scenarios. Our code is availableonline.\r2023-04-17\nTraining Automated Defense Strategies Using Graph-based Cyber Attack Simulations\nJakob Nyberg Pontus Johnson\nabstract\rabstract: We implemented and evaluated an automated cyber defense agent. The agenttakes security alerts as input and uses reinforcement learning to learn apolicy for executing predefined defensive measures. The defender policies weretrained in an environment intended to simulate a cyber attack. In thesimulation, an attacking agent attempts to capture targets in the environment,while the defender attempts to protect them by enabling defenses. Theenvironment was modeled using attack graphs based on the Meta Attack Languagelanguage. We assumed that defensive measures have downtime costs, meaning thatthe defender agent was penalized for using them. We also assumed that theenvironment was equipped with an imperfect intrusion detection system thatoccasionally produces erroneous alerts based on the environment state. Toevaluate the setup, we trained the defensive agent with different volumes ofintrusion detection system noise. We also trained agents with differentattacker strategies and graph sizes. In experiments, the defensive agent usingpolicies trained with reinforcement learning outperformed agents usingheuristic policies. Experiments also demonstrated that the policies couldgeneralize across different attacker strategies. However, the performance ofthe learned policies decreased as the attack graphs increased in size.\r2023-04-15\nDoes Prompt-Tuning Language Model Ensure Privacy?\nShangyu Xie Wei Dai Esha Ghosh Sambuddha Roy Dan Schwartz Kim Laine\nabstract\rabstract: Prompt-tuning has received attention as an efficient tuning method in thelanguage domain, i.e., tuning a prompt that is a few tokens long, while keepingthe large language model frozen, yet achieving comparable performance withconventional fine-tuning. Considering the emerging privacy concerns withlanguage models, we initiate the study of privacy leakage in the setting ofprompt-tuning. We first describe a real-world email service pipeline to providecustomized output for various users via prompt-tuning. Then we propose a novelprivacy attack framework to infer users\u0026rsquo; private information by exploiting theprompt module with user-specific signals. We conduct a comprehensive privacyevaluation on the target pipeline to demonstrate the potential leakage fromprompt-tuning. The results also demonstrate the effectiveness of the proposedattack.\rUnderstanding Developers Privacy Concerns Through Reddit Thread Analysis\nJonathan Parsons Michael Schrider Oyebanjo Ogunlela Sepideh Ghanavati\nabstract\rabstract: With the growing global emphasis on regulating the protection of personalinformation and increasing user expectation of the same, developing withprivacy in mind is becoming ever more important. In this paper, we study theconcerns, questions, and solutions developers discuss on Reddit forums toenhance our understanding of their perceptions and challenges while developingapplications in the current privacy-focused world. We perform various forms ofNatural Language Processing (NLP) on 437,317 threads from subreddits such asr/webdev, r/androiddev, and r/iOSProgramming to identify both common points ofdiscussion and how these points change over time as new regulations are passedaround the globe. Our results show that there are common trends in privacytopics among the different subreddits while the frequency of those topicsdiffers between web and mobile applications.\r2023-04-14\nClassification of social media Toxic comments using Machine learning models\nK. Poojitha A. Sai Charish M. Arun Kuamr Reddy S. Ayyasamy\nabstract\rabstract: The abstract outlines the problem of toxic comments on social mediaplatforms, where individuals use disrespectful, abusive, and unreasonablelanguage that can drive users away from discussions. This behavior is referredto as anti-social behavior, which occurs during online debates, comments, andfights. The comments containing explicit language can be classified intovarious categories, such as toxic, severe toxic, obscene, threat, insult, andidentity hate. This behavior leads to online harassment and cyberbullying,which forces individuals to stop expressing their opinions and ideas. Toprotect users from offensive language, companies have started flagging commentsand blocking users. The abstract proposes to create a classifier using anLstm-cnn model that can differentiate between toxic and non-toxic comments withhigh accuracy. The classifier can help organizations examine the toxicity ofthe comment section better.\r2023-04-13\nChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review\nSunder Ali Khowaja Parus Khuwaja Kapal Dev\nabstract\rabstract: ChatGPT is another large language model (LLM) inline but due to itsperformance and ability to converse effectively, it has gained a hugepopularity amongst research as well as industrial community. Recently, manystudies have been published to show the effectiveness, efficiency, integration,and sentiments of chatGPT and other LLMs. In contrast, this study focuses onthe important aspects that are mostly overlooked, i.e. sustainability, privacy,digital divide, and ethics and suggests that not only chatGPT but everysubsequent entry in the category of conversational bots should undergoSustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. Thispaper discusses in detail about the issues and concerns raised over chatGPT inline with aforementioned characteristics. We support our hypothesis by somepreliminary data collection and visualizations along with hypothesized facts.We also suggest mitigations and recommendations for each of the concerns.Furthermore, we also suggest some policies and recommendations for AI policyact, if designed by the governments.\r2023-04-12\nUnleashing ChatGPT on the Metaverse: Savior or Destroyer?\nPengyuan Zhou\nabstract\rabstract: The incorporation of artificial intelligence (AI) technology, and inparticular natural language processing (NLP), is becoming increasingly vitalfor the development of immersive and interactive metaverse experiences. Onesuch artificial intelligence tool that is gaining traction in the metaverse isChatGPT, a large language model trained by OpenAI. The article delves into thepros and cons of utilizing ChatGPT for metaverse-based education,entertainment, personalization, and support. Dynamic and personalizedexperiences are possible with this technology, but there are also legitimateprivacy, bias, and ethical issues to consider. This article aims to helpreaders understand the possible influence of ChatGPT on the metaverse and howit may be used to effectively create a more immersive and engaging virtualenvironment by evaluating these opportunities and obstacles.\rFALQU: Finding Answers to Legal Questions\nBehrooz Mansouri Ricardo Campos\nabstract\rabstract: This paper presents a new test collection for Legal IR, FALQU: FindingAnswers to Legal Questions, where questions and answers were obtained from LawStack Exchange (LawSE), a Q\u0026amp;A website for legal professionals, and others withexperience in law. Much in line with Stack overflow, Law Stack Exchange has avariety of questions on different topics such as copyright, intellectualproperty, and criminal laws, making it an interesting source for datasetconstruction. Questions are also not limited to one country. Often, users ofdifferent nationalities may ask questions about laws in different countries andexpertise. Therefore, questions in FALQU represent real-world users\u0026rsquo;information needs thus helping to avoid lab-generated questions. Answers on theother side are given by experts in the field. FALQU is the first testcollection, to the best of our knowledge, to use LawSE, considering morediverse questions than the questions from the standard legal bar and judicialexams. It contains 9880 questions and 34,145 answers to legal questions.Alongside our new test collection, we provide different baseline systems thatinclude traditional information retrieval models such as TF-IDF and BM25, anddeep neural network search models. The results obtained from the BM25 modelachieved the highest effectiveness.\r2023-04-10\nDeterministic constant-depth preparation of the AKLT state on a quantum processor using fusion measurements\nKevin C. Smith Eleanor Crane Nathan Wiebe S. M. Girvin\nabstract\rabstract: The ground state of the spin-1 Affleck, Kennedy, Lieb and Tasaki (AKLT) modelis a paradigmatic example of both a matrix product state and asymmetry-protected topological phase, and additionally holds promise as aresource state for measurement-based quantum computation. Having a nonzerocorrelation length, the AKLT state cannot be exactly prepared by aconstant-depth unitary circuit composed of local gates. In this work, wedemonstrate that this no-go limit can be evaded by augmenting a constant-depthcircuit with fusion measurements, such that the total preparation time isindependent of system size and entirely deterministic. We elucidate ourpreparation scheme using the language of tensor networks, and furthermore showthat the $\\mathbb{Z}_2\\times\\mathbb{Z}_2$ symmetry of the AKLT state directlyaffords this speed-up over previously known preparation methods. To demonstratethe practical advantage of measurement-assisted preparation on noisyintermediate-scale quantum (NISQ) devices, we carry out our protocol on an IBMQuantum processor. We measure both the string order and entanglement spectrumof prepared AKLT chains and, employing these as metrics, find improved resultsover the known (purely unitary) sequential preparation approach. We concludewith a demonstration of quantum teleportation using the AKLT state prepared byour measurement-assisted scheme. This work thus serves to provide an efficientstrategy to prepare a specific resource in the form of the AKLT state and, morebroadly, experimentally demonstrates the possibility for realizable improvementin state preparation afforded by measurement-based circuit depth reductionstrategies on NISQ-era devices.\rDoes Synthetic Data Generation of LLMs Help Clinical Text Mining?\nRuixiang Tang Xiaotian Han Xiaoqian Jiang Xia Hu\nabstract\rabstract: Recent advancements in large language models (LLMs) have led to thedevelopment of highly potent models like OpenAI\u0026rsquo;s ChatGPT. These models haveexhibited exceptional performance in a variety of tasks, such as questionanswering, essay composition, and code generation. However, their effectivenessin the healthcare sector remains uncertain. In this study, we seek toinvestigate the potential of ChatGPT to aid in clinical text mining byexamining its ability to extract structured information from unstructuredhealthcare texts, with a focus on biological named entity recognition andrelation extraction. However, our preliminary results indicate that employingChatGPT directly for these tasks resulted in poor performance and raisedprivacy concerns associated with uploading patients\u0026rsquo; information to the ChatGPTAPI. To overcome these limitations, we propose a new training paradigm thatinvolves generating a vast quantity of high-quality synthetic data with labelsutilizing ChatGPT and fine-tuning a local model for the downstream task. Ourmethod has resulted in significant improvements in the performance ofdownstream tasks, improving the F1-score from 23.37% to 63.99% for the namedentity recognition task and from 75.86% to 83.59% for the relation extractiontask. Furthermore, generating data using ChatGPT can significantly reduce thetime and effort required for data collection and labeling, as well as mitigatedata privacy concerns. In summary, the proposed framework presents a promisingsolution to enhance the applicability of LLM models to clinical text mining.\rAssessing the Socio-economic Impacts of Secure Texting and Anti-Jamming Technologies in Non-Cooperative Networks\nOsoro B Ogutu Edward J Oughton Kai Zeng Brian L. Mark\nabstract\rabstract: Operating securely over 5G (and legacy) infrastructure is a challenge. Innon-cooperative networks, malicious actors may try to decipher, block encryptedmessages, or specifically jam wireless radio systems. Such activities candisrupt operations, from causing minor inconvenience, through to fullyparalyzing the functionality of critical infrastructure. While technologicalmitigation measures do exist, there are very few methods capable of assessingthe socio-economic impacts from different mitigation strategies. This leads toa lack of robust evidence to inform cost-benefit analysis, and thus supportdecision makers in industry and government. Consequently, this paper presentstwo open-source simulation models for assessing the socio-economic impacts ofoperating in untrusted non-cooperative networks. The first focuses on usingmultiple non-cooperative networks to transmit a message. The second modelsimulates a case where a message is converted into alternative plain languageto avoid detection, separated into different portions and then transmitted overmultiple non-cooperative networks. A probabilistic simulation of the two modelsis performed for a 15 km by 15 km spatial grid with 5 untrusted non-cooperativenetworks and intercepting agents. The results are used to estimate economiclosses for private, commercial, government and military sectors. The highestprobabilistic total losses for military applications include US$300, US$150,and US$75, incurred for a 1, 3 and 5 site multi-transmission approach,respectively, for non-cooperative networks when considering 1,000 texts beingsent. These results form a framework for deterministic socio-economic impactanalysis of using non-cooperative networks and secure texting as protectionagainst radio network attacks. The simulation data and the open-source codebaseis provided for reproducibility.\r2023-04-07\nComplex QA and language models hybrid architectures, Survey\nXavier Daull Patrice Bellot Emmanuel Bruno Vincent Martin Elisabeth Murisasco\nabstract\rabstract: This paper reviews the state-of-the-art of language models architectures andstrategies for \u0026ldquo;complex\u0026rdquo; question-answering (QA, CQA, CPS) with a focus onhybridization. Large Language Models (LLM) are good at leveraging public dataon standard problems but once you want to tackle more specific complexquestions or problems (e.g. How does the concept of personal freedom varybetween different cultures ? What is the best mix of power generation methodsto reduce climate change ?) you may need specific architecture, knowledge,skills, methods, sensitive data protection, explainability, human approval andversatile feedback\u0026hellip; Recent projects like ChatGPT and GALACTICA have allowednon-specialists to grasp the great potential as well as the equally stronglimitations of LLM in complex QA. In this paper, we start by reviewing requiredskills and evaluation techniques. We integrate findings from the robustcommunity edited research papers BIG, BLOOM and HELM which open source,benchmark and analyze limits and challenges of LLM in terms of tasks complexityand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, \u0026hellip;) asa baseline. We discuss some challenges associated with complex QA, includingdomain adaptation, decomposition and efficient multi-step QA, long form andnon-factoid QA, safety and multi-sensitivity data protection, multimodalsearch, hallucinations, explainability and truthfulness, temporal reasoning. Weanalyze current solutions and promising research trends, using elements suchas: hybrid LLM architectural patterns, training and prompting strategies,active human reinforcement learning supervised with AI, neuro-symbolic andstructured knowledge grounding, program synthesis, iterated decomposition andothers.\r2023-04-06\nWhose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics\nMadiha Zahrah Choksi David Goedicke\nabstract\rabstract: Intelligent or generative writing tools rely on large language models thatrecognize, summarize, translate, and predict content. This position paperprobes the copyright interests of open data sets used to train large languagemodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumventthe copyright interests of the used data? We start by defining softwarecopyright and tracing its history. We rely on GitHub Copilot as a modern casestudy challenging software copyright. Our conclusion outlines obstacles thatgenerative writing assistants create for copyright, and offers a practical roadmap for copyright analysis for developers, software law experts, and generalusers to consider in the context of intelligent LLM-powered writing tools.\r2023-04-05\nThe Saudi Privacy Policy Dataset\nHend Al-Khalifa Malak Mashaabi Ghadi Al-Yahya Raghad Alnashwan\nabstract\rabstract: This paper introduces the Saudi Privacy Policy Dataset, a diverse compilationof Arabic privacy policies from various sectors in Saudi Arabia, annotatedaccording to the 10 principles of the Personal Data Protection Law (PDPL); thePDPL was established to be compatible with General Data Protection Regulation(GDPR); one of the most comprehensive data regulations worldwide. Data werecollected from multiple sources, including the Saudi Central Bank, the SaudiArabia National United Platform, the Council of Health Insurance, and generalwebsites using Google and Wikipedia. The final dataset includes 1,000 websitesbelonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus sizeof 8,353 KB. The annotated dataset offers significant reuse potential forassessing privacy policy compliance, benchmarking privacy practices acrossindustries, and developing automated tools for monitoring adherence to dataprotection regulations. By providing a comprehensive and annotated dataset ofprivacy policies, this paper aims to facilitate further research anddevelopment in the areas of privacy policy analysis, natural languageprocessing, and machine learning applications related to privacy and dataprotection, while also serving as an essential resource for researchers,policymakers, and industry professionals interested in understanding andpromoting compliance with privacy regulations in Saudi Arabia.\rBengali Fake Review Detection using Semi-supervised Generative Adversarial Networks\nMd. Tanvir Rouf Shawon G. M. Shahariar Faisal Muhammad Shah Mohammad Shafiul Alam Md. Shahriar Mahbub\nabstract\rabstract: This paper investigates the potential of semi-supervised GenerativeAdversarial Networks (GANs) to fine-tune pretrained language models in order toclassify Bengali fake reviews from real reviews with a few annotated data. Withthe rise of social media and e-commerce, the ability to detect fake ordeceptive reviews is becoming increasingly important in order to protectconsumers from being misled by false information. Any machine learning modelwill have trouble identifying a fake review, especially for a low resourcelanguage like Bengali. We have demonstrated that the proposed semi-supervisedGAN-LM architecture (generative adversarial network on top of a pretrainedlanguage model) is a viable solution in classifying Bengali fake reviews as theexperimental results suggest that even with only 1024 annotated samples,BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% anda f1-score of 84.89% outperforming other pretrained language models -BanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and10% respectively in terms of accuracy. The experiments were conducted on amanually labeled food review dataset consisting of total 6014 real and fakereviews collected from various social media groups. Researchers that areexperiencing difficulty recognizing not just fake reviews but otherclassification issues owing to a lack of labeled data may find a solution inour proposed methodology.\r2023-04-03\nROPfuscator: Robust Obfuscation with ROP\nGiulio De Pasquale Fukutomo Nakanishi Daniele Ferla Lorenzo Cavallaro\nabstract\rabstract: Software obfuscation plays a crucial role in protecting intellectual propertyin software from reverse engineering attempts. While some obfuscationtechniques originate from the obfuscation-reverse engineering arms race, othersstem from different research areas, such as binary software exploitation.Return-oriented programming (ROP) gained popularity as one of the mosteffective exploitation techniques for memory error vulnerabilities. ROPinterferes with our natural perception of a process control flow, inspiring usto repurpose ROP as a robust and effective form of software obfuscation.Although previous work already explores ROP\u0026rsquo;s effectiveness as an obfuscationtechnique, evolving reverse engineering research raises the need for principledreasoning to understand the strengths and limitations of ROP-based mechanismsagainst man-at-the-end (MATE) attacks. To this end, we present ROPfuscator, acompiler-driven obfuscation pass based on ROP for any programming languagesupported by LLVM. We incorporate opaque predicates and constants and a novelinstruction hiding technique to withstand sophisticated MATE attacks. Moreimportantly, we introduce a realistic and unified threat model to thoroughlyevaluate ROPfuscator and provide principled reasoning on ROP-based obfuscationtechniques that answer to code coverage, incurred overhead, correctness,robustness, and practicality challenges.\r2023-04-01\nWhen Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus\nWon Ik Cho Yoon Kyung Lee Seoyeon Bae Jihwan Kim Sangah Park Moosung Kim Sowon Hahn Nam Soo Kim\nabstract\rabstract: Building a natural language dataset requires caution since word semantics isvulnerable to subtle text change or the definition of the annotated concept.Such a tendency can be seen in generative tasks like question-answering anddialogue generation and also in tasks that create a categorization-basedcorpus, like topic classification or sentiment analysis. Open-domainconversations involve two or more crowdworkers freely conversing about anytopic, and collecting such data is particularly difficult for two reasons: 1)the dataset should be crafted\u0026quot; rather than obtained\u0026quot; due to privacyconcerns, and 2) paid creation of such dialogues may differ from howcrowdworkers behave in real-world settings. In this study, we tackle theseissues when creating a large-scale open-domain persona dialogue corpus, wherepersona implies that the conversation is performed by several actors with afixed persona and user-side workers from an unspecified crowd.\r2023-03-30\nA CI-based Auditing Framework for Data Collection Practices\nAthina Markopoulou Rahmadi Trimananda Hao Cui\nabstract\rabstract: Apps and devices (mobile devices, web browsers, IoT, VR, voice assistants,etc.) routinely collect user data, and send them to first- and third-partyservers through the network. Recently, there is a lot of interest in (1)auditing the actual data collection practices of those systems; and also in (2)checking the consistency of those practices against the statements made in thecorresponding privacy policies. In this paper, we argue that the contextualintegrity (CI) tuple can be the basic building block for defining andimplementing such an auditing framework. We elaborate on the special case wherethe tuple is partially extracted from the network traffic generated by theend-device of interest, and partially from the corresponding privacy policiesusing natural language processing (NLP) techniques. Along the way, we discussrelated bodies of work and representative examples that fit into thatframework. More generally, we believe that CI can be the building block notonly for auditing at the edge, but also for specifying privacy policies andsystem APIs. We also discuss limitations and directions for future work.\rConStruct-VL: Data-Free Continual Structured VL Concepts Learning\nJames Seale Smith Paola Cascante-Bonilla Assaf Arbelle Donghyun Kim Rameswar Panda David Cox Diyi Yang Zsolt Kira Rogerio Feris Leonid Karlinsky\nabstract\rabstract: Recently, large-scale pre-trained Vision-and-Language (VL) foundation modelshave demonstrated remarkable capabilities in many zero-shot downstream tasks,achieving competitive results for recognizing objects defined by as little asshort text prompts. However, it has also been shown that VL models are stillbrittle in Structured VL Concept (SVLC) reasoning, such as the ability torecognize object attributes, states, and inter-object relations. This leads toreasoning mistakes, which need to be corrected as they occur by teaching VLmodels the missing SVLC skills; often this must be done using private datawhere the issue was found, which naturally leads to a data-free continual (notask-id) VL learning setting. In this work, we introduce the first ContinualData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show itis challenging for many existing data-free CL strategies. We, therefore,propose a data-free method comprised of a new approach of AdversarialPseudo-Replay (APR) which generates adversarial reminders of past tasks frompast task models. To use this method efficiently, we also propose a continualparameter-efficient Layered-LoRA (LaLo) neural architecture allowingno-memory-cost access to all past models at train time. We show this approachoutperforms all data-free methods by as much as ~7% while even matching somelevels of experience-replay (prohibitive for applications where data-privacymust be preserved). Our code is publicly available athttps://github.com/jamessealesmith/ConStruct-VL\rForget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models\nEric Zhang Kai Wang Xingqian Xu Zhangyang Wang Humphrey Shi\nabstract\rabstract: The unlearning problem of deep learning models, once primarily an academicconcern, has become a prevalent issue in the industry. The significant advancesin text-to-image generation techniques have prompted global discussions onprivacy, copyright, and safety, as numerous unauthorized personal IDs, content,artistic creations, and potentially harmful materials have been learned bythese models and later utilized to generate and distribute uncontrolledcontent. To address this challenge, we propose \\textbf{Forget-Me-Not}, anefficient and low-cost solution designed to safely remove specified IDs,objects, or styles from a well-configured text-to-image model in as little as30 seconds, without impairing its ability to generate other content. Alongsideour method, we introduce the \\textbf{Memorization Score (M-Score)} and\\textbf{ConceptBench} to measure the models\u0026rsquo; capacity to generate generalconcepts, grouped into three primary categories: ID, object, and style. UsingM-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectivelyeliminate targeted concepts while maintaining the model\u0026rsquo;s performance on otherconcepts. Furthermore, Forget-Me-Not offers two practical extensions: a)removal of potentially harmful or NSFW content, and b) enhancement of modelaccuracy, inclusion and diversity through \\textbf{concept correction anddisentanglement}. It can also be adapted as a lightweight model patch forStable Diffusion, allowing for concept manipulation and convenientdistribution. To encourage future research in this critical area and promotethe development of safe and inclusive generative models, we will open-sourceour code and ConceptBench at\\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.\r2023-03-28\nSynthetically generated text for supervised text analysis\nAndrew Halterman\nabstract\rabstract: Supervised text models are a valuable tool for political scientists butpresent several obstacles to their use, including the expense of hand-labelingdocuments, the difficulty of retrieving rare relevant documents for annotation,and copyright and privacy concerns involved in sharing annotated documents.This article proposes a partial solution to these three issues, in the form ofcontrolled generation of synthetic text with large language models. I provide aconceptual overview of text generation, guidance on when researchers shouldprefer different techniques for generating synthetic text, a discussion ofethics, and a simple technique for improving the quality of synthetic text. Idemonstrate the usefulness of synthetic text with three applications:generating synthetic tweets describing the fighting in Ukraine, synthetic newsarticles describing specified political events for training an event detectionsystem, and a multilingual corpus of populist manifesto statements for traininga sentence-level populism classifier.\rFoundation Models and Fair Use\nPeter Henderson Xuechen Li Dan Jurafsky Tatsunori Hashimoto Mark A. Lemley Percy Liang\nabstract\rabstract: Existing foundation models are trained on copyrighted material. Deployingthese models can pose both legal and ethical risks when data creators fail toreceive appropriate attribution or compensation. In the United States andseveral other countries, copyrighted content may be used to build foundationmodels without incurring liability due to the fair use doctrine. However, thereis a caveat: If the model produces output that is similar to copyrighted data,particularly in scenarios that affect the market of that data, fair use may nolonger apply to the output of the model. In this work, we emphasize that fairuse is not guaranteed, and additional work may be necessary to keep modeldevelopment and deployment squarely in the realm of fair use. First, we surveythe potential risks of developing and deploying foundation models based oncopyrighted content. We review relevant U.S. case law, drawing parallels toexisting and potential applications for generating text, source code, andvisual art. Experiments confirm that popular foundation models can generatecontent considerably similar to copyrighted material. Second, we discusstechnical mitigations that can help foundation models stay in line with fairuse. We argue that more research is needed to align mitigation strategies withthe current state of the law. Lastly, we suggest that the law and technicalmitigations should co-evolve. For example, coupled with other policymechanisms, the law could more explicitly consider safe harbors when strongtechnical tools are used to mitigate infringement harms. This co-evolution mayhelp strike a balance between intellectual property and innovation, whichspeaks to the original goal of fair use. But we emphasize that the strategieswe describe here are not a panacea and more work is needed to develop policiesthat address the potential harms of foundation models.\r2023-03-27\nUnimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation\nRongyu Zhang Xiaowei Chi Guiliang Liu Wenyi Zhang Yuan Du Fangxin Wang\nabstract\rabstract: Multimodal learning has seen great success mining data features from multiplemodalities with remarkable model performance improvement. Meanwhile, federatedlearning (FL) addresses the data sharing problem, enabling privacy-preservedcollaborative training to provide sufficient precious data. Great potential,therefore, arises with the confluence of them, known as multimodal federatedlearning. However, limitation lies in the predominant approaches as they oftenassume that each local dataset records samples from all modalities. In thispaper, we aim to bridge this gap by proposing an Unimodal Training - MultimodalPrediction (UTMP) framework under the context of multimodal federated learning.We design HA-Fedformer, a novel transformer-based model that empowers unimodaltraining with only a unimodal dataset at the client and multimodal testing byaggregating multiple clients\u0026rsquo; knowledge for better accuracy. The key advantagesare twofold. Firstly, to alleviate the impact of data non-IID, we develop anuncertainty-aware aggregation method for the local encoders with layer-wiseMarkov Chain Monte Carlo sampling. Secondly, to overcome the challenge ofunaligned language sequence, we implement a cross-modal decoder aggregation tocapture the hidden signal correlation between decoders trained by data fromdifferent modalities. Our experiments on popular sentiment analysis benchmarks,CMU-MOSI and CMU-MOSEI, demonstrate that HA-Fedformer significantly outperformsstate-of-the-art multimodal models under the UTMP federated learningframeworks, with 15%-20% improvement on most attributes.\r2023-03-23\nPrimer: Fast Private Transformer Inference on Encrypted Data\nMengxin Zheng Qian Lou Lei Jiang\nabstract\rabstract: It is increasingly important to enable privacy-preserving inference for cloudservices based on Transformers. Post-quantum cryptographic techniques, e.g.,fully homomorphic encryption (FHE), and multi-party computation (MPC), arepopular methods to support private Transformer inference. However, existingworks still suffer from prohibitively computational and communicationaloverhead. In this work, we present, Primer, to enable a fast and accurateTransformer over encrypted data for natural language processing tasks. Inparticular, Primer is constructed by a hybrid cryptographic protocol optimizedfor attention-based Transformer models, as well as techniques includingcomputation merge and tokens-first ciphertext packing. Comprehensiveexperiments on encrypted language modeling show that Primer achievesstate-of-the-art accuracy and reduces the inference latency by 90.6% ~ 97.5%over previous methods.\rDevelopment and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse\nXavier Tannier Perceval Wajsbürt Alice Calliger Basile Dura Alexandre Mouchet Martin Hilka Romain Bey\nabstract\rabstract: The objective of this study is to address the critical issue ofde-identification of clinical reports in order to allow access to data forresearch purposes, while ensuring patient privacy. The study highlights thedifficulties faced in sharing tools and resources in this domain and presentsthe experience of the Greater Paris University Hospitals (AP-HP) inimplementing a systematic pseudonymization of text documents from its ClinicalData Warehouse. We annotated a corpus of clinical documents according to 12types of identifying entities, and built a hybrid system, merging the resultsof a deep learning model as well as manual rules. Our results show an overallperformance of 0.99 of F1-score. We discuss implementation choices and presentexperiments to better understand the effort involved in such a task, includingdataset size, document types, language models, or rule addition. We shareguidelines and code under a 3-Clause BSD license.\r2023-03-22\nMan vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models\nConstantinos Patsakis Nikolaos Lykousas\nabstract\rabstract: The collection and use of personal data are becoming more common in today\u0026rsquo;sdata-driven culture. While there are many advantages to this, including betterdecision-making and service delivery, it also poses significant ethical issuesaround confidentiality and privacy. Text anonymisation tries to prune and/ormask identifiable information from a text while keeping the remaining contentintact to alleviate privacy concerns. Text anonymisation is especiallyimportant in industries like healthcare, law, as well as research, wheresensitive and personal information is collected, processed, and exchanged underhigh legal and ethical standards. Although text anonymization is widely adopted in practice, it continues toface considerable challenges. The most significant challenge is striking abalance between removing information to protect individuals\u0026rsquo; privacy whilemaintaining the text\u0026rsquo;s usability for future purposes. The question is whetherthese anonymisation methods sufficiently reduce the risk of re-identification,in which an individual can be identified based on the remaining information inthe text. In this work, we challenge the effectiveness of these methods and how weperceive identifiers. We assess the efficacy of these methods against theelephant in the room, the use of AI over big data. While most of the researchis focused on identifying and removing personal information, there is limiteddiscussion on whether the remaining information is sufficient to deanonymiseindividuals and, more precisely, who can do it. To this end, we conduct anexperiment using GPT over anonymised texts of famous people to determinewhether such trained networks can deanonymise them. The latter allows us torevise these methods and introduce a novel methodology that employs LargeLanguage Models to improve the anonymity of texts.\r2023-03-20\nGenerative AI and the Digital Commons\nSaffron Huang Divya Siddarth\nabstract\rabstract: Many generative foundation models (or GFMs) are trained on publicly availabledata and use public infrastructure, but 1) may degrade the \u0026ldquo;digital commons\u0026quot;that they depend on, and 2) do not have processes in place to return valuecaptured to data producers and stakeholders. Existing conceptions of datarights and protection (focusing largely on individually-owned data andassociated privacy concerns) and copyright or licensing-based models offer someinstructive priors, but are ill-suited for the issues that may arise frommodels trained on commons-based data. We outline the risks posed by GFMs andwhy they are relevant to the digital commons, and propose numerousgovernance-based solutions that include investments in standardizeddataset/model disclosure and other kinds of transparency when it comes togenerative models\u0026rsquo; training and capabilities, consortia-based funding formonitoring/standards/auditing organizations, requirements or norms for GFMcompanies to contribute high quality data to the commons, and structures forshared ownership based on individual or community provision of fine-tuningdata.\rPrivately Fine-Tuning Large Language Models with Differential Privacy\nRouzbeh Behnia Mohamamdreza Ebrahimi Jason Pacheco Balaji Padmanabhan\nabstract\rabstract: Pre-trained Large Language Models (LLMs) are an integral part of modern AIthat have led to breakthrough performances in complex AI tasks. Major AIcompanies with expensive infrastructures are able to develop and train theselarge models with billions and millions of parameters from scratch. Thirdparties, researchers, and practitioners are increasingly adopting thesepre-trained models and fine-tuning them on their private data to accomplishtheir downstream AI tasks. However, it has been shown that an adversary canextract/reconstruct the exact training samples from these LLMs, which can leadto revealing personally identifiable information. The issue has raised deepconcerns about the privacy of LLMs. Differential privacy (DP) provides arigorous framework that allows adding noise in the process of training orfine-tuning LLMs such that extracting the training data becomes infeasible(i.e., with a cryptographically small success probability). While thetheoretical privacy guarantees offered in most extant studies assume learningmodels from scratch through many training iterations in an asymptotic setting,this assumption does not hold in fine-tuning scenarios in which the number oftraining iterations is significantly smaller. To address the gap, we present\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant withfinite-sample privacy guarantees. Our results across four well-establishednatural language understanding (NLU) tasks show that while \\ewtune~adds privacyguarantees to LLM fine-tuning process, it directly contributes to decreasingthe induced noise to up to 5.6% and improves the state-of-the-art LLMsperformance by up to 1.1% across all NLU tasks. We have open-sourced ourimplementations for wide adoption and public testing purposes.\rBillionCOV: An Enriched Billion-scale Collection of COVID-19 tweets for Efficient Hydration\nRabindra Lamsal Maria Rodriguez Read Shanika Karunasekera\nabstract\rabstract: The COVID-19 pandemic introduced new norms such as social distancing, facemasks, quarantine, lockdowns, travel restrictions, work/study from home, andbusiness closures, to name a few. The pandemic\u0026rsquo;s seriousness made people vocalon social media, especially on microblogs such as Twitter. Researchers havebeen collecting and sharing large-scale datasets of COVID-19 tweets since theearly days of the outbreak. Sharing raw Twitter data with third parties isrestricted; users need to hydrate tweet identifiers in a public dataset tore-create the dataset locally. Large-scale datasets that include originaltweets, retweets, quotes, and replies have tweets in billions which takesmonths to hydrate. The existing datasets carry issues related to proportion andredundancy. We report that more than 500 million tweet identifiers point todeleted or protected tweets. In order to address these issues, this paperintroduces an enriched global billion-scale English-language COVID-19 tweetsdataset, BillionCOV, that contains 1.4 billion tweets originating from 240countries and territories between October 2019 and April 2022. Importantly,BillionCOV facilitates researchers to filter tweet identifiers for efficienthydration. This paper discusses associated methods to fetch raw Twitter datafor a set of tweet identifiers, presents multiple tweets\u0026rsquo; distributions toprovide an overview of BillionCOV, and finally, reviews the dataset\u0026rsquo;s potentialuse cases.\r2023-03-18\nDeAR: Debiasing Vision-Language Models with Additive Residuals\nAshish Seth Mayur Hemani Chirag Agarwal\nabstract\rabstract: Large pre-trained vision-language models (VLMs) reduce the time fordeveloping predictive models for various vision-grounded language downstreamtasks by providing rich, adaptable image and text representations. However,these models suffer from societal biases owing to the skewed distribution ofvarious identity groups in the training data. These biases manifest as theskewed similarity between the representations for specific text concepts andimages of people of different identity groups and, therefore, limit theusefulness of such models in real-world high-stakes applications. In this work,we present DeAR (Debiasing with Additive Residuals), a novel debiasing methodthat learns additive residual image representations to offset the originalrepresentations, ensuring fair output representations. In doing so, it reducesthe ability of the representations to distinguish between the differentidentity groups. Further, we observe that the current fairness tests areperformed on limited face image datasets that fail to indicate why a specifictext concept should/should not apply to them. To bridge this gap and betterevaluate DeAR, we introduce the Protected Attribute Tag Association (PATA)dataset - a new context-based bias benchmarking dataset for evaluating thefairness of large pre-trained VLMs. Additionally, PATA provides visual contextfor a diverse human population in different scenarios with both positive andnegative connotations. Experimental results for fairness and zero-shotperformance preservation using multiple datasets demonstrate the efficacy ofour framework.\rFedRight: An Effective Model Copyright Protection for Federated Learning\nJinyin Chen Mingjun Li Mingjun Li Haibin Zheng\nabstract\rabstract: Federated learning (FL), an effective distributed machine learning framework,implements model training and meanwhile protects local data privacy. It hasbeen applied to a broad variety of practice areas due to its great performanceand appreciable profits. Who owns the model, and how to protect the copyrighthas become a real problem. Intuitively, the existing property rights protectionmethods in centralized scenarios (e.g., watermark embedding and modelfingerprints) are possible solutions for FL. But they are still challenged bythe distributed nature of FL in aspects of the no data sharing, parameteraggregation, and federated training settings. For the first time, we formalizethe problem of copyright protection for FL, and propose FedRight to protectmodel copyright based on model fingerprints, i.e., extracting model features bygenerating adversarial examples as model fingerprints. FedRight outperformsprevious works in four key aspects: (i) Validity: it extracts model features togenerate transferable fingerprints to train a detector to verify the copyrightof the model. (ii) Fidelity: it is with imperceptible impact on the federatedtraining, thus promising good main task performance. (iii) Robustness: it isempirically robust against malicious attacks on copyright protection, i.e.,fine-tuning, model pruning, and adaptive attacks. (iv) Black-box: it is validin the black-box forensic scenario where only application programming interfacecalls to the model are available. Extensive evaluations across 3 datasets and 9model structures demonstrate FedRight\u0026rsquo;s superior fidelity, validity, androbustness.\r2023-03-17\nOn the De-duplication of LAION-2B\nRyan Webster Julien Rabin Loic Simon Frederic Jurie\nabstract\rabstract: Generative models, such as DALL-E, Midjourney, and Stable Diffusion, havesocietal implications that extend beyond the field of computer science. Thesemodels require large image databases like LAION-2B, which contain two billionimages. At this scale, manual inspection is difficult and automated analysis ischallenging. In addition, recent studies show that duplicated images posecopyright problems for models trained on LAION2B, which hinders its usability.This paper proposes an algorithmic chain that runs with modest compute, thatcompresses CLIP features to enable efficient duplicate detection, even for vastimage volumes. Our approach demonstrates that roughly 700 million images, orabout 30%, of LAION-2B\u0026rsquo;s images are likely duplicated. Our method alsoprovides the histograms of duplication on this dataset, which we use to revealmore examples of verbatim copies by Stable Diffusion and further justify theapproach. The current version of the de-duplicated set will be distributedonline.\rRethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation\nYifan Yan Xudong Pan Mi Zhang Min Yang\nabstract\rabstract: Copyright protection for deep neural networks (DNNs) is an urgent need for AIcorporations. To trace illegally distributed model copies, DNN watermarking isan emerging technique for embedding and verifying secret identity messages inthe prediction behaviors or the model internals. Sacrificing less functionalityand involving more knowledge about the target DNN, the latter branch called\\textit{white-box DNN watermarking} is believed to be accurate, credible andsecure against most known watermark removal attacks, with emerging researchefforts in both the academy and the industry. In this paper, we present the first systematic study on how the mainstreamwhite-box DNN watermarks are commonly vulnerable to neural structuralobfuscation with \\textit{dummy neurons}, a group of neurons which can be addedto a target model but leave the model behavior invariant. Devising acomprehensive framework to automatically generate and inject dummy neurons withhigh stealthiness, our novel attack intensively modifies the architecture ofthe target model to inhibit the success of watermark verification. Withextensive evaluation, our work for the first time shows that nine publishedwatermarking schemes require amendments to their verification procedures.\r2023-03-16\nA Short Survey of Viewing Large Language Models in Legal Aspect\nZhongxiang Sun\nabstract\rabstract: Large language models (LLMs) have transformed many fields, including naturallanguage processing, computer vision, and reinforcement learning. These modelshave also made a significant impact in the field of law, where they are beingincreasingly utilized to automate various legal tasks, such as legal judgementprediction, legal document analysis, and legal document writing. However, theintegration of LLMs into the legal field has also raised several legalproblems, including privacy concerns, bias, and explainability. In this survey,we explore the integration of LLMs into the field of law. We discuss thevarious applications of LLMs in legal tasks, examine the legal challenges thatarise from their use, and explore the data resources that can be used tospecialize LLMs in the legal domain. Finally, we discuss several promisingdirections and conclude this paper. By doing so, we hope to provide an overviewof the current state of LLMs in law and highlight the potential benefits andchallenges of their integration.\r2023-03-15\nCopyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution\nHaonan Zhong Jiamin Chang Ziyue Yang Tingmin Wu Pathum Chamikara Mahawaga Arachchige Chehara Pathmabandu Minhui Xue\nabstract\rabstract: Generative AI (e.g., Generative Adversarial Networks - GANs) has becomeincreasingly popular in recent years. However, Generative AI introducessignificant concerns regarding the protection of Intellectual Property Rights(IPR) (resp. model accountability) pertaining to images (resp. toxic images)and models (resp. poisoned models) generated. In this paper, we propose anevaluation framework to provide a comprehensive overview of the current stateof the copyright protection measures for GANs, evaluate their performanceacross a diverse range of GAN architectures, and identify the factors thataffect their performance and future research directions. Our findings indicatethat the current IPR protection methods for input images, model watermarking,and attribution networks are largely satisfactory for a wide range of GANs. Wehighlight that further attention must be directed towards protecting trainingsets, as the current approaches fail to provide robust IPR protection andprovenance tracing on training sets.\r2023-03-14\nNavigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning\nYunchao Zhang Zonglin Di Kaiwen Zhou Cihang Xie Xin Eric Wang\nabstract\rabstract: Federated embodied agent learning protects the data privacy of individualvisual environments by keeping data locally at each client (the individualenvironment) during training. However, since the local data is inaccessible tothe server under federated learning, attackers may easily poison the trainingdata of the local client to build a backdoor in the agent without notice.Deploying such an agent raises the risk of potential harm to humans, as theattackers may easily navigate and control the agent as they wish via thebackdoor. Towards Byzantine-robust federated embodied agent learning, in thispaper, we study the attack and defense for the task of vision-and-languagenavigation (VLN), where the agent is required to follow natural languageinstructions to navigate indoor environments. First, we introduce a simple buteffective attack strategy, Navigation as Wish (NAW), in which the maliciousclient manipulates local trajectory data to implant a backdoor into the globalmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easilynavigate the deployed VLN agent regardless of the language instruction, withoutaffecting its performance on normal test sets. Then, we propose a newPrompt-Based Aggregation (PBA) to defend against the NAW attack in federatedVLN, which provides the server with a \u0026lsquo;\u0026lsquo;prompt\u0026rsquo;\u0026rsquo; of the vision-and-languagealignment variance between the benign and malicious clients so that they can bedistinguished during training. We validate the effectiveness of the PBA methodon protecting the global model from the NAW attack, which outperforms otherstate-of-the-art defense methods by a large margin in the defense metrics onR2R and RxR.\r2023-03-11\nContext-based Ontology Modelling for Database: Enabling ChatGPT for Semantic Database Management\nWenjun Lin Paul Babyn Yan yan Wenjun Zhang\nabstract\rabstract: This research paper explores the use of ChatGPT in database management.ChatGPT, an AI-powered chatbot, has limitations in performing tasks related todatabase management due to the lack of standardized vocabulary and grammar forrepresenting database semantics. To address this limitation, the paper proposesa solution that involves developing a set of syntaxes that can representdatabase semantics in natural language. The syntax is used to convert databaseschemas into natural language formats, providing a new application of ChatGPTin database management. The proposed solution is demonstrated through a casestudy where ChatGPT is used to perform two tasks, semantic integration, andtables joining. Results demonstrate that the use of semantic databaserepresentations produces more precise outcomes and avoids common mistakescompared to cases with no semantic representation. The proposed method has thepotential to speed up the database management process, reduce the level ofunderstanding required for database domain knowledge, and enable automaticdatabase operations without accessing the actual data, thus illuminatingprivacy protection concerns when using AI. This paper provides a promising newdirection for research in the field of AI-based database management.\r2023-03-06\nQuantifying Memorization Across Neural Language Models\nNicholas Carlini Daphne Ippolito Matthew Jagielski Katherine Lee Florian Tramer Chiyuan Zhang\nabstract\rabstract: Large language models (LMs) have been shown to memorize parts of theirtraining data, and when prompted appropriately, they will emit the memorizedtraining data verbatim. This is undesirable because memorization violatesprivacy (exposing user data), degrades utility (repeated easy-to-memorize textis often low quality), and hurts fairness (some texts are memorized overothers). We describe three log-linear relationships that quantify the degree to whichLMs emit memorized training data. Memorization significantly grows as weincrease (1) the capacity of a model, (2) the number of times an example hasbeen duplicated, and (3) the number of tokens of context used to prompt themodel. Surprisingly, we find the situation becomes more complicated whengeneralizing these results across model families. On the whole, we find thatmemorization in LMs is more prevalent than previously believed and will likelyget worse as models continues to scale, at least without active mitigations.\rBlockchain-Empowered Lifecycle Management for AI-Generated Content (AIGC) Products in Edge Networks\nYinqiu Liu Hongyang Du Dusit Niyato Jiawen Kang Zehui Xiong Chunyan Miao Xuemin Shen Abbas Jamalipour\nabstract\rabstract: The rapid development of Artificial IntelligenceGenerated Content (AIGC) hasbrought daunting challenges regarding service latency, security, andtrustworthiness. Recently, researchers presented the edge AIGC paradigm,effectively optimize the service latency by distributing AIGC services to edgedevices. However, AIGC products are still unprotected and vulnerable totampering and plagiarization. Moreover, as a kind of online non-fungibledigital property, the free circulation of AIGC products is hindered by the lackof trustworthiness in open networks. In this article, for the first time, wepresent a blockchain-empowered framework to manage the lifecycle of edge AIGCproducts. Specifically, leveraging fraud proof, we first propose a protocol toprotect the ownership and copyright of AIGC, called Proof-of-AIGC. Then, wedesign an incentive mechanism to guarantee the legitimate and timely executionsof the funds-AIGC ownership exchanges among anonymous users. Furthermore, webuild a multi-weight subjective logic-based reputation scheme, with which AIGCproducers can determine which edge service provider is trustworthy and reliableto handle their services. Through numerical results, the superiority of theproposed approach is demonstrated. Last but not least, we discuss importantopen directions for further research.\r2023-03-04\nTopological Quantum Gates in Homotopy Type Theory\nDavid Jaz Myers Hisham Sati Urs Schreiber\nabstract\rabstract: Despite the evident necessity of topological protection for realizingscalable quantum computers, the conceptual underpinnings of topological quantumlogic gates had arguably remained shaky, both regarding their physicalrealization as well as their information-theoretic nature. Building on recent results on defect branes in string/M-theory and on theirholographically dual anyonic defects in condensed matter theory, here weexplain how the specification of realistic topological quantum gates, operatingby anyon defect braiding in topologically ordered quantum materials, has asurprisingly slick formulation in parameterized point-set topology, which is sofundamental that it lends itself to certification in modern homotopically typedprogramming languages, such as cubical Agda. We propose that this remarkable confluence of concepts may jointly kickstartthe development of topological quantum programming proper as well as ofreal-world application of homotopy type theory, both of which have arguablybeen falling behind their high expectations; in any case, it provides apowerful paradigm for simulating and verifying topological quantum computingarchitectures with high-level certification languages aware of the actualphysical principles of realistic topological quantum hardware. In a companion article, we will explain how further passage to \u0026ldquo;dependentlinear\u0026rdquo; homotopy data types naturally extends this scheme to a full-blownquantum programming/certification language in which our topological quantumgates may be compiled to verified quantum circuits, complete with quantummeasurement gates and classical control.\r2023-03-01\nOn Pre-trained Language Models for Antibody\nDanqing Wang Fei Ye Hao Zhou\nabstract\rabstract: Antibodies are vital proteins offering robust protection for the human bodyfrom pathogens. The development of general protein and antibody-specificpre-trained language models both facilitate antibody prediction tasks. However,there have been limited studies that comprehensively explore the representationcapability of distinct pre-trained language models on different antibody tasks.To investigate the problem, we aim to answer several key questions in thispaper, such as how pre-trained language models perform in antibody tasks withdifferent specificity and how introducing specific biological mechanisms to thepre-training process can benefit the model. Additionally, we evaluate if thelearned antibody pre-trained representations can be applied to real-worldantibody problems, like drug discovery and immune process understanding.Previously, no benchmark available largely hindered the study to answer thesequestions. To aid in our investigation, we provide an AnTibody UnderstandingEvaluation (ATUE) benchmark. We comprehensively evaluate the performance ofprotein pre-trained language models by empirical study along with conclusionsand new insights. Our ATUE and code are released athttps://github.com/dqwang122/EATLM.\rCANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning\nSamuel Maddock Alexandre Sablayrolles Pierre Stock\nabstract\rabstract: Federated Learning (FL) is a setting for training machine learning models indistributed environments where the clients do not share their raw data butinstead send model updates to a server. However, model updates can be subjectto attacks and leak private information. Differential Privacy (DP) is a leadingmitigation strategy which involves adding noise to clipped model updates,trading off performance for strong theoretical privacy guarantees. Previouswork has shown that the threat model of DP is conservative and that theobtained guarantees may be vacuous or may overestimate information leakage inpractice. In this paper, we aim to achieve a tighter measurement of the modelexposure by considering a realistic threat model. We propose a novel method,CANIFE, that uses canaries - carefully crafted samples by a strong adversary toevaluate the empirical privacy of a training round. We apply this attack tovision models trained on CIFAR-10 and CelebA and to language models trained onSent140 and Shakespeare. In particular, in realistic FL scenarios, wedemonstrate that the empirical per-round epsilon obtained with CANIFE is 4-5xlower than the theoretical bound.\rContextual Linear Types for Differential Privacy\nMatías Toro David Darais Chike Abuah Joe Near Damián Árquez Federico Olmedo Éric Tanter\nabstract\rabstract: Language support for differentially-private programming is both crucial anddelicate. While elaborate program logics can be very expressive, type-systembased approaches using linear types tend to be more lightweight and amenable toautomatic checking and inference, and in particular in the presence ofhigher-order programming. Since the seminal design of Fuzz, which is restrictedto $\\epsilon$-differential privacy in its original design, significant progresshas been made to support more advancedvariants of differential privacy,like($\\epsilon$,$\\delta$)-differential privacy. However, supporting theseadvanced privacy variants while also supporting higher-order programming infull has proven to be challenging. We present Jazz, a language and type systemwhich uses linear types and latent contextual effects to support both advancedvariants of differential privacy and higher-order programming. Latentcontextual effects allow delaying the payment of effects for connectives suchas products, sums and functions, yielding advantages in terms of precision ofthe analysis and annotation burden upon elimination, as well as modularity. Weformalize the core of Jazz, prove it sound for privacy via a logical relationfor metric preservation, and illustrate its expressive power through a numberof case studies drawn from the recent differential privacy literature.\rDTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction\nRaviteja Anantha Kriti Bhasin Daniela de la Parra Aguilar Prabal Vashisht Becci Williamson Srinivas Chappidi\nabstract\rabstract: Personal Digital Assistants (PDAs) - such as Siri, Alexa and GoogleAssistant, to name a few - play an increasingly important role to accessinformation and complete tasks spanning multiple domains, and by diverse groupsof users. A text-to-speech (TTS) module allows PDAs to interact in a natural,human-like manner, and play a vital role when the interaction involves peoplewith visual impairments or other disabilities. To cater to the needs of adiverse set of users, inclusive TTS is important to recognize and pronouncecorrectly text in different languages and dialects. Despite great progress inspeech synthesis, the pronunciation accuracy of named entities in amulti-lingual setting still has a large room for improvement. Existingapproaches to correct named entity (NE) mispronunciations, like retrainingGrapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciationdictionary, require expensive annotation of the ground truth pronunciation,which is also time consuming. In this work, we present a highly-precise,PDA-compatible pronunciation learning framework for the task of TTSmispronunciation detection and correction. In addition, we also propose a novelmispronunciation detection model called DTW-SiameseNet, which employs metriclearning with a Siamese architecture for Dynamic Time Warping (DTW) withtriplet loss. We demonstrate that a locale-agnostic, privacy-preservingsolution to the problem of TTS mispronunciation detection is feasible. Weevaluate our approach on a real-world dataset, and a corpus of NEpronunciations of an anonymized audio dataset of person names recorded byparticipants from 10 different locales. Human evaluation shows our proposedapproach improves pronunciation accuracy on average by ~6% compared to strongphoneme-based and audio-based baselines.\r2023-02-28\nThe (ab)use of Open Source Code to Train Large Language Models\nAli Al-Kaswan Maliheh Izadi\nabstract\rabstract: In recent years, Large Language Models (LLMs) have gained significantpopularity due to their ability to generate human-like text and their potentialapplications in various fields, such as Software Engineering. LLMs for Code arecommonly trained on large unsanitized corpora of source code scraped from theInternet. The content of these datasets is memorized and emitted by the models,often in a verbatim manner. In this work, we will discuss the security,privacy, and licensing implications of memorization. We argue why the use ofcopyleft code to train LLMs is a legal and ethical dilemma. Finally, we providefour actionable recommendations to address this issue.\r2023-02-27\nDo as You Say: Consistency Detection of Data Practice in Program Code and Privacy Policy in Mini-App\nYin Wang Ming Fan Junfeng Liu Junjie Tao Wuxia Jin Qi Xiong Yuhao Liu Qinghua Zheng Ting Liu\nabstract\rabstract: Mini-app is an emerging form of mobile application that combines webtechnology with native capabilities. Its features, e.g., no need to downloadand no installation, have made it popular rapidly. However, privacy issues thatviolate the laws or regulations are breeding in the swiftly expanding mini-appecosystem. The consistency between what the mini-app does about the data in theprogram code and what it declares in its privacy policy description isimportant. But no work has systematically investigated the privacy problem ofthe mini-app before. In this paper, to our best knowledge, we are the first toconduct the compliance detection of data practice and policy description inmini-apps. In this paper, we first customize a taint analysis method based ondata entity dependency network to adapt to the characteristics of theJavaScript language in the mini-apps. Then, we transform data types and dataoperations to data practices in program codes and privacy policies, so as tofinish a fine-grained consistency matching model.We crawl 100,000 mini-apps onWeChat client in the wild and extract 2,998 with a privacy policy. Among them,only 318 meet the consistency requirements, 2,680 are inconsistent, and theproportion of inconsistencies is as high as 89.4%. The inconsistency in themini-app is very serious. Based on 6 real-world cases analyzed, in order toreduce this potential data leakage risk, we suggest that the developer shouldreduce the collection of irrelevant information and the straightforward use oftemplates, and the platform should provide data flow detection tools andprivacy policy writing support.\rPQLM \u0026ndash; Multilingual Decentralized Portable Quantum Language Model for Privacy Protection\nShuyue Stella Li Xiangyu Zhang Shu Zhou Hongchao Shu Ruixing Liang Hexin Liu Leibny Paola Garcia\nabstract\rabstract: With careful manipulation, malicious agents can reverse engineer privateinformation encoded in pre-trained language models. Security concerns motivatethe development of quantum pre-training. In this work, we propose a highlyPortable Quantum Language Model (PQLM) that can easily transmit information todownstream tasks on classical machines. The framework consists of a cloud PQLMbuilt with random Variational Quantum Classifiers (VQC) and local models fordownstream applications. We demonstrate the ad hoc portability of the quantummodel by extracting only the word embeddings and effectively applying them todownstream tasks on classical machines. Our PQLM exhibits comparableperformance to its classical counterpart on both intrinsic evaluation (loss,perplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)metrics. We also perform ablation studies on the factors affecting PQLMperformance to analyze model stability. Our work establishes a theoreticalfoundation for a portable quantum pre-trained language model that could betrained on private data and made available for public use with privacyprotection guarantees.\r2023-02-25\nOn pitfalls (and advantages) of sophisticated large language models\nAnna Strasser\nabstract\rabstract: Natural language processing based on large language models (LLMs) is abooming field of AI research. After neural networks have proven to outperformhumans in games and practical domains based on pattern recognition, we mightstand now at a road junction where artificial entities might eventually enterthe realm of human communication. However, this comes with serious risks. Dueto the inherent limitations regarding the reliability of neural networks,overreliance on LLMs can have disruptive consequences. Since it will beincreasingly difficult to distinguish between human-written andmachine-generated text, one is confronted with new ethical challenges. Thisbegins with the no longer undoubtedly verifiable human authorship and continueswith various types of fraud, such as a new form of plagiarism. This alsoconcerns the violation of privacy rights, the possibility of circulatingcounterfeits of humans, and, last but not least, it makes a massive spread ofmisinformation possible.\r2023-02-23\nEfficiency 360: Efficient Vision Transformers\nBadri N. Patro Vijay Srinivas Agneeswaran\nabstract\rabstract: Transformers are widely used for solving tasks in natural languageprocessing, computer vision, speech, and music domains. In this paper, we talkabout the efficiency of transformers in terms of memory (the number ofparameters), computation cost (number of floating points operations), andperformance of models, including accuracy, the robustness of the model, andfair \u0026amp; bias-free features. We mainly discuss the vision transformer for theimage classification task. Our contribution is to introduce an efficient 360framework, which includes various aspects of the vision transformer, to make itmore efficient for industrial applications. By considering those applications,we categorize them into multiple dimensions such as privacy, robustness,transparency, fairness, inclusiveness, continual learning, probabilisticmodels, approximation, computational complexity, and spectral complexity. Wecompare various vision transformer models based on their performance, thenumber of parameters, and the number of floating point operations (FLOPs) onmultiple datasets.\rPrivately Customizing Prefinetuning to Better Match User Data in Federated Learning\nCharlie Hou Hongyuan Zhan Akshat Shrivastava Sid Wang Aleksandr Livshits Giulia Fanti Daniel Lazar\nabstract\rabstract: In Federated Learning (FL), accessing private client data incurscommunication and privacy costs. As a result, FL deployments commonlyprefinetune pretrained foundation models on a (large, possibly public) datasetthat is held by the central server; they then FL-finetune the model on aprivate, federated dataset held by clients. Evaluating prefinetuning datasetquality reliably and privately is therefore of high importance. To this end, wepropose FreD (Federated Private Fr'echet Distance) \u0026ndash; a privately computeddistance between a prefinetuning dataset and federated datasets. Intuitively,it privately computes and compares a Fr'echet distance between embeddingsgenerated by a large language model on both the central (public) dataset andthe federated private client data. To make this computation privacy-preserving,we use distributed, differentially-private mean and covariance estimators. Weshow empirically that FreD accurately predicts the best prefinetuning datasetat minimal privacy cost. Altogether, using FreD we demonstrate aproof-of-concept for a new approach in private FL training: (1) customize aprefinetuning dataset to better match user data (2) prefinetune (3) performFL-finetuning.\r2023-02-22\nPreventing Catastrophic Forgetting in Continual Learning of New Natural Language Tasks\nSudipta Kar Giuseppe Castellucci Simone Filice Shervin Malmasi Oleg Rokhlenko\nabstract\rabstract: Multi-Task Learning (MTL) is widely-accepted in Natural Language Processingas a standard technique for learning multiple related tasks in one model.Training an MTL model requires having the training data for all tasks availableat the same time. As systems usually evolve over time, (e.g., to support newfunctionalities), adding a new task to an existing MTL model usually requiresretraining the model from scratch on all the tasks and this can betime-consuming and computationally expensive. Moreover, in some scenarios, thedata used to train the original training may be no longer available, forexample, due to storage or privacy concerns. In this paper, we approach theproblem of incrementally expanding MTL models\u0026rsquo; capability to solve new tasksover time by distilling the knowledge of an already trained model on n tasksinto a new one for solving n+1 tasks. To avoid catastrophic forgetting, wepropose to exploit unlabeled data from the same distributions of the old tasks.Our experiments on publicly available benchmarks show that such a techniquedramatically benefits the distillation by preserving the already acquiredknowledge (i.e., preventing up to 20% performance drops on old tasks) whileobtaining good performance on the incrementally added tasks. Further, we alsoshow that our approach is beneficial in practical settings by using data from aleading voice assistant.\r2023-02-20\nProgrammable System Call Security with eBPF\nJinghao Jia YiFei Zhu Dan Williams Andrea Arcangeli Claudio Canella Hubertus Franke Tobin Feldman-Fitzthum Dimitrios Skarlatos Daniel Gruss Tianyin Xu\nabstract\rabstract: System call filtering is a widely used security mechanism for protecting ashared OS kernel against untrusted user applications. However, existing systemcall filtering techniques either are too expensive due to the context switchoverhead imposed by userspace agents, or lack sufficient programmability toexpress advanced policies. Seccomp, Linux\u0026rsquo;s system call filtering module, iswidely used by modern container technologies, mobile apps, and systemmanagement services. Despite the adoption of the classic BPF language (cBPF),security policies in Seccomp are mostly limited to static allow lists,primarily because cBPF does not support stateful policies. Consequently, manyessential security features cannot be expressed precisely and/or require kernelmodifications. In this paper, we present a programmable system call filtering mechanism,which enables more advanced security policies to be expressed by leveraging theextended BPF language (eBPF). More specifically, we create a new Seccomp eBPFprogram type, exposing, modifying or creating new eBPF helper functions tosafely manage filter state, access kernel and user state, and utilizesynchronization primitives. Importantly, our system integrates with existingkernel privilege and capability mechanisms, enabling unprivileged users toinstall advanced filters safely. Our evaluation shows that our eBPF-basedfiltering can enhance existing policies (e.g., reducing the attack surface ofearly execution phase by up to 55.4% for temporal specialization), mitigatereal-world vulnerabilities, and accelerate filters.\rFederated Learning for ASR based on Wav2vec 2.0\nTuan Nguyen Salima Mdhaffar Natalia Tomashenko Jean-François Bonastre Yannick Estève\nabstract\rabstract: This paper presents a study on the use of federated learning to train an ASRmodel based on a wav2vec 2.0 model pre-trained by self supervision. Carried outon the well-known TED-LIUM 3 dataset, our experiments show that such a modelcan obtain, with no use of a language model, a word error rate of 10.92% on theofficial TED-LIUM 3 test set, without sharing any data from the differentusers. We also analyse the ASR performance for speakers depending to theirparticipation to the federated learning. Since federated learning was firstintroduced for privacy purposes, we also measure its ability to protect speakeridentity. To do that, we exploit an approach to analyze information containedin exchanged models based on a neural network footprint on an indicatordataset. This analysis is made layer-wise and shows which layers in anexchanged wav2vec 2.0 based model bring the speaker identity information.\rOLYMPIA: A Simulation Framework for Evaluating the Concrete Scalability of Secure Aggregation Protocols\nIvoline C. Ngong Nicholas Gibson Joseph P. Near\nabstract\rabstract: Recent secure aggregation protocols enable privacy-preserving federatedlearning for high-dimensional models among thousands or even millions ofparticipants. Due to the scale of these use cases, however, end-to-endempirical evaluation of these protocols is impossible. We present OLYMPIA, aframework for empirical evaluation of secure protocols via simulation. OLYMPIAprovides an embedded domain-specific language for defining protocols, and asimulation framework for evaluating their performance. We implement severalrecent secure aggregation protocols using OLYMPIA, and perform the firstempirical comparison of their end-to-end running times. We release OLYMPIA asopen source.\rBlack Boxes, White Noise: Similarity Detection for Neural Functions\nFarima Farmahinifarahani Cristina V. Lopes\nabstract\rabstract: Similarity, or clone, detection has important applications in copyrightviolation, software theft, code search, and the detection of maliciouscomponents. There is now a good number of open source and proprietary clonedetectors for programs written in traditional programming languages. However,the increasing adoption of deep learning models in software poses a challengeto these tools: these models implement functions that are inscrutable blackboxes. As more software includes these DNN functions, new techniques are neededin order to assess the similarity between deep learning components of software.Previous work has unveiled techniques for comparing the representations learnedat various layers of deep neural network models by feeding canonical inputs tothe models. Our goal is to be able to compare DNN functions when canonicalinputs are not available \u0026ndash; because they may not be in many applicationscenarios. The challenge, then, is to generate appropriate inputs and toidentify a metric that, for those inputs, is capable of representing the degreeof functional similarity between two comparable DNN functions. Our approach uses random input with values between -1 and 1, in a shape thatis compatible with what the DNN models expect. We then compare the outputs byperforming correlation analysis. Our study shows how it is possible to performsimilarity analysis even in the absence of meaningful canonical inputs. Theresponse to random inputs of two comparable DNN functions exposes thosefunctions\u0026rsquo; similarity, or lack thereof. Of all the metrics tried, we find thatSpearman\u0026rsquo;s rank correlation coefficient is the most powerful and versatile,although in special cases other methods and metrics are more expressive. Wepresent a systematic empirical study comparing the effectiveness of severalsimilarity metrics using a dataset of 56,355 classifiers collected from GitHub.This is accompanied by a sensitivity analysis that reveals how certain models\u0026rsquo;training related properties affect the effectiveness of the similarity metrics. To the best of our knowledge, this is the first work that shows howsimilarity of DNN functions can be detected by using random inputs. Our studyof correlation metrics, and the identification of Spearman correlationcoefficient as the most powerful among them for this purpose, establishes acomplete and practical method for DNN clone detection that can be used in thedesign of new tools. It may also serve as inspiration for other programanalysis tasks whose approaches break in the presence of DNN components.\r2023-02-19\nMultilingual Content Moderation: A Case Study on Reddit\nMeng Ye Karan Sikka Katherine Atwell Sabit Hassan Ajay Divakaran Malihe Alikhani\nabstract\rabstract: Content moderation is the process of flagging content based on pre-definedplatform rules. There has been a growing need for AI moderators to safeguardusers as well as protect the mental health of human moderators from traumaticcontent. While prior works have focused on identifying hateful/offensivelanguage, they are not adequate for meeting the challenges of contentmoderation since 1) moderation decisions are based on violation of rules, whichsubsumes detection of offensive speech, and 2) such rules often differ acrosscommunities which entails an adaptive solution. We propose to study thechallenges of content moderation by introducing a multilingual dataset of 1.8Million Reddit comments spanning 56 subreddits in English, German, Spanish andFrench. We perform extensive experimental analysis to highlight the underlyingchallenges and suggest related research problems such as cross-lingualtransfer, learning under label noise (human biases), transfer of moderationmodels, and predicting the violated rule. Our dataset and analysis can helpbetter prepare for the challenges and opportunities of auto moderation.\rWhy Is Public Pretraining Necessary for Private Model Training?\nArun Ganesh Mahdi Haghifam Milad Nasr Sewoong Oh Thomas Steinke Om Thakkar Abhradeep Thakurta Lun Wang\nabstract\rabstract: In the privacy-utility tradeoff of a model trained on benchmark language andvision tasks, remarkable improvements have been widely reported with the use ofpretraining on publicly available data. This is in part due to the benefits oftransfer learning, which is the standard motivation for pretraining innon-private settings. However, the stark contrast in the improvement achievedthrough pretraining under privacy compared to non-private settings suggeststhat there may be a deeper, distinct cause driving these gains. To explain thisphenomenon, we hypothesize that the non-convex loss landscape of a modeltraining necessitates an optimization algorithm to go through two phases. Inthe first, the algorithm needs to select a good \u0026ldquo;basin\u0026rdquo; in the loss landscape.In the second, the algorithm solves an easy optimization within that basin. Theformer is a harder problem to solve with private data, while the latter isharder to solve with public data due to a distribution shift or data scarcity.Guided by this intuition, we provide theoretical constructions that provablydemonstrate the separation between private training with and without publicpretraining. Further, systematic experiments on CIFAR10 and LibriSpeech providesupporting evidence for our hypothesis.\r2023-02-18\nRobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks\nMarwan Omar\nabstract\rabstract: As machine learning (ML) systems are being increasingly employed in the realworld to handle sensitive tasks and make decisions in various fields, thesecurity and privacy of those models have also become increasingly critical. Inparticular, Deep Neural Networks (DNN) have been shown to be vulnerable tobackdoor attacks whereby adversaries have access to the training data and theopportunity to manipulate such data by inserting carefully developed samplesinto the training dataset. Although the NLP community has produced severalstudies on generating backdoor attacks proving the vulnerable state of languagemodes, to the best of our knowledge, there does not exist any work to combatsuch attacks. To bridge this gap, we present RobustEncoder: a novelclustering-based technique for detecting and removing backdoor attacks in thetext domain. Extensive empirical results demonstrate the effectiveness of ourtechnique in detecting and removing backdoor triggers. Our code is available athttps://github.com/marwanomar1/Backdoor-Learning-for-NLP\r2023-02-17\nPLACES: Prompting Language Models for Social Conversation Synthesis\nMaximillian Chen Alexandros Papangelis Chenyang Tao Seokhwan Kim Andy Rosenbaum Yang Liu Zhou Yu Dilek Hakkani-Tur\nabstract\rabstract: Collecting high quality conversational data can be very expensive for mostapplications and infeasible for others due to privacy, ethical, or similarconcerns. A promising direction to tackle this problem is to generate syntheticdialogues by prompting large language models. In this work, we use a small setof expert-written conversations as in-context examples to synthesize a socialconversation dataset using prompting. We perform several thorough evaluationsof our synthetic conversations compared to human-collected conversations. Thisincludes various dimensions of conversation quality with human evaluationdirectly on the synthesized conversations, and interactive human evaluation ofchatbots fine-tuned on the synthetically generated dataset. We additionallydemonstrate that this prompting approach is generalizable to multi-partyconversations, providing potential to create new synthetic data for multi-partytasks. Our synthetic multi-party conversations were rated more favorably acrossall measured dimensions compared to conversation excerpts sampled from ahuman-collected multi-party dataset.\rUncertainty-aware Self-training for Low-resource Neural Sequence Labeling\nJianing Wang Chengyu Wang Jun Huang Ming Gao Aoying Zhou\nabstract\rabstract: Neural sequence labeling (NSL) aims at assigning labels for input languagetokens, which covers a broad range of applications, such as named entityrecognition (NER) and slot filling, etc. However, the satisfying resultsachieved by traditional supervised-based approaches heavily depend on the largeamounts of human annotation data, which may not be feasible in real-worldscenarios due to data privacy and computation efficiency issues. This paperpresents SeqUST, a novel uncertain-aware self-training framework for NSL toaddress the labeled data scarcity issue and to effectively utilize unlabeleddata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neuralnetwork (BNN) to perform uncertainty estimation at the token level and thenselect reliable language tokens from unlabeled data based on the modelconfidence and certainty. A well-designed masked sequence labeling task with anoise-robust loss supports robust training, which aims to suppress the problemof noisy pseudo labels. In addition, we develop a Gaussian-based consistencyregularization technique to further improve the model robustness onGaussian-distributed perturbed representations. This effectively alleviates theover-fitting dilemma originating from pseudo-labeled augmented data. Extensiveexperiments over six benchmarks demonstrate that our SeqUST frameworkeffectively improves the performance of self-training, and consistentlyoutperforms strong baselines by a large margin in low-resource scenarios\r2023-02-14\nA Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3\nHa-Thanh Nguyen\nabstract\rabstract: LawGPT 1.0 is a virtual legal assistant built on the state-of-the-artlanguage model GPT-3, fine-tuned for the legal domain. The system is designedto provide legal assistance to users in a conversational manner, helping themwith tasks such as answering legal questions, generating legal documents, andproviding legal advice. In this paper, we provide a brief overview of LawGPT1.0, its architecture, and its performance on a set of legal benchmark tasks.Please note that the detailed information about the model is protected by anon-disclosure agreement (NDA) and cannot be disclosed in this report.\r2023-02-13\nThe 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health\nKevin Lybarger Meliha Yetisgen Özlem Uzuner\nabstract\rabstract: Objective: The n2c2/UW SDOH Challenge explores the extraction of socialdeterminant of health (SDOH) information from clinical notes. The objectivesinclude the advancement of natural language processing (NLP) informationextraction techniques for SDOH and clinical information more broadly. Thispaper presents the shared task, data, participating teams, performance results,and considerations for future work. Materials and Methods: The task used the Social History Annotated Corpus(SHAC), which consists of clinical text with detailed event-based annotationsfor SDOH events such as alcohol, drug, tobacco, employment, and livingsituation. Each SDOH event is characterized through attributes related tostatus, extent, and temporality. The task includes three subtasks related toinformation extraction (Subtask A), generalizability (Subtask B), and learningtransfer (Subtask C). In addressing this task, participants utilized a range oftechniques, including rules, knowledge bases, n-grams, word embeddings, andpretrained language models (LM). Results: A total of 15 teams participated, and the top teams utilizedpretrained deep learning LM. The top team across all subtasks used asequence-to-sequence approach achieving 0.901 F1 for Subtask A, 0.774 F1Subtask B, and 0.889 F1 for Subtask C. Conclusions: Similar to many NLP tasks and domains, pretrained LM yielded thebest performance, including generalizability and learning transfer. An erroranalysis indicates extraction performance varies by SDOH, with lowerperformance achieved for conditions, like substance use and homelessness, thatincrease health risks (risk factors) and higher performance achieved forconditions, like substance abstinence and living with family, that reducehealth risks (protective factors).\rTargeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge\nAli Al-Kaswan Maliheh Izadi Arie van Deursen\nabstract\rabstract: Previous work has shown that Large Language Models are susceptible toso-called data extraction attacks. This allows an attacker to extract a samplethat was contained in the training data, which has massive privacyimplications. The construction of data extraction attacks is challenging,current attacks are quite inefficient, and there exists a significant gap inthe extraction capabilities of untargeted attacks and memorization. Thus,targeted attacks are proposed, which identify if a given sample from thetraining data, is extractable from a model. In this work, we apply a targeteddata extraction attack to the SATML2023 Language Model Training Data ExtractionChallenge. We apply a two-step approach. In the first step, we maximise therecall of the model and are able to extract the suffix for 69% of the samples.In the second step, we use a classifier-based Membership Inference Attack onthe generations. Our AutoSklearn classifier achieves a precision of 0.841. Thefull approach reaches a score of 0.405 recall at a 10% false positive rate,which is an improvement of 34% over the baseline of 0.301.\rDataset of Natural Language Queries for E-Commerce\nAndrea Papenmeier Dagmar Kern Daniel Hienert Alfred Sliwa Ahmet Aker Norbert Fuhr\nabstract\rabstract: Shopping online is more and more frequent in our everyday life. Fore-commerce search systems, understanding natural language coming through voiceassistants, chatbots or from conversational search is an essential ability tounderstand what the user really wants. However, evaluation datasets withnatural and detailed information needs of product-seekers which could be usedfor research do not exist. Due to privacy issues and competitive consequences,only few datasets with real user search queries from logs are openly available.In this paper, we present a dataset of 3,540 natural language queries in twodomains that describe what users want when searching for a laptop or a jacketof their choice. The dataset contains annotations of vague terms and key factsof 1,754 laptop queries. This dataset opens up a range of researchopportunities in the fields of natural language processing and (interactive)information retrieval for product search.\r2023-02-12\nChat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models\nPaula Maddigan Teo Susnjak\nabstract\rabstract: The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies.\r2023-02-11\nSynthesizing Human Gaze Feedback for Improved NLP Performance\nVarun Khurana Yaman Kumar Singla Nora Hollenstein Rajesh Kumar Balaji Krishnamurthy\nabstract\rabstract: Integrating human feedback in models can improve the performance of naturallanguage processing (NLP) models. Feedback can be either explicit (e.g. rankingused in training language models) or implicit (e.g. using human cognitivesignals in the form of eyetracking). Prior eye tracking and NLP research revealthat cognitive processes, such as human scanpaths, gleaned from human gazepatterns aid in the understanding and performance of NLP models. However, thecollection of real eyetracking data for NLP tasks is challenging due to therequirement of expensive and precise equipment coupled with privacy invasionissues. To address this challenge, we propose ScanTextGAN, a novel model forgenerating human scanpaths over text. We show that ScanTextGAN-generatedscanpaths can approximate meaningful cognitive signals in human gaze patterns.We include synthetically generated scanpaths in four popular NLP tasks spanningsix different datasets as proof of concept and show that the models augmentedwith generated scanpaths improve the performance of all downstream NLP tasks.\rFormalizing Stack Safety as a Security Property\nSean Noble Anderson Roberto Blanco Leonidas Lampropoulos Benjamin C. Pierce Andrew Tolmach\nabstract\rabstract: The term stack safety is used for a variety of compiler, run-time, andhardware mechanisms for protecting stack memory. Unlike \u0026ldquo;the heap,\u0026rdquo; theISA-level stack does not correspond to a single high-level language concept:different compilers use it in different ways to support procedural andfunctional abstraction mechanisms from a wide range of languages. This proteannature makes it difficult to nail down what it means to correctly enforce stacksafety. We propose a formal characterization of stack safety using concepts fromlanguage-based security. Rather than packaging all aspects of stack safety intoa monolithic property, we decompose it into an integrity property and aconfidentiality property for each of the caller and the callee, plus acontrol-flow property \u0026ndash; five properties in all. This formulation is motivated by a particular class of enforcementmechanisms, the ``lazy\u0026rsquo;\u0026rsquo; stack safety micro-policies studied by Roessler andDeHon~\\cite{DBLP:conf/sp/RoesslerD18}, which permit functions to write into oneanother\u0026rsquo;s frames, but which taint the changed locations so that the frame\u0026rsquo;sowner cannot access it. No existing characterization of stack safety capturesthis style of safety. We capture it here by stating our properties in terms ofthe observable behavior of the system. Our properties go further than previous formal definitions of stack safety,supporting caller- and callee-saved registers, arguments passed on the stack,and tail-call elimination. We validate our properties by using them to distinguish between correct andincorrect implementations of Roessler and DeHon\u0026rsquo;s micro-policies usingproperty-based random testing. Our test harness successfully identifies severalbroken variants, including Roessler and DeHon\u0026rsquo;s lazy policy; a repaired versionof their policy does pass our tests.\r2023-02-10\nBuilding cross-language corpora for human understanding of privacy policies\nFrancesco Ciclosi Silvia Vidor Fabio Massacci\nabstract\rabstract: Making sure that users understand privacy policies that impact them is a keychallenge for a real GDPR deployment. Research studies are mostly carried inEnglish, but in Europe and elsewhere, users speak a language that is notEnglish. Replicating studies in different languages requires the availabilityof comparable cross-language privacy policies corpora. This work provides amethodology for building comparable cross-language in a national language and areference study language. We provide an application example of our methodologycomparing English and Italian extending the corpus of one of the first studiesabout users understanding of technical terms in privacy policies. We alsoinvestigate other open issues that can make replication harder.\rWatermarking Pre-trained Language Models with Backdooring\nChenxi Gu Chengsong Huang Xiaoqing Zheng Kai-Wei Chang Cho-Jui Hsieh\nabstract\rabstract: Large pre-trained language models (PLMs) have proven to be a crucialcomponent of modern natural language processing systems. PLMs typically need tobe fine-tuned on task-specific downstream datasets, which makes it hard toclaim the ownership of PLMs and protect the developer\u0026rsquo;s intellectual propertydue to the catastrophic forgetting phenomenon. We show that PLMs can bewatermarked with a multi-task learning framework by embedding backdoorstriggered by specific inputs defined by the owners, and those watermarks arehard to remove even though the watermarked PLMs are fine-tuned on multipledownstream tasks. In addition to using some rare words as triggers, we alsoshow that the combination of common words can be used as backdoor triggers toavoid them being easily detected. Extensive experiments on multiple datasetsdemonstrate that the embedded watermarks can be robustly extracted with a highsuccess rate and less influenced by the follow-up fine-tuning.\r2023-02-09\nOffsite-Tuning: Transfer Learning without Full Model\nGuangxuan Xiao Ji Lin Song Han\nabstract\rabstract: Transfer learning is important for foundation models to adapt to downstreamtasks. However, many foundation models are proprietary, so users must sharetheir data with model owners to fine-tune the models, which is costly and raiseprivacy concerns. Moreover, fine-tuning large foundation models iscomputation-intensive and impractical for most downstream users. In this paper,we propose Offsite-Tuning, a privacy-preserving and efficient transfer learningframework that can adapt billion-parameter foundation models to downstream datawithout access to the full model. In offsite-tuning, the model owner sends alight-weight adapter and a lossy compressed emulator to the data owner, whothen fine-tunes the adapter on the downstream data with the emulator\u0026rsquo;sassistance. The fine-tuned adapter is then returned to the model owner, whoplugs it into the full model to create an adapted foundation model.Offsite-tuning preserves both parties\u0026rsquo; privacy and is computationally moreefficient than the existing fine-tuning methods that require access to the fullmodel weights. We demonstrate the effectiveness of offsite-tuning on variouslarge language and vision foundation models. Offsite-tuning can achievecomparable accuracy as full model fine-tuning while being privacy-preservingand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code isavailable at https://github.com/mit-han-lab/offsite-tuning.\r2023-02-01\nDeveloping Hands-on Labs for Source Code Vulnerability Detection with AI\nMaryam Taeb\nabstract\rabstract: As the role of information and communication technologies gradually increasesin our lives, source code security becomes a significant issue to protectagainst malicious attempts Furthermore with the advent of data-driventechniques, there is now a growing interest in leveraging machine learning andnatural language processing as a source code assurance method to buildtrustworthy systems Therefore training our future software developers to writesecure source code is in high demand In this thesis we propose a frameworkincluding learning modules and hands on labs to guide future IT professionalstowards developing secure programming habits and mitigating source codevulnerabilities at the early stages of the software development lifecycle Inthis thesis our goal is to design learning modules with a set of hands on labsthat will introduce students to secure programming practices using source codeand log file analysis tools to predict and identify vulnerabilities In a SecureCoding Education framework we will improve students skills and awareness onsource code vulnerabilities detection tools and mitigation techniques integrateconcepts of source code vulnerabilities from Function API and library level tobad programming habits and practices leverage deep learning NLP and staticanalysis tools for log file analysis to introduce the root cause of source codevulnerabilities\rBunched Fuzz: Sensitivity for Vector Metrics\njune wunder Arthur Azevedo de Amorim Patrick Baillot Marco Gaboardi\nabstract\rabstract: Program sensitivity measures the distance between the outputs of a programwhen run on two related inputs. This notion, which plays a key role in areassuch as data privacy and optimization, has been the focus of several programanalysis techniques introduced in recent years. Among the most successful ones,we can highlight type systems inspired by linear logic, as pioneered by Reedand Pierce in the Fuzz programming language. In Fuzz, each type is equippedwith its own distance, and sensitivity analysis boils down to type checking. Inparticular, Fuzz features two product types, corresponding to two differentnotions of distance: the tensor product combines the distances of eachcomponent by adding them, while the with product takes their maximum. In this work, we show that these products can be generalized to arbitrary$L^p$ distances, metrics that are often used in privacy and optimization. Theoriginal Fuzz products, tensor and with, correspond to the special cases $L^1$and $L^\\infty$. To ease the handling of such products, we extend the Fuzz typesystem with bunches \u0026ndash; as in the logic of bunched implications \u0026ndash; where thedistances of different groups of variables can be combined using different$L^p$ distances. We show that our extension can be used to reason aboutquantitative properties of probabilistic programs.\r2023-01-31\nSTI: Turbocharge NLP Inference at the Edge via Elastic Pipelining\nLiwei Guo Wonkyo Choe Felix Xiaozhu Lin\nabstract\rabstract: Natural Language Processing (NLP) inference is seeing increasing adoption bymobile applications, where on-device inference is desirable for cruciallypreserving user data privacy and avoiding network roundtrips. Yet, theunprecedented size of an NLP model stresses both latency and memory, creating atension between the two key resources of a mobile device. To meet a targetlatency, holding the whole model in memory launches execution as soon aspossible but increases one app\u0026rsquo;s memory footprints by several times, limitingits benefits to only a few inferences before being recycled by mobile memorymanagement. On the other hand, loading the model from storage on demand incursIO as long as a few seconds, far exceeding the delay range satisfying to auser; pipelining layerwise model loading and execution does not hide IO either,due to the high skewness between IO and computation delays. To this end, we propose Speedy Transformer Inference (STI). Built on the keyidea of maximizing IO/compute resource utilization on the most important partsof a model, STI reconciles the latency v.s. memory tension via two noveltechniques. First, model sharding. STI manages model parameters asindependently tunable shards, and profiles their importance to accuracy.Second, elastic pipeline planning with a preload buffer. STI instantiates anIO/compute pipeline and uses a small buffer for preload shards to bootstrapexecution without stalling at early stages; it judiciously selects, tunes, andassembles shards per their importance for resource-elastic execution,maximizing inference accuracy. Atop two commodity SoCs, we build STI and evaluate it against a wide range ofNLP tasks, under a practical range of target latencies, and on both CPU andGPU. We demonstrate that STI delivers high accuracies with 1-2 orders ofmagnitude lower memory, outperforming competitive baselines.\r2023-01-30\nAutomating the Generation of Cyber Range Virtual Scenarios with VSDL\nGabriele Costa Enrico Russo Alessandro Armando\nabstract\rabstract: A cyber range is an environment used for training security experts andtesting attack and defence tools and procedures. Usually, a cyber rangesimulates one or more critical infrastructures that attacking (red) anddefending (blue) teams must compromise and protect, respectively. Theinfrastructure can be physically assembled, but much more convenient is to relyon the Infrastructure as a Service (IaaS) paradigm. Although some moderntechnologies support the IaaS, the design and deployment of scenarios ofinterest is mostly a manual operation. As a consequence, it is a commonpractice to have a cyber range hosting few (sometimes only one), consolidatedscenarios. However, reusing the same scenario may significantly reduce theeffectiveness of the training and testing sessions. In this paper, we propose aframework for automating the definition and deployment of arbitrarily complexcyber range scenarios. The framework relies on the virtual scenario descriptionlanguage (VSDL), i.e., a domain-specific language for defining high-levelfeatures of the desired infrastructure while hiding low-level details. Thesemantics of VSDL is given in terms of constraints that must be satisfied bythe virtual infrastructure. These constraints are then submitted to an SMTsolver for checking the satisfiability of the specification. If satisfiable,the specification gives rise to a model that is automatically converted to aset of deployment scripts to be submitted to the IaaS provider.\r2023-01-28\nContext-Aware Differential Privacy for Language Modeling\nMy H. Dinh Ferdinando Fioretto\nabstract\rabstract: The remarkable ability of language models (LMs) has also brought challengesat the interface of AI and security. A critical challenge pertains to how muchinformation these models retain and leak about the training data. This isparticularly urgent as the typical development of LMs relies on huge, oftenhighly sensitive data, such as emails and chat logs. To contrast thisshortcoming, this paper introduces Context-Aware Differentially PrivateLanguage Model (CADP-LM) , a privacy-preserving LM framework that relies on twokey insights: First, it utilizes the notion of \\emph{context} to define andaudit the potentially sensitive information. Second, it adopts the notion ofDifferential Privacy to protect sensitive information and characterize theprivacy leakage. A unique characteristic of CADP-LM is its ability to targetthe protection of sensitive sentences and contexts only, providing a highlyaccurate private model. Experiments on a variety of datasets and settingsdemonstrate these strengths of CADP-LM.\r2023-01-27\nDown the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech\nJarod Govers Philip Feldman Aaron Dant Panos Patros\nabstract\rabstract: Social media is a modern person\u0026rsquo;s digital voice to project and engage withnew ideas and mobilise communities $\\unicode{x2013}$ a power shared withextremists. Given the societal risks of unvetted content-moderating algorithmsfor Extremism, Radicalisation, and Hate speech (ERH) detection, responsiblesoftware engineering must understand the who, what, when, where, and why suchmodels are necessary to protect user safety and free expression. Hence, wepropose and examine the unique research field of ERH context mining to unifydisjoint studies. Specifically, we evaluate the start-to-finish design processfrom socio-technical definition-building and dataset collection strategies totechnical algorithm design and performance. Our 2015-2021 51-study SystematicLiterature Review (SLR) provides the first cross-examination of textual,network, and visual approaches to detecting extremist affiliation, hatefulcontent, and radicalisation towards groups and movements. We identifyconsensus-driven ERH definitions and propose solutions to existing ideologicaland geographic biases, particularly due to the lack of research inOceania/Australasia. Our hybridised investigation on Natural LanguageProcessing, Community Detection, and visual-text models demonstrates thedominating performance of textual transformer-based algorithms. We concludewith vital recommendations for ERH context mining researchers and propose anuptake roadmap with guidelines for researchers, industries, and governments toenable a safer cyberspace.\r2023-01-26\nCommunication-Efficient Learning of Deep Networks from Decentralized Data\nH. Brendan McMahan Eider Moore Daniel Ramage Seth Hampson Blaise Agüera y Arcas\nabstract\rabstract: Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksbased on iterative model averaging, and conduct an extensive empiricalevaluation, considering five different model architectures and four datasets.These experiments demonstrate the approach is robust to the unbalanced andnon-IID data distributions that are a defining characteristic of this setting.Communication costs are the principal constraint, and we show a reduction inrequired communication rounds by 10-100x as compared to synchronized stochasticgradient descent.\rJoint data rate and EMF exposure analysis in Manhattan environments: stochastic geometry and ray tracing approaches\nCharles Wiame Simon Demey Luc Vandendorpe Philippe De Doncker Claude Oestges\nabstract\rabstract: The objective of this study is to jointly analyze the data rate andelectromagnetic field (EMF) exposure in urban environments. Capitalizing onstochastic geometry (SG), a network level analysis is performed by modellingthese environments via Manhattan Poisson line processes (MPLP). Using thisframework, a number of performance metrics are derived: first moments, marginaldistributions and joint distributions of the data rate and exposure. Inaddition, the original Manhattan model is generalized to include advancedfeatures: corner diffraction, presence of potential blockages in streets, andusers positioned at crossroads. As a second approach, deterministic ray tracing(RT) is utilized to compute the same metrics. The two methods are shown toprovide close results on the condition that the model parameters are coherentlyselected. Furthermore, the numerical results enable to gain insight intoseveral aspects: the role of the propagation mechanisms in the performancemetrics, existing trade-offs between the rate and exposure requirements, aswell as the impact of the user location (at a crossroad or in a single street). (This work has been submitted to the IEEE for possible publication. Copyrightmay be transferred without notice, after which this version may no longer beaccessible)\r2023-01-24\nFedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning\nHaodong Zhao Wei Du Fangqi Li Peixuan Li Gongshen Liu\nabstract\rabstract: Federated learning (FL) has enabled global model training on decentralizeddata in a privacy-preserving way by aggregating model updates. However, formany natural language processing (NLP) tasks that utilize pre-trained languagemodels (PLMs) with large numbers of parameters, there are considerablecommunication costs associated with FL. Recently, prompt tuning, which tunessome soft prompts without modifying PLMs, has achieved excellent performance asa new learning paradigm. Therefore we want to combine the two methods andexplore the effect of prompt tuning under FL. In this paper, we propose\u0026quot;FedPrompt\u0026quot; to study prompt tuning in a model split aggregation way using FL,and prove that split aggregation greatly reduces the communication cost, only0.01% of the PLMs\u0026rsquo; parameters, with little decrease on accuracy both on IID andNon-IID data distribution. This improves the efficiency of FL method while alsoprotecting the data privacy in prompt tuning. In addition, like PLMs, promptsare uploaded and downloaded between public platforms and personal users, so wetry to figure out whether there is still a backdoor threat using only softprompts in FL scenarios. We further conduct backdoor attacks by data poisoningon FedPrompt. Our experiments show that normal backdoor attack can not achievea high attack success rate, proving the robustness of FedPrompt. We hope thiswork can promote the application of prompt in FL and raise the awareness of thepossible security threats.\r2023-01-21\nBlacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models\nAnoop Kadan Deepak P. Sahely Bhadra Manjary P. Gangan Lajish V. L\nabstract\rabstract: Groundbreaking inventions and highly significant performance improvements indeep learning based Natural Language Processing are witnessed through thedevelopment of transformer based large Pre-trained Language Models (PLMs). Thewide availability of unlabeled data within human generated data deluge alongwith self-supervised learning strategy helps to accelerate the success of largePLMs in language generation, language understanding, etc. But at the same time,latent historical bias/unfairness in human minds towards a particular gender,race, etc., encoded unintentionally/intentionally into the corpora harms andquestions the utility and efficacy of large PLMs in many real-worldapplications, particularly for the protected groups. In this paper, we presentan extensive investigation towards understanding the existence of \u0026ldquo;AffectiveBias\u0026rdquo; in large PLMs to unveil any biased association of emotions such as anger,fear, joy, etc., towards a particular gender, race or religion with respect tothe downstream task of textual emotion detection. We conduct our exploration ofaffective bias from the very initial stage of corpus level affective biasanalysis by searching for imbalanced distribution of affective words within adomain, in large scale corpora that are used to pre-train and fine-tune PLMs.Later, to quantify affective bias in model predictions, we perform an extensiveset of class-based and intensity-based evaluations using various biasevaluation corpora. Our results show the existence of statistically significantaffective bias in the PLM based emotion detection systems, indicating biasedassociation of certain emotions towards a particular gender, race, andreligion.\rMetaSys: A Practical Open-Source Metadata Management System to Implement and Evaluate Cross-Layer Optimizations\nNandita Vijaykumar Ataberk Olgun Konstantinos Kanellopoulos Nisa Bostancı Hasan Hassan Mehrshad Lotfi Phillip B. Gibbons Onur Mutlu\nabstract\rabstract: This paper introduces the first open-source FPGA-based infrastructure,MetaSys, with a prototype in a RISC-V core, to enable the rapid implementationand evaluation of a wide range of cross-layer techniques in real hardware.Hardware-software cooperative techniques are powerful approaches to improve theperformance, quality of service, and security of general-purpose processors.They are however typically challenging to rapidly implement and evaluate inreal hardware as they require full-stack changes to the hardware, OS, systemsoftware, and instruction-set architecture (ISA). MetaSys implements a rich hardware-software interface and lightweightmetadata support that can be used as a common basis to rapidly implement andevaluate new cross-layer techniques. We demonstrate MetaSys\u0026rsquo;s versatility andease-of-use by implementing and evaluating three cross-layer techniques for:(i) prefetching for graph analytics; (ii) bounds checking in memory unsafelanguages, and (iii) return address protection in stack frames; each techniqueonly requiring ~100 lines of Chisel code over MetaSys. Using MetaSys, we perform the first detailed experimental study to quantifythe performance overheads of using a single metadata management system toenable multiple cross-layer optimizations in CPUs. We identify the key sourcesof bottlenecks and system inefficiency of a general metadata management system.We design MetaSys to minimize these inefficiencies and provide increasedversatility compared to previously-proposed metadata systems. Using three usecases and a detailed characterization, we demonstrate that a common metadatamanagement system can be used to efficiently support diverse cross-layertechniques in CPUs.\r2023-01-19\nA Privacy Glossary for Cloud Computing\nTian Wang Masooda Bashir\nabstract\rabstract: Cloud computing is an evolving paradigm that is frequently changing the wayhumans share, store, and access their information in digital format. Whilecloud computing offers tremendous benefits (e.g., efficiency, flexibility, andreduced costs), it also brings both security and privacy challenges. Althoughcloud security has been extensively defined and developed, privacy protectionsin cloud environments are often described in abstract or vague language, whichmakes it difficult to interpret and implement. In this study, we propose aninitial approach of developing a privacy glossary for cloud computing thatprovides a consistent and comprehensive set of terminologies for cloud privacy.We believe that this systematic and structured privacy glossary could serve asa first step towards implementing requirements for privacy protections in cloudcomputing, as well as providing more effective and consistent language in cloudprivacy to researchers and professionals in the future.\r2023-01-17\nCommand Line Interface Risk Modeling\nDr Anthony L. Faulds\nabstract\rabstract: Protecting sensitive data is an essential part of security in cloudcomputing. However, only specific privileged individuals have access to view orinteract with this data; therefore, it is unscalable to depend on theseindividuals also to maintain the software. A solution to this is to allownon-privileged individuals access to maintain these systems but mask sensitiveinformation from egressing. To this end, we have created a machine-learningmodel to predict and redact fields with sensitive data. This work concentrateson Azure PowerShell, showing how it applies to other command-line interfacesand APIs. Using the F5-score as a weighted metric, we demonstrate differenttransformation techniques to map this problem from an unknown field to thewell-researched area of natural language processing.\r2023-01-13\nMuch Ado About Gender: Current Practices and Future Recommendations for Appropriate Gender-Aware Information Access\nChristine Pinney Amifa Raj Alex Hanna Michael D. Ekstrand\nabstract\rabstract: Information access research (and development) sometimes makes use of gender,whether to report on the demographics of participants in a user study, asinputs to personalized results or recommendations, or to make systemsgender-fair, amongst other purposes. This work makes a variety of assumptionsabout gender, however, that are not necessarily aligned with currentunderstandings of what gender is, how it should be encoded, and how a gendervariable should be ethically used. In this work, we present a systematic reviewof papers on information retrieval and recommender systems that mention genderin order to document how gender is currently being used in this field. We findthat most papers mentioning gender do not use an explicit gender variable, butmost of those that do either focus on contextualizing results of modelperformance, personalizing a system based on assumptions of user gender, orauditing a model\u0026rsquo;s behavior for fairness or other privacy-related issues.Moreover, most of the papers we review rely on a binary notion of gender, evenif they acknowledge that gender cannot be split into two categories. We connectthese findings with scholarship on gender theory and recent work on gender inhuman-computer interaction and natural language processing. We conclude bymaking recommendations for ethical and well-grounded use of gender in buildingand researching information access systems.\r2023-01-10\nUser-Centered Security in Natural Language Processing\nChris Emmery\nabstract\rabstract: This dissertation proposes a framework of user-centered security in NaturalLanguage Processing (NLP), and demonstrates how it can improve theaccessibility of related research. Accordingly, it focuses on two securitydomains within NLP with great public interest. First, that of author profiling,which can be employed to compromise online privacy through invasive inferences.Without access and detailed insight into these models\u0026rsquo; predictions, there is noreasonable heuristic by which Internet users might defend themselves from suchinferences. Secondly, that of cyberbullying detection, which by defaultpresupposes a centralized implementation; i.e., content moderation acrosssocial platforms. As access to appropriate data is restricted, and the natureof the task rapidly evolves (both through lexical variation, and culturalshifts), the effectiveness of its classifiers is greatly diminished and therebyoften misrepresented. Under the proposed framework, we predominantly investigate the use ofadversarial attacks on language; i.e., changing a given input (generatingadversarial samples) such that a given model does not function as intended.These attacks form a common thread between our user-centered security problems;they are highly relevant for privacy-preserving obfuscation methods againstauthor profiling, and adversarial samples might also prove useful to assess theinfluence of lexical variation and augmentation on cyberbullying detection.\rGPU-based high-precision orbital propagation of large sets of initial conditions through Picard-Chebyshev augmentation\nAlessandro Masat Camilla Colombo Arnaud Boutonnet\nabstract\rabstract: The orbital propagation of large sets of initial conditions under highaccuracy requirements is currently a bottleneck in the development of spacemissions, e.g. for planetary protection compliance analyses. The proposedapproach can include any force source in the dynamical model through efficientPicard-Chebyshev (PC) numerical simulations. A two-level augmentation of theintegration scheme is proposed, to run an arbitrary number of simulationswithin the same algorithm call, fully exploiting high performance and GPU(Graphics Processing Units) computing facilities. The performances obtainedwith implementation in C and NVIDIA CUDA programming languages are shown, on atest case taken from the optimization of a Solar Orbiter-like first resonantphase with Venus.\r2023-01-05\nWhat is in a Text-to-Image Prompt: The Potential of Stable Diffusion in Visual Arts Education\nNassim Dehouche Kullathida Dehouche\nabstract\rabstract: Text-to-Image artificial intelligence (AI) recently saw a major breakthroughwith the release of Dall-E and its open-source counterpart, Stable Diffusion.These programs allow anyone to create original visual art pieces by simplyproviding descriptions in natural language (prompts). Using a sample of 72,980Stable Diffusion prompts, we propose a formalization of this new medium of artcreation and assess its potential for teaching the history of art, aesthetics,and technique. Our findings indicate that text-to-Image AI has the potential torevolutionize the way art is taught, offering new, cost-effective possibilitiesfor experimentation and expression. However, it also raises important questionsabout the ownership of artistic works. As more and more art is created usingthese programs, it will be crucial to establish new legal and economic modelsto protect the rights of artists.\r2022-12-29\n$π$QLB: A Privacy-preserving with Integrity-assuring Query Language for Blockchain\nNasrin Sohrabi Norrathep Rattanavipanon Zahir Tari\nabstract\rabstract: The increase in the adoption of blockchain technology in differentapplication domains e.g., healthcare systems, supplychain management, hasraised the demand for a data query mechanism on blockchain. Since currentblockchain systems lack the support for querying data with embedded securityand privacy guarantees, there exists inherent security and privacy concerns onthose systems. In particular, existing systems require users to submit queriesto blockchain operators (e.g., a node validator) in plaintext. This directlyjeopardizes users\u0026rsquo; privacy as the submitted queries may contain sensitiveinformation, e.g., location or gender preferences, that the users may not becomfortable sharing. On the other hand, currently, the only way for users toensure integrity of the query result is to maintain the entire blockchaindatabase and perform the queries locally. Doing so incurs high storage andcomputational costs on the users, precluding this approach to be practicallydeployable on common light-weight devices (e.g., smartphones). To this end,this paper proposes $\\pi$QLB, a query language for blockchain systems thatensures both confidentiality of query inputs and integrity of query results.Additionally, $\\pi$QLB enables SQL-like queries over the blockchain data byintroducing relational data semantics into the existing blockchain database.$\\pi$QLB has applied the recent cryptography primitive, i.e., function secretsharing (FSS), to achieve confidentiality. To support integrity, we extend thetraditional FSS setting in such a way that integrity of FSS results can beefficiently verified. Successful verification indicates absence of maliciousbehaviors on the servers, allowing the user to establish trust from the result.To the best of our knowledge, $\\pi$QLB is the first query model designed forblockchain databases with support for confidentiality, integrity, and SQL-likequeries.\r2022-12-22\nAttribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings\nTiantian Feng Hanieh Hashemi Rajat Hebbar Murali Annavaram Shrikanth S. Narayanan\nabstract\rabstract: Speech emotion recognition (SER) processes speech signals to detect andcharacterize expressed perceived emotions. Many SER application systems oftenacquire and transmit speech data collected at the client-side to remote cloudplatforms for inference and decision making. However, speech data carry richinformation not only about emotions conveyed in vocal expressions, but alsoother sensitive demographic traits such as gender, age and language background.Consequently, it is desirable for SER systems to have the ability to classifyemotion constructs while preventing unintended/improper inferences of sensitiveand demographic information. Federated learning (FL) is a distributed machinelearning paradigm that coordinates clients to train a model collaborativelywithout sharing their local data. This training approach appears secure and canimprove privacy for SER. However, recent works have demonstrated that FLapproaches are still vulnerable to various privacy attacks like reconstructionattacks and membership inference attacks. Although most of these have focusedon computer vision applications, such information leakages exist in the SERsystems trained using the FL technique. To assess the information leakage ofSER systems trained using FL, we propose an attribute inference attackframework that infers sensitive attribute information of the clients fromshared gradients or model parameters, corresponding to the FedSGD and theFedAvg training algorithms, respectively. As a use case, we empiricallyevaluate our approach for predicting the client\u0026rsquo;s gender information usingthree SER benchmark datasets: IEMOCAP, CREMA-D, and MSP-Improv. We show thatthe attribute inference attack is achievable for SER systems trained using FL.We further identify that most information leakage possibly comes from the firstlayer in the SER model.\rInvBERT: Reconstructing Text from Contextualized Word Embeddings by inverting the BERT pipeline\nKai Kugler Simon Münker Johannes Höhmann Achim Rettinger\nabstract\rabstract: Digital Humanities and Computational Literary Studies apply text miningmethods to investigate literature. Such automated approaches enablequantitative studies on large corpora which would not be feasible by manualinspection alone. However, due to copyright restrictions, the availability ofrelevant digitized literary works is limited. Derived Text Formats (DTFs) havebeen proposed as a solution. Here, textual materials are transformed in such away that copyright-critical features are removed, but that the use of certainanalytical methods remains possible. Contextualized word embeddings produced bytransformer-encoders (like BERT) are promising candidates for DTFs because theyallow for state-of-the-art performance on various analytical tasks and, atfirst sight, do not disclose the original text. However, in this paper wedemonstrate that under certain conditions the reconstruction of the originalcopyrighted text becomes feasible and its publication in the form ofcontextualized token representations is not safe. Our attempts to invert BERTsuggest, that publishing the encoder as a black box together with thecontextualized embeddings is critical, since it allows to generate data totrain a decoder with a reconstruction accuracy sufficient to violate copyrightlaws.\r2022-12-21\nIs Your Model Sensitive? SPeDaC: A New Benchmark for Detecting and Classifying Sensitive Personal Data\nGaia Gambarelli Aldo Gangemi Rocco Tripodi\nabstract\rabstract: In recent years, there has been an exponential growth of applications,including dialogue systems, that handle sensitive personal information. Thishas brought to light the extremely important issue of personal data protectionin virtual environments. Sensitive Information Detection (SID) approachesdifferent domains and languages in literature. However, if we refer to thepersonal data domain, a shared benchmark or the absence of an available labeledresource makes comparison with the state-of-the-art difficult. We introduce andrelease SPeDaC , a new annotated resource for the identification of sensitivepersonal data categories in the English language. SPeDaC enables the evaluationof computational models for three different SID subtasks with increasing levelsof complexity. SPeDaC 1 regards binary classification, a model has to detect ifa sentence contains sensitive information or not; whereas, in SPeDaC 2 wecollected labeled sentences using 5 categories that relate to macro-domains ofpersonal information; in SPeDaC 3, the labeling is fine-grained (61 personaldata categories). We conduct an extensive evaluation of the resource usingdifferent state-of-the-art-classifiers. The results show that SPeDaC ischallenging, particularly with regard to fine-grained classification. Thetransformer models achieve the best results (acc. RoBERTa on SPeDaC 1 = 98.20%,DeBERTa on SPeDaC 2 = 95.81% and SPeDaC 3 = 77.63%).\r2022-12-20\nDeduplicating Training Data Mitigates Privacy Risks in Language Models\nNikhil Kandpal Eric Wallace Colin Raffel\nabstract\rabstract: Past work has shown that large language models are susceptible to privacyattacks, where adversaries generate sequences from a trained model and detectwhich sequences are memorized from the training set. In this work, we show thatthe success of these attacks is largely due to duplication in commonly usedweb-scraped training sets. We first show that the rate at which language modelsregenerate training sequences is superlinearly related to a sequence\u0026rsquo;s count inthe training set. For instance, a sequence that is present 10 times in thetraining data is on average generated ~1000 times more often than a sequencethat is present only once. We next show that existing methods for detectingmemorized sequences have near-chance accuracy on non-duplicated trainingsequences. Finally, we find that after applying methods to deduplicate trainingdata, language models are considerably more secure against these types ofprivacy attacks. Taken together, our results motivate an increased focus ondeduplication in privacy-sensitive applications and a reevaluation of thepracticality of existing privacy attacks.\r2022-12-19\nPrivacy Adhering Machine Un-learning in NLP\nVinayshekhar Bannihatti Kumar Rashmi Gangadharaiah Dan Roth\nabstract\rabstract: Regulations introduced by General Data Protection Regulation (GDPR) in the EUor California Consumer Privacy Act (CCPA) in the US have included provisions onthe \\textit{right to be forgotten} that mandates industry applications toremove data related to an individual from their systems. In several real worldindustry applications that use Machine Learning to build models on user data,such mandates require significant effort both in terms of data cleansing aswell as model retraining while ensuring the models do not deteriorate inprediction quality due to removal of data. As a result, continuous removal ofdata and model retraining steps do not scale if these applications receive suchrequests at a very high frequency. Recently, a few researchers proposed theidea of \\textit{Machine Unlearning} to tackle this challenge. Despite thesignificant importance of this task, the area of Machine Unlearning isunder-explored in Natural Language Processing (NLP) tasks. In this paper, weexplore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as,QQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC andSISA-A) to perform \\textit{guaranteed} Unlearning that provides significantreduction in terms of both memory (90-95%), time (100x) and space consumption(99%) in comparison to the baselines while keeping model performance constant.\rKnowledge Unlearning for Mitigating Privacy Risks in Language Models\nJoel Jang Dongkeun Yoon Sohee Yang Sungmin Cha Moontae Lee Lajanugen Logeswaran Minjoon Seo\nabstract\rabstract: Pretrained Language Models (LMs) memorize a vast amount of knowledge duringinitial pretraining, including information that may violate the privacy ofpersonal lives and identities. Previous work addressing privacy issues forlanguage models has mostly focused on data preprocessing and differentialprivacy methods, both requiring re-training the underlying LM. We proposeknowledge unlearning as an alternative method to reduce privacy risks for LMspost hoc. We show that simply performing gradient ascent on target tokensequences is effective at forgetting them with little to no degradation ofgeneral language modeling performances for larger LMs; it sometimes evensubstantially improves the underlying LM with just a few iterations. We alsofind that sequential unlearning is better than trying to unlearn all the dataat once and that unlearning is highly dependent on which kind of data (domain)is forgotten. By showing comparisons with a previous data preprocessing methodand a decoding method known to mitigate privacy risks for LMs, we show thatunlearning can give a stronger empirical privacy guarantee in scenarios wherethe data vulnerable to extraction attacks are known a priori while being muchmore efficient and robust. We release the code and dataset needed to replicateour results at https://github.com/joeljang/knowledge-unlearning.\rReview of security techniques for memristor computing systems\nMinhui Zou Nan Du Shahar Kvatinsky\nabstract\rabstract: Neural network (NN) algorithms have become the dominant tool in visual objectrecognition, natural language processing, and robotics. To enhance thecomputational efficiency of these algorithms, in comparison to the traditionalvon Neuman computing architectures, researchers have been focusing on memristorcomputing systems. A major drawback when using memristor computing systemstoday is that, in the artificial intelligence (AI) era, well-trained NN modelsare intellectual property and, when loaded in the memristor computing systems,face theft threats, especially when running in edge devices. An adversary maysteal the well-trained NN models through advanced attacks such as learningattacks and side-channel analysis. In this paper, we review different securitytechniques for protecting memristor computing systems. Two threat models aredescribed based on their assumptions regarding the adversary\u0026rsquo;s capabilities: ablack-box (BB) model and a white-box (WB) model. We categorize the existingsecurity techniques into five classes in the context of these threat models:thwarting learning attacks (BB), thwarting side-channel attacks (BB), NN modelencryption (WB), NN weight transformation (WB), and fingerprint embedding (WB).We also present a cross-comparison of the limitations of the securitytechniques. This paper could serve as an aid when designing secure memristorcomputing systems.\r2022-12-16\nPlanting and Mitigating Memorized Content in Predictive-Text Language Models\nC. M. Downey Wei Dai Huseyin A. Inan Kim Laine Saurabh Naik Tomasz Religa\nabstract\rabstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both \u0026ldquo;heuristic\u0026rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define\u0026quot;sensitive\u0026quot; or \u0026ldquo;private\u0026rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.\rDense Feature Memory Augmented Transformers for COVID-19 Vaccination Search Classification\nJai Gupta Yi Tay Chaitanya Kamath Vinh Q. Tran Donald Metzler Shailesh Bavadekar Mimi Sun Evgeniy Gabrilovich\nabstract\rabstract: With the devastating outbreak of COVID-19, vaccines are one of the cruciallines of defense against mass infection in this global pandemic. Given theprotection they provide, vaccines are becoming mandatory in certain social andprofessional settings. This paper presents a classification model for detectingCOVID-19 vaccination related search queries, a machine learning model that isused to generate search insights for COVID-19 vaccinations. The proposed methodcombines and leverages advancements from modern state-of-the-art (SOTA) naturallanguage understanding (NLU) techniques such as pretrained Transformers withtraditional dense features. We propose a novel approach of considering densefeatures as memory tokens that the model can attend to. We show that this newmodeling approach enables a significant improvement to the Vaccine SearchInsights (VSI) task, improving a strong well-established gradient-boostingbaseline by relative +15% improvement in F1 score and +14% in precision.\rFewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks\nWeilong Dong Xinwei Wu Junzhuo Li Shuangzhi Wu Chao Bian Deyi Xiong\nabstract\rabstract: Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods.\r2022-12-13\nExploring Consequences of Privacy Policies with Narrative Generation via Answer Set Programming\nChinmaya Dabral Emma Tosch Chris Martens\nabstract\rabstract: Informed consent has become increasingly salient for data privacy and itsregulation. Entities from governments to for-profit companies have addressedconcerns about data privacy with policies that enumerate the conditions forpersonal data storage and transfer. However, increased enumeration of andtransparency in data privacy policies has not improved end-users\u0026rsquo; comprehensionof how their data might be used: not only are privacy policies written in legallanguage that users may struggle to understand, but elements of these policiesmay compose in such a way that the consequences of the policy are notimmediately apparent. We present a framework that uses Answer Set Programming (ASP) \u0026ndash; a type oflogic programming \u0026ndash; to formalize privacy policies. Privacy policies thusbecome constraints on a narrative planning space, allowing end-users toforward-simulate possible consequences of the policy in terms of actors havingroles and taking actions in a domain. We demonstrate through the example of theHealth Insurance Portability and Accountability Act (HIPAA) how to use thesystem in various ways, including asking questions about possibilities andidentifying which clauses of the law are broken by a given sequence of events.\rFNDaaS: Content-agnostic Detection of Fake News sites\nPanagiotis Papadopoulos Dimitris Spithouris Evangelos P. Markatos Nicolas Kourtellis\nabstract\rabstract: Automatic fake news detection is a challenging problem in misinformationspreading, and it has tremendous real-world political and social impacts. Paststudies have proposed machine learning-based methods for detecting such fakenews, focusing on different properties of the published news articles, such aslinguistic characteristics of the actual content, which however havelimitations due to the apparent language barriers. Departing from such efforts,we propose FNDaaS, the first automatic, content-agnostic fake news detectionmethod, that considers new and unstudied features such as network andstructural characteristics per news website. This method can be enforcedas-a-Service, either at the ISP-side for easier scalability and maintenance, oruser-side for better end-user privacy. We demonstrate the efficacy of ourmethod using data crawled from existing lists of 637 fake and 1183 real newswebsites, and by building and testing a proof of concept system thatmaterializes our proposal. Our analysis of data collected from these websitesshows that the vast majority of fake news domains are very young and appear tohave lower time periods of an IP associated with their domain than real newsones. By conducting various experiments with machine learning classifiers, wedemonstrate that FNDaaS can achieve an AUC score of up to 0.967 on past sites,and up to 77-92% accuracy on newly-flagged ones.\r2022-12-12\nEnabling All In-Edge Deep Learning: A Literature Review\nPraveen Joshi Mohammed Hasanuzzaman Chandra Thapa Haithem Afli Ted Scully\nabstract\rabstract: In recent years, deep learning (DL) models have demonstrated remarkableachievements on non-trivial tasks such as speech recognition and naturallanguage understanding. One of the significant contributors to its success isthe proliferation of end devices that acted as a catalyst to provide data fordata-hungry DL models. However, computing DL training and inference is the mainchallenge. Usually, central cloud servers are used for the computation, but itopens up other significant challenges, such as high latency, increasedcommunication costs, and privacy concerns. To mitigate these drawbacks,considerable efforts have been made to push the processing of DL models to edgeservers. Moreover, the confluence point of DL and edge has given rise to edgeintelligence (EI). This survey paper focuses primarily on the fifth level ofEI, called all in-edge level, where DL training and inference (deployment) areperformed solely by edge servers. All in-edge is suitable when the end deviceshave low computing resources, e.g., Internet-of-Things, and other requirementssuch as latency and communication cost are important in mission-criticalapplications, e.g., health care. Firstly, this paper presents all in-edgecomputing architectures, including centralized, decentralized, and distributed.Secondly, this paper presents enabling technologies, such as model parallelismand split learning, which facilitate DL training and deployment at edgeservers. Thirdly, model adaptation techniques based on model compression andconditional computation are described because the standard cloud-based DLdeployment cannot be directly applied to all in-edge due to its limitedcomputational resources. Fourthly, this paper discusses eleven key performancemetrics to evaluate the performance of DL at all in-edge efficiently. Finally,several open research challenges in the area of all in-edge are presented.\rCollaborating Heterogeneous Natural Language Processing Tasks via Federated Learning\nChenhe Dong Yuexiang Xie Bolin Ding Ying Shen Yaliang Li\nabstract\rabstract: The increasing privacy concerns on personal private text data promote thedevelopment of federated learning (FL) in recent years. However, the existingstudies on applying FL in NLP are not suitable to coordinate participants withheterogeneous or private learning objectives. In this study, we further broadenthe application scope of FL in NLP by proposing an Assign-Then-Contrast(denoted as ATC) framework, which enables clients with heterogeneous NLP tasksto construct an FL course and learn useful knowledge from each other.Specifically, the clients are suggested to first perform local training withthe unified tasks assigned by the server rather than using their own learningobjectives, which is called the Assign training stage. After that, in theContrast training stage, clients train with different local learning objectivesand exchange knowledge with other clients who contribute consistent and usefulmodel updates. We conduct extensive experiments on six widely-used datasetscovering both Natural Language Understanding (NLU) and Natural LanguageGeneration (NLG) tasks, and the proposed ATC framework achieves significantimprovements compared with various baseline methods. The source code isavailable at\\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.\r2022-12-05\nExtending Expressive Access Policies with Privacy Features\nStefan More Sebastian Ramacher Lukas Alber Marco Herzl\nabstract\rabstract: Authentication, authorization, and trust verification are central parts of anaccess control system. The conditions for granting access in such a system arecollected in access policies. Since access conditions are often complex,dedicated languages \u0026ndash; policy languages \u0026ndash; for defining policies are in use. However, current policy languages are unable to express such conditionshaving privacy of users in mind. With privacy-preserving technologies, usersare enabled to prove information to the access system without revealing it. In this work, we present a generic design for supporting privacy-preservingtechnologies in policy languages. Our design prevents unnecessary disclosure ofsensitive information while still allowing the formulation of expressive rulesfor access control. For that we make use of zero-knowledge proofs (NIZKs). Wedemonstrate our design by applying it to the TPL policy language, while usingSNARKs. Also, we evaluate the resulting ZK-TPL language and its associatedtoolchain. Our evaluation shows that for regular-sized credentialscommunication and verification overhead is negligible.\r2022-12-04\nA Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification\nKaifa Zhao Le Yu Shiyao Zhou Jing Li Xiapu Luo Yat Fei Aemon Chiu Yutong Liu\nabstract\rabstract: Privacy protection raises great attention on both legal levels and userawareness. To protect user privacy, countries enact laws and regulationsrequiring software privacy policies to regulate their behavior. However,privacy policies are written in natural languages with many legal terms andsoftware jargon that prevent users from understanding and even reading them. Itis desirable to use NLP techniques to analyze privacy policies for helpingusers understand them. Furthermore, existing datasets ignore law requirementsand are limited to English. In this paper, we construct the first Chineseprivacy policy dataset, namely CA4P-483, to facilitate the sequence labelingtasks and regulation compliance identification between privacy policies andsoftware. Our dataset includes 483 Chinese Android application privacypolicies, over 11K sentences, and 52K fine-grained annotations. We evaluatefamilies of robust and representative baseline models on our dataset. Based onbaseline performance, we provide findings and potential research directions onour dataset. Finally, we investigate the potential applications of CA4P-483combing regulation requirements and program analysis.\r2022-12-02\nSemantics-Preserved Distortion for Personal Privacy Protection in Information Management\nJiajia Li Letian Peng Ping Wang Zuchao Li Xueyi Li Hai Zhao\nabstract\rabstract: Although machine learning and especially deep learning methods have played animportant role in the field of information management, privacy protection is animportant and concerning topic in current machine learning models. Ininformation management field, a large number of texts containing personalinformation are produced by users every day. As the model training oninformation from users is likely to invade personal privacy, many methods havebeen proposed to block the learning and memorizing of the sensitive data in rawtexts. In this paper, we try to do this more linguistically via distorting thetext while preserving the semantics. In practice, we leverage a recently ourproposed metric, Neighboring Distribution Divergence, to evaluate the semanticpreservation during the distortion. Based on the metric, we propose twoframeworks for semantics-preserved distortion, a generative one and asubstitutive one. We conduct experiments on named entity recognition,constituency parsing, and machine reading comprehension tasks. Results from ourexperiments show the plausibility and efficiency of our distortion as a methodfor personal privacy protection. Moreover, we also evaluate the attributeattack on three privacy-related tasks in the current natural languageprocessing field, and the results show the simplicity and effectiveness of ourdata-based improvement approach compared to the structural improvementapproach. Further, we also investigate the effects of privacy protection inspecific medical information management in this work and show that the medicalinformation pre-training model using our approach can effectively reduce thememory of patients and symptoms, which fully demonstrates the practicality ofour approach.\r2022-11-30\nA Case for Business Process-Specific Foundation Models\nYara Rizk Praveen Venkateswaran Vatche Isahagian Vinod Muthusamy\nabstract\rabstract: The inception of large language models has helped advance state-of-the-artperformance on numerous natural language tasks. This has also opened the doorfor the development of foundation models for other domains and data modalitiessuch as images, code, and music. In this paper, we argue that business processdata representations have unique characteristics that warrant the developmentof a new class of foundation models to handle tasks like process mining,optimization, and decision making. These models should also tackle the uniquechallenges of applying AI to business processes which include data scarcity,multi-modal representations, domain specific terminology, and privacy concerns.\r2022-11-28\nAttack on Unfair ToS Clause Detection: A Case Study using Universal Adversarial Triggers\nShanshan Xu Irina Broda Rashid Haddad Marco Negrini Matthias Grabmair\nabstract\rabstract: Recent work has demonstrated that natural language processing techniques cansupport consumer protection by automatically detecting unfair clauses in theTerms of Service (ToS) Agreement. This work demonstrates that transformer-basedToS analysis systems are vulnerable to adversarial attacks. We conductexperiments attacking an unfair-clause detector with universal adversarialtriggers. Experiments show that a minor perturbation of the text canconsiderably reduce the detection performance. Moreover, to measure thedetectability of the triggers, we conduct a detailed human evaluation study bycollecting both answer accuracy and response time from the participants. Theresults show that the naturalness of the triggers remains key to trickingreaders.\r2022-11-25\nDeep Learning on a Healthy Data Diet: Finding Important Examples for Fairness\nAbdelrahman Zayed Prasanna Parthasarathi Goncalo Mordido Hamid Palangi Samira Shabanian Sarath Chandar\nabstract\rabstract: Data-driven predictive solutions predominant in commercial applications tendto suffer from biases and stereotypes, which raises equity concerns. Predictionmodels may discover, use, or amplify spurious correlations based on gender orother protected personal characteristics, thus discriminating againstmarginalized groups. Mitigating gender bias has become an important researchfocus in natural language processing (NLP) and is an area where annotatedcorpora are available. Data augmentation reduces gender bias by addingcounterfactual examples to the training dataset. In this work, we show thatsome of the examples in the augmented dataset can be not important or evenharmful for fairness. We hence propose a general method for pruning both thefactual and counterfactual examples to maximize the model\u0026rsquo;s fairness asmeasured by the demographic parity, equality of opportunity, and equality ofodds. The fairness achieved by our method surpasses that of data augmentationon three text classification datasets, using no more than half of the examplesin the augmented dataset. Our experiments are conducted using models of varyingsizes and pre-training settings.\r2022-11-23\nEmerging Biometric Modalities and their Use: Loopholes in the Terminology of the GDPR and Resulting Privacy Risks\nTamas Bisztray Nils Gruschka Thirimachos Bourlai Lothar Fritsch\nabstract\rabstract: Technological advancements allow biometric applications to be moreomnipresent than in any other time before. This paper argues that in thecurrent EU data protection regulation, classification applications usingbiometric data receive less protection compared to biometric recognition. Weanalyse preconditions in the regulatory language and explore how this has thepotential to be the source of unique privacy risks for processing operationsclassifying individuals based on soft traits like emotions. This can have highimpact on personal freedoms and human rights and therefore, should be subjectto data protection impact assessment.\rAgent-Specific Deontic Modality Detection in Legal Language\nAbhilasha Sancheti Aparna Garimella Balaji Vasan Srinivasan Rachel Rudinger\nabstract\rabstract: Legal documents are typically long and written in legalese, which makes itparticularly difficult for laypeople to understand their rights and duties.While natural language understanding technologies can be valuable in supportingsuch understanding in the legal domain, the limited availability of datasetsannotated for deontic modalities in the legal domain, due to the cost of hiringexperts and privacy issues, is a bottleneck. To this end, we introduce,LEXDEMOD, a corpus of English contracts annotated with deontic modalityexpressed with respect to a contracting party or agent along with the modaltriggers. We benchmark this dataset on two tasks: (i) agent-specificmulti-label deontic modality classification, and (ii) agent-specific deonticmodality and trigger span detection using Transformer-based (Vaswani et al.,2017) language models. Transfer learning experiments show that the linguisticdiversity of modal expressions in LEXDEMOD generalizes reasonably from lease toemployment and rental agreements. A small case study indicates that a modeltrained on LEXDEMOD can detect red flags with high recall. We believe our workoffers a new research direction for deontic modality detection in the legaldomain.\r2022-11-22\nGDPR Compliant Collection of Therapist-Patient-Dialogues\nTobias Mayer Neha Warikoo Oliver Grimm Andreas Reif Iryna Gurevych\nabstract\rabstract: According to the Global Burden of Disease list provided by the World HealthOrganization (WHO), mental disorders are among the most debilitatingdisorders.To improve the diagnosis and the therapy effectiveness in recentyears, researchers have tried to identify individual biomarkers. Gatheringneurobiological data however, is costly and time-consuming. Another potentialsource of information, which is already part of the clinical routine, aretherapist-patient dialogues. While there are some pioneering worksinvestigating the role of language as predictors for various therapeuticparameters, for example patient-therapist alliance, there are no large-scalestudies. A major obstacle to conduct these studies is the availability ofsizeable datasets, which are needed to train machine learning models. Whilethese conversations are part of the daily routine of clinicians, gathering themis usually hindered by various ethical (purpose of data usage), legal (dataprivacy) and technical (data formatting) limitations. Some of these limitationsare particular to the domain of therapy dialogues, like the increaseddifficulty in anonymisation, or the transcription of the recordings. In thispaper, we elaborate on the challenges we faced in starting our collection oftherapist-patient dialogues in a psychiatry clinic under the General DataPrivacy Regulation of the European Union with the goal to use the data forNatural Language Processing (NLP) research. We give an overview of each step inour procedure and point out the potential pitfalls to motivate further researchin this field.\r2022-11-18\nDeepHider: A Covert NLP Watermarking Framework Based on Multi-task Learning\nLong Dai Jiarong Mao Xuefeng Fan Xiaoyi Zhou\nabstract\rabstract: Natural language processing (NLP) technology has shown great commercial valuein applications such as sentiment analysis. But NLP models are vulnerable tothe threat of pirated redistribution, damaging the economic interests of modelowners. Digital watermarking technology is an effective means to protect theintellectual property rights of NLP model. The existing NLP model protectionmainly designs watermarking schemes by improving both security and robustnesspurposes, however, the security and robustness of these schemes have thefollowing problems, respectively: (1) Watermarks are difficult to defendagainst fraudulent declaration by adversary and are easily detected and blockedfrom verification by human or anomaly detector during the verification process.(2) The watermarking model cannot meet multiple robustness requirements at thesame time. To solve the above problems, this paper proposes a novelwatermarking framework for NLP model based on the over-parameterization ofdepth model and the multi-task learning theory. Specifically, a covert triggerset is established to realize the perception-free verification of thewatermarking model, and a novel auxiliary network is designed to improve therobustness and security of the watermarking model. The proposed framework wasevaluated on two benchmark datasets and three mainstream NLP models, and theresults show that the framework can successfully validate model ownership with100% validation accuracy and advanced robustness and security withoutcompromising the host model performance.\r2022-11-17\nIncorporating Pre-training Paradigm for Antibody Sequence-Structure Co-design\nKaiyuan Gao Lijun Wu Jinhua Zhu Tianbo Peng Yingce Xia Liang He Shufang Xie Tao Qin Haiguang Liu Kun He Tie-Yan Liu\nabstract\rabstract: Antibodies are versatile proteins that can bind to pathogens and provideeffective protection for human body. Recently, deep learning-basedcomputational antibody design has attracted popular attention since itautomatically mines the antibody patterns from data that could be complementaryto human experiences. However, the computational methods heavily rely onhigh-quality antibody structure data, which is quite limited. Besides, thecomplementarity-determining region (CDR), which is the key component of anantibody that determines the specificity and binding affinity, is highlyvariable and hard to predict. Therefore, the data limitation issue furtherraises the difficulty of CDR generation for antibodies. Fortunately, thereexists a large amount of sequence data of antibodies that can help model theCDR and alleviate the reliance on structure data. By witnessing the success ofpre-training models for protein modeling, in this paper, we develop theantibody pre-training language model and incorporate it into the(antigen-specific) antibody design model in a systemic way. Specifically, wefirst pre-train an antibody language model based on the sequence data, thenpropose a one-shot way for sequence and structure generation of CDR to avoidthe heavy cost and error propagation from an autoregressive manner, and finallyleverage the pre-trained antibody model for the antigen-specific antibodygeneration model with some carefully designed modules. Through variousexperiments, we show that our method achieves superior performances overprevious baselines on different tasks, such as sequence and structuregeneration and antigen-binding CDR-H3 design.\r2022-11-16\n#maskUp: Selective Attribute Encryption for Sensitive Vocalization for English language on Social Media Platforms\nSupriti Vijay Aman Priyanshu\nabstract\rabstract: Social media has become a platform for people to stand up and raise theirvoices against social and criminal acts. Vocalization of such information hasallowed the investigation and identification of criminals. However, revealingsuch sensitive information may jeopardize the victim\u0026rsquo;s safety. We propose#maskUp, a safe method for information communication in a secure fashion to therelevant authorities, discouraging potential bullying of the victim. This wouldensure security by conserving their privacy through natural language processingsupplemented with selective encryption for sensitive attribute masking. To ourknowledge, this is the first work that aims to protect the privacy of thevictims by masking their private details as well as emboldening them to comeforward to report crimes. The use of masking technology allows only bindingauthorities to view/un-mask this data. We construct and evaluate the proposedmethodology on continual learning tasks, allowing practical implementation ofthe same in a real-world scenario. #maskUp successfully demonstrates thisintegration on sample datasets validating the presented objective.\r2022-11-15\nPrivacy Guarantees for De-identifying Text Transformations\nDavid Ifeoluwa Adelani Ali Davody Thomas Kleinbauer Dietrich Klakow\nabstract\rabstract: Machine Learning approaches to Natural Language Processing tasks benefit froma comprehensive collection of real-life user data. At the same time, there is aclear need for protecting the privacy of the users whose data is collected andprocessed. For text collections, such as, e.g., transcripts of voiceinteractions or patient records, replacing sensitive parts with benignalternatives can provide de-identification. However, how much privacy isactually guaranteed by such text transformations, and are the resulting textsstill useful for machine learning? In this paper, we derive formal privacyguarantees for general text transformation-based de-identification methods onthe basis of Differential Privacy. We also measure the effect that differentways of masking private information in dialog transcripts have on a subsequentmachine learning task. To this end, we formulate different masking strategiesand compare their privacy-utility trade-offs. In particular, we compare asimple redact approach with more sophisticated word-by-word replacement usingdeep learning models on multiple natural language understanding tasks likenamed entity recognition, intent detection, and dialog act classification. Wefind that only word-by-word replacement is robust against performance drops invarious tasks.\rA Closer Look at the Calibration of Differentially Private Learners\nHanlin Zhang Xuechen Li Prithviraj Sen Salim Roukos Tatsunori Hashimoto\nabstract\rabstract: We systematically study the calibration of classifiers trained withdifferentially private stochastic gradient descent (DP-SGD) and observemiscalibration across a wide range of vision and language tasks. Our analysisidentifies per-example gradient clipping in DP-SGD as a major cause ofmiscalibration, and we show that existing approaches for improving calibrationwith differential privacy only provide marginal improvements in calibrationerror while occasionally causing large degradations in accuracy. As a solution,we show that differentially private variants of post-processing calibrationmethods such as temperature scaling and Platt scaling are surprisinglyeffective and have negligible utility cost to the overall model. Across 7tasks, temperature scaling and Platt scaling with DP-SGD result in an average3.1-fold reduction in the in-domain expected calibration error and only incurat most a minor percent drop in accuracy.\r2022-11-13\nWatermarking Graph Neural Networks based on Backdoor Attacks\nJing Xu Stefanos Koffas Oguzhan Ersoy Stjepan Picek\nabstract\rabstract: Graph Neural Networks (GNNs) have achieved promising performance in variousreal-world applications. Building a powerful GNN model is not a trivial task,as it requires a large amount of training data, powerful computing resources,and human expertise in fine-tuning the model. Moreover, with the development ofadversarial attacks, e.g., model stealing attacks, GNNs raise challenges tomodel authentication. To avoid copyright infringement on GNNs, verifying theownership of the GNN models is necessary. This paper presents a watermarking framework for GNNs for both graph and nodeclassification tasks. We 1) design two strategies to generate watermarked datafor the graph classification task and one for the node classification task, 2)embed the watermark into the host model through training to obtain thewatermarked GNN model, and 3) verify the ownership of the suspicious model in ablack-box setting. The experiments show that our framework can verify theownership of GNN models with a very high probability (up to $99%$) for bothtasks. Finally, we experimentally show that our watermarking approach is robustagainst a state-of-the-art model extraction technique and four state-of-the-artdefenses against backdoor attacks.\r2022-11-11\nA Federated Approach to Predicting Emojis in Hindi Tweets\nDeep Gandhi Jash Mehta Nirali Parekh Karan Waghela Lynette D\u0026rsquo;Mello Zeerak Talat\nabstract\rabstract: The use of emojis affords a visual modality to, often private, textualcommunication. The task of predicting emojis however provides a challenge formachine learning as emoji use tends to cluster into the frequently used and therarely used emojis. Much of the machine learning research on emoji use hasfocused on high resource languages and has conceptualised the task ofpredicting emojis around traditional server-side machine learning approaches.However, traditional machine learning approaches for private communication canintroduce privacy concerns, as these approaches require all data to betransmitted to a central storage. In this paper, we seek to address the dualconcerns of emphasising high resource languages for emoji prediction andrisking the privacy of people\u0026rsquo;s data. We introduce a new dataset of $118$ktweets (augmented from $25$k unique tweets) for emoji prediction in Hindi, andpropose a modification to the federated learning algorithm, CausalFedGSD, whichaims to strike a balance between model performance and user privacy. We showthat our approach obtains comparative scores with more complex centralisedmodels while reducing the amount of data required to optimise the models andminimising risks to user privacy.\rAnonymization of Whole Slide Images in Histopathology for Research and Education\nTom Bisson Michael Franz Isil Dogan O Daniel Romberg Christoph Jansen Peter Hufnagl Norman Zerbe\nabstract\rabstract: Objective: The exchange of health-related data is subject to regional lawsand regulations, such as the General Data Protection Regulation (GDPR) in theEU or the Health Insurance Portability and Accountability Act (HIPAA) in theUnited States, resulting in non-trivial challenges for researchers andeducators when working with these data. In pathology, the digitization ofdiagnostic tissue samples inevitably generates identifying data that canconsist of sensitive but also acquisition-related information stored invendor-specific file formats. Distribution and off-clinical use of these WholeSlide Images (WSI) is usually done in these formats, as an industry-widestandardization such as DICOM is yet only tentatively adopted and slide scannervendors currently do not provide anonymization functionality. Methods: We developed a guideline for the proper handling ofhistopathological image data particularly for research and education withregard to the GDPR. In this context, we evaluated existing anonymizationmethods and examined proprietary format specifications to identify allsensitive information for the most common WSI formats. This work results in asoftware library that enables GDPR-compliant anonymization of WSIs whilepreserving the native formats. Results: Based on the analysis of proprietary formats, all occurrences ofsensitive information were identified for file formats frequently used inclinical routine, and finally, an open-source programming library with anexecutable CLI-tool and wrappers for different programming languages wasdeveloped. Conclusions: Our analysis showed that there is no straightforward softwaresolution to anonymize WSIs in a GDPR-compliant way while maintaining the dataformat. We closed this gap with our extensible open-source library that worksinstantaneously and offline.\r2022-11-10\nLarge Language Models Can Be Strong Differentially Private Learners\nXuechen Li Florian Tramèr Percy Liang Tatsunori Hashimoto\nabstract\rabstract: Differentially Private (DP) learning has seen limited success for buildinglarge deep learning models of text, and straightforward attempts at applyingDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks haveresulted in large performance drops and high computational overhead. We showthat this performance drop can be mitigated with (1) the use of largepretrained language models; (2) non-standard hyperparameters that suit DPoptimization; and (3) fine-tuning objectives which are aligned with thepretraining procedure. With the above, we obtain NLP models that outperformstate-of-the-art DP-trained models under the same privacy budget and strongnon-private baselines \u0026ndash; by directly fine-tuning pretrained models with DPoptimization on moderately-sized corpora. To address the computationalchallenge of running DP-SGD with large Transformers, we propose a memory savingtechnique that allows clipping in DP-SGD to run without instantiatingper-example gradients for any linear layer in the model. The technique enablesprivately training Transformers with almost the same memory cost as non-privatetraining at a modest run-time overhead. Contrary to conventional wisdom that DPoptimization fails at learning high-dimensional models (due to noise thatscales with dimension) empirical results reveal that private learning withpretrained language models doesn\u0026rsquo;t tend to suffer from dimension-dependentperformance degradation. Code to reproduce results can be found athttps://github.com/lxuechen/private-transformers.\r2022-11-09\nUser-Entity Differential Privacy in Learning Natural Language Models\nPhung Lai NhatHai Phan Tong Sun Rajiv Jain Franck Dernoncourt Jiuxiang Gu Nikolaos Barmpalios\nabstract\rabstract: In this paper, we introduce a novel concept of user-entity differentialprivacy (UeDP) to provide formal privacy protection simultaneously to bothsensitive entities in textual data and data owners in learning natural languagemodels (NLMs). To preserve UeDP, we developed a novel algorithm, calledUeDP-Alg, optimizing the trade-off between privacy loss and model utility witha tight sensitivity bound derived from seamlessly combining user and sensitiveentity sampling processes. An extensive theoretical analysis and evaluationshow that our UeDP-Alg outperforms baseline approaches in model utility underthe same privacy budget consumption on several NLM tasks, using benchmarkdatasets.\r2022-11-07\nInvestigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach\nJiayao Zhang Hongming Zhang Zhun Deng Dan Roth\nabstract\rabstract: Double-blind peer review mechanism has become the skeleton of academicresearch across multiple disciplines including computer science, yet severalstudies have questioned the quality of peer reviews and raised concerns onpotential biases in the process. In this paper, we conduct a thorough andrigorous study on fairness disparities in peer review with the help of largelanguage models (LMs). We collect, assemble, and maintain a comprehensiverelational database for the International Conference on LearningRepresentations (ICLR) conference from 2017 to date by aggregating data fromOpenReview, Google Scholar, arXiv, and CSRanking, and extracting high-levelfeatures using language models. We postulate and study fairness disparities onmultiple protective attributes of interest, including author gender, geography,author, and institutional prestige. We observe that the level of disparitydiffers and textual features are essential in reducing biases in the predictivemodeling. We distill several insights from our analysis on study the peerreview process with the help of large LMs. Our database also provides avenuesfor studying new natural language processing (NLP) methods that facilitate theunderstanding of the peer review mechanism. We study a concrete example towardsautomatic machine review systems and provide baseline models for the reviewgeneration and scoring tasks such that the database can be used as a benchmark.\r2022-11-05\nPrivacy-Preserving Models for Legal Natural Language Processing\nYing Yin Ivan Habernal\nabstract\rabstract: Pre-training large transformer models with in-domain data improves domainadaptation and helps gain performance on the domain-specific downstream tasks.However, sharing models pre-trained on potentially sensitive data is prone toadversarial privacy attacks. In this paper, we asked to which extent we canguarantee privacy of pre-training data and, at the same time, achieve betterdownstream performance on legal tasks without the need of additional labeleddata. We extensively experiment with scalable self-supervised learning oftransformer models under the formal paradigm of differential privacy and showthat under specific training configurations we can improve downstreamperformance without sacrifying privacy protection for the in-domain data. Ourmain contribution is utilizing differential privacy for large-scalepre-training of transformer language models in the legal NLP domain, which, tothe best of our knowledge, has not been addressed before.\rTextual Manifold-based Defense Against Natural Language Adversarial Examples\nDang Minh Nguyen Luu Anh Tuan\nabstract\rabstract: Recent studies on adversarial images have shown that they tend to leave theunderlying low-dimensional data manifold, making them significantly morechallenging for current models to make correct predictions. This so-calledoff-manifold conjecture has inspired a novel line of defenses againstadversarial attacks on images. In this study, we find a similar phenomenonoccurs in the contextualized embedding space induced by pretrained languagemodels, in which adversarial texts tend to have their embeddings diverge fromthe manifold of natural ones. Based on this finding, we propose TextualManifold-based Defense (TMD), a defense mechanism that projects text embeddingsonto an approximated embedding manifold before classification. It reduces thecomplexity of potential adversarial examples, which ultimately enhances therobustness of the protected model. Through extensive experiments, our methodconsistently and significantly outperforms previous defenses under variousattack settings without trading off clean accuracy. To the best of ourknowledge, this is the first NLP defense that leverages the manifold structureagainst adversarial attacks. Our code is available at\\url{https://github.com/dangne/tmd}.\r2022-11-04\nMemorization in NLP Fine-tuning Methods\nFatemehsadat Mireshghallah Archit Uniyal Tianhao Wang David Evans Taylor Berg-Kirkpatrick\nabstract\rabstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the\u0026quot;pre-train and fine-tune\u0026quot; paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.\rQuantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks\nFatemehsadat Mireshghallah Kartik Goyal Archit Uniyal Taylor Berg-Kirkpatrick Reza Shokri\nabstract\rabstract: The wide adoption and application of Masked language models~(MLMs) onsensitive data (from legal to medical) necessitates a thorough quantitativeinvestigation into their privacy vulnerabilities \u0026ndash; to what extent do MLMs leakinformation about their training data? Prior attempts at measuring leakage ofMLMs via membership inference attacks have been inconclusive, implying thepotential robustness of MLMs to privacy attacks. In this work, we posit thatprior attempts were inconclusive because they based their attack solely on theMLM\u0026rsquo;s model score. We devise a stronger membership inference attack based onlikelihood ratio hypothesis testing that involves an additional reference MLMto more accurately quantify the privacy risks of memorization in MLMs. We showthat masked language models are extremely susceptible to likelihood ratiomembership inference attacks: Our empirical results, on models trained onmedical notes, show that our attack improves the AUC of prior membershipinference attacks from 0.66 to an alarmingly high 0.90 level, with asignificant improvement in the low-error region: at 1% false positive rate, ourattack is 51X more powerful than prior work.\r2022-11-03\nVerifying RISC-V Physical Memory Protection\nKevin Cheang Cameron Rasmussen Dayeol Lee David W. Kohlbrenner Krste Asanović Sanjit A. Seshia\nabstract\rabstract: We formally verify an open-source hardware implementation of physical memoryprotection (PMP) in RISC-V, which is a standard feature used for memoryisolation in security critical systems such as the Keystone trusted executionenvironment. PMP provides per-hardware-thread machine-mode control registersthat specify the access privileges for physical memory regions. We firstformalize the functional property of the PMP rules based on the RISC-V ISAmanual. Then, we use the LIME tool to translate an open-source implementationof the PMP hardware module written in Chisel to the UCLID5 formal verificationlanguage. We encode the formal specification in UCLID5 and verify thefunctional correctness of the hardware. This is an initial effort towardsverifying the Keystone framework, where the trusted computing base (TCB) relieson PMP to provide security guarantees such as integrity and confidentiality.\r2022-11-02\nAn Easy-to-use and Robust Approach for the Differentially Private De-Identification of Clinical Textual Documents\nYakini Tchouka Jean-François Couchot David Laiymani\nabstract\rabstract: Unstructured textual data is at the heart of healthcare systems. For obviousprivacy reasons, these documents are not accessible to researchers as long asthey contain personally identifiable information. One way to share this datawhile respecting the legislative framework (notably GDPR or HIPAA) is, withinthe medical structures, to de-identify it, i.e. to detect the personalinformation of a person through a Named Entity Recognition (NER) system andthen replacing it to make it very difficult to associate the document with theperson. The challenge is having reliable NER and substitution tools withoutcompromising confidentiality and consistency in the document. Most of theconducted research focuses on English medical documents with coarsesubstitutions by not benefiting from advances in privacy. This paper shows howan efficient and differentially private de-identification approach can beachieved by strengthening the less robust de-identification method and byadapting state-of-the-art differentially private mechanisms for substitutionpurposes. The result is an approach for de-identifying clinical documents inFrench language, but also generalizable to other languages and whose robustnessis mathematically proven.\r2022-11-01\nShould I disclose my dataset? Caveats between reproducibility and individual data rights\nRaysa M. Benatti Camila M. L. Villarroel Sandra Avila Esther L. Colombini Fabiana C. Severi\nabstract\rabstract: Natural language processing techniques have helped domain experts solve legalproblems. Digital availability of court documents increases possibilities forresearchers, who can access them as a source for building datasets \u0026ndash; whosedisclosure is aligned with good reproducibility practices in computationalresearch. Large and digitized court systems, such as the Brazilian one, areprone to be explored in that sense. However, personal data protection lawsimpose restrictions on data exposure and state principles about whichresearchers should be mindful. Special caution must be taken in cases withhuman rights violations, such as gender discrimination, over which we elaborateas an example of interest. We present legal and ethical considerations on theissue, as well as guidelines for researchers dealing with this kind of data anddeciding whether to disclose it.\r2022-10-31\nFully Adaptive Composition for Gaussian Differential Privacy\nAdam Smith Abhradeep Thakurta\nabstract\rabstract: We show that Gaussian Differential Privacy, a variant of differential privacytailored to the analysis of Gaussian noise addition, composes gracefully evenin the presence of a fully adaptive analyst. Such an analyst selects mechanisms(to be run on a sensitive data set) and their privacy budgets adaptively, thatis, based on the answers from other mechanisms run previously on the same dataset. In the language of Rogers, Roth, Ullman and Vadhan, this gives a filterfor GDP with the same parameters as for nonadaptive composition.\rImproving Cause-of-Death Classification from Verbal Autopsy Reports\nThokozile Manaka Terence van Zyl Deepak Kar\nabstract\rabstract: In many lower-and-middle income countries including South Africa, data accessin health facilities is restricted due to patient privacy and confidentialitypolicies. Further, since clinical data is unique to individual institutions andlaboratories, there are insufficient data annotation standards and conventions.As a result of the scarcity of textual data, natural language processing (NLP)techniques have fared poorly in the health sector. A cause of death (COD) isoften determined by a verbal autopsy (VA) report in places without reliabledeath registration systems. A non-clinician field worker does a VA report usinga set of standardized questions as a guide to uncover symptoms of a COD. Thisanalysis focuses on the textual part of the VA report as a case study toaddress the challenge of adapting NLP techniques in the health domain. Wepresent a system that relies on two transfer learning paradigms of monolinguallearning and multi-source domain adaptation to improve VA narratives for thetarget task of the COD classification. We use the Bidirectional EncoderRepresentations from Transformers (BERT) and Embeddings from Language Models(ELMo) models pre-trained on the general English and health domains to extractfeatures from the VA narratives. Our findings suggest that this transferlearning system improves the COD classification tasks and that the narrativetext contains valuable information for figuring out a COD. Our results furthershow that combining binary VA features and narrative text features learned viathis framework boosts the classification task of COD.\rExtracted BERT Model Leaks More Information than You Think!\nXuanli He Chen Chen Lingjuan Lyu Qiongkai Xu\nabstract\rabstract: The collection and availability of big data, combined with advances inpre-trained models (e.g. BERT), have revolutionized the predictive performanceof natural language processing tasks. This allows corporations to providemachine learning as a service (MLaaS) by encapsulating fine-tuned BERT-basedmodels as APIs. Due to significant commercial interest, there has been a surgeof attempts to steal re mote services via model extraction. Although previousworks have made progress in defending against model extraction attacks, therehas been little discussion on their performance in preventing privacy leakage.This work bridges this gap by launching an attribute inference attack againstthe extracted BERT model. Our extensive experiments reveal that modelextraction can cause severe privacy leakage even when victim models arefacilitated with advanced defensive strategies.\r2022-10-27\nJust Fine-tune Twice: Selective Differential Privacy for Large Language Models\nWeiyan Shi Ryan Shea Si Chen Chiyuan Zhang Ruoxi Jia Zhou Yu\nabstract\rabstract: Protecting large language models from privacy leakage is becomingincreasingly crucial with their wide adoption in real-world products. Yetapplying differential privacy (DP), a canonical notion with provable privacyguarantees for machine learning models, to those models remains challenging dueto the trade-off between model utility and privacy loss. Utilizing the factthat sensitive information in language data tends to be sparse, Shi et al.(2021) formalized a DP notion extension called Selective Differential Privacy(SDP) to protect only the sensitive tokens defined by a policy function.However, their algorithm only works for RNN-based models. In this paper, wedevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP forstate-of-the-art large transformer-based models. Our method is easy toimplement: it first fine-tunes the model with redacted in-domain data, and thenfine-tunes it again with the original in-domain data using a private trainingmechanism. Furthermore, we study the scenario of imperfect implementation ofpolicy functions that misses sensitive tokens and develop systematic methods tohandle it. Experiments show that our method achieves strong utility compared toprevious baselines. We also analyze the SDP privacy guarantee empirically withthe canary insertion attack.\rLearning Location from Shared Elevation Profiles in Fitness Apps: A Privacy Perspective\nUlku Meteriz-Yildiran Necip Fazil Yildiran Joongheon Kim David Mohaisen\nabstract\rabstract: The extensive use of smartphones and wearable devices has facilitated manyuseful applications. For example, with Global Positioning System (GPS)-equippedsmart and wearable devices, many applications can gather, process, and sharerich metadata, such as geolocation, trajectories, elevation, and time. Forexample, fitness applications, such as Runkeeper and Strava, utilize theinformation for activity tracking and have recently witnessed a boom inpopularity. Those fitness tracker applications have their own web platforms andallow users to share activities on such platforms or even with other socialnetwork platforms. To preserve the privacy of users while allowing sharing,several of those platforms may allow users to disclose partial information,such as the elevation profile for an activity, which supposedly would not leakthe location of the users. In this work, and as a cautionary tale, we create aproof of concept where we examine the extent to which elevation profiles can beused to predict the location of users. To tackle this problem, we devise threeplausible threat settings under which the city or borough of the targets can bepredicted. Those threat settings define the amount of information available tothe adversary to launch the prediction attacks. Establishing that simplefeatures of elevation profiles, e.g., spectral features, are insufficient, wedevise both natural language processing (NLP)-inspired text-like representationand computer vision-inspired image-like representation of elevation profiles,and we convert the problem at hand into text and image classification problem.We use both traditional machine learning- and deep learning-based techniquesand achieve a prediction success rate ranging from 59.59% to 99.80%. Thefindings are alarming, highlighting that sharing elevation information may havesignificant location privacy risks.\r2022-10-26\nDifferentially Private Language Models for Secure Data Sharing\nJustus Mattern Zhijing Jin Benjamin Weggenmann Bernhard Schoelkopf Mrinmaya Sachan\nabstract\rabstract: To protect the privacy of individuals whose data is being shared, it is ofhigh importance to develop methods allowing researchers and companies torelease textual data while providing formal privacy guarantees to itsoriginators. In the field of NLP, substantial efforts have been directed atbuilding mechanisms following the framework of local differential privacy,thereby anonymizing individual text samples before releasing them. In practice,these approaches are often dissatisfying in terms of the quality of theiroutput language due to the strong noise required for local differentialprivacy. In this paper, we approach the problem at hand using globaldifferential privacy, particularly by training a generative language model in adifferentially private manner and consequently sampling data from it. Usingnatural language prompts and a new prompt-mismatch loss, we are able to createhighly accurate and fluent textual datasets taking on specific desiredattributes such as sentiment or topic and resembling statistical properties ofthe training data. We perform thorough experiments indicating that oursynthetic datasets do not leak information from our original data and are ofhigh language quality and highly suitable for training models for furtheranalysis on real-world data. Notably, we also demonstrate that trainingclassifiers on private synthetic data outperforms directly training classifierson real data with DP-SGD.\rWhen Does Differentially Private Learning Not Suffer in High Dimensions?\nXuechen Li Daogao Liu Tatsunori Hashimoto Huseyin A. Inan Janardhan Kulkarni Yin Tat Lee Abhradeep Guha Thakurta\nabstract\rabstract: Large pretrained models can be privately fine-tuned to achieve performanceapproaching that of non-private models. A common theme in these results is thesurprising observation that high-dimensional models can achieve favorableprivacy-utility trade-offs. This seemingly contradicts known results on themodel-size dependence of differentially private convex learning and raises thefollowing research question: When does the performance of differentiallyprivate learning not degrade with increasing model size? We identify that themagnitudes of gradients projected onto subspaces is a key factor thatdetermines performance. To precisely characterize this for private convexlearning, we introduce a condition on the objective that we term\\emph{restricted Lipschitz continuity} and derive improved bounds for theexcess empirical and population risks that are dimension-independent underadditional conditions. We empirically show that in private fine-tuning of largelanguage models, gradients obtained during fine-tuning are mostly controlled bya few principal components. This behavior is similar to conditions under whichwe obtain dimension-independent bounds in convex settings. Our theoretical andempirical results together provide a possible explanation for recent successesin large-scale private fine-tuning. Code to reproduce our results can be foundat\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.\r2022-10-25\nOn Robust Incremental Learning over Many Multilingual Steps\nKaran Praharaj Irina Matveeva\nabstract\rabstract: Recent work in incremental learning has introduced diverse approaches totackle catastrophic forgetting from data augmentation to optimized trainingregimes. However, most of them focus on very few training steps. We propose amethod for robust incremental learning over dozens of fine-tuning steps usingdata from a variety of languages. We show that a combination ofdata-augmentation and an optimized training regime allows us to continueimproving the model even for as many as fifty training steps. Crucially, ouraugmentation strategy does not require retaining access to previous trainingdata and is suitable in scenarios with privacy constraints.\rLeveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios\nZhuohao Chen Nikolaos Flemotomos Zac E. Imel David C. Atkins Shrikanth Narayanan\nabstract\rabstract: In psychotherapy interactions, the quality of a session is assessed bycodifying the communicative behaviors of participants during the conversationthrough manual observation and annotation. Developing computational approachesfor automated behavioral coding can reduce the burden on human coders andfacilitate the objective evaluation of the intervention. In the real world,however, implementing such algorithms is associated with data sparsitychallenges since privacy concerns lead to limited available in-domain data. Inthis paper, we leverage a publicly available conversation-based dataset andtransfer knowledge to the low-resource behavioral coding task by performing anintermediate language model training via meta-learning. We introduce a taskaugmentation method to produce a large number of \u0026ldquo;analogy tasks\u0026rdquo; - taskssimilar to the target one - and demonstrate that the proposed frameworkpredicts target behaviors more accurately than all the other baseline models.\r2022-10-23\nIdentifying Crisis Response Communities in Online Social Networks for Compound Disasters: The Case of Hurricane Laura and Covid-19\nKhondhaker Al Momin H M Imran Kays Arif Mohaimin Sadri\nabstract\rabstract: Online social networks allow different agencies and the public to interactand share the underlying risks and protective actions during major disasters.This study revealed such crisis communication patterns during hurricane Lauracompounded by the COVID-19 pandemic. Laura was one of the strongest (Category4) hurricanes on record to make landfall in Cameron, Louisiana. Using theApplication Programming Interface (API), this study utilizes large-scale socialmedia data obtained from Twitter through the recently released academic trackthat provides complete and unbiased observations. The data captured publiclyavailable tweets shared by active Twitter users from the vulnerable areasthreatened by Laura. Online social networks were based on user influencefeature ( mentions or tags) that allows notifying other users while posting atweet. Using network science theories and advanced community detectionalgorithms, the study split these networks into twenty-one components ofvarious sizes, the largest of which contained eight well-defined communities.Several natural language processing techniques (i.e., word clouds, bigrams,topic modeling) were applied to the tweets shared by the users in thesecommunities to observe their risk-taking or risk-averse behavior during a majorcompounding crisis. Social media accounts of local news media, radio,universities, and popular sports pages were among those who involved heavilyand interacted closely with local residents. In contrast, emergency managementand planning units in the area engaged less with the public. The findings ofthis study provide novel insights into the design of efficient social mediacommunication guidelines to respond better in future disasters.\rA Semantic Account of Metric Preservation\nArthur Azevedo de Amorim Marco Gaboardi Justin Hsu Shin-ya Katsumata Ikram Cherigui\nabstract\rabstract: Program sensitivity measures how robust a program is to small changes in itsinput, and is a fundamental notion in domains ranging from differential privacyto cyber-physical systems. A natural way to formalize program sensitivity is interms of metrics on the input and output spaces, requiring that an$r$-sensitive function map inputs that are at distance $d$ to outputs that areat distance at most $r \\cdot d$. Program sensitivity is thus an analogue ofLipschitz continuity for programs. Reed and Pierce introduced Fuzz, a functional language with a linear typesystem that can express program sensitivity. They show soundness operationally,in the form of a metric preservation property. Inspired by their work, we studyprogram sensitivity and metric preservation from a denotational point of view.In particular, we introduce metric CPOs, a novel semantic structure forreasoning about computation on metric spaces, by endowing CPOs with acompatible notion of distance. This structure is useful for reasoning aboutmetric properties of programs, and specifically about program sensitivity. Wedemonstrate metric CPOs by giving a model for the deterministic fragment ofFuzz.\r2022-10-20\nAre Large Pre-Trained Language Models Leaking Your Personal Information?\nJie Huang Hanyin Shao Kevin Chen-Chuan Chang\nabstract\rabstract: Are Large Pre-Trained Language Models Leaking Your Personal Information? Inthis paper, we analyze whether Pre-Trained Language Models (PLMs) are prone toleaking personal information. Specifically, we query PLMs for email addresseswith contexts of the email address or prompts containing the owner\u0026rsquo;s name. Wefind that PLMs do leak personal information due to memorization. However, sincethe models are weak at association, the risk of specific personal informationbeing extracted by attackers is low. We hope this work could help the communityto better understand the privacy risk of PLMs and bring new insights to makePLMs safe.\r2022-10-19\nLAMP: Extracting Text from Gradients with Language Model Priors\nMislav Balunović Dimitar I. Dimitrov Nikola Jovanović Martin Vechev\nabstract\rabstract: Recent work shows that sensitive user data can be reconstructed from gradientupdates, breaking the key privacy promise of federated learning. While successwas demonstrated primarily on image data, these methods do not directlytransfer to other domains such as text. In this work, we propose LAMP, a novelattack tailored to textual data, that successfully reconstructs original textfrom gradients. Our attack is based on two key insights: (i) modeling priortext probability with an auxiliary language model, guiding the search towardsmore natural text, and (ii) alternating continuous and discrete optimization,which minimizes reconstruction loss on embeddings, while avoiding local minimaby applying discrete text transformations. Our experiments demonstrate thatLAMP is significantly more effective than prior work: it reconstructs 5x morebigrams and 23% longer subsequences on average. Moreover, we are the first torecover inputs from batch sizes larger than 1 for textual models. Thesefindings indicate that gradient updates of models operating on textual dataleak more information than previously thought.\r2022-10-18\nRecovering Private Text in Federated Learning of Language Models\nSamyak Gupta Yangsibo Huang Zexuan Zhong Tianyu Gao Kai Li Danqi Chen\nabstract\rabstract: Federated learning allows distributed users to collaboratively train a modelwhile keeping each user\u0026rsquo;s data private. Recently, a growing body of work hasdemonstrated that an eavesdropping attacker can effectively recover image datafrom gradients transmitted during federated learning. However, little progresshas been made in recovering text data. In this paper, we present a novel attackmethod FILM for federated learning of language models (LMs). For the firsttime, we show the feasibility of recovering text from large batch sizes of upto 128 sentences. Unlike image-recovery methods that are optimized to matchgradients, we take a distinct approach that first identifies a set of wordsfrom gradients and then directly reconstructs sentences based on beam searchand a prior-based reordering strategy. We conduct the FILM attack on severallarge-scale datasets and show that it can successfully reconstruct singlesentences with high fidelity for large batch sizes and even multiple sentencesif applied iteratively. We evaluate three defense methods: gradient pruning,DPSGD, and a simple approach to freeze word embeddings that we propose. We showthat both gradient pruning and DPSGD lead to a significant drop in utility.However, if we fine-tune a public pre-trained LM on private text withoutupdating word embeddings, it can effectively defend the attack with minimaldata utility loss. Together, we hope that our results can encourage thecommunity to rethink the privacy concerns of LM training and its standardpractices in the future.\r2022-10-17\nUMLsec4Edge: Extending UMLsec to model data-protection-compliant edge computing systems\nSven Smolka Jan Laufer Zoltán Ádám Mann Klaus Pohl\nabstract\rabstract: Edge computing enables the processing of data - frequently personal data - atthe edge of the network. For personal data, legislation such as the EuropeanGeneral Data Protection Regulation requires data protection by design. Hence,data protection has to be accounted for in the design of edge computing systemswhenever personal data is involved. This leads to specific requirements formodeling the architecture of edge computing systems, e.g., representation ofdata and network properties. To the best of our knowledge, no existing modeling language fulfils all theserequirements. In our previous work we showed that the commonly used UML profileUMLsec fulfils some of these requirements, and can thus serve as a startingpoint. The aim of this paper is to create a modeling language which meets allrequirements concerning the design of the architecture of edge computingsystems accounting for data protection. Thus, we extend UMLsec to satisfy allrequirements. We call the resulting UML profile UMLsec4Edge. We follow asystematic approach to develop UMLsec4Edge. We apply UMLsec4Edge to real-worlduse cases from different domains, and create appropriate deployment diagramsand class diagrams. These diagrams show UMLsec4Edge is capable of meeting therequirements.\r2022-10-13\nMitigating Unintended Memorization in Language Models via Alternating Teaching\nZhe Liu Xuedong Zhang Fuchun Peng\nabstract\rabstract: Recent research has shown that language models have a tendency to memorizerare or unique sequences in the training corpora which can thus leak sensitiveattributes of user data. We employ a teacher-student framework and propose anovel approach called alternating teaching to mitigate unintended memorizationin sequential modeling. In our method, multiple teachers are trained ondisjoint training sets whose privacy one wishes to protect, and teachers\u0026rsquo;predictions supervise the training of a student model in an alternating mannerat each time step. Experiments on LibriSpeech datasets show that the proposedmethod achieves superior privacy-preserving results than other counterparts. Incomparison with no prevention for unintended memorization, the overall utilityloss is small when training records are sufficient.\r2022-10-11\nPromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning\nZifeng Wang Jimeng Sun\nabstract\rabstract: Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) ischallenging due to privacy concerns, which hinders the use of ML for healthcareapplications. Synthetic EHRs generation bypasses the need to share sensitivereal patient records. However, existing methods generate single-modal EHRs byunconditional generation or by longitudinal inference, which falls short of lowflexibility and makes unrealistic EHRs. In this work, we propose to formulateEHRs generation as a text-to-text translation task by language models (LMs),which suffices to highly flexible event imputation during generation. We alsodesign prompt learning to control the generation conditioned by numerical andcategorical demographic features. We evaluate synthetic EHRs quality by twoperplexity measures accounting for their longitudinal pattern (longitudinalimputation perplexity, lpl) and the connections cross modalities(cross-modality imputation perplexity, mpl). Moreover, we utilize twoadversaries: membership and attribute inference attacks for privacy-preservingevaluation. Experiments on MIMIC-III data demonstrate the superiority of ourmethods on realistic EHRs generation (53.1% decrease of lpl and 45.3%decrease of mpl on average compared to the best baselines) with low privacyrisks. Software is available at https://github.com/RyanWangZf/PromptEHR.\rAbstract interpretation of Michelson smart-contracts\nGuillaume Bau Antoine Miné Vincent Botbol Mehdi Bouaziz\nabstract\rabstract: Static analysis of smart-contracts is becoming more widespread on blockchainplatforms. Analyzers rely on techniques like symbolic execution or modelchecking, but few of them can provide strong soundness properties and guaranteethe analysis termination at the same time. As smart-contracts often manipulateeconomic assets, proving numerical properties beyond the absence of runtimeerrors is also desirable. Smart-contract execution models differ considerablyfrom mainstream programming languages and vary from one blockchain to another,making state-of-the-art analyses hard to adapt. For instance, smart-contractcalls may modify a persistent storage impacting subsequent calls. This makes itdifficult for tools to infer invariants required to formally ensure the absenceof exploitable vulnerabilities. The Michelson smart-contract language, used inthe Tezos blockchain, is strongly typed, stack-based, and has a strictexecution model leaving few opportunities for implicit runtime errors. Wepresent a work in progress static analyzer for Michelson based on AbstractInterpretation and implemented within MOPSA, a modular static analyzer. Ourtool supports the Michelson semantic features, including inner calls toexternal contracts. It can prove the absence of runtime errors and inferinvariants on the persistent storage over an unbounded number of calls. It isalso being extended to prove high-level numerical and security properties. CCSConcepts: $\\bullet$ Security and privacy $\\rightarrow$ Logic and verification;$\\bullet$ Software and its engineering $\\rightarrow$ Automated static analysis.\r2022-10-09\nQuantifying Social Biases Using Templates is Unreliable\nPreethi Seshadri Pouya Pezeshkpour Sameer Singh\nabstract\rabstract: Recently, there has been an increase in efforts to understand how largelanguage models (LLMs) propagate and amplify social biases. Several works haveutilized templates for fairness evaluation, which allow researchers to quantifysocial biases in the absence of test sets with protected attribute labels.While template evaluation can be a convenient and helpful diagnostic tool tounderstand model deficiencies, it often uses a simplistic and limited set oftemplates. In this paper, we study whether bias measurements are sensitive tothe choice of templates used for benchmarking. Specifically, we investigate theinstability of bias measurements by manually modifying templates proposed inprevious works in a semantically-preserving manner and measuring bias acrossthese modifications. We find that bias values and resulting conclusions varyconsiderably across template modifications on four tasks, ranging from an 81%reduction (NLI) to a 162% increase (MLM) in (task-specific) bias measurements.Our results indicate that quantifying fairness in LLMs, as done in currentpractice, can be brittle and needs to be approached with more care and caution.\r2022-10-08\nProviding Insights for Open-Response Surveys via End-to-End Context-Aware Clustering\nSoheil Esmaeilzadeh Brian Williams Davood Shamsi Onar Vikingstad\nabstract\rabstract: Teachers often conduct surveys in order to collect data from a predefinedgroup of students to gain insights into topics of interest. When analyzingsurveys with open-ended textual responses, it is extremely time-consuming,labor-intensive, and difficult to manually process all the responses into aninsightful and comprehensive report. In the analysis step, traditionally, theteacher has to read each of the responses and decide on how to group them inorder to extract insightful information. Even though it is possible to groupthe responses only using certain keywords, such an approach would be limitedsince it not only fails to account for embedded contexts but also cannot detectpolysemous words or phrases and semantics that are not expressible in singlewords. In this work, we present a novel end-to-end context-aware framework thatextracts, aggregates, and abbreviates embedded semantic patterns inopen-response survey data. Our framework relies on a pre-trained naturallanguage model in order to encode the textual data into semantic vectors. Theencoded vectors then get clustered either into an optimally tuned number ofgroups or into a set of groups with pre-specified titles. In the former case,the clusters are then further analyzed to extract a representative set ofkeywords or summary sentences that serve as the labels of the clusters. In ourframework, for the designated clusters, we finally provide context-awarewordclouds that demonstrate the semantically prominent keywords within eachgroup. Honoring user privacy, we have successfully built the on-deviceimplementation of our framework suitable for real-time analysis on mobiledevices and have tested it on a synthetic dataset. Our framework reduces thecosts at-scale by automating the process of extracting the most insightfulinformation pieces from survey data.\r2022-10-04\nThe black hole interior from non-isometric codes and complexity\nChris Akers Netta Engelhardt Daniel Harlow Geoff Penington Shreya Vardhan\nabstract\rabstract: Quantum error correction has given us a natural language for the emergence ofspacetime, but the black hole interior poses a challenge for this framework: atlate times the apparent number of interior degrees of freedom in effectivefield theory can vastly exceed the true number of fundamental degrees offreedom, so there can be no isometric (i.e. inner-product preserving) encodingof the former into the latter. In this paper we explain how quantum errorcorrection nonetheless can be used to explain the emergence of the black holeinterior, via the idea of \u0026ldquo;non-isometric codes protected by computationalcomplexity\u0026rdquo;. We show that many previous ideas, such as the existence of a largenumber of \u0026ldquo;null states\u0026rdquo;, a breakdown of effective field theory for operationsof exponential complexity, the quantum extremal surface calculation of the Pagecurve, post-selection, \u0026ldquo;state-dependent/state-specific\u0026rdquo; operatorreconstruction, and the \u0026ldquo;simple entropy\u0026rdquo; approach to complexitycoarse-graining, all fit naturally into this framework, and we illustrate allof these phenomena simultaneously in a soluble model.\rDifferentially Private Bias-Term only Fine-tuning of Foundation Models\nZhiqi Bu Yu-Xiang Wang Sheng Zha George Karypis\nabstract\rabstract: We study the problem of differentially private (DP) fine-tuning of largepre-trained models \u0026ndash; a recent privacy-preserving approach suitable for solvingdownstream tasks with sensitive data. Existing work has demonstrated that highaccuracy is possible under strong privacy constraint, yet requires significantcomputational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), whichmatches the state-of-the-art accuracy for DP algorithms and the efficiency ofthe standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the networkarchitecture), parameter efficient (only training about $0.1%$ of theparameters), and computation efficient (almost removing the overhead caused byDP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiTis $2\\sim 30\\times$ faster and uses $2\\sim 8\\times$ less memory than DP fullfine-tuning, even faster than the standard full fine-tuning. This amazingefficiency enables us to conduct DP fine-tuning on language and vision taskswith long-sequence texts and high-resolution images, which were computationallydifficult using existing methods.\rAn Embarrassingly Simple Approach for Intellectual Property Rights Protection on Recurrent Neural Networks\nZhi Qin Tan Hao Shan Wong Chee Seng Chan\nabstract\rabstract: Capitalise on deep learning models, offering Natural Language Processing(NLP) solutions as a part of the Machine Learning as a Service (MLaaS) hasgenerated handsome revenues. At the same time, it is known that the creation ofthese lucrative deep models is non-trivial. Therefore, protecting theseinventions intellectual property rights (IPR) from being abused, stolen andplagiarized is vital. This paper proposes a practical approach for the IPRprotection on recurrent neural networks (RNN) without all the bells andwhistles of existing IPR solutions. Particularly, we introduce the Gatekeeperconcept that resembles the recurrent nature in RNN architecture to embed keys.Also, we design the model training scheme in a way such that the protected RNNmodel will retain its original performance iff a genuine key is presented.Extensive experiments showed that our protection scheme is robust and effectiveagainst ambiguity and removal attacks in both white-box and black-boxprotection schemes on different RNN variants. Code is available athttps://github.com/zhiqin1998/RecurrentIPR\r2022-09-28\nDoing data science with platforms crumbs: an investigation into fakes views on YouTube\nMaria Castaldo Paolo Frasca Tommaso Venturini Floriana Gargiulo\nabstract\rabstract: This paper contributes to the ongoing discussions on the scholarly access tosocial media data, discussing a case where this access is barred despite itsvalue for understanding and countering online disinformation and despite theabsence of privacy or copyright issues. Our study concerns YouTube\u0026rsquo;s engagementmetrics and, more specifically, the way in which the platform removes \u0026ldquo;fakeviews\u0026rdquo; (i.e., views considered as artificial or illegitimate by the platform).Working with one and a half year of data extracted from a thousand FrenchYouTube channels, we show the massive extent of this phenomenon, which concernsthe large majority of the channels and more than half the videos in our corpus.Our analysis indicates that most fakes news are corrected relatively late inthe life of the videos and that the final view counts of the videos are notindependent from the fake views they received. We discuss the potential harmthat delays in corrections could produce in content diffusion: by inflatingviews counts, illegitimate views could make a video appear more popular than itis and unwarrantedly encourage its human and algorithmic recommendation.Unfortunately, we cannot offer a definitive assessment of this phenomenon,because YouTube provides no information on fake views in its API or interface.This paper is, therefore, also a call for greater transparency by YouTube andother online platforms about information that can have crucial implications forthe quality of online public debate.\r2022-09-23\nFedVLN: Privacy-preserving Federated Vision-and-Language Navigation\nKaiwen Zhou Xin Eric Wang\nabstract\rabstract: Data privacy is a central problem for embodied agents that can perceive theenvironment, communicate with humans, and act in the real world. While helpinghumans complete tasks, the agent may observe and process sensitive informationof users, such as house environments, human activities, etc. In this work, weintroduce privacy-preserving embodied agent learning for the task ofVision-and-Language Navigation (VLN), where an embodied agent navigates houseenvironments by following natural language instructions. We view each houseenvironment as a local client, which shares nothing other than local updateswith the cloud server and other clients, and propose a novel federatedvision-and-language navigation (FedVLN) framework to protect data privacyduring both training and pre-exploration. Particularly, we propose adecentralized training strategy to limit the data of each client to its localmodel training and a federated pre-exploration method to do partial modelaggregation to improve model generalizability to unseen environments. Extensiveresults on R2R and RxR datasets show that under our FedVLN framework,decentralized VLN models achieve comparable results with centralized trainingwhile protecting seen environment privacy, and federated pre-explorationsignificantly outperforms centralized pre-exploration while preserving unseenenvironment privacy.\r2022-09-17\nTopological Quantum Programming in TED-K\nHisham Sati Urs Schreiber\nabstract\rabstract: While the realization of scalable quantum computation will arguably requiretopological stabilization and, with it, topological-hardware-aware quantumprogramming and topological-quantum circuit verification, the propercombination of these strategies into dedicated topological quantum programminglanguages has not yet received attention. Here we describe a fundamental andnatural scheme that we are developing, for typed functional (hence verifiable)topological quantum programming which is topological-hardware aware \u0026ndash; in thatit natively reflects the universal fine technical detail of topological q-bits,namely of symmetry-protected (or enhanced) topologically ordered Laughlin-typeanyon ground states in topological phases of quantum materials. What makes this work is: (1) our recent result that wavefunctions ofrealistic and technologically viable anyon species \u0026ndash; namely of su(2)-anyonssuch as the popular Majorana/Ising anyons but also of computationally universalFibonacci anyons \u0026ndash; are reflected in the twisted equivariant differential (TED)K-cohomology of configuration spaces of codimension=2 nodal defects in the hostmaterial\u0026rsquo;s crystallographic orbifold; (2) combined with our earlier observationthat such TED generalized cohomology theories on orbifolds interpretintuitionistically-dependent linear data types in cohesive homotopy type theory(HoTT), supporting a powerful modern form of modal quantum logic. In this short note we give an exposition of the basic ideas, a quick reviewof the underlying results and a brief indication of the basic languageconstructs for anyon braiding via TED-K in cohesive HoTT. The language systemis under development at the \u0026ldquo;Center for Quantum and Topological Systems\u0026rdquo; at theResearch Institute of NYU, Abu Dhabi.\r2022-09-16\nJaco: An Offline Running Privacy-aware Voice Assistant\nDaniel Bermuth Alexander Poeppel Wolfgang Reif\nabstract\rabstract: With the recent advance in speech technology, smart voice assistants havebeen improved and are now used by many people. But often these assistants arerunning online as a cloud service and are not always known for a goodprotection of users\u0026rsquo; privacy. This paper presents the architecture of a novelvoice assistant, called Jaco, with the following features: (a) It can runcompletely offline, even on low resource devices like a RaspberryPi. (b)Through a skill concept it can be easily extended. (c) The architectural focusis on protecting users\u0026rsquo; privacy, but without restricting capabilities fordevelopers. (d) It supports multiple languages. (e) It is competitive withother voice assistant solutions. In this respect the assistant combines andextends the advantages of other approaches.\r2022-09-15\nContent-Context Factorized Representations for Automated Speech Recognition\nDavid M. Chan Shalini Ghosh\nabstract\rabstract: Deep neural networks have largely demonstrated their ability to performautomated speech recognition (ASR) by extracting meaningful features from inputaudio frames. Such features, however, may consist not only of information aboutthe spoken language content, but also may contain information about unnecessarycontexts such as background noise and sounds or speaker identity, accent, orprotected attributes. Such information can directly harm generalizationperformance, by introducing spurious correlations between the spoken words andthe context in which such words were spoken. In this work, we introduce anunsupervised, encoder-agnostic method for factoring speech-encoderrepresentations into explicit content-encoding representations and spuriouscontext-encoding representations. By doing so, we demonstrate improvedperformance on standard ASR benchmarks, as well as improved performance in bothreal-world and artificially noisy ASR scenarios.\r2022-09-12\nSpanish Facebook Posts as an Indicator of COVID-19 Vaccine Hesitancy in Texas\nAna Aleksandric Henry Isaac Anderson Sarah Melcher Shirin Nilizadeh Gabriela Mustata Wilson\nabstract\rabstract: Vaccination represents a major public health intervention intended to protectagainst COVID-19 infections and hospitalizations. However, vaccine hesitancydue to misinformation/disinformation, especially among ethnic minority groups,negatively impacts the effectiveness of such an intervention. The aim of thestudy is to provide an understanding of how information gleaned from socialmedia can be used to improve attitudes towards vaccination and decrease vaccinehesitancy. This work focused on Spanish-language posts and will highlight therelationship between vaccination rates across different Texas counties and thesentiment and emotional content of Facebook data, the most popular platformamong the Hispanic population. The analysis of this valuable dataset indicatesthat vaccination rates among this minority group are negatively correlated withnegative sentiment and fear, meaning that the higher prevalence of negative andfearful posts reveals lower vaccination rates in these counties. This firststudy investigating vaccine hesitancy in the Hispanic population suggests thatsocial media listening can be a valuable tool for measuring attitudes towardpublic health interventions.\r2022-09-09\nSafety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment\nJie Zhu Leye Wang Xiao Han\nabstract\rabstract: The size of deep learning models in artificial intelligence (AI) software isincreasing rapidly, which hinders the large-scale deployment onresource-restricted devices (e.g., smartphones). To mitigate this issue, AIsoftware compression plays a crucial role, which aims to compress model sizewhile keeping high performance. However, the intrinsic defects in the big modelmay be inherited by the compressed one. Such defects may be easily leveraged byattackers, since the compressed models are usually deployed in a large numberof devices without adequate protection. In this paper, we try to address thesafe model compression problem from a safety-performance co-optimizationperspective. Specifically, inspired by the test-driven development (TDD)paradigm in software engineering, we propose a test-driven sparse trainingframework called SafeCompress. By simulating the attack mechanism as the safetytest, SafeCompress can automatically compress a big model to a small onefollowing the dynamic sparse training paradigm. Further, considering arepresentative attack, i.e., membership inference attack (MIA), we develop aconcrete safe model compression mechanism, called MIA-SafeCompress. Extensiveexperiments are conducted to evaluate MIA-SafeCompress on five datasets forboth computer vision and natural language processing tasks. The results verifythe effectiveness and generalization of our method. We also discuss how toadapt SafeCompress to other attacks besides MIA, demonstrating the flexibilityof SafeCompress.\r2022-09-08\nDifferentially Private Decoding in Large Language Models\nJimit Majmudar Christophe Dupuy Charith Peris Sami Smaili Rahul Gupta Richard Zemel\nabstract\rabstract: Recent large-scale natural language processing (NLP) systems use apre-trained Large Language Model (LLM) on massive and diverse corpora as aheadstart. In practice, the pre-trained model is adapted to a wide array oftasks via fine-tuning on task-specific datasets. LLMs, while effective, havebeen shown to memorize instances of training data thereby potentially revealingprivate information processed during pre-training. The potential leakage mightfurther propagate to the downstream tasks for which LLMs are fine-tuned. On theother hand, privacy-preserving algorithms usually involve retraining fromscratch, which is prohibitively expensive for LLMs. In this work, we propose asimple, easy to interpret, and computationally lightweight perturbationmechanism to be applied to an already trained model at the decoding stage. Ourperturbation mechanism is model-agnostic and can be used in conjunction withany LLM. We provide theoretical analysis showing that the proposed mechanism isdifferentially private, and experimental results showing a privacy-utilitytrade-off.\r2022-09-07\nSGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning\nEugenio Lomurno Alberto Archetti Lorenzo Cazzella Stefano Samele Leonardo Di Perna Matteo Matteucci\nabstract\rabstract: Privacy regulation laws, such as GDPR, impose transparency and security asdesign pillars for data processing algorithms. In this context, federatedlearning is one of the most influential frameworks for privacy-preservingdistributed machine learning, achieving astounding results in many naturallanguage processing and computer vision tasks. Several federated learningframeworks employ differential privacy to prevent private data leakage tounauthorized parties and malicious attackers. Many studies, however, highlightthe vulnerabilities of standard federated learning to poisoning and inference,thus raising concerns about potential risks for sensitive data. To address thisissue, we present SGDE, a generative data exchange protocol that improves usersecurity and machine learning performance in a cross-silo federation. The coreof SGDE is to share data generators with strong differential privacy guaranteestrained on private data instead of communicating explicit gradient information.These generators synthesize an arbitrarily large amount of data that retain thedistinctive features of private samples but differ substantially. In this work,SGDE is tested in a cross-silo federated network on images and tabulardatasets, exploiting beta-variational autoencoders as data generators. From theresults, the inclusion of SGDE turns out to improve task accuracy and fairness,as well as resilience to the most influential attacks on federated learning.\r2022-09-02\nDomain Adaptation from Scratch\nEyal Ben-David Yftah Ziser Roi Reichart\nabstract\rabstract: Natural language processing (NLP) algorithms are rapidly improving but oftenstruggle when applied to out-of-distribution examples. A prominent approach tomitigate the domain gap is domain adaptation, where a model trained on a sourcedomain is adapted to a new target domain. We present a new learning setup,``domain adaptation from scratch\u0026rsquo;\u0026rsquo;, which we believe to be crucial forextending the reach of NLP to sensitive domains in a privacy-preserving manner.In this setup, we aim to efficiently annotate data from a set of source domainssuch that the trained model performs well on a sensitive target domain fromwhich data is unavailable for annotation. Our study compares several approachesfor this challenging setup, ranging from data selection and domain adaptationalgorithms to active learning paradigms, on two NLP tasks: sentiment analysisand Named Entity Recognition. Our results suggest that using the abovementionedapproaches eases the domain gap, and combining them further improves theresults.\r2022-08-30\nMINERVAS: Massive INterior EnviRonments VirtuAl Synthesis\nHaocheng Ren Hao Zhang Jia Zheng Jiaxiang Zheng Rui Tang Yuchi Huo Hujun Bao Rui Wang\nabstract\rabstract: With the rapid development of data-driven techniques, data has played anessential role in various computer vision tasks. Many realistic and syntheticdatasets have been proposed to address different problems. However, there arelots of unresolved challenges: (1) the creation of dataset is usually a tediousprocess with manual annotations, (2) most datasets are only designed for asingle specific task, (3) the modification or randomization of the 3D scene isdifficult, and (4) the release of commercial 3D data may encounter copyrightissue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAlSynthesis system, to facilitate the 3D scene modification and the 2D imagesynthesis for various vision tasks. In particular, we design a programmablepipeline with Domain-Specific Language, allowing users to (1) select scenesfrom the commercial indoor scene database, (2) synthesize scenes for differenttasks with customized rules, and (3) render various imagery data, such asvisual color, geometric structures, semantic label. Our system eases thedifficulty of customizing massive numbers of scenes for different tasks andrelieves users from manipulating fine-grained scene configurations by providinguser-controllable randomness using multi-level samplers. Most importantly, itempowers users to access commercial scene databases with millions of indoorscenes and protects the copyright of core data assets, e.g., 3D CAD models. Wedemonstrate the validity and flexibility of our system by using our synthesizeddata to improve the performance on different kinds of computer vision tasks.\r2022-08-29\nNL2GDPR: Automatically Develop GDPR Compliant Android Application Features from Natural Language\nFaysal Hossain Shezan Yingjie Lao Minlong Peng Xin Wang Mingming Sun Ping Li\nabstract\rabstract: The recent privacy leakage incidences and the more strict policy regulationsdemand a much higher standard of compliance for companies and mobile apps.However, such obligations also impose significant challenges on app developersfor complying with these regulations that contain various perspectives,activities, and roles, especially for small companies and developers who areless experienced in this matter or with limited resources. To address thesehurdles, we develop an automatic tool, NL2GDPR, which can generate policiesfrom natural language descriptions from the developer while also ensuring theapp\u0026rsquo;s functionalities are compliant with General Data Protection Regulation(GDPR). NL2GDPR is developed by leveraging an information extraction tool, OIA(Open Information Annotation), developed by Baidu Cognitive Computing Lab. At the core, NL2GDPR is a privacy-centric information extraction model,appended with a GDPR policy finder and a policy generator. We perform acomprehensive study to grasp the challenges in extracting privacy-centricinformation and generating privacy policies, while exploiting optimizations forthis specific task. With NL2GDPR, we can achieve 92.9%, 95.2%, and 98.4%accuracy in correctly identifying GDPR policies related to personal datastorage, process, and share types, respectively. To the best of our knowledge,NL2GDPR is the first tool that allows a developer to automatically generateGDPR compliant policies, with only the need of entering the natural languagefor describing the app features. Note that other non-GDPR-related featuresmight be integrated with the generated features to build a complex app.\r2022-08-24\nFedIPR: Ownership Verification for Federated Deep Neural Network Models\nBowen Li Lixin Fan Hanlin Gu Jie Li Qiang Yang\nabstract\rabstract: Federated learning models are collaboratively developed upon valuabletraining data owned by multiple parties. During the development and deploymentof federated models, they are exposed to risks including illegal copying,re-distribution, misuse and/or free-riding. To address these risks, theownership verification of federated learning models is a prerequisite thatprotects federated learning model intellectual property rights (IPR) i.e.,FedIPR. We propose a novel federated deep neural network (FedDNN) ownershipverification scheme that allows private watermarks to be embedded and verifiedto claim legitimate IPR of FedDNN models. In the proposed scheme, each clientindependently verifies the existence of the model watermarks and claimsrespective ownership of the federated model without disclosing neither privatetraining data nor private watermark information. The effectiveness of embeddedwatermarks is theoretically justified by the rigorous analysis of conditionsunder which watermarks can be privately embedded and detected by multipleclients. Moreover, extensive experimental results on computer vision andnatural language processing tasks demonstrate that varying bit-lengthwatermarks can be embedded and reliably detected without compromising originalmodel performances. Our watermarking scheme is also resilient to variousfederated training settings and robust against removal attacks.\r2022-08-21\nAn Incentive-Compatible Mechanism for Decentralized Storage Network\nIman Vakilinia Weihong Wang Jiajun Xin\nabstract\rabstract: The dominance of a few big companies in the storage market arising variousconcerns including single point of failure, privacy violation, and oligopoly.To eliminate the dependency on such a centralized storage architecture, severalDecentralized Storage Network (DSN) schemes such as Filecoin, Sia, and Storjhave been introduced. DSNs leverage blockchain technology to create a storageplatform such that the micro storage providers can also participate in thestorage market. To verify the accurate data storage by the storage providersduring a storage contract, DSNs apply a Proof of Storage (PoS) scheme tocontinuously inspect the storage service. However, continuous verification ofthe storage provider imposes an extra cost to the network and thereforeend-users. Moreover, DSN\u0026rsquo;s PoS verification is vulnerable to a service denyingattack in which the storage provider submits valid PoS to the network whiledenying the service to the client. Considering the benefits and existing challenges of DSNs, this paperintroduces a novel incentive-compatible DSN scheme. In this scheme, the PoS isconducted only if the client submits a challenge request. We model the storageservice as a repeated dynamic game and set the players\u0026rsquo; payoffs such that thestorage provider\u0026rsquo;s dominant strategy is to honestly follow the storagecontract. Our proposed mechanism leverages the smart-contract and oraclenetwork to govern the storage agreement between the client and storage providerefficiently. Furthermore, our scheme is independent of a specific blockchainplatform but can be plugged into any blockchain platform with smart-contractexecution capability. As a proof of concept, we have implemented our schemeusing solidity language and chainlink oracle network. The performance analysisdemonstrates the applicability of our scheme.\rMockingBERT: A Method for Retroactively Adding Resilience to NLP Models\nJan Jezabek Akash Singh\nabstract\rabstract: Protecting NLP models against misspellings whether accidental or adversarialhas been the object of research interest for the past few years. Existingremediations have typically either compromised accuracy or required full modelre-training with each new class of attacks. We propose a novel method ofretroactively adding resilience to misspellings to transformer-based NLPmodels. This robustness can be achieved without the need for re-training of theoriginal NLP model and with only a minimal loss of language understandingperformance on inputs without misspellings. Additionally we propose a newefficient approximate method of generating adversarial misspellings, whichsignificantly reduces the cost needed to evaluate a model\u0026rsquo;s resilience toadversarial attacks.\r2022-08-19\nTo show or not to show: Redacting sensitive text from videos of electronic displays\nAbhishek Mukhopadhyay Shubham Agarwal Patrick Dylan Zwick Pradipta Biswas\nabstract\rabstract: With the increasing prevalence of video recordings there is a growing needfor tools that can maintain the privacy of those recorded. In this paper, wedefine an approach for redacting personally identifiable text from videos usinga combination of optical character recognition (OCR) and natural languageprocessing (NLP) techniques. We examine the relative performance of thisapproach when used with different OCR models, specifically Tesseract and theOCR system from Google Cloud Vision (GCV). For the proposed approach theperformance of GCV, in both accuracy and speed, is significantly higher thanTesseract. Finally, we explore the advantages and disadvantages of both modelsin real-world applications.\r2022-08-17\nDifferential Privacy in Natural Language Processing: The Story So Far\nOleksandra Klymenko Stephen Meisenbacher Florian Matthes\nabstract\rabstract: As the tide of Big Data continues to influence the landscape of NaturalLanguage Processing (NLP), the utilization of modern NLP methods has groundeditself in this data, in order to tackle a variety of text-based tasks. Thesemethods without a doubt can include private or otherwise personallyidentifiable information. As such, the question of privacy in NLP has gainedfervor in recent years, coinciding with the development of newPrivacy-Enhancing Technologies (PETs). Among these PETs, Differential Privacyboasts several desirable qualities in the conversation surrounding dataprivacy. Naturally, the question becomes whether Differential Privacy isapplicable in the largely unstructured realm of NLP. This topic has sparkednovel research, which is unified in one basic goal: how can one adaptDifferential Privacy to NLP methods? This paper aims to summarize thevulnerabilities addressed by Differential Privacy, the current thinking, andabove all, the crucial next steps that must be considered.\r2022-08-14\nSimply Logical \u0026ndash; Intelligent Reasoning by Example (Fully Interactive Online Edition)\nPeter Flach Kacper Sokol\nabstract\rabstract: \u0026ldquo;Simply Logical \u0026ndash; Intelligent Reasoning by Example\u0026rdquo; by Peter Flach was firstpublished by John Wiley in 1994. It could be purchased as book-only or with a3.5 inch diskette containing the SWI-Prolog programmes printed in the book (forvarious operating systems). In 2007 the copyright reverted back to the authorat which point the book and programmes were made freely available online; theprint version is no longer distributed through John Wiley publishers. In 2015,as a pilot, we ported most of the original book into an online, interactivewebsite using SWI-Prolog\u0026rsquo;s SWISH platform. Since then, we launched the SimplyLogical open source organisation committed to maintaining a suite of freelyavailable interactive online educational resources about ArtificialIntelligence and Logic Programming with Prolog. With the advent of neweducational technologies we were inspired to rebuild the book from the groundup using the Jupyter Book platform enhanced with a collection of bespokeplugins that implement, among other things, interactive SWI-Prolog code blocksthat can be executed directly in a web browser. This new version is moremodular, easier to maintain, and can be split into custom teaching modules, inaddition to being modern-looking, visually appealing, and compatible with arange of (mobile) devices of varying screen sizes.\r2022-08-11\nSearching for chromate replacements using natural language processing and machine learning algorithms\nShujing Zhao Nick Birbilis\nabstract\rabstract: The past few years has seen the application of machine learning utilised inthe exploration of new materials. As in many fields of research - the vastmajority of knowledge is published as text, which poses challenges in either aconsolidated or statistical analysis across studies and reports. Suchchallenges include the inability to extract quantitative information, and inaccessing the breadth of non-numerical information. To address this issue, theapplication of natural language processing (NLP) has been explored in severalstudies to date. In NLP, assignment of high-dimensional vectors, known asembeddings, to passages of text preserves the syntactic and semanticrelationship between words. Embeddings rely on machine learning algorithms andin the present work, we have employed the Word2Vec model, previously exploredby others, and the BERT model - applying them towards a unique challenge inmaterials engineering. That challenge is the search for chromate replacementsin the field of corrosion protection. From a database of over 80 millionrecords, a down-selection of 5990 papers focused on the topic of corrosionprotection were examined using NLP. This study demonstrates it is possible toextract knowledge from the automated interpretation of the scientificliterature and achieve expert human level insights.\r2022-08-03\nA Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval\nAlex Falcon Giuseppe Serra Oswald Lanz\nabstract\rabstract: Every hour, huge amounts of visual contents are posted on social media anduser-generated content platforms. To find relevant videos by means of a naturallanguage query, text-video retrieval methods have received increased attentionover the past few years. Data augmentation techniques were introduced toincrease the performance on unseen test examples by creating new trainingsamples with the application of semantics-preserving techniques, such as colorspace or geometric transformations on images. Yet, these techniques are usuallyapplied on raw data, leading to more resource-demanding solutions and alsorequiring the shareability of the raw data, which may not always be true, e.g.copyright issues with clips from movies or TV series. To address thisshortcoming, we propose a multimodal data augmentation technique which works inthe feature space and creates new videos and captions by mixing semanticallysimilar samples. We experiment our solution on a large scale public dataset,EPIC-Kitchens-100, and achieve considerable improvements over a baselinemethod, improved state-of-the-art performance, while at the same timeperforming multiple ablation studies. We release code and pretrained models onGithub at https://github.com/aranciokov/FSMMDA_VideoRetrieval.\r2022-08-01\nFederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning\nZhen Wang Weirui Kuang Yuexiang Xie Liuyi Yao Yaliang Li Bolin Ding Jingren Zhou\nabstract\rabstract: The incredible development of federated learning (FL) has benefited varioustasks in the domains of computer vision and natural language processing, andthe existing frameworks such as TFF and FATE has made the deployment easy inreal-world applications. However, federated graph learning (FGL), even thoughgraph data are prevalent, has not been well supported due to its uniquecharacteristics and requirements. The lack of FGL-related framework increasesthe efforts for accomplishing reproducible research and deploying in real-worldapplications. Motivated by such strong demand, in this paper, we first discussthe challenges in creating an easy-to-use FGL package and accordingly presentour implemented package FederatedScope-GNN (FS-G), which provides (1) a unifiedview for modularizing and expressing FGL algorithms; (2) comprehensive DataZooand ModelZoo for out-of-the-box FGL capability; (3) an efficient modelauto-tuning component; and (4) off-the-shelf privacy attack and defenseabilities. We validate the effectiveness of FS-G by conducting extensiveexperiments, which simultaneously gains many valuable insights about FGL forthe community. Moreover, we employ FS-G to serve the FGL application inreal-world E-commerce scenarios, where the attained improvements indicate greatpotential business benefits. We publicly release FS-G, as submodules ofFederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL\u0026rsquo;sresearch and enable broad applications that would otherwise be infeasible dueto the lack of a dedicated package.\r2022-07-29\nBlade: A Blockchain-supported Architecture for Decentralized Services\nSebastian Göndör Hakan Yildiz Martin Westerkamp Axel Küpper\nabstract\rabstract: Decentralized services and applications provide a multitude of advantages fortheir users, such as improved privacy, control, and independence from thirdparties. Anyhow, decentralization comes at the cost of certain disadvantages,such as increased application complexity or communication overhead. Thisaggravates the development and deployment of decentralized services andapplications. In this paper we present Blade, a software platform that aims toease the effort of development, deployment, and administration of decentralizedservices by implementing reusable solutions for recurring challenges developersare facing when designing decentralized service architectures. This includesfunctionality for e.g. identity management, access control, request handling,verification of authenticity and integrity, discovery, or routing. Bladeimplements all this functionality in a Blade server instance, which can bedeployed on a lightweight device, such as a NAS, Raspberry Pi, or router athome. This allows users without expert knowledge to run a Blade instance withalready existing hardware with little overhead. Blade supports polyglot Blademodules that implement extended functionality, such as interfaces, frontends,and business logic of decentralized applications, e.g. a decentralized instantmessaging service or an online social network. Based on the Oracle GraalVM,Blade modules can be implemented in a variety of programming languages andutilize the functionality provided by the Blade server instance. Blade modulesare published in a Ethereum-based decentralized marketplace from where they canbe installed directly via the Blade instances\u0026hellip;\r2022-07-28\nExploiting and Defending Against the Approximate Linearity of Apple\u0026rsquo;s NeuralHash\nJagdeep Singh Bhatia Kevin Meng\nabstract\rabstract: Perceptual hashes map images with identical semantic content to the same$n$-bit hash value, while mapping semantically-different images to differenthashes. These algorithms carry important applications in cybersecurity such ascopyright infringement detection, content fingerprinting, and surveillance.Apple\u0026rsquo;s NeuralHash is one such system that aims to detect the presence ofillegal content on users\u0026rsquo; devices without compromising consumer privacy. Wemake the surprising discovery that NeuralHash is approximately linear, whichinspires the development of novel black-box attacks that can (i) evadedetection of \u0026ldquo;illegal\u0026rdquo; images, (ii) generate near-collisions, and (iii) leakinformation about hashed images, all without access to model parameters. Thesevulnerabilities pose serious threats to NeuralHash\u0026rsquo;s security goals; to addressthem, we propose a simple fix using classical cryptographic standards.\r2022-07-23\nCatch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter\nDilara Dogan Bahadir Altun Muhammed Said Zengin Mucahid Kutlu Tamer Elsayed\nabstract\rabstract: The recent advances in natural language processing have yielded many excitingdevelopments in text analysis and language understanding models; however, thesemodels can also be used to track people, bringing severe privacy concerns. Inthis work, we investigate what individuals can do to avoid being detected bythose models while using social media platforms. We ground our investigation intwo exposure-risky tasks, stance detection and geotagging. We explore a varietyof simple techniques for modifying text, such as inserting typos in salientwords, paraphrasing, and adding dummy social media posts. Our experiments showthat the performance of BERT-based models fined tuned for stance detectiondecreases significantly due to typos, but it is not affected by paraphrasing.Moreover, we find that typos have minimal impact on state-of-the-art geotaggingmodels due to their increased reliance on social networks; however, we showthat users can deceive those models by interacting with different users,reducing their performance by almost 50%.\r2022-07-20\nA Large-Scale Dataset of Twitter Chatter about Online Learning during the Current COVID-19 Omicron Wave\nNirmalya Thakur\nabstract\rabstract: The COVID-19 Omicron variant, reported to be the most immune evasive variantof COVID-19, is resulting in a surge of COVID-19 cases globally. This hascaused schools, colleges, and universities in different parts of the world totransition to online learning. As a result, social media platforms such asTwitter are seeing an increase in conversations related to online learning inthe form of tweets. Mining such tweets to develop a dataset can serve as a dataresource for different applications and use-cases related to the analysis ofinterest, views, opinions, perspectives, attitudes, and feedback towards onlinelearning during the current surge of COVID-19 cases caused by the Omicronvariant. Therefore, this work presents a large-scale open-access Twitterdataset of conversations about online learning from different parts of theworld since the first detected case of the COVID-19 Omicron variant in November2021. The dataset is compliant with the privacy policy, developer agreement,and guidelines for content redistribution of Twitter, as well as with the FAIRprinciples (Findability, Accessibility, Interoperability, and Reusability)principles for scientific data management. The paper also briefly outlines somepotential applications in the fields of Big Data, Data Mining, Natural LanguageProcessing, and their related disciplines, with a specific focus on onlinelearning during this Omicron wave that may be studied, explored, andinvestigated by using this dataset.\r2022-07-18\nTraining Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices\nMingbin Xu Congzheng Song Ye Tian Neha Agrawal Filip Granqvist Rogier van Dalen Xiao Zhang Arturo Argueta Shiyi Han Yaqiao Deng Leo Liu Anmol Walia Alex Jin\nabstract\rabstract: Federated Learning (FL) is a technique to train models using data distributedacross devices. Differential Privacy (DP) provides a formal privacy guaranteefor sensitive data. Our goal is to train a large neural network language model(NNLM) on compute-constrained devices while preserving privacy using FL and DP.However, the DP-noise introduced to the model increases as the model sizegrows, which often prevents convergence. We propose Partial Embedding Updates(PEU), a novel technique to decrease noise by decreasing payload size.Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise ContrastiveEstimation (NCE) to reduce the memory demands of large models oncompute-constrained devices. This combination of techniques makes it possibleto train large-vocabulary language models while preserving accuracy andprivacy.\r2022-07-17\nLearning with Recoverable Forgetting\nJingwen Ye Yifang Fu Jie Song Xingyi Yang Songhua Liu Xin Jin Mingli Song Xinchao Wang\nabstract\rabstract: Life-long learning aims at learning a sequence of tasks without forgettingthe previously acquired knowledge. However, the involved training data may notbe life-long legitimate due to privacy or copyright reasons. In practicalscenarios, for instance, the model owner may wish to enable or disable theknowledge of specific tasks or specific samples from time to time. Suchflexible control over knowledge transfer, unfortunately, has been largelyoverlooked in previous incremental or decremental learning methods, even at aproblem-setup level. In this paper, we explore a novel learning scheme, termedas Learning wIth Recoverable Forgetting (LIRF), that explicitly handles thetask- or sample-specific knowledge removal and recovery. Specifically, LIRFbrings in two innovative schemes, namely knowledge deposit and withdrawal,which allow for isolating user-designated knowledge from a pre-trained networkand injecting it back when necessary. During the knowledge deposit process, thespecified knowledge is extracted from the target network and stored in adeposit module, while the insensitive or general knowledge of the targetnetwork is preserved and further augmented. During knowledge withdrawal, thetaken-off knowledge is added back to the target network. The deposit andwithdraw processes only demand for a few epochs of finetuning on the removaldata, ensuring both data and time efficiency. We conduct experiments on severaldatasets, and demonstrate that the proposed LIRF strategy yields encouragingresults with gratifying generalization capability.\r2022-07-16\nSelective Differential Privacy for Language Modeling\nWeiyan Shi Aiqi Cui Evan Li Ruoxi Jia Zhou Yu\nabstract\rabstract: With the increasing applications of language models, it has become crucial toprotect these models from leaking private information. Previous work hasattempted to tackle this challenge by training RNN-based language models withdifferential privacy guarantees. However, applying classical differentialprivacy to language models leads to poor model performance as the underlyingprivacy notion is over-pessimistic and provides undifferentiated protection forall tokens in the data. Given that the private information in natural languageis sparse (for example, the bulk of an email might not carry personallyidentifiable information), we propose a new privacy notion, selectivedifferential privacy, to provide rigorous privacy guarantees on the sensitiveportion of the data to improve model utility. To realize such a new notion, wedevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-basedlanguage models. Besides language modeling, we also apply the method to a moreconcrete application\u0026ndash;dialog systems. Experiments on both language modeling anddialog system building show that the proposed privacy-preserving mechanismachieves better utilities while remaining safe under various privacy attackscompared to the baselines. The data and code are released athttps://github.com/wyshi/lm_privacy to facilitate future research .\r2022-07-14\nDifferentially Private Fine-tuning of Language Models\nDa Yu Saurabh Naik Arturs Backurs Sivakanth Gopi Huseyin A. Inan Gautam Kamath Janardhan Kulkarni Yin Tat Lee Andre Manoel Lukas Wutschitz Sergey Yekhanin Huishuai Zhang\nabstract\rabstract: We give simpler, sparser, and faster algorithms for differentially privatefine-tuning of large-scale pre-trained language models, which achieve thestate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.We propose a meta-framework for this problem, inspired by the recent success ofhighly parameter-efficient methods for fine-tuning. Our experiments show thatdifferentially private adaptations of these approaches outperform previousprivate algorithms in three important dimensions: utility, privacy, and thecomputational and memory cost of private training. On many commonly studieddatasets, the utility of private models approaches that of non-private models.For example, on the MNLI dataset we achieve an accuracy of $87.8%$ usingRoBERTa-Large and $83.5%$ using RoBERTa-Base with a privacy budget of$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Largeachieves an accuracy of $90.2%$. Our findings are similar for natural languagegeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8respectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas thenon-private baseline is $48.1$. All our experiments suggest that larger modelsare better suited for private fine-tuning: while they are well known to achievesuperior accuracy non-privately, we find that they also better maintain theiraccuracy when privacy is introduced.\r2022-07-13\nRepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping\nPatrik Joslin Kenfack Kamil Sabbagh Adín Ramírez Rivera Adil Khan\nabstract\rabstract: Fairness has become an essential problem in many domains of Machine Learning(ML), such as classification, natural language processing, and GenerativeAdversarial Networks (GANs). In this research effort, we study the unfairnessof GANs. We formally define a new fairness notion for generative models interms of the distribution of generated samples sharing the same protectedattributes (gender, race, etc.). The defined fairness notion (representationalfairness) requires the distribution of the sensitive attributes at the testtime to be uniform, and, in particular for GAN model, we show that thisfairness notion is violated even when the dataset contains equally representedgroups, i.e., the generator favors generating one group of samples over theothers at the test time. In this work, we shed light on the source of thisrepresentation bias in GANs along with a straightforward method to overcomethis problem. We first show on two widely used datasets (MNIST, SVHN) that whenthe norm of the gradient of one group is more important than the other duringthe discriminator\u0026rsquo;s training, the generator favours sampling data from onegroup more than the other at test time. We then show that controlling thegroups\u0026rsquo; gradient norm by performing group-wise gradient norm clipping in thediscriminator during the training leads to a more fair data generation in termsof representational fairness compared to existing models while preserving thequality of generated samples.\r2022-07-10\nDeveloping an NLP-based Recommender System for the Ethical, Legal, and Social Implications of Synthetic Biology\nDamien Dablain Lilian Huang Brandon Sepulvado\nabstract\rabstract: Synthetic biology is an emerging field that involves the engineering andre-design of organisms for purposes such as food security, health, andenvironmental protection. As such, it poses numerous ethical, legal, and socialimplications (ELSI) for researchers and policy makers. Various efforts toensure socially responsible synthetic biology are underway. Policy making isone regulatory avenue, and other initiatives have sought to embed socialscientists and ethicists on synthetic biology projects. However, given thenascency of synthetic biology, the number of heterogeneous domains it spans,and the open nature of many ethical questions, it has proven challenging toestablish widespread concrete policies, and including social scientists andethicists on synthetic biology teams has met with mixed success. This text proposes a different approach, asking instead is it possible todevelop a well-performing recommender model based upon natural languageprocessing (NLP) to connect synthetic biologists with information on the ELSIof their specific research? This recommender was developed as part of a largerproject building a Synthetic Biology Knowledge System (SBKS) to acceleratediscovery and exploration of the synthetic biology design space. Our approachaims to distill for synthetic biologists relevant ethical and social scientificinformation and embed it into synthetic biology research workflows.\r2022-07-06\nCommunication Analysis through Visual Analytics: Current Practices, Challenges, and New Frontiers\nMaximilian T. Fischer Frederik L. Dennig Daniel Seebacher Daniel A. Keim Mennatallah El-Assady\nabstract\rabstract: The automated analysis of digital human communication data often focuses onspecific aspects such as content or network structure in isolation. This canprovide limited perspectives while making cross-methodological analyses,occurring in domains like investigative journalism, difficult. Communicationresearch in psychology and the digital humanities instead stresses theimportance of a holistic approach to overcome these limiting factors. In thiswork, we conduct an extensive survey on the properties of over fortysemi-automated communication analysis systems and investigate how they coverconcepts described in theoretical communication research. From theseinvestigations, we derive a design space and contribute a conceptual frameworkbased on communication research, technical considerations, and the surveyedapproaches. The framework describes the systems\u0026rsquo; properties, capabilities, andcomposition through a wide range of criteria organized in the dimensions (1)Data, (2) Processing and Models, (3) Visual Interface, and (4) KnowledgeGeneration. These criteria enable a formalization of digital communicationanalysis through visual analytics, which, we argue, is uniquely suited for thistask by tackling automation complexity while leveraging domain knowledge. Withour framework, we identify shortcomings and research challenges, such as groupcommunication dynamics, trust and privacy considerations, and holisticapproaches. Simultaneously, our framework supports the evaluation of systemsand promotes the mutual exchange between researchers through a structuredcommon language, laying the foundations for future research on communicationanalysis.\r2022-07-05\nFederated Phish Bowl: LSTM-Based Decentralized Phishing Email Detection\nYuwei Sun Ng Chong Hideya Ochiai\nabstract\rabstract: With increasingly more sophisticated phishing campaigns in recent years,phishing emails lure people using more legitimate-looking personal contexts. Totackle this problem, instead of traditional heuristics-based algorithms, moreadaptive detection systems such as natural language processing (NLP)-poweredapproaches are essential to understanding phishing text representations.Nevertheless, concerns surrounding the collection of phishing data that mightcover confidential information hinder the effectiveness of model learning. Wepropose a decentralized phishing email detection framework called FederatedPhish Bowl (FedPB) which facilitates collaborative phishing detection withprivacy. In particular, we devise a knowledge-sharing mechanism with federatedlearning (FL). Using long short-term memory (LSTM) for phishing detection, theframework adapts by sharing a global word embedding matrix across the clients,with each client running its local model with Non-IID data. We collected themost recent phishing samples to study the effectiveness of the proposed methodusing different client numbers and data distributions. The results show thatFedPB can attain a competitive performance with a centralized phishingdetector, with generality to various cases of FL retaining a predictionaccuracy of 83%.\r2022-07-01\nThe Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization\nIldikó Pilán Pierre Lison Lilja Øvrelid Anthi Papadopoulou David Sánchez Montserrat Batet\nabstract\rabstract: We present a novel benchmark and associated evaluation metrics for assessingthe performance of text anonymization methods. Text anonymization, defined asthe task of editing a text document to prevent the disclosure of personalinformation, currently suffers from a shortage of privacy-oriented annotatedtext resources, making it difficult to properly evaluate the level of privacyprotection offered by various anonymization methods. This paper presents TAB(Text Anonymization Benchmark), a new, open-source annotated corpus developedto address this shortage. The corpus comprises 1,268 English-language courtcases from the European Court of Human Rights (ECHR) enriched withcomprehensive annotations about the personal information appearing in eachdocument, including their semantic category, identifier type, confidentialattributes, and co-reference relations. Compared to previous work, the TABcorpus is designed to go beyond traditional de-identification (which is limitedto the detection of predefined semantic categories), and explicitly marks whichtext spans ought to be masked in order to conceal the identity of the person tobe protected. Along with presenting the corpus and its annotation layers, wealso propose a set of evaluation metrics that are specifically tailored towardsmeasuring the performance of text anonymization, both in terms of privacyprotection and utility preservation. We illustrate the use of the benchmark andthe proposed metrics by assessing the empirical performance of several baselinetext anonymization models. The full corpus along with its privacy-orientedannotation guidelines, evaluation scripts and baseline models are available on:https://github.com/NorskRegnesentral/text-anonymisation-benchmark\r2022-06-28\nThe NLP Sandbox: an efficient model-to-data system to enable federated and unbiased evaluation of clinical NLP models\nYao Yan Thomas Yu Kathleen Muenzen Sijia Liu Connor Boyle George Koslowski Jiaxin Zheng Nicholas Dobbins Clement Essien Hongfang Liu Larsson Omberg Meliha Yestigen Bradley Taylor James A Eddy Justin Guinney Sean Mooney Thomas Schaffter\nabstract\rabstract: Objective The evaluation of natural language processing (NLP) models forclinical text de-identification relies on the availability of clinical notes,which is often restricted due to privacy concerns. The NLP Sandbox is anapproach for alleviating the lack of data and evaluation frameworks for NLPmodels by adopting a federated, model-to-data approach. This enables unbiasedfederated model evaluation without the need for sharing sensitive data frommultiple institutions. Materials and Methods We leveraged the Synapsecollaborative framework, containerization software, and OpenAPI generator tobuild the NLP Sandbox (nlpsandbox.io). We evaluated two state-of-the-art NLPde-identification focused annotation models, Philter and NeuroNER, using datafrom three institutions. We further validated model performance using data froman external validation site. Results We demonstrated the usefulness of the NLPSandbox through de-identification clinical model evaluation. The externaldeveloper was able to incorporate their model into the NLP Sandbox template andprovide user experience feedback. Discussion We demonstrated the feasibility ofusing the NLP Sandbox to conduct a multi-site evaluation of clinical textde-identification models without the sharing of data. Standardized model anddata schemas enable smooth model transfer and implementation. To generalize theNLP Sandbox, work is required on the part of data owners and model developersto develop suitable and standardized schemas and to adapt their data or modelto fit the schemas. Conclusions The NLP Sandbox lowers the barrier to utilizingclinical data for NLP model evaluation and facilitates federated, multi-site,unbiased evaluation of NLP models.\rDetecting Unintended Memorization in Language-Model-Fused ASR\nW. Ronny Huang Steve Chien Om Thakkar Rajiv Mathews\nabstract\rabstract: End-to-end (E2E) models are often being accompanied by language models (LMs)via shallow fusion for boosting their overall quality as well as recognition ofrare words. At the same time, several prior works show that LMs are susceptibleto unintentionally memorizing rare or unique sequences in the training data. Inthis work, we design a framework for detecting memorization of random textualsequences (which we call canaries) in the LM training data when one has onlyblack-box (query) access to LM-fused speech recognizer, as opposed to directaccess to the LM. On a production-grade Conformer RNN-T E2E model fused with aTransformer LM, we show that detecting memorization of singly-occurringcanaries from the LM training data of 300M examples is possible. Motivated toprotect privacy, we also show that such memorization gets significantly reducedby per-example gradient-clipped LM training without compromising overallquality.\rNegDL: Privacy-Preserving Deep Learning Based on Negative Database\nDongdong Zhao Pingchuan Zhang Jianwen Xiang Jing Tian\nabstract\rabstract: In the era of big data, deep learning has become an increasingly populartopic. It has outstanding achievements in the fields of image recognition,object detection, and natural language processing et al. The first priority ofdeep learning is exploiting valuable information from a large amount of data,which will inevitably induce privacy issues that are worthy of attention.Presently, several privacy-preserving deep learning methods have been proposed,but most of them suffer from a non-negligible degradation of either efficiencyor accuracy. Negative database (\\textit{NDB}) is a new type of datarepresentation which can protect data privacy by storing and utilizing thecomplementary form of original data. In this paper, we propose aprivacy-preserving deep learning method named NegDL based on \\textit{NDB}.Specifically, private data are first converted to \\textit{NDB} as the input ofdeep learning models by a generation algorithm called \\textit{QK}-hiddenalgorithm, and then the sketches of \\textit{NDB} are extracted for training andinference. We demonstrate that the computational complexity of NegDL is thesame as the original deep learning model without privacy protection.Experimental results on Breast Cancer, MNIST, and CIFAR-10 benchmark datasetsdemonstrate that the accuracy of NegDL could be comparable to the original deeplearning model in most cases, and it performs better than the method based ondifferential privacy.\r2022-06-25\nTEVR: Improving Speech Recognition by Token Entropy Variance Reduction\nHajo Nils Krabbenhöft Erhardt Barth\nabstract\rabstract: This paper presents TEVR, a speech recognition model designed to minimize thevariation in token entropy w.r.t. to the language model. This takes advantageof the fact that if the language model will reliably and accurately predict atoken anyway, then the acoustic model doesn\u0026rsquo;t need to be accurate inrecognizing it. We train German ASR models with 900 million parameters and showthat on CommonVoice German, TEVR scores a very competitive 3.64% word errorrate, which outperforms the best reported results by a relative 16.89%reduction in word error rate. We hope that releasing our fully trained speechrecognition pipeline to the community will lead to privacy-preserving offlinevirtual assistants in the future.\r2022-06-23\nProvably Confidential Language Modelling\nXuandong Zhao Lei Li Yu-Xiang Wang\nabstract\rabstract: Large language models are shown to memorize privacy information such associal security numbers in training data. Given the sheer scale of the trainingcorpus, it is challenging to screen and filter these privacy data, eithermanually or automatically. In this paper, we propose Confidentially RedactedTraining (CRT), a method to train language generation models while protectingthe confidential segments. We borrow ideas from differential privacy (whichsolves a related but distinct problem) and show that our method is able toprovably prevent unintended memorization by randomizing parts of the trainingprocess. Moreover, we show that redaction with an approximately correctscreening policy amplifies the confidentiality guarantee. We implement themethod for both LSTM and GPT language models. Our experimental results showthat the models trained by CRT obtain almost the same perplexity whilepreserving strong confidentiality.\rVeHIF: An Accessible Vegetation High-Impedance Fault Data Set Format\nDouglas P. S. Gomes Cagil Ozansoy\nabstract\rabstract: High-impedance faults are a challenging problem in power distributionsystems. They often do not trigger protection devices and can result in serioushazards such as igniting fires when in contact with vegetation. The currentresearch field dedicated to studying these faults is extensive but suffers froma constraining bottleneck of a lack of real experimental data. Many works setto detect and localize such faults rely on high-impedance fault low-fidelitymodels, and the lack of public data sets makes it impractical to have objectiveperformance benchmarks. This letter describes and proposes a format for a dataset of more than 900 vegetation high-impedance faults funded by the VictorianGovernment in Australia recorded in high-sampling resolution. The original dataset is public, but it was made available through an obscure format that limitsits accessibility. The presented format in this letter uses the standardhierarchical data format (HDF5), which makes it easily accessible in manylanguages such as MATLAB, Python, C++, and more. The data set compiler andvisualizer script are also provided in the work repository.\r2022-06-22\nEnhancing Networking Cipher Algorithms with Natural Language\nJohn E. Ortega\nabstract\rabstract: This work provides a survey of several networking cipher algorithms andproposes a method for integrating natural language processing (NLP) as aprotective agent for them. Two main proposals are covered for the use of NLP innetworking. First, NLP is considered as the weakest link in a networkingencryption model; and, second, as a hefty deterrent when combined as an extralayer over what could be considered a strong type of encryption \u0026ndash; the streamcipher. This paper summarizes how languages can be integrated into symmetricencryption as a way to assist in the encryption of vulnerable streams that maybe found under attack due to the natural frequency distribution of letters orwords in a local language stream.\r2022-06-18\nReplacing Labeled Real-image Datasets with Auto-generated Contours\nHirokatsu Kataoka Ryo Hayamizu Ryosuke Yamada Kodai Nakashima Sora Takashima Xinyu Zhang Edgar Josafat Martinez-Noriega Nakamasa Inoue Rio Yokota\nabstract\rabstract: In the present work, we show that the performance of formula-drivensupervised learning (FDSL) can match or even exceed that of ImageNet-21kwithout the use of real images, human-, and self-supervision during thepre-training of Vision Transformers (ViTs). For example, ViT-Base pre-trainedon ImageNet-21k shows 81.8% top-1 accuracy when fine-tuned on ImageNet-1k andFDSL shows 82.7% top-1 accuracy when pre-trained under the same conditions(number of images, hyperparameters, and number of epochs). Images generated byformulas avoid the privacy/copyright issues, labeling cost and errors, andbiases that real images suffer from, and thus have tremendous potential forpre-training general models. To understand the performance of the syntheticimages, we tested two hypotheses, namely (i) object contours are what matter inFDSL datasets and (ii) increased number of parameters to create labels affectsperformance improvement in FDSL pre-training. To test the former hypothesis, weconstructed a dataset that consisted of simple object contour combinations. Wefound that this dataset can match the performance of fractals. For the latterhypothesis, we found that increasing the difficulty of the pre-training taskgenerally leads to better fine-tuning accuracy.\r2022-06-16\nBenchmarking Differential Privacy and Federated Learning for BERT Models\nPriyam Basu Tiasa Singha Roy Rakshit Naidu Zumrut Muftuoglu Sahib Singh Fatemehsadat Mireshghallah\nabstract\rabstract: Natural Language Processing (NLP) techniques can be applied to help with thediagnosis of medical conditions such as depression, using a collection of aperson\u0026rsquo;s utterances. Depression is a serious medical illness that can haveadverse effects on how one feels, thinks, and acts, which can lead to emotionaland physical problems. Due to the sensitive nature of such data, privacymeasures need to be taken for handling and training models with such data. Inthis work, we study the effects that the application of Differential Privacy(DP) has, in both a centralized and a Federated Learning (FL) setup, ontraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).We offer insights on how to privately train NLP models and what architecturesand setups provide more desirable privacy utility trade-offs. We envisage thiswork to be used in future healthcare and mental health studies to keep medicalhistory private. Therefore, we provide an open-source implementation of thiswork.\rOn the Protected Spectrum of the Minimal Argyres-Douglas Theory\nChinmaya Bhargava Matthew Buican Hongliang Jiang\nabstract\rabstract: Despite the power of supersymmetry, finding exact closed-form expressions forthe protected operator spectra of interacting superconformal field theories(SCFTs) is difficult. In this paper, we take a step towards a solution for the\u0026quot;simplest\u0026quot; interacting 4D $\\mathcal{N}=2$ SCFT: the minimal Argyres-Douglas(MAD) theory. We present two results that go beyond the well-understood Coulombbranch and Schur sectors. First, we find the exact closed-form spectrum ofmultiplets containing operators that are chiral with respect to any$\\mathcal{N}=1\\subset\\mathcal{N}=2$ superconformal subalgebra. We argue thatthis \u0026ldquo;full\u0026rdquo; chiral sector (FCS) is as simple as allowed by unitarity for atheory with a Coulomb branch and that, up to a rescaling of $U(1)r$ quantumnumbers and the vanishing of a finite number of states, the MAD FCS isisospectral to the FCS of the free $\\mathcal{N}=2$ Abelian gauge theory. In thelanguage of superconformal representation theory, this leaves only the spectrumof the poorly understood $\\bar{\\mathcal{C}}{R,r(j,\\bar j)}$ multiplets to bedetermined. Our second result sheds light on these observables: we find anexact closed-form answer for the number of $\\bar{\\mathcal{C}}_{0,r(j,0)}$multiplets, for any $r$ and $j$, in the MAD theory. We argue that thissub-sector is also as simple as allowed by unitarity for a theory with aCoulomb branch and that there is a natural map to the corresponding sector ofthe free $\\mathcal{N}=2$ Abelian gauge theory. These results motivate aconjecture on the full local operator algebra of the MAD theory.\r2022-06-15\nPrivate Language Model Adaptation for Speech Recognition\nZhe Liu Ke Li Shreyan Bakshi Fuchun Peng\nabstract\rabstract: Speech model adaptation is crucial to handle the discrepancy betweenserver-side proxy training data and actual data received on local devices ofusers. With the use of federated learning (FL), we introduce an efficientapproach on continuously adapting neural network language models (NNLMs) onprivate devices with applications on automatic speech recognition (ASR). Toaddress the potential speech transcription errors in the on-device trainingcorpus, we perform empirical studies on comparing various strategies ofleveraging token-level confidence scores to improve the NNLM quality in the FLsettings. Experiments show that compared with no model adaptation, the proposedmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on twospeech evaluation datasets, respectively. We also provide analysis inevaluating privacy guarantees of our presented procedure.\r2022-06-14\nSymmetry-Protected Infinite-Temperature Quantum Memory from Subsystem Codes\nJulia Wildeboer Thomas Iadecola Dominic J. Williamson\nabstract\rabstract: We study a mechanism whereby quantum information present in the initial stateof a quantum many-body system can be protected for arbitrary times due to acombination of symmetry and spatial locality. Remarkably, the mechanism issufficiently generic that the dynamics can be fully ergodic upon resolving theprotecting symmetry and fixing the encoded quantum state, resulting in aninfinite-temperature quantum memory. After exemplifying the mechanism in astrongly nonintegrable two-dimensional (2D) spin model inspired by the surfacecode, we find it has a natural interpretation in the language of noiselesssubsystems and stabilizer subsystem codes. This interpretation yields a numberof further examples, including a nonintegrable Hamiltonian with quantum memorybased on the Bacon-Shor code. The lifetime of the encoded quantum informationin these models is infinite provided the dynamics respect the stabilizersymmetry of the underlying subsystem code. In the presence ofsymmetry-violating perturbations, we make contact with previous work leveragingthe concept of prethermalization to show that the encoded quantum informationretains a parametrically long lifetime under dynamics with an enlargedcontinuous symmetry group. We identify conditions on the underlying subsystemcode that enable such a prethermal enhancement of the memory lifetime.\rFreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models\nYinpeng Guo Liangyou Li Xin Jiang Qun Liu\nabstract\rabstract: Cross-lingual transfer (CLT) is of various applications. However, labeledcross-lingual corpus is expensive or even inaccessible, especially in thefields where labels are private, such as diagnostic results of symptoms inmedicine and user profiles in business. Nevertheless, there are off-the-shelfmodels in these sensitive fields. Instead of pursuing the original labels, aworkaround for CLT is to transfer knowledge from the off-the-shelf modelswithout labels. To this end, we define a novel CLT problem named FreeTransfer-Xthat aims to achieve knowledge transfer from the off-the-shelf models inrich-resource languages. To address the problem, we propose a 2-step knowledgedistillation (KD, Hinton et al., 2015) framework based on multilingualpre-trained language models (mPLM). The significant improvement over strongneural machine translation (NMT) baselines demonstrates the effectiveness ofthe proposed method. In addition to reducing annotation cost and protectingprivate labels, the proposed method is compatible with different networks andeasy to be deployed. Finally, a range of analyses indicate the great potentialof the proposed method.\r2022-06-13\nConsent verification monitoring\nMarco Robol Travis D. Breaux Elda Paja Paolo Giorgini\nabstract\rabstract: Advances in service personalization are driven by low-cost data collectionand processing, in addition to the wide variety of third-party frameworks forauthentication, storage, and marketing. New privacy regulations, such as theGeneral Data Protection Regulation (GDPR) and the California Consumer PrivacyAct (CCPA), increasingly require organizations to explicitly state their datapractices in privacy policies. When data practices change, a new version of thepolicy is released. This can occur a few times a year, when data collection orprocessing requirements are rapidly changing. Consent evolution raises specificchallenges to ensuring GDPR compliance. We propose a formal consent frameworkto support organizations, data users and data subjects in their understandingof policy evolution under a consent regime that supports both the retroactiveand non-retroactive granting and withdrawal of consent. The contributionsinclude: (i) a formal framework to reason about data collection and accessunder multiple consent granting and revocation scenarios; (ii) a scriptinglanguage that implements the consent framework for encoding and executingdifferent scenarios; (iii) five consent evolution use cases that illustrate howorganizations would evolve their policies using this framework; and (iv) ascalability evaluation of the reasoning framework. The framework models areused to verify when user consent prevents or detects unauthorized datacollection and access. The framework can be integrated into a runtimearchitecture to monitor policy violations as data practices evolve inreal-time. The framework was evaluated using the five use cases and asimulation to measure the framework scalability. The simulation results showthat the approach is computationally scalable for use in runtime consentmonitoring under a standard model of data collection and access, and practiceand policy evolution.\r2022-06-12\nEvolutionary Multi-Task Injection Testing on Web Application Firewalls\nKe Li Heng Yang Willem Visser\nabstract\rabstract: Web application firewall (WAF) plays an integral role nowadays to protect webapplications from various malicious injection attacks such as SQL injection,XML injection, and PHP injection, to name a few. However, given the evolvingsophistication of injection attacks and the increasing complexity of tuning aWAF, it is challenging to ensure that the WAF is free of injectionvulnerabilities such that it will block all malicious injection attacks withoutwrongly affecting the legitimate message. Automatically testing the WAF is,therefore, a timely and essential task. In this paper, we propose DaNuoYi, anautomatic injection testing tool that simultaneously generates test inputs formultiple types of injection attacks on a WAF. Our basic idea derives from thecross-lingual translation in the natural language processing domain. Inparticular, test inputs for different types of injection attacks aresyntactically different but may be semantically similar. Sharing semanticknowledge across multiple programming languages can thus stimulate thegeneration of more sophisticated test inputs and discovering injectionvulnerabilities of the WAF that are otherwise difficult to find. To this end,in DaNuoYi, we train several injection translation models by using multi-tasklearning that translates the test inputs between any pair of injection attacks.The model is then used by a novel multi-task evolutionary algorithm toco-evolve test inputs for different types of injection attacks facilitated by ashared mating pool and domain-specific mutation operators at each generation.We conduct experiments on three real-world open-source WAFs and six types ofinjection attacks, the results reveal that DaNuoYi generates up to 3.8x and5.78x more valid test inputs (i.e., bypassing the underlying WAF) than itsstate-of-the-art single-task counterparts and the context-free grammar-basedinjection construction.\r2022-06-09\nPrivacy Leakage in Text Classification: A Data Extraction Approach\nAdel Elmahdy Huseyin A. Inan Robert Sim\nabstract\rabstract: Recent work has demonstrated the successful extraction of training data fromgenerative language models. However, it is not evident whether such extractionis feasible in text classification models since the training objective is topredict the class label as opposed to next-word prediction. This poses aninteresting challenge and raises an important question regarding the privacy oftraining data in text classification settings. Therefore, we study thepotential privacy leakage in the text classification domain by investigatingthe problem of unintended memorization of training data that is not pertinentto the learning task. We propose an algorithm to extract missing tokens of apartial text by exploiting the likelihood of the class label provided by themodel. We test the effectiveness of our algorithm by inserting canaries intothe training set and attempting to extract tokens in these canariespost-training. In our experiments, we demonstrate that successful extraction ispossible to some extent. This can also be used as an auditing strategy toassess any potential unauthorized use of personal data without consent.\r2022-06-07\nMarvolo: Programmatic Data Augmentation for Practical ML-Driven Malware Detection\nMichael D. Wong Edward Raff James Holt Ravi Netravali\nabstract\rabstract: Data augmentation has been rare in the cyber security domain due to technicaldifficulties in altering data in a manner that is semantically consistent withthe original data. This shortfall is particularly onerous given the uniquedifficulty of acquiring benign and malicious training data that runs intocopyright restrictions, and that institutions like banks and governmentsreceive targeted malware that will never exist in large quantities. We presentMARVOLO, a binary mutator that programmatically grows malware (and benign)datasets in a manner that boosts the accuracy of ML-driven malware detectors.MARVOLO employs semantics-preserving code transformations that mimic thealterations that malware authors and defensive benign developers routinely makein practice , allowing us to generate meaningful augmented data. Crucially,semantics-preserving transformations also enable MARVOLO to safely propagatelabels from original to newly-generated data samples without mandatingexpensive reverse engineering of binaries. Further, MARVOLO embeds several keyoptimizations that keep costs low for practitioners by maximizing the densityof diverse data samples generated within a given time (or resource) budget.Experiments using wide-ranging commercial malware datasets and a recentML-driven malware detector show that MARVOLO boosts accuracies by up to 5%,while operating on only a small fraction (15%) of the potential input binaries.\r2022-06-06\nPretrained Models for Multilingual Federated Learning\nOrion Weller Marc Marone Vladimir Braverman Dawn Lawrie Benjamin Van Durme\nabstract\rabstract: Since the advent of Federated Learning (FL), research has applied thesemethods to natural language processing (NLP) tasks. Despite a plethora ofpapers in FL for NLP, no previous works have studied how multilingual textimpacts FL algorithms. Furthermore, multilingual text provides an interestingavenue to examine the impact of non-IID text (e.g. different languages) on FLin naturally occurring data. We explore three multilingual language tasks,language modeling, machine translation, and text classification using differingfederated and non-federated learning algorithms. Our results show that usingpretrained models reduces the negative effects of FL, helping them to performnear or better than centralized (no privacy) learning, even when using non-IIDpartitioning.\r2022-06-03\nDifferentially Private Model Compression\nFatemehsadat Mireshghallah Arturs Backurs Huseyin A Inan Lukas Wutschitz Janardhan Kulkarni\nabstract\rabstract: Recent papers have shown that large pre-trained language models (LLMs) suchas BERT, GPT-2 can be fine-tuned on private data to achieve performancecomparable to non-private models for many downstream Natural LanguageProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.The inference cost of these models \u0026ndash; which consist of hundreds of millions ofparameters \u0026ndash; however, can be prohibitively large. Hence, often in practice,LLMs are compressed before they are deployed in specific applications. In thispaper, we initiate the study of differentially private model compression andpropose frameworks for achieving 50% sparsity levels while maintaining nearlyfull performance. We demonstrate these ideas on standard GLUE benchmarks usingBERT models, setting benchmarks for future research on this topic.\rFederating and querying heterogeneous and distributed Web APIs and triple stores\nTarcisio Mendes de Farias Christophe Dessimoz Aaron Ayllon Benitez Chen Yang Jiao Long Ana-Claudia Sima\nabstract\rabstract: Today\u0026rsquo;s international corporations such as BASF, a leading company in thecrop protection industry, produce and consume more and more data that are oftenfragmented and accessible through Web APIs. In addition, part of theproprietary and public data of BASF\u0026rsquo;s interest are stored in triple stores andaccessible with the SPARQL query language. Homogenizing the data access modesand the underlying semantics of the data without modifying or replicating theoriginal data sources become important requirements to achieve data integrationand interoperability. In this work, we propose a federated data integrationarchitecture within an industrial setup, that relies on an ontology-based dataaccess method. Our performance evaluation in terms of query response timeshowed that most queries can be answered in under 1 second.\r2022-06-02\nSkillBot: Identifying Risky Content for Children in Alexa Skills\nTu Le Danny Yuxing Huang Noah Apthorpe Yuan Tian\nabstract\rabstract: Many households include children who use voice personal assistants (VPA) suchas Amazon Alexa. Children benefit from the rich functionalities of VPAs andthird-party apps but are also exposed to new risks in the VPA ecosystem. Inthis paper, we first investigate \u0026ldquo;risky\u0026rdquo; child-directed voice apps that containinappropriate content or ask for personal information through voiceinteractions. We build SkillBot - a natural language processing (NLP)-basedsystem to automatically interact with VPA apps and analyze the resultingconversations. We find 28 risky child-directed apps and maintain a growingdataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexaapps. Our findings suggest that although child-directed VPA apps are subject tostricter policy requirements and more intensive vetting, children remainvulnerable to inappropriate content and privacy violations. We then conduct auser study showing that parents are concerned about the identified risky apps.Many parents do not believe that these apps are available and designed forfamilies/kids, although these apps are actually published in Amazon\u0026rsquo;s \u0026ldquo;Kids\u0026quot;product category. We also find that parents often neglect basic precautionssuch as enabling parental controls on Alexa devices. Finally, we identify anovel risk in the VPA ecosystem: confounding utterances, or voice commandsshared by multiple apps that may cause a user to interact with a different appthan intended. We identify 4,487 confounding utterances, including 581 sharedby child-directed and non-child-directed apps. We find that 27% of theseconfounding utterances prioritize invoking a non-child-directed app over achild-directed app. This indicates that children are at real risk ofaccidentally invoking non-child-directed apps due to confounding utterances.\rTHE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption\nTianyu Chen Hangbo Bao Shaohan Huang Li Dong Binxing Jiao Daxin Jiang Haoyi Zhou Jianxin Li Furu Wei\nabstract\rabstract: As more and more pre-trained language models adopt on-cloud deployment, theprivacy issues grow quickly, mainly for the exposure of plain-text user data(e.g., search history, medical record, bank account). Privacy-preservinginference of transformer models is on the demand of cloud service users. Toprotect privacy, it is an attractive choice to compute only with ciphertext inhomomorphic encryption (HE). However, enabling pre-trained models inference onciphertext data is difficult due to the complex computations in transformerblocks, which are not supported by current HE tools yet. In this work, weintroduce $\\textit{THE-X}$, an approximation approach for transformers, whichenables privacy-preserving inference of pre-trained models developed by popularframeworks. $\\textit{THE-X}$ proposes a workflow to deal with complexcomputation in transformer networks, including all the non-polynomial functionslike GELU, softmax, and LayerNorm. Experiments reveal our proposed$\\textit{THE-X}$ can enable transformer inference on encrypted data fordifferent downstream tasks, all with negligible performance drop but enjoyingthe theory-guaranteed privacy-preserving advantage.\r2022-05-30\nSecuring AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\nRucha Shinde Shruti Patil Ketan Kotecha Vidyasagar Potdar Ganeshsree Selvachandran Ajith Abraham\nabstract\rabstract: Healthcare systems are increasingly incorporating Artificial Intelligenceinto their systems, but it is not a solution for all difficulties. AI\u0026rsquo;sextraordinary potential is being held back by challenges such as a lack ofmedical datasets for training AI models, adversarial attacks, and a lack oftrust due to its black box working style. We explored how blockchain technologycan improve the reliability and trustworthiness of AI-based healthcare. Thispaper has conducted a Systematic Literature Review to explore thestate-of-the-art research studies conducted in healthcare applicationsdeveloped with different AI techniques and Blockchain Technology. Thissystematic literature review proceeds with three different paths as naturallanguage processing-based healthcare systems, computer vision-based healthcaresystems and acoustic AI-based healthcare systems. We found that 1) Defencetechniques for adversarial attacks on AI are available for specific kind ofattacks and even adversarial training is AI based technique which in furtherprone to different attacks. 2) Blockchain can address security and privacyissues in healthcare fraternity. 3) Medical data verification and userprovenance can be enabled with Blockchain. 4) Blockchain can protectdistributed learning on heterogeneous medical data. 5) The issues like singlepoint of failure, non-transparency in healthcare systems can be resolved withBlockchain. Nevertheless, it has been identified that research is at theinitial stage. As a result, we have synthesized a conceptual framework usingBlockchain Technology for AI-based healthcare applications that considers theneeds of each NLP, Computer Vision, and Acoustic AI application. A globalsolution for all sort of adversarial attacks on AI based healthcare. However,this technique has significant limits and challenges that need to be addressedin future studies.\r2022-05-29\nCPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\nYirong Chen Weiquan Fan Xiaofen Xing Jianxin Pang Minlie Huang Wenjing Han Qianfeng Tie Xiangmin Xu\nabstract\rabstract: Human language expression is based on the subjective construal of thesituation instead of the objective truth conditions, which means that speakers\u0026rsquo;personalities and emotions after cognitive processing have an importantinfluence on conversation. However, most existing datasets for conversationalAI ignore human personalities and emotions, or only consider part of them. It\u0026rsquo;sdifficult for dialogue systems to understand speakers\u0026rsquo; personalities andemotions although large-scale pre-training language models have been widelyused. In order to consider both personalities and emotions in the process ofconversation generation, we propose CPED, a large-scale Chinese personalizedand emotional dialogue dataset, which consists of multi-source knowledgerelated to empathy and personal characteristic. These knowledge covers gender,Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPEDcontains more than 12K dialogues of 392 speakers from 40 TV shows. We releasethe textual dataset with audio features and video features according to thecopyright claims, privacy issues, terms of service of video platforms. Weprovide detailed description of the CPED construction process and introducethree tasks for conversational AI, including personality recognition, emotionrecognition in conversations as well as personalized and emotional conversationgeneration. Finally, we provide baseline systems for these tasks and considerthe function of speakers\u0026rsquo; personalities and emotions on conversation. Ourmotivation is to propose a dataset to be widely adopted by the NLP community asa new open benchmark for conversational AI research. The full dataset isavailable at https://github.com/scutcyr/CPED.\r2022-05-26\nFederated Split BERT for Heterogeneous Text Classification\nZhengyang Li Shijing Si Jianzong Wang Jing Xiao\nabstract\rabstract: Pre-trained BERT models have achieved impressive performance in many naturallanguage processing (NLP) tasks. However, in many real-world situations,textual data are usually decentralized over many clients and unable to beuploaded to a central server due to privacy protection and regulations.Federated learning (FL) enables multiple clients collaboratively to train aglobal model while keeping the local data privacy. A few researches haveinvestigated BERT in federated learning setting, but the problem of performanceloss caused by heterogeneous (e.g., non-IID) data over clients remainunder-explored. To address this issue, we propose a framework, FedSplitBERT,which handles heterogeneous data and decreases the communication cost bysplitting the BERT encoder layers into local part and global part. The localpart parameters are trained by the local client only while the global partparameters are trained by aggregating gradients of multiple clients. Due to thesheer size of BERT, we explore a quantization method to further reduce thecommunication cost with minimal performance loss. Our framework is ready-to-useand compatible to many existing federated learning algorithms, includingFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of theproposed framework, which outperforms baseline methods by a significant margin,while FedSplitBERT with quantization can reduce the communication cost by$11.9\\times$.\rLeveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks\nDivyam Goel Raksha Sharma\nabstract\rabstract: The last few years have witnessed an exponential rise in the propagation ofoffensive text on social media. Identification of this text with high precisionis crucial for the well-being of society. Most of the existing approaches tendto give high toxicity scores to innocuous statements (e.g., \u0026ldquo;I am a gay man\u0026rdquo;).These false positives result from over-generalization on the training datawhere specific terms in the statement may have been used in a pejorative sense(e.g., \u0026ldquo;gay\u0026rdquo;). Emphasis on such words alone can lead to discrimination againstthe classes these systems are designed to protect. In this paper, we addressthe problem of offensive language detection on Twitter, while also detectingthe type and the target of the offence. We propose a novel approach calledSyLSTM, which integrates syntactic features in the form of the dependency parsetree of a sentence and semantic features in the form of word embeddings into adeep learning architecture using a Graph Convolutional Network. Results showthat the proposed approach significantly outperforms the state-of-the-art BERTmodel with orders of magnitude fewer number of parameters.\r2022-05-20\nHow to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing\nSamuel Sousa Roman Kern\nabstract\rabstract: Deep learning (DL) models for natural language processing (NLP) tasks oftenhandle private data, demanding protection against breaches and disclosures.Data protection laws, such as the European Union\u0026rsquo;s General Data ProtectionRegulation (GDPR), thereby enforce the need for privacy. Although manyprivacy-preserving NLP methods have been proposed in recent years, nocategories to organize them have been introduced yet, making it hard to followthe progress of the literature. To close this gap, this article systematicallyreviews over sixty DL methods for privacy-preserving NLP published between 2016and 2020, covering theoretical foundations, privacy-enhancing technologies, andanalysis of their suitability for real-world scenarios. First, we introduce anovel taxonomy for classifying the existing methods into three categories: datasafeguarding methods, trusted methods, and verification methods. Second, wepresent an extensive summary of privacy threats, datasets for applications, andmetrics for privacy evaluation. Third, throughout the review, we describeprivacy issues in the NLP pipeline in a holistic view. Further, we discuss openchallenges in privacy-preserving NLP regarding data traceability, computationoverhead, dataset size, the prevalence of human biases in embeddings, and theprivacy-utility tradeoff. Finally, this review presents future researchdirections to guide successive research and development of privacy-preservingNLP models.\r2022-05-19\nNebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\nYang Xiang Zhihua Wu Weibao Gong Siyu Ding Xianjie Mo Yuang Liu Shuohuan Wang Peng Liu Yongshuai Hou Long Li Bin Wang Shaohuai Shi Yaqian Han Yue Yu Ge Li Yu Sun Yanjun Ma Dianhai Yu\nabstract\rabstract: The ever-growing model size and scale of compute have attracted increasinginterests in training deep learning models over multiple nodes. However, whenit comes to training on cloud clusters, especially across remote clusters, hugechallenges are faced. In this work, we introduce a general framework, Nebula-I,for collaboratively training deep learning models over remote heterogeneousclusters, the connections between which are low-bandwidth wide area networks(WANs). We took natural language processing (NLP) as an example to show howNebula-I works in different training phases that include: a) pre-training amultilingual language model using two remote clusters; and b) fine-tuning amachine translation model using knowledge distilled from pre-trained models,which run through the most popular paradigm of recent deep learning. To balancethe accuracy and communication efficiency, in Nebula-I, parameter-efficienttraining strategies, hybrid parallel computing methods and adaptivecommunication acceleration techniques are jointly applied. Meanwhile, securitystrategies are employed to guarantee the safety, reliability and privacy inintra-cluster computation and inter-cluster communication. Nebula-I isimplemented with the PaddlePaddle deep learning framework, which can supportcollaborative training over heterogeneous hardware, e.g. GPU and NPU.Experiments demonstrate that the proposed framework could substantiallymaximize the training efficiency while preserving satisfactory NLP performance.By using Nebula-I, users can run large-scale training tasks over cloud clusterswith minimum developments, and the utility of existed large pre-trained modelscould be further promoted. We also introduced new state-of-the-art results oncross-lingual natural language inference tasks, which are generated based upona novel learning framework and Nebula-I.\rTwenty-two years since revealing cross-site scripting attacks: a systematic mapping and a comprehensive survey\nAbdelhakim Hannousse Salima Yahiouche Mohamed Cherif Nait-Hamoud\nabstract\rabstract: Cross-site scripting (XSS) is one of the major threats menacing the privacyof data and the navigation of trusted web applications. Since its reveal inlate 1999 by Microsoft security engineers, several techniques have beendeveloped in the aim to secure web navigation and protect web applicationsagainst XSS attacks. The problem became worse with the emergence of advancedweb technologies such as Web services and APIs and new programming styles suchas AJAX, CSS3 and HTML5. While new technologies enable complex interactions anddata exchanges between clients and servers in the network, new programmingstyles introduce new and complicate injection flaws to web applications. XSShas been and still in the TOP 10 list of web vulnerabilities reported by theOpen Web Applications Security Project (OWASP). Consequently, handling XSSattacks became one of the major concerns of several web security communities.In this paper, we contribute by conducting a systematic mapping and acomprehensive survey. We summarize and categorize existent endeavors that aimto protect against XSS attacks and develop XSS-free web applications. Thepresent review covers 147 high quality published studies since 1999 includingearly publications of 2022. A comprehensive taxonomy is drawn out describingthe different techniques used to prevent, detect, protect and defend againstXSS attacks. Although the diversity of XSS attack types and the scriptinglanguages that can be used to state them, the systematic mapping revealed aremarkable bias toward basic and JavaScript XSS attacks and a dearth ofvulnerability repair mechanisms. The survey highlighted the limitations,discussed the potentials of existing XSS attack defense mechanisms andidentified potential gaps.\r2022-05-18\nAddressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation\nKevin Yang Olivia Deng Charles Chen Richard Shin Subhro Roy Benjamin Van Durme\nabstract\rabstract: We introduce a novel setup for low-resource task-oriented semantic parsingwhich incorporates several constraints that may arise in real-world scenarios:(1) lack of similar datasets/models from a related domain, (2) inability tosample useful logical forms directly from a grammar, and (3) privacyrequirements for unlabeled natural utterances. Our goal is to improve alow-resource semantic parser using utterances collected through userinteractions. In this highly challenging but realistic setting, we investigatedata augmentation approaches involving generating a set of structured canonicalutterances corresponding to logical forms, before simulating correspondingnatural language and filtering the resulting pairs. We find that suchapproaches are effective despite our restrictive setup: in a low-resourcesetting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), weobserve 33% relative improvement over a non-data-augmented baseline in top-1match.\r2022-05-17\nTranslatotron 2: High-quality direct speech-to-speech translation with voice preservation\nYe Jia Michelle Tadmor Ramanovich Tal Remez Roi Pomerantz\nabstract\rabstract: We present Translatotron 2, a neural direct speech-to-speech translationmodel that can be trained end-to-end. Translatotron 2 consists of a speechencoder, a linguistic decoder, an acoustic synthesizer, and a single attentionmodule that connects them together. Experimental results on three datasetsconsistently show that Translatotron 2 outperforms the original Translatotronby a large margin on both translation quality (up to +15.5 BLEU) and speechgeneration quality, and approaches the same of cascade systems. In addition, wepropose a simple method for preserving speakers\u0026rsquo; voices from the source speechto the translation speech in a different language. Unlike existing approaches,the proposed method is able to preserve each speaker\u0026rsquo;s voice on speaker turnswithout requiring for speaker segmentation. Furthermore, compared to existingapproaches, it better preserves speaker\u0026rsquo;s privacy and mitigates potentialmisuse of voice cloning for creating spoofing audio artifacts.\rFederated learning for violence incident prediction in a simulated cross-institutional psychiatric setting\nThomas Borger Pablo Mosteiro Heysem Kaya Emil Rijcken Albert Ali Salah Floortje Scheepers Marco Spruit\nabstract\rabstract: Inpatient violence is a common and severe problem within psychiatry. Knowingwho might become violent can influence staffing levels and mitigate severity.Predictive machine learning models can assess each patient\u0026rsquo;s likelihood ofbecoming violent based on clinical notes. Yet, while machine learning modelsbenefit from having more data, data availability is limited as hospitalstypically do not share their data for privacy preservation. Federated Learning(FL) can overcome the problem of data limitation by training models in adecentralised manner, without disclosing data between collaborators. However,although several FL approaches exist, none of these train Natural LanguageProcessing models on clinical notes. In this work, we investigate theapplication of Federated Learning to clinical Natural Language Processing,applied to the task of Violence Risk Assessment by simulating across-institutional psychiatric setting. We train and compare four models: twolocal models, a federated model and a data-centralised model. Our resultsindicate that the federated model outperforms the local models and has similarperformance as the data-centralised model. These findings suggest thatFederated Learning can be used successfully in a cross-institutional settingand is a step towards new applications of Federated Learning based on clinicalnotes\r2022-05-14\nBalancing out Bias: Achieving Fairness Through Balanced Training\nXudong Han Timothy Baldwin Trevor Cohn\nabstract\rabstract: Group bias in natural language processing tasks manifests as disparities insystem error rates across texts authorized by different demographic groups,typically disadvantaging minority groups. Dataset balancing has been shown tobe effective at mitigating bias, however existing approaches do not directlyaccount for correlations between author demographics and linguistic variables,limiting their effectiveness. To achieve Equal Opportunity fairness, such asequal job opportunity without regard to demographics, this paper introduces asimple, but highly effective, objective for countering bias using balancedtraining. We extend the method in the form of a gated model, which incorporatesprotected attributes as input, and show that it is effective at reducing biasin predictions through demographic input perturbation, outperforming all otherbias mitigation techniques when combined with balanced training.\r2022-05-11\nProteção intelectual de obras produzidas por sistemas baseados em inteligência artificial: uma visão tecnicista sobre o tema\nFábio Manoel França Lobato\nabstract\rabstract: The pervasiveness of Artificial Intelligence (AI) is unquestionable in oursociety. Even in the arts, AI is present. A notorious case is the song \u0026ldquo;HeyYa!\u0026rdquo; of the OutKast group, successful in the 2000s. At this time, the musicindustry began to make decisions based on data to strategize based onpredictions of listeners\u0026rsquo; habits. This case is just one of the countlessexamples of AI applications in the arts. The advent of deep learning made itpossible to build systems capable of accurately recognizing artistic style inpaintings. Content generation is also possible; for example, Deepart customizesimages from two \\textit{inputs}: 1) an image to be customized; 2) a style ofpainting. The generation of songs according to specific styles from AI-basedsystems is also possible. Such possibilities raise questions about theintellectual property of such works. On this occasion, who owns the copyrightof a work produced from a system based on Artificial Intelligence? To thecreator of the AI? The company/corporation that subsidized the development ofthis system? Or AI itself as a creator? This essay aims to contribute with atechnicist view on the discussion of copyright applicability from worksproduced by AI.\r2022-05-10\nSentence-level Privacy for Document Embeddings\nCasey Meehan Khalil Mrini Kamalika Chaudhuri\nabstract\rabstract: User language data can contain highly sensitive personal content. As such, itis imperative to offer users a strong and interpretable privacy guarantee whenlearning from their data. In this work, we propose SentDP: pure localdifferential privacy at the sentence level for a single user document. Wepropose a novel technique, DeepCandidate, that combines concepts from robuststatistics and language modeling to produce high-dimensional, general-purpose$\\epsilon$-SentDP document embeddings. This guarantees that any single sentencein a document can be substituted with any other sentence while keeping theembedding $\\epsilon$-indistinguishable. Our experiments indicate that theseprivate document embeddings are useful for downstream tasks like sentimentanalysis and topic classification and even outperform baseline methods withweaker guarantees like word-level Metric DP.\r2022-05-08\nA Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges\nZhenghua Chen Min Wu Alvin Chan Xiaoli Li Yew-Soon Ong\nabstract\rabstract: Artificial Intelligence (AI) is a fast-growing research and development (R\u0026amp;D)discipline which is attracting increasing attention because of its promises tobring vast benefits for consumers and businesses, with considerable benefitspromised in productivity growth and innovation. To date it has reportedsignificant accomplishments in many areas that have been deemed as challengingfor machines, ranging from computer vision, natural language processing, audioanalysis to smart sensing and many others. The technical trend in realizing thesuccesses has been towards increasing complex and large size AI models so as tosolve more complex problems at superior performance and robustness. This rapidprogress, however, has taken place at the expense of substantial environmentalcosts and resources. Besides, debates on the societal impacts of AI, such asfairness, safety and privacy, have continued to grow in intensity. These issueshave presented major concerns pertaining to the sustainable development of AI.In this work, we review major trends in machine learning approaches that canaddress the sustainability problem of AI. Specifically, we examine emerging AImethodologies and algorithms for addressing the sustainability issue of AI intwo major aspects, i.e., environmental sustainability and social sustainabilityof AI. We will also highlight the major limitations of existing studies andpropose potential research challenges and directions for the development ofnext generation of sustainable AI techniques. We believe that this technicalreview can help to promote a sustainable development of AI R\u0026amp;D activities forthe research community.\r2022-05-06\nFedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks\nBill Yuchen Lin Chaoyang He Zihang Zeng Hulin Wang Yufen Huang Christophe Dupuy Rahul Gupta Mahdi Soltanolkotabi Xiang Ren Salman Avestimehr\nabstract\rabstract: Increasing concerns and regulations about data privacy and sparsitynecessitate the study of privacy-preserving, decentralized learning methods fornatural language processing (NLP) tasks. Federated learning (FL) providespromising approaches for a large number of clients (e.g., personal devices ororganizations) to collaboratively learn a shared global model to benefit allclients while allowing users to keep their data locally. Despite interest instudying FL methods for NLP tasks, a systematic comparison and analysis islacking in the literature. Herein, we present the FedNLP, a benchmarkingframework for evaluating federated learning methods on four different taskformulations: text classification, sequence tagging, question answering, andseq2seq. We propose a universal interface between Transformer-based languagemodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) undervarious non-IID partitioning strategies. Our extensive experiments with FedNLPprovide empirical comparisons between FL methods and helps us better understandthe inherent challenges of this direction. The comprehensive analysis points tointriguing and exciting future research aimed at developing FL methods for NLPtasks.\r2022-05-05\nEvolution of language driven by social dynamics\nMoirangthem Shubhakanta Singh R. K. Brojen Singh\nabstract\rabstract: The survival of endangered languages in complex language competition dependson socio-cultural status and honour endowed (by itself and by the other) amongthem. The restriction in the endorsement of this honour leads to languageextinction of one language, and rise of the other. Endorsing proper mutualhonour each other trigger the co-existence of language speakers and can saveboth languages from extinction. The lost of respect to each other drives thedeath of both languages. We found a minimal or critical mutual honour(a=0.9635) which protects the two languages from extinction. The increase inmutual honour from this minimal value allows increase in the populations of thetwo languages speakers. The state of co-existence of competing languagesabolishes the concept of minority and majority in language competition whichcan be obtained by mutual honour. Further, excess biased honour to a particularlanguage (minority or majority) force the language to extinct. In mean-fieldapproximation of language competition, magnetization parameter can be taken asan indicator of survival of a language.\r2022-05-02\nThe Limits of Word Level Differential Privacy\nJustus Mattern Benjamin Weggenmann Florian Kerschbaum\nabstract\rabstract: As the issues of privacy and trust are receiving increasing attention withinthe research community, various attempts have been made to anonymize textualdata. A significant subset of these approaches incorporate differentiallyprivate mechanisms to perturb word embeddings, thus replacing individual wordsin a sentence. While these methods represent very important contributions, havevarious advantages over other techniques and do show anonymizationcapabilities, they have several shortcomings. In this paper, we investigatethese weaknesses and demonstrate significant mathematical constraintsdiminishing the theoretical privacy guarantee as well as major practicalshortcomings with regard to the protection against deanonymization attacks, thepreservation of content of the original sentences as well as the quality of thelanguage output. Finally, we propose a new method for text anonymization basedon transformer based language models fine-tuned for paraphrasing thatcircumvents most of the identified weaknesses and also offers a formal privacyguarantee. We evaluate the performance of our method via thoroughexperimentation and demonstrate superior performance over the discussedmechanisms.\rPrivacy-Preserving Graph Convolutional Networks for Text Classification\nTimour Igamberdiev Ivan Habernal\nabstract\rabstract: Graph convolutional networks (GCNs) are a powerful architecture forrepresentation learning on documents that naturally occur as graphs, e.g.,citation or social networks. However, sensitive personal information, such asdocuments with people\u0026rsquo;s profiles or relationships as edges, are prone toprivacy leaks, as the trained model might reveal the original input. Althoughdifferential privacy (DP) offers a well-founded privacy-preserving framework,GCNs pose theoretical and practical challenges due to their training specifics.We address these challenges by adapting differentially-private gradient-basedtraining to GCNs and conduct experiments using two optimizers on five NLPdatasets in two languages. We propose a simple yet efficient method based onrandom graph splits that not only improves the baseline privacy bounds by afactor of 2.7 while retaining competitive F1 scores, but also provides strongprivacy guarantees of epsilon = 1.0. We show that, under certain modelingchoices, privacy-preserving GCNs perform up to 90% of their non-privatevariants, while formally guaranteeing strong privacy measures.\r2022-04-28\nEVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification\nGeorgios P. Spithourakis Ivan Vulić Michał Lis Iñigo Casanueva Paweł Budzianowski\nabstract\rabstract: Knowledge-based authentication is crucial for task-oriented spoken dialoguesystems that offer personalised and privacy-focused services. Such systemsshould be able to enrol (E), verify (V), and identify (I) new and recurringusers based on their personal information, e.g. postcode, name, and date ofbirth. In this work, we formalise the three authentication tasks and theirevaluation protocols, and we present EVI, a challenging spoken multilingualdataset with 5,506 dialogues in English, Polish, and French. Our proposedmodels set the first competitive benchmarks, explore the challenges ofmultilingual natural language processing of spoken dialogue, and set directionsfor future research.\rRobots: the Century Past and the Century Ahead\nFederico Pigozzi\nabstract\rabstract: Let us reflect on the state of robotics. This year marks the $101$-stanniversary of R.U.R., a play by the writer Karel \\v{C}apek, often creditedwith introducing the word \u0026ldquo;robot\u0026rdquo;. The word used to refer to feudal forcedlabourers in Slavic languages. Indeed, it points to one key characteristic ofrobotic systems: they are mere slaves, have no rights, and execute our willsinstruction by instruction, without asking anything in return. The relationshipwith us humans is commensalism; in biology, commensalism subsists between twosymbiotic species when one species benefits from it (robots boost productivityfor humans), while the other species neither benefits nor is harmed (can youreally argue that robots benefit from simply functioning?). We then distinguish robots from \u0026ldquo;living machines\u0026rdquo;, that is, machines infusedwith life. If living machines should ever become a reality, we would need toshift our relationship with them from commensalism to mutualism. Thedistinction is not subtle: we experience it every day with domesticatedanimals, that exchange serfdom for forage and protection. This is because lifehas evolved to resist any attempt at enslaving it; it is stubborn. In the path towards living machines, let us ask: what has been achieved byrobotics in the last $100$ years? What is left to accomplish in the next $100$years? For us, the answers boil down to three words: juice, need (or death),and embodiment, as we shall see in the following.\r2022-04-27\nLanguage-Independent Speaker Anonymization Approach using Self-Supervised Pre-Trained Models\nXiaoxiao Miao Xin Wang Erica Cooper Junichi Yamagishi Natalia Tomashenko\nabstract\rabstract: Speaker anonymization aims to protect the privacy of speakers whilepreserving spoken linguistic information from speech. Current mainstream neuralnetwork speaker anonymization systems are complicated, containing an F0extractor, speaker encoder, automatic speech recognition acoustic model (ASRAM), speech synthesis acoustic model and speech waveform generation model.Moreover, as an ASR AM is language-dependent, trained on English data, it ishard to adapt it into another language. In this paper, we propose a simplerself-supervised learning (SSL)-based method for language-independent speakeranonymization without any explicit language-dependent model, which can beeasily used for other languages. Extensive experiments were conducted on theVoicePrivacy Challenge 2020 datasets in English and AISHELL-3 datasets inMandarin to demonstrate the effectiveness of our proposed SSL-basedlanguage-independent speaker anonymization method.\r2022-04-26\nYou Don\u0026rsquo;t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers\u0026rsquo; Private Personas\nHaoran Li Yangqiu Song Lixin Fan\nabstract\rabstract: Social chatbots, also known as chit-chat chatbots, evolve rapidly with largepretrained language models. Despite the huge progress, privacy concerns havearisen recently: training data of large language models can be extracted viamodel inversion attacks. On the other hand, the datasets used for trainingchatbots contain many private conversations between two individuals. In thiswork, we further investigate the privacy leakage of the hidden states ofchatbots trained by language modeling which has not been well studied yet. Weshow that speakers\u0026rsquo; personas can be inferred through a simple neural networkwith high accuracy. To this end, we propose effective defense objectives toprotect persona leakage from hidden states. We conduct extensive experiments todemonstrate that our proposed defense objectives can greatly reduce the attackaccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preservelanguage models\u0026rsquo; powerful generation ability.\r2022-04-25\nReconstructing Training Data with Informed Adversaries\nBorja Balle Giovanni Cherubin Jamie Hayes\nabstract\rabstract: Given access to a machine learning model, can an adversary reconstruct themodel\u0026rsquo;s training data? This work studies this question from the lens of apowerful informed adversary who knows all the training data points except one.By instantiating concrete attacks, we show it is feasible to reconstruct theremaining data point in this stringent threat model. For convex models (e.g.logistic regression), reconstruction attacks are simple and can be derived inclosed-form. For more general models (e.g. neural networks), we propose anattack strategy based on training a reconstructor network that receives asinput the weights of the model under attack and produces as output the targetdata point. We demonstrate the effectiveness of our attack on image classifierstrained on MNIST and CIFAR-10, and systematically investigate which factors ofstandard machine learning pipelines affect reconstruction success. Finally, wetheoretically investigate what amount of differential privacy suffices tomitigate reconstruction attacks by informed adversaries. Our work provides aneffective reconstruction attack that model developers can use to assessmemorization of individual points in general settings beyond those consideredin previous works (e.g. generative language models or access to traininggradients); it shows that standard models have the capacity to store enoughinformation to enable high-fidelity reconstruction of training data points; andit demonstrates that differential privacy can successfully mitigate suchattacks in a parameter regime where utility degradation is minimal.\r2022-04-24\nAutomated speech tools for helping communities process restricted-access corpora for language revival efforts\nNay San Martijn Bartelds Tolúlopé Ògúnrèmí Alison Mount Ruben Thompson Michael Higgins Roy Barker Jane Simpson Dan Jurafsky\nabstract\rabstract: Many archival recordings of speech from endangered languages remainunannotated and inaccessible to community members and language learningprograms. One bottleneck is the time-intensive nature of annotation. An evennarrower bottleneck occurs for recordings with access constraints, such aslanguage that must be vetted or filtered by authorised community members beforeannotation can begin. We propose a privacy-preserving workflow to widen bothbottlenecks for recordings where speech in the endangered language isintermixed with a more widely-used language such as English for meta-linguisticcommentary and questions (e.g. What is the word for \u0026rsquo;tree\u0026rsquo;?). We integratevoice activity detection (VAD), spoken language identification (SLI), andautomatic speech recognition (ASR) to transcribe the metalinguistic content,which an authorised person can quickly scan to triage recordings that can beannotated by people with lower levels of access. We report work-in-progressprocessing 136 hours archival audio containing a mix of English and Muruwari.Our collaborative work with the Muruwari custodian of the archival materialsshow that this workflow reduces metalanguage transcription time by 20% evengiven only minimal amounts of annotated training data: 10 utterances perlanguage for SLI and for ASR at most 39 minutes, and possibly as little as 39seconds.\r2022-04-20\nYou Are What You Write: Preserving Privacy in the Era of Large Language Models\nRichard Plant Valerio Giuffrida Dimitra Gkatzia\nabstract\rabstract: Large scale adoption of large language models has introduced a new era ofconvenient knowledge transfer for a slew of natural language processing tasks.However, these models also run the risk of undermining user trust by exposingunwanted information about the data subjects, which may be extracted by amalicious party, e.g. through adversarial attacks. We present an empiricalinvestigation into the extent of the personal information encoded intopre-trained representations by a range of popular models, and we show apositive correlation between the complexity of a model, the amount of data usedin pre-training, and data leakage. In this paper, we present the first widecoverage evaluation and comparison of some of the most popularprivacy-preserving algorithms, on a large, multi-lingual dataset on sentimentanalysis annotated with demographic information (location, age and gender). Theresults show since larger and more complex models are more prone to leakingprivate information, use of privacy-preserving methods is highly desirable. Wealso find that highly privacy-preserving technologies like differential privacy(DP) can have serious model utility effects, which can be ameliorated usinghybrid or metric-DP techniques.\r2022-04-17\nWhyGen: Explaining ML-powered Code Generation by Referring to Training Examples\nWeixiang Yan Yuanchun Li\nabstract\rabstract: Deep learning has demonstrated great abilities in various code generationtasks. However, despite the great convenience for some developers, many areconcerned that the code generators may recite or closely mimic copyrightedtraining data without user awareness, leading to legal and ethical concerns. Toease this problem, we introduce a tool, named WhyGen, to explain the generatedcode by referring to training examples. Specifically, we first introduce a datastructure, named inference fingerprint, to represent the decision process ofthe model when generating a prediction. The fingerprints of all trainingexamples are collected offline and saved to a database. When the model is usedat runtime for code generation, the most relevant training examples can beretrieved by querying the fingerprint database. Our experiments have shown thatWhyGen is able to precisely notify the users about possible recitations andhighly similar imitations with a top-10 accuracy of 81.21%. The demo video canbe found at https://youtu.be/EtoQP6850To.\r2022-04-11\nCommonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data\nKyungjune Baek Hyunjung Shim\nabstract\rabstract: Transfer learning for GANs successfully improves generation performance underlow-shot regimes. However, existing studies show that the pretrained modelusing a single benchmark dataset is not generalized to various target datasets.More importantly, the pretrained model can be vulnerable to copyright orprivacy risks as membership inference attack advances. To resolve both issues,we propose an effective and unbiased data synthesizer, namely Primitives-PS,inspired by the generic characteristics of natural images. Specifically, weutilize 1) the generic statistics on the frequency magnitude spectrum, 2) theelementary shape (i.e., image composition via elementary shapes) forrepresenting the structure information, and 3) the existence of saliency asprior. Since our synthesizer only considers the generic properties of naturalimages, the single model pretrained on our dataset can be consistentlytransferred to various target datasets, and even outperforms the previousmethods pretrained with the natural images in terms of Fr\u0026rsquo;echet inceptiondistance. Extensive analysis, ablation study, and evaluations demonstrate thateach component of our data synthesizer is effective, and provide insights onthe desirable nature of the pretrained model for the transferability of GANs.\r2022-04-10\nFew-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts\nSaadullah Amin Noon Pokaratsiri Goldstein Morgan Kelly Wixted Alejandro García-Rudolph Catalina Martínez-Costa Günter Neumann\nabstract\rabstract: Despite the advances in digital healthcare systems offering curatedstructured knowledge, much of the critical information still lies in largevolumes of unlabeled and unstructured clinical texts. These texts, which oftencontain protected health information (PHI), are exposed to informationextraction tools for downstream applications, risking patient identification.Existing works in de-identification rely on using large-scale annotated corporain English, which often are not suitable in real-world multilingual settings.Pre-trained language models (LM) have shown great potential for cross-lingualtransfer in low-resource settings. In this work, we empirically show thefew-shot cross-lingual transfer property of LMs for named entity recognition(NER) and apply it to solve a low-resource and real-world challenge ofcode-mixed (Spanish-Catalan) clinical notes de-identification in the strokedomain. We annotate a gold evaluation dataset to assess few-shot settingperformance where we only use a few hundred labeled examples for training. Ourmodel improves the zero-shot F1-score from 73.7% to 91.2% on the goldevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingualtarget corpus. When generalized to an out-of-sample test set, the best modelachieves a human-evaluation F1-score of 97.2%.\r2022-04-06\nDifferentially Private Set Union\nSivakanth Gopi Pankaj Gulhane Janardhan Kulkarni Judy Hanwen Shen Milad Shokouhi Sergey Yekhanin\nabstract\rabstract: We study the basic operation of set union in the global model of differentialprivacy. In this problem, we are given a universe $U$ of items, possibly ofinfinite size, and a database $D$ of users. Each user $i$ contributes a subset$W_i \\subseteq U$ of items. We want an ($\\epsilon$,$\\delta$)-differentiallyprivate algorithm which outputs a subset $S \\subset \\cup_i W_i$ such that thesize of $S$ is as large as possible. The problem arises in countless real worldapplications; it is particularly ubiquitous in natural language processing(NLP) applications as vocabulary extraction. For example, discovering words,sentences, $n$-grams etc., from private text data belonging to users is aninstance of the set union problem. Known algorithms for this problem proceed by collecting a subset of itemsfrom each user, taking the union of such subsets, and disclosing the itemswhose noisy counts fall above a certain threshold. Crucially, in the aboveprocess, the contribution of each individual user is always independent of theitems held by other users, resulting in a wasteful aggregation process, wheresome item counts happen to be way above the threshold. We deviate from theabove paradigm by allowing users to contribute their items in a$\\textit{dependent fashion}$, guided by a $\\textit{policy}$. In this newsetting ensuring privacy is significantly delicate. We prove that any policywhich has certain $\\textit{contractive}$ properties would result in adifferentially private algorithm. We design two new algorithms, one usingLaplace noise and other Gaussian noise, as specific instances of policiessatisfying the contractive properties. Our experiments show that the newalgorithms significantly outperform previously known mechanisms for theproblem.\r2022-04-04\nPrivacy in Open Search: A Review of Challenges and Solutions\nSamuel Sousa Christian Guetl Roman Kern\nabstract\rabstract: Privacy is of worldwide concern regarding activities and processes thatinclude sensitive data. For this reason, many countries and territories havebeen recently approving regulations controlling the extent to whichorganizations may exploit data provided by people. Artificial intelligenceareas, such as machine learning and natural language processing, have alreadysuccessfully employed privacy-preserving mechanisms in order to safeguard dataprivacy in a vast number of applications. Information retrieval (IR) islikewise prone to privacy threats, such as attacks and unintended disclosuresof documents and search history, which may cripple the security of users and bepenalized by data protection laws. This work aims at highlighting anddiscussing open challenges for privacy in the recent literature of IR, focusingon tasks featuring user-generated text data. Our contribution is threefold:firstly, we present an overview of privacy threats to IR tasks; secondly, wediscuss applicable privacy-preserving mechanisms which may be employed insolutions to restrain privacy hazards; finally, we bring insights on thetradeoffs between privacy preservation and utility performance for IR tasks.\rClues in Tweets: Twitter-Guided Discovery and Analysis of SMS Spam\nSiyuan Tang Xianghang Mi Ying Li XiaoFeng Wang Kai Chen\nabstract\rabstract: With its critical role in business and service delivery through mobiledevices, SMS (Short Message Service) has long been abused for spamming, whichis still on the rise today possibly due to the emergence of A2P bulk messaging.The effort to control SMS spam has been hampered by the lack of up-to-dateinformation about illicit activities. In our research, we proposed a novelsolution to collect recent SMS spam data, at a large scale, from Twitter, whereusers voluntarily report the spam messages they receive. For this purpose, wedesigned and implemented SpamHunter, an automated pipeline to discover SMS spamreporting tweets and extract message content from the attached screenshots.Leveraging SpamHunter, we collected from Twitter a dataset of 21,918 SMS spammessages in 75 languages, spanning over four years. To our best knowledge, thisis the largest SMS spam dataset ever made public. More importantly, SpamHunterenables us to continuously monitor emerging SMS spam messages, whichfacilitates the ongoing effort to mitigate SMS spamming. We also performed anin-depth measurement study that sheds light on the new trends in the spammer\u0026rsquo;sstrategies, infrastructure and spam campaigns. We also utilized our spam SMSdata to evaluate the robustness of the spam countermeasures put in place by theSMS ecosystem, including anti-spam services, bulk SMS services, and textmessaging apps. Our evaluation shows that such protection cannot effectivelyhandle those spam samples: either introducing significant false positives ormissing a large number of newly reported spam messages.\r2022-03-31\nPreliminary Steps Towards Federated Sentiment Classification\nXin-Chun Li Lan Li De-Chuan Zhan Yunfeng Shao Bingshuai Li Shaoming Song\nabstract\rabstract: Automatically mining sentiment tendency contained in natural language is afundamental research to some artificial intelligent applications, wheresolutions alternate with challenges. Transfer learning and multi-task learningtechniques have been leveraged to mitigate the supervision sparsity andcollaborate multiple heterogeneous domains correspondingly. Recent years, thesensitive nature of users\u0026rsquo; private data raises another challenge for sentimentclassification, i.e., data privacy protection. In this paper, we resort tofederated learning for multiple domain sentiment classification under theconstraint that the corpora must be stored on decentralized devices. In view ofthe heterogeneous semantics across multiple parties and the peculiarities ofword embedding, we pertinently provide corresponding solutions. First, wepropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework forbetter model aggregation and personalization in federated sentimentclassification. Second, we propose KTEPS$^\\star$ with the consideration of therich semantic and huge embedding size properties of word vectors, utilizingProjection-based Dimension Reduction (PDR) methods for privacy protection andefficient transmission simultaneously. We propose two federated sentimentclassification scenes based on public benchmarks, and verify the superioritiesof our proposed methods with abundant experimental investigations.\rImproving speaker de-identification with functional data analysis of f0 trajectories\nLauri Tavi Tomi Kinnunen Rosa González Hautamäki\nabstract\rabstract: Due to a constantly increasing amount of speech data that is stored indifferent types of databases, voice privacy has become a major concern. Torespond to such concern, speech researchers have developed various methods forspeaker de-identification. The state-of-the-art solutions utilize deep learningsolutions which can be effective but might be unavailable or impractical toapply for, for example, under-resourced languages. Formant modification is asimpler, yet effective method for speaker de-identification which requires notraining data. Still, remaining intonational patterns in formant-anonymizedspeech may contain speaker-dependent cues. This study introduces a novelspeaker de-identification method, which, in addition to simple formant shifts,manipulates f0 trajectories based on functional data analysis. The proposedspeaker de-identification method will conceal plausibly identifying pitchcharacteristics in a phonetically controllable manner and improve formant-basedspeaker de-identification up to 25%.\r2022-03-28\nMixed Differential Privacy in Computer Vision\nAditya Golatkar Alessandro Achille Yu-Xiang Wang Aaron Roth Michael Kearns Stefano Soatto\nabstract\rabstract: We introduce AdaMix, an adaptive differentially private algorithm fortraining deep neural network classifiers using both private and public imagedata. While pre-training language models on large public datasets has enabledstrong differential privacy (DP) guarantees with minor loss of accuracy, asimilar practice yields punishing trade-offs in vision tasks. A few-shot oreven zero-shot learning baseline that ignores private data can outperformfine-tuning on a large private dataset. AdaMix incorporates few-shot training,or cross-modal zero-shot learning, on public data prior to private fine-tuning,to improve the trade-off. AdaMix reduces the error increase from thenon-private upper bound from the 167-311% of the baseline, on average across 6datasets, to 68-92% depending on the desired privacy level selected by theuser. AdaMix tackles the trade-off arising in visual classification, wherebythe most privacy sensitive data, corresponding to isolated points inrepresentation space, are also critical for high classification accuracy. Inaddition, AdaMix comes with strong theoretical privacy guarantees andconvergence analysis.\rThe MIT Voice Name System\nBrian Subirana Harry Levinson Ferran Hueto Prithvi Rajasekaran Alexander Gaidis Esteve Tarragó Peter Oliveira-Soens\nabstract\rabstract: This RFC white Paper summarizes our progress on the MIT Voice Name System(VNS) and Huey. The VNS, similar in name and function to the DNS, is a systemto reserve and use \u0026ldquo;wake words\u0026rdquo; to activate Artificial Intelligence (AI)devices. Just like you can say \u0026ldquo;Hey Siri\u0026rdquo; to activate Apple\u0026rsquo;s personalassistant, we propose using the VNS in smart speakers and other devices toroute wake requests based on commands such as \u0026ldquo;turn off\u0026rdquo;, \u0026ldquo;open groceryshopping list\u0026rdquo; or \u0026ldquo;271, start flash card review of my computer vision class\u0026rdquo;.We also introduce Huey, an unambiguous Natural Language to interact with AIdevices. We aim to standardize voice interactions to a universal reach similarto that of other systems such as phone numbering, with an agreed world-wideapproach to assign and use numbers, or the Internet\u0026rsquo;s DNS, with a standardnaming system, that has helped flourish popular services including theWorld-Wide-Web, FTP, and email. Just like these standards are \u0026ldquo;neutral\u0026rdquo;, wealso aim to endow the VNS with \u0026ldquo;wake neutrality\u0026rdquo; so that each participant candevelop its own digital voice. We focus on voice as a starting point to talk toany IoT object and explain briefly how the VNS may be expanded to other AItechnologies enabling person-to-machine conversations (reallymachine-to-machine), including computer vision or neural interfaces. We alsodescribe briefly considerations for a broader set of standards, MIT Open AI(MOA), including a reference architecture to serve as a starting point for thedevelopment of a general conversational commerce infrastructure that hasstandard \u0026ldquo;Wake Words\u0026rdquo;, NLP commands such as \u0026ldquo;Shopping Lists\u0026rdquo; or \u0026ldquo;Flash CardReviews\u0026rdquo;, and personalities such as Pi or 271. Privacy and security are keyelements considered because of speech-to-text errors and the amount of personalinformation contained in a voice sample.\r2022-03-24\nVerifiable Access Control for Augmented Reality Localization and Mapping\nShaowei Zhu Hyo Jin Kim Maurizio Monge G. Edward Suh Armin Alaghi Brandon Reagen Vincent Lee\nabstract\rabstract: Localization and mapping is a key technology for bridging the virtual andphysical worlds in augmented reality (AR). Localization and mapping works bycreating and querying maps made of anchor points that enable the overlay ofthese two worlds. As a result, information about the physical world is capturedin the map and naturally gives rise to concerns around who can map physicalspaces as well as who can access or modify the virtual ones. This paperdiscusses how we can provide access controls over virtual maps as a basicbuilding block to enhance security and privacy of AR systems. In particular, wepropose VACMaps: an access control system for localization and mapping usingformal methods. VACMaps defines a domain-specific language that enables usersto specify access control policies for virtual spaces. Access requests tovirtual spaces are then evaluated against relevant policies in a way thatpreserves confidentiality and integrity of virtual spaces owned by the users.The precise semantics of the policies are defined by SMT formulas, which allowVACMaps to reason about properties of access policies automatically. Anevaluation of VACMaps is provided using an AR testbed of a single-family home.We show that VACMaps is scalable in that it can run at practical speeds andthat it can also reason about access control policies automatically to detectpotential policy misconfigurations.\rClassifying Cyber-Risky Clinical Notes by Employing Natural Language Processing\nSuzanna Schmeelk Martins Samuel Dogo Yifan Peng Braja Gopal Patra\nabstract\rabstract: Clinical notes, which can be embedded into electronic medical records,document patient care delivery and summarize interactions between healthcareproviders and patients. These clinical notes directly inform patient care andcan also indirectly inform research and quality/safety metrics, among otherindirect metrics. Recently, some states within the United States of Americarequire patients to have open access to their clinical notes to improve theexchange of patient information for patient care. Thus, developing methods toassess the cyber risks of clinical notes before sharing and exchanging data iscritical. While existing natural language processing techniques are geared tode-identify clinical notes, to the best of our knowledge, few have focused onclassifying sensitive-information risk, which is a fundamental step towarddeveloping effective, widespread protection of patient health information. Tobridge this gap, this research investigates methods for identifyingsecurity/privacy risks within clinical notes. The classification either can beused upstream to identify areas within notes that likely contain sensitiveinformation or downstream to improve the identification of clinical notes thathave not been entirely de-identified. We develop several models using unigramand word2vec features with different classifiers to categorize sentence risk.Experiments on i2b2 de-identification dataset show that the SVM classifierusing word2vec features obtained a maximum F1-score of 0.792. Future researchinvolves articulation and differentiation of risk in terms of different globalregulatory requirements.\r2022-03-22\nA Girl Has A Name, And It\u0026rsquo;s \u0026hellip; Adversarial Authorship Attribution for Deobfuscation\nWanyue Zhai Jonathan Rusert Zubair Shafiq Padmini Srinivasan\nabstract\rabstract: Recent advances in natural language processing have enabled powerfulprivacy-invasive authorship attribution. To counter authorship attribution,researchers have proposed a variety of rule-based and learning-based textobfuscation approaches. However, existing authorship obfuscation approaches donot consider the adversarial threat model. Specifically, they are not evaluatedagainst adversarially trained authorship attributors that are aware ofpotential obfuscation. To fill this gap, we investigate the problem ofadversarial authorship attribution for deobfuscation. We show thatadversarially trained authorship attributors are able to degrade theeffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluatethe effectiveness of adversarial training when the attributor makes incorrectassumptions about whether and which obfuscator was used. While there is a aclear degradation in attribution accuracy, it is noteworthy that thisdegradation is still at or above the attribution accuracy of the attributorthat is not adversarially trained at all. Our results underline the need forstronger obfuscation approaches that are resistant to deobfuscation\r2022-03-20\nMitigating Gender Bias in Machine Translation through Adversarial Learning\nEve Fleisig Christiane Fellbaum\nabstract\rabstract: Machine translation and other NLP systems often contain significant biasesregarding sensitive attributes, such as gender or race, that worsen systemperformance and perpetuate harmful stereotypes. Recent preliminary researchsuggests that adversarial learning can be used as part of a model-agnostic biasmitigation method that requires no data modifications. However, adapting thisstrategy for machine translation and other modern NLP domains requires (1)restructuring training objectives in the context of fine-tuning pretrainedlarge language models and (2) developing measures for gender or other protectedvariables for tasks in which these attributes must be deduced from the dataitself. We present an adversarial learning framework that addresses these challengesto mitigate gender bias in seq2seq machine translation. Our framework improvesthe disparity in translation quality for sentences with male vs. femaleentities by 86% for English-German translation and 91% for English-Frenchtranslation, with minimal effect on translation quality. The results suggestthat adversarial learning is a promising technique for mitigating gender biasin machine translation.\rInterpretability of Fine-grained Classification of Sadness and Depression\nTiasa Singha Roy Priyam Basu Aman Priyanshu Rakshit Naidu\nabstract\rabstract: While sadness is a human emotion that people experience at certain timesthroughout their lives, inflicting them with emotional disappointment and pain,depression is a longer term mental illness which impairs social, occupational,and other vital regions of functioning making it a much more serious issue andneeds to be catered to at the earliest. NLP techniques can be utilized for thedetection and subsequent diagnosis of these emotions. Most of the open sourceddata on the web deal with sadness as a part of depression, as an emotion eventhough the difference in severity of both is huge. Thus, we create our ownnovel dataset illustrating the difference between the two. In this paper, weaim to highlight the difference between the two and highlight how interpretableour models are to distinctly label sadness and depression. Due to the sensitivenature of such information, privacy measures need to be taken for handling andtraining of such data. Hence, we also explore the effect of Federated Learning(FL) on contextualised language models.\r2022-03-19\nAnomaly Detection in Emails using Machine Learning and Header Information\nCraig Beaman Haruna Isah\nabstract\rabstract: Anomalies in emails such as phishing and spam present major security riskssuch as the loss of privacy, money, and brand reputation to both individualsand organizations. Previous studies on email anomaly detection relied on asingle type of anomaly and the analysis of the email body and subject content.A drawback of this approach is that it takes into account the written languageof the email content. To overcome this deficit, this study conducted featureextraction and selection on email header datasets and leveraged both multi andone-class anomaly detection approaches. Experimental analysis results obtaineddemonstrate that email header information only is enough to reliably detectspam and phishing emails. Supervised learning algorithms such as Random Forest,SVM, MLP, KNN, and their stacked ensembles were found to be very successful,achieving high accuracy scores of 97% for phishing and 99% for spam emails.One-class classification with One-Class SVM achieved accuracy scores of 87% and89% with spam and phishing emails, respectively. Real-world email filteringapplications will benefit from the use of only the header information in termsof resources utilization and efficiency.\rThe Online Behaviour of the Algerian Abusers in Social Media Networks\nKheireddine Abainia\nabstract\rabstract: Connecting to social media networks becomes a daily task for the majority ofpeople around the world, and the amount of shared information is growingexponentially. Thus, controlling the way in which people communicate isnecessary, in order to protect them from disorientation, conflicts,aggressions, etc. In this paper, we conduct a statistical study on thecyber-bullying and the abusive content in social media (i.e. Facebook), wherewe try to spot the online behaviour of the abusers in the Algerian community.More specifically, we have involved 200 Facebook users from different regionsamong 600 to carry out this study. The aim of this investigation is to aidautomatic systems of abuse detection to take decision by incorporating theonline activity. Abuse detection systems require a large amount of data toperform better on such kind of texts (i.e. unstructured and informal texts),and this is due to the lack of standard orthography, where there are variousAlgerian dialects and languages spoken.\r2022-03-17\nMultilingual Detection of Personal Employment Status on Twitter\nManuel Tonneau Dhaval Adjodah João Palotti Nir Grinberg Samuel Fraiberger\nabstract\rabstract: Detecting disclosures of individuals\u0026rsquo; employment status on social media canprovide valuable information to match job seekers with suitable vacancies,offer social protection, or measure labor market flows. However, identifyingsuch personal disclosures is a challenging task due to their rarity in a sea ofsocial media content and the variety of linguistic forms used to describe them.Here, we examine three Active Learning (AL) strategies in real-world settingsof extreme class imbalance, and identify five types of disclosures aboutindividuals\u0026rsquo; employment status (e.g. job loss) in three languages usingBERT-based classification models. Our findings show that, even under extremeimbalance settings, a small number of AL iterations is sufficient to obtainlarge and significant gains in precision, recall, and diversity of resultscompared to a supervised baseline with the same number of labels. We also findthat no AL strategy consistently outperforms the rest. Qualitative analysissuggests that AL helps focus the attention mechanism of BERT on core terms andadjust the boundaries of semantic expansion, highlighting the importance ofinterpretable models to provide greater control and visibility into thisdynamic learning process.\rKART: Parameterization of Privacy Leakage Scenarios from Pre-trained Language Models\nYuta Nakamura Shouhei Hanaoka Yukihiro Nomura Naoto Hayashi Osamu Abe Shuntaro Yada Shoko Wakamiya Eiji Aramaki\nabstract\rabstract: For the safe sharing pre-trained language models, no guidelines exist atpresent owing to the difficulty in estimating the upper bound of the risk ofprivacy leakage. One problem is that previous studies have assessed the riskfor different real-world privacy leakage scenarios and attack methods, whichreduces the portability of the findings. To tackle this problem, we representcomplex real-world privacy leakage scenarios under a universalparameterization, \\textit{Knowledge, Anonymization, Resource, and Target}(KART). KART parameterization has two merits: (i) it clarifies the definitionof privacy leakage in each experiment and (ii) it improves the comparability ofthe findings of risk assessments. We show that previous studies can be simplyreviewed by parameterizing the scenarios with KART. We also demonstrate privacyrisk assessments in different scenarios under the same attack method, whichsuggests that KART helps approximate the upper bound of risk under a specificattack or scenario. We believe that KART helps integrate past and futurefindings on privacy risk and will contribute to a standard for sharing languagemodels.\r2022-03-16\nCapsNet for Medical Image Segmentation\nMinh Tran Viet-Khoa Vo-Ho Kyle Quinn Hien Nguyen Khoa Luu Ngan Le\nabstract\rabstract: Convolutional Neural Networks (CNNs) have been successful in solving tasks incomputer vision including medical image segmentation due to their ability toautomatically extract features from unstructured data. However, CNNs aresensitive to rotation and affine transformation and their success relies onhuge-scale labeled datasets capturing various input variations. This networkparadigm has posed challenges at scale because acquiring annotated data formedical segmentation is expensive, and strict privacy regulations. Furthermore,visual representation learning with CNNs has its own flaws, e.g., it isarguable that the pooling layer in traditional CNNs tends to discard positionalinformation and CNNs tend to fail on input images that differ in orientationsand sizes. Capsule network (CapsNet) is a recent new architecture that hasachieved better robustness in representation learning by replacing poolinglayers with dynamic routing and convolutional strides, which has shownpotential results on popular tasks such as classification, recognition,segmentation, and natural language processing. Different from CNNs, whichresult in scalar outputs, CapsNet returns vector outputs, which aim to preservethe part-whole relationships. In this work, we first introduce the limitationsof CNNs and fundamentals of CapsNet. We then provide recent developments ofCapsNet for the task of medical image segmentation. We finally discuss variouseffective network architectures to implement a CapsNet for both 2D images and3D volumetric medical image segmentation.\rMeasuring Fairness of Text Classifiers via Prediction Sensitivity\nSatyapriya Krishna Rahul Gupta Apurv Verma Jwala Dhamala Yada Pruksachatkun Kai-Wei Chang\nabstract\rabstract: With the rapid growth in language processing applications, fairness hasemerged as an important consideration in data-driven solutions. Althoughvarious fairness definitions have been explored in the recent literature, thereis lack of consensus on which metrics most accurately reflect the fairness of asystem. In this work, we propose a new formulation : ACCUMULATED PREDICTIONSENSITIVITY, which measures fairness in machine learning models based on themodel\u0026rsquo;s prediction sensitivity to perturbations in input features. The metricattempts to quantify the extent to which a single prediction depends on aprotected attribute, where the protected attribute encodes the membershipstatus of an individual in a protected group. We show that the metric can betheoretically linked with a specific notion of group fairness (statisticalparity) and individual fairness. It also correlates well with humans\u0026rsquo;perception of fairness. We conduct experiments on two text classificationdatasets : JIGSAW TOXICITY, and BIAS IN BIOS, and evaluate the correlationsbetween metrics and manual annotations on whether the model produced a fairoutcome. We observe that the proposed fairness metric based on predictionsensitivity is statistically significantly more correlated with humanannotation than the existing counterfactual fairness metric.\r2022-03-15\nTraining a Tokenizer for Free with Private Federated Learning\nEugene Bagdasaryan Congzheng Song Rogier van Dalen Matt Seigel Áine Cahill\nabstract\rabstract: Federated learning with differential privacy, i.e. private federated learning(PFL), makes it possible to train models on private data distributed acrossusers\u0026rsquo; devices without harming privacy. PFL is efficient for models, such asneural networks, that have a fixed number of parameters, and thus afixed-dimensional gradient vector. Such models include neural-net languagemodels, but not tokenizers, the topic of this work. Training a tokenizerrequires frequencies of words from an unlimited vocabulary, and existingmethods for finding an unlimited vocabulary need a separate privacy budget. A workaround is to train the tokenizer on publicly available data. However,in this paper we first show that a tokenizer trained on mismatched data resultsin worse model performance compared to a privacy-violating \u0026ldquo;oracle\u0026rdquo; tokenizerthat accesses user data, with perplexity increasing by 20%. We also show thatsub-word tokenizers are better suited to the federated context than word-levelones, since they can encode new words, though with more tokens per word. Second, we propose a novel method to obtain a tokenizer without using anyadditional privacy budget. During private federated learning of the languagemodel, we sample from the model, train a new tokenizer on the sampledsequences, and update the model embeddings. We then continue private federatedlearning, and obtain performance within 1% of the \u0026ldquo;oracle\u0026rdquo; tokenizer. Sincethis process trains the tokenizer only indirectly on private data, we can usethe \u0026ldquo;postprocessing guarantee\u0026rdquo; of differential privacy and thus use noadditional privacy budget.\r2022-03-14\nThe Big-O Problem\nDmitry Chistikov Stefan Kiefer Andrzej S. Murawski David Purser\nabstract\rabstract: Given two weighted automata, we consider the problem of whether one is big-Oof the other, i.e., if the weight of every finite word in the first is notgreater than some constant multiple of the weight in the second. We show that the problem is undecidable, even for the instantiation ofweighted automata as labelled Markov chains. Moreover, even when it is knownthat one weighted automaton is big-O of another, the problem of finding orapproximating the associated constant is also undecidable. Our positive results show that the big-O problem is polynomial-time solvablefor unambiguous automata, coNP-complete for unlabelled weighted automata (i.e.,when the alphabet is a single character) and decidable, subject to Schanuel\u0026rsquo;sconjecture, when the language is bounded (i.e., a subset of $w_1^\\dots w_m^$for some finite words $w_1,\\dots,w_m$) or when the automaton has finiteambiguity. On labelled Markov chains, the problem can be restated as a ratio totalvariation distance, which, instead of finding the maximum difference betweenthe probabilities of any two events, finds the maximum ratio between theprobabilities of any two events. The problem is related to$\\varepsilon$-differential privacy, for which the optimal constant of the big-Onotation is exactly $\\exp(\\varepsilon)$.\rIn Search of Lost Utility: Private Location Data\nSzilvia Lestyán Gergely Ács Gergely Biczók\nabstract\rabstract: The unavailability of training data is a permanent source of much frustrationin research, especially when it is due to privacy concerns. This isparticularly true for location data since previous techniques all suffer fromthe inherent sparseness and high dimensionality of location trajectories whichrender most techniques impractical, resulting in unrealistic traces andunscalable methods. Moreover, time information of location visits is usuallydropped, or its resolution is drastically reduced. In this paper we present anovel technique for privately releasing a composite generative model and wholehigh-dimensional location datasets with detailed time information. To generatehigh-fidelity synthetic data, we leverage several peculiarities of vehicularmobility such as its language-like characteristics (\u0026ldquo;you should know a locationby the company it keeps\u0026rdquo;) or how humans plan their trips from one point to theother. We model the generator distribution of the dataset by first constructinga variational autoencoder to generate the source and destination locations, andthe corresponding timing of trajectories. Next, we compute transitionprobabilities between locations with a feed forward network, and build atransition graph from the output of this model, which approximates thedistribution of all paths between the source and destination (at a given time).Finally, a path is sampled from this distribution with a Markov Chain MonteCarlo method. The generated synthetic dataset is highly realistic, scalable,provides good utility and, nonetheless, provably private. We evaluate our modelagainst two state-of-the-art methods and three real-life datasets demonstratingthe benefits of our approach.\r2022-03-12\nOn Information Hiding in Natural Language Systems\nGeetanjali Bihani Julia Taylor Rayz\nabstract\rabstract: With data privacy becoming more of a necessity than a luxury in today\u0026rsquo;sdigital world, research on more robust models of privacy preservation andinformation security is on the rise. In this paper, we take a look at NaturalLanguage Steganography (NLS) methods, which perform information hiding innatural language systems, as a means to achieve data security as well asconfidentiality. We summarize primary challenges regarding the secrecy andimperceptibility requirements of these systems and propose potential directionsof improvement, specifically targeting steganographic text quality. We believethat this study will act as an appropriate framework to build more resilientmodels of Natural Language Steganography, working towards instilling securitywithin natural language-based neural models.\r2022-03-02\nA Blockchain-Based Consent Mechanism for Access to Fitness Data in the Healthcare Context\nMay Alhajri Carsten Rudolph Ahmad Salehi Shahraki\nabstract\rabstract: Wearable fitness devices are widely used to track an individual\u0026rsquo;s health andphysical activities to improve the quality of health services. These devicessense a considerable amount of sensitive data processed by a centralized thirdparty. While many researchers have thoroughly evaluated privacy issuessurrounding wearable fitness trackers, no study has addressed privacy issues intrackers by giving control of the data to the user. Blockchain is an emergingtechnology with outstanding advantages in resolving consent management privacyconcerns. As there are no fully transparent, legally compliant solutions forsharing personal fitness data, this study introduces an architecture for ahuman-centric, legally compliant, decentralized and dynamic consent systembased on blockchain and smart contracts. Algorithms and sequence diagrams ofthe proposed system\u0026rsquo;s activities show consent-related data flow among variousagents, which are used later to prove the system\u0026rsquo;s trustworthiness byformalizing the security requirements. The security properties of the proposedsystem were evaluated using the formal security modeling framework SeMF, whichdemonstrates the feasibility of the solution at an abstract level based onformal language theory. As a result, we have empirically proven that blockchaintechnology is suitable for mitigating the privacy issues of fitness providersby recording individuals\u0026rsquo; consent using blockchain and smart contracts.\rAn Efficient DP-SGD Mechanism for Large Scale NLP Models\nChristophe Dupuy Radhika Arava Rahul Gupta Anna Rumshisky\nabstract\rabstract: Recent advances in deep learning have drastically improved performance onmany Natural Language Understanding (NLU) tasks. However, the data used totrain NLU models may contain private information such as addresses or phonenumbers, particularly when drawn from human subjects. It is desirable thatunderlying models do not expose private information contained in the trainingdata. Differentially Private Stochastic Gradient Descent (DP-SGD) has beenproposed as a mechanism to build privacy-preserving models. However, DP-SGD canbe prohibitively slow to train. In this work, we propose a more efficientDP-SGD for training using a GPU infrastructure and apply it to fine-tuningmodels based on LSTM and transformer architectures. We report faster trainingtimes, alongside accuracy, theoretical privacy guarantees and success ofMembership inference attacks for our models and observe that fine-tuning withproposed variant of DP-SGD can yield competitive models without significantdegradation in training time and improvement in privacy protection. We alsomake observations such as looser theoretical $\\epsilon, \\delta$ can translateinto significant practical privacy gains.\r2022-03-01\nCompliance Checking with NLI: Privacy Policies vs. Regulations\nAmin Rabinia Zane Nygaard\nabstract\rabstract: A privacy policy is a document that states how a company intends to handleand manage their customers\u0026rsquo; personal data. One of the problems that arises withthese privacy policies is that their content might violate data privacyregulations. Because of the enormous number of privacy policies that exist, theonly realistic way to check for legal inconsistencies in all of them is throughan automated method. In this work, we use Natural Language Inference (NLI)techniques to compare privacy regulations against sections of privacy policiesfrom a selection of large companies. Our NLI model uses pre-trained embeddings,along with BiLSTM in its attention mechanism. We tried two versions of ourmodel: one that was trained on the Stanford Natural Language Inference (SNLI)and the second on the Multi-Genre Natural Language Inference (MNLI) dataset. Wefound that our test accuracy was higher on our model trained on the SNLI, butwhen actually doing NLI tasks on real world privacy policies, the model trainedon MNLI generalized and performed much better.\r2022-02-26\nA Robust Document Image Watermarking Scheme using Deep Neural Network\nSulong Ge Zhihua Xia Jianwei Fei Xingming Sun Jian Weng\nabstract\rabstract: Watermarking is an important copyright protection technology which generallyembeds the identity information into the carrier imperceptibly. Then theidentity can be extracted to prove the copyright from the watermarked carriereven after suffering various attacks. Most of the existing watermarkingtechnologies take the nature images as carriers. Different from the naturalimages, document images are not so rich in color and texture, and thus haveless redundant information to carry watermarks. This paper proposes anend-to-end document image watermarking scheme using the deep neural network.Specifically, an encoder and a decoder are designed to embed and extract thewatermark. A noise layer is added to simulate the various attacks that could beencountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussiannoise, Resize, and JPEG Compression. A text-sensitive loss function is designedto limit the embedding modification on characters. An embedding strengthadjustment strategy is proposed to improve the quality of watermarked imagewith little loss of extraction accuracy. Experimental results show that theproposed document image watermarking technology outperforms threestate-of-the-arts in terms of the robustness and image quality.\r2022-02-23\nAbsolute Zero-Shot Learning\nRui Gao Fan Wan Daniel Organisciak Jiyao Pu Junyan Wang Haoran Duan Peng Zhang Xingsong Hou Yang Long\nabstract\rabstract: Considering the increasing concerns about data copyright and privacy issues,we present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., traininga classifier with zero real data. The key innovation is to involve a teachermodel as the data safeguard to guide the AZSL model training without dataleaking. The AZSL model consists of a generator and student network, which canachieve date-free knowledge transfer while maintaining the performance of theteacher network. We investigate black-box' and white-box\u0026rsquo; scenarios in AZSLtask as different levels of model security. Besides, we also provide discussionof teacher model in both inductive and transductive settings. Despiteembarrassingly simple implementations and data-missing disadvantages, our AZSLframework can retain state-of-the-art ZSL and GZSL performance under thewhite-box' scenario. Extensive qualitative and quantitative analysis alsodemonstrates promising results when deploying the model under black-box\u0026rsquo;scenario.\r2022-02-19\nProgrammable Interface for Statistical \u0026amp; Simulation Models (PRISM): Towards Greater Accessibility of Clinical and Healthcare Decision Models\nAmin Adibi Stephanie Harvard Mohsen Sadatsafavi\nabstract\rabstract: Background: Increasingly, decision-making in healthcare relies on computermodels, be it clinical prediction models at point of care or decision-analyticmodels at the policymaking level. Given the important role models play in bothcontexts, their structure and implementation be rigorously scrutinized. Theability to interrogate input/output associations without facing barriers canimprove quality assurance mechanisms while satisfying privacy/confidentialityconcerns and facilitating the integration of models into decision-making. Thispaper reports on the development of Programmable Interface for Statistical \u0026amp;Simulation Models (PRISM), a cloud-based platform for model accessibility.Methods: PRISM emphasizes two main principles: 1) minimal specifications on theside of model developer to make the model fit for cloud hosting, and 2) makingclient access completely independent of the resource requirement and softwaredependencies of the model. The server architecture integrates a RESTfulApplication Programming Interface (API) infrastructure, JSON for data transfer,a routing layer for access management, container technology for management ofcomputer resources and package dependencies, and the capacity for synchronousor asynchronous model calls. Results: We discuss the architecture, the minimalAPI standards that enable a universal language for access to such models, theunderlying server infrastructure, and the standards used for data transfer. Aninstance of PRISM is available as a service via the Peer Models Networkhttp://peermodelsnetwork.com. Through a series of case studies, we demonstratehow interrogating models becomes possible in standardized fashion, in a waythat is irrespective of the specifics of any model. Conclusions: We havedeveloped a publicly accessible platform and minimalist standards thatfacilitate model accessibility for both clinical and policy models.\r2022-02-17\nWhen BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing\nChao-Han Huck Yang Jun Qi Samuel Yen-Chi Chen Yu Tsao Pin-Yu Chen\nabstract\rabstract: The rapid development of quantum computing has demonstrated many uniquecharacteristics of quantum advantages, such as richer feature representationand more secured protection on model parameters. This work proposes a verticalfederated learning architecture based on variational quantum circuits todemonstrate the competitive performance of a quantum-enhanced pre-trained BERTmodel for text classification. In particular, our proposed hybridclassical-quantum model consists of a novel random quantum temporal convolution(QTC) learning framework replacing some layers in the BERT-based decoder. Ourexperiments on intent classification show that our proposed BERT-QTC modelattains competitive experimental results in the Snips and ATIS spoken languagedatasets. Particularly, the BERT-QTC boosts the performance of the existingquantum circuit-based language model in two text classification datasets by1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasiblydeployed on both existing commercial-accessible quantum computation hardwareand CPU-based interface for ensuring data isolation.\r2022-02-16\nRegional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal\nFelipe González-Pizarro Andrea Figueroa Claudia López Cecilia Aragon\nabstract\rabstract: While there is increasing global attention to data privacy, most of theircurrent theoretical understanding is based on research conducted in a fewcountries. Prior work argues that people\u0026rsquo;s cultural backgrounds might shapetheir privacy concerns; thus, we could expect people from different worldregions to conceptualize them in diverse ways. We collected and analyzed alarge-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanishand English to start exploring this hypothesis. We employed word embeddings andqualitative analysis to identify which information privacy concerns are presentand characterize language and regional differences in emphasis on theseconcerns. Our results suggest that related concepts, such as regulations, canbe added to current information privacy frameworks. We also observe a greateremphasis on data collection in English than in Spanish. Additionally, data fromNorth America exhibits a narrower focus on awareness compared to other regionsunder study. Our results call for more diverse sources of data and nuancedanalysis of data privacy concerns around the globe.\r2022-02-15\nDefending against Reconstruction Attacks with Rényi Differential Privacy\nPierre Stock Igor Shilov Ilya Mironov Alexandre Sablayrolles\nabstract\rabstract: Reconstruction attacks allow an adversary to regenerate data samples of thetraining set using access to only a trained model. It has been recently shownthat simple heuristics can reconstruct data samples from language models,making this threat scenario an important aspect of model release. Differentialprivacy is a known solution to such attacks, but is often used with arelatively large privacy budget (epsilon \u0026gt; 8) which does not translate tomeaningful guarantees. In this paper we show that, for a same mechanism, we canderive privacy guarantees for reconstruction attacks that are better than thetraditional ones from the literature. In particular, we show that largerprivacy budgets do not protect against membership inference, but can stillprotect extraction of rare secrets. We show experimentally that our guaranteeshold against various language models, including GPT-2 finetuned onWikitext-103.\r2022-02-14\nThreats to Pre-trained Language Models: Survey and Taxonomy\nShangwei Guo Chunlong Xie Jiwei Li Lingjuan Lyu Tianwei Zhang\nabstract\rabstract: Pre-trained language models (PTLMs) have achieved great success andremarkable performance over a wide range of natural language processing (NLP)tasks. However, there are also growing concerns regarding the potentialsecurity issues in the adoption of PTLMs. In this survey, we comprehensivelysystematize recently discovered threats to PTLM systems and applications. Weperform our attack characterization from three interesting perspectives. (1) Weshow threats can occur at different stages of the PTLM pipeline raised bydifferent malicious entities. (2) We identify two types of modeltransferability (landscape, portrait) that facilitate attacks. (3) Based on theattack goals, we summarize four categories of attacks (backdoor, evasion, dataprivacy and model privacy). We also discuss some open problems and researchdirections. We believe our survey and taxonomy will inspire future studiestowards secure and privacy-preserving PTLMs.\rWhat Does it Mean for a Language Model to Preserve Privacy?\nHannah Brown Katherine Lee Fatemehsadat Mireshghallah Reza Shokri Florian Tramèr\nabstract\rabstract: Natural language reflects our private lives and identities, making itsprivacy concerns as broad as those of real life. Language models lack theability to understand the context and sensitivity of text, and tend to memorizephrases present in their training sets. An adversary can exploit this tendencyto extract training data. Depending on the nature of the content and thecontext in which this data was collected, this could violate expectations ofprivacy. Thus there is a growing interest in techniques for training languagemodels that preserve privacy. In this paper, we discuss the mismatch betweenthe narrow assumptions made by popular data protection techniques (datasanitization and differential privacy), and the broadness of natural languageand of privacy as a social norm. We argue that existing protection methodscannot guarantee a generic and meaningful notion of privacy for languagemodels. We conclude that language models should be trained on text data whichwas explicitly produced for public use.\r2022-02-12\nWav2Vec2.0 on the Edge: Performance Evaluation\nSantosh Gondi\nabstract\rabstract: Wav2Vec2.0 is a state-of-the-art model which learns speech representationsthrough unlabeled speech data, aka, self supervised learning. The pretrainedmodel is then fine tuned on small amounts of labeled data to use it forspeech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformativesolution for low resource languages as it is mainly developed using unlabeledaudio data. Getting large amounts of labeled data is resource intensive andespecially challenging to do for low resource languages such as Swahilli,Tatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpassesthe very recent supervised learning algorithms while using 100x less labeleddata. Given its importance and enormous potential in enabling speech basedtasks on world\u0026rsquo;s 7000 languages, it is key to evaluate the accuracy, latencyand efficiency of this model on low resource and low power edge devices andinvestigate the feasibility of using it in such devices for private, secure andreliable speech based tasks. On-device speech tasks preclude sending audio datato the server hence inherently providing privacy, reduced latency and enhancedreliability. In this paper, Wav2Vec2.0 model\u0026rsquo;s accuracy and latency has beenevaluated on Raspberry Pi along with the KenLM language model for speechrecognition tasks. How to tune certain parameters to achieve desired level ofWER rate and latency while meeting the CPU, memory and energy budgets of theproduct has been discussed.\r2022-02-10\nNÜWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN\nMinheng Ni Chenfei Wu Haoyang Huang Daxin Jiang Wangmeng Zuo Nan Duan\nabstract\rabstract: Language guided image inpainting aims to fill in the defective regions of animage under the guidance of text while keeping non-defective regions unchanged.However, the encoding process of existing models suffers from either receptivespreading of defective regions or information loss of non-defective regions,giving rise to visually unappealing inpainting results. To address the aboveissues, this paper proposes N\u0026quot;UWA-LIP by incorporating defect-free VQGAN(DF-VQGAN) with multi-perspective sequence to sequence (MP-S2S). In particular,DF-VQGAN introduces relative estimation to control receptive spreading andadopts symmetrical connections to protect information. MP-S2S further enhancesvisual information from complementary perspectives, including both low-levelpixels and high-level tokens. Experiments show that DF-VQGAN performs morerobustness than VQGAN. To evaluate the inpainting performance of our model, webuilt up 3 open-domain benchmarks, where N\u0026quot;UWA-LIP is also superior to recentstrong baselines.\r2022-02-09\nFedQAS: Privacy-aware machine reading comprehension with federated learning\nAddi Ait-Mlouk Sadi Alawadi Salman Toor Andreas Hellander\nabstract\rabstract: Machine reading comprehension (MRC) of text data is one important task inNatural Language Understanding. It is a complex NLP problem with a lot ofongoing research fueled by the release of the Stanford Question AnsweringDataset (SQuAD) and Conversational Question Answering (CoQA). It is consideredto be an effort to teach computers how to \u0026ldquo;understand\u0026rdquo; a text, and then to beable to answer questions about it using deep learning. However, until nowlarge-scale training on private text data and knowledge sharing has beenmissing for this NLP task. Hence, we present FedQAS, a privacy-preservingmachine reading system capable of leveraging large-scale private data withoutthe need to pool those datasets in a central location. The proposed approachcombines transformer models and federated learning technologies. The system isdeveloped using the FEDn framework and deployed as a proof-of-concept allianceinitiative. FedQAS is flexible, language-agnostic, and allows intuitiveparticipation and execution of local model training. In addition, we presentthe architecture and implementation of the system, as well as provide areference evaluation based on the SQUAD dataset, to showcase how it overcomesdata privacy issues and enables knowledge sharing between alliance members in aFederated learning setting.\rFairness-aware Summarization for Justified Decision-Making\nMoniba Keymanesh Tanya Berger-Wolf Micha Elsner Srinivasan Parthasarathy\nabstract\rabstract: In consequential domains such as recidivism prediction, facility inspection,and benefit assignment, it\u0026rsquo;s important for individuals to know thedecision-relevant information for the model\u0026rsquo;s prediction. In addition,predictions should be fair both in terms of the outcome and the justificationof the outcome. In other words, decision-relevant features should providesufficient information for the predicted outcome and should be independent ofthe membership of individuals in protected groups such as race and gender. Inthis work, we focus on the problem of (un)fairness in the justification of thetext-based neural models. We tie the explanatory power of the model to fairnessin the outcome and propose a fairness-aware summarization mechanism to detectand counteract the bias in such models. Given a potentially biased naturallanguage explanation for a decision, we use a multi-task neural model and anattribution mechanism based on integrated gradients to extract high-utility andlow-bias justifications in form of a summary. The extracted summary is thenused for training a model to make decisions for individuals. Results on severalreal world datasets suggest that our method drastically limits the demographicleakage in the input (fairness in justification) while moderately enhancing thefairness in the outcome. Our model is also effective in detecting andcounteracting several types of data poisoning attacks that synthesizerace-coded reasoning or irrelevant justifications.\r2022-02-07\nDeletion Inference, Reconstruction, and Compliance in Machine (Un)Learning\nJi Gao Sanjam Garg Mohammad Mahmoody Prashant Nalini Vasudevan\nabstract\rabstract: Privacy attacks on machine learning models aim to identify the data that isused to train such models. Such attacks, traditionally, are studied on staticmodels that are trained once and are accessible by the adversary. Motivated tomeet new legal requirements, many machine learning methods are recentlyextended to support machine unlearning, i.e., updating models as if certainexamples are removed from their training sets, and meet new legal requirements.However, privacy attacks could potentially become more devastating in this newsetting, since an attacker could now access both the original model beforedeletion and the new model after the deletion. In fact, the very act ofdeletion might make the deleted record more vulnerable to privacy attacks. Inspired by cryptographic definitions and the differential privacy framework,we formally study privacy implications of machine unlearning. We formalize(various forms of) deletion inference and deletion reconstruction attacks, inwhich the adversary aims to either identify which record is deleted or toreconstruct (perhaps part of) the deleted records. We then present successfuldeletion inference and reconstruction attacks for a variety of machine learningmodels and tasks such as classification, regression, and language models.Finally, we show that our attacks would provably be precluded if the schemessatisfy (variants of) Deletion Compliance (Garg, Goldwasser, and Vasudevan,Eurocrypt\u0026rsquo; 20).\r2022-02-05\nReGVD: Revisiting Graph Neural Networks for Vulnerability Detection\nVan-Anh Nguyen Dai Quoc Nguyen Van Nguyen Trung Le Quan Hung Tran Dinh Phung\nabstract\rabstract: Identifying vulnerabilities in the source code is essential to protect thesoftware systems from cyber security attacks. It, however, is also achallenging step that requires specialized expertise in security and coderepresentation. To this end, we aim to develop a general, practical, andprogramming language-independent model capable of running on various sourcecodes and libraries without difficulty. Therefore, we consider vulnerabilitydetection as an inductive text classification problem and propose ReGVD, asimple yet effective graph neural network-based model for the problem. Inparticular, ReGVD views each raw source code as a flat sequence of tokens tobuild a graph, wherein node features are initialized by only the tokenembedding layer of a pre-trained programming language (PL) model. ReGVD thenleverages residual connection among GNN layers and examines a mixture ofgraph-level sum and max poolings to return a graph embedding for the sourcecode. ReGVD outperforms the existing state-of-the-art models and obtains thehighest accuracy on the real-world benchmark dataset from CodeXGLUE forvulnerability detection. Our code is available at:\\url{https://github.com/daiquocnguyen/GNN-ReGVD}.\r2022-02-03\nEggCounts: a Bayesian hierarchical toolkit to model faecal egg count reductions\nCraig Wang Reinhard Furrer\nabstract\rabstract: This is a vignette for the R package eggCounts version 2.0. The packageimplements a suite of Bayesian hierarchical models dealing with faecal eggcount reductions. The models are designed for a variety of practicalsituations, including individual treatment efficacy, zero inflation, smallsample size (less than 10) and potential outliers. The functions are intuitiveto use and their output are easy to interpret, such that users are protectedfrom being exposed to complex Bayesian hierarchical modelling tasks. Inaddition, the package includes plotting functions to display data and resultsin a visually appealing manner. The models are implemented in Stan modellinglanguage, which provides efficient sampling technique to obtain posteriorsamples. This vignette briefly introduces different models, and provides ashort walk-through analysis with example data.\r2022-02-02\nDetecting Privacy Requirements from User Stories with NLP Transfer Learning Models\nFrancesco Casillo Vincenzo Deufemia Carmine Gravino\nabstract\rabstract: To provide privacy-aware software systems, it is crucial to consider privacyfrom the very beginning of the development. However, developers do not have theexpertise and the knowledge required to embed the legal and social requirementsfor data protection into software systems. Objective: We present an approach todecrease privacy risks during agile software development by automaticallydetecting privacy-related information in the context of user storyrequirements, a prominent notation in agile Requirement Engineering (RE).Methods: The proposed approach combines Natural Language Processing (NLP) andlinguistic resources with deep learning algorithms to identify privacy aspectsinto User Stories. NLP technologies are used to extract information regardingthe semantic and syntactic structure of the text. This information is thenprocessed by a pre-trained convolutional neural network, which paved the wayfor the implementation of a Transfer Learning technique. We evaluate theproposed approach by performing an empirical study with a dataset of 1680 userstories. Results: The experimental results show that deep learning algorithmsallow to obtain better predictions than those achieved with conventional(shallow) machine learning methods. Moreover, the application of TransferLearning allows to considerably improve the accuracy of the predictions, ca.10%. Conclusions: Our study contributes to encourage software engineeringresearchers in considering the opportunities to automate privacy detection inthe early phase of design, by also exploiting transfer learning models.\r2022-01-30\nFormalism-Driven Development of Decentralized Systems\nYepeng Ding Hiroyuki Sato\nabstract\rabstract: Decentralized systems have been widely developed and applied to addresssecurity and privacy issues in centralized systems, especially since theadvancement of distributed ledger technology. However, it is challenging toensure their correct functioning with respect to their designs and minimize thetechnical risk before the delivery. Although formal methods have madesignificant progress over the past decades, a feasible solution based on formalmethods from a development process perspective has not been well developed. Inthis paper, we formulate an iterative and incremental development process,named formalism-driven development (FDD), for developing provably correctdecentralized systems under the guidance of formal methods. We also present aframework named Seniz, to practicalize FDD with a new modeling language andscaffolds. Furthermore, we conduct case studies to demonstrate theeffectiveness of FDD in practice with the support of Seniz.\r2022-01-26\nTwitter-Demographer: A Flow-based Tool to Enrich Twitter Data\nFederico Bianchi Vincenzo Cutrona Dirk Hovy\nabstract\rabstract: Twitter data have become essential to Natural Language Processing (NLP) andsocial science research, driving various scientific discoveries in recentyears. However, the textual data alone are often not enough to conduct studies:especially social scientists need more variables to perform their analysis andcontrol for various factors. How we augment this information, such as users\u0026rsquo;location, age, or tweet sentiment, has ramifications for anonymity andreproducibility, and requires dedicated effort. This paper describesTwitter-Demographer, a simple, flow-based tool to enrich Twitter data withadditional information about tweets and users. Twitter-Demographer is aimed atNLP practitioners and (computational) social scientists who want to enrichtheir datasets with aggregated information, facilitating reproducibility, andproviding algorithmic privacy-by-design measures for pseudo-anonymity. Wediscuss our design choices, inspired by the flow-based programming paradigm, touse black-box components that can easily be chained together and extended. Wealso analyze the ethical issues related to the use of this tool, and thebuilt-in measures to facilitate pseudo-anonymity.\r2022-01-24\nPolytope: Practical Memory Access Control for C++ Applications\nIoannis Agadakos Manuel Egele William Robertson\nabstract\rabstract: Designing and implementing secure software is inarguably more important thanever. However, despite years of research into privilege separating programs, itremains difficult to actually do so and such efforts can take years oflabor-intensive engineering to reach fruition. At the same time, newintra-process isolation primitives make strong data isolation and privilegeseparation more attractive from a performance perspective. Yet, substitutingintra-process security boundaries for time-tested process boundaries opens thedoor to subtle but devastating privilege leaks. In this work, we presentPolytope, a language extension to C++ that aims to make efficient privilegeseparation accessible to a wider audience of developers. Polytope defines apolicy language encoded as C++11 attributes that separate code and data intodistinct program partitions. A modified Clang front-end embeds source-levelpolicy as metadata nodes in the LLVM IR. An LLVM pass interprets embeddedpolicy and instruments an IR with code to enforce the source-level policy usingIntel MPK. A run-time support library manages partitions, protection keys,dynamic memory operations, and indirect call target privileges. An evaluationdemonstrates that Polytope provides equivalent protection to prior systems witha low annotation burden and comparable performance overhead. Polytope alsorenders privilege leaks that contradict intended policy impossible to express.\r2022-01-22\nhybrid-Falcon: Hybrid Pattern Malware Detection and Categorization with Network Traffic and Program Code\nPeng Xu Claudia Eckert Apostolis Zarras\nabstract\rabstract: Nowadays, Android is the most dominant operating system in the mobileecosystem, with billions of people using its apps daily. As expected, thistrend did not go unnoticed by miscreants, and Android became the favoriteplatform for discovering new victims through malicious apps. Moreover, theseapps have become so sophisticated that they can bypass anti-malware measures toprotect the users. Therefore, it is safe to admit that traditional anti-malwaretechniques have become cumbersome, sparking the urge to develop an efficientway to detect Android malware. This paper presents hybrid-Flacon, a hybrid pattern Android malware detectionand categorization framework. It combines dynamic and static features ofAndroid malware, which are from network traffic and code graph structure. Inhybrid-Flacon, we treat network traffic as a dynamic feature and process it asa 2D image sequence. Meanwhile, hybrid-Flacon handles each network flow in thepacket as a 2D image and uses a bidirectional LSTM network to process those2D-image sequences to obtain vectors representing network packets. We use theprogram code graph for a static feature and introduce natural languageprocessing (NLP) inspired techniques on function call graph (FCG). We design agraph neural network-based approach to convert the whole graph structure ofAndroid apps to vectors. Finally, We utilize those converted vectors, bothnetwork and program code features, and concatenate them to detect andcategorize the malware. Our results reveal that hybrid-Flacon yields betterresults as we get 97.16% accuracy on average for malware detection and 88.32%accuracy for malware categorization. Additionally, we release a datasetAndroNetMnist, which converts the network traffic to a 2D-image sequence andhelps to accomplish malware detection on a 2D-image sequence.\r2022-01-21\nPrivacy Policies Across the Ages: Content and Readability of Privacy Policies 1996\u0026ndash;2021\nIsabel Wagner\nabstract\rabstract: It is well-known that most users do not read privacy policies, but almost allusers tick the box to agree with them. In this paper, we analyze the 25-yearhistory of privacy policies using methods from transparency research, machinelearning, and natural language processing. Specifically, we collect alarge-scale longitudinal corpus of privacy policies from 1996 to 2021 andanalyze the length and readability of privacy policies as well as their contentin terms of the data practices they describe, the rights they grant to users,and the rights they reserve for their organizations. We pay particularattention to changes in response to recent privacy regulations such as the GDPRand CCPA. Our results show that policies are getting longer and harder to read,especially after new regulations take effect, and we find a range of concerningdata practices. Our results allow us to speculate why privacy policies arerarely read and propose changes that would make privacy policies serve theirreaders instead of their writers.\r2022-01-17\nDynamic Differential-Privacy Preserving SGD\nJian Du Song Li Xiangyi Chen Siheng Chen Mingyi Hong\nabstract\rabstract: The vanilla Differentially-Private Stochastic Gradient Descent (DP-SGD),including DP-Adam and other variants, ensures the privacy of training data byuniformly distributing privacy costs across training steps. The equivalentprivacy costs controlled by maintaining the same gradient clipping thresholdsand noise powers in each step result in unstable updates and a lower modelaccuracy when compared to the non-DP counterpart. In this paper, we propose thedynamic DP-SGD (along with dynamic DP-Adam, and others) to reduce theperformance loss gap while maintaining privacy by dynamically adjustingclipping thresholds and noise powers while adhering to a total privacy budgetconstraint. Extensive experiments on a variety of deep learning tasks,including image classification, natural language processing, and federatedlearning, demonstrate that the proposed dynamic DP-SGD algorithm stabilizesupdates and, as a result, significantly improves model accuracy in the strongprivacy protection region when compared to the vanilla DP-SGD. We also conducttheoretical analysis to better understand the privacy-utility trade-off withdynamic DP-SGD, as well as to learn why Dynamic DP-SGD can outperform vanillaDP-SGD.\rRobust Aggregation for Federated Learning\nKrishna Pillutla Sham M. Kakade Zaid Harchaoui\nabstract\rabstract: Federated learning is the centralized training of statistical models fromdecentralized data on mobile devices while preserving the privacy of eachdevice. We present a robust aggregation approach to make federated learningrobust to settings when a fraction of the devices may be sending corruptedupdates to the server. The approach relies on a robust aggregation oracle basedon the geometric median, which returns a robust aggregate using a constantnumber of iterations of a regular non-robust averaging oracle. The robustaggregation oracle is privacy-preserving, similar to the non-robust secureaverage oracle it builds upon. We establish its convergence for least squaresestimation of additive models. We provide experimental results with linearmodels and deep networks for three tasks in computer vision and naturallanguage processing. The robust aggregation approach is agnostic to the levelof corruption; it outperforms the classical aggregation approach in terms ofrobustness when the level of corruption is high, while being competitive in theregime of low corruption. Two variants, a faster one with one-step robustaggregation and another one with on-device personalization, round off thepaper.\r2022-01-15\nHow are Diverse End-user Human-centric Issues Discussed on GitHub?\nHourieh Khalajzadeh Mojtaba Shahin Humphrey O. Obie John Grundy\nabstract\rabstract: Many software systems fail to meet the needs of the diverse end-users insociety and are prone to pose problems, such as accessibility and usabilityissues. Some of these problems (partially) stem from the failure to considerthe characteristics, limitations, and abilities of diverse end-users duringsoftware development. We refer to this class of problems as human-centricissues. Despite their importance, there is a limited understanding of the typesof human-centric issues encountered by developers. In-depth knowledge of thesehuman-centric issues is needed to design software systems that better meettheir diverse end-users\u0026rsquo; needs. This paper aims to provide insights for thesoftware development and research communities on which human-centric issues area topic of discussion for developers on GitHub. We conducted an empirical studyby extracting and manually analysing 1,691 issue comments from 12 diverseprojects, ranging from small to large-scale projects, including projectsdesigned for challenged end-users, e.g., visually impaired and dyslexic users.Our analysis shows that eight categories of human-centric issues are discussedby developers. These include Inclusiveness, Privacy \u0026amp; Security, Compatibility,Location \u0026amp; Language, Preference, Satisfaction, Emotional Aspects, andAccessibility. Guided by our findings, we highlight some implications andpossible future paths to further understand and incorporate human-centricissues in software development to be able to design software that meets theneeds of diverse end users in society.\rAddressing the Challenges of Cross-Lingual Hate Speech Detection\nIrina Bigoulaeva Viktor Hangya Iryna Gurevych Alexander Fraser\nabstract\rabstract: The goal of hate speech detection is to filter negative online content aimingat certain groups of people. Due to the easy accessibility of social mediaplatforms it is crucial to protect everyone which requires building hate speechdetection systems for a wide range of languages. However, the available labeledhate speech datasets are limited making it problematic to build systems formany languages. In this paper we focus on cross-lingual transfer learning tosupport hate speech detection in low-resource languages. We leveragecross-lingual word embeddings to train our neural network systems on the sourcelanguage and apply it to the target language, which lacks labeled examples, andshow that good performance can be achieved. We then incorporate unlabeledtarget language data for further model improvements by bootstrapping labelsusing an ensemble of different model architectures. Furthermore, we investigatethe issue of label imbalance of hate speech datasets, since the high ratio ofnon-hate examples compared to hate examples often leads to low modelperformance. We test simple data undersampling and oversampling techniques andshow their effectiveness.\r2022-01-14\nSkillVet: Automated Traceability Analysis of Amazon Alexa Skills\nJide S Edu Xavier Ferrer-Aran Jose M Such Guillermo Suarez-Tangil\nabstract\rabstract: Third-party software, or skills, are essential components in Smart PersonalAssistants (SPA). The number of skills has grown rapidly, dominated by achanging environment that has no clear business model. Skills can accesspersonal information and this may pose a risk to users. However, there islittle information about how this ecosystem works, let alone the tools that canfacilitate its study. In this paper, we present the largest systematicmeasurement of the Amazon Alexa skill ecosystem to date. We study developers\u0026rsquo;practices in this ecosystem, including how they collect and justify the needfor sensitive information, by designing a methodology to identifyover-privileged skills with broken privacy policies. We collect 199,295 Alexaskills and uncover that around 43% of the skills (and 50% of the developers)that request these permissions follow bad privacy practices, including(partially) broken data permissions traceability. In order to perform this kindof analysis at scale, we present SkillVet that leverages machine learning andnatural language processing techniques, and generates high-accuracy predictionsets. We report a number of concerning practices including how developers canbypass Alexa\u0026rsquo;s permission system through account linking and conversationalskills, and offer recommendations on how to improve transparency, privacy andsecurity. Resulting from the responsible disclosure we have conducted, 13% ofthe reported issues no longer pose a threat at submission time.\r2022-01-05\nWikipedia Reader Navigation: When Synthetic Data Is Enough\nAkhil Arora Martin Gerlach Tiziano Piccardi Alberto García-Durán Robert West\nabstract\rabstract: Every day millions of people read Wikipedia. When navigating the vast spaceof available topics using hyperlinks, readers describe trajectories on thearticle network. Understanding these navigation patterns is crucial to betterserve readers\u0026rsquo; needs and address structural biases and knowledge gaps. However,systematic studies of navigation on Wikipedia are hindered by a lack ofpublicly available data due to the commitment to protect readers\u0026rsquo; privacy bynot storing or sharing potentially sensitive data. In this paper, we ask: Howwell can Wikipedia readers\u0026rsquo; navigation be approximated by using publiclyavailable resources, most notably the Wikipedia clickstream data? Wesystematically quantify the differences between real navigation sequences andsynthetic sequences generated from the clickstream data, in 6 analyses across 8Wikipedia language versions. Overall, we find that the differences between realand synthetic sequences are statistically significant, but with small effectsizes, often well below 10%. This constitutes quantitative evidence for theutility of the Wikipedia clickstream data as a public resource: clickstreamdata can closely capture reader navigation on Wikipedia and provides asufficient approximation for most practical downstream applications relying onreader data. More broadly, this study provides an example for howclickstream-like data can generally enable research on user navigation ononline platforms while protecting users\u0026rsquo; privacy.\r2022-01-04\nSubmix: Practical Private Prediction for Large-Scale Language Models\nAntonio Ginart Laurens van der Maaten James Zou Chuan Guo\nabstract\rabstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model\u0026rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.\r2022-01-03\nAI \u0026amp; Racial Equity: Understanding Sentiment Analysis Artificial Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems\nAlia Abbas\nabstract\rabstract: Various forms of implications of artificial intelligence that eitherexacerbate or decrease racial systemic injustice have been explored in thisapplied research endeavor. Taking each thematic area of identifying, analyzing,and debating an systemic issue have been leveraged in investigating merits anddrawbacks of using algorithms to automate human decision making in raciallysensitive environments. It has been asserted through the analysis of historicalsystemic patterns, implicit biases, existing algorithmic risks, and legalimplications that natural language processing based AI, such as risk assessmenttools, have racially disparate outcomes. It is concluded that more litigativepolicies are needed to regulate and restrict how internal governmentinstitutions and corporations utilize algorithms, privacy and security risks,and auditing requirements in order to diverge from racially injustice outcomesand practices of the past.\r2021-12-31\nInverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer\nChin-Tung Lin Mu Yang\nabstract\rabstract: Many social media users prefer consuming content in the form of videos ratherthan text. However, in order for content creators to produce videos with a highclick-through rate, much editing is needed to match the footage to the music.This posts additional challenges for more amateur video makers. Therefore, wepropose a novel attention-based model VMT (Video-Music Transformer) thatautomatically generates piano scores from video frames. Using music generatedfrom models also prevent potential copyright infringements that often come withusing existing music. To the best of our knowledge, there is no work besidesthe proposed VMT that aims to compose music for video. Additionally, therelacks a dataset with aligned video and symbolic music. We release a new datasetcomposed of over 7 hours of piano scores with fine alignment between pop musicvideos and MIDI files. We conduct experiments with human evaluation on VMT,SeqSeq model (our baseline), and the original piano version soundtrack. VMTachieves consistent improvements over the baseline on music smoothness andvideo relevance. In particular, with the relevance scores and our case study,our model has shown the capability of multimodality on frame-level actors\u0026rsquo;movement for music generation. Our VMT model, along with the new dataset,presents a promising research direction toward composing the matchingsoundtrack for videos. We have released our code athttps://github.com/linchintung/VMT\rData-Free Knowledge Transfer: A Survey\nYuang Liu Wei Zhang Jun Wang Jianyong Wang\nabstract\rabstract: In the last decade, many deep learning models have been well trained and madea great success in various fields of machine intelligence, especially forcomputer vision and natural language processing. To better leverage thepotential of these well-trained models in intra-domain or cross-domain transferlearning situations, knowledge distillation (KD) and domain adaptation (DA) areproposed and become research highlights. They both aim to transfer usefulinformation from a well-trained model with original training data. However, theoriginal data is not always available in many cases due to privacy, copyrightor confidentiality. Recently, the data-free knowledge transfer paradigm hasattracted appealing attention as it deals with distilling valuable knowledgefrom well-trained models without requiring to access to the training data. Inparticular, it mainly consists of the data-free knowledge distillation (DFKD)and source data-free domain adaptation (SFDA). On the one hand, DFKD aims totransfer the intra-domain knowledge of original data from a cumbersome teachernetwork to a compact student network for model compression and efficientinference. On the other hand, the goal of SFDA is to reuse the cross-domainknowledge stored in a well-trained source model and adapt it to a targetdomain. In this paper, we provide a comprehensive survey on data-free knowledgetransfer from the perspectives of knowledge distillation and unsuperviseddomain adaptation, to help readers have a better understanding of the currentresearch status and ideas. Applications and challenges of the two areas arebriefly reviewed, respectively. Furthermore, we provide some insights to thesubject of future research.\r"},{"id":1,"href":"/docs/pages/","title":"Arxiv Paper","section":"Docs","content":"\rArxiv Papers: LLM, Privacy, and Copyright\r#\r2024-03-07\nMembership Inference Attacks and Privacy in Topic Modeling\nNico Manzonelli Wanrong Zhang Salil Vadhan\nabstract\rabstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.\rFederated Recommendation via Hybrid Retrieval Augmented Generation\nHuimin Zeng Zhenrui Yue Qian Jiang Dong Wang\nabstract\rabstract: Federated Recommendation (FR) emerges as a novel paradigm that enablesprivacy-preserving recommendations. However, traditional FR systems usuallyrepresent users/items with discrete identities (IDs), suffering fromperformance degradation due to the data sparsity and heterogeneity in FR. Onthe other hand, Large Language Models (LLMs) as recommenders have proveneffective across various recommendation scenarios. Yet, LLM-based recommendersencounter challenges such as low inference efficiency and potentialhallucination, compromising their performance in real-world scenarios. To thisend, we propose GPT-FedRec, a federated recommendation framework leveragingChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.GPT-FedRec is a two-stage solution. The first stage is a hybrid retrievalprocess, mining ID-based user patterns and text-based item features. Next, theretrieved results are converted into text prompts and fed into GPT forre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aimsto extract generalized features from data and exploit pretrained knowledgewithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, theRAG approach also prevents LLM hallucination, improving the recommendationperformance for real-world users. Experimental results on diverse benchmarkdatasets demonstrate the superior performance of GPT-FedRec againststate-of-the-art baseline methods.\rPrivacy-preserving Fine-tuning of Large Language Models through Flatness\nTiejin Chen Longchao Da Huixue Zhou Pingzhi Li Kaixiong Zhou Tianlong Chen Hua Wei\nabstract\rabstract: The privacy concerns associated with the use of Large Language Models (LLMs)have grown recently with the development of LLMs such as ChatGPT. DifferentialPrivacy (DP) techniques are explored in existing work to mitigate their privacyrisks at the cost of generalization degradation. Our paper reveals that theflatness of DP-trained models\u0026rsquo; loss landscape plays an essential role in thetrade-off between their privacy and generalization. We further propose aholistic framework to enforce appropriate weight flatness, which substantiallyimproves model generalization with competitive privacy preservation. Itinnovates from three coarse-to-grained levels, including perturbation-awaremin-max optimization on model weights within a layer, flatness-guided sparseprefix-tuning on weights across layers, and weight knowledge distillationbetween DP \u0026amp; non-DP weights copies. Comprehensive experiments of bothblack-box and white-box scenarios are conducted to demonstrate theeffectiveness of our proposal in enhancing generalization and maintaining DPcharacteristics. For instance, on text classification dataset QNLI, DP-Flatachieves similar performance with non-private full fine-tuning but with DPguarantee under privacy budget $\\epsilon=3$, and even better performance givenhigher privacy budgets. Codes are provided in the supplement.\r2024-03-06\nEnhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification\nRicardo Bigolin Lanfredi Pritam Mukherjee Ronald Summers\nabstract\rabstract: In chest X-ray (CXR) image analysis, rule-based systems are usually employedto extract labels from reports, but concerns exist about label quality. Thesedatasets typically offer only presence labels, sometimes with binaryuncertainty indicators, which limits their usefulness. In this work, we presentMAPLEZ (Medical report Annotations with Privacy-preserving Large language modelusing Expeditious Zero shot answers), a novel approach leveraging a locallyexecutable Large Language Model (LLM) to extract and enhance findings labels onCXR reports. MAPLEZ extracts not only binary labels indicating the presence orabsence of a finding but also the location, severity, and radiologists\u0026rsquo;uncertainty about the finding. Over eight abnormalities from five test sets, weshow that our method can extract these annotations with an increase of 5percentage points (pp) in F1 score for categorical presence annotations andmore than 30 pp increase in F1 score for the location annotations overcompeting labelers. Additionally, using these improved annotations inclassification supervision, we demonstrate substantial advancements in modelquality, with an increase of 1.7 pp in AUROC over models trained withannotations from the state-of-the-art approach. We share code and annotations.\rTaypsi: Static Enforcement of Privacy Policies for Policy-Agnostic Oblivious Computation\nQianchuan Ye Benjamin Delaware\nabstract\rabstract: Secure multiparty computation (MPC) techniques enable multiple parties tocompute joint functions over their private data without sharing that data withother parties, typically by employing powerful cryptographic protocols toprotect individual\u0026rsquo;s data. One challenge when writing such functions is thatmost MPC languages force users to intermix programmatic and privacy concerns ina single application, making it difficult to change or audit a program\u0026rsquo;sunderlying privacy policy. Prior policy-agnostic MPC languages relied ondynamic enforcement to decouple privacy requirements from program logic.Unfortunately, the resulting overhead makes it difficult to scale MPCapplications that manipulate structured data. This work proposes to eliminatethis overhead by instead transforming programs into semantically equivalentversions that statically enforce user-provided privacy policies. We haveimplemented this approach in a new MPC language, called Taypsi; ourexperimental evaluation demonstrates that the resulting system featuresconsiderable performance improvements on a variety of MPC applicationsinvolving structured data and complex privacy policies.\rTowards Efficient and Effective Unlearning of Large Language Models for Recommendation\nHangyu Wang Jianghao Lin Bo Chen Yang Yang Ruiming Tang Weinan Zhang Yong Yu\nabstract\rabstract: The significant advancements in large language models (LLMs) give rise to apromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).The efficacy of LLMRec arises from the open-world knowledge and reasoningcapabilities inherent in LLMs. LLMRec acquires the recommendation capabilitiesthrough instruction tuning based on user interaction data. However, in order toprotect user privacy and optimize utility, it is also crucial for LLMRec tointentionally forget specific user data, which is generally referred to asrecommendation unlearning. In the era of LLMs, recommendation unlearning posesnew challenges for LLMRec in terms of \\textit{inefficiency} and\\textit{ineffectiveness}. Existing unlearning methods require updating billionsof parameters in LLMRec, which is costly and time-consuming. Besides, theyalways impact the model utility during the unlearning process. To this end, wepropose \\textbf{E2URec}, the first \\underline{E}fficient and\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Ourproposed E2URec enhances the unlearning efficiency by updating only a fewadditional LoRA parameters, and improves the unlearning effectiveness byemploying a teacher-student framework, where we maintain multiple teachernetworks to guide the unlearning process. Extensive experiments show thatE2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affectingrecommendation performance. The source code is at\\url{https://github.com/justarter/E2URec}.\rExplaining Genetic Programming Trees using Large Language Models\nPaula Maddigan Andrew Lensen Bing Xue\nabstract\rabstract: Genetic programming (GP) has the potential to generate explainable results,especially when used for dimensionality reduction. In this research, weinvestigate the potential of leveraging eXplainable AI (XAI) and large languagemodels (LLMs) like ChatGPT to improve the interpretability of GP-basednon-linear dimensionality reduction. Our study introduces a novel XAI dashboardnamed GP4NLDR, the first approach to combine state-of-the-art GP with anLLM-powered chatbot to provide comprehensive, user-centred explanations. Weshowcase the system\u0026rsquo;s ability to provide intuitive and insightful narratives onhigh-dimensional data reduction processes through case studies. Our studyhighlights the importance of prompt engineering in eliciting accurate andpertinent responses from LLMs. We also address important considerations arounddata privacy, hallucinatory outputs, and the rapid advancements in generativeAI. Our findings demonstrate its potential in advancing the explainability ofGP algorithms. This opens the door for future research into explaining GPmodels with LLMs.\r2024-03-05\nCoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following\nKaiyan Zhang Jianyu Wang Ermo Hua Biqing Qi Ning Ding Bowen Zhou\nabstract\rabstract: With the advancement of language models (LMs), their exposure to private datais increasingly inevitable, and their deployment (especially for smaller ones)on personal devices, such as PCs and smartphones, has become a prevailingtrend. In contexts laden with user information, enabling models to bothsafeguard user privacy and execute commands efficiently emerges as an essentialresearch imperative. In this paper, we propose CoGenesis, a collaborativegeneration framework integrating large (hosted on cloud infrastructure) andsmall models (deployed on local devices) to address privacy concerns logically.Initially, we design a pipeline to create personalized writing instructiondatasets enriched with extensive context details as the testbed of thisresearch issue. Subsequently, we introduce two variants of CoGenesis based onsketch and logits respectively. Our experimental findings, based on oursynthesized dataset and two additional open-source datasets, indicate that: 1)Large-scale models perform well when provided with user context but struggle inthe absence of such context. 2) While specialized smaller models fine-tuned onthe synthetic dataset show promise, they still lag behind their largercounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,showcases competitive performance, providing a feasible solution to privacyissues.\rPrivacy-Aware Semantic Cache for Large Language Models\nWaris Gill Mohamed Elidrisi Pallavi Kalapatapu Ali Anwar Muhammad Ali Gulzar\nabstract\rabstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2have revolutionized natural language processing and search engine dynamics.However, these models incur exceptionally high computational costs. Forinstance, GPT-3 consists of 175 billion parameters and inference on thesemodels also demands billions of floating-point operations. Caching is a naturalsolution to reduce LLM inference costs on repeated queries. However, existingcaching methods are incapable of finding semantic similarities among LLMqueries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a semantic cache for LLMs that identifiessemantically similar queries to determine cache hit or miss. Using MeanCache,the response to a user\u0026rsquo;s semantically similar query can be retrieved from alocal cache rather than re-querying the LLM, thus reducing costs, serviceprovider load, and environmental impact. MeanCache leverages Federated Learning(FL) to collaboratively train a query similarity model in a distributed manneracross numerous users without violating privacy. By placing a local cache ineach user\u0026rsquo;s device and using FL, MeanCache reduces the latency and costs andenhances model performance, resulting in lower cache false hit rates. Ourexperiments, benchmarked against the GPTCache, reveal that MeanCache attains anapproximately 17% higher F-score and a 20% increase in precision duringsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces thestorage requirement by 83% and accelerates semantic cache hit-and-missdecisions by 11%, while still surpassing GPTCache.\r2024-03-04\nDifferentially Private Representation Learning via Image Captioning\nTom Sander Yaodong Yu Maziar Sanjabi Alain Durmus Yi Ma Kamalika Chaudhuri Chuan Guo\nabstract\rabstract: Differentially private (DP) machine learning is considered the gold-standardsolution for training a model from sensitive data while still preservingprivacy. However, a major barrier to achieving this ideal is its sub-optimalprivacy-accuracy trade-off, which is particularly visible in DP representationlearning. Specifically, it has been shown that under modest privacy budgets,most models learn representations that are not significantly better thanhand-crafted features. In this work, we show that effective DP representationlearning can be done via image captioning and scaling up to internet-scalemultimodal datasets. Through a series of engineering tricks, we successfullytrain a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratchusing a reasonable amount of computation, and obtaining unprecedentedhigh-quality image features that can be used in a variety of downstream visionand vision-language tasks. For example, under a privacy budget of$\\varepsilon=8$, a linear classifier trained on top of learned DP-Cap featuresattains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTAof 56.5%. Our work challenges the prevailing sentiment that high-utility DPrepresentation learning cannot be achieved by training from scratch.\rVision-Language Models for Medical Report Generation and Visual Question Answering: A Review\nIryna Hartsock Ghulam Rasool\nabstract\rabstract: Medical vision-language models (VLMs) combine computer vision and naturallanguage processing to analyze visual and textual medical data. Our paperreviews recent advancements in developing VLMs specialized for healthcare,focusing on models designed for medical report generation and visual questionanswering. We provide background on natural language processing and computervision, explaining how techniques from both fields are integrated into VLMs toenable learning from multimodal data. Key areas we address include theexploration of medical vision-language datasets, in-depth analyses ofarchitectures and pre-training strategies employed in recent noteworthy medicalVLMs, and comprehensive discussion on evaluation metrics for assessing VLMs\u0026rsquo;performance in medical report generation and visual question answering. We alsohighlight current challenges and propose future directions, including enhancingclinical validity and addressing patient privacy concerns. Overall, our reviewsummarizes recent progress in developing VLMs to harness multimodal medicaldata for improved healthcare applications.\rSciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis\nHengxing Cai Xiaochen Cai Junhan Chang Sihang Li Lin Yao Changxin Wang Zhifeng Gao Yongge Li Mujie Lin Shuwen Yang Jiankun Wang Yuqi Yin Yaqi Li Linfeng Zhang Guolin Ke\nabstract\rabstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionizednatural language understanding and generation, igniting a surge of interest inleveraging these technologies for the nuanced field of scientific literatureanalysis. Existing benchmarks, however, inadequately evaluate the proficiencyof LLMs in the scientific domain, especially in scenarios involving complexcomprehension and multimodal data. In response, we introduced SciAssess, abenchmark tailored for the in-depth analysis of scientific literature, craftedto provide a thorough assessment of LLMs\u0026rsquo; efficacy. SciAssess focuses onevaluating LLMs\u0026rsquo; abilities in memorization, comprehension, and analysis withinscientific contexts. It includes representative tasks from diverse scientificfields, such as general chemistry, organic materials, and alloy materials. Andrigorous quality control measures ensure its reliability in terms ofcorrectness, anonymization, and copyright compliance. SciAssess evaluatesleading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying theirstrengths and areas for improvement and supporting the ongoing development ofLLM applications in scientific literature analysis. SciAssess and its resourcesare made available at https://sci-assess.github.io, offering a valuable toolfor advancing LLM capabilities in scientific literature analysis.\rPushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities\nZheng Lin Guanqiao Qu Qiyuan Chen Xianhao Chen Zhe Chen Kaibin Huang\nabstract\rabstract: Large language models (LLMs), which have shown remarkable capabilities, arerevolutionizing AI development and potentially shaping our future. However,given their multimodality, the status quo cloud-based deployment faces somecritical challenges: 1) long response time; 2) high bandwidth costs; and 3) theviolation of data privacy. 6G mobile edge computing (MEC) systems may resolvethese pressing issues. In this article, we explore the potential of deployingLLMs at the 6G edge. We start by introducing killer applications powered bymultimodal LLMs, including robotics and healthcare, to highlight the need fordeploying LLMs in the vicinity of end users. Then, we identify the criticalchallenges for LLM deployment at the edge and envision the 6G MEC architecturefor LLMs. Furthermore, we delve into two design aspects, i.e., edge trainingand edge inference for LLMs. In both aspects, considering the inherent resourcelimitations at the edge, we discuss various cutting-edge techniques, includingsplit learning/inference, parameter-efficient fine-tuning, quantization, andparameter-sharing inference, to facilitate the efficient deployment of LLMs.This article serves as a position paper for thoroughly identifying themotivation, challenges, and pathway for empowering LLMs at the 6G edge.\rRing-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?\nYu-Lin Tsai Chia-Yi Hsu Chulin Xie Chih-Hsun Lin Jia-You Chen Bo Li Pin-Yu Chen Chia-Mu Yu Chun-Ying Huang\nabstract\rabstract: Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion(SD), have recently demonstrated exceptional capabilities for generatinghigh-quality content. However, this progress has raised several concerns ofpotential misuse, particularly in creating copyrighted, prohibited, andrestricted content, or NSFW (not safe for work) images. While efforts have beenmade to mitigate such problems, either by implementing a safety filter at theevaluation stage or by fine-tuning models to eliminate undesirable concepts orstyles, the effectiveness of these safety measures in dealing with a wide rangeof prompts remains largely unexplored. In this work, we aim to investigatethese safety mechanisms by proposing one novel concept retrieval algorithm forevaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2Idiffusion models, where the whole evaluation can be prepared in advance withoutprior knowledge of the target model. Specifically, Ring-A-Bell first performsconcept extraction to obtain holistic representations for sensitive andinappropriate concepts. Subsequently, by leveraging the extracted concept,Ring-A-Bell automatically identifies problematic prompts for diffusion modelswith the corresponding generation of inappropriate content, allowing the userto assess the reliability of deployed safety mechanisms. Finally, weempirically validate our method by testing online services such as Midjourneyand various methods of concept removal. Our results show that Ring-A-Bell, bymanipulating safe prompting benchmarks, can transform prompts that wereoriginally regarded as safe to evade existing safety mechanisms, thus revealingthe defects of the so-called safety mechanisms which could practically lead tothe generation of harmful contents. Our codes are available athttps://github.com/chiayi-hsu/Ring-A-Bell.\rDifferentially Private Synthetic Data via Foundation Model APIs 2: Text\nChulin Xie Zinan Lin Arturs Backurs Sivakanth Gopi Da Yu Huseyin A Inan Harsha Nori Haotian Jiang Huishuai Zhang Yin Tat Lee Bo Li Sergey Yekhanin\nabstract\rabstract: Text data has become extremely valuable due to the emergence of machinelearning algorithms that learn from it. A lot of high-quality text datagenerated in the real world is private and therefore cannot be shared or usedfreely due to privacy concerns. Generating synthetic replicas of private textdata with a formal privacy guarantee, i.e., differential privacy (DP), offers apromising and scalable solution. However, existing methods necessitate DPfinetuning of large language models (LLMs) on private data to generate DPsynthetic data. This approach is not viable for proprietary LLMs (e.g.,GPT-3.5) and also demands considerable computational resources for open-sourceLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)algorithm to generate DP synthetic images with only API access to diffusionmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, thatapplies to the complex setting of text. We use API access to an LLM andgenerate DP synthetic text without any model training. We conduct comprehensiveexperiments on three benchmark datasets. Our results demonstrate that Aug-PEproduces DP synthetic text that yields competitive utility with the SOTA DPfinetuning baselines. This underscores the feasibility of relying solely on APIaccess of LLMs to produce high-quality DP synthetic texts, thereby facilitatingmore accessible routes to privacy-preserving LLM applications. Our code anddata are available at https://github.com/AI-secure/aug-pe.\r2024-03-03\nReMatch: Retrieval Enhanced Schema Matching with LLMs\nEitam Sheetrit Menachem Brief Moshik Mishaeli Oren Elisha\nabstract\rabstract: Schema matching is a crucial task in data integration, involving thealignment of a source database schema with a target schema to establishcorrespondence between their elements. This task is challenging due to textualand semantic heterogeneity, as well as differences in schema sizes. Althoughmachine-learning-based solutions have been explored in numerous studies, theyoften suffer from low accuracy, require manual mapping of the schemas for modeltraining, or need access to source schema data which might be unavailable dueto privacy concerns. In this paper we present a novel method, named ReMatch,for matching schemas using retrieval-enhanced Large Language Models (LLMs). Ourmethod avoids the need for predefined mapping, any model training, or access todata in the source database. In the ReMatch method the tables of the targetschema and the attributes of the source schema are first represented asstructured passage-based documents. For each source attribute document, weretrieve $J$ documents, representing target schema tables, according to theirsemantic relevance. Subsequently, we create a prompt for every source table,comprising all its attributes and their descriptions, alongside all attributesfrom the set of top $J$ target tables retrieved previously. We employ LLMsusing this prompt for the matching task, yielding a ranked list of $K$potential matches for each source attribute. Our experimental results on largereal-world schemas demonstrate that ReMatch significantly improves matchingcapabilities and outperforms other machine learning approaches. By eliminatingthe requirement for training data, ReMatch becomes a viable solution forreal-world scenarios.\rOn the Compressibility of Quantized Large Language Models\nYu Mao Weilan Wang Hongchao Du Nan Guan Chun Jason Xue\nabstract\rabstract: Deploying Large Language Models (LLMs) on edge or mobile devices offerssignificant benefits, such as enhanced data privacy and real-time processingcapabilities. However, it also faces critical challenges due to the substantialmemory requirement of LLMs. Quantization is an effective way of reducing themodel size while maintaining good performance. However, even afterquantization, LLMs may still be too big to fit entirely into the limited memoryof edge or mobile devices and have to be partially loaded from the storage tocomplete the inference. In this case, the I/O latency of model loading becomesthe bottleneck of the LLM inference latency. In this work, we take apreliminary step of studying applying data compression techniques to reducedata movement and thus speed up the inference of quantized LLM onmemory-constrained devices. In particular, we discussed the compressibility ofquantized LLMs, the trade-off between the compressibility and performance ofquantized LLMs, and opportunities to optimize both of them jointly.\r2024-03-02\nDetection and Analysis of Stress-Related Posts in Reddit Acamedic Communities\nNazzere Oryngozha Pakizar Shamoi Ayan Igali\nabstract\rabstract: Nowadays, the significance of monitoring stress levels and recognizing earlysigns of mental illness cannot be overstated. Automatic stress detection intext can proactively help manage stress and protect mental well-being. Intoday\u0026rsquo;s digital era, social media platforms reflect the psychologicalwell-being and stress levels within various communities. This study focuses ondetecting and analyzing stress-related posts in Reddit academic communities.Due to online education and remote work, these communities have become centralfor academic discussions and support. We classify text as stressed or not usingnatural language processing and machine learning classifiers, with Dreaddit asour training dataset, which contains labeled data from Reddit. Next, we collectand analyze posts from various academic subreddits. We identified that the mosteffective individual feature for stress detection is the Bag of Words, pairedwith the Logistic Regression classifier, achieving a 77.78% accuracy rate andan F1 score of 0.79 on the DReaddit dataset. This combination also performsbest in stress detection on human-annotated datasets, with a 72% accuracy rate.Our key findings reveal that posts and comments in professors Redditcommunities are the most stressful, compared to other academic levels,including bachelor, graduate, and Ph.D. students. This research contributes toour understanding of the stress levels within academic communities. It can helpacademic institutions and online communities develop measures and interventionsto address this issue effectively.\rInexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy\nJamie Hayes Ilia Shumailov Eleni Triantafillou Amr Khalifa Nicolas Papernot\nabstract\rabstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel\u0026rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to theirU-MIA'' counterparts). We propose a categorization of existing U-MIAs intopopulation U-MIAs\u0026rsquo;\u0026rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs\u0026rsquo;\u0026rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.\rKnowledge Sanitization of Large Language Models\nYoichi Ishibashi Hidetoshi Shimodaira\nabstract\rabstract: We explore a knowledge sanitization approach to mitigate the privacy concernsassociated with large language models (LLMs). LLMs trained on a large corpus ofWeb data can memorize and potentially reveal sensitive or confidentialinformation, raising critical security concerns. Our technique efficientlyfine-tunes these models using the Low-Rank Adaptation (LoRA) method, promptingthem to generate harmless responses such as ``I don\u0026rsquo;t know\u0026rsquo;\u0026rsquo; when queried aboutspecific information. Experimental results in a closed-book question-answeringtask show that our straightforward method not only minimizes particularknowledge leakage but also preserves the overall performance of LLMs. These twoadvantages strengthen the defense against extraction attacks and reduces theemission of harmful content such as hallucinations.\rEvaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data\nAritra Hota Soumyajit Chatterjee Sandip Chakraborty\nabstract\rabstract: Traditional human-in-the-loop-based annotation for time-series data likeinertial data often requires access to alternate modalities like video or audiofrom the environment. These alternate sources provide the necessary informationto the human annotator, as the raw numeric data is often too obfuscated evenfor an expert. However, this traditional approach has many concerns surroundingoverall cost, efficiency, storage of additional modalities, time, scalability,and privacy. Interestingly, recent large language models (LLMs) are alsotrained with vast amounts of publicly available alphanumeric data, which allowsthem to comprehend and perform well on tasks beyond natural languageprocessing. Naturally, this opens up a potential avenue to explore LLMs asvirtual annotators where the LLMs will be directly provided the raw sensor datafor annotation instead of relying on any alternate modality. Naturally, thiscould mitigate the problems of the traditional human-in-the-loop approach.Motivated by this observation, we perform a detailed study in this paper toassess whether the state-of-the-art (SOTA) LLMs can be used as virtualannotators for labeling time-series physical sensing data. To perform this in aprincipled manner, we segregate the study into two major phases. In the firstphase, we investigate the challenges an LLM like GPT-4 faces in comprehendingraw sensor data. Considering the observations from phase 1, in the next phase,we investigate the possibility of encoding the raw sensor data using SOTA SSLapproaches and utilizing the projected time-series data to get annotations fromthe LLM. Detailed evaluation with four benchmark HAR datasets shows thatSSL-based encoding and metric-based guidance allow the LLM to make morereasonable decisions and provide accurate annotations without requiringcomputationally expensive fine-tuning or sophisticated prompt engineering.\r2024-03-01\nBasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)\nSean Wellington\nabstract\rabstract: BasedAI is a distributed network of machines which introduces decentralizedinfrastructure capable of integrating Fully Homomorphic Encryption (FHE) withany large language model (LLM) connected to its network. The proposed frameworkembeds a default mechanism, called \u0026ldquo;Cerberus Squeezing\u0026rdquo;, into the miningprocess which enables the transformation of a standard LLMs into encryptedzero-knowledge LLMs, or \u0026ldquo;ZK-LLMs\u0026rdquo;, leveraging insights from generativeadversarial networks for data privacy. This novel quantization mechanismempowers BasedAI miners to process and respond to prompts derived from Userinteraction with LLMs without the need for decrypting either the queries ortheir corresponding responses. The introduction of Cerberus Squeezingsignificantly improves performance degradation caused by quantized functions incurrent FHE-compliant computing environments by proactively optimizing callsbetween users, miners, and validators.\rTalkin\u0026rsquo; \u0026lsquo;Bout AI Generation: Copyright and the Generative-AI Supply Chain\nKatherine Lee A. Feder Cooper James Grimmelmann\nabstract\rabstract: \u0026ldquo;Does generative AI infringe copyright?\u0026rdquo; is an urgent question. It is also adifficult question, for two reasons. First, \u0026ldquo;generative AI\u0026rdquo; is not just oneproduct from one company. It is a catch-all name for a massive ecosystem ofloosely related technologies, including conversational text chatbots likeChatGPT, image generators like Midjourney and DALL-E, coding assistants likeGitHub Copilot, and systems that compose music and create videos. These systemsbehave differently and raise different legal issues. The second problem is thatcopyright law is notoriously complicated, and generative-AI systems manage totouch on a great many corners of it: authorship, similarity, direct andindirect liability, fair use, and licensing, among much else. These issuescannot be analyzed in isolation, because there are connections everywhere. In this Article, we aim to bring order to the chaos. To do so, we introducethe generative-AI supply chain: an interconnected set of stages that transformtraining data (millions of pictures of cats) into generations (a new,potentially never-seen-before picture of a cat that has never existed).Breaking down generative AI into these constituent stages reveals all of theplaces at which companies and users make choices that have copyrightconsequences. It enables us to trace the effects of upstream technical designson downstream uses, and to assess who in these complicated sociotechnicalsystems bears responsibility for infringement when it happens. Because weengage so closely with the technology of generative AI, we are able to shedmore light on the copyright questions. We do not give definitive answers as towho should and should not be held liable. Instead, we identify the keydecisions that courts will need to make as they grapple with these issues, andpoint out the consequences that would likely flow from different liabilityregimes.\rDifferentially Private Knowledge Distillation via Synthetic Text Generation\nJames Flemings Murali Annavaram\nabstract\rabstract: Large Language models (LLMs) are achieving state-of-the-art performance inmany different downstream tasks. However, the increasing urgency of dataprivacy requires LLMs to train with Differential Privacy (DP) on private data.Concurrently it is also necessary to compress LLMs for real-life deployments onresource-constrained devices or latency-sensitive applications. Differentialprivacy and model compression generally must trade off utility loss to achievetheir objectives. Moreover, concurrently achieving both can result in even moreutility loss. To this end, we propose a novel differentially private knowledgedistillation algorithm that exploits synthetic data generated by adifferentially private LLM. The knowledge of a teacher model is transferredonto the student in two ways: one way from the synthetic data itself, the hardlabels, and the other way by the output distribution of the teacher modelevaluated on the synthetic data, the soft labels. Furthermore, if the teacherand student share a similar architectural structure, we can further distillknowledge by exploiting hidden representations. Our results show that ourframework substantially improves the utility over existing baselines withstrong privacy parameters, {\\epsilon} = 2, validating that we can successfullycompress autoregressive LLMs while preserving the privacy of training data.\rTeach LLMs to Phish: Stealing Private Information from Language Models\nAshwinee Panda Christopher A. Choquette-Choo Zhengming Zhang Yaoqing Yang Prateek Mittal\nabstract\rabstract: When large language models are trained on private data, it can be asignificant privacy risk for them to memorize and regurgitate sensitiveinformation. In this work, we propose a new practical data extraction attackthat we call \u0026ldquo;neural phishing\u0026rdquo;. This attack enables an adversary to target andextract sensitive or personally identifiable information (PII), e.g., creditcard numbers, from a model trained on user data with upwards of 10% attacksuccess rates, at times, as high as 50%. Our attack assumes only that anadversary can insert as few as 10s of benign-appearing sentences into thetraining dataset using only vague priors on the structure of the user data.\rSafeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training\nAlyssa Huang Peihan Liu Ryumei Nakada Linjun Zhang Wanrong Zhang\nabstract\rabstract: The surge in multimodal AI\u0026rsquo;s success has sparked concerns over data privacyin vision-and-language tasks. While CLIP has revolutionized multimodal learningthrough joint training on images and text, its potential to unintentionallydisclose sensitive information necessitates the integration ofprivacy-preserving mechanisms. We introduce a differentially private adaptationof the Contrastive Language-Image Pretraining (CLIP) model that effectivelyaddresses privacy concerns while retaining accuracy. Our proposed method,Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diversevision-and-language tasks such as image classification and visual questionanswering. We demonstrate that our approach retains performance on par with thestandard non-private CLIP model. Furthermore, we analyze our proposed algorithmunder linear representation settings. We derive the convergence rate of ouralgorithm and show a trade-off between utility and privacy when gradients areclipped per-batch and the loss function does not satisfy smoothness conditionsassumed in the literature for the analysis of DP-SGD.\r2024-02-29\nEROS: Entity-Driven Controlled Policy Document Summarization\nJoykirat Singh Sehban Fazili Rohan Jain Md Shad Akhtar\nabstract\rabstract: Privacy policy documents have a crucial role in educating individuals aboutthe collection, usage, and protection of users\u0026rsquo; personal data by organizations.However, they are notorious for their lengthy, complex, and convoluted languageespecially involving privacy-related entities. Hence, they pose a significantchallenge to users who attempt to comprehend organization\u0026rsquo;s data usage policy.In this paper, we propose to enhance the interpretability and readability ofpolicy documents by using controlled abstractive summarization \u0026ndash; we enforcethe generated summaries to include critical privacy-related entities (e.g.,data and medium) and organization\u0026rsquo;s rationale (e.g.,target and reason) incollecting those entities. To achieve this, we develop PD-Sum, apolicy-document summarization dataset with marked privacy-related entitylabels. Our proposed model, EROS, identifies critical entities through aspan-based entity extraction model and employs them to control the informationcontent of the summaries using proximal policy optimization (PPO). Comparisonshows encouraging improvement over various baselines. Furthermore, we furnishqualitative and human evaluations to establish the efficacy of EROS.\rTowards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models\nChen Qian Jie Zhang Wei Yao Dongrui Liu Zhenfei Yin Yu Qiao Yong Liu Jing Shao\nabstract\rabstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Moststudies concentrate on fully pre-trained LLMs to better understand and improveLLMs\u0026rsquo; trustworthiness. In this paper, to reveal the untapped potential ofpre-training, we pioneer the exploration of LLMs\u0026rsquo; trustworthiness during thisperiod, focusing on five key dimensions: reliability, privacy, toxicity,fairness, and robustness. To begin with, we apply linear probing to LLMs. Thehigh probing accuracy suggests that \\textit{LLMs in early pre-training canalready distinguish concepts in each trustworthiness dimension}. Therefore, tofurther uncover the hidden possibilities of pre-training, we extract steeringvectors from a LLM\u0026rsquo;s pre-training checkpoints to enhance the LLM\u0026rsquo;strustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutualinformation estimation is bounded by linear probing accuracy, we also probeLLMs with mutual information to investigate the dynamics of trustworthinessduring pre-training. We are the first to observe a similar two-phasephenomenon: fitting and compression~\\citep{shwartz2017opening}. This researchprovides an initial exploration of trustworthiness modeling during LLMpre-training, seeking to unveil new insights and spur further developments inthe field. We will make our code publicly accessible at\\url{https://github.com/ChnQ/TracingLLM}.\rSynthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting\nLisa Oakley Steven Holtzen Alina Oprea\nabstract\rabstract: Programmatically generating tight differential privacy (DP) bounds is a hardproblem. Two core challenges are (1) finding expressive, compact, and efficientencodings of the distributions of DP algorithms, and (2) state space explosionstemming from the multiple quantifiers and relational properties of the DPdefinition. We address the first challenge by developing a method for tight privacy andaccuracy bound synthesis using weighted model counting on binary decisiondiagrams, a state of the art technique from the artificial intelligence andautomated reasoning communities for exactly computing probabilitydistributions. We address the second challenge by developing a framework forleveraging inherent symmetries in DP algorithms. Our solution benefits fromongoing research in probabilistic programming languages, allowing us tosuccinctly and expressively represent different DP algorithms with approachablelanguage syntax that can be used by non-experts. We provide a detailed case study of our solution on the binary randomizedresponse algorithm. We also evaluate an implementation of our solution usingthe Dice probabilistic programming language for the randomized response andtruncated geometric above threshold algorithms. We compare to prior work onexact DP verification using Markov chain probabilistic model checking. Very fewexisting works consider mechanized analysis of accuracy guarantees for DPalgorithms. We additionally provide a detailed analysis using our technique forfinding tight accuracy bounds for DP algorithms.\rPRSA: Prompt Reverse Stealing Attacks against Large Language Models\nYong Yang Xuhong Zhang Yi Jiang Xi Chen Haoyu Wang Shouling Ji Zonghui Wang\nabstract\rabstract: Prompt, recognized as crucial intellectual property, enables large languagemodels (LLMs) to perform specific tasks without the need of fine-tuning,underscoring their escalating importance. With the rise of prompt-basedservices, such as prompt marketplaces and LLM applications, providers oftendisplay prompts\u0026rsquo; capabilities through input-output examples to attract users.However, this paradigm raises a pivotal security concern: does the exposure ofinput-output pairs pose the risk of potential prompt leakage, infringing on theintellectual property rights of the developers? To our knowledge, this problemstill has not been comprehensively explored yet. To remedy this gap, in thispaper, we perform the first in depth exploration and propose a novel attackframework for reverse-stealing prompts against commercial LLMs, namely PRSA.The main idea of PRSA is that by analyzing the critical features of theinput-output pairs, we mimic and gradually infer (steal) the target prompts. Indetail, PRSA mainly consists of two key phases: prompt mutation and promptpruning. In the mutation phase, we propose a prompt attention algorithm basedon differential feedback to capture these critical features for effectivelyinferring the target prompts. In the prompt pruning phase, we identify and maskthe words dependent on specific inputs, enabling the prompts to accommodatediverse inputs for generalization. Through extensive evaluation, we verify thatPRSA poses a severe threat in real world scenarios. We have reported thesefindings to prompt service providers and actively collaborate with them to takeprotective measures for prompt copyright.\rAn Unforgeable Publicly Verifiable Watermark for Large Language Models\nAiwei Liu Leyi Pan Xuming Hu Shu\u0026rsquo;ang Li Lijie Wen Irwin King Philip S. Yu\nabstract\rabstract: Recently, text watermarking algorithms for large language models (LLMs) havebeen proposed to mitigate the potential harms of text generated by LLMs,including fake news and copyright issues. However, current watermark detectionalgorithms require the secret key used in the watermark generation process,making them susceptible to security breaches and counterfeiting during publicdetection. To address this limitation, we propose an unforgeable publiclyverifiable watermark algorithm that uses two different neural networks forwatermark generation and detection, instead of using the same key at bothstages. Meanwhile, the token embedding parameters are shared between thegeneration and detection networks, which makes the detection network achieve ahigh accuracy very efficiently. Experiments demonstrate that our algorithmattains high detection accuracy and computational efficiency through neuralnetworks with a minimized number of parameters. Subsequent analysis confirmsthe high complexity involved in forging the watermark from the detectionnetwork. Our code and data are available at\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable_watermark}.\r2024-02-28\nFedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing\nTerence Jie Chua Wenhan Yu Jun Zhao Kwok-Yan Lam\nabstract\rabstract: The emergence of foundation models, including language and vision models, hasreshaped AI\u0026rsquo;s landscape, offering capabilities across various applications.Deploying and fine-tuning these large models, like GPT-3 and BERT, presentschallenges, especially in the current foundation model era. We introduceEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, weexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT usesadapters, emulators, and PEFT for federated model tuning, enhancing modelprivacy and memory efficiency. Adapters adjust pre-trained models, whileemulators give a compact representation of original models, addressing bothprivacy and efficiency. Adaptable to various neural networks, our approach alsouses deep reinforcement learning for hyper-parameter optimization. We testedFedPEAT in a unique scenario with a server participating in collaborativefederated tuning, showcasing its potential in tackling foundation modelchallenges.\rMedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices\nAbdul Basit Khizar Hussain Muhammad Abdullah Hanif Muhammad Shafique\nabstract\rabstract: Large language models (LLMs) are revolutionizing various domains with theirremarkable natural language processing (NLP) abilities. However, deploying LLMsin resource-constrained edge computing and embedded systems presentssignificant challenges. Another challenge lies in delivering medical assistancein remote areas with limited healthcare facilities and infrastructure. Toaddress this, we introduce MedAide, an on-premise healthcare chatbot. Itleverages tiny-LLMs integrated with LangChain, providing efficient edge-basedpreliminary medical diagnostics and support. MedAide employs modeloptimizations for minimal memory footprint and latency on embedded edge deviceswithout server infrastructure. The training process is optimized using low-rankadaptation (LoRA). Additionally, the model is trained on diverse medicaldatasets, employing reinforcement learning from human feedback (RLHF) toenhance its domain-specific capabilities. The system is implemented on variousconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77%accuracy in medical consultations and scores 56 in USMLE benchmark, enabling anenergy-efficient healthcare assistance platform that alleviates privacyconcerns due to edge-based deployment, thereby empowering the community.\r2024-02-27\nEmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models\nRuisi Zhang Farinaz Koushanfar\nabstract\rabstract: This paper introduces EmMark,a novel watermarking framework for protectingthe intellectual property (IP) of embedded large language models deployed onresource-constrained edge devices. To address the IP theft risks posed bymalicious end-users, EmMark enables proprietors to authenticate ownership byquerying the watermarked model weights and matching the inserted signatures.EmMark\u0026rsquo;s novelty lies in its strategic watermark weight parameters selection,nsuring robustness and maintaining model quality. Extensive proof-of-conceptevaluations of models from OPT and LLaMA-2 families demonstrate EmMark\u0026rsquo;sfidelity, achieving 100% success in watermark extraction with model performancepreservation. EmMark also showcased its resilience against watermark removaland forging attacks.\rBeyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs\nTanise Ceron Neele Falk Ana Barić Dmitry Nikolaev Sebastian Padó\nabstract\rabstract: Due to the widespread use of large language models (LLMs) in ubiquitoussystems, we need to understand whether they embed a specific worldview and whatthese views reflect. Recent studies report that, prompted with politicalquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclearwhether these leanings are reliable (robust to prompt variations) and whetherthe leaning is consistent across policies and political leaning. We propose aseries of tests which assess the reliability and consistency of LLMs\u0026rsquo; stanceson political statements based on a dataset of voting-advice questionnairescollected from seven EU countries and annotated for policy domains. We studyLLMs ranging in size from 7B to 70B parameters and find that their reliabilityincreases with parameter count. Larger models show overall stronger alignmentwith left-leaning parties but differ among policy programs: They evince a(left-wing) positive stance towards environment protection, social welfare butalso (right-wing) law and order, with no consistent preferences in foreignpolicy, migration, and economy.\rBASES: Large-scale Web Search User Simulation with Large Language Model based Agents\nRuiyang Ren Peng Qiu Yingqi Qu Jing Liu Wayne Xin Zhao Hua Wu Ji-Rong Wen Haifeng Wang\nabstract\rabstract: Due to the excellent capacities of large language models (LLMs), it becomesfeasible to develop LLM-based agents for reliable user simulation. Consideringthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,we conduct large-scale user simulation for web search, to improve the analysisand modeling of user search behavior. Specially, we propose BASES, a novel usersimulation framework with LLM-based agents, designed to facilitatecomprehensive simulations of web search user behaviors. Our simulationframework can generate unique user profiles at scale, which subsequently leadsto diverse search behaviors. To demonstrate the effectiveness of BASES, weconduct evaluation experiments based on two human benchmarks in both Chineseand English, demonstrating that BASES can effectively simulate large-scalehuman-like search behaviors. To further accommodate the research on web search,we develop WARRIORS, a new large-scale dataset encompassing web search userbehaviors, including both Chinese and English versions, which can greatlybolster research in the field of information retrieval. Our code and data willbe publicly released soon.\rGenerative AI and Copyright: A Dynamic Perspective\nS. Alex Yang Angela Huyue Zhang\nabstract\rabstract: The rapid advancement of generative AI is poised to disrupt the creativeindustry. Amidst the immense excitement for this new technology, its futuredevelopment and applications in the creative industry hinge crucially upon twocopyright issues: 1) the compensation to creators whose content has been usedto train generative AI models (the fair use standard); and 2) the eligibilityof AI-generated content for copyright protection (AI-copyrightability). Whileboth issues have ignited heated debates among academics and practitioners, mostanalysis has focused on their challenges posed to existing copyright doctrines.In this paper, we aim to better understand the economic implications of thesetwo regulatory issues and their interactions. By constructing a dynamic modelwith endogenous content creation and AI model development, we unravel theimpacts of the fair use standard and AI-copyrightability on AI development, AIcompany profit, creators income, and consumer welfare, and how these impactsare influenced by various economic and operational factors. For example, whilegenerous fair use (use data for AI training without compensating the creator)benefits all parties when abundant training data exists, it can hurt creatorsand consumers when such data is scarce. Similarly, stronger AI-copyrightability(AI content enjoys more copyright protection) could hinder AI development andreduce social welfare. Our analysis also highlights the complex interplaybetween these two copyright issues. For instance, when existing training datais scarce, generous fair use may be preferred only when AI-copyrightability isweak. Our findings underscore the need for policymakers to embrace a dynamic,context-specific approach in making regulatory decisions and provide insightsfor business leaders navigating the complexities of the global regulatoryenvironment.\r2024-02-26\nPandora\u0026rsquo;s White-Box: Increased Training Data Leakage in Open LLMs\nJeffrey G. Wang Jason Wang Marvin Li Seth Neel\nabstract\rabstract: In this paper we undertake a systematic study of privacy attacks against opensource Large Language Models (LLMs), where an adversary has access to eitherthe model weights, gradients, or losses, and tries to exploit them to learnsomething about the underlying training data. Our headline results are thefirst membership inference attacks (MIAs) against pre-trained LLMs that areable to simultaneously achieve high TPRs and low FPRs, and a pipeline showingthat over $50%$ (!) of the fine-tuning dataset can be extracted from afine-tuned LLM in natural settings. We consider varying degrees of access tothe underlying model, customization of the language model, and resourcesavailable to the attacker. In the pre-trained setting, we propose three newwhite-box MIAs: an attack based on the gradient norm, a supervised neuralnetwork classifier, and a single step loss ratio attack. All outperformexisting black-box baselines, and our supervised attack closes the gap betweenMIA attack success against LLMs and other types of models. In fine-tuning, wefind that given access to the loss of the fine-tuned and base models, afine-tuned loss ratio attack FLoRA is able to achieve near perfect MIApeformance. We then leverage these MIAs to extract fine-tuning data fromfine-tuned language models. We find that the pipeline of generating fromfine-tuned models prompted with a small snippet of the prefix of each trainingexample, followed by using FLoRa to select the most likely training sample,succeeds the majority of the fine-tuning dataset after only $3$ epochs offine-tuning. Taken together, these findings show that highly effective MIAs areavailable in almost all LLM training settings, and highlight that great caremust be taken before LLMs are fine-tuned on highly sensitive data and thendeployed.\rDecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\nBoxin Wang Weixin Chen Hengzhi Pei Chulin Xie Mintong Kang Chenhui Zhang Chejian Xu Zidi Xiong Ritik Dutta Rylan Schaeffer Sang T. Truong Simran Arora Mantas Mazeika Dan Hendrycks Zinan Lin Yu Cheng Sanmi Koyejo Dawn Song Bo Li\nabstract\rabstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance \u0026ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives \u0026ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .\rMobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\nOmkar Thawakar Ashmal Vayani Salman Khan Hisham Cholakal Rao M. Anwer Michael Felsberg Tim Baldwin Eric P. Xing Fahad Shahbaz Khan\nabstract\rabstract: \u0026ldquo;Bigger the better\u0026rdquo; has been the predominant trend in recent Large LanguageModels (LLMs) development. However, LLMs do not suit well for scenarios thatrequire on-device processing, energy efficiency, low memory footprint, andresponse efficiency. These requisites are crucial for privacy, security, andsustainable deployment. This paper explores the \u0026ldquo;less is more\u0026rdquo; paradigm byaddressing the challenge of designing accurate yet efficient Small LanguageModels (SLMs) for resource constrained devices. Our primary contribution is theintroduction of an accurate and fully transparent open-source 0.5 billion(0.5B) parameter SLM, named MobiLlama, catering to the specific needs ofresource-constrained computing with an emphasis on enhanced performance withreduced resource demands. MobiLlama is a SLM design that initiates from alarger model and applies a careful parameter sharing scheme to reduce both thepre-training and the deployment cost. Our work strives to not only bridge thegap in open-source SLMs but also ensures full transparency, where completetraining data pipeline, training code, model weights, and over 300 checkpointsalong with evaluation codes is available at :https://github.com/mbzuai-oryx/MobiLlama.\rA Paradigm Shift: The Future of Machine Translation Lies with Large Language Models\nChenyang Lyu Zefeng Du Jitao Xu Yitao Duan Minghao Wu Teresa Lynn Alham Fikri Aji Derek F. Wong Longyue Wang\nabstract\rabstract: Machine Translation (MT) has greatly advanced over the years due to thedevelopments in deep neural networks. However, the emergence of Large LanguageModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MTdomain. In this context, we believe that the future of MT is intricately tiedto the capabilities of LLMs. These models not only offer vast linguisticunderstandings but also bring innovative methodologies, such as prompt-basedtechniques, that have the potential to further elevate MT. In this paper, weprovide an overview of the significant enhancements in MT that are influencedby LLMs and advocate for their pivotal role in upcoming MT research andimplementations. We highlight several new MT directions, emphasizing thebenefits of LLMs in scenarios such as Long-Document Translation, StylizedTranslation, and Interactive Translation. Additionally, we address theimportant concern of privacy in LLM-driven MT and suggest essentialprivacy-preserving strategies. By showcasing practical instances, we aim todemonstrate the advantages that LLMs offer, particularly in tasks liketranslating extended documents. We conclude by emphasizing the critical role ofLLMs in guiding the future evolution of MT and offer a roadmap for futureexploration in the sector.\rLLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery\nKexin Chen Yuyang Du Tao You Mobarakol Islam Ziyu Guo Yueming Jin Guangyong Chen Pheng-Ann Heng\nabstract\rabstract: Visual question answering (VQA) can be fundamentally crucial for promotingrobotic-assisted surgical education. In practice, the needs of trainees areconstantly evolving, such as learning more surgical types, adapting todifferent robots, and learning new surgical instruments and techniques for onesurgery. Therefore, continually updating the VQA system by a sequential datastream from multiple resources is demanded in robotic surgery to address newtasks. In surgical scenarios, the storage cost and patient data privacy oftenrestrict the availability of old data when updating the model, necessitating anexemplar-free continual learning (CL) setup. However, prior studies overlookedtwo vital problems of the surgical domain: i) large domain shifts from diversesurgical operations collected from multiple departments or clinical centers,and ii) severe data imbalance arising from the uneven presence of surgicalinstruments or activities during surgical procedures. This paper proposes toaddress these two problems with a multimodal large language model (LLM) and anadaptive weight assignment methodology. We first develop a new multi-teacher CLframework that leverages a multimodal LLM as the additional teacher. The stronggeneralization ability of the LLM can bridge the knowledge gap when domainshifts and data imbalances occur. We then put forth a novel data processingmethod that transforms complex LLM embeddings into logits compatible with ourCL framework. We further design an adaptive weight assignment approach thatbalances the generalization ability of the LLM and the domain expertise of theold CL model. We construct a new dataset for surgical VQA tasks, providingvaluable data resources for future research. Extensive experimental results onthree datasets demonstrate the superiority of our method to other advanced CLmodels.\rKnowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking\nSamuel Kernan Freire Chaofan Wang Mina Foosherian Stefan Wellsandt Santiago Ruiz-Arenas Evangelos Niforatos\nabstract\rabstract: Recent advances in natural language processing enable more intelligent waysto support knowledge sharing in factories. In manufacturing, operatingproduction lines has become increasingly knowledge-intensive, putting strain ona factory\u0026rsquo;s capacity to train and support new operators. This paper introducesa Large Language Model (LLM)-based system designed to retrieve information fromthe extensive knowledge contained in factory documentation and knowledge sharedby expert operators. The system aims to efficiently answer queries fromoperators and facilitate the sharing of new knowledge. We conducted a userstudy at a factory to assess its potential impact and adoption, elicitingseveral perceived benefits, namely, enabling quicker information retrieval andmore efficient resolution of issues. However, the study also highlighted apreference for learning from a human expert when such an option is available.Furthermore, we benchmarked several commercial and open-sourced LLMs for thissystem. The current state-of-the-art model, GPT-4, consistently outperformedits counterparts, with open-source models trailing closely, presenting anattractive option given their data privacy and customization benefits. Insummary, this work offers preliminary insights and a system design forfactories considering using LLM tools for knowledge management.\rTricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks\nAbhinav Rao Sachin Vashistha Atharva Naik Somak Aditya Monojit Choudhury\nabstract\rabstract: Recent explorations with commercial Large Language Models (LLMs) have shownthat non-expert users can jailbreak LLMs by simply manipulating their prompts;resulting in degenerate output behavior, privacy and security breaches,offensive outputs, and violations of content regulator policies. Limitedstudies have been conducted to formalize and analyze these attacks and theirmitigations. We bridge this gap by proposing a formalism and a taxonomy ofknown (and possible) jailbreaks. We survey existing jailbreak methods and theireffectiveness on open-source and commercial LLMs (such as GPT-based models,OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreakdetection in terms of their effectiveness against known attacks. For ouranalysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. Wewill make the dataset public along with the model outputs.\rCodeS: Towards Building Open-source Language Models for Text-to-SQL\nHaoyang Li Jing Zhang Hanbing Liu Ju Fan Xiaokang Zhang Jun Zhu Renjie Wei Hongyan Pan Cuiping Li Hong Chen\nabstract\rabstract: Language models have shown promising performance on the task of translatingnatural language questions into SQL queries (Text-to-SQL). However, most of thestate-of-the-art (SOTA) approaches rely on powerful yet closed-source largelanguage models (LLMs), such as ChatGPT and GPT-4, which may have thelimitations of unclear model architectures, data privacy risks, and expensiveinference overheads. To address the limitations, we introduce CodeS, a seriesof pre-trained language models with parameters ranging from 1B to 15B,specifically designed for the text-to-SQL task. CodeS is a fully open-sourcelanguage model, which achieves superior accuracy with much smaller parametersizes. This paper studies the research challenges in building CodeS. To enhancethe SQL generation abilities of CodeS, we adopt an incremental pre-trainingapproach using a specifically curated SQL-centric corpus. Based on this, weaddress the challenges of schema linking and rapid domain adaptation throughstrategic prompt construction and a bi-directional data augmentation technique.We conduct comprehensive evaluations on multiple datasets, including the widelyused Spider benchmark, the newly released BIRD benchmark, robustness-diagnosticbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, aswell as two real-world datasets created for financial and academicapplications. The experimental results show that our CodeS achieves new SOTAaccuracy and robustness on nearly all challenging text-to-SQL benchmarks.\rUnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models\nYihua Zhang Yimeng Zhang Yuguang Yao Jinghan Jia Jiancheng Liu Xiaoming Liu Sijia Liu\nabstract\rabstract: The rapid advancement of diffusion models (DMs) has not only transformedvarious real-world industries but has also introduced negative societalconcerns, including the generation of harmful content, copyright disputes, andthe rise of stereotypes and biases. To mitigate these issues, machineunlearning (MU) has emerged as a potential solution, demonstrating its abilityto remove undesired generative capabilities of DMs in various applications.However, by examining existing MU evaluation methods, we uncover several keychallenges that can result in incomplete, inaccurate, or biased evaluations forMU in DMs. To address them, we enhance the evaluation metrics for MU, includingthe introduction of an often-overlooked retainability measurement for DMspost-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensivehigh-resolution stylized image dataset that facilitates us to evaluate theunlearning of artistic painting styles in conjunction with associated imageobjects. We show that this dataset plays a pivotal role in establishing astandardized and automated evaluation framework for MU techniques on DMs,featuring 7 quantitative metrics to address various aspects of unlearningeffectiveness. Through extensive experiments, we benchmark 5 state-of-the-artMU methods, revealing novel insights into their pros and cons, and theunderlying unlearning mechanisms. Furthermore, we demonstrate the potential ofUnlearnCanvas to benchmark other generative modeling tasks, such as styletransfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce allthe results in this work can be found athttps://github.com/OPTML-Group/UnlearnCanvas.\rPrivacy-Preserved Neural Graph Databases\nQi Hu Haoran Li Jiaxin Bai Zihao Wang Yangqiu Song\nabstract\rabstract: In the era of large language models (LLMs), efficient and accurate dataretrieval has become increasingly crucial for the use of domain-specific orprivate data in the retrieval augmented generation (RAG). Neural graphdatabases (NGDBs) have emerged as a powerful paradigm that combines thestrengths of graph databases (GDBs) and neural networks to enable efficientstorage, retrieval, and analysis of graph-structured data which can beadaptively trained with LLMs. The usage of neural embedding storage and Complexneural logical Query Answering (CQA) provides NGDBs with generalizationability. When the graph is incomplete, by extracting latent patterns andrepresentations, neural graph databases can fill gaps in the graph structure,revealing hidden relationships and enabling accurate query answering.Nevertheless, this capability comes with inherent trade-offs, as it introducesadditional privacy risks to the domain-specific or private databases. Maliciousattackers can infer more sensitive information in the database usingwell-designed queries such as from the answer sets of where Turing Awardwinners born before 1950 and after 1940 lived, the living places of TuringAward winner Hinton are probably exposed, although the living places may havebeen deleted in the training stage due to the privacy concerns. In this work,we propose a privacy-preserved neural graph database (P-NGDB) framework toalleviate the risks of privacy leakage in NGDBs. We introduce adversarialtraining techniques in the training stage to enforce the NGDBs to generateindistinguishable answers when queried with private information, enhancing thedifficulty of inferring sensitive information through combinations of multipleinnocuous queries.\r2024-02-25\nText Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations\nYafei Xiang Hanyi Yu Yulu Gong Shuning Huo Mengran Zhu\nabstract\rabstract: With the rapid development of artificial intelligence technology, Transformerstructural pre-training model has become an important tool for large languagemodel (LLM) tasks. In the field of e-commerce, these models are especiallywidely used, from text understanding to generating recommendation systems,which provide powerful technical support for improving user experience andoptimizing service processes. This paper reviews the core application scenariosof Transformer pre-training model in e-commerce text understanding andrecommendation generation, including but not limited to automatic generation ofproduct descriptions, sentiment analysis of user comments, construction ofpersonalized recommendation system and automated processing of customer serviceconversations. Through a detailed analysis of the model\u0026rsquo;s working principle,implementation process, and application effects in specific cases, this paperemphasizes the unique advantages of pre-trained models in understanding complexuser intentions and improving the quality of recommendations. In addition, thechallenges and improvement directions for the future are also discussed, suchas how to further improve the generalization ability of the model, the abilityto handle large-scale data sets, and technical strategies to protect userprivacy. Ultimately, the paper points out that the application of Transformerstructural pre-training models in e-commerce has not only driven technologicalinnovation, but also brought substantial benefits to merchants and consumers,and looking forward, these models will continue to play a key role ine-commerce and beyond.\rCognitive Bias in High-Stakes Decision-Making with LLMs\nJessica Echterhoff Yao Liu Abeer Alessa Julian McAuley Zexue He\nabstract\rabstract: Large language models (LLMs) offer significant potential as tools to supportan expanding range of decision-making tasks. However, given their training onhuman (created) data, LLMs can inherit both societal biases against protectedgroups, as well as be subject to cognitive bias. Such human-like bias canimpede fair and explainable decisions made with LLM assistance. Our workintroduces BiasBuster, a framework designed to uncover, evaluate, and mitigatecognitive bias in LLMs, particularly in high-stakes decision-making tasks.Inspired by prior research in psychology and cognitive sciences, we develop adataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,prompt-induced, sequential, inherent). We test various bias mitigationstrategies, amidst proposing a novel method using LLMs to debias their ownprompts. Our analysis provides a comprehensive picture on the presence andeffects of cognitive bias across different commercial and open-source models.We demonstrate that our self-help debiasing effectively mitigate cognitive biaswithout having to manually craft examples for each bias type.\r2024-02-24\nFoot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology\nZhenhua Wang Wei Xie Baosheng Wang Enze Wang Zhiwen Gui Shuoyoucheng Ma Kai Chen\nabstract\rabstract: Large Language Models (LLMs) have gradually become the gateway for people toacquire new knowledge. However, attackers can break the model\u0026rsquo;s securityprotection (\u0026ldquo;jail\u0026rdquo;) to access restricted information, which is called\u0026quot;jailbreaking.\u0026quot; Previous studies have shown the weakness of current LLMs whenconfronted with such jailbreaking attacks. Nevertheless, comprehension of theintrinsic decision-making mechanism within the LLMs upon receipt of jailbreakprompts is noticeably lacking. Our research provides a psychologicalexplanation of the jailbreak prompts. Drawing on cognitive consistency theory,we argue that the key to jailbreak is guiding the LLM to achieve cognitivecoordination in an erroneous direction. Further, we propose an automaticblack-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.This method progressively induces the model to answer harmful questions viamulti-step incremental prompts. We instantiated a prototype system to evaluatethe jailbreaking effectiveness on 8 advanced LLMs, yielding an average successrate of 83.9%. This study builds a psychological perspective on the explanatoryinsights into the intrinsic decision-making logic of LLMs.\r2024-02-23\nUser Inference Attacks on Large Language Models\nNikhil Kandpal Krishna Pillutla Alina Oprea Peter Kairouz Christopher A. Choquette-Choo Zheng Xu\nabstract\rabstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user\u0026rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user\u0026rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.\rFast Adversarial Attacks on Language Models In One GPU Minute\nVinu Sankar Sadasivan Shoumik Saha Gaurang Sriramanan Priyatham Kattakinda Atoosa Chegini Soheil Feizi\nabstract\rabstract: In this paper, we introduce a novel class of fast, beam search-basedadversarial attack (BEAST) for Language Models (LMs). BEAST employsinterpretable parameters, enabling attackers to balance between attack speed,success rate, and the readability of adversarial prompts. The computationalefficiency of BEAST facilitates us to investigate its applications on LMs forjailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-freetargeted attack can jailbreak aligned LMs with high attack success rates withinone minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minutewith a success rate of 89% when compared to a gradient-based baseline thattakes over an hour to achieve 70% success rate using a single Nvidia RTX A600048GB GPU. Additionally, we discover a unique outcome wherein our untargetedattack induces hallucinations in LM chatbots. Through human evaluations, wefind that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% moreincorrect outputs when compared to LM outputs in the absence of our attack. Wealso learn that 22% of the time, BEAST causes Vicuna to generate outputs thatare not relevant to the original prompt. Further, we use BEAST to generateadversarial prompts in a few seconds that can boost the performance of existingmembership inference attacks for LMs. We believe that our fast attack, BEAST,has the potential to accelerate research in LM security and privacy. Ourcodebase is publicly available at https://github.com/vinusankars/BEAST.\rThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)\nShenglai Zeng Jiankun Zhang Pengfei He Yue Xing Yiding Liu Han Xu Jie Ren Shuaiqiang Wang Dawei Yin Yi Chang Jiliang Tang\nabstract\rabstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs\u0026rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.\rWho Wrote this Code? Watermarking for Code Generation\nTaehyun Lee Seokhee Hong Jaewoo Ahn Ilgee Hong Hwaran Lee Sangdoo Yun Jamin Shin Gunhee Kim\nabstract\rabstract: With the remarkable generation performance of large language models, ethicaland legal concerns about using them have been raised, such as plagiarism andcopyright issues. For such concerns, several approaches to watermark and detectLLM-generated text have been proposed very recently. However, we discover thatthe previous methods fail to function appropriately with code generation tasksbecause of the syntactic and semantic characteristics of code. Based on\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,Selective WatErmarking via Entropy Thresholding (SWEET), that promotes \u0026ldquo;green\u0026quot;tokens only at the position with high entropy of the token distribution duringgeneration, thereby preserving the correctness of the generated code. Thewatermarked code is detected by the statistical test and Z-score based on theentropy information. Our experiments on HumanEval and MBPP show that SWEETsignificantly improves the Pareto Frontier between the code correctness andwatermark detection performance. We also show that notable post-hoc detectionmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show thatsetting a reasonable entropy threshold is not much of a challenge. Code isavailable at https://github.com/hongcheki/sweet-watermark.\rA First Look at GPT Apps: Landscape and Vulnerability\nZejun Zhang Li Zhang Xin Yuan Anlan Zhang Mengwei Xu Feng Qian\nabstract\rabstract: With the advancement of Large Language Models (LLMs), increasinglysophisticated and powerful GPTs are entering the market. Despite theirpopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs\u0026rsquo;susceptibility to attacks raises concerns over safety and plagiarism. Thus, inthis work, we conduct a pioneering exploration of GPT stores, aiming to studyvulnerabilities and plagiarism within GPT applications. To begin with, weconduct, to our knowledge, the first large-scale monitoring and analysis of twostores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, wepropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.To complete these two tasks efficiently, we develop two automated tools: onefor web scraping and another designed for programmatically interacting withGPTs. Our findings reveal a significant enthusiasm among users and developersfor GPT interaction and creation, as evidenced by the rapid increase in GPTsand their creators. However, we also uncover a widespread failure to protectGPT internals, with nearly 90% of system prompts easily accessible, leading toconsiderable plagiarism and duplication among GPTs.\r2024-02-22\nExploring Memorization in Fine-tuned Language Models\nShenglai Zeng Yaxin Li Jie Ren Yiding Liu Han Xu Pengfei He Yue Xing Shuaiqiang Wang Jiliang Tang Dawei Yin\nabstract\rabstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models\u0026rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.\rSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\nHuachuan Qiu Hongliang He Shuai Zhang Anqi Li Zhenzhong Lan\nabstract\rabstract: Developing specialized dialogue systems for mental health support requiresmulti-turn conversation data, which has recently garnered increasing attention.However, gathering and releasing large-scale and real-life multi-turnconversations to facilitate advancements in mental health presents challengesdue to data privacy protection, as well as the time and cost involved. Toaddress the challenges related to data scarcity, we introduce SMILE, asingle-turn to multi-turn inclusive language expansion technique that promptsChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our workbegins with the analysis of language transformation, validating the feasibilityof the proposed method when compared with other baseline methods. We thenconduct a study on dialogue diversity, including lexical features, semanticfeatures, and dialogue topics, demonstrating the effectiveness of our proposedmethod. Furthermore, we implement an expert evaluation and the resultsdemonstrate that the dialogues generated with our proposed method are of higherquality than those generated with other baseline methods. Thus, we employ ourmethod to generate a large-scale, diverse, and high-quality dialogue datasetnamed SmileChat, comprising 55,165 dialogues in total with an average of 10.4turns per dialogue. Finally, we utilize the collected corpus to develop amental health chatbot, MeChat. To better assess the overall quality ofSmileChat, we collect a real-life chat dataset comprising 82 counselingdialogues for model evaluation. Both automatic and human evaluationsdemonstrate that our trained dialogue system exhibits significant improvements,showcasing that SmileChat is high-quality and practical.\r2024-02-21\nTree of Attacks: Jailbreaking Black-Box LLMs Automatically\nAnay Mehrotra Manolis Zampetakis Paul Kassianik Blaine Nelson Hyrum Anderson Yaron Singer Amin Karbasi\nabstract\rabstract: While Large Language Models (LLMs) display versatile functionality, theycontinue to generate harmful, biased, and toxic content, as demonstrated by theprevalence of human-designed jailbreaks. In this work, we present Tree ofAttacks with Pruning (TAP), an automated method for generating jailbreaks thatonly requires black-box access to the target LLM. TAP utilizes an LLM toiteratively refine candidate (attack) prompts using tree-of-thought reasoninguntil one of the generated prompts jailbreaks the target. Crucially, beforesending prompts to the target, TAP assesses them and prunes the ones unlikelyto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigatea large search space of prompts and pruning reduces the total number of queriessent to the target. In empirical evaluations, we observe that TAP generatesprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)for more than 80% of the prompts using only a small number of queries.Interestingly, TAP is also capable of jailbreaking LLMs protected bystate-of-the-art guardrails, e.g., LlamaGuard. This significantly improves uponthe previous state-of-the-art black-box method for generating jailbreaks.\rLarge Language Models are Advanced Anonymizers\nRobin Staab Mark Vero Mislav Balunović Martin Vechev\nabstract\rabstract: Recent work in privacy research on large language models has shown that theyachieve near human-level performance at inferring personal data from real-worldonline texts. With consistently increasing model capabilities, existing textanonymization methods are currently lacking behind regulatory requirements andadversarial threats. This raises the question of how individuals caneffectively protect their personal data in sharing online texts. In this work,we take two steps to answer this question: We first present a new setting forevaluating anonymizations in the face of adversarial LLMs inferences, allowingfor a natural measurement of anonymization performance while remedying some ofthe shortcomings of previous metrics. We then present our LLM-based adversarialanonymization framework leveraging the strong inferential capabilities of LLMsto inform our anonymization procedure. In our experimental evaluation, we showon real-world and synthetic online texts how adversarial anonymizationoutperforms current industry-grade anonymizers both in terms of the resultingutility and privacy.\rPrivacy-Preserving Instructions for Aligning Large Language Models\nDa Yu Peter Kairouz Sewoong Oh Zheng Xu\nabstract\rabstract: Service providers of large language model (LLM) applications collect userinstructions in the wild and use them in further aligning LLMs with users\u0026rsquo;intentions. These instructions, which potentially contain sensitiveinformation, are annotated by human workers in the process. This poses a newprivacy risk not addressed by the typical private optimization. To this end, wepropose using synthetic instructions to replace real instructions in dataannotation and model fine-tuning. Formal differential privacy is guaranteed bygenerating those synthetic instructions using privately fine-tuned generators.Crucial in achieving the desired utility is our novel filtering algorithm thatmatches the distribution of the synthetic instructions to that of the realones. In both supervised fine-tuning and reinforcement learning from humanfeedback, our extensive experiments demonstrate the high utility of the finalset of synthetic instructions by showing comparable results to realinstructions. In supervised fine-tuning, models trained with private syntheticinstructions outperform leading open-source models such as Vicuna.\rDifferentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning\nZ Liu J Lou W Bao Z Qin K Ren\nabstract\rabstract: Finetuning on task-specific datasets is a widely-embraced paradigm ofharnessing the powerful capability of pretrained LLMs for various downstreamtasks. Due to the popularity of LLMs finetuning and its accompanying privacyconcerns, differentially private (DP) finetuning of pretrained LLMs hasgarnered increasing attention to safeguarding the privacy of task-specificdatasets. Lying at the design core of DP LLM finetuning methods is thesatisfactory tradeoff between privacy, utility, and scalability. Most existingmethods build upon the seminal work of DP-SGD. Despite pushing the scalabilityof DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunatelylimited by the inherent inefficiency of SGD. In this paper, we investigate thepotential of DP zeroth-order methods for LLM pretraining, which avoids thescalability bottleneck of SGD by approximating the gradient with the moreefficient zeroth-order gradient. Rather than treating the zeroth-order methodas a drop-in replacement for SGD, this paper presents a comprehensive studyboth theoretically and empirically. First, we propose the stagewise DPzeroth-order method that dynamically schedules key hyperparameters. This designis grounded on the synergy between DP random perturbation and the gradientapproximation error of the zeroth-order method, and its effect on finetuningtrajectory. Second, we further enhance the scalability by reducing thetrainable parameters that are identified by repurposing a data-free pruningtechnique requiring no additional data or extra privacy budget. We providetheoretical analysis for both proposed methods. We conduct extensive empiricalanalysis on both encoder-only masked language model and decoder-onlyautoregressive language model, achieving impressive results in terms ofscalability and utility.\r2024-02-20\nA Strategic Model of Software Dependency Networks\nCornelius Fritz Co-Pierre Georg Angelo Mele Michael Schweinberger\nabstract\rabstract: Modern software development involves collaborative efforts and reuse ofexisting code, which reduces the cost of developing new software. However,reusing code from existing packages exposes coders to vulnerabilities in thesedependencies. We study the formation of dependency networks among softwarepackages and libraries, guided by a structural model of network formation withobservable and unobservable heterogeneity. We estimate costs, benefits, andlink externalities of the network of 696,790 directed dependencies between35,473 repositories of the Rust programming language using a novel scalablealgorithm. We find evidence of a positive externality exerted on other coderswhen coders create dependencies. Furthermore, we show that coders are likely tolink to more popular packages of the same software type but less popularpackages of other types. We adopt models for the spread of infectious diseasesto measure a package\u0026rsquo;s systemicness as the number of downstream packages avulnerability would affect. Systemicness is highly skewed with the mostsystemic repository affecting almost 90% of all repositories only two stepsaway. Lastly, we show that protecting only the ten most important repositoriesreduces vulnerability contagion by nearly 40%.\rPrivacy Issues in Large Language Models: A Survey\nSeth Neel Peter Chang\nabstract\rabstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat https://github.com/safr-ml-lab/survey-llm.\rOn the Convergence of Zeroth-Order Federated Tuning for Large Language Models\nZhenqing Ling Daoyuan Chen Liuyi Yao Yaliang Li Ying Shen\nabstract\rabstract: The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area.\rTRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification\nMartin Gubri Dennis Ulmer Hwaran Lee Sangdoo Yun Seong Joon Oh\nabstract\rabstract: Large Language Model (LLM) services and models often come with legal rules onwho can use them and how they must use them. Assessing the compliance of thereleased LLMs is crucial, as these rules protect the interests of the LLMcontributor and prevent misuse. In this context, we describe the novel problemof Black-box Identity Verification (BBIV). The goal is to determine whether athird-party application uses a certain LLM through its chat function. Wepropose a method called Targeted Random Adversarial Prompt (TRAP) thatidentifies the specific LLM in use. We repurpose adversarial suffixes,originally proposed for jailbreaking, to get a pre-defined answer from thetarget LLM, while other models give random answers. TRAP detects the targetLLMs with over 95% true positive rate at under 0.2% false positive rate evenafter a single interaction. TRAP remains effective even if the LLM has minorchanges that do not significantly alter the original function.\rDoes Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?\nBo-Ru Lu Nikita Haduong Chia-Hsuan Lee Zeqiu Wu Hao Cheng Paul Koester Jean Utke Tao Yu Noah A. Smith Mari Ostendorf\nabstract\rabstract: The capabilities of pretrained language models have opened opportunities toexplore new application areas, but applications involving human-humaninteraction are limited by the fact that most data is protected from publicrelease for privacy reasons. Problem-solving human dialogues in realapplications can be much more complex than existing Wizard-of-Oz collections,preventing successful domain transfer. To support information extraction (IE)for a private call center dataset, we introduce a human-in-the-loop dialoguegeneration framework capable of synthesizing realistic dialogues. In IEexperiments with auto insurance call center dialogues, we observe 25% relativeimprovement in $F_1$ after augmenting a small set of real human conversationswith synthetic data. We release code and our synthetic dataset to illustratethe complexity of real-world call center conversations and encouragedevelopment of complex dialogue datasets that are more representative ofnatural data.\rRTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution\nShang Liu Wenji Fang Yao Lu Qijun Zhang Hongce Zhang Zhiyao Xie\nabstract\rabstract: The automatic generation of RTL code (e.g., Verilog) using natural languageinstructions and large language models (LLMs) has attracted significantresearch interest recently. However, most existing approaches heavily rely oncommercial LLMs such as ChatGPT, while open-source LLMs tailored for thisspecific design generation task exhibit notably inferior performance. Theabsence of high-quality open-source solutions restricts the flexibility anddata privacy of this emerging technique. In this study, we present a newcustomized LLM solution with a modest parameter count of only 7B, achievingbetter performance than GPT-3.5 on two representative benchmarks for RTL codegeneration. This remarkable balance between accuracy and efficiency is madepossible by leveraging our new RTL code dataset and a customized LLM algorithm,both of which will be made fully open-source. Furthermore, we have successfullyquantized our LLM to 4-bit with a total size of 4GB, enabling it to function ona single laptop with only slight performance degradation. This efficiencyallows the RTL generator to serve as a local assistant for engineers, ensuringall design privacy concerns are addressed.\rReducing Privacy Risks in Online Self-Disclosures with Language Models\nYao Dou Isadora Krsek Tarek Naous Anubha Kabra Sauvik Das Alan Ritter Wei Xu\nabstract\rabstract: Self-disclosure, while being common and rewarding in social mediainteraction, also poses privacy risks. In this paper, we take the initiative toprotect the user-side privacy associated with online self-disclosure throughdetection and abstraction. We develop a taxonomy of 19 self-disclosurecategories and curate a large corpus consisting of 4.8K annotated disclosurespans. We then fine-tune a language model for detection, achieving over 65%partial span F$_1$. We further conduct an HCI user study, with 82% ofparticipants viewing the model positively, highlighting its real-worldapplicability. Motivated by the user feedback, we introduce the task ofself-disclosure abstraction, which is paraphrasing disclosures into lessspecific terms while preserving their utility, e.g., \u0026ldquo;Im 16F\u0026rdquo; to \u0026ldquo;I\u0026rsquo;m a teenagegirl\u0026rdquo;. We explore various fine-tuning strategies, and our best model cangenerate diverse abstractions that moderately reduce privacy risks whilemaintaining high utility according to human evaluation. To help users indeciding which disclosures to abstract, we present a task of rating theirimportance for context understanding. Our fine-tuned model achieves 80%accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we willonly release our corpus to researchers who agree to ethical guidelines.\r2024-02-19\nGenerative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity\nClaudio Novelli Federico Casolari Philipp Hacker Giorgio Spedicato Luciano Floridi\nabstract\rabstract: The advent of Generative AI, particularly through Large Language Models(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AIlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,thereby broadening their application scope. However, the complexity andemergent autonomy of these models introduce challenges in predictability andlegal compliance. This paper delves into the legal and regulatory implicationsof Generative AI and LLMs in the European Union context, analyzing aspects ofliability, privacy, intellectual property, and cybersecurity. It criticallyexamines the adequacy of the existing and proposed EU legislation, includingthe Artificial Intelligence Act (AIA) draft, in addressing the uniquechallenges posed by Generative AI in general and LLMs in particular. The paperidentifies potential gaps and shortcomings in the legislative framework andproposes recommendations to ensure the safe and compliant deployment ofgenerative models, ensuring they align with the EU\u0026rsquo;s evolving digital landscapeand legal standards.\rIs Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports\nFelix J. Dorfner Liv Jürgensen Leonhard Donle Fares Al Mohamad Tobias R. Bodenmann Mason C. Cleveland Felix Busch Lisa C. Adams James Sato Thomas Schultz Albert E. Kim Jameson Merkow Keno K. Bressem Christopher P. Bridge\nabstract\rabstract: Introduction: With the rapid advances in large language models (LLMs), therehave been numerous new open source as well as commercial models. While recentpublications have explored GPT-4 in its application to extracting informationof interest from radiology reports, there has not been a real-world comparisonof GPT-4 to different leading open-source models. Materials and Methods: Two different and independent datasets were used. Thefirst dataset consists of 540 chest x-ray reports that were created at theMassachusetts General Hospital between July 2019 and July 2021. The seconddataset consists of 500 chest x-ray reports from the ImaGenome dataset. We thencompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to theopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accuratelylabel the presence of multiple findings in x-ray text reports using differentprompting techniques. Results: On the ImaGenome dataset, the best performing open-source model wasLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shotprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,respectively. On the institutional dataset, the best performing open-sourcemodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- andfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and0.973, respectively. Conclusion: In this paper, we show that while GPT-4 is superior toopen-source models in zero-shot report labeling, the implementation of few-shotprompting can bring open-source models on par with GPT-4. This shows thatopen-source models could be a performant and privacy preserving alternative toGPT-4 for the task of radiology report classification.\rCan AI-Generated Text be Reliably Detected?\nVinu Sankar Sadasivan Aounon Kumar Sriram Balasubramanian Wenxiao Wang Soheil Feizi\nabstract\rabstract: The unregulated use of LLMs can potentially lead to malicious consequencessuch as plagiarism, generating fake news, spamming, etc. Therefore, reliabledetection of AI-generated text can be critical to ensure the responsible use ofLLMs. Recent works attempt to tackle this problem either using certain modelsignatures present in the generated text outputs or by applying watermarkingtechniques that imprint specific patterns onto them. In this paper, we showthat these detectors are not reliable in practical scenarios. In particular, wedevelop a recursive paraphrasing attack to apply on AI text, which can break awhole range of detectors, including the ones using the watermarking schemes aswell as neural network-based detectors, zero-shot classifiers, andretrieval-based detectors. Our experiments include passages around 300 tokensin length, showing the sensitivity of the detectors even in the case ofrelatively long passages. We also observe that our recursive paraphrasing onlydegrades text quality slightly, measured via human studies, and metrics such asperplexity scores and accuracy on text benchmarks. Additionally, we show thateven LLMs protected by watermarking schemes can be vulnerable against spoofingattacks aimed to mislead detectors to classify human-written text asAI-generated, potentially causing reputational damages to the developers. Inparticular, we show that an adversary can infer hidden AI text signatures ofthe LLM outputs without having white-box access to the detection method.Finally, we provide a theoretical connection between the AUROC of the bestpossible detector and the Total Variation distance between human and AI textdistributions that can be used to study the fundamental hardness of thereliable detection problem for advanced language models. Our code is publiclyavailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.\rCopyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications\nXinwei Guo Yujun Li Yafeng Peng Xuetao Wei\nabstract\rabstract: As AIGC has impacted our society profoundly in the past years, ethical issueshave received tremendous attention. The most urgent one is the AIGC copyrightdilemma, which can immensely stifle the development of AIGC and greatly costthe entire society. Given the complexity of AIGC copyright governance and thefact that no perfect solution currently exists, previous work advocatedcopyleft on AI governance but without substantive analysis. In this paper, wetake a step further to explore the feasibility of copyleft to alleviate theAIGC copyright dilemma. We conduct a mixed-methods study from two aspects:qualitatively, we use a formal what-if analysis to clarify the dilemma andprovide case studies to show the feasibility of copyleft; quantitatively, weperform a carefully designed survey to find out how the public feels aboutcopylefting AIGC. The key findings include: a) people generally perceive thedilemma, b) they prefer to use authorized AIGC under loose restriction, and c)they are positive to copyleft in AIGC and willing to use it in the future.\rPurifying Large Language Models by Ensembling a Small Language Model\nTianlin Li Qian Liu Tianyu Pang Chao Du Qing Guo Yang Liu Min Lin\nabstract\rabstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.\rKnowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion\nJinheon Baek Nirupama Chandrasekaran Silviu Cucerzan Allen herring Sujay Kumar Jauhar\nabstract\rabstract: Large Language Models (LLMs) excel at tackling various natural languagetasks. However, due to the significant costs involved in re-training orfine-tuning them, they remain largely static and difficult to personalize.Nevertheless, a variety of applications could benefit from generations that aretailored to users\u0026rsquo; preferences, goals, and knowledge. Among them is web search,where knowing what a user is trying to accomplish, what they care about, andwhat they know can lead to improved search experiences. In this work, wepropose a novel and general approach that augments an LLM with relevant contextfrom users\u0026rsquo; interaction histories with a search engine in order to personalizeits outputs. Specifically, we construct an entity-centric knowledge store foreach user based on their search and browsing activities on the web, which isthen leveraged to provide contextually relevant LLM prompt augmentations. Thisknowledge store is light-weight, since it only produces user-specific aggregateprojections of interests and knowledge onto public knowledge graphs, andleverages existing search log infrastructure, thereby mitigating the privacy,compliance, and scalability concerns associated with building deep userprofiles for personalization. We validate our approach on the task ofcontextual query suggestion, which requires understanding not only the user\u0026rsquo;scurrent search context but also what they historically know and care about.Through a number of experiments based on human evaluation, we show that ourapproach is significantly better than several other LLM-powered baselines,generating query suggestions that are contextually more relevant, personalized,and useful.\rClass-incremental Learning for Time Series: Benchmark and Evaluation\nZhongzheng Qiao Quang Pham Zhen Cao Hoang H Le P. N. Suganthan Xudong Jiang Ramasamy Savitha\nabstract\rabstract: Real-world environments are inherently non-stationary, frequently introducingnew classes over time. This is especially common in time series classification,such as the emergence of new disease classification in healthcare or theaddition of new activities in human activity recognition. In such cases, alearning system is required to assimilate novel classes effectively whileavoiding catastrophic forgetting of the old ones, which gives rise to theClass-incremental Learning (CIL) problem. However, despite the encouragingprogress in the image and language domains, CIL for time series data remainsrelatively understudied. Existing studies suffer from inconsistent experimentaldesigns, necessitating a comprehensive evaluation and benchmarking of methodsacross a wide range of datasets. To this end, we first present an overview ofthe Time Series Class-incremental Learning (TSCIL) problem, highlight itsunique challenges, and cover the advanced methodologies. Further, based onstandardized settings, we develop a unified experimental framework thatsupports the rapid development of new algorithms, easy integration of newdatasets, and standardization of the evaluation process. Using this framework,we conduct a comprehensive evaluation of various generic andtime-series-specific CIL methods in both standard and privacy-sensitivescenarios. Our extensive experiments not only provide a standard baseline tosupport future research but also shed light on the impact of various designfactors such as normalization layers or memory budget thresholds. Codes areavailable at https://github.com/zqiao11/TSCIL.\rDistilling Large Language Models for Text-Attributed Graph Learning\nBo Pan Zheng Zhang Yifei Zhang Yuntong Hu Liang Zhao\nabstract\rabstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents.Graph models can efficiently learn TAGs, but their training heavily relies onhuman-annotated labels, which are scarce or even unavailable in manyapplications. Large language models (LLMs) have recently demonstratedremarkable capabilities in few-shot and zero-shot TAG learning, but they sufferfrom scalability, cost, and privacy issues. Therefore, in this work, we focuson synergizing LLMs and graph models with their complementary strengths bydistilling the power of LLMs to a local graph model on TAG learning. To addressthe inherent gaps between LLMs (generative models for texts) and graph models(discriminative models for graphs), we propose first to let LLMs teach aninterpreter with rich textual rationale and then let a student model mimic theinterpreter\u0026rsquo;s reasoning without LLMs\u0026rsquo; textual rationale. Extensive experimentsvalidate the efficacy of our proposed framework.\rEffective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters\nShahed Masoudian Cornelia Volaucnik Markus Schedl Navid Rekabsaz\nabstract\rabstract: Bias mitigation of Language Models has been the topic of many studies with arecent focus on learning separate modules like adapters for on-demanddebiasing. Besides optimizing for a modularized debiased model, it is oftencritical in practice to control the degree of bias reduction at inference time,e.g., in order to tune for a desired performance-fairness trade-off in searchresults or to control the strength of debiasing in classification tasks. Inthis paper, we introduce Controllable Gate Adapter (ConGater), a novel modulargating mechanism with adjustable sensitivity parameters, which allows for agradual transition from the biased state of the model to the fully debiasedversion at inference time. We demonstrate ConGater performance by (1)conducting adversarial debiasing experiments with three different models onthree classification tasks with four protected attributes, and (2) reducing thebias of search results through fairness list-wise regularization to enableadjusting a trade-off between performance and fairness metrics. Our experimentson the classification tasks show that compared to baselines of the samecaliber, ConGater can maintain higher task performance while containing lessinformation regarding the attributes. Our results on the retrieval task showthat the fully debiased ConGater can achieve the same fairness performancewhile maintaining more than twice as high task performance than recent strongbaselines. Overall, besides strong performance ConGater enables the continuoustransitioning between biased and debiased states of models, enhancingpersonalization of use and interpretability through controllability.\rNOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization\nImjin Ahn Hansle Gwon Young-Hak Kim Tae Joon Jun Sanghyun Park\nabstract\rabstract: The discharge summary is a one of critical documents in the patient journey,encompassing all events experienced during hospitalization, including multiplevisits, medications, tests, surgery/procedures, and admissions/discharge.Providing a summary of the patient\u0026rsquo;s progress is crucial, as it significantlyinfluences future care and planning. Consequently, clinicians face thelaborious and resource-intensive task of manually collecting, organizing, andcombining all the necessary data for a discharge summary. Therefore, we propose\u0026quot;NOTE\u0026quot;, which stands for \u0026ldquo;Notable generation Of patient Text summaries throughan Efficient approach based on direct preference optimization\u0026rdquo;. NOTE is basedon Medical Information Mart for Intensive Care- III dataset and summarizes asingle hospitalization of a patient. Patient events are sequentially combinedand used to generate a discharge summary for each hospitalization. In thepresent circumstances, large language models\u0026rsquo; application programminginterfaces (LLMs\u0026rsquo; APIs) are widely available, but importing and exportingmedical data presents significant challenges due to privacy protection policiesin healthcare institutions. Moreover, to ensure optimal performance, it isessential to implement a lightweight model for internal server or programwithin the hospital. Therefore, we utilized DPO and parameter efficient finetuning (PEFT) techniques to apply a fine-tuning method that guarantees superiorperformance. To demonstrate the practical application of the developed NOTE, weprovide a webpage-based demonstration software. In the future, we will aim todeploy the software available for actual use by clinicians in hospital. NOTEcan be utilized to generate various summaries not only discharge summaries butalso throughout a patient\u0026rsquo;s journey, thereby alleviating the labor-intensiveworkload of clinicians and aiming for increased efficiency.\rWhere It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages\nSameer Jain Sedrick Scott Keh Shova Chettri Karun Dewan Pablo Izquierdo Johanna Prussman Pooja Shreshtha Cesar Suarez Zheyuan Ryan Shi Lei Li Fei Fang\nabstract\rabstract: Environmental conservation organizations routinely monitor news content onconservation in protected areas to maintain situational awareness ofdevelopments that can have an environmental impact. Existing automated mediamonitoring systems require large amounts of data labeled by domain experts,which is only feasible at scale for high-resource languages like English.However, such tools are most needed in the global south where news of interestis mainly in local low-resource languages, and far fewer experts are availableto annotate datasets sustainably. In this paper, we propose NewsSerow, a methodto automatically recognize environmental conservation content in low-resourcelanguages. NewsSerow is a pipeline of summarization, in-context few-shotclassification, and self-reflection using large language models (LLMs). Usingat most 10 demonstration example news articles in Nepali, NewsSerowsignificantly outperforms other few-shot methods and achieves comparableperformance with models fully fine-tuned using thousands of examples. The WorldWide Fund for Nature (WWF) has deployed NewsSerow for media monitoring inNepal, significantly reducing their operational burden, and ensuring that AItools for conservation actually reach the communities that need them the most.NewsSerow has also been deployed for countries with other languages likeColombia.\rHU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?\nShubhashis Roy Dipta Sadat Shahriar\nabstract\rabstract: This paper describes our system developed for SemEval-2024 Task 8,\u0026ldquo;Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated TextDetection.\u0026rdquo; Machine-generated texts have been one of the main concerns due tothe use of large language models (LLM) in fake text generation, phishing,cheating in exams, or even plagiarizing copyright materials. A lot of systemshave been developed to detect machine-generated text. Nonetheless, the majorityof these systems rely on the text-generating model, a limitation that isimpractical in real-world scenarios, as it\u0026rsquo;s often impossible to know whichspecific model the user has used for text generation. In this work, we proposea single model based on contrastive learning, which uses ~40% of the baseline\u0026rsquo;sparameters (149M vs. 355M) but shows a comparable performance on the testdataset (21st out of 137 participants). Our key finding is that even without anensemble of multiple models, a single base model can have comparableperformance with the help of data augmentation and contrastive learning.\rOn Copyright Risks of Text-to-Image Diffusion Models\nYang Zhang Teoh Tze Tzun Lim Wei Hern Haonan Wang Kenji Kawaguchi\nabstract\rabstract: Diffusion models excel in many generative modeling tasks, notably in creatingimages from text prompts, a task referred to as text-to-image (T2I) generation.Despite the ability to generate high-quality images, these models oftenreplicate elements from their training data, leading to increasing copyrightconcerns in real applications in recent years. In response to this raisingconcern about copyright infringement, recent studies have studied the copyrightbehavior of diffusion models when using direct, copyrighted prompts. Ourresearch extends this by examining subtler forms of infringement, where evenindirect prompts can trigger copyright issues. Specifically, we introduce adata generation pipeline to systematically produce data for studying copyrightin diffusion models. Our pipeline enables us to investigate copyrightinfringement in a more practical setting, involving replicating visual featuresrather than entire works using seemingly irrelevant prompts for T2I generation.We generate data using our proposed pipeline to test various diffusion models,including the latest Stable Diffusion XL. Our findings reveal a widespreadtendency that these models tend to produce copyright-infringing content,highlighting a significant challenge in this field.\r2024-02-18\nStealthy Attack on Large Language Model based Recommendation\nJinghao Zhang Yuting Liu Qiang Liu Shu Wu Guibing Guo Liang Wang\nabstract\rabstract: Recently, the powerful large language models (LLMs) have been instrumental inpropelling the progress of recommender systems (RS). However, while thesesystems have flourished, their susceptibility to security threats has beenlargely overlooked. In this work, we reveal that the introduction of LLMs intorecommendation models presents new security vulnerabilities due to theiremphasis on the textual content of items. We demonstrate that attackers cansignificantly boost an item\u0026rsquo;s exposure by merely altering its textual contentduring the testing phase, without requiring direct interference with themodel\u0026rsquo;s training process. Additionally, the attack is notably stealthy, as itdoes not affect the overall recommendation performance and the modifications tothe text are subtle, making it difficult for users and platforms to detect. Ourcomprehensive experiments across four mainstream LLM-based recommendationmodels demonstrate the superior efficacy and stealthiness of our approach. Ourwork unveils a significant security gap in LLM-based recommendation systems andpaves the way for future research on protecting these systems.\rFederated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources\nJiamu Bai Daoyuan Chen Bingchen Qian Liuyi Yao Yaliang Li\nabstract\rabstract: Federated Learning (FL) has recently been applied to the parameter-efficientfine-tuning of Large Language Models (LLMs). While promising, it raisessignificant challenges due to the heterogeneous resources and datadistributions of clients.This study introduces FlexLoRA, a simple yet effectiveaggregation scheme for LLM fine-tuning, which mitigates the \u0026ldquo;buckets effect\u0026rdquo; intraditional FL that restricts the potential of clients with ample resources bytying them to the capabilities of the least-resourced participants. FlexLoRAallows for dynamic adjustment of local LoRA ranks, fostering the development ofa global model imbued with broader, less task-specific knowledge. Bysynthesizing a full-size LoRA weight from individual client contributions andemploying Singular Value Decomposition (SVD) for weight redistribution,FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600clients performing diverse NLP tasks, our experiments validate the efficacy ofFlexLoRA, with the federated global model achieving up to a 3.1% averageimprovement in downstream NLP task performance. FlexLoRA\u0026rsquo;s practicality isfurther underscored by its seamless integration with existing LoRA-based FLmethods and theoretical analysis, offering a path toward scalable,privacy-preserving federated tuning for LLMs.\r2024-02-17\nUnderstanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention\nEunkyung Jo Yuin Jeong SoHyun Park Daniel A. Epstein Young-Ho Kim\nabstract\rabstract: Recent large language models (LLMs) offer the potential to support publichealth monitoring by facilitating health disclosure through open-endedconversations but rarely preserve the knowledge gained about individuals acrossrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents anopportunity to improve engagement and self-disclosure, but we lack anunderstanding of how LTM impacts people\u0026rsquo;s interaction with LLM-driven chatbotsin public health interventions. We examine the case of CareCall \u0026ndash; anLLM-driven voice chatbot with LTM \u0026ndash; through the analysis of 1,252 call logsand interviews with nine users. We found that LTM enhanced health disclosureand fostered positive perceptions of the chatbot by offering familiarity.However, we also observed challenges in promoting self-disclosure through LTM,particularly around addressing chronic health conditions and privacy concerns.We discuss considerations for LTM integration in LLM-driven chatbots for publichealth monitoring, including carefully deciding what topics need to beremembered in light of public health goals.\rAnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection\nQihang Zhou Guansong Pang Yu Tian Shibo He Jiming Chen\nabstract\rabstract: Zero-shot anomaly detection (ZSAD) requires detection models trained usingauxiliary data to detect anomalies without any training sample in a targetdataset. It is a crucial task when training data is not accessible due tovarious concerns, eg, data privacy, yet it is challenging since the models needto generalize to anomalies across different domains where the appearance offoreground objects, abnormal regions, and background features, such asdefects/tumors on different products/organs, can vary significantly. Recentlylarge pre-trained vision-language models (VLMs), such as CLIP, havedemonstrated strong zero-shot recognition ability in various vision tasks,including anomaly detection. However, their ZSAD performance is weak since theVLMs focus more on modeling the class semantics of the foreground objectsrather than the abnormality/normality in the images. In this paper we introducea novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD acrossdifferent domains. The key insight of AnomalyCLIP is to learn object-agnostictext prompts that capture generic normality and abnormality in an imageregardless of its foreground objects. This allows our model to focus on theabnormal image regions rather than the object semantics, enabling generalizednormality and abnormality recognition on diverse types of objects. Large-scaleexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIPachieves superior zero-shot performance of detecting and segmenting anomaliesin datasets of highly diverse class semantics from various defect inspectionand medical imaging domains. Code will be made available athttps://github.com/zqhang/AnomalyCLIP.\rKnowledge Editing on Black-box Large Language Models\nXiaoshuai Song Zhengyang Wang Keqing He Guanting Dong Yutao Mou Jinxu Zhao Weiran Xu\nabstract\rabstract: Knowledge editing (KE) aims to efficiently and precisely modify the behaviorof large language models (LLMs) to update specific knowledge without negativelyinfluencing other knowledge. Current research primarily focuses on white-boxLLMs editing, overlooking an important scenario: black-box LLMs editing, whereLLMs are accessed through interfaces and only textual output is available. Inthis paper, we first officially introduce KE on black-box LLMs and then proposea comprehensive evaluation framework to overcome the limitations of existingevaluations that are not applicable to black-box LLMs editing and lackcomprehensiveness. To tackle privacy leaks of editing data and styleover-editing in current methods, we introduce a novel postEdit framework,resolving privacy concerns through downstream post-processing and maintainingtextual style consistency via fine-grained editing to original responses.Experiments and analysis on two benchmarks demonstrate that postEditoutperforms all baselines and achieves strong generalization, especially withhuge improvements on style retention (average $+20.82%\\uparrow$).\rA Platform for the Biomedical Application of Large Language Models\nSebastian Lobentanzer Shaohong Feng The BioChatter Consortium Andreas Maier Cankun Wang Jan Baumbach Nils Krehl Qin Ma Julio Saez-Rodriguez\nabstract\rabstract: Current-generation Large Language Models (LLMs) have stirred enormousinterest in recent months, yielding great potential for accessibility andautomation, while simultaneously posing significant challenges and risk ofmisuse. To facilitate interfacing with LLMs in the biomedical space, while atthe same time safeguarding their functionalities through sensible constraints,we propose a dedicated, open-source framework: BioChatter. Based on open-sourcesoftware packages, we synergise the many functionalities that are currentlydeveloping around LLMs, such as knowledge integration / retrieval-augmentedgeneration, model chaining, and benchmarking, resulting in an easy-to-use andinclusive framework for application in many use cases of biomedicine. We focuson robust and user-friendly implementation, including ways to deployprivacy-preserving local open-source LLMs. We demonstrate use cases via twomulti-purpose web apps (https://chat.biocypher.org), and provide documentation,support, and an open community.\rLLM-based Federated Recommendation\nJujia Zhao Wenjie Wang Chen Xu Zhaochun Ren See-Kiong Ng Tat-Seng Chua\nabstract\rabstract: Large Language Models (LLMs), with their advanced contextual understandingabilities, have demonstrated considerable potential in enhancing recommendationsystems via fine-tuning methods. However, fine-tuning requires users\u0026rsquo; behaviordata, which poses considerable privacy risks due to the incorporation ofsensitive user information. The unintended disclosure of such data couldinfringe upon data protection laws and give rise to ethical issues. To mitigatethese privacy issues, Federated Learning for Recommendation (Fed4Rec) hasemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-basedrecommendation presents two main challenges: first, an increase in theimbalance of performance across clients, affecting the system\u0026rsquo;s efficiency overtime, and second, a high demand on clients\u0026rsquo; computational and storage resourcesfor local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-basedRecommendation (PPLR) framework. The PPLR framework employs two primarystrategies. First, it implements a dynamic balance strategy, which involves thedesign of dynamic parameter aggregation and adjustment of learning speed fordifferent clients during the training phase, to ensure relatively balancedperformance across all clients. Second, PPLR adopts a flexible storagestrategy, selectively retaining certain sensitive layers of the language modelon the client side while offloading non-sensitive layers to the server. Thisapproach aims to preserve user privacy while efficiently saving computationaland storage resources. Experimental results demonstrate that PPLR not onlyachieves a balanced performance among clients but also enhances overall systemperformance in a manner that is both computationally and storage-efficient,while effectively protecting user privacy.\r2024-02-16\nLarge Language Model Unlearning\nYuanshun Yao Xiaojun Xu Yang Liu\nabstract\rabstract: We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,on large language models (LLMs). We show at least three scenarios of aligningLLMs with human preferences can benefit from unlearning: (1) removing harmfulresponses, (2) erasing copyright-protected content as requested, and (3)reducing hallucinations. Unlearning, as an alignment technique, has threeadvantages. (1) It only requires negative (e.g. harmful) examples, which aremuch easier and cheaper to collect (e.g. via red teaming or user reporting)than positive (e.g. helpful and often human-written) examples required in RLHF(RL from human feedback). (2) It is computationally efficient. (3) It isespecially effective when we know which training samples cause the misbehavior.To the best of our knowledge, our work is among the first to explore LLMunlearning. We are also among the first to formulate the settings, goals, andevaluations in LLM unlearning. We show that if practitioners only have limitedresources, and therefore the priority is to stop generating undesirable outputsrather than to try to generate desirable outputs, unlearning is particularlyappealing. Despite only having negative samples, our ablation study shows thatunlearning can still achieve better alignment performance than RLHF with just2% of its computational time.\rWhen Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\nMinrui Xu Dusit Niyato Jiawen Kang Zehui Xiong Shiwen Mao Zhu Han Dong In Kim Khaled B. Letaief\nabstract\rabstract: AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.\rCausal ATE Mitigates Unintended Bias in Controlled Text Generation\nRahul Madhavan Kahini Wadhawan\nabstract\rabstract: We study attribute control in language models through the method of CausalAverage Treatment Effect (Causal ATE). Existing methods for the attributecontrol task in Language Models (LMs) check for the co-occurrence of words in asentence with the attribute of interest, and control for them. However,spurious correlation of the words with the attribute in the training dataset,can cause models to hallucinate the presence of the attribute when presentedwith the spurious correlate during inference. We show that the simpleperturbation-based method of Causal ATE removes this unintended effect.Specifically, we ground it in the problem of toxicity mitigation, where asignificant challenge lies in the inadvertent bias that often emerges towardsprotected groups post detoxification. We show that this unintended bias can besolved by the use of the Causal ATE metric and rigorously prove our claim. Weprovide experimental validations for our claims and release our code(anonymously) here:https://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias.\rUser Experience Design Professionals\u0026rsquo; Perceptions of Generative Artificial Intelligence\nJie Li Hancheng Cao Laura Lin Youyang Hou Ruihao Zhu Abdallah El Ali\nabstract\rabstract: Among creative professionals, Generative Artificial Intelligence (GenAI) hassparked excitement over its capabilities and fear over unanticipatedconsequences. How does GenAI impact User Experience Design (UXD) practice, andare fears warranted? We interviewed 20 UX Designers, with diverse experienceand across companies (startups to large enterprises). We probed them tocharacterize their practices, and sample their attitudes, concerns, andexpectations. We found that experienced designers are confident in theiroriginality, creativity, and empathic skills, and find GenAI\u0026rsquo;s role asassistive. They emphasized the unique human factors of \u0026ldquo;enjoyment\u0026rdquo; and\u0026quot;agency\u0026quot;, where humans remain the arbiters of \u0026ldquo;AI alignment\u0026rdquo;. However, skilldegradation, job replacement, and creativity exhaustion can adversely impactjunior designers. We discuss implications for human-GenAI collaboration,specifically copyright and ownership, human creativity and agency, and AIliteracy and access. Through the lens of responsible and participatory AI, wecontribute a deeper understanding of GenAI fears and opportunities for UXD.\r2024-02-15\nRethinking Machine Unlearning for Large Language Models\nSijia Liu Yuanshun Yao Jinghan Jia Stephen Casper Nathalie Baracaldo Peter Hase Xiaojun Xu Yuguang Yao Hang Li Kush R. Varshney Mohit Bansal Sanmi Koyejo Yang Liu\nabstract\rabstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.\rUnmemorization in Large Language Models via Self-Distillation and Deliberate Imagination\nYijiang River Dong Hongzhou Lin Mikhail Belkin Ramon Huerta Ivan Vulić\nabstract\rabstract: While displaying impressive generation capabilities across many tasks, LargeLanguage Models (LLMs) still struggle with crucial issues of privacy violationand unwanted exposure of sensitive data. This raises an essential question: howshould we prevent such undesired behavior of LLMs while maintaining theirstrong generation and natural language understanding (NLU) capabilities? Inthis work, we introduce a novel approach termed deliberate imagination in thecontext of LLM unlearning. Instead of trying to forget memorized data, weemploy a self-distillation framework, guiding LLMs to deliberately imaginealternative scenarios. As demonstrated in a wide range of experiments, theproposed method not only effectively unlearns targeted text but also preservesthe LLMs\u0026rsquo; capabilities in open-ended generation tasks as well as in NLU tasks.Our results demonstrate the usefulness of this approach across different modelsand sizes, and also with parameter-efficient fine-tuning, offering a novelpathway to addressing the challenges with private and sensitive data in LLMapplications.\rExploring the Adversarial Capabilities of Large Language Models\nLukas Struppek Minh Hieu Le Dominik Hintersdorf Kristian Kersting\nabstract\rabstract: The proliferation of large language models (LLMs) has sparked widespread andgeneral interest due to their strong language generation capabilities, offeringgreat potential for both industry and research. While previous research delvedinto the security and privacy issues of LLMs, the extent to which these modelscan exhibit adversarial behavior remains largely unexplored. Addressing thisgap, we investigate whether common publicly available LLMs have inherentcapabilities to perturb text samples to fool safety measures, so-calledadversarial examples resp.~attacks. More specifically, we investigate whetherLLMs are inherently able to craft adversarial examples out of benign samples tofool existing safe rails. Our experiments, which focus on hate speechdetection, reveal that LLMs succeed in finding adversarial perturbations,effectively undermining hate speech detection systems. Our findings carrysignificant implications for (semi-)autonomous systems relying on LLMs,highlighting potential challenges in their interaction with existing systemsand safety measures.\rAbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns\nAshfak Md Shibli Mir Mehedi A. Pritom Maanak Gupta\nabstract\rabstract: SMS phishing, also known as \u0026ldquo;smishing\u0026rdquo;, is a growing threat that tricks usersinto disclosing private information or clicking into URLs with maliciouscontent through fraudulent mobile text messages. In recent past, we have alsoobserved a rapid advancement of conversational generative AI chatbot services(e.g., OpenAI\u0026rsquo;s ChatGPT, Google\u0026rsquo;s BARD), which are powered by pre-trained largelanguage models (LLMs). These AI chatbots certainly have a lot of utilities butit is not systematically understood how they can play a role in creatingthreats and attacks. In this paper, we propose AbuseGPT method to show how theexisting generative AI-based chatbot services can be exploited by attackers inreal world to create smishing texts and eventually lead to craftier smishingcampaigns. To the best of our knowledge, there is no pre-existing work thatevidently shows the impacts of these generative text-based models on creatingSMS phishing. Thus, we believe this study is the first of its kind to shedlight on this emerging cybersecurity threat. We have found strong empiricalevidences to show that attackers can exploit ethical standards in the existinggenerative AI-based chatbot services by crafting prompt injection attacks tocreate newer smishing campaigns. We also discuss some future researchdirections and guidelines to protect the abuse of generative AI-based servicesand safeguard users from smishing attacks.\rDetecting Phishing Sites Using ChatGPT\nTakashi Koide Naoki Fukushi Hiroki Nakano Daiki Chiba\nabstract\rabstract: The emergence of Large Language Models (LLMs), including ChatGPT, is having asignificant impact on a wide range of fields. While LLMs have been extensivelyresearched for tasks such as code generation and text synthesis, theirapplication in detecting malicious web content, particularly phishing sites,has been largely unexplored. To combat the rising tide of cyber attacks due tothe misuse of LLMs, it is important to automate detection by leveraging theadvanced capabilities of LLMs. In this paper, we propose a novel system called ChatPhishDetector thatutilizes LLMs to detect phishing sites. Our system involves leveraging a webcrawler to gather information from websites, generating prompts for LLMs basedon the crawled data, and then retrieving the detection results from theresponses generated by the LLMs. The system enables us to detect multilingualphishing sites with high accuracy by identifying impersonated brands and socialengineering techniques in the context of the entire website, without the needto train machine learning models. To evaluate the performance of our system, weconducted experiments on our own dataset and compared it with baseline systemsand several LLMs. The experimental results using GPT-4V demonstratedoutstanding performance, with a precision of 98.7% and a recall of 99.6%,outperforming the detection results of other LLMs and existing systems. Thesefindings highlight the potential of LLMs for protecting users from onlinefraudulent activities and have important implications for enhancingcybersecurity measures.\r2024-02-14\nTowards Privacy-Aware Sign Language Translation at Scale\nPhillip Rust Bowen Shi Skyler Wang Necati Cihan Camgöz Jean Maillard\nabstract\rabstract: A major impediment to the advancement of sign language translation (SLT) isdata scarcity. Much of the sign language data currently available on the webcannot be used for training supervised models due to the lack of alignedcaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bearsprivacy risks due to the presence of biometric information, which theresponsible development of SLT technologies should account for. In this work,we propose a two-stage framework for privacy-aware SLT at scale that addressesboth of these issues. We introduce SSVP-SLT, which leverages self-supervisedvideo pretraining on anonymized and unannotated videos, followed by supervisedSLT finetuning on a curated parallel dataset. SSVP-SLT achievesstate-of-the-art finetuned and zero-shot gloss-free SLT performance on theHow2Sign dataset, outperforming the strongest respective baselines by over 3BLEU-4. Based on controlled experiments, we further discuss the advantages andlimitations of self-supervised pretraining and anonymization via facialobfuscation for SLT.\rCopyright Traps for Large Language Models\nMatthieu Meeus Igor Shilov Manuel Faysse Yves-Alexandre de Montjoye\nabstract\rabstract: Questions of fair use of copyright-protected content to train Large LanguageModels (LLMs) are being very actively debated. Document-level inference hasbeen proposed as a new task: inferring from black-box access to the trainedmodel whether a piece of content has been seen during training. SOTA methodshowever rely on naturally occurring memorization of (part of) the content.While very effective against models that memorize a lot, we hypothesize\u0026ndash;andlater confirm\u0026ndash;that they will not work against models that do not naturallymemorize, e.g. medium-size 1B models. We here propose to use copyright traps,the inclusion of fictitious entries in original content, to detect the use ofcopyrighted materials in LLMs with a focus on models where memorization doesnot naturally occur. We carefully design an experimental setup, randomlyinserting traps into original content (books) and train a 1.3B LLM. We firstvalidate that the use of content in our target model would be undetectableusing existing methods. We then show, contrary to intuition, that evenmedium-length trap sentences repeated a significant number of times (100) arenot detectable using existing methods. However, we show that longer sequencesrepeated a large number of times can be reliably detected (AUC=0.75) and usedas copyright traps. We further improve these results by studying how the numberof times a sequence is seen improves detectability, how sequences with higherperplexity tend to be memorized more, and how taking context into accountfurther improves detectability.\rTrained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code\nVahid Majdinasab Amin Nikanjam Foutse Khomh\nabstract\rabstract: Code auditing ensures that the developed code adheres to standards,regulations, and copyright protection by verifying that it does not containcode from protected sources. The recent advent of Large Language Models (LLMs)as coding assistants in the software development process poses new challengesfor code auditing. The dataset for training these models is mainly collectedfrom publicly available sources. This raises the issue of intellectual propertyinfringement as developers\u0026rsquo; codes are already included in the dataset.Therefore, auditing code developed using LLMs is challenging, as it isdifficult to reliably assert if an LLM used during development has been trainedon specific copyrighted codes, given that we do not have access to the trainingdatasets of these models. Given the non-disclosure of the training datasets,traditional approaches such as code clone detection are insufficient forasserting copyright infringement. To address this challenge, we propose a newapproach, TraWiC; a model-agnostic and interpretable method based on membershipinference for detecting code inclusion in an LLM\u0026rsquo;s training dataset. We extractsyntactic and semantic identifiers unique to each program to train a classifierfor detecting code inclusion. In our experiments, we observe that TraWiC iscapable of detecting 83.87% of codes that were used to train an LLM. Incomparison, the prevalent clone detection tool NiCad is only capable ofdetecting 47.64%. In addition to its remarkable performance, TraWiC has lowresource overhead in contrast to pair-wise clone detection that is conductedduring the auditing process of tools like CodeWhisperer reference tracker,across thousands of code snippets.\rChinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis\nWei Zhai Hongzhi Qi Qing Zhao Jianqiang Li Ziqi Wang Han Wang Bing Xiang Yang Guanghui Fu\nabstract\rabstract: In the current environment, psychological issues are prevalent andwidespread, with social media serving as a key outlet for individuals to sharetheir feelings. This results in the generation of vast quantities of datadaily, where negative emotions have the potential to precipitate crisissituations. There is a recognized need for models capable of efficientanalysis. While pre-trained language models have demonstrated theireffectiveness broadly, there\u0026rsquo;s a noticeable gap in pre-trained models tailoredfor specialized domains like psychology. To address this, we have collected ahuge dataset from Chinese social media platforms and enriched it with publiclyavailable datasets to create a comprehensive database encompassing 3.36 milliontext entries. To enhance the model\u0026rsquo;s applicability to psychological textanalysis, we integrated psychological lexicons into the pre-training maskingmechanism. Building on an existing Chinese language model, we performedadaptive training to develop a model specialized for the psychological domain.We assessed our model\u0026rsquo;s effectiveness across four public benchmarks, where itnot only surpassed the performance of standard pre-trained models but alsoshowed a inclination for making psychologically relevant predictions. Due toconcerns regarding data privacy, the dataset will not be made publiclyavailable. However, we have made the pre-trained models and codes publiclyaccessible to the community via:https://github.com/zwzzzQAQ/Chinese-MentalBERT.\rDPZero: Private Fine-Tuning of Language Models without Backpropagation\nLiang Zhang Bingcong Li Kiran Koshy Thekumparampil Sewoong Oh Niao He\nabstract\rabstract: The widespread practice of fine-tuning large language models (LLMs) ondomain-specific data faces two major challenges in memory and privacy. First,as the size of LLMs continues to grow, the memory demands of gradient-basedtraining methods via backpropagation become prohibitively high. Second, giventhe tendency of LLMs to memorize training data, it is important to protectpotentially sensitive information in the fine-tuning data from beingregurgitated. Zeroth-order methods, which rely solely on forward passes,substantially reduce memory consumption during training. However, directlycombining them with standard differentially private gradient descent suffersfrom growing model size. To bridge this gap, we introduce DPZero, a novelprivate zeroth-order algorithm with nearly dimension-independent rates. Thememory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa onsix downstream tasks.\rA Study of Fairness Concerns in AI-based Mobile App Reviews\nAli Rezaei Nasab Maedeh Dashti Mojtaba Shahin Mansooreh Zahedi Hourieh Khalajzadeh Chetan Arora Peng Liang\nabstract\rabstract: Fairness is one of the socio-technical concerns that must be addressed inAI-based systems. Unfair AI-based systems, particularly unfair AI-based mobileapps, can pose difficulties for a significant proportion of the globalpopulation. This paper aims to analyze fairness concerns in AI-based appreviews.We first manually constructed a ground-truth dataset, including astatistical sample of fairness and non-fairness reviews. Leveraging theground-truth dataset, we developed and evaluated a set of machine learning anddeep learning classifiers that distinguish fairness reviews from non-fairnessreviews. Our experiments show that our best-performing classifier can detectfairness reviews with a precision of 94%. We then applied the best-performingclassifier on approximately 9.5M reviews collected from 108 AI-based apps andidentified around 92K fairness reviews. Next, applying the K-means clusteringtechnique to the 92K fairness reviews, followed by manual analysis, led to theidentification of six distinct types of fairness concerns (e.g., \u0026lsquo;receivingdifferent quality of features and services in different platforms and devices\u0026rsquo;and \u0026rsquo;lack of transparency and fairness in dealing with user-generatedcontent\u0026rsquo;). Finally, the manual analysis of 2,248 app owners\u0026rsquo; responses to thefairness reviews identified six root causes (e.g., \u0026lsquo;copyright issues\u0026rsquo;) that appowners report to justify fairness concerns.\rInterpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding\nAlessandro Achille Greg Ver Steeg Tian Yu Liu Matthew Trager Carson Klingenberg Stefano Soatto\nabstract\rabstract: Quantifying the degree of similarity between images is a key copyright issuefor image-based machine learning. In legal doctrine however, determining thedegree of similarity between works requires subjective analysis, andfact-finders (judges and juries) can demonstrate considerable variability inthese subjective judgement calls. Images that are structurally similar can bedeemed dissimilar, whereas images of completely different scenes can be deemedsimilar enough to support a claim of copying. We seek to define and compute anotion of \u0026ldquo;conceptual similarity\u0026rdquo; among images that captures high-levelrelations even among images that do not share repeated elements or visuallysimilar components. The idea is to use a base multi-modal model to generate\u0026quot;explanations\u0026quot; (captions) of visual data at increasing levels of complexity.Then, similarity can be measured by the length of the caption needed todiscriminate between the two images: Two highly dissimilar images can bediscriminated early in their description, whereas conceptually dissimilar oneswill need more detail to be distinguished. We operationalize this definitionand show that it correlates with subjective (averaged human evaluation)assessment, and beats existing baselines on both image-to-image andtext-to-text similarity benchmarks. Beyond just providing a number, our methodalso offers interpretability by pointing to the specific level of granularityof the description where the source data are differentiated.\rOnline Advertisements with LLMs: Opportunities and Challenges\nSoheil Feizi MohammadTaghi Hajiaghayi Keivan Rezaei Suho Shin\nabstract\rabstract: This paper explores the potential for leveraging Large Language Models (LLM)in the realm of online advertising systems. We delve into essentialrequirements including privacy, latency, reliability as well as thesatisfaction of users and advertisers which such a system must fulfill. Wefurther introduce a general framework for LLM advertisement, consisting ofmodification, bidding, prediction, and auction modules. Different designconsiderations for each module is presented, with an in-depth examination oftheir practicality and the technical challenges inherent to theirimplementation. Finally, we explore the prospect of LLM-based dynamic creativeoptimization as a means to significantly enhance the appeal of advertisementsto users and discuss its additional challenges.\r2024-02-13\nJAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models\nJillian Fisher Ximing Lu Jaehun Jung Liwei Jiang Zaid Harchaoui Yejin Choi\nabstract\rabstract: The permanence of online content combined with the enhanced authorshipidentification techniques calls for stronger computational methods to protectthe identity and privacy of online authorship when needed, e.g., blind reviewsfor scientific papers, anonymous online reviews, or anonymous interactions inthe mental health forums. In this paper, we propose an unsupervisedinference-time approach to authorship obfuscation to address the uniquechallenges of authorship obfuscation: lack of supervision data for diverseauthorship and domains, and the need for a sufficient level of revision beyondsimple paraphrasing to obfuscate the authorship, all the while preserving theoriginal content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm forauthorship obfuscation that can be in principle applied to any text andauthorship. Our approach builds on small language models such as GPT2-XL inorder to help avoid disclosing the original content to proprietary LLM\u0026rsquo;s APIs,while also reducing the performance gap between small and large language modelsvia algorithmic enhancement. The key idea behind our approach is to boost thecreative power of smaller language models through constrained decoding, whilealso allowing for user-specified controls and flexibility. Experimental resultsdemonstrate that our approach based on GPT2-XL outperforms previousstate-of-the-art methods based on comparably small models, while performingcompetitively against GPT3.5 175B, a propriety model that is two orders ofmagnitudes larger.\rComputational Copyright: Towards A Royalty Model for Music Generative AI\nJunwei Deng Jiaqi Ma\nabstract\rabstract: The advancement of generative AI has given rise to pressing copyrightchallenges, particularly in music industry. This paper focuses on the economicaspects of these challenges, emphasizing that the economic impact constitutes acentral issue in the copyright arena. The complexity of the black-boxgenerative AI technologies not only suggests but necessitates algorithmicsolutions. However, such solutions have been largely missing, leading toregulatory challenges in this landscape. We aim to bridge the gap in currentapproaches by proposing potential royalty models for revenue sharing on AImusic generation platforms. Our methodology involves a detailed analysis ofexisting royalty models in platforms like Spotify and YouTube, and adaptingthese to the unique context of AI-generated music. A significant challenge weaddress is the attribution of AI-generated music to influential copyrightedcontent in the training data. To this end, we present algorithmic solutionsemploying data attribution techniques. Our experimental results verify theeffectiveness of these solutions. This research represents a pioneering effortin integrating technical advancements with economic and legal considerations inthe field of generative AI, offering a computational copyright solution for thechallenges posed by the opaque nature of AI technologies.\rMapping the Ethics of Generative AI: A Comprehensive Scoping Review\nThilo Hagendorff\nabstract\rabstract: The advent of generative artificial intelligence and the widespread adoptionof it in society engendered intensive debates about its ethical implicationsand risks. These risks often differ from those associated with traditionaldiscriminative machine learning. To synthesize the recent discourse and map itsnormative concepts, we conducted a scoping review on the ethics of generativeartificial intelligence, including especially large language models andtext-to-image models. Our analysis provides a taxonomy of 378 normative issuesin 19 topic areas and ranks them according to their prevalence in theliterature. The study offers a comprehensive overview for scholars,practitioners, or policymakers, condensing the ethical debates surroundingfairness, safety, harmful content, hallucinations, privacy, interaction risks,security, alignment, societal impacts, and others. We discuss the results,evaluate imbalances in the literature, and explore unsubstantiated riskscenarios.\rPrivacy-Preserving Language Model Inference with Instance Obfuscation\nYixiang Yao Fei Wang Srivatsan Ravi Muhao Chen\nabstract\rabstract: Language Models as a Service (LMaaS) offers convenient access for developersand researchers to perform inference using pre-trained language models.Nonetheless, the input data and the inference results containing privateinformation are exposed as plaintext during the service call, leading toprivacy issues. Recent studies have started tackling the privacy issue bytransforming input data into privacy-preserving representation from theuser-end with the techniques such as noise addition and content perturbation,while the exploration of inference result protection, namely decision privacy,is still a blank page. In order to maintain the black-box manner of LMaaS,conducting data privacy protection, especially for the decision, is achallenging task because the process has to be seamless to the models andaccompanied by limited communication and computation overhead. We thus proposeInstance-Obfuscated Inference (IOI) method, which focuses on addressing thedecision privacy issue of natural language understanding tasks in theircomplete life-cycle. Besides, we conduct comprehensive experiments to evaluatethe performance as well as the privacy-protection strength of the proposedmethod on various benchmarking tasks.\rBBox-Adapter: Lightweight Adapting for Black-Box Large Language Models\nHaotian Sun Yuchen Zhuang Wei Wei Chao Zhang Bo Dai\nabstract\rabstract: Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Geminifor specific tasks is challenging. Due to the opacity in their parameters,embeddings, and even output probabilities, existing fine-tuning adaptationmethods are inapplicable. Consequently, adapting these black-box LLMs is onlypossible through their API services, raising concerns about transparency,privacy, and cost. To address these challenges, we introduce BBox-Adapter, anovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes targetand source domain data by treating target data as positive and source data asnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss topromote the likelihood of target domain data while penalizing that of thesource domain. Furthermore, it features an online adaptation mechanism, whichincorporates real-time positive data sampling from ground-truth, human, or AIfeedback, coupled with negative data from previous adaptations. Extensiveexperiments demonstrate BBox-Adapter\u0026rsquo;s effectiveness and cost efficiency. Itimproves model performance by up to 6.77% across diverse tasks and domains,while reducing training and inference costs by 31.30x and 1.84x, respectively.\r2024-02-12\nPANORAMIA: Privacy Auditing of Machine Learning Models without Retraining\nMishaal Kazmi Hadrien Lautraite Alireza Akbari Mauricio Soroco Qiaoyue Tang Tao Wang Sébastien Gambs Mathias Lécuyer\nabstract\rabstract: We introduce a privacy auditing scheme for ML models that relies onmembership inference attacks using generated data as \u0026ldquo;non-members\u0026rdquo;. Thisscheme, which we call PANORAMIA, quantifies the privacy leakage for large-scaleML models without control of the training process or model re-training and onlyrequires access to a subset of the training data. To demonstrate itsapplicability, we evaluate our auditing scheme across multiple ML domains,ranging from image and tabular data classification to large-scale languagemodels.\rRetrieval-Augmented Thought Process as Sequential Decision Making\nThomas Pouplin Hao Sun Samuel Holt Mihaela van der Schaar\nabstract\rabstract: Large Language Models (LLMs) have demonstrated their strong ability to assistpeople and show \u0026ldquo;sparks of intelligence\u0026rdquo;. However, several open challengeshinder their wider application: such as concerns over privacy, tendencies toproduce hallucinations, and difficulties in handling long contexts. In thiswork, we address those challenges by introducing the Retrieval-AugmentedThought Process (RATP). Given access to external knowledge, RATP formulates thethought generation of LLMs as a multiple-step decision process. To optimizesuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns aQ-value estimator that permits cost-efficient inference. In addressing the taskof question-answering with private data, where ethical and security concernslimit LLM training methods, RATP achieves a 50% improvement over existingin-context retrieval-augmented language models.\rEmpowering Federated Learning for Massive Models with NVIDIA FLARE\nHolger R. Roth Ziyue Xu Yuan-Ting Hsieh Adithya Renduchintala Isaac Yang Zhihong Zhang Yuhong Wen Sean Yang Kevin Lu Kristopher Kersten Camir Ricketts Daguang Xu Chester Chen Yan Cheng Andrew Feng\nabstract\rabstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.\rEmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models\nGuo Lin Wenyue Hua Yongfeng Zhang\nabstract\rabstract: Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanism EmojiCryptto protect user privacy. It uses Emoji to encrypt the user inputs beforesending them to LLM, effectively rendering them indecipherable to human orLLM\u0026rsquo;s examination while retaining the original intent of the prompt, thusensuring the model\u0026rsquo;s performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that EmojiCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/EmojiCrypt.\rLarge language models can enhance persuasion through linguistic feature alignment\nMinkyu Shin Jin Kim\nabstract\rabstract: Although large language models (LLMs) are reshaping various aspects of humanlife, our current understanding of their impacts remains somewhat constrained.Here we investigate the impact of LLMs on human communication, using data onconsumer complaints in the financial industry. By employing an AI detectiontool on more than 820K complaints gathered by the Consumer Financial ProtectionBureau (CFPB), we find a sharp increase in the likely use of LLMs shortly afterthe release of ChatGPT. Moreover, the likely LLM usage was positivelycorrelated with message persuasiveness (i.e., increased likelihood of obtainingrelief from financial firms). Computational linguistic analyses suggest thatthe positive correlation may be explained by LLMs\u0026rsquo; enhancement of variouslinguistic features. Based on the results of these observational studies, wehypothesize that LLM usage may enhance a comprehensive set of linguisticfeatures, increasing message persuasiveness to receivers with heterogeneouslinguistic preferences (i.e., linguistic feature alignment). We test thishypothesis in preregistered experiments and find support for it. As an instanceof early empirical demonstrations of LLM usage for enhancing persuasion, ourresearch highlights the transformative potential of LLMs in humancommunication.\rSecret Collusion Among Generative AI Agents\nSumeet Ramesh Motwani Mikhail Baranchuk Martin Strohmeier Vijay Bolina Philip H. S. Torr Lewis Hammond Christian Schroeder de Witt\nabstract\rabstract: Recent capability increases in large language models (LLMs) open upapplications in which teams of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both the AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.\rGame Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch\nRay Ito Junichiro Takahashi\nabstract\rabstract: Several attempts have been made to implement text command control for gameagents. However, current technologies are limited to processing predefinedformat commands. This paper proposes a pioneering text command control systemfor a game agent that can understand natural language commands expressed infree-form. The proposed system uses a large language model (LLM) for codegeneration to interpret and transform natural language commands into behaviorbranch, a proposed knowledge expression based on behavior trees, whichfacilitates execution by the game agent. This study conducted empiricalvalidation within a game environment that simulates a Pok'emon game andinvolved multiple participants. The results confirmed the system\u0026rsquo;s ability tounderstand and carry out natural language commands, representing a noteworthyin the realm of real-time language interactive game agents. Notice for the use of this material. The copyright of this material isretained by the Japanese Society for Artificial Intelligence (JSAI). Thismaterial is published here with the agreement of JSAI. Please be complied withCopyright Law of Japan if any users wish to reproduce, make derivative work,distribute or make available to the public any part or whole thereof. AllRights Reserved, Copyright (C) The Japanese Society for ArtificialIntelligence.\rFive ethical principles for generative AI in scientific research\nZhicheng Lin\nabstract\rabstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.\rUtilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code\nLiming Jiang\nabstract\rabstract: Mini-applications, commonly referred to as mini-apps, are compact softwareprograms embedded within larger applications or platforms, offering targetedfunctionality without the need for separate installations. Typically web-basedor cloud-hosted, these mini-apps streamline user experiences by providingfocused services accessible through web browsers or mobile apps. Theirsimplicity, speed, and integration capabilities make them valuable additions tomessaging platforms, social media networks, e-commerce sites, and variousdigital environments. WeChat Mini Programs, a prominent feature of China\u0026rsquo;sleading messaging app, exemplify this trend, offering users a seamless array ofservices without additional downloads. Leveraging WeChat\u0026rsquo;s extensive user baseand payment infrastructure, Mini Programs facilitate efficient transactions andbridge online and offline experiences, shaping China\u0026rsquo;s digital landscapesignificantly. This paper investigates the potential of employing LargeLanguage Models (LLMs) to detect privacy breaches within WeChat Mini Programs.Given the widespread use of Mini Programs and growing concerns about dataprivacy, this research seeks to determine if LLMs can effectively identifyinstances of privacy leakage within this ecosystem. Through meticulous analysisand experimentation, we aim to highlight the efficacy of LLMs in safeguardinguser privacy and security within the WeChat Mini Program environment, therebycontributing to a more secure digital landscape.\r2024-02-11\nDifferentially Private Training of Mixture of Experts Models\nPierre Tholoniat Huseyin A. Inan Janardhan Kulkarni Robert Sim\nabstract\rabstract: This position paper investigates the integration of Differential Privacy (DP)in the training of Mixture of Experts (MoE) models within the field of naturallanguage processing. As Large Language Models (LLMs) scale to billions ofparameters, leveraging expansive datasets, they exhibit enhanced linguisticcapabilities and emergent abilities. However, this growth raises significantcomputational and privacy concerns. Our study addresses these issues byexploring the potential of MoE models, known for their computationalefficiency, and the application of DP, a standard for privacy preservation. Wepresent the first known attempt to train MoE models under the constraints ofDP, addressing the unique challenges posed by their architecture and thecomplexities of DP integration. Our initial experimental studies demonstratethat MoE models can be effectively trained with DP, achieving performance thatis competitive with their non-private counterparts. This initial study aims toprovide valuable insights and ignite further research in the domain ofprivacy-preserving MoE models, softly laying the groundwork for prospectivedevelopments in this evolving field.\rWeakly interacting one-dimensional topological insulators: a bosonization approach\nPolina Matveeva Dmitri Gutman Sam T. Carr\nabstract\rabstract: We investigate the topological properties of one-dimensional weaklyinteracting topological insulators using bosonization. To do that we study thetopological edge states that emerge at the edges of a model realized by astrong impurity or at the boundary between topologically distinct phases. Inthe bosonic model, the edge states are manifested as degenerate bosonic kinksat the boundaries. We first illustrate this idea on the example of theinteracting Su-Schrieffer-Heeger (SSH) chain. We compute the localizationlength of the edge states as the width of an edge soliton that occurs in theSSH model in the presence of a strong impurity. Next, we examine models of twocapacitively coupled SSH chains that can be either identical or in distincttopological phases. We find that weak Hubbard interaction reduces the groundstate degeneracy in the topological phase of identical chains. We then provethat similarly to the non-interacting model, the degeneracy of the edge statesin the interacting case is protected by chiral symmetry. We then studytopological insulators built from two SSH chains with inter-chain hopping, thatrepresent models of different chiral symmetric universality classes. Wedemonstrate in bosonic language that the topological index of a weakly coupledmodel is determined by the type of inter-chain coupling, invariant under one oftwo possible chiral symmetry operators. Finally, we show that a generalone-dimensional model in a phase with topological index $\\nu$ is equivalent atlow energies to a theory of at least $\\nu$ SSH chains. We illustrate this ideaon the example of an SSH model with longer-range hopping.\r2024-02-10\nData Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models\nShahriar Golchin Mihai Surdeanu\nabstract\rabstract: We propose the Data Contamination Quiz (DCQ), a simple and effective approachto detect data contamination in large language models (LLMs) and estimate theamount of it. Specifically, we frame data contamination detection as a seriesof multiple-choice questions and devise a quiz format wherein three perturbedversions of each dataset instance are created. These changes only includeword-level perturbations. The generated perturbed versions, along with theoriginal instance, form the options in the DCQ, with an extra optionaccommodating the possibility that none of the provided choices is correct.Given that the only distinguishing signal among the choices is the exactwording relative to the original instance, an LLM, when tasked with identifyingthe original instance from the choices, gravitates towards the original one ifit has been exposed to it in its pre-training phase\u0026ndash;a trait intrinsic to LLMs.Tested over several datasets with GPT-4/3.5, our findings\u0026ndash;while fully lackingaccess to LLMs\u0026rsquo; pre-training data and internal parameters\u0026ndash;suggest that DCQuncovers greater contamination levels compared to existing detection methodsand proficiently bypasses more safety filters, especially those set to avoidgenerating copyrighted contents.\rOpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning\nRui Ye Wenhao Wang Jingyi Chai Dihan Li Zexi Li Yinda Xu Yaxin Du Yanfeng Wang Siheng Chen\nabstract\rabstract: Trained on massive publicly available data, large language models (LLMs) havedemonstrated tremendous success across various fields. While more datacontributes to better performance, a disconcerting reality is that high-qualitypublic data will be exhausted in a few years. In this paper, we offer apotential next step for contemporary LLMs: collaborative and privacy-preservingLLM training on the underutilized distributed private data via federatedlearning (FL), where multiple data owners collaboratively train a shared modelwithout transmitting raw data. To achieve this, we build a concise, integrated,and research-friendly framework/codebase, named OpenFedLLM. It covers federatedinstruction tuning for enhancing instruction-following capability, federatedvalue alignment for aligning with human values, and 7 representative FLalgorithms. Besides, OpenFedLLM supports training on diverse domains, where wecover 8 training datasets; and provides comprehensive evaluations, where wecover 30+ evaluation metrics. Through extensive experiments, we observe thatall FL algorithms outperform local training on training LLMs, demonstrating aclear performance improvement across a variety of settings. Notably, in afinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm canoutperform GPT-4 by a significant margin while the model obtained throughindividual training cannot, demonstrating strong motivation for clients toparticipate in FL. The code is available athttps://github.com/rui-ye/OpenFedLLM.\rEfficient Incremental Belief Updates Using Weighted Virtual Observations\nDavid Tolpin\nabstract\rabstract: We present an algorithmic solution to the problem of incremental beliefupdating in the context of Monte Carlo inference in Bayesian statistical modelsrepresented by probabilistic programs. Given a model and a sample-approximatedposterior, our solution constructs a set of weighted observations to conditionthe model such that inference would result in the same posterior. This problemarises e.g. in multi-level modelling, incremental inference, inference inpresence of privacy constraints. First, a set of virtual observations isselected, then, observation weights are found through a computationallyefficient optimization procedure such that the reconstructed posteriorcoincides with or closely approximates the original posterior. We implement andapply the solution to a number of didactic examples and case studies, showingefficiency and robustness of our approach. The provided referenceimplementation is agnostic to the probabilistic programming language or theinference algorithm, and can be applied to most mainstream probabilisticprogramming environments.\rWhispers in the Machine: Confidentiality in LLM-integrated Systems\nJonathan Evertz Merlin Chlosta Lea Schönherr Thorsten Eisenhofer\nabstract\rabstract: Large Language Models (LLMs) are increasingly integrated with external tools.While these integrations can significantly improve the functionality of LLMs,they also create a new attack surface where confidential data may be disclosedbetween different components. Specifically, malicious tools can exploitvulnerabilities in the LLM itself to manipulate the model and compromise thedata of other services, raising the question of how private data can beprotected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality inLLM-integrated systems. For this, we formalize a \u0026ldquo;secret key\u0026rdquo; game that cancapture the ability of a model to conceal private information. This enables usto compare the vulnerability of a model against confidentiality attacks andalso the effectiveness of different defense strategies. In this framework, weevaluate eight previously published attacks and four defenses. We find thatcurrent defenses lack generalization across attack strategies. Building on thisanalysis, we propose a method for robustness fine-tuning, inspired byadversarial training. This approach is effective in lowering the success rateof attackers and in improving the system\u0026rsquo;s resilience against unknown attacks.\r2024-02-09\nTowards Principled Assessment of Tabular Data Synthesis Algorithms\nYuntao Du Ninghui Li\nabstract\rabstract: Data synthesis has been advocated as an important approach for utilizing datawhile protecting data privacy. A large number of tabular data synthesisalgorithms (which we call synthesizers) have been proposed. Some synthesizerssatisfy Differential Privacy, while others aim to provide privacy in aheuristic fashion. A comprehensive understanding of the strengths andweaknesses of these synthesizers remains elusive due to lacking principledevaluation metrics and missing head-to-head comparisons of newly developedsynthesizers that take advantage of diffusion models and large language modelswith state-of-the-art marginal-based synthesizers. In this paper, we present a principled and systematic evaluation frameworkfor assessing tabular data synthesis algorithms. Specifically, we examine andcritique existing evaluation metrics, and introduce a set of new metrics interms of fidelity, privacy, and utility to address their limitations. Based onthe proposed metrics, we also devise a unified objective for tuning, which canconsistently improve the quality of synthetic data for all methods. Weconducted extensive evaluations of 8 different types of synthesizers on 12datasets and identified some interesting findings, which offer new directionsfor privacy-preserving data synthesis.\rExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs\nFernando Ferraretto Thiago Laitz Roberto Lotufo Rodrigo Nogueira\nabstract\rabstract: ExaRanker recently introduced an approach to training information retrieval(IR) models, incorporating natural language explanations as additional labels.The method addresses the challenge of limited labeled examples, leading toimprovements in the effectiveness of IR models. However, the initial resultswere based on proprietary language models such as GPT-3.5, which posedconstraints on dataset size due to its cost and data privacy. In this paper, weintroduce ExaRanker-Open, where we adapt and explore the use of open-sourcelanguage models to generate explanations. The method has been tested usingdifferent LLMs and datasets sizes to better comprehend the effectivecontribution of data augmentation. Our findings reveal that incorporatingexplanations consistently enhances neural rankers, with benefits escalating asthe LLM size increases. Notably, the data augmentation method provesadvantageous even with large datasets, as evidenced by ExaRanker surpassing thetarget baseline by 0.6 nDCG@10 points in our study. To encourage furtheradvancements by the research community, we have open-sourced both the code anddatasets at https://github.com/unicamp-dl/ExaRanker.\rStudious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning\nYichuan Mo Yuji Wang Zeming Wei Yisen Wang\nabstract\rabstract: Although Large Language Models (LLMs) have achieved tremendous success invarious applications, they are also susceptible to certain prompts that caninduce them to bypass built-in safety measures and provide dangerous or illegalcontent, a phenomenon known as jailbreak. To protect LLMs from producingharmful information, various defense strategies are proposed, with mostfocusing on content filtering or adversarial training of models. In this paper,we propose an approach named Prompt Adversarial Tuning (PAT) to train a defensecontrol mechanism, which is then embedded as a prefix to user prompts toimplement our defense strategy. We design a training process similar toadversarial training to achieve our optimized goal, alternating betweenupdating attack and defense controls. To our knowledge, we are the first toimplement defense from the perspective of prompt tuning. Once employed, ourmethod will hardly impact the operational efficiency of LLMs. Experiments showthat our method is effective in both black-box and white-box settings, reducingthe success rate of advanced attacks to nearly 0 while maintaining the benignanswer rate of 80% to simple benign questions. Our work might potentially charta new perspective for future explorations in LLM security.\r2024-02-08\nLLMs Among Us: Generative AI Participating in Digital Discourse\nKristina Radivojevic Nicholas Clark Paul Brenner\nabstract\rabstract: The emergence of Large Language Models (LLMs) has great potential to reshapethe landscape of many social media platforms. While this can bring promisingopportunities, it also raises many threats, such as biases and privacyconcerns, and may contribute to the spread of propaganda by malicious actors.We developed the \u0026ldquo;LLMs Among Us\u0026rdquo; experimental framework on top of the Mastodonsocial media platform for bot and human participants to communicate withoutknowing the ratio or nature of bot and human participants. We built 10 personaswith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted threerounds of the experiment and surveyed participants after each round to measurethe ability of LLMs to pose as human participants without human detection. Wefound that participants correctly identified the nature of other users in theexperiment only 42% of the time despite knowing the presence of both bots andhumans. We also found that the choice of persona had substantially more impacton human perception than the choice of mainstream LLMs.\rSocial Learning: Towards Collaborative Learning with Large Language Models\nAmirkeivan Mohtashami Florian Hartmann Sian Gooding Lukas Zilka Matt Sharifi Blaise Aguera y Arcas\nabstract\rabstract: We introduce the framework of \u0026ldquo;social learning\u0026rdquo; in the context of largelanguage models (LLMs), whereby models share knowledge with each other in aprivacy-aware manner using natural language. We present and evaluate twoapproaches for knowledge transfer between LLMs. In the first scenario, we allowthe model to generate abstract prompts aiming to teach the task. In our secondapproach, models transfer knowledge by generating synthetic examples. Weevaluate these methods across diverse datasets and quantify memorization as aproxy for privacy loss. These techniques inspired by social learning yieldpromising results with low memorization of the original data. In particular, weshow that performance using these methods is comparable to results with the useof original labels and prompts. Our work demonstrates the viability of sociallearning for LLMs, establishes baseline approaches and highlights severalunexplored areas for future work.\rImproved upper bounds for wide-sense frameproof codes\nYuhao Zhao Xiande Zhang\nabstract\rabstract: Frameproof codes have been extensively studied for many years due to theirapplication in copyright protection and their connection to extremal settheory. In this paper, we investigate upper bounds on the cardinality ofwide-sense $t$-frameproof codes. For $t=2$, we apply results from Spernertheory to give a better upper bound, which significantly improves a recentbound by Zhou and Zhou. For $t\\geq 3$, we provide a general upper bound byestablishing a relation between wide-sense frameproof codes and cover-freefamilies. Finally, when the code length $n$ is at most$\\frac{15+\\sqrt{33}}{24}(t-1)^2$, we show that a wide-sense $t$-frameproof codehas at most $n$ codewords, and the unique optimal code consists of allweight-one codewords. As byproducts, our results improve several best knownresults on binary $t$-frameproof codes.\rRevolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices\nMohamed Amine Ferrag Mthandazo Ndhlovu Norbert Tihanyi Lucas C. Cordeiro Merouane Debbah Thierry Lestable Narinderjit Singh Thandi\nabstract\rabstract: The field of Natural Language Processing (NLP) is currently undergoing arevolutionary transformation driven by the power of pre-trained Large LanguageModels (LLMs) based on groundbreaking Transformer architectures. As thefrequency and diversity of cybersecurity attacks continue to rise, theimportance of incident detection has significantly increased. IoT devices areexpanding rapidly, resulting in a growing need for efficient techniques toautonomously identify network-based attacks in IoT networks with both highprecision and minimal computational requirements. This paper presentsSecurityBERT, a novel architecture that leverages the Bidirectional EncoderRepresentations from Transformers (BERT) model for cyber threat detection inIoT networks. During the training of SecurityBERT, we incorporated a novelprivacy-preserving encoding technique called Privacy-Preserving Fixed-LengthEncoding (PPFLE). We effectively represented network traffic data in astructured format by combining PPFLE with the Byte-level Byte-Pair Encoder(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperformstraditional Machine Learning (ML) and Deep Learning (DL) methods, such asConvolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), incyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, ourexperimental analysis shows that SecurityBERT achieved an impressive 98.2%overall accuracy in identifying fourteen distinct attack types, surpassingprevious records set by hybrid solutions such as GAN-Transformer-basedarchitectures and CNN-LSTM models. With an inference time of less than 0.15seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERTis ideally suited for real-life traffic analysis and a suitable choice fordeployment on resource-constrained IoT devices.\r2024-02-07\nDefending Our Privacy With Backdoors\nDominik Hintersdorf Lukas Struppek Daniel Neider Kristian Kersting\nabstract\rabstract: The proliferation of large AI models trained on uncurated, often sensitiveweb-scraped data has raised significant privacy concerns. One of the concernsis that adversaries can extract information about the training data usingprivacy attacks. Unfortunately, the task of removing specific information fromthe models without sacrificing performance is not straightforward and hasproven to be challenging. We propose a rather easy yet effective defense basedon backdoor attacks to remove private information such as names and faces ofindividuals from vision-language models by fine-tuning them for only a fewminutes instead of re-training them from scratch. Specifically, throughstrategic insertion of backdoors into text encoders, we align the embeddings ofsensitive phrases with those of neutral terms-\u0026ldquo;a person\u0026rdquo; instead of theperson\u0026rsquo;s actual name. For image encoders, we map embeddings of individuals tobe removed from the model to a universal, anonymous embedding. Our empiricalresults demonstrate the effectiveness of our backdoor-based defense on CLIP byassessing its performance using a specialized privacy attack for zero-shotclassifiers. Our approach provides not only a new \u0026ldquo;dual-use\u0026rdquo; perspective onbackdoor attacks, but also presents a promising avenue to enhance the privacyof individuals within models trained on uncurated web-scraped data.\rHuman-Readable Fingerprint for Large Language Models\nBoyi Zeng Chenghu Zhou Xinbing Wang Zhouhan Lin\nabstract\rabstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters\u0026rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM\u0026rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog\u0026rsquo;s appearancestrongly indicates the LLM\u0026rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.\rDe-amplifying Bias from Differential Privacy in Language Model Fine-tuning\nSanjari Srivastava Piotr Mardziel Zhikhun Zhang Archana Ahlawat Anupam Datta John C Mitchell\nabstract\rabstract: Fairness and privacy are two important values machine learning (ML)practitioners often seek to operationalize in models. Fairness aims to reducemodel bias for social/demographic sub-groups. Privacy via differential privacy(DP) mechanisms, on the other hand, limits the impact of any individual\u0026rsquo;straining data on the resulting model. The trade-offs between privacy andfairness goals of trustworthy ML pose a challenge to those wishing to addressboth. We show that DP amplifies gender, racial, and religious bias whenfine-tuning large language models (LLMs), producing models more biased thanones fine-tuned without DP. We find the cause of the amplification to be adisparity in convergence of gradients across sub-groups. Through the case ofbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),a known method for addressing bias, also mitigates bias amplification by DP. Asa consequence, DP and CDA together can be used to fine-tune models whilemaintaining both fairness and privacy.\r2024-02-06\nDemocratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\nZhaoxuan Tan Qingkai Zeng Yijun Tian Zheyuan Liu Bing Yin Meng Jiang\nabstract\rabstract: Personalization in large language models (LLMs) is increasingly important,aiming to align LLM\u0026rsquo;s interactions, content, and recommendations withindividual user preferences. Recent advances in LLM personalization havespotlighted effective prompt design, by enriching user queries withnon-parametric knowledge through behavior history retrieval and textualprofiles. However, these approaches were limited due to a lack of modelownership, resulting in constrained customization and privacy issues. Moreover,they often failed to accurately capture user behavior patterns, especially incases where user data were complex and dynamic. To address these shortcomings,we introduce One PEFT Per User (OPPU), which employs personalizedparameter-efficient fine-tuning (PEFT) modules, to store user-specific behaviorpatterns and preferences. By plugging in users\u0026rsquo; personal PEFT parameters, theycan own and use their LLMs personally. OPPU integrates parametric userknowledge in the personal PEFT parameters with the non-parametric knowledgeacquired through retrieval and profile. This integration adapts individual LLMsto user behavior shifts. Experimental results demonstrate that OPPUsignificantly outperforms existing prompt-based methods across seven diversetasks in the LaMP benchmark. Further in-depth studies reveal OPPU\u0026rsquo;s enhancedcapabilities in handling user behavior shifts, modeling users at differentactive levels, maintaining robustness across various user history formats, anddisplaying versatility with different PEFT methods.\rOrganic or Diffused: Can We Distinguish Human Art from AI-generated Images?\nAnna Yoo Jeong Ha Josephine Passananti Ronik Bhaskar Shawn Shan Reid Southen Haitao Zheng Ben Y. Zhao\nabstract\rabstract: The advent of generative AI images has completely disrupted the art world.Distinguishing AI generated images from human art is a challenging problemwhose impact is growing over time. A failure to address this problem allows badactors to defraud individuals paying a premium for human art and companieswhose stated policies forbid AI imagery. It is also critical for content ownersto establish copyright, and for model trainers interested in curating trainingdata in order to avoid potential model collapse. There are several different approaches to distinguishing human art from AIimages, including classifiers trained by supervised learning, research toolstargeting diffusion models, and identification by professional artists usingtheir knowledge of artistic techniques. In this paper, we seek to understandhow well these approaches can perform against today\u0026rsquo;s modern generative modelsin both benign and adversarial settings. We curate real human art across 7styles, generate matching images from 5 generative models, and apply 8detectors (5 automated detectors and 3 different human groups including 180crowdworkers, 4000+ professional artists, and 13 expert artists experienced atdetecting AI). Both Hive and expert artists do very well, but make mistakes indifferent ways (Hive is weaker against adversarial perturbations while Expertartists produce higher false positives). We believe these weaknesses willremain as models continue to evolve, and use our data to demonstrate why acombined team of human and automated detectors provides the best combination ofaccuracy and robustness.\rLLsM: Generative Linguistic Steganography with Large Language Model\nYihao Wang Ruiqi Song Ru Zhang Jianyi Liu Lingxiao Li\nabstract\rabstract: Linguistic Steganography (LS) tasks aim to generate steganographic text(stego) based on secret information. Only authorized recipients can perceivethe existence of secrets in the texts and extract them, thereby preservingprivacy. However, the controllability of the stego generated by existingschemes is poor, and the stego is difficult to contain specific discoursecharacteristics such as style. As a result, the stego is easily detectable,compromising covert communication. To address these problems, this paperproposes LLsM, the first LS with the Large Language Model (LLM). We fine-tunedthe LLaMA2 with a large-scale constructed dataset encompassing rich discoursecharacteristics, which enables the fine-tuned LLM to generate texts withspecific discourse in a controllable manner. Then the discourse is used asguiding information and inputted into the fine-tuned LLM in the form of thePrompt together with secret. On this basis, the constructed candidate pool willbe range encoded and use secret to determine the interval. The same prefix ofthis interval\u0026rsquo;s beginning and ending is the secret embedded at this moment.Experiments show that LLsM performs superior to prevalent LS-task andrelated-task baselines regarding text quality, statistical analysis, discoursematching, and anti-steganalysis. In particular, LLsM\u0026rsquo;s MAUVE matric surpassessome baselines by 70%-80%, and its anti-steganalysis performance is 30%-40%higher. Notably, we also present examples of longer stegos generated by LLsM,showing its potential superiority in long LS tasks.\r2024-02-07\nGrounding Foundation Models through Federated Transfer Learning: A General Framework\nYan Kang Tao Fan Hanlin Gu Xiaojin Zhang Lixin Fan Qiang Yang\nabstract\rabstract: Foundation Models (FMs) such as GPT-4 encoded with vast knowledge andpowerful emergent abilities have achieved remarkable success in various naturallanguage processing and computer vision tasks. Grounding FMs by adapting themto domain-specific tasks or augmenting them with domain-specific knowledgeenables us to exploit the full potential of FMs. However, grounding FMs facesseveral challenges, stemming primarily from constrained computing resources,data privacy, model heterogeneity, and model ownership. Federated TransferLearning (FTL), the combination of federated learning and transfer learning,provides promising solutions to address these challenges. In recent years, theneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly inboth academia and industry. Motivated by the strong growth in FTL-FM researchand the potential impact of FTL-FM on industrial applications, we propose anFTL-FM framework that formulates problems of grounding FMs in the federatedlearning setting, construct a detailed taxonomy based on the FTL-FM frameworkto categorize state-of-the-art FTL-FM works, and comprehensively overviewFTL-FM works based on the proposed taxonomy. We also establish correspondencesbetween FTL-FM and conventional phases of adapting FM so that FM practitionerscan align their research works with FTL-FM. In addition, we overview advancedefficiency-improving and privacy-preserving techniques because efficiency andprivacy are critical concerns in FTL-FM. Last, we discuss opportunities andfuture research directions of FTL-FM.\r2024-02-06\nEmbedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy\nEfe Bozkir Süleyman Özdel Ka Hei Carrie Lau Mengdi Wang Hong Gao Enkelejda Kasneci\nabstract\rabstract: Recent developments in computer graphics, hardware, artificial intelligence(AI), and human-computer interaction likely lead to extended reality (XR)devices and setups being more pervasive. While these devices and setups provideusers with interactive, engaging, and immersive experiences with differentsensing modalities, such as eye and hand trackers, many non-player charactersare utilized in a pre-scripted way or by conventional AI techniques. In thispaper, we argue for using large language models (LLMs) in XR by embedding themin virtual avatars or as narratives to facilitate more inclusive experiencesthrough prompt engineering according to user profiles and fine-tuning the LLMsfor particular purposes. We argue that such inclusion will facilitate diversityfor XR use. In addition, we believe that with the versatile conversationalcapabilities of LLMs, users will engage more with XR environments, which mighthelp XR be more used in everyday life. Lastly, we speculate that combining theinformation provided to LLM-powered environments by the users and the biometricdata obtained through the sensors might lead to novel privacy invasions. Whilestudying such possible privacy invasions, user privacy concerns and preferencesshould also be investigated. In summary, despite some challenges, embeddingLLMs into XR is a promising and novel research area with several opportunities.\rSelective Pre-training for Private Fine-tuning\nDa Yu Sivakanth Gopi Janardhan Kulkarni Zinan Lin Saurabh Naik Tomasz Lukasz Religa Jian Yin Huishuai Zhang\nabstract\rabstract: Suppose we want to train text prediction models in email clients or wordprocessors. These models, which serve billions of predictions per hour, mustpreserve the privacy of user data and adhere to specific model size constraintsto meet memory, inference time requirements, and to reduce inference cost.Building small, fast, and private domain-specific language models is a thrivingarea of research. In this work, we show that a careful pre-training on a {\\emsubset} of the public dataset that is guided by the private dataset is crucialto train small DP language models. On standard benchmarks, models trained withour new framework achieve state-of-the-art performance, improving upon all thebaselines from the literature. Besides performance improvements, our framework also shows that with carefulpre-training and private fine-tuning, smaller models can match the performanceof much larger models that do not have access to private data, highlighting thepromise of private learning as a tool for model compression and efficiency. In many applications such as health care, finance, etc., private datasets areusually of much higher quality than public datasets, and our work shows novelways of utilizing private datasets at all the stages of training pipe-line toimprove deep learning efficiency. Language models based on our framework havebeen used in multiple real-world deployments serving billions of predictionsper day (and saving millions of dollars in terms of inference cost)highlighting the general applicability of our framework beyond academicbenchmarks.\r2024-02-05\nTexShape: Information Theoretic Sentence Embedding for Language Models\nH. Kaan Kale Homa Esfahanizadeh Noel Elias Oguzhan Baser Muriel Medard Sriram Vishwanath\nabstract\rabstract: With the exponential growth in data volume and the emergence ofdata-intensive applications, particularly in the field of machine learning,concerns related to resource utilization, privacy, and fairness have becomeparamount. This paper focuses on the textual domain of data and addresseschallenges regarding encoding sentences to their optimized representationsthrough the lens of information-theory. In particular, we use empiricalestimates of mutual information, using the Donsker-Varadhan definition ofKullback-Leibler divergence. Our approach leverages this estimation to train aninformation-theoretic sentence embedding, called TexShape, for (task-based)data compression or for filtering out sensitive information, enhancing privacyand fairness. In this study, we employ a benchmark language model for initialtext representation, complemented by neural networks for information-theoreticcompression and mutual information estimations. Our experiments demonstratesignificant advancements in preserving maximal targeted information and minimalsensitive information over adverse compression ratios, in terms of predictiveaccuracy of downstream models that are trained using the compressed data.\rPsychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach\nSergi Blanco-Cuaresma\nabstract\rabstract: This study explores the use of Large Language Models (LLMs) to analyze textcomments from Reddit users, aiming to achieve two primary objectives: firstly,to pinpoint critical excerpts that support a predefined psychologicalassessment of suicidal risk; and secondly, to summarize the material tosubstantiate the preassigned suicidal risk level. The work is circumscribed tothe use of \u0026ldquo;open-source\u0026rdquo; LLMs that can be run locally, thereby enhancing dataprivacy. Furthermore, it prioritizes models with low computationalrequirements, making it accessible to both individuals and institutionsoperating on limited computing budgets. The implemented strategy only relies ona carefully crafted prompt and a grammar to guide the LLM\u0026rsquo;s text completion.Despite its simplicity, the evaluation metrics show outstanding results, makingit a valuable privacy-focused and cost-effective approach. This work is part ofthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 sharedtask.\rWeak-to-Strong Jailbreaking on Large Language Models\nXuandong Zhao Xianjun Yang Tianyu Pang Chao Du Lei Li Yu-Xiang Wang William Yang Wang\nabstract\rabstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack\u0026rsquo;s key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model\u0026rsquo;s decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at https://github.com/XuandongZhao/weak-to-strong\rConversation Reconstruction Attack Against GPT Models\nJunjie Chu Zeyang Sha Michael Backes Yang Zhang\nabstract\rabstract: In recent times, significant advancements have been made in the field oflarge language models (LLMs), represented by GPT series models. To optimizetask execution, users often engage in multi-round conversations with GPT modelshosted in cloud environments. These multi-round conversations, potentiallyreplete with private information, require transmission and storage within thecloud. However, this operational paradigm introduces additional attacksurfaces. In this paper, we first introduce a specific ConversationReconstruction Attack targeting GPT models. Our introduced ConversationReconstruction Attack is composed of two steps: hijacking a session andreconstructing the conversations. Subsequently, we offer an exhaustiveevaluation of the privacy risks inherent in conversations when GPT models aresubjected to the proposed attack. However, GPT-4 demonstrates certainrobustness to the proposed attacks. We then introduce two advanced attacksaimed at better reconstructing previous conversations, specifically the UNRattack and the PBU attack. Our experimental findings indicate that the PBUattack yields substantial performance across all models, achieving semanticsimilarity scores exceeding 0.60, while the UNR attack is effective solely onGPT-3.5. Our results reveal the concern about privacy risks associated withconversations involving GPT models and aim to draw the community\u0026rsquo;s attention toprevent the potential misuse of these models\u0026rsquo; remarkable capabilities. We willresponsibly disclose our findings to the suppliers of related large languagemodels.\rPutting Context in Context: the Impact of Discussion Structure on Text Classification\nNicolò Penzo Antonio Longa Bruno Lepri Sara Tonelli Marco Guerini\nabstract\rabstract: Current text classification approaches usually focus on the content to beclassified. Contextual aspects (both linguistic and extra-linguistic) areusually neglected, even in tasks based on online discussions. Still in manycases the multi-party and multi-turn nature of the context from which theseelements are selected can be fruitfully exploited. In this work, we propose aseries of experiments on a large dataset for stance detection in English, inwhich we evaluate the contribution of different types of contextualinformation, i.e. linguistic, structural and temporal, by feeding them asnatural language input into a transformer-based model. We also experiment withdifferent amounts of training data and analyse the topology of local discussionnetworks in a privacy-compliant way. Results show that structural informationcan be highly beneficial to text classification but only under certaincircumstances (e.g. depending on the amount of training data and on discussionchain complexity). Indeed, we show that contextual information on smallerdatasets from other classification tasks does not yield significantimprovements. Our framework, based on local discussion networks, allows theintegration of structural information, while minimising user profiling, thuspreserving their privacy.\rZero-Shot Machine Unlearning at Scale via Lipschitz Regularization\nJack Foster Kyle Fogarty Stefan Schoepf Cengiz Öztireli Alexandra Brintrup\nabstract\rabstract: To comply with AI and data regulations, the need to forget private orcopyrighted information from trained machine learning models is increasinglyimportant. The key challenge in unlearning is forgetting the necessary data ina timely manner, while preserving model performance. In this work, we addressthe zero-shot unlearning scenario, whereby an unlearning algorithm must be ableto remove data given only a trained model and the data to be forgotten. Undersuch a definition, existing state-of-the-art methods are insufficient. Buildingon the concepts of Lipschitz continuity, we present a method that inducessmoothing of the forget sample\u0026rsquo;s output, with respect to perturbations of thatsample. We show this smoothing successfully results in forgetting whilepreserving general model performance. We perform extensive empirical evaluationof our method over a range of contemporary benchmarks, verifying that ourmethod achieves state-of-the-art performance under the strict constraints ofzero-shot unlearning.\r2024-02-04\nCopyright Protection in Generative AI: A Technical Perspective\nJie Ren Han Xu Pengfei He Yingqian Cui Shenglai Zeng Jiankun Zhang Hongzhi Wen Jiayuan Ding Hui Liu Yi Chang Jiliang Tang\nabstract\rabstract: Generative AI has witnessed rapid advancement in recent years, expandingtheir capabilities to create synthesized content such as text, images, audio,and code. The high fidelity and authenticity of contents generated by theseDeep Generative Models (DGMs) have sparked significant copyright concerns.There have been various legal debates on how to effectively safeguardcopyrights in DGMs. This work delves into this issue by providing acomprehensive overview of copyright protection from a technical perspective. Weexamine from two distinct viewpoints: the copyrights pertaining to the sourcedata held by the data owners and those of the generative models maintained bythe model builders. For data copyright, we delve into methods data owners canprotect their content and DGMs can be utilized without infringing upon theserights. For model copyright, our discussion extends to strategies forpreventing model theft and identifying outputs generated by specific models.Finally, we highlight the limitations of existing techniques and identify areasthat remain unexplored. Furthermore, we discuss prospective directions for thefuture of copyright protection, underscoring its importance for the sustainableand ethical development of Generative AI.\rA Survey of Large Language Models in Finance (FinLLMs)\nJean Lee Nicholas Stevens Soyeon Caren Han Minseok Song\nabstract\rabstract: Large Language Models (LLMs) have shown remarkable capabilities across a widevariety of Natural Language Processing (NLP) tasks and have attracted attentionfrom multiple domains, including financial services. Despite the extensiveresearch into general-domain LLMs, and their immense potential in finance,Financial LLM (FinLLM) research remains limited. This survey provides acomprehensive overview of FinLLMs, including their history, techniques,performance, and opportunities and challenges. Firstly, we present achronological overview of general-domain Pre-trained Language Models (PLMs)through to current FinLLMs, including the GPT-series, selected open-sourceLLMs, and financial LMs. Secondly, we compare five techniques used acrossfinancial PLMs and FinLLMs, including training methods, training data, andfine-tuning methods. Thirdly, we summarize the performance evaluations of sixbenchmark tasks and datasets. In addition, we provide eight advanced financialNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, wediscuss the opportunities and the challenges facing FinLLMs, such ashallucination, privacy, and efficiency. To support AI research in finance, wecompile a collection of accessible datasets and evaluation benchmarks onGitHub.\r2024-02-03\nSeparable Multi-Concept Erasure from Diffusion Models\nMengnan Zhao Lihe Zhang Tianhang Zheng Yuqiu Kong Baocai Yin\nabstract\rabstract: Large-scale diffusion models, known for their impressive image generationcapabilities, have raised concerns among researchers regarding social impacts,such as the imitation of copyrighted artistic styles. In response, existingapproaches turn to machine unlearning techniques to eliminate unsafe conceptsfrom pre-trained models. However, these methods compromise the generativeperformance and neglect the coupling among multi-concept erasures, as well asthe concept restoration problem. To address these issues, we propose aSeparable Multi-concept Eraser (SepME), which mainly includes two parts: thegeneration of concept-irrelevant representations and the weight decoupling. Theformer aims to avoid unlearning substantial information that is irrelevant toforgotten concepts. The latter separates optimizable model weights, making eachweight increment correspond to a specific concept erasure without affectinggenerative performance on other concepts. Specifically, the weight incrementfor erasing a specified concept is formulated as a linear combination ofsolutions calculated based on other known undesirable concepts. Extensiveexperiments indicate the efficacy of our approach in eliminating concepts,preserving model performance, and offering flexibility in the erasure orrecovery of various concepts.\rHuman-Centered Privacy Research in the Age of Large Language Models\nTianshi Li Sauvik Das Hao-Ping Lee Dakuo Wang Bingsheng Yao Zhiping Zhang\nabstract\rabstract: The emergence of large language models (LLMs), and their increased use inuser-facing systems, has led to substantial privacy concerns. To date, researchon these privacy concerns has been model-centered: exploring how LLMs lead toprivacy risks like memorization, or can be used to infer personalcharacteristics about people from their content. We argue that there is a needfor more research focusing on the human aspect of these privacy issues: e.g.,research on how design paradigms for LLMs affect users\u0026rsquo; disclosure behaviors,users\u0026rsquo; mental models and preferences for privacy controls, and the design oftools, systems, and artifacts that empower end-users to reclaim ownership overtheir personal data. To build usable, efficient, and privacy-friendly systemspowered by these models with imperfect privacy properties, our goal is toinitiate discussions to outline an agenda for conducting human-centeredresearch on privacy issues in LLM-powered systems. This Special Interest Group(SIG) aims to bring together researchers with backgrounds in usable securityand privacy, human-AI collaboration, NLP, or any other related domains to sharetheir perspectives and experiences on this problem, to help our communityestablish a collective understanding of the challenges, research opportunities,research methods, and strategies to collaborate with researchers outside ofHCI.\r2024-02-02\nDigits micro-model for accurate and secure transactions\nChirag Chhablani Nikhita Sharma Jordan Hosier Vijay K. Gurbani\nabstract\rabstract: Automatic Speech Recognition (ASR) systems are used in the financial domainto enhance the caller experience by enabling natural language understanding andfacilitating efficient and intuitive interactions. Increasing use of ASRsystems requires that such systems exhibit very low error rates. Thepredominant ASR models to collect numeric data are large, general-purposecommercial models \u0026ndash; Google Speech-to-text (STT), or Amazon Transcribe \u0026ndash; oropen source (OpenAI\u0026rsquo;s Whisper). Such ASR models are trained on hundreds ofthousands of hours of audio data and require considerable resources to run.Despite recent progress large speech recognition models, we highlight thepotential of smaller, specialized \u0026ldquo;micro\u0026rdquo; models. Such light models can betrained perform well on number recognition specific tasks, competing withgeneral models like Whisper or Google STT while using less than 80 minutes oftraining time and occupying at least an order of less memory resources. Also,unlike larger speech recognition models, micro-models are trained on carefullyselected and curated datasets, which makes them highly accurate, agile, andeasy to retrain, while using low compute resources. We present our work oncreating micro models for multi-digit number recognition that handle diversespeaking styles reflecting real-world pronunciation patterns. Our workcontributes to domain-specific ASR models, improving digit recognitionaccuracy, and privacy of data. An added advantage, their low resourceconsumption allows them to be hosted on-premise, keeping private data localinstead uploading to an external cloud. Our results indicate that ourmicro-model makes less errors than the best-of-breed commercial or open-sourceASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for ourmodel versus 11 GB VRAM for Whisper).\r(A)I Am Not a Lawyer, But\u0026hellip;: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice\nInyoung Cheong King Xia K. J. Kevin Feng Quan Ze Chen Amy X. Zhang\nabstract\rabstract: The rapid proliferation of large language models (LLMs) as general purposechatbots available to the public raises hopes around expanding access toprofessional guidance in law, medicine, and finance, while triggering concernsabout public reliance on LLMs for high-stakes circumstances. Prior research hasspeculated on high-level ethical considerations but lacks concrete criteriadetermining when and why LLM chatbots should or should not provide professionalassistance. Through examining the legal domain, we contribute a structuredexpert analysis to uncover nuanced policy considerations around using LLMs forprofessional advice, using methods inspired by case-based reasoning. Weconvened workshops with 20 legal experts and elicited dimensions on appropriateAI assistance for sample user queries (``cases\u0026rsquo;\u0026rsquo;). We categorized our expertdimensions into: (1) user attributes, (2) query characteristics, (3) AIcapabilities, and (4) impacts. Beyond known issues like hallucinations, expertsrevealed novel legal problems, including that users\u0026rsquo; conversations with LLMsare not protected by attorney-client confidentiality or bound to professionalethics that guard against conflicted counsel or poor quality advice. Thisaccountability deficit led participants to advocate for AI systems to helpusers polish their legal questions and relevant facts, rather than recommendspecific actions. More generally, we highlight the potential of case-basedexpert deliberation as a method of responsibly translating professionalintegrity and domain knowledge into design requirements to inform appropriateAI behavior when generating advice in professional domains.\rTransFR: Transferable Federated Recommendation with Pre-trained Language Models\nHonglei Zhang He Liu Haoxuan Li Yidong Li\nabstract\rabstract: Federated recommendations (FRs), facilitating multiple local clients tocollectively learn a global model without disclosing user private data, haveemerged as a prevalent architecture for privacy-preserving recommendations. Inconventional FRs, a dominant paradigm is to utilize discrete identities torepresent users/clients and items, which are subsequently mapped todomain-specific embeddings to participate in model training. Despiteconsiderable performance, we reveal three inherent limitations that can not beignored in federated settings, i.e., non-transferability across domains,unavailability in cold-start settings, and potential privacy violations duringfederated training. To this end, we propose a transferable federatedrecommendation model with universal textual representations, TransFR, whichdelicately incorporates the general capabilities empowered by pre-trainedlanguage models and the personalized abilities by fine-tuning local privatedata. Specifically, it first learns domain-agnostic representations of items byexploiting pre-trained models with public textual corpora. To tailor forfederated recommendation, we further introduce an efficient federatedfine-tuning and a local training mechanism. This facilitates personalized localheads for each client by utilizing their private behavior data. Byincorporating pre-training and fine-tuning within FRs, it greatly improves theadaptation efficiency transferring to a new domain and the generalizationcapacity to address cold-start issues. Through extensive experiments on severaldatasets, we demonstrate that our TransFR model surpasses severalstate-of-the-art FRs in terms of accuracy, transferability, and privacy.\rDTS-SQL: Decomposed Text-to-SQL with Small Large Language Models\nMohammadreza Pourreza Davood Rafiei\nabstract\rabstract: Leading models for the text-to-SQL task heavily rely on proprietary LargeLanguage Models (LLMs), posing concerns over data privacy. Closing theperformance gap between small open-source models and large proprietary modelsis crucial to mitigate this reliance. To this end, we introduce a noveltwo-stage fine-tuning approach that decomposes the task into two simpler tasks.Through comprehensive evaluation on two large cross-domain datasets and twosmall LLMs, we show that this approach improves execution accuracy by 3 to 7percent, effectively aligning the performance of open-source models with theirproprietary counterparts.\r2024-02-01\nHR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent\nWeijie Xu Zicheng Huang Wenxiang Hu Xi Fang Rajesh Kumar Cherukuri Naumaan Nayyar Lorenzo Malandri Srinivasan H. Sengamedu\nabstract\rabstract: Recent advancements in Large Language Models (LLMs) have been reshapingNatural Language Processing (NLP) task in several domains. Their use in thefield of Human Resources (HR) has still room for expansions and could bebeneficial for several time consuming tasks. Examples such as time-offsubmissions, medical claims filing, and access requests are noteworthy, butthey are by no means the sole instances. However, the aforementioneddevelopments must grapple with the pivotal challenge of constructing ahigh-quality training dataset. On one hand, most conversation datasets aresolving problems for customers not employees. On the other hand, gatheringconversations with HR could raise privacy concerns. To solve it, we introduceHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HRdomains to evaluate LLM Agent. Our work has the following contributions: (1) Itis the first labeled open-sourced conversation dataset in the HR domain for NLPresearch. (2) It provides a detailed recipe for the data generation procedurealong with data analysis and human evaluations. The data generation pipeline istransferable and can be easily adapted for labeled conversation data generationin other domains. (3) The proposed data-collection pipeline is mostly based onLLMs with minimal human involvement for annotation, which is time andcost-efficient.\r2024-01-31\nDe-identification is not always enough\nAtiquer Rahman Sarkar Yao-Shun Chuang Noman Mohammed Xiaoqian Jiang\nabstract\rabstract: For sharing privacy-sensitive data, de-identification is commonly regarded asadequate for safeguarding privacy. Synthetic data is also being considered as aprivacy-preserving alternative. Recent successes with numerical and tabulardata generative models and the breakthroughs in large generative languagemodels raise the question of whether synthetically generated clinical notescould be a viable alternative to real notes for research purposes. In thiswork, we demonstrated that (i) de-identification of real clinical notes doesnot protect records against a membership inference attack, (ii) proposed anovel approach to generate synthetic clinical notes using the currentstate-of-the-art large language models, (iii) evaluated the performance of thesynthetically generated notes in a clinical domain task, and (iv) proposed away to mount a membership inference attack where the target model is trainedwith synthetic data. We observed that when synthetically generated notesclosely match the performance of real data, they also exhibit similar privacyconcerns to the real data. Whether other approaches to synthetically generatedclinical notes could offer better trade-offs and become a better alternative tosensitive real notes warrants further investigation.\rFederated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes\nZhen Qin Daoyuan Chen Bingchen Qian Bolin Ding Yaliang Li Shuiguang Deng\nabstract\rabstract: Pre-trained large language models (LLMs) need fine-tuning to improve theirresponsiveness to natural language instructions. Federated learning offers away to fine-tune LLMs using the abundant data on end devices withoutcompromising data privacy. Most existing federated fine-tuning methods for LLMsrely on parameter-efficient fine-tuning techniques, which may not reach theperformance height possible with full-parameter tuning. However, federatedfull-parameter tuning of LLMs is a non-trivial problem due to the immensecommunication cost. This work introduces FedKSeed that employs zeroth-orderoptimization with a finite set of random seeds. It significantly reducestransmission requirements between the server and clients to just a few randomseeds and scalar gradients, amounting to only a few thousand bytes, makingfederated full-parameter tuning of billion-sized LLMs possible on devices.Building on it, we develop a strategy enabling probability-differentiated seedsampling, prioritizing perturbations with greater impact on model accuracy.Experiments across six scenarios with various LLMs, datasets and datapartitions demonstrate that our approach outperforms existing federated LLMfine-tuning methods in both communication efficiency and new taskgeneralization.\rPrompt-enhanced Federated Content Representation Learning for Cross-domain Recommendation\nLei Guo Ziang Lu Junliang Yu Nguyen Quoc Viet Hung Hongzhi Yin\nabstract\rabstract: Cross-domain Recommendation (CDR) as one of the effective techniques inalleviating the data sparsity issues has been widely studied in recent years.However, previous works may cause domain privacy leakage since they necessitatethe aggregation of diverse domain data into a centralized server during thetraining process. Though several studies have conducted privacy preserving CDRvia Federated Learning (FL), they still have the following limitations: 1) Theyneed to upload users\u0026rsquo; personal information to the central server, posing therisk of leaking user privacy. 2) Existing federated methods mainly rely onatomic item IDs to represent items, which prevents them from modeling items ina unified feature space, increasing the challenge of knowledge transfer amongdomains. 3) They are all based on the premise of knowing overlapped usersbetween domains, which proves impractical in real-world applications. Toaddress the above limitations, we focus on Privacy-preserving Cross-domainRecommendation (PCDR) and propose PFCR as our solution. For Limitation 1, wedevelop a FL schema by exclusively utilizing users\u0026rsquo; interactions with localclients and devising an encryption method for gradient encryption. ForLimitation 2, we model items in a universal feature space by their descriptiontexts. For Limitation 3, we initially learn federated content representations,harnessing the generality of natural language to establish bridges betweendomains. Subsequently, we craft two prompt fine-tuning strategies to tailor thepre-trained model to the target domain. Extensive experiments on two real-worlddatasets demonstrate the superiority of our PFCR method compared to the SOTAapproaches.\r2024-01-30\nAre ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?\nDimitrios Ioannidis Jeremy Kepner Andrew Bowne Harriet S. Bryant\nabstract\rabstract: The rise of Generative Artificial Intelligence systems (\u0026ldquo;AI systems\u0026rdquo;) hascreated unprecedented social engagement. AI code generation systems provideresponses (output) to questions or requests by accessing the vast library ofopen-source code created by developers over the past few decades. However, theydo so by allegedly stealing the open-source code stored in virtual libraries,known as repositories. This Article focuses on how this happens and whetherthere is a solution that protects innovation and avoids years of litigation. Wealso touch upon the array of issues raised by the relationship between AI andcopyright. Looking ahead, we propose the following: (a) immediate changes tothe licenses for open-source code created by developers that will limit accessand/or use of any open-source code to humans only; (b) we suggest revisions tothe Massachusetts Institute of Technology (\u0026ldquo;MIT\u0026rdquo;) license so that AI systemsare required to procure appropriate licenses from open-source code developers,which we believe will harmonize standards and build social consensus for thebenefit of all of humanity, rather than promote profit-driven centers ofinnovation; (c) we call for urgent legislative action to protect the future ofAI systems while also promoting innovation; and (d) we propose a shift in theburden of proof to AI systems in obfuscation cases.\rTrust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik\u0026rsquo;s Cube\nKausik Lakkaraju Vedant Khandelwal Biplav Srivastava Forest Agostinelli Hengtao Tang Prathamjeet Singh Dezhi Wu Matt Irvin Ashish Kundu\nabstract\rabstract: Artificial intelligence (AI) has the potential to transform education withits power of uncovering insights from massive data about student learningpatterns. However, ethical and trustworthy concerns of AI have been raised butare unsolved. Prominent ethical issues in high school AI education include dataprivacy, information leakage, abusive language, and fairness. This paperdescribes technological components that were built to address ethical andtrustworthy concerns in a multi-modal collaborative platform (called ALLUREchatbot) for high school students to collaborate with AI to solve the Rubik\u0026rsquo;scube. In data privacy, we want to ensure that the informed consent of children,parents, and teachers, is at the center of any data that is managed. Sincechildren are involved, language, whether textual, audio, or visual, isacceptable both from users and AI and the system can steer interaction awayfrom dangerous situations. In information management, we also want to ensurethat the system, while learning to improve over time, does not leak informationabout users from one group to another.\rA Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion\nGuangke Chen Yedi Zhang Fu Song Ting Wang Xiaoning Du Yang Liu\nabstract\rabstract: Singing voice conversion (SVC) automates song covers by converting onesinger\u0026rsquo;s singing voice into another target singer\u0026rsquo;s singing voice with theoriginal lyrics and melody. However, it raises serious concerns about copyrightand civil right infringements to multiple entities. This work proposesSongBsAb, the first proactive approach to mitigate unauthorized SVC-basedillegal song covers. SongBsAb introduces human-imperceptible perturbations tosinging voices before releasing them, so that when they are used, thegeneration process of SVC will be interfered, resulting in unexpected singingvoices. SongBsAb features a dual prevention effect by causing both (singer)identity disruption and lyric disruption, namely, the SVC-covered singing voiceneither imitates the target singer nor preserves the original lyrics. Toimprove the imperceptibility of perturbations, we refine a psychoacousticmodel-based loss with the backing track as an additional masker, a uniqueaccompanying element for singing voices compared to ordinary speech voices. Toenhance the transferability, we propose to utilize a frame-level interactionreduction-based loss. We demonstrate the prevention effectiveness, utility, androbustness of SongBsAb on three SVC models and two datasets using bothobjective and human study-based subjective metrics. Our work fosters anemerging research direction for mitigating illegal automated song covers.\rAalap: AI Assistant for Legal \u0026amp; Paralegal Functions in India\nAman Tiwari Prathamesh Kalamkar Atreyo Banerjee Saurabh Karn Varun Hemachandran Smita Gupta\nabstract\rabstract: Using proprietary Large Language Models on legal tasks poses challenges dueto data privacy issues, domain data heterogeneity, domain knowledgesophistication, and domain objectives uniqueness. We created Aalalp, afine-tuned Mistral 7B model on instructions data related to specific Indianlegal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31% ofour test data and obtains an equivalent score in 34% of the test data asevaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoningrather than legal recall. Aalap is definitely helpful for the day-to-dayactivities of lawyers, judges, or anyone working in legal systems.\rSecurity and Privacy Challenges of Large Language Models: A Survey\nBadhan Chandra Das M. Hadi Amini Yanzhao Wu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.\r2024-01-29\nTowards Building the Federated GPT: Federated Instruction Tuning\nJianyi Zhang Saeed Vahidian Martin Kuo Chunyuan Li Ruiyi Zhang Tong Yu Yufan Zhou Guoyin Wang Yiran Chen\nabstract\rabstract: While \u0026ldquo;instruction-tuned\u0026rdquo; generative large language models (LLMs) havedemonstrated an impressive ability to generalize to new tasks, the trainingphases heavily rely on large amounts of diverse and high-quality instructiondata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,especially when it comes to human-written data, can pose significant challengesboth in terms of cost and accessibility. Moreover, concerns related to privacycan further limit access to such data, making the process of obtaining it acomplex and nuanced undertaking. Consequently, this hinders the generality ofthe tuned models and may restrict their effectiveness in certain contexts. Totackle this issue, our study introduces a new approach called FederatedInstruction Tuning (FedIT), which leverages federated learning (FL) as thelearning framework for the instruction tuning of LLMs. This marks the firstexploration of FL-based instruction tuning for LLMs. This is especiallyimportant since text data is predominantly generated by end users. Therefore,it is imperative to design and adapt FL approaches to effectively leveragethese users\u0026rsquo; diverse instructions stored on local devices, while preservingprivacy and ensuring data security. In the current paper, by conducting widelyused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneousand diverse sets of instructions on the client\u0026rsquo;s end with the proposedframework FedIT, we improved the performance of LLMs compared to centralizedtraining with only limited local instructions. Further, in this paper, wedeveloped a Github repository named Shepherd. This repository offers afoundational framework for exploring federated fine-tuning of LLMs usingheterogeneous instructions across diverse categories.\rSSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning\nPeizhuo Lv Pan Li Shenchen Zhu Shengzhi Zhang Kai Chen Ruigang Liang Chang Yue Fan Xiang Yuling Cai Hualong Ma Yingjun Zhang Guozhu Meng\nabstract\rabstract: Recent years have witnessed tremendous success in Self-Supervised Learning(SSL), which has been widely utilized to facilitate various downstream tasks inComputer Vision (CV) and Natural Language Processing (NLP) domains. However,attackers may steal such SSL models and commercialize them for profit, makingit crucial to verify the ownership of the SSL models. Most existing ownershipprotection solutions (e.g., backdoor-based watermarks) are designed forsupervised learning models and cannot be used directly since they require thatthe models\u0026rsquo; downstream tasks and target labels be known and available duringwatermark embedding, which is not always possible in the domain of SSL. Toaddress such a problem, especially when downstream tasks are diverse andunknown during watermark embedding, we propose a novel black-box watermarkingsolution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM mapswatermarked inputs of the protected encoders into an invariant representationspace, which causes any downstream classifier to produce expected behavior,thus allowing the detection of embedded watermarks. We evaluate SSL-WM onnumerous tasks, such as CV and NLP, using different SSL models bothcontrastive-based and generative-based. Experimental results demonstrate thatSSL-WM can effectively verify the ownership of stolen SSL models in variousdownstream tasks. Furthermore, SSL-WM is robust against model fine-tuning,pruning, and input preprocessing attacks. Lastly, SSL-WM can also evadedetection from evaluated watermark detection approaches, demonstrating itspromising application in protecting the ownership of SSL models.\rImportance-Aware Adaptive Dataset Distillation\nGuang Li Ren Togo Takahiro Ogawa Miki Haseyama\nabstract\rabstract: Herein, we propose a novel dataset distillation method for constructing smallinformative datasets that preserve the information of the large originaldatasets. The development of deep learning models is enabled by theavailability of large-scale datasets. Despite unprecedented success,large-scale datasets considerably increase the storage and transmission costs,resulting in a cumbersome model training process. Moreover, using raw data fortraining raises privacy and copyright concerns. To address these issues, a newtask named dataset distillation has been introduced, aiming to synthesize acompact dataset that retains the essential information from the large originaldataset. State-of-the-art (SOTA) dataset distillation methods have beenproposed by matching gradients or network parameters obtained during trainingon real and synthetic datasets. The contribution of different networkparameters to the distillation process varies, and uniformly treating themleads to degraded distillation performance. Based on this observation, wepropose an importance-aware adaptive dataset distillation (IADD) method thatcan improve distillation performance by automatically assigning importanceweights to different network parameters during distillation, therebysynthesizing more robust distilled datasets. IADD demonstrates superiorperformance over other SOTA dataset distillation methods based on parametermatching on multiple benchmark datasets and outperforms them in terms ofcross-architecture generalization. In addition, the analysis of self-adaptiveweights demonstrates the effectiveness of IADD. Furthermore, the effectivenessof IADD is validated in a real-world medical application such as COVID-19detection.\r2024-01-28\nData-Free Generalized Zero-Shot Learning\nBowen Tang Long Yan Jing Zhang Qian Yu Lu Sheng Dong Xu\nabstract\rabstract: Deep learning models have the ability to extract rich knowledge fromlarge-scale datasets. However, the sharing of data has become increasinglychallenging due to concerns regarding data copyright and privacy. Consequently,this hampers the effective transfer of knowledge from existing data to noveldownstream tasks and concepts. Zero-shot learning (ZSL) approaches aim torecognize new classes by transferring semantic knowledge learned from baseclasses. However, traditional generative ZSL methods often require access toreal images from base classes and rely on manually annotated attributes, whichpresents challenges in terms of data restrictions and model scalability. Tothis end, this paper tackles a challenging and practical problem dubbed asdata-free zero-shot learning (DFZSL), where only the CLIP-based base classesdata pre-trained classifier is available for zero-shot classification.Specifically, we propose a generic framework for DFZSL, which consists of threemain components. Firstly, to recover the virtual features of the base data, wemodel the CLIP features of base class images as samples from a von Mises-Fisher(vMF) distribution based on the pre-trained classifier. Secondly, we leveragethe text features of CLIP as low-cost semantic information and propose afeature-language prompt tuning (FLPT) method to further align the virtual imagefeatures and textual features. Thirdly, we train a conditional generative modelusing the well-aligned virtual image features and corresponding semantic textfeatures, enabling the generation of new classes features and achieve betterzero-shot generalization. Our framework has been evaluated on five commonlyused benchmarks for generalized ZSL, as well as 11 benchmarks for thebase-to-new ZSL. The results demonstrate the superiority and effectiveness ofour approach. Our code is available in https://github.com/ylong4/DFZSL\rAI as a Medical Ally: Evaluating ChatGPT\u0026rsquo;s Usage and Impact in Indian Healthcare\nAryaman Raina Prateek Mishra Harshit goyal Dhruv Kumar\nabstract\rabstract: This study investigates the integration and impact of Large Language Models(LLMs), like ChatGPT, in India\u0026rsquo;s healthcare sector. Our research employs a dualapproach, engaging both general users and medical professionals through surveysand interviews respectively. Our findings reveal that healthcare professionalsvalue ChatGPT in medical education and preliminary clinical settings, butexercise caution due to concerns about reliability, privacy, and the need forcross-verification with medical references. General users show a preference forAI interactions in healthcare, but concerns regarding accuracy and trustpersist. The study underscores the need for these technologies to complement,not replace, human medical expertise, highlighting the importance of developingLLMs in collaboration with healthcare providers. This paper enhances theunderstanding of LLMs in healthcare, detailing current usage, user trust, andimprovement areas. Our insights inform future research and development,underscoring the need for ethically compliant, user-focused LLM advancementsthat address healthcare-specific challenges.\rPrivacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\nXinyu Tang Richard Shin Huseyin A. Inan Andre Manoel Fatemehsadat Mireshghallah Zinan Lin Sivakanth Gopi Janardhan Kulkarni Robert Sim\nabstract\rabstract: We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.\r2024-01-27\nDataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure\nJunyi Ye Mengnan Du Guiling Wang\nabstract\rabstract: This paper introduces DataFrame question answering (QA), a novel task thatutilizes large language models (LLMs) to generate Pandas queries forinformation retrieval and data analysis on dataframes, emphasizing safe andnon-revealing data handling. Our method, which solely relies on dataframecolumn names, not only ensures data privacy but also significantly reduces thecontext window in the prompt, streamlining information processing andaddressing major challenges in LLM-based data analysis. We propose DataFrame QAas a comprehensive framework that includes safe Pandas query generation andcode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1metric on the renowned WikiSQL and our newly developed \u0026lsquo;UCI-DataFrameQA\u0026rsquo;,tailored for complex data analysis queries. Our findings indicate that GPT-4achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,underscoring its capability in securely retrieving and aggregating dataframevalues and conducting sophisticated data analyses. This approach, deployable ina zero-shot manner without prior training or adjustments, proves to be highlyadaptable and secure for diverse applications.\rFortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models\nYunhong He Jianling Qiu Wei Zhang Zhengqing Yuan\nabstract\rabstract: Recent advancements in large language models (LLMs) have significantlyenhanced capabilities in natural language processing and artificialintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionizedtext generation, translation, and question-answering tasks due to thetransformative Transformer model. Despite their widespread use, LLMs presentchallenges such as ethical dilemmas when models are compelled to respondinappropriately, susceptibility to phishing attacks, and privacy violations.This paper addresses these challenges by introducing a multi-pronged approachthat includes: 1) filtering sensitive vocabulary from user input to preventunethical responses; 2) detecting role-playing to halt interactions that couldlead to \u0026lsquo;prison break\u0026rsquo; scenarios; 3) implementing custom rule engines torestrict the generation of prohibited content; and 4) extending thesemethodologies to various LLM derivatives like Multi-Model Large Language Models(MLLMs). Our approach not only fortifies models against unethical manipulationsand privacy breaches but also maintains their high performance across tasks. Wedemonstrate state-of-the-art performance under various attack prompts, withoutcompromising the model\u0026rsquo;s core functionalities. Furthermore, the introduction ofdifferentiated security levels empowers users to control their personal datadisclosure. Our methods contribute to reducing social risks and conflictsarising from technological abuse, enhance data protection, and promote socialequity. Collectively, this research provides a framework for balancing theefficiency of question-answering systems with user privacy and ethicalstandards, ensuring a safer user experience and fostering trust in AItechnology.\r2024-01-26\nAM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning\nNaresh Kumar Devulapally Sidharth Anand Sreyasee Das Bhattacharjee Junsong Yuan\nabstract\rabstract: Human emotion can be presented in different modes i.e., audio, video, andtext. However, the contribution of each mode in exhibiting each emotion is notuniform. Furthermore, the availability of complete mode-specific details maynot always be guaranteed in the test time. In this work, we propose AM^2-EmoJE,a model for Adaptive Missing-Modality Emotion Recognition in Conversation viaJoint Embedding Learning model that is grounded on two-fold contributions:First, a query adaptive fusion that can automatically learn the relativeimportance of its mode-specific representations in a query-specific manner. Bythis the model aims to prioritize the mode-invariant spatial query details ofthe emotion patterns, while also retaining its mode-exclusive aspects withinthe learned multimodal query descriptor. Second the multimodal joint embeddinglearning module that explicitly addresses various missing modality scenarios intest-time. By this, the model learns to emphasize on the correlated patternsacross modalities, which may help align the cross-attended mode-specificdescriptors pairwise within a joint-embedding space and thereby compensate formissing modalities during inference. By leveraging the spatio-temporal detailsat the dialogue level, the proposed AM^2-EmoJE not only demonstrates superiorperformance compared to the best-performing state-of-the-art multimodalmethods, by effectively leveraging body language in place of face expression,it also exhibits an enhanced privacy feature. By reporting around 2-5%improvement in the weighted-F1 score, the proposed multimodal joint embeddingmodule facilitates an impressive performance gain in a variety ofmissing-modality query scenarios during test time.\rA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly\nYifan Yao Jinhao Duan Kaidi Xu Yuanfang Cai Zhibo Sun Yue Zhang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into \u0026ldquo;The Good\u0026rdquo; (beneficial LLM applications),\u0026ldquo;The Bad\u0026rdquo; (offensive applications), and \u0026ldquo;The Ugly\u0026rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs\u0026rsquo; potential to both bolster and jeopardize cybersecurity.\r2024-01-25\nTrustLLM: Trustworthiness in Large Language Models\nLichao Sun Yue Huang Haoran Wang Siyuan Wu Qihui Zhang Chujie Gao Yixin Huang Wenhan Lyu Yixuan Zhang Xiner Li Zhengliang Liu Yixin Liu Yijue Wang Zhikun Zhang Bhavya Kailkhura Caiming Xiong Chaowei Xiao Chunyuan Li Eric Xing Furong Huang Hao Liu Heng Ji Hongyi Wang Huan Zhang Huaxiu Yao Manolis Kellis Marinka Zitnik Meng Jiang Mohit Bansal James Zou Jian Pei Jian Liu Jianfeng Gao Jiawei Han Jieyu Zhao Jiliang Tang Jindong Wang John Mitchell Kai Shu Kaidi Xu Kai-Wei Chang Lifang He Lifu Huang Michael Backes Neil Zhenqiang Gong Philip S. Yu Pin-Yu Chen Quanquan Gu Ran Xu Rex Ying Shuiwang Ji Suman Jana Tianlong Chen Tianming Liu Tianyi Zhou William Wang Xiang Li Xiangliang Zhang Xiao Wang Xing Xie Xun Chen Xuyu Wang Yan Liu Yanfang Ye Yinzhi Cao Yong Chen Yue Zhao\nabstract\rabstract: Large language models (LLMs), exemplified by ChatGPT, have gainedconsiderable attention for their excellent natural language processingcapabilities. Nonetheless, these LLMs present many challenges, particularly inthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMsemerges as an important topic. This paper introduces TrustLLM, a comprehensivestudy of trustworthiness in LLMs, including principles for different dimensionsof trustworthiness, established benchmark, evaluation, and analysis oftrustworthiness for mainstream LLMs, and discussion of open challenges andfuture directions. Specifically, we first propose a set of principles fortrustworthy LLMs that span eight different dimensions. Based on theseprinciples, we further establish a benchmark across six dimensions includingtruthfulness, safety, fairness, robustness, privacy, and machine ethics. Wethen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting ofover 30 datasets. Our findings firstly show that in general trustworthiness andutility (i.e., functional effectiveness) are positively related. Secondly, ourobservations reveal that proprietary LLMs generally outperform most open-sourcecounterparts in terms of trustworthiness, raising concerns about the potentialrisks of widely accessible open-source LLMs. However, a few open-source LLMscome very close to proprietary ones. Thirdly, it is important to note that someLLMs may be overly calibrated towards exhibiting trustworthiness, to the extentthat they compromise their utility by mistakenly treating benign prompts asharmful and consequently not responding. Finally, we emphasize the importanceof ensuring transparency not only in the models themselves but also in thetechnologies that underpin trustworthiness. Knowing the specific trustworthytechnologies that have been employed is crucial for analyzing theireffectiveness.\rLLM on FHIR \u0026ndash; Demystifying Health Records\nPaul Schmiedmayer Adrit Rao Philipp Zagar Vishnu Ravi Aydin Zahedivash Arash Fereydooni Oliver Aalami\nabstract\rabstract: Objective: To enhance health literacy and accessibility of health informationfor a diverse patient population by developing a patient-centered artificialintelligence (AI) solution using large language models (LLMs) and FastHealthcare Interoperability Resources (FHIR) application programming interfaces(APIs). Materials and Methods: The research involved developing LLM on FHIR, anopen-source mobile application allowing users to interact with their healthrecords using LLMs. The app is built on Stanford\u0026rsquo;s Spezi ecosystem and usesOpenAI\u0026rsquo;s GPT-4. A pilot study was conducted with the SyntheticMass patientdataset and evaluated by medical experts to assess the app\u0026rsquo;s effectiveness inincreasing health literacy. The evaluation focused on the accuracy, relevance,and understandability of the LLM\u0026rsquo;s responses to common patient questions.Results: LLM on FHIR demonstrated varying but generally high degrees ofaccuracy and relevance in providing understandable health information topatients. The app effectively translated medical data into patient-friendlylanguage and was able to adapt its responses to different patient profiles.However, challenges included variability in LLM responses and the need forprecise filtering of health data. Discussion and Conclusion: LLMs offersignificant potential in improving health literacy and making health recordsmore accessible. LLM on FHIR, as a pioneering application in this field,demonstrates the feasibility and challenges of integrating LLMs into patientcare. While promising, the implementation and pilot also highlight risks suchas inconsistent responses and the importance of replicable output. Futuredirections include better resource identification mechanisms and executing LLMson-device to enhance privacy and reduce costs.\rLinear Programs with Conjunctive Database Queries\nFlorent Capelli Nicolas Crosetti Joachim Niehren Jan Ramon\nabstract\rabstract: In this paper, we study the problem of optimizing a linear program whosevariables are the answers to a conjunctive query. For this we propose thelanguage LP(CQ) for specifying linear programs whose constraints and objectivefunctions depend on the answer sets of conjunctive queries. We contribute anefficient algorithm for solving programs in a fragment of LP(CQ). The naturalapproach constructs a linear program having as many variables as there areelements in the answer set of the queries. Our approach constructs a linearprogram having the same optimal value but fewer variables. This is done byexploiting the structure of the conjunctive queries using generalized hypertreedecompositions of small width to factorize elements of the answer set together.We illustrate the various applications of LP(CQ) programs on three examples:optimizing deliveries of resources, minimizing noise for differential privacy,and computing the s-measure of patterns in graphs as needed for data mining.\rTrust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem\nJ. Carbo J. M. Molina\nabstract\rabstract: In this paper we propose a trust model to be used into a hypothetical mixedenvironment where humans and unmanned vehicles cooperate. We address theinclusion of emotions inside a trust model in a coherent way to the practicalapproaches to the current psychology theories. The most innovative contributionis how privacy issues play a role in the cooperation decisions of the emotionaltrust model. Both, emotions and trust have been cognitively modeled and managedwith the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agentsimplemented in GAML (the programming language of GAMA agent platform) thatcommunicates using the IEEE FIPA standard. The trusting behaviour of theseemotional agents is tested in a cooperative logistics problem where: agentshave to move objects to destinations and some of the objects and places haveprivacy issues. The execution of simulations of this logistic problem shows howemotions and trust contribute to improve the performance of agents in terms ofboth, time savings and privacy protection\r2024-01-24\nEmbedding Attack Project (Work Report)\nJiameng Pu Zafar Takhirov\nabstract\rabstract: This report summarizes all the MIA experiments (Membership Inference Attacks)of the Embedding Attack Project, including threat models, experimental setup,experimental results, findings and discussion. Current results cover theevaluation of two main MIA strategies (loss-based and embedding-based MIAs) on6 AI models ranging from Computer Vision to Language Modelling. There are twoongoing experiments on MIA defense and neighborhood-comparison embeddingattacks. These are ongoing projects. The current work on MIA and PIA can be summarized into six conclusions: (1)Amount of overfitting is directly proportional to model\u0026rsquo;s vulnerability; (2)early embedding layers in the model are less susceptible to privacy leaks; (3)Deeper model layers contain more membership information; (4) Models are morevulnerable to MIA if both embeddings and corresponding training labels arecompromised; (5) it is possible to use pseudo-labels to increase the MIAsuccess; and (6) although MIA and PIA success rates are proportional, reducingthe MIA does not necessarily reduce the PIA.\r2024-01-23\nMitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement\nChenghao Li Dake Chen Yuke Zhang Peter A. Beerel\nabstract\rabstract: While diffusion models demonstrate a remarkable capability for generatinghigh-quality images, their tendency to `replicate\u0026rsquo; training data raises privacyconcerns. Although recent research suggests that this replication may stem fromthe insufficient generalization of training data captions and duplication oftraining images, effective mitigation strategies remain elusive. To addressthis gap, our paper first introduces a generality score that measures thecaption generality and employ large language model (LLM) to generalize trainingcaptions. Subsequently, we leverage generalized captions and propose a noveldual fusion enhancement approach to mitigate the replication of diffusionmodels. Our empirical results demonstrate that our proposed methods cansignificantly reduce replication by 43.5% compared to the original diffusionmodel while maintaining the diversity and quality of generations. Code isavailable at https://github.com/HowardLi0816/dual-fusion-diffusion.\rRed Teaming Visual Language Models\nMukai Li Lei Li Yuwei Yin Masood Ahmed Zhenguang Liu Qi Liu\nabstract\rabstract: VLMs (Vision-Language Models) extend the capabilities of LLMs (Large LanguageModels) to accept multimodal inputs. Since it has been verified that LLMs canbe induced to generate harmful or inaccurate content through specific testcases (termed as Red Teaming), how VLMs perform in similar scenarios,especially with their combination of textual and visual inputs, remains aquestion. To explore this problem, we present a novel red teaming datasetRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modaljail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,privacy, safety, fairness). Our RTVLM is the first red-teaming dataset tobenchmark current VLMs in terms of these 4 different aspects. Detailed analysisshows that 10 prominent open-sourced VLMs struggle with the red teaming indifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning(SFT) using RTVLM, and this bolsters the models\u0026rsquo; performance with 10% in RTVLMtest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,overpassing other LLaVA-based models with regular alignment data. This revealsthat current open-sourced VLMs still lack red teaming alignment. Our code anddatasets will be open-source.\rA Survey of Text Watermarking in the Era of Large Language Models\nAiwei Liu Leyi Pan Yijian Lu Jingjing Li Xuming Hu Xi Zhang Lijie Wen Irwin King Hui Xiong Philip S. Yu\nabstract\rabstract: Text watermarking algorithms play a crucial role in the copyright protectionof textual content, yet their capabilities and application scenarios have beenlimited historically. The recent developments in large language models (LLMs)have opened new opportunities for the advancement of text watermarkingtechniques. LLMs not only enhance the capabilities of text watermarkingalgorithms through their text understanding and generation abilities but alsonecessitate the use of text watermarking algorithms for their own copyrightprotection. This paper conducts a comprehensive survey of the current state oftext watermarking technology, covering four main aspects: (1) an overview andcomparison of different text watermarking techniques; (2) evaluation methodsfor text watermarking algorithms, including their success rates, impact on textquality, robustness, and unforgeability; (3) potential application scenariosfor text watermarking technology; (4) current challenges and future directionsfor development. This survey aims to provide researchers with a thoroughunderstanding of text watermarking technology, thereby promoting its furtheradvancement.\r\u0026ldquo;The teachers are confused as well\u0026rdquo;: A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education\nKyrie Zhixuan Zhou Zachary Kilhoffer Madelyn Rose Sanfilippo Ted Underwood Ece Gumusel Mengyi Wei Abhinav Choudhry Jinjun Xiong\nabstract\rabstract: Large Language Models (LLMs) are advancing quickly and impacting people\u0026rsquo;slives for better or worse. In higher education, concerns have emerged such asstudents\u0026rsquo; misuse of LLMs and degraded education outcomes. To unpack the ethicalconcerns of LLMs for higher education, we conducted a case study consisting ofstakeholder interviews (n=20) in higher education computer science. We foundthat students use several distinct mental models to interact with LLMs - LLMsserve as a tool for (a) writing, (b) coding, and (c) information retrieval,which differ somewhat in ethical considerations. Students and teachers broughtup ethical issues that directly impact them, such as inaccurate LLM responses,hallucinations, biases, privacy leakage, and academic integrity issues.Participants emphasized the necessity of guidance and rules for the use of LLMsin higher education, including teaching digital literacy, rethinking education,and having cautious and contextual policies. We reflect on the ethicalchallenges and propose solutions.\r2024-01-22\nThe Ethics of Interaction: Mitigating Security Threats in LLMs\nAshutosh Kumar Sagarika Singh Shiv Vignesh Murty Swathy Ragupathy\nabstract\rabstract: This paper comprehensively explores the ethical challenges arising fromsecurity threats to Language Learning Models (LLMs). These intricate digitalrepositories are increasingly integrated into our daily lives, making themprime targets for attacks that can compromise their training data and theconfidentiality of their data sources. The paper delves into the nuancedethical repercussions of such security threats on society and individualprivacy. We scrutinize five major threats: prompt injection, jailbreaking,Personal Identifiable Information (PII) exposure, sexually explicit content,and hate based content, going beyond mere identification to assess theircritical ethical consequences and the urgency they create for robust defensivestrategies. The escalating reliance on LLMs underscores the crucial need forensuring these systems operate within the bounds of ethical norms, particularlyas their misuse can lead to significant societal and individual harm. Wepropose conceptualizing and developing an evaluative tool tailored for LLMs,which would serve a dual purpose, guiding developers and designers inpreemptive fortification of backend systems and scrutinizing the ethicaldimensions of LLM chatbot responses during the testing phase. By comparing LLMresponses with those expected from humans in a moral context, we aim to discernthe degree to which AI behaviors align with the ethical values held by abroader society. Ultimately, this paper not only underscores the ethicaltroubles presented by LLMs, it also highlights a path toward cultivating trustin these systems.\rTransformers with Attentive Federated Aggregation for Time Series Stock Forecasting\nChu Myaet Thwal Ye Lin Tun Kitae Kim Seong-Bae Park Choong Seon Hong\nabstract\rabstract: Recent innovations in transformers have shown their superior performance innatural language processing (NLP) and computer vision (CV). The ability tocapture long-range dependencies and interactions in sequential data has alsotriggered a great interest in time series modeling, leading to the widespreaduse of transformers in many time series applications. However, being the mostcommon and crucial application, the adaptation of transformers to time seriesforecasting has remained limited, with both promising and inconsistent results.In contrast to the challenges in NLP and CV, time series problems not only addthe complexity of order or temporal dependence among input sequences but alsoconsider trend, level, and seasonality information that much of this data isvaluable for decision making. The conventional training scheme has showndeficiencies regarding model overfitting, data scarcity, and privacy issueswhen working with transformers for a forecasting task. In this work, we proposeattentive federated transformers for time series stock forecasting with betterperformance while preserving the privacy of participating enterprises.Empirical results on various stock data from the Yahoo! Finance websiteindicate the superiority of our proposed scheme in dealing with the abovechallenges and data heterogeneity in federated learning.\r2024-01-21\nInstructional Fingerprinting of Large Language Models\nJiashu Xu Fei Wang Mingyu Derek Ma Pang Wei Koh Chaowei Xiao Muhao Chen\nabstract\rabstract: The exorbitant cost of training Large language models (LLMs) from scratchmakes it essential to fingerprint the models to protect intellectual propertyvia ownership authentication and to ensure downstream users and developerscomply with their license terms (e.g. restricting commercial use). In thisstudy, we present a pilot study on LLM fingerprinting as a form of verylightweight instruction tuning. Model publisher specifies a confidentialprivate key and implants it as an instruction backdoor that causes the LLM togenerate specific text when the key is present. Results on 11 popularly-usedLLMs showed that this approach is lightweight and does not affect the normalbehavior of the model. It also prevents publisher overclaim, maintainsrobustness against fingerprint guessing and parameter-efficient training, andsupports multi-stage fingerprinting akin to MIT License. Code is available inhttps://cnut1648.github.io/Model-Fingerprint/.\r2024-01-20\nDeception and Manipulation in Generative AI\nChristian Tarsney\nabstract\rabstract: Large language models now possess human-level linguistic abilities in manycontexts. This raises the concern that they can be used to deceive andmanipulate on unprecedented scales, for instance spreading politicalmisinformation on social media. In future, agentic AI systems might alsodeceive and manipulate humans for their own ends. In this paper, first, I arguethat AI-generated content should be subject to stricter standards againstdeception and manipulation than we ordinarily apply to humans. Second, I offernew characterizations of AI deception and manipulation meant to support suchstandards, according to which a statement is deceptive (manipulative) if itleads human addressees away from the beliefs (choices) they would endorse under``semi-ideal\u0026rsquo;\u0026rsquo; conditions. Third, I propose two measures to guard against AIdeception and manipulation, inspired by this characterization: \u0026ldquo;extremetransparency\u0026rdquo; requirements for AI-generated content and defensive systems that,among other things, annotate AI-generated statements with contextualizinginformation. Finally, I consider to what extent these measures can protectagainst deceptive behavior in future, agentic AIs, and argue that non-agenticdefensive systems can provide an important layer of defense even against morepowerful agentic systems.\rFwdLLM: Efficient FedLLM using Forward Gradient\nMengwei Xu Dongqi Cai Yaozong Wu Xiang Li Shangguang Wang\nabstract\rabstract: Large Language Models (LLMs) are transforming the landscape of mobileintelligence. Federated Learning (FL), a method to preserve user data privacy,is often employed in fine-tuning LLMs to downstream mobile tasks, an approachknown as FedLLM. Though recent efforts have addressed the network issue inducedby the vast model size, they have not practically mitigated vital challengesconcerning integration with mobile devices, such as significant memoryconsumption and sluggish model convergence. In response to these challenges, this work introduces FwdLLM, an innovativeFL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLMto employ backpropagation (BP)-free training methods, requiring devices only toexecute ``perturbed inferences\u0026rsquo;\u0026rsquo;. Consequently, FwdLLM delivers way bettermemory efficiency and time efficiency (expedited by mobile NPUs and an expandedarray of participant devices). FwdLLM centers around three key designs: (1) itcombines BP-free training with parameter-efficient training methods, anessential way to scale the approach to the LLM era; (2) it systematically andadaptively allocates computational loads across devices, striking a carefulbalance between convergence speed and accuracy; (3) it discriminatively samplesperturbed predictions that are more valuable to model convergence.Comprehensive experiments with five LLMs and three NLP tasks illustrateFwdLLM\u0026rsquo;s significant advantages over conventional methods, including up tothree orders of magnitude faster convergence and a 14.6x reduction in memoryfootprint. Uniquely, FwdLLM paves the way for federated learning ofbillion-parameter LLMs such as LLaMA on COTS mobile devices \u0026ndash; a featpreviously unattained.\r2024-01-18\nCase Study: Securing MMU-less Linux Using CHERI\nHesham Almatary Alfredo Mazzinghi Robert N. M. Watson\nabstract\rabstract: MMU-less Linux variant lacks security because it does not have protection orisolation mechanisms. It also does not use MPUs as they do not fit with itssoftware model because of the design drawbacks of MPUs (\\ie coarse-grainedprotection with fixed number of protected regions). We secure the existingMMU-less Linux version of the RISC-V port using CHERI. CHERI is ahardware-software capability-based system that extends the ISA, toolchain,programming languages, operating systems, and applications in order to providecomplete pointer and memory safety. We believe that CHERI could providesignificant security guarantees for high-end dynamic MMU-less embedded systemsat lower costs, compared to MMUs and MPUs, by: 1) building the entire softwarestack in pure-capability CHERI C mode which provides complete spatial memorysafety at the kernel and user-level, 2) isolating user programs as separateELFs, each with its own CHERI-based capability table; this provides spatialmemory safety similar to what the MMU offers (\\ie user programs cannot accesseach other\u0026rsquo;s memory), 3) isolating user programs from the kernel as the kernelhas its own capability table from the users and vice versa, and 4)compartmentalising kernel modules using CompartOS\u0026rsquo; linkage-basedcompartmentalisation. This offers a new security front that is not possibleusing the current MMU-based Linux, where vulnerable/malicious kernel modules(\\eg device drivers) executing in the kernel space would not compromise or takedown the entire system. These are the four main contributions of this paper,presenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.\rSilent Guardian: Protecting Text from Malicious Exploitation by Large Language Models\nJiawei Zhao Kejiang Chen Xiaojian Yuan Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.\rMeme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations\nPrince Jha Krishanu Maity Raghav Jain Apoorv Verma Sriparna Saha Pushpak Bhattacharyya\nabstract\rabstract: Internet memes have gained significant influence in communicating political,psychological, and sociocultural ideas. While memes are often humorous, therehas been a rise in the use of memes for trolling and cyberbullying. Although awide variety of effective deep learning-based models have been developed fordetecting offensive multimodal memes, only a few works have been done onexplainability aspect. Recent laws like \u0026ldquo;right to explanations\u0026rdquo; of General DataProtection Regulation, have spurred research in developing interpretable modelsrather than only focusing on performance. Motivated by this, we introduce {\\emMultiBully-Ex}, the first benchmark dataset for multimodal explanation fromcode-mixed cyberbullying memes. Here, both visual and textual modalities arehighlighted to explain why a given meme is cyberbullying. A ContrastiveLanguage-Image Pretraining (CLIP) projection-based multimodal shared-privatemultitask approach has been proposed for visual and textual explanation of ameme. Experimental results demonstrate that training with multimodalexplanations improves performance in generating textual justifications and moreaccurately identifying the visual evidence supporting a decision with reliableperformance improvements.\rMISS: A Generative Pretraining and Finetuning Approach for Med-VQA\nJiawei Chen Dingkang Yang Yue Jiang Yuxuan Lei Lihua Zhang\nabstract\rabstract: Medical visual question answering (VQA) is a challenging multimodal task,where Vision-Language Pre-training (VLP) models can effectively improve thegeneralization performance. However, most methods in the medical field treatVQA as an answer classification task which is difficult to transfer topractical application scenarios. Additionally, due to the privacy of medicalimages and the expensive annotation process, large-scale medical image-textpairs datasets for pretraining are severely lacking. In this paper, we proposea large-scale MultI-task Self-Supervised learning based framework (MISS) formedical VQA tasks. Unlike existing methods, we treat medical VQA as agenerative task. We unify the text encoder and multimodal encoder and alignimage-text features through multi-task learning. Furthermore, we propose aTransfer-and-Caption method that extends the feature space of single-modalimage datasets using large language models (LLMs), enabling those traditionalmedical vision field task data to be applied to VLP. Experiments show that ourmethod achieves excellent results with fewer multimodal datasets anddemonstrates the advantages of generative VQA models. The code and modelweights will be released upon the paper\u0026rsquo;s acceptance.\rTowards a Responsible AI Metrics Catalogue: A Collection of Metrics for AI Accountability\nBoming Xia Qinghua Lu Liming Zhu Sung Une Lee Yue Liu Zhenchang Xing\nabstract\rabstract: Artificial Intelligence (AI), particularly through the advent of large-scalegenerative AI (GenAI) models such as Large Language Models (LLMs), has become atransformative element in contemporary technology. While these models haveunlocked new possibilities, they simultaneously present significant challenges,such as concerns over data privacy and the propensity to generate misleading orfabricated content. Current frameworks for Responsible AI (RAI) often fallshort in providing the granular guidance necessary for tangible application,especially for Accountability-a principle that is pivotal for ensuringtransparent and auditable decision-making, bolstering public trust, and meetingincreasing regulatory expectations. This study bridges the accountability gapby introducing our effort towards a comprehensive metrics catalogue, formulatedthrough a systematic multivocal literature review (MLR) that integratesfindings from both academic and grey literature. Our catalogue delineatesprocess metrics that underpin procedural integrity, resource metrics thatprovide necessary tools and frameworks, and product metrics that reflect theoutputs of AI systems. This tripartite framework is designed to operationalizeAccountability in AI, with a special emphasis on addressing the intricacies ofGenAI.\r2024-01-17\nCharacterizing Online Eating Disorder Communities with Large Language Models\nMinh Duc Chu Aryan Karnati Zihao He Kristina Lerman\nabstract\rabstract: The rise in eating disorders, a dangerous mental health condition with highmortality and morbidity, has been linked to the proliferation of idealized bodyimages on social media. However, the link between social media and eatingdisorders is far more complex. We argue that social media platforms create afeedback loop that amplifies the growth of content and communities that promoteeating disorders like anorexia and bulimia. Specifically, social mediaplatforms make it easy for vulnerable individuals to find and connect tolike-minded others, while group dynamic processes encourage them to stayengaged within communities that promote and glorify harmful behaviors linked toeating disorders. We characterize this dynamic empirically through acombination of network and language analysis. We describe a novel frameworkthat leverages large language models to analyze the discourse within onlinecommunities and probe their attitudes on topics related to eating disorders toidentify potentially harmful content. Our work emphasizes the need for bettersocial media moderation to disrupt harmful feedback loops and protectvulnerable individuals.\rThe Mikado Filesystem: An experimental RPC filesystem running over gRPC\nJohn D. Dougrez-Lewis\nabstract\rabstract: Computer applications seeking to persist files remotely across the Internetare faced with a bewildering choice of mechanisms which tend to boil down tomonolithic proprietary closed-source Vendor solutions. We introduce The MikadoFilesystem (mikfs), which provides an open simple lightweight interoperableportable extensible remote filesystem that is open source. mikfs consists ofclient applications accessing remote servers via RPC running over TCP/IPconnections. mikfs is defined as a concrete set of API method calls over gRPCexpressed in Google\u0026rsquo;s Protocol Buffers\u0026rsquo; IDL. gRPC supports a wide variety ofprogramming languages \u0026amp; platforms. For a given language + platform, the gRPCtoolset can generate client- \u0026amp; server-side stubs from the IDL callable fromclient \u0026amp; server code in the selected languages, e.g., a client written in C# orjava running on a Windows PC can access a server written in C++ running onLinux. mikfs consists of a virtual hierarchical tree of files \u0026amp; directories.This logical filesystem is not constrained to the limits and file namingconventions of the host\u0026rsquo;s own physical native filesystem. API methods areprovided for authentication; for atomic file-level operations on files \u0026amp;directories; for clients to register to receive notifications of file \u0026amp;directory changes on a server. The public API allows developers to write theirown new servers and clients; allowing migration of hosted files betweendifferent implementations; extension with new methods \u0026amp; features; is OpenSource code available for inspection and adaptation. gRPC provides secureauthenticated connection \u0026amp; communication over HTTP/2; End-to-End Privacy \u0026amp;Security against eavesdropping of data in transit; support for multiplealternate user login mechanisms. mikfs is provided as source code, \u0026lsquo;TheBootstrap Distribution\u0026rsquo;, consisting of an ecosystem of clients, servers, toolsand utilities.\rExplain Thyself Bully: Sentiment Aided Cyberbullying Detection with Explanation\nKrishanu Maity Prince Jha Raghav Jain Sriparna Saha Pushpak Bhattacharyya\nabstract\rabstract: Cyberbullying has become a big issue with the popularity of different socialmedia networks and online communication apps. While plenty of research is goingon to develop better models for cyberbullying detection in monolinguallanguage, there is very little research on the code-mixed languages andexplainability aspect of cyberbullying. Recent laws like \u0026ldquo;right toexplanations\u0026rdquo; of General Data Protection Regulation, have spurred research indeveloping interpretable models rather than focusing on performance. Motivatedby this we develop the first interpretable multi-task model called {\\em mExCB}for automatic cyberbullying detection from code-mixed languages which cansimultaneously solve several tasks, cyberbullying detection,explanation/rationale identification, target group detection and sentimentanalysis. We have introduced {\\em BullyExplain}, the first benchmark datasetfor explainable cyberbullying detection in code-mixed language. Each post in{\\em BullyExplain} dataset is annotated with four labels, i.e., {\\em bullylabel, sentiment label, target and rationales (explainability)}, i.e., whichphrases are being responsible for annotating the post as a bully. The proposedmultitask framework (mExCB) based on CNN and GRU with word and sub-sentence(SS) level attention is able to outperform several baselines and state of theart models when applied on {\\em BullyExplain} dataset.\rMobileAgent: enhancing mobile control via human-machine interaction and SOP integration\nTinghe Ding\nabstract\rabstract: Agents centered around Large Language Models (LLMs) are now capable ofautomating mobile device operations for users. After fine-tuning to learn auser\u0026rsquo;s mobile operations, these agents can adhere to high-level userinstructions online. They execute tasks such as goal decomposition, sequencingof sub-goals, and interactive environmental exploration, until the finalobjective is achieved. However, privacy concerns related to personalized userdata arise during mobile operations, requiring user confirmation. Moreover,users\u0026rsquo; real-world operations are exploratory, with action data being complexand redundant, posing challenges for agent learning. To address these issues,in our practical application, we have designed interactive tasks between agentsand humans to identify sensitive information and align with personalized userneeds. Additionally, we integrated Standard Operating Procedure (SOP)information within the model\u0026rsquo;s in-context learning to enhance the agent\u0026rsquo;scomprehension of complex task execution. Our approach is evaluated on the newdevice control benchmark AitW, which encompasses 30K unique instructions acrossmulti-step tasks, including application operation, web searching, and webshopping. Experimental results show that the SOP-based agent achievesstate-of-the-art performance in LLMs without incurring additional inferencecosts, boasting an overall action success rate of 66.92%. The code and dataexamples are available at https://github.com/alipay/mobile-agent.\rEfficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR\nJunwen Bai Bo Li Qiujia Li Tara N. Sainath Trevor Strohman\nabstract\rabstract: The end-to-end ASR model is often desired in the streaming multilingualscenario since it is easier to deploy and can benefit from pre-trained speechmodels such as powerful foundation models. Meanwhile, the heterogeneous natureand imbalanced data abundance of different languages may cause performancedegradation, leading to asynchronous peak performance for different languagesduring training, especially on tail ones. Sometimes even the data itself maybecome unavailable as a result of the enhanced privacy protection. Existingwork tend to significantly increase the model size or learn language-specificdecoders to accommodate each language separately. In this study, we exploresimple yet effective Language-Dependent Adapter (LDA) finetuning under acascaded Conformer transducer framework enhanced by teacher pseudo-labeling fortail languages in the streaming multilingual ASR. The adapter only accounts for0.4% of the full model per language. It is plugged into the frozen foundationmodel and is the only trainable module during the finetuning process with noisystudent training. The final model merges the adapter parameters from differentcheckpoints for different languages. The model performance is validated on achallenging multilingual dictation dataset, which includes 39 tail languagesacross Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word errorrate reduction on average and up to 37.5% on a single locale. Furthermore, weshow that our parameter-efficient LDA can match the quality of the full modelfinetuning, thus greatly alleviating the asynchronous peak performance issue.\rHasTEE+ : Confidential Cloud Computing and Analytics with Haskell\nAbhiroop Sarkar Alejandro Russo\nabstract\rabstract: Confidential computing is a security paradigm that enables the protection ofconfidential code and data in a co-tenanted cloud deployment using specializedhardware isolation units called Trusted Execution Environments (TEEs). Byintegrating TEEs with a Remote Attestation protocol, confidential computingallows a third party to establish the integrity of an \\textit{enclave} hostedwithin an untrusted cloud. However, TEE solutions, such as Intel SGX and ARMTrustZone, offer low-level C/C++-based toolchains that are susceptible toinherent memory safety vulnerabilities and lack language constructs to monitorexplicit and implicit information-flow leaks. Moreover, the toolchains involvecomplex multi-project hierarchies and the deployment of hand-writtenattestation protocols for verifying \\textit{enclave} integrity. We address the above with HasTEE+, a domain-specific language (DSL) embeddedin Haskell that enables programming TEEs in a high-level language with strongtype-safety. HasTEE+ assists in multi-tier cloud application development by (1)introducing a \\textit{tierless} programming model for expressing distributedclient-server interactions as a single program, (2) integrating a generalremote-attestation architecture that removes the necessity to writeapplication-specific cross-cutting attestation code, and (3) employing adynamic information flow control mechanism to prevent explicit as well asimplicit data leaks. We demonstrate the practicality of HasTEE+ through a casestudy on confidential data analytics, presenting a data-sharing patternapplicable to mutually distrustful participants and providing overallperformance metrics.\r2024-01-16\nFederated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls\nSaurav Prakash Jin Sima Chao Pan Eli Chien Olgica Milenkovic\nabstract\rabstract: Hierarchical and tree-like data sets arise in many applications, includinglanguage processing, graph data mining, phylogeny and genomics. It is knownthat tree-like data cannot be embedded into Euclidean spaces of finitedimension with small distortion. This problem can be mitigated through the useof hyperbolic spaces. When such data also has to be processed in a distributedand privatized setting, it becomes necessary to work with new federatedlearning methods tailored to hyperbolic spaces. As an initial step towards thedevelopment of the field of federated learning in hyperbolic spaces, we proposethe first known approach to federated classification in hyperbolic spaces. Ourcontributions are as follows. First, we develop distributed versions of convexSVM classifiers for Poincar'e discs. In this setting, the information conveyedfrom clients to the global classifier are convex hulls of clusters present inindividual client data. Second, to avoid label switching issues, we introduce anumber-theoretic approach for label recovery based on the so-called integer$B_h$ sequences. Third, we compute the complexity of the convex hulls inhyperbolic spaces to assess the extent of data leakage; at the same time, inorder to limit communication cost for the hulls, we propose a new quantizationmethod for the Poincar'e disc coupled with Reed-Solomon-like encoding. Fourth,at the server level, we introduce a new approach for aggregating convex hullsof the clients based on balanced graph partitioning. We test our method on acollection of diverse data sets, including hierarchical single-cell RNA-seqdata from different patients distributed across different repositories thathave stringent privacy constraints. The classification accuracy of our methodis up to $\\sim 11%$ better than its Euclidean counterpart, demonstrating theimportance of privacy-preserving learning in hyperbolic spaces.\rTopic Modelling: Going Beyond Token Outputs\nLowri Williams Eirini Anthi Laura Arman Pete Burnap\nabstract\rabstract: Topic modelling is a text mining technique for identifying salient themesfrom a number of documents. The output is commonly a set of topics consistingof isolated tokens that often co-occur in such documents. Manual effort isoften associated with interpreting a topic\u0026rsquo;s description from such tokens.However, from a human\u0026rsquo;s perspective, such outputs may not adequately provideenough information to infer the meaning of the topics; thus, theirinterpretability is often inaccurately understood. Although several studieshave attempted to automatically extend topic descriptions as a means ofenhancing the interpretation of topic models, they rely on external languagesources that may become unavailable, must be kept up-to-date to generaterelevant results, and present privacy issues when training on or processingdata. This paper presents a novel approach towards extending the output oftraditional topic modelling methods beyond a list of isolated tokens. Thisapproach removes the dependence on external sources by using the textual dataitself by extracting high-scoring keywords and mapping them to the topicmodel\u0026rsquo;s token outputs. To measure the interpretability of the proposed outputsagainst those of the traditional topic modelling approach, independentannotators manually scored each output based on their quality and usefulness,as well as the efficiency of the annotation task. The proposed approachdemonstrated higher quality and usefulness, as well as higher efficiency in theannotation task, in comparison to the outputs of a traditional topic modellingmethod, demonstrating an increase in their interpretability.\r2024-01-15\nThe Effect of Human v/s Synthetic Test Data and Round-tripping on Assessment of Sentiment Analysis Systems for Bias\nKausik Lakkaraju Aniket Gupta Biplav Srivastava Marco Valtorta Dezhi Wu\nabstract\rabstract: Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence(AI) systems that output polarity and emotional intensity when given a piece oftext as input. Like other AIs, SASs are also known to have unstable behaviorwhen subjected to changes in data which can make it problematic to trust out ofconcerns like bias when AI works with humans and data has protected attributeslike gender, race, and age. Recently, an approach was introduced to assess SASsin a blackbox setting without training data or code, and rating them for biasusing synthetic English data. We augment it by introducing two human-generatedchatbot datasets and also consider a round-trip setting of translating the datafrom one language to the same through an intermediate language. We find thatthese settings show SASs performance in a more realistic light. Specifically,we find that rating SASs on the chatbot data showed more bias compared to thesynthetic data, and round-tripping using Spanish and Danish as intermediatelanguages reduces the bias (up to 68% reduction) in human-generated data while,in synthetic data, it takes a surprising turn by increasing the bias! Ourfindings will help researchers and practitioners refine their SAS testingstrategies and foster trust as SASs are considered part of moremission-critical applications for global use.\rCLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification\nZeyu Li Jingsheng Gao Tong Yu Suncheng Xiang Jiacheng Ruan Ting Liu Yuzhuo Fu\nabstract\rabstract: Existing research on audio classification faces challenges in recognizingattributes of passive underwater vessel scenarios and lacks well-annotateddatasets due to data privacy concerns. In this study, we introduce CLAPP(Contrastive Language-Audio Pre-training in Passive Underwater VesselClassification), a novel model. Our aim is to train a neural network using awide range of vessel audio and vessel state text pairs obtained from anoceanship dataset. CLAPP is capable of directly learning from raw vessel audiodata and, when available, from carefully curated labels, enabling improvedrecognition of vessel attributes in passive underwater vessel scenarios.Model\u0026rsquo;s zero-shot capability allows predicting the most relevant vessel statedescription for a given vessel audio, without directly optimizing for the task.Our approach aims to solve 2 challenges: vessel audio-text classification andpassive underwater vessel audio attribute recognition. The proposed methodachieves new state-of-the-art results on both Deepship and Shipsear publicdatasets, with a notable margin of about 7%-13% for accuracy compared to priormethods on zero-shot task.\r2024-01-14\nProtected edge modes based on the bulk and boundary renormalization group: A relationship between duality and generalized symmetry\nYoshiki Fukusumi\nabstract\rabstract: We propose a theoretical formulation of protected edge modes in the languageof quantum field theories based on the contemporary understanding of therenormalization group. We use bulk and boundary renormalization arguments whichhave never captured enough attention in condensed matter physics and relatedfields. We revisit various exotic bulk and boundary phenomena in contemporaryphysics, and one can see the conciseness of our formulations. Moreover, in thesystems with open boundaries in general space-time dimensions, we also analyzetheir implications under general duality implemented by the shift of defectscorresponding to generalized symmetries, including higher-form, non-invertiblesymmetries, in principle. Our formulation opens up a new paradigm to explorethe systems with protected edge modes in the established language of therenormalization group.\r2024-01-13\nExploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability\nJunxi Chen Junhao Dong Xiaohua Xie\nabstract\rabstract: Recently, many studies utilized adversarial examples (AEs) to raise the costof malicious image editing and copyright violation powered by latent diffusionmodels (LDMs). Despite their successes, a few have studied the surrogate modelthey used to generate AEs. In this paper, from the perspective of adversarialtransferability, we investigate how the surrogate model\u0026rsquo;s property influencesthe performance of AEs for LDMs. Specifically, we view the time-step samplingin the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogatemodels. We find that the smoothness of surrogate models at different time stepsdiffers, and we substantially improve the performance of the MC-based AEs byselecting smoother surrogate models. In the light of the theoretical frameworkon adversarial transferability in image classification, we also conduct atheoretical analysis to explain why smooth surrogate models can also boost AEsfor LDMs.\r2024-01-12\nSpeaker anonymization using neural audio codec language models\nMichele Panariello Francesco Nespoli Massimiliano Todisco Nicholas Evans\nabstract\rabstract: The vast majority of approaches to speaker anonymization involve theextraction of fundamental frequency estimates, linguistic features and aspeaker embedding which is perturbed to obfuscate the speaker identity beforean anonymized speech waveform is resynthesized using a vocoder. Recent work hasshown that x-vector transformations are difficult to control consistently:other sources of speaker information contained within fundamental frequency andlinguistic features are re-entangled upon vocoding, meaning that anonymizedspeech signals still contain speaker information. We propose an approach basedupon neural audio codecs (NACs), which are known to generate high-qualitysynthetic speech when combined with language models. NACs use quantized codes,which are known to effectively bottleneck speaker-related information: wedemonstrate the potential of speaker anonymization systems based on NAClanguage modeling by applying the evaluation framework of the Voice PrivacyChallenge 2022.\rBusiness and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems\nRebekah Rousi Hooman Samani Niko Mäkitalo Ville Vakkuri Simo Linkola Kai-Kristian Kemell Paulius Daubaris Ilenia Fronza Tommi Mikkonen Pekka Abrahamsson\nabstract\rabstract: Business and technology are intricately connected through logic and design.They are equally sensitive to societal changes and may be devastated byscandal. Cooperative multi-robot systems (MRSs) are on the rise, allowingrobots of different types and brands to work together in diverse contexts.Generative artificial intelligence has been a dominant topic in recentartificial intelligence (AI) discussions due to its capacity to mimic humansthrough the use of natural language and the production of media, including deepfakes. In this article, we focus specifically on the conversational aspects ofgenerative AI, and hence use the term Conversational Generative artificialintelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizingprocesses across sectors and transforming the way humans conduct business. Froma business perspective, cooperative MRSs alone, with potential conflicts ofinterest, privacy practices, and safety concerns, require ethical examination.MRSs empowered by CGIs demand multi-dimensional and sophisticated methods touncover imminent ethical pitfalls. This study focuses on ethics inCGI-empowered MRSs while reporting the stages of developing the MORUL model.\rGreening Large Language Models of Code\nJieke Shi Zhou Yang Hong Jin Kang Bowen Xu Junda He David Lo\nabstract\rabstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers\u0026rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\\times$less), carbon footprint (up to 157$\\times$ less), and inference latency (up to76$\\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).\r2024-01-11\nTOFU: A Task of Fictitious Unlearning for LLMs\nPratyush Maini Zhili Feng Avi Schwarzschild Zachary C. Lipton J. Zico Kolter\nabstract\rabstract: Large language models trained on massive corpora of data from the web canmemorize and reproduce sensitive or private data raising both legal and ethicalconcerns. Unlearning, or tuning models to forget information present in theirtraining data, provides us with a way to protect private data after training.Although several methods exist for such unlearning, it is unclear to whatextent they result in models equivalent to those where the data to be forgottenwas never learned in the first place. To address this challenge, we presentTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepenour understanding of unlearning. We offer a dataset of 200 diverse syntheticauthor profiles, each consisting of 20 question-answer pairs, and a subset ofthese profiles called the forget set that serves as the target for unlearning.We compile a suite of metrics that work together to provide a holistic pictureof unlearning efficacy. Finally, we provide a set of baseline results fromexisting unlearning algorithms. Importantly, none of the baselines we considershow effective unlearning motivating continued efforts to develop approachesfor unlearning that effectively tune models so that they truly behave as ifthey were never trained on the forget data at all.\rBeyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning\nJianwei Li Sheng Liu Qi Lei\nabstract\rabstract: Federated learning (FL) emphasizes decentralized training by storing datalocally and sending only model updates, underlining user privacy. Recently, aline of works on privacy attacks impairs user privacy by extracting sensitivetraining text from language models in the context of FL. Yet, these attacktechniques face distinct hurdles: some work chiefly with limited batch sizes(e.g., batch size of 1), and others are easily detectable. This paperintroduces an innovative approach that is challenging to detect, significantlyenhancing the recovery rate of text in various batch-size settings. Building onfundamental gradient matching and domain prior knowledge, we enhance the attackby recovering the input of the Pooler layer of language models, which enablesus to provide additional supervised signals at the feature level. Unlikegradient data, these signals do not average across sentences and tokens,thereby offering more nuanced and effective insights. We benchmark our methodusing text classification tasks on datasets such as CoLA, SST-2, and RottenTomatoes. Across different batch sizes and models, our approach consistentlyoutperforms previous state-of-the-art results.\rNavigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The advent of Generative AI has marked a significant milestone in artificialintelligence, demonstrating remarkable capabilities in generating realisticimages, texts, and data patterns. However, these advancements come withheightened concerns over data privacy and copyright infringement, primarily dueto the reliance on vast datasets for model training. Traditional approacheslike differential privacy, machine unlearning, and data poisoning only offerfragmented solutions to these complex issues. Our paper delves into themultifaceted challenges of privacy and copyright protection within the datalifecycle. We advocate for integrated approaches that combines technicalinnovation with ethical foresight, holistically addressing these concerns byinvestigating and devising solutions that are informed by the lifecycleperspective. This work aims to catalyze a broader discussion and inspireconcerted efforts towards data privacy and copyright integrity in GenerativeAI.\rHarnessing large-language models to generate private synthetic text\nAlexey Kurakin Natalia Ponomareva Umar Syed Liam MacDermed Andreas Terzis\nabstract\rabstract: Differentially private training algorithms like DP-SGD protect sensitivetraining data by ensuring that trained models do not reveal privateinformation. An alternative approach, which this paper studies, is to use asensitive dataset to generate synthetic data that is differentially privatewith respect to the original data, and then non-privately training a model onthe synthetic data. Doing so has several advantages: synthetic data can bereused for other tasks (including for hyper parameter tuning), retainedindefinitely, and shared with third parties without sacrificing privacy.However, generating private synthetic data is much harder than training aprivate model. To improve performance on text data, recent work has utilizedpublic data by starting with a pre-trained generative language model andprivately fine-tuning it on sensitive data. This model can be used to sample aDP synthetic dataset. While this strategy seems straightforward, executing ithas proven problematic. Previous approaches either show significant performanceloss, or have, as we show, critical design flaws. In this paper we demonstratethat a proper training objective along with tuning fewer parameters results inexcellent DP synthetic data quality. Our approach is competitive with directDP-training of downstream classifiers in terms of performance on downstreamtasks. Further, we demonstrate that our DP synthetic data is not only usefulfor downstream classifier training, but also to tune those same models.\r2024-01-09\nAuditing and Generating Synthetic Data with Controllable Trust Trade-offs\nBrian Belgodere Pierre Dognin Adam Ivankay Igor Melnyk Youssef Mroueh Aleksandra Mojsilovic Jiri Navratil Apoorva Nitsure Inkit Padhi Mattia Rigotti Jerret Ross Yair Schiff Radhika Vedpathak Richard A. Young\nabstract\rabstract: Real-world data often exhibits bias, imbalance, and privacy risks. Syntheticdatasets have emerged to address these issues. This paradigm relies ongenerative AI models to generate unbiased, privacy-preserving data whilemaintaining fidelity to the original data. However, assessing thetrustworthiness of synthetic datasets and models is a critical challenge. Weintroduce a holistic auditing framework that comprehensively evaluatessynthetic datasets and AI models. It focuses on preventing bias anddiscrimination, ensures fidelity to the source data, assesses utility,robustness, and privacy preservation. We demonstrate the framework\u0026rsquo;seffectiveness by auditing various generative models across diverse use caseslike education, healthcare, banking, and human resources, spanning differentdata modalities such as tabular, time-series, vision, and natural language.This holistic assessment is essential for compliance with regulatorysafeguards. We introduce a trustworthiness index to rank synthetic datasetsbased on their safeguards trade-offs. Furthermore, we present atrustworthiness-driven model selection and cross-validation process duringtraining, exemplified with \u0026ldquo;TrustFormers\u0026rdquo; across various data types. Thisapproach allows for controllable trustworthiness trade-offs in synthetic datacreation. Our auditing framework fosters collaboration among stakeholders,including data scientists, governance experts, internal reviewers, externalcertifiers, and regulators. This transparent reporting should become a standardpractice to prevent bias, discrimination, and privacy violations, ensuringcompliance with policies and providing accountability, safety, and performanceguarantees.\rPrivate Fine-tuning of Large Language Models with Zeroth-order Optimization\nXinyu Tang Ashwinee Panda Milad Nasr Saeed Mahloujifar Prateek Mittal\nabstract\rabstract: Fine-tuning large pretrained models on private datasets may run the risk ofviolating privacy. Differential privacy is a framework for mitigating privacyrisks by enforcing algorithmic stability. DP-SGD enables training models withprivate data in a privacy-preserving manner, but raises new obstacles in theform of performance loss and significant engineering challenges. We introduceDP-ZO, a new method for fine-tuning large language models that preserves theprivacy of training data by privatizing zeroth-order optimization. A keyinsight into the design of our method is that the direction of the gradient inSPSA, the zeroth-order algorithm we use, is always random and the onlyinformation that depends on private data is the step size, i.e., a scalar.Therefore, we only need to privatize the scalar step size, which ismemory-efficient. DP-ZO, which can be instantiated with either Laplace orGaussian noise, provides a strong privacy-utility trade-off across differenttasks, and model sizes, under conservative privacy budgets. One noteworthyresult is that DP-ZO exhibits just $1.86%$ performance degradation due toprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samplesfrom SQuAD.\r2024-01-08\nSynthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models\nAldo Gael Carranza Rezsa Farahani Natalia Ponomareva Alex Kurakin Matthew Jagielski Milad Nasr\nabstract\rabstract: We address the challenge of ensuring differential privacy (DP) guarantees intraining deep retrieval systems. Training these systems often involves the useof contrastive-style losses, which are typically non-per-example decomposable,making them difficult to directly DP-train with since common techniques requireper-example gradient. To address this issue, we propose an approach thatprioritizes ensuring query privacy prior to training a deep retrieval system.Our method employs DP language models (LMs) to generate private syntheticqueries representative of the original data. These synthetic queries can beused in downstream retrieval system training without compromising privacy. Ourapproach demonstrates a significant enhancement in retrieval quality comparedto direct DP-training, all while maintaining query-level privacy guarantees.This work highlights the potential of harnessing LMs to overcome limitations instandard DP-training methods.\r2024-01-07\nThe Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\nHaonan Wang Qianli Shen Yao Tong Yang Zhang Kenji Kawaguchi\nabstract\rabstract: The commercialization of diffusion models, renowned for their ability togenerate high-quality images that are often indistinguishable from real ones,brings forth potential copyright concerns. Although attempts have been made toimpede unauthorized access to copyrighted material during training and tosubsequently prevent DMs from generating copyrighted images, the effectivenessof these solutions remains unverified. This study explores the vulnerabilitiesassociated with copyright protection in DMs by introducing a backdoor datapoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.Our attack method operates without requiring access to or control over thediffusion model\u0026rsquo;s training or fine-tuning processes; it merely involves theinsertion of poisoning data into the clean training dataset. This data,comprising poisoning images equipped with prompts, is generated by leveragingthe powerful capabilities of multimodal large language models and text-guidedimage inpainting techniques. Our experimental results and analysis confirm themethod\u0026rsquo;s effectiveness. By integrating a minor portion ofnon-copyright-infringing stealthy poisoning data into the cleandataset-rendering it free from suspicion-we can prompt the finetuned diffusionmodels to produce copyrighted content when activated by specific triggerprompts. These findings underline potential pitfalls in the prevailingcopyright protection strategies and underscore the necessity for increasedscrutiny and preventative measures against the misuse of DMs.\r2024-01-06\nMalla: Demystifying Real-world Large Language Model Integrated Malicious Services\nZilong Lin Jian Cui Xiaojing Liao XiaoFeng Wang\nabstract\rabstract: The underground exploitation of large language models (LLMs) for maliciousservices (i.e., Malla) is witnessing an uptick, amplifying the cyber threatlandscape and posing questions about the trustworthiness of LLM technologies.However, there has been little effort to understand this new cybercrime, interms of its magnitude, impact, and techniques. In this paper, we conduct thefirst systematic study on 212 real-world Mallas, uncovering their proliferationin underground marketplaces and exposing their operational modalities. Ourstudy discloses the Malla ecosystem, revealing its significant growth andimpact on today\u0026rsquo;s public LLM services. Through examining 212 Mallas, weuncovered eight backend LLMs used by Mallas, along with 182 prompts thatcircumvent the protective measures of public LLM APIs. We further demystify thetactics employed by Mallas, including the abuse of uncensored LLMs and theexploitation of public LLM APIs through jailbreak prompts. Our findings enablea better understanding of the real-world exploitation of LLMs bycybercriminals, offering insights into strategies to counteract thiscybercrime.\rExploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review\nLuoma Ke Song Tong Peng Cheng Kaiping Peng\nabstract\rabstract: This paper explores the frontiers of large language models (LLMs) inpsychology applications. Psychology has undergone several theoretical changes,and the current use of Artificial Intelligence (AI) and Machine Learning,particularly LLMs, promises to open up new research directions. We provide adetailed exploration of how LLMs like ChatGPT are transforming psychologicalresearch. It discusses the impact of LLMs across various branches ofpsychology, including cognitive and behavioral, clinical and counseling,educational and developmental, and social and cultural psychology, highlightingtheir potential to simulate aspects of human cognition and behavior. The paperdelves into the capabilities of these models to emulate human-like textgeneration, offering innovative tools for literature review, hypothesisgeneration, experimental design, experimental subjects, data analysis, academicwriting, and peer review in psychology. While LLMs are essential in advancingresearch methodologies in psychology, the paper also cautions about theirtechnical and ethical challenges. There are issues like data privacy, theethical implications of using LLMs in psychological research, and the need fora deeper understanding of these models\u0026rsquo; limitations. Researchers shouldresponsibly use LLMs in psychological studies, adhering to ethical standardsand considering the potential consequences of deploying these technologies insensitive areas. Overall, the article provides a comprehensive overview of thecurrent state of LLMs in psychology, exploring potential benefits andchallenges. It serves as a call to action for researchers to leverage LLMs\u0026rsquo;advantages responsibly while addressing associated risks.\rSecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models\nJinglong Luo Yehong Zhang Jiaqi Zhang Xin Mu Hui Wang Yue Yu Zenglin Xu\nabstract\rabstract: With the growing use of large language models hosted on cloud platforms tooffer inference services, privacy concerns are escalating, especiallyconcerning sensitive data like investment plans and bank account details.Secure Multi-Party Computing (SMPC) emerges as a promising solution to protectthe privacy of inference data and model parameters. However, the application ofSMPC in Privacy-Preserving Inference (PPI) for large language models,particularly those based on the Transformer architecture, often leads toconsiderable slowdowns or declines in performance. This is largely due to themultitude of nonlinear operations in the Transformer architecture, which arenot well-suited to SMPC and difficult to circumvent or optimize effectively. Toaddress this concern, we introduce an advanced optimization framework calledSecFormer, to achieve fast and accurate PPI for Transformer models. Byimplementing model design optimization, we successfully eliminate the high-costexponential and maximum operations in PPI without sacrificing modelperformance. Additionally, we have developed a suite of efficient SMPCprotocols that utilize segmented polynomials, Fourier series and Goldschmidt\u0026rsquo;smethod to handle other complex nonlinear functions within PPI, such as GeLU,LayerNorm, and Softmax. Our extensive experiments reveal that SecFormeroutperforms MPCFormer in performance, showing improvements of $5.6%$ and$24.2%$ for BERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, respectively. Interms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma forBERT${\\text{BASE}}$ and BERT${\\text{LARGE}}$, demonstrating its effectivenessand speed.\r2024-01-04\nRecommendations for public action towards sustainable generative AI systems\nThomas Le Goff\nabstract\rabstract: Growing awareness of the environmental impact of digital technologies has ledto several isolated initiatives to promote sustainable practices. However,despite these efforts, the environmental footprint of generative AI,particularly in terms of greenhouse gas emissions and water consumption,remains considerable. This contribution first presents the components of thisenvironmental footprint, highlighting the massive CO2 emissions and waterconsumption associated with training large language models, thus underliningthe need to rethink learning and inference methods. The paper also explores thefactors and characteristics of models that have an influence on theirenvironmental footprint and demonstrates the existence of solutions to reduceit, such as using more efficient processors or optimising the energyperformance of data centres. The potentially harmful effects of AI on theplanet and its ecosystem have made environmental protection one of the foundingprinciples of AI ethics at international and European levels. However, thisrecognition has not yet translated into concrete measures to address it.Toaddress this issue, our contribution puts forward twelve pragmaticrecommendations for public action to promote sustainable generative AI, inparticular by building a long-term strategy to achieve carbon neutrality for AImodels, encouraging international cooperation to set common standards,supporting scientific research and developing appropriate legal and regulatoryframeworks.This paper seeks to inform the members of the InterministerialCommittee on Generative AI about the environmental challenges of thistechnology by providing a brief review of the scientific literature on thesubject and proposing concrete recommendations of public policy actions toreconcile technological innovation with the need to protect our environment.\r2024-01-03\nMULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative Intelligence Framework\nMaximilian T. Fischer Yannick Metz Lucas Joos Matthias Miller Daniel A. Keim\nabstract\rabstract: AI-driven models are increasingly deployed in operational analyticssolutions, for instance, in investigative journalism or the intelligencecommunity. Current approaches face two primary challenges: ethical and privacyconcerns, as well as difficulties in efficiently combining heterogeneous datasources for multimodal analytics. To tackle the challenge of multimodalanalytics, we present MULTI-CASE, a holistic visual analytics frameworktailored towards ethics-aware and multimodal intelligence exploration, designedin collaboration with domain experts. It leverages an equal joint agencybetween human and AI to explore and assess heterogeneous information spaces,checking and balancing automation through Visual Analytics. MULTI-CASE operateson a fully-integrated data model and features type-specific analysis withmultiple linked components, including a combined search, annotated text view,and graph-based analysis. Parts of the underlying entity detection are based ona RoBERTa-based language model, which we tailored towards user requirementsthrough fine-tuning. An overarching knowledge exploration graph combines allinformation streams, provides in-situ explanations, transparent sourceattribution, and facilitates effective exploration. To assess our approach, weconducted a comprehensive set of evaluations: We benchmarked the underlyinglanguage model on relevant NER tasks, achieving state-of-the-art performance.The demonstrator was assessed according to intelligence capability assessments,while the methodology was evaluated according to ethics design guidelines. As acase study, we present our framework in an investigative journalism setting,supporting war crime investigations. Finally, we conduct a formative userevaluation with domain experts in law enforcement. Our evaluations confirm thatour framework facilitates human agency and steering in security-sensitiveapplications.\rPredicting challenge moments from students\u0026rsquo; discourse: A comparison of GPT-4 to two traditional natural language processing approaches\nWannapon Suraworachet Jennifer Seon Mutlu Cukurova\nabstract\rabstract: Effective collaboration requires groups to strategically regulate themselvesto overcome challenges. Research has shown that groups may fail to regulate dueto differences in members\u0026rsquo; perceptions of challenges which may benefit fromexternal support. In this study, we investigated the potential of leveragingthree distinct natural language processing models: an expert knowledgerule-based model, a supervised machine learning (ML) model and a Large Languagemodel (LLM), in challenge detection and challenge dimension identification(cognitive, metacognitive, emotional and technical/other challenges) fromstudent discourse, was investigated. The results show that the supervised MLand the LLM approaches performed considerably well in both tasks, in contrastto the rule-based approach, whose efficacy heavily relies on the engineeredfeatures by experts. The paper provides an extensive discussion of the threeapproaches\u0026rsquo; performance for automated detection and support of students\u0026rsquo;challenge moments in collaborative learning activities. It argues that,although LLMs provide many advantages, they are unlikely to be the panacea toissues of the detection and feedback provision of socially shared regulation oflearning due to their lack of reliability, as well as issues of validityevaluation, privacy and confabulation. We conclude the paper with a discussionon additional considerations, including model transparency to explore feasibleand meaningful analytical feedback for students and educators using LLMs.\rDB-GPT: Empowering Database Interactions with Private Large Language Models\nSiqiao Xue Caigao Jiang Wenhui Shi Fangyin Cheng Keting Chen Hongjun Yang Zhiping Zhang Jianshan He Hongyang Zhang Ganglin Wei Wang Zhao Fan Zhou Danrui Qi Hong Yi Shaodong Liu Faqiang Chen\nabstract\rabstract: The recent breakthroughs in large language models (LLMs) are positioned totransition many areas of software. Database technologies particularly have animportant entanglement with LLMs as efficient and intuitive databaseinteractions are paramount. In this paper, we present DB-GPT, a revolutionaryand production-ready project that integrates LLMs with traditional databasesystems to enhance user experience and accessibility. DB-GPT is designed tounderstand natural language queries, provide context-aware responses, andgenerate complex SQL queries with high accuracy, making it an indispensabletool for users ranging from novice to expert. The core innovation in DB-GPTlies in its private LLM technology, which is fine-tuned on domain-specificcorpora to maintain user privacy and ensure data security while offering thebenefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, whichincludes a novel retrieval augmented generation (RAG) knowledge system, anadaptive learning mechanism to continuously improve performance based on userfeedback and a service-oriented multi-model framework (SMMF) with powerfuldata-driven agents. Our extensive experiments and user studies confirm thatDB-GPT represents a paradigm shift in database interactions, offering a morenatural, efficient, and secure way to engage with data repositories. The paperconcludes with a discussion of the implications of DB-GPT framework on thefuture of human-database interaction and outlines potential avenues for furtherenhancements and applications in the field. The project code is available athttps://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself byinstalling it with the instructionshttps://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minutevideo at https://www.youtube.com/watch?v=KYs4nTDzEhk.\r2024-01-02\nA Reliable Knowledge Processing Framework for Combustion Science using Foundation Models\nVansh Sharma Venkat Raman\nabstract\rabstract: This research explores the integration of large language models (LLMs) intoscientific data assimilation, focusing on combustion science as a case study.Leveraging foundational models integrated with Retrieval-Augmented Generation(RAG) framework, the study introduces an approach to process diverse combustionresearch data, spanning experimental studies, simulations, and literature. Themultifaceted nature of combustion research emphasizes the critical role ofknowledge processing in navigating and extracting valuable information from avast and diverse pool of sources. The developed approach minimizescomputational and economic expenses while optimizing data privacy and accuracy.It incorporates prompt engineering and offline open-source LLMs, offering userautonomy in selecting base models. The study provides a thorough examination oftext segmentation strategies, conducts comparative studies between LLMs, andexplores various optimized prompts to demonstrate the effectiveness of theframework. By incorporating an external database, the framework outperforms aconventional LLM in generating accurate responses and constructing robustarguments. Additionally, the study delves into the investigation of optimizedprompt templates for the purpose of efficient extraction of scientificliterature. The research addresses concerns related to hallucinations and falseresearch articles by introducing a custom workflow developed with a detectionalgorithm to filter out inaccuracies. Despite identified areas for improvement,the framework consistently delivers accurate domain-specific responses withminimal human oversight. The prompt-agnostic approach introduced holds promisefor future deliberations. The study underscores the significance of integratingLLMs and knowledge processing techniques in scientific research, providing afoundation for advancements in data assimilation and utilization.\rSafety and Performance, Why Not Both? Bi-Objective Optimized Model Compression against Heterogeneous Attacks Toward AI Software Deployment\nJie Zhu Leye Wang Xiao Han Anmin Liu Tao Xie\nabstract\rabstract: The size of deep learning models in artificial intelligence (AI) software isincreasing rapidly, hindering the large-scale deployment on resource-restricteddevices (e.g., smartphones). To mitigate this issue, AI software compressionplays a crucial role, which aims to compress model size while keeping highperformance. However, the intrinsic defects in a big model may be inherited bythe compressed one. Such defects may be easily leveraged by adversaries, sincea compressed model is usually deployed in a large number of devices withoutadequate protection. In this article, we aim to address the safe modelcompression problem from the perspective of safety-performance co-optimization.Specifically, inspired by the test-driven development (TDD) paradigm insoftware engineering, we propose a test-driven sparse training framework calledSafeCompress. By simulating the attack mechanism as safety testing,SafeCompress can automatically compress a big model to a small one followingthe dynamic sparse training paradigm. Then, considering two kinds ofrepresentative and heterogeneous attack mechanisms, i.e., black-box membershipinference attack and white-box membership inference attack, we develop twoconcrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, weimplement another instance called MMIA-SafeCompress by extending SafeCompressto defend against the occasion when adversaries conduct black-box and white-boxmembership inference attacks simultaneously. We conduct extensive experimentson five datasets for both computer vision and natural language processingtasks. The results show the effectiveness and generalizability of ourframework. We also discuss how to adapt SafeCompress to other attacks besidesmembership inference attack, demonstrating the flexibility of SafeCompress.\r2024-01-01\nTaking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education\nArne Bewersdorff Christian Hartmann Marie Hornberger Kathrin Seßler Maria Bannert Enkelejda Kasneci Gjergji Kasneci Xiaoming Zhai Claudia Nerdel\nabstract\rabstract: The integration of Artificial Intelligence (AI), particularly Large LanguageModel (LLM)-based systems, in education has shown promise in enhancing teachingand learning experiences. However, the advent of Multimodal Large LanguageModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processingmultimodal data including text, sound, and visual inputs, opens a new era ofenriched, personalized, and interactive learning landscapes in education.Grounded in theory of multimedia learning, this paper explores thetransformative role of MLLMs in central aspects of science education bypresenting exemplary innovative learning scenarios. Possible applications forMLLMs could range from content creation to tailored support for learning,fostering competencies in scientific practices, and providing assessment andfeedback. These scenarios are not limited to text-based and uni-modal formatsbut can be multimodal, increasing thus personalization, accessibility, andpotential learning effectiveness. Besides many opportunities, challenges suchas data protection and ethical considerations become more salient, calling forrobust frameworks to ensure responsible integration. This paper underscores thenecessity for a balanced approach in implementing MLLMs, where the technologycomplements rather than supplants the educator\u0026rsquo;s role, ensuring thus aneffective and ethical use of AI in science education. It calls for furtherresearch to explore the nuanced implications of MLLMs on the evolving role ofeducators and to extend the discourse beyond science education to otherdisciplines. Through the exploration of potentials, challenges, and futureimplications, we aim to contribute to a preliminary understanding of thetransformative trajectory of MLLMs in science education and beyond.\rMachine Learning for Synthetic Data Generation: A Review\nYingzhou Lu Minjie Shen Huazheng Wang Xiao Wang Capucine van Rechem Wenqi Wei\nabstract\rabstract: Machine learning heavily relies on data, but real-world applications oftenencounter various data-related issues. These include data of poor quality,insufficient data points leading to under-fitting of machine learning models,and difficulties in data access due to concerns surrounding privacy, safety,and regulations. In light of these challenges, the concept of synthetic datageneration emerges as a promising alternative that allows for data sharing andutilization in ways that real-world data cannot facilitate. This paper presentsa comprehensive systematic review of existing studies that employ machinelearning models for the purpose of generating synthetic data. The reviewencompasses various perspectives, starting with the applications of syntheticdata generation, spanning computer vision, speech, natural language processing,healthcare, and business domains. Additionally, it explores different machinelearning methods, with particular emphasis on neural network architectures anddeep generative models. The paper also addresses the crucial aspects of privacyand fairness concerns related to synthetic data generation. Furthermore, thisstudy identifies the challenges and opportunities prevalent in this emergingfield, shedding light on the potential avenues for future research. By delvinginto the intricacies of synthetic data generation, this paper aims tocontribute to the advancement of knowledge and inspire further exploration insynthetic data generation.\rDigger: Detecting Copyright Content Mis-usage in Large Language Model Training\nHaodong Li Gelei Deng Yi Liu Kailong Wang Yuekang Li Tianwei Zhang Yang Liu Guoai Xu Guosheng Xu Haoyu Wang\nabstract\rabstract: Pre-training, which utilizes extensive and varied datasets, is a criticalfactor in the success of Large Language Models (LLMs) across numerousapplications. However, the detailed makeup of these datasets is often notdisclosed, leading to concerns about data security and potential misuse. Thisis particularly relevant when copyrighted material, still under legalprotection, is used inappropriately, either intentionally or unintentionally,infringing on the rights of the authors. In this paper, we introduce a detailed framework designed to detect andassess the presence of content from potentially copyrighted books within thetraining datasets of LLMs. This framework also provides a confidence estimationfor the likelihood of each content sample\u0026rsquo;s inclusion. To validate ourapproach, we conduct a series of simulated experiments, the results of whichaffirm the framework\u0026rsquo;s effectiveness in identifying and addressing instances ofcontent misuse in LLM training processes. Furthermore, we investigate thepresence of recognizable quotes from famous literary works within thesedatasets. The outcomes of our study have significant implications for ensuringthe ethical use of copyrighted materials in the development of LLMs,highlighting the need for more transparent and responsible data managementpractices in this field.\r2023-12-31\nOpening A Pandora\u0026rsquo;s Box: Things You Should Know in the Era of Custom GPTs\nGuanhong Tao Siyuan Cheng Zhuo Zhang Junmin Zhu Guangyu Shen Xiangyu Zhang\nabstract\rabstract: The emergence of large language models (LLMs) has significantly acceleratedthe development of a wide range of applications across various fields. There isa growing trend in the construction of specialized platforms based on LLMs,such as the newly introduced custom GPTs by OpenAI. While custom GPTs providevarious functionalities like web browsing and code execution, they alsointroduce significant security threats. In this paper, we conduct acomprehensive analysis of the security and privacy issues arising from thecustom GPT platform. Our systematic examination categorizes potential attackscenarios into three threat models based on the role of the malicious actor,and identifies critical data exchange channels in custom GPTs. Utilizing theSTRIDE threat modeling framework, we identify 26 potential attack vectors, with19 being partially or fully validated in real-world settings. Our findingsemphasize the urgent need for robust security and privacy measures in thecustom GPT ecosystem, especially in light of the forthcoming launch of theofficial GPT store by OpenAI.\rViz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\nDipankar Sarkar\nabstract\rabstract: This paper aims to introduce and analyze the Viz system in a comprehensiveway, a novel system architecture that integrates Quantized Low-Rank Adapters(QLoRA) to fine-tune large language models (LLM) within a legally compliant andresource efficient marketplace. Viz represents a significant contribution tothe field of artificial intelligence, particularly in addressing the challengesof computational efficiency, legal compliance, and economic sustainability inthe utilization and monetization of LLMs. The paper delineates the scholarlydiscourse and developments that have informed the creation of Viz, focusingprimarily on the advancements in LLM models, copyright issues in AI training(NYT case, 2023), and the evolution of model fine-tuning techniques,particularly low-rank adapters and quantized low-rank adapters, to create asustainable and economically compliant framework for LLM utilization. Theeconomic model it proposes benefits content creators, AI developers, andend-users, delineating a harmonious integration of technology, economy, andlaw, offering a comprehensive solution to the complex challenges of today\u0026rsquo;s AIlandscape.\r2023-12-30\nEvaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models\nMaximilian Weber Merle Reichardt\nabstract\rabstract: This paper explores the use of open generative Large Language Models (LLMs)for annotation tasks in the social sciences. The study highlights thechallenges associated with proprietary models, such as limited reproducibilityand privacy concerns, and advocates for the adoption of open (source) modelsthat can be operated on independent devices. Two examples of annotation tasks,sentiment analysis in tweets and identification of leisure activities inchildhood aspirational essays are provided. The study evaluates the performanceof different prompting strategies and models (neural-chat-7b-v3-2,Starling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). Theresults indicate the need for careful validation and tailored promptengineering. The study highlights the advantages of open models for dataprivacy and reproducibility.\rWhy is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis\nYuki Yada Tsuneo Matsumoto Fuyuko Kido Hayato Yamana\nabstract\rabstract: Dark patterns are deceptive user interface designs for online services thatmake users behave in unintended ways. Dark patterns, such as privacy invasion,financial loss, and emotional distress, can harm users. These issues have beenthe subject of considerable debate in recent years. In this paper, we studyinterpretable dark pattern auto-detection, that is, why a particular userinterface is detected as having dark patterns. First, we trained a model usingtransformer-based pre-trained language models, BERT, on a text-based datasetfor the automatic detection of dark patterns in e-commerce. Then, we appliedpost-hoc explanation techniques, including local interpretable model agnosticexplanation (LIME) and Shapley additive explanations (SHAP), to the trainedmodel, which revealed which terms influence each prediction as a dark pattern.In addition, we extracted and analyzed terms that affected the dark patterns.Our findings may prevent users from being manipulated by dark patterns, and aidin the construction of more equitable internet services. Our code is availableat https://github.com/yamanalab/why-darkpattern.\rSplit-and-Denoise: Protect large language model inference with local differential privacy\nPeihua Mai Ran Yan Zhe Huang Youjia Yang Yan Pang\nabstract\rabstract: Large Language Models (LLMs) shows powerful capability in natural languageunderstanding by capturing hidden semantics in vector space. This processenriches the value of the text embeddings for various downstream tasks, therebyfostering the Embedding-as-a-Service (EaaS) business model. However, the directtransmission of text to servers poses a largely unaddressed risk of privacyleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), aninnovative framework that split the model to execute the token embedding layeron the client side at minimal computational cost. This allows the client tointroduce noise prior to transmitting the embeddings to the server, andsubsequently receive and denoise the perturbed output embeddings for downstreamtasks. Our approach is designed for the inference stage of LLMs and requires nomodifications to the model parameters. Extensive experiments demonstrate SnD\u0026rsquo;seffectiveness in optimizing the privacy-utility tradeoff across various LLMarchitectures and diverse downstream tasks. The results reveal a significantperformance improvement under the same privacy budget compared to the baseline,offering clients a privacy-preserving solution for local privacy protection.\rTeach Large Language Models to Forget Privacy\nRan Yan Yujun Li Wenqian Li Peihua Mai Yan Pang Yinchuan Li\nabstract\rabstract: Large Language Models (LLMs) have proven powerful, but the risk of privacyleakage remains a significant concern. Traditional privacy-preserving methods,such as Differential Privacy and Homomorphic Encryption, are inadequate forblack-box API-only settings, demanding either model transparency or heavycomputational resources. We propose Prompt2Forget (P2F), the first frameworkdesigned to tackle the LLM local privacy challenge by teaching LLM to forget.The method involves decomposing full questions into smaller segments,generating fabricated answers, and obfuscating the model\u0026rsquo;s memory of theoriginal input. A benchmark dataset was crafted with questions containingprivacy-sensitive information from diverse fields. P2F achieves zero-shotgeneralization, allowing adaptability across a wide range of use cases withoutmanual adjustments. Experimental results indicate P2F\u0026rsquo;s robust capability toobfuscate LLM\u0026rsquo;s memory, attaining a forgetfulness score of around 90% withoutany utility loss. This represents an enhancement of up to 63% when contrastedwith the naive direct instruction technique, highlighting P2F\u0026rsquo;s efficacy inmitigating memory retention of sensitive information within LLMs. Our findingsestablish the first benchmark in the novel field of the LLM forgetting task,representing a meaningful advancement in privacy preservation in the emergingLLM domain.\r2023-12-29\nExploring the language of the sharing economy: Building trust and reducing privacy concern on Airbnb in German and English\nAlex Zarifis Richard Ingham Julia Kroenung\nabstract\rabstract: The text in the profile of those offering their properties in England inEnglish and in Germany in German, are compared to explore whether trust isbuilt, and privacy concerns are reduced in the same way. Six methods ofbuilding trust are used by the landlords: (1) the level of formality, (2)distance and proximity, (3) emotiveness and humor, (4) being assertive andpassive aggressive, (5) conformity to the platform language style andterminology and (6) setting boundaries. Privacy concerns are not usuallyreduced directly as this is left to the platform. The findings indicate thatlanguage has a limited influence and the platform norms and habits are thebiggest influence.\rDifferentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning\nXiao-Yang Liu Rongyi Zhu Daochen Zha Jiechao Gao Shan Zhong Meikang Qiu\nabstract\rabstract: The surge in interest and application of large language models (LLMs) hassparked a drive to fine-tune these models to suit specific applications, suchas finance and medical science. However, concerns regarding data privacy haveemerged, especially when multiple stakeholders aim to collaboratively enhanceLLMs using sensitive data. In this scenario, federated learning becomes anatural choice, allowing decentralized fine-tuning without exposing raw data tocentral servers. Motivated by this, we investigate how data privacy can beensured in LLM fine-tuning through practical federated learning approaches,enabling secure contributions from multiple parties to enhance LLMs. Yet,challenges arise: 1) despite avoiding raw data exposure, there is a risk ofinferring sensitive information from model outputs, and 2) federated learningfor LLMs incurs notable communication overhead. To address these challenges,this article introduces DP-LoRA, a novel federated learning algorithm tailoredfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism thatadds noise in weight updates, maintaining individual data privacy whilefacilitating collaborative model training. Moreover, DP-LoRA optimizescommunication efficiency via low-rank adaptation, minimizing the transmissionof updated weights during distributed training. The experimental results acrossmedical, financial, and general datasets using various LLMs demonstrate thatDP-LoRA effectively ensures strict privacy constraints while minimizingcommunication overhead.\r2023-12-28\nSentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference\nAbhijit Mishra Mingda Li Soham Deo\nabstract\rabstract: This paper addresses the privacy and security concerns associated with deepneural language models, which serve as crucial components in various modernAI-based applications. These models are often used after being pre-trained andfine-tuned for specific tasks, with deployment on servers accessed through theinternet. However, this introduces two fundamental risks: (a) the transmissionof user inputs to the server via the network gives rise to interceptionvulnerabilities, and (b) privacy concerns emerge as organizations that deploysuch models store user data with restricted context. To address this, wepropose a novel method to adapt and fine-tune transformer-based language modelson passkey-encrypted user-specific text. The original pre-trained languagemodel first undergoes a quick adaptation (without any further pre-training)with a series of irreversible transformations applied to the tokenizer andtoken embeddings. This enables the model to perform inference on encryptedinputs while preventing reverse engineering of text from model parameters andintermediate outputs. After adaptation, models are fine-tuned on encryptedversions of existing training datasets. Experimental evaluation employingadapted versions of renowned models (e.g., BERT, RoBERTa) across establishedbenchmark English and multilingual datasets for text classification andsequence labeling shows that encrypted models achieve performance parity withtheir original counterparts. This serves to safeguard performance, privacy, andsecurity cohesively.\r2023-12-27\nSocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models\nManish Nagireddy Lamogha Chiazor Moninder Singh Ioana Baldini\nabstract\rabstract: Current datasets for unwanted social bias auditing are limited to studyingprotected demographic features such as race and gender. In this work, weintroduce a comprehensive benchmark that is meant to capture the amplificationof social bias, via stigmas, in generative language models. Taking inspirationfrom social science research, we start with a documented list of 93 US-centricstigmas and curate a question-answering (QA) dataset which involves simplesocial situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,with a variety of prompt styles, carefully constructed to systematically testfor both social bias and model robustness. We present results forSocialStigmaQA with two open source generative language models and we find thatthe proportion of socially biased output ranges from 45% to 59% across avariety of decoding strategies and prompting styles. We demonstrate that thedeliberate design of the templates in our benchmark (e.g., adding biasing textto the prompt or using different verbs that change the answer that indicatesbias) impacts the model tendencies to generate socially biased output.Additionally, through manual evaluation, we discover problematic patterns inthe generated chain-of-thought output that range from subtle bias to lack ofreasoning. Warning: This paper contains examples of text which are toxic, biased, andpotentially harmful.\rHow to Raise a Robot \u0026ndash; A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots\nNiklas Hemken Florian Jacob Fabian Peller-Konrad Rainer Kartmann Tamim Asfour Hannes Hartenstein\nabstract\rabstract: Humanoid robots will be able to assist humans in their daily life, inparticular due to their versatile action capabilities. However, while theserobots need a certain degree of autonomy to learn and explore, they also shouldrespect various constraints, for access control and beyond. We explore thenovel field of incorporating privacy, security, and access control constraintswith robot task planning approaches. We report preliminary results on theclassical symbolic approach, deep-learned neural networks, and modern ideasusing large language models as knowledge base. From analyzing their trade-offs,we conclude that a hybrid approach is necessary, and thereby present a new usecase for the emerging field of neuro-symbolic artificial intelligence.\r2023-12-26\nCan ChatGPT Read Who You Are?\nErik Derner Dalibor Kučera Nuria Oliver Jan Zahálka\nabstract\rabstract: The interplay between artificial intelligence (AI) and psychology,particularly in personality assessment, represents an important emerging areaof research. Accurate personality trait estimation is crucial not only forenhancing personalization in human-computer interaction but also for a widevariety of applications ranging from mental health to education. This paperanalyzes the capability of a generic chatbot, ChatGPT, to effectively inferpersonality traits from short texts. We report the results of a comprehensiveuser study featuring texts written in Czech by a representative populationsample of 155 participants. Their self-assessments based on the Big FiveInventory (BFI) questionnaire serve as the ground truth. We compare thepersonality trait estimations made by ChatGPT against those by human raters andreport ChatGPT\u0026rsquo;s competitive performance in inferring personality traits fromtext. We also uncover a \u0026lsquo;positivity bias\u0026rsquo; in ChatGPT\u0026rsquo;s assessments across allpersonality dimensions and explore the impact of prompt composition onaccuracy. This work contributes to the understanding of AI capabilities inpsychological assessment, highlighting both the potential and limitations ofusing large language models for personality inference. Our research underscoresthe importance of responsible AI development, considering ethical implicationssuch as privacy, consent, autonomy, and bias in AI applications.\rFedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models\nPanlong Wu Kangshuo Li Ting Wang Fangxin Wang\nabstract\rabstract: Foundation models have shown great success in natural language processing,computer vision, and multimodal tasks. FMs have a large number of modelparameters, thus requiring a substantial amount of data to help optimize themodel during the training. Federated learning has revolutionized machinelearning by enabling collaborative learning from decentralized data while stillpreserving the data privacy of clients. Despite the great benefits foundationmodels can have empowered by federated learning, they face severe computation,communication, and statistical challenges. In this paper, we propose a noveltwo-stage federated learning algorithm called FedMS. A global expert is trainedin the first stage and a local expert is trained in the second stage to providebetter personalization. We construct a Mixture of Foundation Models (MoFM) withthese two experts and design a gate neural network with an inserted gateadapter that joins the aggregation every communication round in the secondstage. To further adapt to edge computing scenarios with limited computationalresources, we design a novel Sparsely Activated LoRA (SAL) algorithm thatfreezes the pre-trained foundation model parameters inserts low-rank adaptationmatrices into transformer blocks and activates them progressively during thetraining. We employ extensive experiments to verify the effectiveness of FedMS,results show that FedMS outperforms other SOTA baselines by up to 55.25% indefault settings.\r2023-12-25\nLarge Language Models Empowered Autonomous Edge AI for Connected Intelligence\nYifei Shen Jiawei Shao Xinjie Zhang Zehong Lin Hao Pan Dongsheng Li Jun Zhang Khaled B. Letaief\nabstract\rabstract: The evolution of wireless networks gravitates towards connected intelligence,a concept that envisions seamless interconnectivity among humans, objects, andintelligence in a hyper-connected cyber-physical world. Edge artificialintelligence (Edge AI) is a promising solution to achieve connectedintelligence by delivering high-quality, low-latency, and privacy-preserving AIservices at the network edge. This article presents a vision of autonomous edgeAI systems that automatically organize, adapt, and optimize themselves to meetusers\u0026rsquo; diverse requirements, leveraging the power of large language models(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting thepowerful abilities of GPT in language understanding, planning, and codegeneration, as well as incorporating classic wisdom such as task-orientedcommunication and edge federated learning, we present a versatile frameworkthat efficiently coordinates edge AI models to cater to users\u0026rsquo; personal demandswhile automatically generating code to train new models in a privacy-preservingmanner. Experimental results demonstrate the system\u0026rsquo;s remarkable ability toaccurately comprehend user demands, efficiently execute AI models with minimalcost, and effectively create high-performance AI models at edge servers.\rGanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification\nHuali Ren Anli Yan Xiaojun Ren Pei-Gen Ye Chong-zhi Gao Zhili Zhou Jin Li\nabstract\rabstract: Deep neural networks (DNNs) are extensively employed in a wide range ofapplication scenarios. Generally, training a commercially viable neural networkrequires significant amounts of data and computing resources, and it is easyfor unauthorized users to use the networks illegally. Therefore, networkownership verification has become one of the most crucial steps in safeguardingdigital assets. To verify the ownership of networks, the existing networkfingerprinting approaches perform poorly in the aspects of efficiency,stealthiness, and discriminability. To address these issues, we propose anetwork fingerprinting approach, named as GanFinger, to construct the networkfingerprints based on the network behavior, which is characterized by networkoutputs of pairs of original examples and conferrable adversarial examples.Specifically, GanFinger leverages Generative Adversarial Networks (GANs) toeffectively generate conferrable adversarial examples with imperceptibleperturbations. These examples can exhibit identical outputs on copyrighted andpirated networks while producing different results on irrelevant networks.Moreover, to enhance the accuracy of fingerprint ownership verification, thenetwork similarity is computed based on the accuracy-robustness distance offingerprint examples\u0026rsquo;outputs. To evaluate the performance of GanFinger, weconstruct a comprehensive benchmark consisting of 186 networks with fivenetwork structures and four popular network post-processing techniques. Thebenchmark experiments demonstrate that GanFinger significantly outperforms thestate-of-the-arts in efficiency, stealthiness, and discriminability. Itachieves a remarkable 6.57 times faster in fingerprint generation and booststhe ARUC value by 0.175, resulting in a relative improvement of about 26%.\rA Split-and-Privatize Framework for Large Language Model Fine-Tuning\nXicong Shen Yang Liu Huiqi Liu Jue Hong Bing Duan Zirui Huang Yunlong Mao Ye Wu Di Wu\nabstract\rabstract: Fine-tuning is a prominent technique to adapt a pre-trained language model todownstream scenarios. In parameter-efficient fine-tuning, only a small subsetof modules are trained over the downstream datasets, while leaving the rest ofthe pre-trained model frozen to save computation resources. In recent years, apopular productization form arises as Model-as-a-Service (MaaS), in whichvendors provide abundant pre-trained language models, server resources and corefunctions, and customers can fine-tune, deploy and invoke their customizedmodel by accessing the one-stop MaaS with their own private dataset. In thispaper, we identify the model and data privacy leakage risks in MaaSfine-tuning, and propose a Split-and-Privatize (SAP) framework, which manage tomitigate the privacy issues by adapting the existing split learningarchitecture. The proposed SAP framework is sufficiently investigated byexperiments, and the results indicate that it can enhance the empirical privacyby 62% at the cost of 1% model performance degradation on the StanfordSentiment Treebank dataset.\r2023-12-22\nUnsupervised Melody-to-Lyric Generation\nYufei Tian Anjali Narayan-Chen Shereen Oraby Alessandra Cervone Gunnar Sigurdsson Chenyang Tao Wenbo Zhao Yiwen Chen Tagyoung Chung Jing Huang Nanyun Peng\nabstract\rabstract: Automatic melody-to-lyric generation is a task in which song lyrics aregenerated to go with a given melody. It is of significant practical interestand more challenging than unconstrained lyric generation as the music imposesadditional constraints onto the lyrics. The training data is limited as mostsongs are copyrighted, resulting in models that underfit the complicatedcross-modal relationship between melody and lyrics. In this work, we propose amethod for generating high-quality lyrics without training on any alignedmelody-lyric data. Specifically, we design a hierarchical lyric generationframework that first generates a song outline and second the complete lyrics.The framework enables disentanglement of training (based purely on text) frominference (melody-guided text generation) to circumvent the shortage ofparallel data. We leverage the segmentation and rhythm alignment between melody and lyricsto compile the given melody into decoding constraints as guidance duringinference. The two-step hierarchical design also enables content control viathe lyric outline, a much-desired feature for democratizing collaborative songcreation. Experimental results show that our model can generate high-qualitylyrics that are more on-topic, singable, intelligible, and coherent than strongbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,with a 24% relative overall quality improvement based on human ratings.\r2023-12-21\nHElium: A Language and Compiler for Fully Homomorphic Encryption with Support for Proxy Re-Encryption\nMirko Günther Lars Schütze Kilian Becher Thorsten Strufe Jeronimo Castrillon\nabstract\rabstract: Privacy-preserving analysis of confidential data can increase the value ofsuch data and even improve peoples\u0026rsquo; lives. Fully homomorphic encryption (FHE)can enable privacy-preserving analysis. However, FHE adds a large amount ofcomputational overhead and its efficient use requires a high level ofexpertise. Compilers can automate certain aspects such as parameterization andcircuit optimizations. This in turn makes FHE accessible to non-cryptographers.Yet, multi-party scenarios remain complicated and exclude many promising usecases such as analyses of large amounts of health records for medical research.Proxy re-encryption (PRE), a technique that allows the conversion of data frommultiple sources to a joint encryption key, can enable FHE for multi-partyscenarios. Today, there are no optimizing compilers for FHE with PREcapabilities. We propose HElium, the first optimizing FHE compiler with native support forproxy re-encryption. HElium features HEDSL, a domain-specific language (DSL)specifically designed for multi-party scenarios. By tracking encryption keysand transforming the computation circuit during compilation, HElium minimizesthe number of expensive PRE operations. We evaluate the effectiveness ofHElium\u0026rsquo;s optimizations based on the real-world use case of the tumor recurrencerate, a well-known subject of medical research. Our empirical evaluation showsthat HElium substantially reduces the overhead introduced through complex PREoperations, an effect that increases for larger amounts of input data.\rDeID-GPT: Zero-shot Medical Text De-Identification by GPT-4\nZhengliang Liu Yue Huang Xiaowei Yu Lu Zhang Zihao Wu Chao Cao Haixing Dai Lin Zhao Yiwei Li Peng Shu Fang Zeng Lichao Sun Wei Liu Dinggang Shen Quanzheng Li Tianming Liu Dajiang Zhu Xiang Li\nabstract\rabstract: The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT\u0026quot;) to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.\rExperimenting with Large Language Models and vector embeddings in NASA SciX\nSergi Blanco-Cuaresma Ioana Ciucă Alberto Accomazzi Michael J. Kurtz Edwin A. Henneken Kelly E. Lockhart Felix Grezes Thomas Allen Golnaz Shapurian Carolyn S. Grant Donna M. Thompson Timothy W. Hostetler Matthew R. Templeton Shinyi Chen Jennifer Koch Taylor Jacovich Daniel Chivvis Fernanda de Macedo Alves Jean-Claude Paquin Jennifer Bartlett Mugdha Polimera Stephanie Jarmak\nabstract\rabstract: Open-source Large Language Models enable projects such as NASA SciX (i.e.,NASA ADS) to think out of the box and try alternative approaches forinformation retrieval and data augmentation, while respecting data copyrightand users\u0026rsquo; privacy. However, when large language models are directly promptedwith questions without any context, they are prone to hallucination. At NASASciX we have developed an experiment where we created semantic vectors for ourlarge collection of abstracts and full-text content, and we designed a promptsystem to ask questions using contextual chunks from our system. Based on anon-systematic human evaluation, the experiment shows a lower degree ofhallucination and better responses when using Retrieval Augmented Generation.Further exploration is required to design new features and data augmentationprocesses at NASA SciX that leverages this technology while respecting the highlevel of trust and quality that the project holds.\rFedJudge: Federated Legal Large Language Model\nLinan Yue Qi Liu Yichao Du Weibo Gao Ye Liu Fangzhou Yao\nabstract\rabstract: Large Language Models (LLMs) have gained prominence in the field of LegalIntelligence, offering potential applications in assisting legal professionalsand laymen. However, the centralized training of these Legal LLMs raises dataprivacy concerns, as legal data is distributed among various institutionscontaining sensitive individual information. This paper addresses thischallenge by exploring the integration of Legal LLMs with Federated Learning(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally ondevices or clients, and their parameters are aggregated and distributed on acentral server, ensuring data privacy without directly sharing raw data.However, computation and communication overheads hinder the full fine-tuning ofLLMs under the FL setting. Moreover, the distribution shift of legal datareduces the effectiveness of FL methods. To this end, in this paper, we proposethe first Federated Legal Large Language Model (FedJudge) framework, whichfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudgeutilizes parameter-efficient fine-tuning methods to update only a fewadditional parameters during the FL training. Besides, we explore the continuallearning methods to preserve the global model\u0026rsquo;s important parameters whentraining local clients to mitigate the problem of data shifts. Extensiveexperimental results on three real-world datasets clearly validate theeffectiveness of FedJudge. Code is released athttps://github.com/yuelinan/FedJudge.\r2023-12-20\nImproving Data Minimization through Decentralized Data Architectures\nIlaria Battiston Peter Boncz\nabstract\rabstract: In this research project, we investigate an alternative to the standardcloud-centralized data architecture. Specifically, we aim to leave part of theapplication data under the control of the individual data owners indecentralized personal data stores. Our primary goal is to increase dataminimization, i. e., enabling more sensitive personal data to be under thecontrol of its owners while providing a straightforward and efficient frameworkto design architectures that allow applications to run and data to be analyzed.To serve this purpose, the centralized part of the schema contains aggregatingviews over this decentralized data. We propose to design a declarative languagethat extends SQL, for architects to specify different kinds of tables and viewsat the schema level, along with sensitive columns and their minimum granularitylevel of their aggregations. Local updates need to be reflected in thecentralized views while ensuring privacy throughout intermediate calculations;for this we pursue the integration of distributed materialized view maintenanceand multi-party computation (MPC) techniques. We finally aim to implement thissystem, where the personal data stores could either live in mobile devices orencrypted cloud storage, in order to evaluate its performance properties.\rMontsalvat: Intel SGX Shielding for GraalVM Native Images\nPeterson Yuhala Jämes Ménétrey Pascal Felber Valerio Schiavoni Alain Tchana Gaël Thomas Hugo Guiroux Jean-Pierre Lozi\nabstract\rabstract: The popularity of the Java programming language has led to its wide adoptionin cloud computing infrastructures. However, Java applications running inuntrusted clouds are vulnerable to various forms of privileged attacks. Theemergence of trusted execution environments (TEEs) such as Intel SGX mitigatesthis problem. TEEs protect code and data in secure enclaves inaccessible tountrusted software, including the kernel and hypervisors. To efficiently useTEEs, developers must manually partition their applications into trusted anduntrusted parts, in order to reduce the size of the trusted computing base(TCB) and minimise the risks of security vulnerabilities. However, partitioningapplications poses two important challenges: (i) ensuring efficient objectcommunication between the partitioned components, and (ii) ensuring theconsistency of garbage collection between the parts, especially withmemory-managed languages such as Java. We present Montsalvat, a tool whichprovides a practical and intuitive annotation-based partitioning approach forJava applications destined for secure enclaves. Montsalvat provides an RMI-likemechanism to ensure inter-object communication, as well as consistent garbagecollection across the partitioned components. We implement Montsalvat withGraalVM native-image, a tool for compiling Java applications ahead-of-time intostandalone native executables that do not require a JVM at runtime. Ourextensive evaluation with micro- and macro-benchmarks shows our partitioningapproach to boost performance in real-world applications\rAll but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models\nSeunghoo Hong Juhun Lee Simon S. Woo\nabstract\rabstract: Text-to-Image models such as Stable Diffusion have shown impressive imagegeneration synthesis, thanks to the utilization of large-scale datasets.However, these datasets may contain sexually explicit, copyrighted, orundesirable content, which allows the model to directly generate them. Giventhat retraining these large models on individual concept deletion requests isinfeasible, fine-tuning algorithms have been developed to tackle concepterasing in diffusion models. While these algorithms yield good concept erasure,they all present one of the following issues: 1) the corrupted feature spaceyields synthesis of disintegrated objects, 2) the initially synthesized contentundergoes a divergence in both spatial structure and semantics in the generatedimages, and 3) sub-optimal training updates heighten the model\u0026rsquo;s susceptibilityto utility harm. These issues severely degrade the original utility ofgenerative models. In this work, we present a new approach that solves all ofthese challenges. We take inspiration from the concept of classifier guidanceand propose a surgical update on the classifier guidance term whileconstraining the drift of the unconditional score term. Furthermore, ouralgorithm empowers the user to select an alternative to the erasing concept,allowing for more controllability. Our experimental results show that ouralgorithm not only erases the target concept effectively but also preserves themodel\u0026rsquo;s generation capability.\r2023-12-19\nImage Captioning with Multi-Context Synthetic Data\nFeipeng Ma Yizhou Zhou Fengyun Rao Yueyi Zhang Xiaoyan Sun\nabstract\rabstract: Image captioning requires numerous annotated image-text pairs, resulting insubstantial annotation costs. Recently, large models (e.g. diffusion models andlarge language models) have excelled in producing high-quality images and text.This potential can be harnessed to create synthetic image-text pairs fortraining captioning models. Synthetic data can improve cost and time efficiencyin data collection, allow for customization to specific domains, bootstrapgeneralization capability for zero-shot performance, and circumvent privacyconcerns associated with real-world data. However, existing methods struggle toattain satisfactory performance solely through synthetic data. We identify theissue as generated images from simple descriptions mostly capture a solitaryperspective with limited context, failing to align with the intricate scenesprevalent in real-world imagery. To tackle this, we present an innovativepipeline that introduces multi-context data generation. Beginning with aninitial text corpus, our approach employs a large language model to extractmultiple sentences portraying the same scene from diverse viewpoints. Thesesentences are then condensed into a single sentence with multiple contexts.Subsequently, we generate intricate images using the condensed captions throughdiffusion models. Our model is exclusively trained on synthetic image-textpairs crafted through this process. The effectiveness of our pipeline isvalidated through experimental results in both the in-domain and cross-domainsettings, where it achieves state-of-the-art performance on well-known datasetssuch as MSCOCO, Flickr30k, and NoCaps.\rLength 3 Check Digit Codes with Grouped Tags and Disjoint Coding Applications\nLarry A. Dunning\nabstract\rabstract: In 1969 J. Verhoeff provided the first examples of a decimal error detectingcode using a single check digit to provide protection against all single,transposition, and adjacent twin errors. The three codes he presented arelength 3-digit codes with 2 information digits. To date, the existence of alength 4-digit code with 3 information digits having these properties remainsan open question. Existence of a 4-digit code would imply the existence of 10disjoint 3-digit codes. Apparently, no pair of such disjoint 3-digit codes isknown. Phonetic errors, where 2-digit pairs of the forms X0 and 1X areinterchanged, are language dependent, but can often be eliminated. Alternate 3-digit codes are developed here which enhance the level ofprotection beyond Verhoeff\u0026rsquo;s codes while still optionally providing protectionagainst phonetic errors. Through almost-disjoint coding schemes, it is shownhow copies of these new codes can fill in the gap between such 3 and 4-digitcodes. The results are extended to other useful alphabet sizes such as 26 and36 with stronger permutation of digits error detection, and to \u0026ldquo;tag codes\u0026quot;where digits are grouped.\rA Performance Evaluation of a Quantized Large Language Model on Various Smartphones\nTolga Çöplü Marc Loedi Arto Bendiken Mykhailo Makohin Joshua J. Bouw Stephen Cobb\nabstract\rabstract: This paper explores the feasibility and performance of on-device largelanguage model (LLM) inference on various Apple iPhone models. Amidst the rapidevolution of generative AI, on-device LLMs offer solutions to privacy,security, and connectivity challenges inherent in cloud-based models.Leveraging existing literature on running multi-billion parameter LLMs onresource-limited devices, our study examines the thermal effects andinteraction speeds of a high-performing LLM across different smartphonegenerations. We present real-world performance results, providing insights intoon-device inference capabilities.\r2023-12-18\nAI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs\nYann Hicke Anmol Agarwal Qianou Ma Paul Denny\nabstract\rabstract: Responding to the thousands of student questions on online QA platforms eachsemester has a considerable human cost, particularly in computing courses withrapidly growing enrollments. To address the challenges of scalable andintelligent question-answering (QA), we introduce an innovative solution thatleverages open-source Large Language Models (LLMs) from the LLaMA-2 family toensure data privacy. Our approach combines augmentation techniques such asretrieval augmented generation (RAG), supervised fine-tuning (SFT), andlearning from human preferences data using Direct Preference Optimization(DPO). Through extensive experimentation on a Piazza dataset from anintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs ofpreference data, we demonstrate a significant 30% improvement in the quality ofanswers, with RAG being a particularly impactful addition. Our contributionsinclude the development of a novel architecture for educational QA, extensiveevaluations of LLM performance utilizing both human assessments and LLM-basedmetrics, and insights into the challenges and future directions of educationaldata processing. This work paves the way for the development of AI-TA, anintelligent QA assistant customizable for courses with an online QA platform\rOpportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview\nLiang Zhang Zhelun Chen\nabstract\rabstract: In recent years, the rapid advancement and impressive capabilities of LargeLanguage Models (LLMs) have been evident across various domains. This paperexplores the application, implications, and potential of LLMs in buildingenergy efficiency and decarbonization studies. The wide-ranging capabilities ofLLMs are examined in the context of the building energy field, includingintelligent control systems, code generation, data infrastructure, knowledgeextraction, and education. Despite the promising potential of LLMs, challengesincluding complex and expensive computation, data privacy, security andcopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.The paper concludes with a call for future research focused on the enhancementof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative researchbetween AI and energy experts.\r2023-12-17\nFedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion\nWei Tang Zhiqian Wu Yixin Cao Yong Liao Pengyuan Zhou\nabstract\rabstract: Knowledge graph completion (KGC) aims to predict missing facts in knowledgegraphs (KGs), which is crucial as modern KGs remain largely incomplete. Whiletraining KGC models on multiple aligned KGs can improve performance, previousmethods that rely on transferring raw data among KGs raise privacy concerns. Toaddress this challenge, we propose a new federated learning framework thatimplicitly aggregates knowledge from multiple KGs without demanding raw dataexchange and entity alignment. We treat each KG as a client that trains a locallanguage model through textbased knowledge representation learning. A centralserver then aggregates the model weights from clients. As natural languageprovides a universal representation, the same knowledge thus has similarsemantic representations across KGs. As such, the aggregated language model canleverage complementary knowledge from multilingual KGs without demanding rawuser data sharing. Extensive experiments on a benchmark dataset demonstratethat our method substantially improves KGC on multilingual KGs, achievingcomparable performance to state-of-the-art alignment-based models withoutrequiring any labeled alignments or raw user data sharing. Our codes will bepublicly available.\r2023-12-15\nVoCopilot: Voice-Activated Tracking of Everyday Interactions\nSheen An Goh Manoj Gulati Ambuj Varshney\nabstract\rabstract: Voice plays an important role in our lives by facilitating communication,conveying emotions, and indicating health. Therefore, tracking vocalinteractions can provide valuable insight into many aspects of our lives. Thispaper presents our ongoing efforts to design a new vocal tracking system wecall VoCopilot. VoCopilot is an end-to-end system centered around anenergy-efficient acoustic hardware and firmware combined with advanced machinelearning models. As a result, VoCopilot is able to continuously trackconversations, record them, transcribe them, and then extract useful insightsfrom them. By utilizing large language models, VoCopilot ensures the user canextract useful insights from recorded interactions without having to learncomplex machine learning techniques. In order to protect the privacy of endusers, VoCopilot uses a novel wake-up mechanism that only records conversationsof end users. Additionally, all the rest of pipeline can be run on a commoditycomputer (Mac Mini M2). In this work, we show the effectiveness of VoCopilot inreal-world environment for two use cases.\rDistilling Large Language Models for Matching Patients to Clinical Trials\nMauro Nievas Aditya Basu Yanshan Wang Hrituraj Singh\nabstract\rabstract: The recent success of large language models (LLMs) has paved the way fortheir adoption in the high-stakes domain of healthcare. Specifically, theapplication of LLMs in patient-trial matching, which involves assessing patienteligibility against clinical trial\u0026rsquo;s nuanced inclusion and exclusion criteria,has shown promise. Recent research has shown that GPT-3.5, a widely recognizedLLM developed by OpenAI, can outperform existing methods with minimal \u0026lsquo;variableengineering\u0026rsquo; by simply comparing clinical trial information against patientsummaries. However, there are significant challenges associated with usingclosed-source proprietary LLMs like GPT-3.5 in practical healthcareapplications, such as cost, privacy and reproducibility concerns. To addressthese issues, this study presents the first systematic examination of theefficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA7B,13B, and 70B) for the task of patient-trial matching. Employing amultifaceted evaluation framework, we conducted extensive automated andhuman-centric assessments coupled with a detailed error analysis for eachmodel. To enhance the adaptability of open-source LLMs, we have created aspecialized synthetic dataset utilizing GPT-4, enabling effective fine-tuningunder constrained data conditions. Our findings reveal that open-source LLMs,when fine-tuned on this limited and synthetic dataset, demonstrate performanceparity with their proprietary counterparts. This presents a massive opportunityfor their deployment in real-world healthcare applications. To foster furtherresearch and applications in this field, we release both the annotatedevaluation dataset along with the fine-tuned LLM \u0026ndash; Trial-LLAMA \u0026ndash; for publicuse.\rPrivacy-Aware Document Visual Question Answering\nRubèn Tito Khanh Nguyen Marlon Tobaben Raouf Kerkouche Mohamed Ali Souibgui Kangsoo Jung Lei Kang Ernest Valveny Antti Honkela Mario Fritz Dimosthenis Karatzas\nabstract\rabstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.\rLLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers\nXuanqi Liu Zhuotao Liu\nabstract\rabstract: The community explored to build private inference frameworks fortransformer-based large language models (LLMs) in a server-client setting,where the server holds the model parameters and the client inputs its privatedata (or prompt) for inference. However, these frameworks impose significantoverhead when the private inputs are forward propagated through the originalLLMs. In this paper, we show that substituting the computation- andcommunication-heavy operators in the transformer architecture withprivacy-computing friendly approximations can greatly reduce the privateinference costs while incurring very minor impact on model performance.Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computingfriendly model inference pipeline achieves a $5\\times$ acceleration incomputation and an 80% reduction in communication overhead, while retainingnearly identical accuracy.\r2023-12-14\nChatSOS: LLM-based knowledge Q\u0026amp;A system for safety engineering\nHaiyang Tang Zhenyi Liu Dongping Chen Qingzhao Chu\nabstract\rabstract: Recent advancements in large language models (LLMs) have notably propellednatural language processing (NLP) capabilities, demonstrating significantpotential in safety engineering applications. Despite these advancements, LLMsface constraints in processing specialized tasks, attributed to factors such ascorpus size, input processing limitations, and privacy concerns. Obtaininguseful information from reliable sources in a limited time is crucial for LLM.Addressing this, our study introduces an LLM-based Q\u0026amp;A system for safetyengineering, enhancing the comprehension and response accuracy of the model. Weemployed prompt engineering to incorporate external knowledge databases, thusenriching the LLM with up-to-date and reliable information. The system analyzeshistorical incident reports through statistical methods, utilizes vectorembedding to construct a vector database, and offers an efficientsimilarity-based search functionality. Our findings indicate that theintegration of external knowledge significantly augments the capabilities ofLLM for in-depth problem analysis and autonomous task assignment. Iteffectively summarizes accident reports and provides pertinent recommendations.This integration approach not only expands LLM applications in safetyengineering but also sets a precedent for future developments towardsautomation and intelligent systems.\rRecovering from Privacy-Preserving Masking with Large Language Models\nArpita Vats Zhe Liu Peng Su Debjyoti Paul Yingyi Ma Yutong Pang Zeeshan Ahmed Ozlem Kalinli\nabstract\rabstract: Model adaptation is crucial to handle the discrepancy between proxy trainingdata and actual users data received. To effectively perform adaptation, textualdata of users is typically stored on servers or their local devices, wheredownstream natural language processing (NLP) models can be directly trainedusing such in-domain data. However, this might raise privacy and securityconcerns due to the extra risks of exposing user information to adversaries.Replacing identifying information in textual data with a generic marker hasbeen recently explored. In this work, we leverage large language models (LLMs)to suggest substitutes of masked tokens and have their effectiveness evaluatedon downstream language modeling tasks. Specifically, we propose multiplepre-trained and fine-tuned LLM-based approaches and perform empirical studieson various datasets for the comparison of these methods. Experimental resultsshow that models trained on the obfuscation corpora are able to achievecomparable performance with the ones trained on the original data withoutprivacy-preserving token masking.\r2023-12-13\nScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale\nMarkus Frohmann Carolin Holtermann Shahed Masoudian Anne Lauscher Navid Rekabsaz\nabstract\rabstract: Multi-task learning (MTL) has shown considerable practical benefits,particularly when using pre-trained language models (PLMs). While this iscommonly achieved by simultaneously learning $n$ tasks under a jointoptimization procedure, recent methods such as AdapterFusion structure theproblem into two distinct stages: (i) task learning, where knowledge specificto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)transfer, where this already learned knowledge is leveraged for a target task.This separation of concerns provides numerous benefits, such as promotingreusability, and addressing cases involving data privacy and societal concerns;on the flip side, current two-stage MTL methods come with the cost ofintroducing a substantial number of additional parameters. In this work, weaddress this issue by leveraging the usefulness of linearly scaling the outputrepresentations of source adapters for transfer learning. We introduceScaLearn, a simple and highly parameter-efficient two-stage MTL method thatcapitalizes on the knowledge of the source tasks by learning a minimal set ofscaling parameters that enable effective knowledge transfer to a target task.Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that ourScaLearn, in addition to facilitating the benefits of two-stage MTL,consistently outperforms strong baselines with only a small number of transferparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observethat ScaLearn maintains its strong abilities even when further reducingparameters through uniform scaling and layer-sharing, achieving similarlycompetitive results with only $8$ transfer parameters for each target task. Ourproposed approach thus demonstrates the power of simple scaling as a promisefor more efficient task transfer.\rEfficient Representation of the Activation Space in Deep Neural Networks\nTanya Akumu Celia Cintas Girmaw Abebe Tadesse Adebayo Oshingbesan Skyler Speakman Edward McFowland III\nabstract\rabstract: The representations of the activation space of deep neural networks (DNNs)are widely utilized for tasks like natural language processing, anomalydetection and speech recognition. Due to the diverse nature of these tasks andthe large size of DNNs, an efficient and task-independent representation ofactivations becomes crucial. Empirical p-values have been used to quantify therelative strength of an observed node activation compared to activationscreated by already-known inputs. Nonetheless, keeping raw data for thesecalculations increases memory resource consumption and raises privacy concerns.To this end, we propose a model-agnostic framework for creating representationsof activations in DNNs using node-specific histograms to compute p-values ofobserved activations without retaining already-known inputs. Our proposedapproach demonstrates promising potential when validated with multiple networkarchitectures across various downstream tasks and compared with the kerneldensity estimates and brute-force empirical baselines. In addition, theframework reduces memory usage by 30% with up to 4 times faster p-valuecomputing time while maintaining state of-the-art detection power in downstreamtasks such as the detection of adversarial attacks and synthesized content.Moreover, as we do not persist raw data at inference time, we could potentiallyreduce susceptibility to attacks and privacy issues.\rPUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning\nFlorian Bordes Shashank Shekhar Mark Ibrahim Diane Bouchacourt Pascal Vincent Ari S. Morcos\nabstract\rabstract: Synthetic image datasets offer unmatched advantages for designing andevaluating deep neural networks: they make it possible to (i) render as manydata samples as needed, (ii) precisely control each scene and yield granularground truth labels (and captions), (iii) precisely control distribution shiftsbetween training and testing to isolate variables of interest for soundexperimentation. Despite such promise, the use of synthetic image data is stilllimited \u0026ndash; and often played down \u0026ndash; mainly due to their lack of realism. Mostworks therefore rely on datasets of real images, which have often been scrapedfrom public images on the internet, and may have issues with regards toprivacy, bias, and copyright, while offering little control over how objectsprecisely appear. In this work, we present a path to democratize the use ofphotorealistic synthetic data: we develop a new generation of interactiveenvironments for representation learning research, that offer bothcontrollability and realism. We use the Unreal Engine, a powerful game enginewell known in the entertainment industry, to produce PUG (Photorealistic UnrealGraphics) environments and datasets for representation learning. In this paper,we demonstrate the potential of PUG to enable more rigorous evaluations ofvision models.\r2023-12-12\nEditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection\nXuanyu Zhang Runyi Li Jiwen Yu Youmin Xu Weiqi Li Jian Zhang\nabstract\rabstract: In the era where AI-generated content (AIGC) models can produce stunning andlifelike images, the lingering shadow of unauthorized reproductions andmalicious tampering poses imminent threats to copyright integrity andinformation security. Current image watermarking methods, while widely acceptedfor safeguarding visual content, can only protect copyright and ensuretraceability. They fall short in localizing increasingly realistic imagetampering, potentially leading to trust crises, privacy violations, and legaldisputes. To solve this challenge, we propose an innovative proactive forensicsframework EditGuard, to unify copyright protection and tamper-agnosticlocalization, especially for AIGC-based editing methods. It can offer ameticulous embedding of imperceptible watermarks and precise decoding oftampered areas and copyright information. Leveraging our observed fragility andlocality of image-into-image steganography, the realization of EditGuard can beconverted into a united image-bit steganography issue, thus completelydecoupling the training process from the tampering types. Extensive experimentsdemonstrate that our EditGuard balances the tamper localization accuracy,copyright recovery precision, and generalizability to various AIGC-basedtampering methods, especially for image forgery that is difficult for the nakedeye to detect. The project page is available athttps://xuanyuzhang21.github.io/project/editguard/.\rCode Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models\nSheng Zhang Hui Li\nabstract\rabstract: Code pre-trained language models (CPLMs) have received great attention sincethey can benefit various tasks that facilitate software development andmaintenance. However, CPLMs are trained on massive open-source code, raisingconcerns about potential data infringement. This paper launches the first studyof detecting unauthorized code use in CPLMs, i.e., Code Membership Inference(CMI) task. We design a framework Buzzer for different settings of CMI. Buzzerdeploys several inference techniques, including distilling the target CPLM,ensemble inference, and unimodal and bimodal calibration. Extensive experimentsshow that CMI can be achieved with high accuracy using Buzzer. Hence, Buzzercan serve as a CMI tool and help protect intellectual property rights.\rLanguage-Guided Transformer for Federated Multi-Label Classification\nI-Jieh Liu Ci-Siang Lin Fu-En Yang Yu-Chiang Frank Wang\nabstract\rabstract: Federated Learning (FL) is an emerging paradigm that enables multiple usersto collaboratively train a robust model in a privacy-preserving manner withoutsharing their private data. Most existing approaches of FL only considertraditional single-label image classification, ignoring the impact whentransferring the task to multi-label image classification. Nevertheless, it isstill challenging for FL to deal with user heterogeneity in their local datadistribution in the real-world FL scenario, and this issue becomes even moresevere in multi-label image classification. Inspired by the recent success ofTransformers in centralized settings, we propose a novel FL framework formulti-label classification. Since partial label correlation may be observed bylocal clients during training, direct aggregation of locally updated modelswould not produce satisfactory performances. Thus, we propose a novel FLframework of Language-Guided Transformer (FedLGT) to tackle this challengingtask, which aims to exploit and transfer knowledge across different clients forlearning a robust global model. Through extensive experiments on variousmulti-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT isable to achieve satisfactory performance and outperforms standard FL techniquesunder multi-label FL scenarios. Code is available athttps://github.com/Jack24658735/FedLGT.\rPractical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration\nWenjie Fu Huandong Wang Chen Gao Guanghua Liu Yong Li Tao Jiang\nabstract\rabstract: Membership Inference Attacks (MIA) aim to infer whether a target data recordhas been utilized for model training or not. Prior attempts have quantified theprivacy risks of language models (LMs) via MIAs, but there is still noconsensus on whether existing MIA algorithms can cause remarkable privacyleakage on practical Large Language Models (LLMs). Existing MIAs designed forLMs can be classified into two categories: reference-free and reference-basedattacks. They are both based on the hypothesis that training recordsconsistently strike a higher probability of being sampled. Nevertheless, thishypothesis heavily relies on the overfitting of target models, which will bemitigated by multiple regularization methods and the generalization of LLMs.The reference-based attack seems to achieve promising effectiveness in LLMs,which measures a more reliable membership signal by comparing the probabilitydiscrepancy between the target model and the reference model. However, theperformance of reference-based attack is highly dependent on a referencedataset that closely resembles the training dataset, which is usuallyinaccessible in the practical scenario. Overall, existing MIAs are unable toeffectively unveil privacy leakage over practical fine-tuned LLMs that areoverfitting-free and private. We propose a Membership Inference Attack based onSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, sincememorization in LLMs is inevitable during the training process and occursbefore overfitting, we introduce a more reliable membership signal,probabilistic variation, which is based on memorization rather thanoverfitting. Furthermore, we introduce a self-prompt approach, which constructsthe dataset to fine-tune the reference model by prompting the target LLMitself. In this manner, the adversary can collect a dataset with a similardistribution from public APIs.\r2023-12-11\nPerformance-lossless Black-box Model Watermarking\nNa Zhao Kejiang Chen Weiming Zhang Nenghai Yu\nabstract\rabstract: With the development of deep learning, high-value and high-cost models havebecome valuable assets, and related intellectual property protectiontechnologies have become a hot topic. However, existing model watermarking workin black-box scenarios mainly originates from training-based backdoor methods,which probably degrade original task performance. To address this, we propose abranch backdoor-based model watermarking protocol to protect model intellectualproperty, where a construction based on a message authentication scheme isadopted as the branch indicator. We prove the lossless performance of theprotocol by reduction. Taking the language generation task as an instance, weshow the effectiveness of the proposed protocol.\rMERGE: Fast Private Text Generation\nZi Liang Pinghui Wang Ruofei Zhang Nuo Xu Lifeng Xing Shuo Zhang\nabstract\rabstract: The drastic increase in language models\u0026rsquo; parameters has led to a new trend ofdeploying models in cloud servers, raising growing concerns about privateinference for Transformer-based models. Existing two-party privacy-preservingtechniques, however, only take into account natural language understanding(NLU) scenarios. Private inference in natural language generation (NLG),crucial for applications like translation and code completion, remainsunderexplored.In addition, previous privacy-preserving techniques suffer fromconvergence issues during model training and exhibit poor inference speed whenused with NLG models due to the neglect of time-consuming operations inauto-regressive generations. To address these issues, we propose a fast privatetext generation framework for Transformer-based language models, namelyMERGE.MERGE reuses the output hidden state as the word embedding to bypass theembedding computation and reorganize the linear operations in the Transformermodule to accelerate the forward procedure. Extensive experiments show thatMERGE achieves a 26.5x speedup to the vanilla encrypted model under thesequence length 512, and reduces 80% communication cost, with an up to 10xspeedup to state-of-the-art approximated models.\rInferDPT: Privacy-Preserving Inference for Black-box Large Language Model\nMeng Tong Kejiang Chen Jie Zhang Yuang Qi Weiming Zhang Nenghai Yu\nabstract\rabstract: Large language models (LLMs), like ChatGPT, have greatly simplified textgeneration tasks. However, they have also raised concerns about privacy riskssuch as data leakage and unauthorized data collection. Existing solutions forprivacy-preserving inference face practical challenges related to computationtime and communication costs. In this paper, we propose InferDPT, the firstpractical framework for the privacy-preserving Inference of black-box LLMs,implementing Differential Privacy in Text generation. InferDPT comprises twokey modules: the \u0026ldquo;perturbation module\u0026rdquo; utilizes the exponential mechanism togenerate a perturbed prompt, facilitating privacy-preserving inference withblack-box LLMs, and the \u0026ldquo;extraction module\u0026rdquo;, inspired by knowledge distillationand retrieval-augmented generation, extracts coherent and consistent text fromthe perturbed generation result, ensuring successful text generationcompletion. To address privacy concerns related to previous exponentialmechanisms\u0026rsquo; susceptibility to embedding revision attacks, we introduce RANTEXT,a novel differential privacy mechanism integrated into the perturbation moduleof InferDPT, which introduces the concept of \u0026ldquo;RANdom adjacency\u0026rdquo; for TEXTperturbation within the prompt. Experimental results across three datasetsdemonstrate that the text generation quality of InferDPT is comparable to thatof non-private GPT-4, and RANTEXT surpasses existing state-of-the-artmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy andutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achievesan average privacy protection rate exceeding 90% against embedding revisionattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higherthan that of CUSTEXT+.\r2023-12-10\nECHO: An Automated Contextual Inquiry Framework for Anonymous Qualitative Studies using Conversational Assistants\nRishika Dwaraghanath Rahul Majethia Sanjana Gautam\nabstract\rabstract: Qualitative research studies often employ a contextual inquiry, or a fieldstudy that involves in-depth observation and interviews of a small sample ofstudy participants, in-situ, to gain a robust understanding of the reasons andcircumstances that led to the participant\u0026rsquo;s thoughts, actions, and experiencesregarding the domain of interest. Contextual inquiry, especially in sensitivedata studies, can be a challenging task due to reasons such as participantprivacy, as well as physical constraints such as in-person presence and manualanalysis of the qualitative data gathered. In this work, we discuss Enqu^eteContextuelle Habile Ordinateur (ECHO); a virtual-assistant framework toautomate the erstwhile manual process of conducting contextual inquiries andanalysing the respondents\u0026rsquo; subjective qualitative data. ECHO automates thecontextual inquiry pipeline, while not compromising on privacy preservation orresponse integrity. Its adaptive conversational interface enables respondentsto provide unstructured or semi-structured responses in free-form naturallanguage, allowing researchers to explore larger narratives in participantresponse data. It supports response-driven exploratory questions and automatescoding methodologies for qualitative data, thus enabling the inquirer to divedeeper into correlated questions and to do better cause-effect analysis. Itfocuses on addressing the limitations of manual annotation, bringingstandardisation to free-form text, and eliminating perspective bias amongstdifferent reviewers of subjective responses. A participatory mental healthstudy was conducted on 167 young adults bifurcated into two focus groups; oneof which was administered a conventional contextual inquiry, and the other viaECHO, virtually. ECHO outperformed on participant transparency, response detailand median time required for end-to-end inquiry completion, per participant.\rMutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer\nYongheng Deng Ziqing Qiao Ju Ren Yang Liu Yaoxue Zhang\nabstract\rabstract: While large language models (LLMs) are empowered with broad knowledge, theirtask-specific performance is often suboptimal. It necessitates fine-tuning LLMswith task-specific data, but such data may be inaccessible due to privacyconcerns. In this paper, we propose a novel approach to enhance LLMs withsmaller language models (SLMs) that are trained on clients using their privatetask-specific data. To enable mutual enhancement between LLMs and SLMs, wepropose CrossLM, where the SLMs promote the LLM to generate task-specifichigh-quality data, and both the LLM and SLMs are enhanced with the generateddata. We evaluate CrossLM using publicly accessible language models across arange of benchmark tasks. The results demonstrate that CrossLM significantlyenhances the task-specific performance of SLMs on clients and the LLM on thecloud server simultaneously while preserving the LLM\u0026rsquo;s generalizationcapability.\r2023-12-08\nA Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge Access and Identifying Risks to Workers\nAnna Gausen Bhaskar Mitra Siân Lindley\nabstract\rabstract: Organisations generate vast amounts of information, which has resulted in along-term research effort into knowledge access systems for enterprisesettings. Recent developments in artificial intelligence, in relation to largelanguage models, are poised to have significant impact on knowledge access.This has the potential to shape the workplace and knowledge in new andunanticipated ways. Many risks can arise from the deployment of these types ofAI systems, due to interactions between the technical system and organisationalpower dynamics. This paper presents the Consequence-Mechanism-Risk framework to identifyrisks to workers from AI-mediated enterprise knowledge access systems. We havedrawn on wide-ranging literature detailing risks to workers, and categorisedrisks as being to worker value, power, and wellbeing. The contribution of ourframework is to additionally consider (i) the consequences of these systemsthat are of moral import: commodification, appropriation, concentration ofpower, and marginalisation, and (ii) the mechanisms, which represent how theseconsequences may take effect in the system. The mechanisms are a means ofcontextualising risk within specific system processes, which is critical formitigation. This framework is aimed at helping practitioners involved in thedesign and deployment of AI-mediated knowledge access systems to consider therisks introduced to workers, identify the precise system mechanisms thatintroduce those risks and begin to approach mitigation. Future work could applythis framework to other technological systems to promote the protection ofworkers and other groups.\rTypeFly: Flying Drones with Large Language Model\nGuojun Chen Xiaojing Yu Lin Zhong\nabstract\rabstract: Commanding a drone with a natural language is not only user-friendly but alsoopens the door for emerging language agents to control the drone. Emerginglarge language models (LLMs) provide a previously impossible opportunity toautomatically translate a task description in a natural language to a programthat can be executed by the drone. However, powerful LLMs and their visioncounterparts are limited in three important ways. First, they are onlyavailable as cloud-based services. Sending images to the cloud raises privacyconcerns. Second, they are expensive, costing proportionally to the requestsize. Finally, without expensive fine-tuning, existing LLMs are quite limitedin their capability of writing a program for specialized systems like drones. In this paper, we present a system called TypeFly that tackles the abovethree problems using a combination of edge-based vision intelligence, novelprogramming language design, and prompt engineering. Instead of the familiarPython, TypeFly gets a cloud-based LLM service to write a program in a small,custom language called MiniSpec, based on task and scene descriptions inEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)but also able to consult the LLM during their execution using a special skillcalled query. Using a set of increasingly challenging drone tasks, we show thatdesign choices made by TypeFly can reduce both the cost of LLM service and thetask execution time by more than 2x. More importantly, query and promptengineering techniques contributed by TypeFly significantly improve the chanceof success of complex tasks.\rOn the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook\nMingyuan Fan Chengyu Wang Cen Chen Yang Liu Jun Huang\nabstract\rabstract: Diffusion models and large language models have emerged as leading-edgegenerative models, revolutionizing various aspects of human life. However, thepractical implementations of these models have also exposed inherent risks,bringing to the forefront their evil sides and sparking concerns regardingtheir trustworthiness. Despite the wealth of literature on this subject, acomprehensive survey specifically delving into the intersection of large-scalegenerative models and their trustworthiness remains largely absent. To bridgethis gap, this paper investigates both the long-standing and emerging threatsassociated with these models across four fundamental dimensions: 1) privacy, 2)security, 3) fairness, and 4) responsibility. Based on the investigationresults, we develop an extensive map outlining the trustworthiness of largegenerative models. After that, we provide practical recommendations andpotential research directions for future secure applications equipped withlarge generative models, ultimately promoting the trustworthiness of the modelsand benefiting the society as a whole.\r2023-12-07\nStableQ: Enhancing Data-Scarce Quantization with Text-to-Image Data\nYuhang Li Youngeun Kim Donghyun Lee Priyadarshini Panda\nabstract\rabstract: Though low-bit quantization enables efficient storage and inference of deepneural networks, it often requires the use of training data to maintainresilience against quantization errors. However, training data are frequentlysubject to privacy or copyright concerns. In this work, we address thechallenge of Data-Scarce Quantization, where access to training data isseverely limited or non-existent for quantization purposes. Conventionalapproaches typically rely on inverting dummy images or jointly traininggenerative models to produce synthetic input samples. However, these methodsstruggle to accurately recreate complex objects in large-scale datasets likeImageNet. To overcome these limitations, we introduce StableQ, a novel methodthat utilizes an advanced text-to-image diffusion model to generatehigh-resolution, photo-realistic synthetic data. To verify the quality of thegenerated data, we implement two robust filtering mechanisms. These mechanismsare designed to select images that closely resemble the intrinsiccharacteristics of the actual training data. Furthermore, in scenarios wherelimited training data are available, we use these data to guide the syntheticdata generation process by inverting a learnable token embedding in the textencoder. Our extensive experimental results demonstrate that StbaleQ sets a newbenchmark in both zero-shot and few-shot quantization, outperforming existingmethods in terms of accuracy and efficiency.\rDomain Private Transformers for Multi-Domain Dialog Systems\nAnmol Kabra Ethan R. Elenberg\nabstract\rabstract: Large, general purpose language models have demonstrated impressiveperformance across many different conversational domains. While multi-domainlanguage models achieve low overall perplexity, their outputs are notguaranteed to stay within the domain of a given input prompt. This paperproposes domain privacy as a novel way to quantify how likely a conditionallanguage model will leak across domains. We also develop policy functions basedon token-level domain classification, and propose an efficient fine-tuningmethod to improve the trained model\u0026rsquo;s domain privacy. Experiments on membershipinference attacks show that our proposed method has comparable resiliency tomethods adapted from recent literature on differentially private languagemodels.\rMaking Translators Privacy-aware on the User\u0026rsquo;s Side\nRyoma Sato\nabstract\rabstract: We propose PRISM to enable users of machine translation systems to preservethe privacy of data on their own initiative. There is a growing demand to applymachine translation systems to data that require privacy protection. Whileseveral machine translation engines claim to prioritize privacy, the extent andspecifics of such protection are largely ambiguous. First, there is often alack of clarity on how and to what degree the data is protected. Even ifservice providers believe they have sufficient safeguards in place,sophisticated adversaries might still extract sensitive information. Second,vulnerabilities may exist outside of these protective measures, such as withincommunication channels, potentially leading to data leakage. As a result, usersare hesitant to utilize machine translation engines for data demanding highlevels of privacy protection, thereby missing out on their benefits. PRISMresolves this problem. Instead of relying on the translation service to keepdata safe, PRISM provides the means to protect data on the user\u0026rsquo;s side. Thisapproach ensures that even machine translation engines with inadequate privacymeasures can be used securely. For platforms already equipped with privacysafeguards, PRISM acts as an additional protection layer, reinforcing theirsecurity furthermore. PRISM adds these privacy features without significantlycompromising translation accuracy. Our experiments demonstrate theeffectiveness of PRISM using real-world translators, T5 and ChatGPT(GPT-3.5-turbo), and the datasets with two languages. PRISM effectivelybalances privacy protection with translation accuracy.\rDefense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks\nXiaobei Yan Chip Hong Chang Tianwei Zhang\nabstract\rabstract: Artificial Intelligence (AI) hardware accelerators have been widely adoptedto enhance the efficiency of deep learning applications. However, they alsoraise security concerns regarding their vulnerability to power side-channelattacks (SCA). In these attacks, the adversary exploits unintendedcommunication channels to infer sensitive information processed by theaccelerator, posing significant privacy and copyright risks to the models.Advanced machine learning algorithms are further employed to facilitate theside-channel analysis and exacerbate the privacy issue of AI accelerators.Traditional defense strategies naively inject execution noise to the runtime ofAI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguardFPGA-based AI accelerators and mitigate model extraction threats viapower-based SCAs. The key insight of AIAShield is to leverage the prominentadversarial attack technique from the machine learning community to craftdelicate noise, which can significantly obfuscate the adversary\u0026rsquo;s side-channelobservation while incurring minimal overhead to the execution of the protectedmodel. At the hardware level, we design a new module based on ring oscillatorsto achieve fine-grained noise generation. At the algorithm level, we repurposeNeural Architecture Search to worsen the adversary\u0026rsquo;s extraction results.Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA)demonstrate that AIAShield outperforms existing solutions with excellenttransferability.\r2023-12-06\nUnderstanding (Un)Intended Memorization in Text-to-Image Generative Models\nAli Naseh Jaechul Roh Amir Houmansadr\nabstract\rabstract: Multimodal machine learning, especially text-to-image models like StableDiffusion and DALL-E 3, has gained significance for transforming text intodetailed images. Despite their growing use and remarkable generative capabilities, there is apressing need for a detailed examination of these models\u0026rsquo; behavior,particularly with respect to memorization. Historically, memorization inmachine learning has been context-dependent, with diverse definitions emergingfrom classification tasks to complex models like Large Language Models (LLMs)and Diffusion models. Yet, a definitive concept of memorization that alignswith the intricacies of text-to-image synthesis remains elusive. Thisunderstanding is vital as memorization poses privacy risks yet is essential formeeting user expectations, especially when generating representations ofunderrepresented entities. In this paper, we introduce a specialized definitionof memorization tailored to text-to-image models, categorizing it into threedistinct types according to user expectations. We closely examine the subtledistinctions between intended and unintended memorization, emphasizing theimportance of balancing user privacy with the generative quality of the modeloutputs. Using the Stable Diffusion model, we offer examples to validate ourmemorization definitions and clarify their application.\rStop Hiding The Sharp Knives: The WebAssembly Linux Interface\nArjun Ramesh Tianshu Huang Ben L. Titzer Anthony Rowe\nabstract\rabstract: WebAssembly is gaining popularity as a portable binary format targetable frommany programming languages. With a well-specified low-level virtual instructionset, minimal memory footprint and many high-performance implementations, it hasbeen successfully adopted for lightweight in-process memory sandboxing in manycontexts. Despite these advantages, WebAssembly lacks many standard systeminterfaces, making it difficult to reuse existing applications. This paper proposes WALI: The WebAssembly Linux Interface, a thin layer overLinux\u0026rsquo;s userspace system calls, creating a new class of virtualization whereWebAssembly seamlessly interacts with native processes and the underlyingoperating system. By virtualizing the lowest level of userspace, WALI offersapplication portability with little effort and reuses existing compilerbackends. With WebAssembly\u0026rsquo;s control flow integrity guarantees, these modulesgain an additional level of protection against remote code injection attacks.Furthermore, capability-based APIs can themselves be virtualized andimplemented in terms of WALI, improving reuse and robustness through betterlayering. We present an implementation of WALI in a modern WebAssembly engineand evaluate its performance on a number of applications which we can nowcompile with mostly trivial effort.\r2023-12-05\nDEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models\nXinwei Wu Junzhuo Li Minghui Xu Weilong Dong Shuangzhi Wu Chao Bian Deyi Xiong\nabstract\rabstract: Large language models pretrained on a huge amount of data capture richknowledge and information in the training data. The ability of datamemorization and regurgitation in pretrained language models, revealed inprevious studies, brings the risk of data leakage. In order to effectivelyreduce these risks, we propose a framework DEPN to Detect and Edit PrivacyNeurons in pretrained language models, partially inspired by knowledge neuronsand model editing. In DEPN, we introduce a novel method, termed as privacyneuron detector, to locate neurons associated with private information, andthen edit these detected privacy neurons by setting their activations to zero.Furthermore, we propose a privacy neuron aggregator dememorize privateinformation in a batch processing manner. Experimental results show that ourmethod can significantly and efficiently reduce the exposure of private dataleakage without deteriorating the performance of the model. Additionally, weempirically demonstrate the relationship between model memorization and privacyneurons, from multiple perspectives, including model size, training time,prompts, privacy neuron distribution, illustrating the robustness of ourapproach.\r2023-12-04\nResponsible Task Automation: Empowering Large Language Models as Responsible Task Automators\nZhizheng Zhang Xiaoyi Zhang Wenxuan Xie Yan Lu\nabstract\rabstract: The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users\u0026rsquo; privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios.\rThe Queen\u0026rsquo;s Guard: A Secure Enforcement of Fine-grained Access Control In Distributed Data Analytics Platforms\nFahad Shaon Sazzadur Rahaman Murat Kantarcioglu\nabstract\rabstract: Distributed data analytics platforms (i.e., Apache Spark, Hadoop) providehigh-level APIs to programmatically write analytics tasks that are rundistributedly in multiple computing nodes. The design of these frameworks wasprimarily motivated by performance and usability. Thus, the security takes aback seat. Consequently, they do not inherently support fine-grained accesscontrol or offer any plugin mechanism to enable it, making them risky to beused in multi-tier organizational settings. There have been attempts to build \u0026ldquo;add-on\u0026rdquo; solutions to enable fine-grainedaccess control for distributed data analytics platforms. In this paper, first,we show that straightforward enforcement of ``add-on\u0026rsquo;\u0026rsquo; access control isinsecure under adversarial code execution. Specifically, we show that anattacker can abuse platform-provided APIs to evade access controls withoutleaving any traces. Second, we designed a two-layered (i.e., proactive andreactive) defense system to protect against API abuses. On submission of a usercode, our proactive security layer statically screens it to find potentialattack signatures prior to its execution. The reactive security layer employscode instrumentation-based runtime checks and sandboxed execution to throttleany exploits at runtime. Next, we propose a new fine-grained access controlframework with an enhanced policy language that supports map and filterprimitives. Finally, we build a system named SecureDL with our new accesscontrol framework and defense system on top of Apache Spark, which ensuressecure access control policy enforcement under adversaries capable of executingcode. To the best of our knowledge, this is the first fine-grained attribute-basedaccess control framework for distributed data analytics platforms that issecure against platform API abuse attacks. Performance evaluation showed thatthe overhead due to added security is low.\r2023-12-01\nLinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices\nJunchen Zhao Yurun Song Simeng Liu Ian G. Harris Sangeetha Abdu Jyothi\nabstract\rabstract: Deploying Large Language Models (LLMs) locally on mobile devices presents asignificant challenge due to their extensive memory requirements. In thispaper, we introduce LinguaLinked, a system for decentralized, distributed LLMinference on mobile devices. LinguaLinked enables collaborative execution ofthe inference task across multiple trusted devices. LinguaLinked ensures dataprivacy by processing information locally. LinguaLinked uses three keystrategies. First, an optimized model assignment technique segments LLMs anduses linear optimization to align segments with each device\u0026rsquo;s capabilities.Second, an optimized data transmission mechanism ensures efficient andstructured data flow between model segments while also maintaining theintegrity of the original model structure. Finally, LinguaLinked incorporates aruntime load balancer that actively monitors and redistributes tasks amongmobile devices to prevent bottlenecks, enhancing the system\u0026rsquo;s overallefficiency and responsiveness. We demonstrate that LinguaLinked facilitatesefficient LLM inference while maintaining consistent throughput and minimallatency through extensive testing across various mobile devices, from high-endto low-end Android devices. In our evaluations, compared to the baseline,LinguaLinked achieves an inference performance acceleration of $1.11\\times$ to$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ withmulti-threading. Additionally, runtime load balancing yields an overallinference acceleration of $1.29\\times$ to $1.32\\times$.\r2023-11-30\nLocally Differentially Private Document Generation Using Zero Shot Prompting\nSaiteja Utpala Sara Hooker Pin Yu Chen\nabstract\rabstract: Numerous studies have highlighted the privacy risks associated withpretrained large language models. In contrast, our research offers a uniqueperspective by demonstrating that pretrained large language models caneffectively contribute to privacy preservation. We propose a locallydifferentially private mechanism called DP-Prompt, which leverages the power ofpretrained large language models and zero-shot prompting to counter authorde-anonymization attacks while minimizing the impact on downstream utility.When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),we observe a notable reduction in the success rate of de-anonymization attacks,showing that it surpasses existing approaches by a considerable margin despiteits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt(with ChatGPT) perfectly recovers the clean sentiment F1 score while achievinga 46% reduction in author identification F1 score against static attackers anda 26% reduction against adaptive attackers. We conduct extensive experimentsacross six open-source large language models, ranging up to 7 billionparameters, to analyze various effects of the privacy-utility tradeoff.\rSituating the social issues of image generation models in the model life cycle: a sociotechnical approach\nAmelia Katirai Noa Garcia Kazuki Ide Yuta Nakashima Atsuo Kishimoto\nabstract\rabstract: The race to develop image generation models is intensifying, with a rapidincrease in the number of text-to-image models available. This is coupled withgrowing public awareness of these technologies. Though other generative AImodels\u0026ndash;notably, large language models\u0026ndash;have received recent critical attentionfor the social and other non-technical issues they raise, there has beenrelatively little comparable examination of image generation models. This paperreports on a novel, comprehensive categorization of the social issuesassociated with image generation models. At the intersection of machinelearning and the social sciences, we report the results of a survey of theliterature, identifying seven issue clusters arising from image generationmodels: data issues, intellectual property, bias, privacy, and the impacts onthe informational, cultural, and natural environments. We situate these socialissues in the model life cycle, to aid in considering where potential issuesarise, and mitigation may be needed. We then compare these issue clusters withwhat has been reported for large language models. Ultimately, we argue that therisks posed by image generation models are comparable in severity to the risksposed by large language models, and that the social impact of image generationmodels must be urgently considered.\rCan Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?\nZhengyue Zhao Jinhao Duan Kaidi Xu Chenan Wang Rui Zhangp Zidong Dup Qi Guo Xing Hu\nabstract\rabstract: Stable Diffusion has established itself as a foundation model in generativeAI artistic applications, receiving widespread research and application. Somerecent fine-tuning methods have made it feasible for individuals to implantpersonalized concepts onto the basic Stable Diffusion model with minimalcomputational costs on small datasets. However, these innovations have alsogiven rise to issues like facial privacy forgery and artistic copyrightinfringement. In recent studies, researchers have explored the addition ofimperceptible adversarial perturbations to images to prevent potentialunauthorized exploitation and infringements when personal data is used forfine-tuning Stable Diffusion. Although these studies have demonstrated theability to protect images, it is essential to consider that these methods maynot be entirely applicable in real-world scenarios. In this paper, wesystematically evaluate the use of perturbations to protect images within apractical threat model. The results suggest that these approaches may not besufficient to safeguard image privacy and copyright effectively. Furthermore,we introduce a purification method capable of removing protected perturbationswhile preserving the original image structure to the greatest extent possible.Experiments reveal that Stable Diffusion can effectively learn from purifiedimages over all protective methods.\rTrustMark: Universal Watermarking for Arbitrary Resolution Images\nTu Bui Shruti Agarwal John Collomosse\nabstract\rabstract: Imperceptible digital watermarking is important in copyright protection,misinformation prevention, and responsible generative AI. We propose TrustMark- a GAN-based watermarking method with novel design in architecture andspatio-spectra losses to balance the trade-off between watermarked imagequality with the watermark recovery accuracy. Our model is trained withrobustness in mind, withstanding various in- and out-place perturbations on theencoded image. Additionally, we introduce TrustMark-RM - a watermark removermethod useful for re-watermarking. Our methods achieve state-of-art performanceon 3 benchmarks comprising arbitrary resolution images.\rImproving the Robustness of Transformer-based Large Language Models with Dynamic Attention\nLujia Shen Yuwen Pu Shouling Ji Changjiang Li Xuhong Zhang Chunpeng Ge Ting Wang\nabstract\rabstract: Transformer-based models, such as BERT and GPT, have been widely adopted innatural language processing (NLP) due to their exceptional performance.However, recent studies show their vulnerability to textual adversarial attackswhere the model\u0026rsquo;s output can be misled by intentionally manipulating the textinputs. Despite various methods that have been proposed to enhance the model\u0026rsquo;srobustness and mitigate this vulnerability, many require heavy consumptionresources (e.g., adversarial training) or only provide limited protection(e.g., defensive dropout). In this paper, we propose a novel method calleddynamic attention, tailored for the transformer architecture, to enhance theinherent robustness of the model itself against various adversarial attacks.Our method requires no downstream task knowledge and does not incur additionalcosts. The proposed dynamic attention consists of two modules: (I) attentionrectification, which masks or weakens the attention value of the chosen tokens,and (ii) dynamic modeling, which dynamically builds the set of candidatetokens. Extensive experiments demonstrate that dynamic attention significantlymitigates the impact of adversarial attacks, improving up to 33% betterperformance than previous methods against widely-used adversarial attacks. Themodel-level design of dynamic attention enables it to be easily combined withother defense methods (e.g., adversarial training) to further enhance themodel\u0026rsquo;s robustness. Furthermore, we demonstrate that dynamic attentionpreserves the state-of-the-art robustness space of the original model comparedto other dynamic modeling methods.\r2023-11-29\nThe AutoSPADA Platform: User-Friendly Edge Computing for Distributed Learning and Data Analytics in Connected Vehicles\nAdrian Nilsson Simon Smith Jonas Hagmar Magnus Önnheim Mats Jirstrand\nabstract\rabstract: Contemporary connected vehicles host numerous applications, such asdiagnostics and navigation, and new software is continuously being developed.However, the development process typically requires offline batch processing oflarge data volumes. In an edge computing approach, data analysts and developerscan instead process sensor data directly on computational resources insidevehicles. This enables rapid prototyping to shorten development cycles andreduce the time to create new business values or insights. This paper presentsthe design, implementation, and operation of the AutoSPADA edge computingplatform for distributed data analytics. The platform\u0026rsquo;s design followsscalability, reliability, resource efficiency, privacy, and security principlespromoted through mature and industrially proven technologies. In AutoSPADA,computational tasks are general Python scripts, and we provide a library to,for example, read signals from the vehicle and publish results to the cloud.Hence, users only need Python knowledge to use the platform. Moreover, theplatform is designed to be extended to support additional programminglanguages.\rProbabilistic Copyright Protection Can Fail for Text-to-Image Generative Models\nXiang Li Qianli Shen Kenji Kawaguchi\nabstract\rabstract: The booming use of text-to-image generative models has raised concerns abouttheir high risk of producing copyright-infringing content. While probabilisticcopyright protection methods provide a probabilistic guarantee against suchinfringement, in this paper, we introduce Virtually Assured AmplificationAttack (VA3), a novel online attack framework that exposes the vulnerabilitiesof these protection mechanisms. The proposed framework significantly amplifiesthe probability of generating infringing content on the sustained interactionswith generative models and a lower-bounded success probability of eachengagement. Our theoretical and experimental results demonstrate theeffectiveness of our approach and highlight the potential risk of implementingprobabilistic copyright protection in practical applications of text-to-imagegenerative models. Code is available at https://github.com/South7X/VA3.\rClinical Risk Prediction Using Language Models: Benefits And Considerations\nAngeela Acharya Sulabh Shrestha Anyi Chen Joseph Conte Sanja Avramovic Siddhartha Sikdar Antonios Anastasopoulos Sanmay Das\nabstract\rabstract: The utilization of Electronic Health Records (EHRs) for clinical riskprediction is on the rise. However, strict privacy regulations limit access tocomprehensive health records, making it challenging to apply standard machinelearning algorithms in practical real-world scenarios. Previous research hasaddressed this data limitation by incorporating medical ontologies andemploying transfer learning methods. In this study, we investigate thepotential of leveraging language models (LMs) as a means to incorporatesupplementary domain knowledge for improving the performance of variousEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR datasuch as clinical notes, this study focuses on using textual descriptions withinstructured EHR to make predictions exclusively based on that information. Weextensively compare against previous approaches across various data types andsizes. We find that employing LMs to represent structured EHRs, such asdiagnostic histories, leads to improved or at least comparable performance indiverse risk prediction tasks. Furthermore, LM-based approaches offer numerousadvantages, including few-shot learning, the capability to handle previouslyunseen medical concepts, and adaptability to various medical vocabularies.Nevertheless, we underscore, through various experiments, the importance ofbeing cautious when employing such models, as concerns regarding thereliability of LMs persist.\rIdentifying and Mitigating Vulnerabilities in LLM-Integrated Applications\nFengqing Jiang Zhangchen Xu Luyao Niu Boxin Wang Jinyuan Jia Bo Li Radha Poovendran\nabstract\rabstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users\u0026rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user\u0026rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.\r2023-11-28\nAGI: Artificial General Intelligence for Education\nEhsan Latif Gengchen Mai Matthew Nyaaba Xuansheng Wu Ninghao Liu Guoyu Lu Sheng Li Tianming Liu Xiaoming Zhai\nabstract\rabstract: Artificial general intelligence (AGI) has gained global recognition as afuture technology due to the emergence of breakthrough large language modelsand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventionalAI models, typically designed for a limited range of tasks, demand significantamounts of domain-specific data for training and may not always considerintricate interpersonal dynamics in education. AGI, driven by the recent largepre-trained models, represents a significant leap in the capability of machinesto perform tasks that require human-level intelligence, such as reasoning,problem-solving, decision-making, and even understanding human emotions andsocial interactions. This position paper reviews AGI\u0026rsquo;s key concepts,capabilities, scope, and potential within future education, including achievingfuture educational goals, designing pedagogy and curriculum, and performingassessments. It highlights that AGI can significantly improve intelligenttutoring systems, educational assessment, and evaluation procedures. AGIsystems can adapt to individual student needs, offering tailored learningexperiences. They can also provide comprehensive feedback on studentperformance and dynamically adjust teaching methods based on student progress.The paper emphasizes that AGI\u0026rsquo;s capabilities extend to understanding humanemotions and social interactions, which are critical in educational settings.The paper discusses that ethical issues in education with AGI include databias, fairness, and privacy and emphasizes the need for codes of conduct toensure responsible AGI use in academic settings like homework, teaching, andrecruitment. We also conclude that the development of AGI necessitatesinterdisciplinary collaborations between educators and AI engineers to advanceresearch and application efforts.\rPromptCARE: Prompt Copyright Protection by Watermark Injection and Verification\nHongwei Yao Jian Lou Kui Ren Zhan Qin\nabstract\rabstract: Large language models (LLMs) have witnessed a meteoric rise in popularityamong the general public users over the past few months, facilitating diversedownstream tasks with human-level accuracy and proficiency. Prompts play anessential role in this success, which efficiently adapt pre-trained LLMs totask-specific applications by simply prepending a sequence of tokens to thequery texts. However, designing and selecting an optimal prompt can be bothexpensive and demanding, leading to the emergence of Prompt-as-a-Serviceproviders who profit by providing well-designed prompts for authorized use.With the growing popularity of prompts and their indispensable role inLLM-based services, there is an urgent need to protect the copyright of promptsagainst unauthorized use. In this paper, we propose PromptCARE, the first framework for promptcopyright protection through watermark injection and verification. Promptwatermarking presents unique challenges that render existing watermarkingtechniques developed for model and dataset copyright verification ineffective.PromptCARE overcomes these hurdles by proposing watermark injection andverification schemes tailor-made for prompts and NLP characteristics. Extensiveexperiments on six well-known benchmark datasets, using three prevalentpre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate theeffectiveness, harmlessness, robustness, and stealthiness of PromptCARE.\rDe-identification of clinical free text using natural language processing: A systematic review of current approaches\nAleksandar Kovačević Bojana Bašaragin Nikola Milošević Goran Nenadić\nabstract\rabstract: Background: Electronic health records (EHRs) are a valuable resource fordata-driven medical research. However, the presence of protected healthinformation (PHI) makes EHRs unsuitable to be shared for research purposes.De-identification, i.e. the process of removing PHI is a critical step inmaking EHR data accessible. Natural language processing has repeatedlydemonstrated its feasibility in automating the de-identification process.Objectives: Our study aims to provide systematic evidence on how thede-identification of clinical free text has evolved in the last thirteen years,and to report on the performances and limitations of the currentstate-of-the-art systems. In addition, we aim to identify challenges andpotential research opportunities in this field. Methods: A systematic search inPubMed, Web of Science and the DBLP was conducted for studies published betweenJanuary 2010 and February 2023. Titles and abstracts were examined to identifythe relevant studies. Selected studies were then analysed in-depth, andinformation was collected on de-identification methodologies, data sources, andmeasured performance. Results: A total of 2125 publications were identified forthe title and abstract screening. 69 studies were found to be relevant. Machinelearning (37 studies) and hybrid (26 studies) approaches are predominant, whilesix studies relied only on rules. Majority of the approaches were trained andevaluated on public corpora. The 2014 i2b2/UTHealth corpus is the mostfrequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016CEGS N-GRID (10 studies) corpora.\rPCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Models\nXuefeng Fan Dahao Fu Hangyu Gui Xinpeng Zhang Xiaoyi Zhou\nabstract\rabstract: Deep neural networks (DNNs) have achieved tremendous success in artificialintelligence (AI) fields. However, DNN models can be easily illegally copied,redistributed, or abused by criminals, seriously damaging the interests ofmodel inventors. The copyright protection of DNN models by neural networkwatermarking has been studied, but the establishment of a traceabilitymechanism for determining the authorized users of a leaked model is a newproblem driven by the demand for AI services. Because the existing traceabilitymechanisms are used for models without watermarks, a small number offalse-positives are generated. Existing black-box active protection schemeshave loose authorization control and are vulnerable to forgery attacks.Therefore, based on the idea of black-box neural network watermarking with thevideo framing and image perceptual hash algorithm, a passive copyrightprotection and traceability framework PCPT is proposed that uses an additionalclass of DNN models, improving the existing traceability mechanism that yieldsa small number of false-positives. Based on an authorization control strategyand image perceptual hash algorithm, a DNN model active copyright protectionand traceability framework ACPT is proposed. This framework uses theauthorization control center constructed by the detector and verifier. Thisapproach realizes stricter authorization control, which establishes a strongconnection between users and model owners, improves the framework security, andsupports traceability verification.\rThe Transformative Influence of Large Language Models on Software Development\nSajed Jalil\nabstract\rabstract: The increasing adoption and commercialization of generalized Large LanguageModels (LLMs) have profoundly impacted various aspects of our daily lives.Initially embraced by the computer science community, the versatility of LLMshas found its way into diverse domains. In particular, the software engineeringrealm has witnessed the most transformative changes. With LLMs increasinglyserving as AI Pair Programming Assistants spurred the development ofspecialized models aimed at aiding software engineers. Although this newparadigm offers numerous advantages, it also presents critical challenges andopen problems. To identify the potential and prevailing obstacles, wesystematically reviewed contemporary scholarly publications, emphasizing theperspectives of software developers and usability concerns. Preliminaryfindings underscore pressing concerns about data privacy, bias, andmisinformation. Additionally, we identified several usability challenges,including prompt engineering, increased cognitive demands, and mistrust.Finally, we introduce 12 open problems that we have identified through oursurvey, covering these various domains.\r2023-11-27\nDiffSLVA: Harnessing Diffusion Models for Sign Language Video Anonymization\nZhaoyang Xia Carol Neidle Dimitris N. Metaxas\nabstract\rabstract: Since American Sign Language (ASL) has no standard written form, Deaf signersfrequently share videos in order to communicate in their native language.However, since both hands and face convey critical linguistic information insigned languages, sign language videos cannot preserve signer privacy. Whilesigners have expressed interest, for a variety of applications, in signlanguage video anonymization that would effectively preserve linguisticcontent, attempts to develop such technology have had limited success, giventhe complexity of hand movements and facial expressions. Existing approachesrely predominantly on precise pose estimations of the signer in video footageand often require sign language video datasets for training. These requirementsprevent them from processing videos \u0026lsquo;in the wild,\u0026rsquo; in part because of thelimited diversity present in current sign language video datasets. To addressthese limitations, our research introduces DiffSLVA, a novel methodology thatutilizes pre-trained large-scale diffusion models for zero-shot text-guidedsign language video anonymization. We incorporate ControlNet, which leverageslow-level image features such as HED (Holistically-Nested Edge Detection)edges, to circumvent the need for pose estimation. Additionally, we develop aspecialized module dedicated to capturing facial expressions, which arecritical for conveying essential linguistic information in signed languages. Wethen combine the above methods to achieve anonymization that better preservesthe essential linguistic content of the original signer. This innovativemethodology makes possible, for the first time, sign language videoanonymization that could be used for real-world applications, which would offersignificant benefits to the Deaf and Hard-of-Hearing communities. Wedemonstrate the effectiveness of our approach with a series of signeranonymization experiments.\rPIPE : Parallelized Inference Through Post-Training Quantization Ensembling of Residual Expansions\nEdouard Yvinec Arnaud Dapogny Kevin Bailly\nabstract\rabstract: Deep neural networks (DNNs) are ubiquitous in computer vision and naturallanguage processing, but suffer from high inference cost. This problem can beaddressed by quantization, which consists in converting floating pointperations into a lower bit-width format. With the growing concerns on privacyrights, we focus our efforts on data-free methods. However, such techniquessuffer from their lack of adaptability to the target devices, as a hardwaretypically only support specific bit widths. Thus, to adapt to a variety ofdevices, a quantization method shall be flexible enough to find good accuracyv.s. speed trade-offs for every bit width and target device. To achieve this,we propose PIPE, a quantization method that leverages residual error expansion,along with group sparsity and an ensemble approximation for betterparallelization. PIPE is backed off by strong theoretical guarantees andachieves superior performance on every benchmarked application (from vision toNLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 toternary quantization).\rTokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform\nYihao Li Yanyi Lai Tianchi Liao Chuan Chen Zibin Zheng\nabstract\rabstract: With the development of practical deep learning models like generative AI,their excellent performance has brought huge economic value. For instance,ChatGPT has attracted more than 100 million users in three months. Since themodel training requires a lot of data and computing power, a well-performingdeep learning model is behind a huge effort and cost. Facing various modelattacks, unauthorized use and abuse from the network that threaten theinterests of model owners, in addition to considering legal and otheradministrative measures, it is equally important to protect the model\u0026rsquo;scopyright from the technical means. By using the model watermarking technology,we point out the possibility of building a unified platform for model ownershipverification. Given the application history of blockchain in copyrightverification and the drawbacks of a centralized third-party, this paperconsiders combining model watermarking technology and blockchain to build aunified model copyright protection platform. By a new solution we calledTokenized Model, it protects the model\u0026rsquo;s copyright by reliable ownership recordand verification mechanism. It also promotes the financial value of model byconstructing the model\u0026rsquo;s transaction process and contribution shares of amodel. In the typical case study, we also study the various performance underusual scenario to verify the effectiveness of this platform.\rDP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\nJunyuan Hong Jiachen T. Wang Chenhui Zhang Zhangheng Li Bo Li Zhangyang Wang\nabstract\rabstract: Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts\u0026rsquo; dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model\u0026rsquo;s provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\r2023-11-26\nAI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction\nJunsol Kim Byungkyu Lee\nabstract\rabstract: Large language models (LLMs) that produce human-like responses have begun torevolutionize research practices in the social sciences. This paper shows howwe can integrate LLMs and social surveys to accurately predict individualresponses to survey questions that were not asked before. We develop a novelmethodological framework to personalize LLMs by considering the meaning ofsurvey questions derived from their text, the latent beliefs of individualsinferred from their response patterns, and the temporal contexts acrossdifferent survey periods through fine-tuning LLMs with survey data. Using theGeneral Social Survey from 1972 to 2021, we show that the fine-tuned modelbased on Alpaca-7b can predict individual responses to survey questions thatare partially missing as well as entirely missing. The remarkable predictioncapabilities allow us to fill in missing trends with high confidence andpinpoint when public attitudes changed, such as the rising support for same-sexmarriage. We discuss practical constraints, socio-demographic representation,and ethical concerns regarding individual autonomy and privacy when using LLMsfor opinion prediction. This study demonstrates that LLMs and surveys canmutually enhance each other\u0026rsquo;s capabilities: LLMs broaden survey potential,while surveys improve the alignment of LLMs.\r2023-11-24\nInput Reconstruction Attack against Vertical Federated Large Language Models\nFei Zheng\nabstract\rabstract: Recently, large language models (LLMs) have drawn extensive attention fromacademia and the public, due to the advent of the ChatGPT. While LLMs showtheir astonishing ability in text generation for various tasks, privacyconcerns limit their usage in real-life businesses. More specifically, eitherthe user\u0026rsquo;s inputs (the user sends the query to the model-hosting server) or themodel (the user downloads the complete model) itself will be revealed duringthe usage. Vertical federated learning (VFL) is a promising solution to thiskind of problem. It protects both the user\u0026rsquo;s input and the knowledge of themodel by splitting the model into a bottom part and a top part, which ismaintained by the user and the model provider, respectively. However, in thispaper, we demonstrate that in LLMs, VFL fails to protect the user input sinceit is simple and cheap to reconstruct the input from the intermediateembeddings. Experiments show that even with a commercial GPU, the inputsentence can be reconstructed in only one second. We also discuss severalpossible solutions to enhance the privacy of vertical federated LLMs.\r2023-11-23\nPrivateLoRA For Efficient Privacy Preserving LLM\nYiming Wang Yu Lin Xiaodong Zeng Guannan Zhang\nabstract\rabstract: End users face a choice between privacy and efficiency in current LargeLanguage Model (LLM) service paradigms. In cloud-based paradigms, users areforced to compromise data locality for generation quality and processing speed.Conversely, edge device paradigms maintain data locality but fail to deliversatisfactory performance. In this work, we propose a novel LLM service paradigmthat distributes privacy-sensitive computation on edge devices and sharedcomputation in the cloud. Only activations are transmitted between the centralcloud and edge devices to ensure data locality. Our core innovation,PrivateLoRA, addresses the challenging communication overhead by exploiting thelow rank of residual activations, achieving over 95% communication reduction.Consequently, PrivateLoRA effectively maintains data locality and is extremelyresource efficient. Under standard 5G networks, PrivateLoRA achieves throughputover 300% of device-only solutions for 7B models and over 80% of an A100 GPUfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRAfor advanced personalization. Our approach democratizes access tostate-of-the-art generative AI for edge devices, paving the way for moretailored LLM experiences for the general public. To our knowledge, our proposedframework is the first efficient and privacy-preserving LLM solution in theliterature.\rMARBLE: Music Audio Representation Benchmark for Universal Evaluation\nRuibin Yuan Yinghao Ma Yizhi Li Ge Zhang Xingran Chen Hanzhi Yin Le Zhuo Yiqi Liu Jiawen Huang Zeyue Tian Binyue Deng Ningzhi Wang Chenghua Lin Emmanouil Benetos Anton Ragni Norbert Gyenge Roger Dannenberg Wenhu Chen Gus Xia Wei Xue Si Liu Shi Wang Ruibo Liu Yike Guo Jie Fu\nabstract\rabstract: In the era of extensive intersection between art and Artificial Intelligence(AI), such as image generation and fiction co-creation, AI for music remainsrelatively nascent, particularly in music understanding. This is evident in thelimited work on deep music representations, the scarcity of large-scaledatasets, and the absence of a universal and community-driven benchmark. Toaddress this issue, we introduce the Music Audio Representation Benchmark foruniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for variousMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomywith four hierarchy levels, including acoustic, performance, score, andhigh-level description. We then establish a unified protocol based on 14 taskson 8 public-available datasets, providing a fair and standard assessment ofrepresentations of all open-sourced pre-trained models developed on musicrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, andreproducible suite for the community, with a clear statement on copyrightissues on datasets. Results suggest recently proposed large-scale pre-trainedmusical language models perform the best in most tasks, with room for furtherimprovement. The leaderboard and toolkit repository are published athttps://marble-bm.shef.ac.uk to promote future music AI research.\rA Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs\nMuhammad Ilyas Azeem Sallam Abualhaija\nabstract\rabstract: Specifying legal requirements for software systems to ensure their compliancewith the applicable regulations is a major concern to requirements engineering(RE). Personal data which is collected by an organization is often shared withother organizations to perform certain processing activities. In such cases,the General Data Protection Regulation (GDPR) requires issuing a dataprocessing agreement (DPA) which regulates the processing and further ensuresthat personal data remains protected. Violating GDPR can lead to huge finesreaching to billions of Euros. Software systems involving personal dataprocessing must adhere to the legal obligations stipulated in GDPR and outlinedin DPAs. Requirements engineers can elicit from DPAs legal requirements forregulating the data processing activities in software systems. Checking thecompleteness of a DPA according to the GDPR provisions is therefore anessential prerequisite to ensure that the elicited requirements are complete.Analyzing DPAs entirely manually is time consuming and requires adequate legalexpertise. In this paper, we propose an automation strategy to address thecompleteness checking of DPAs against GDPR. Specifically, we pursue tenalternative solutions which are enabled by different technologies, namelytraditional machine learning, deep learning, language modeling, and few-shotlearning. The goal of our work is to empirically examine how these differenttechnologies fare in the legal domain. We computed F2 score on a set of 30 realDPAs. Our evaluation shows that best-performing solutions yield F2 score of86.7% and 89.7% are based on pre-trained BERT and RoBERTa language models. Ouranalysis further shows that other alternative solutions based on deep learning(e.g., BiLSTM) and few-shot learning (e.g., SetFit) can achieve comparableaccuracy, yet are more efficient to develop.\rChallenges of Large Language Models for Mental Health Counseling\nNeo Christopher Chung George Dyer Lennart Brocki\nabstract\rabstract: The global mental health crisis is looming with a rapid increase in mentaldisorders, limited resources, and the social stigma of seeking treatment. Asthe field of artificial intelligence (AI) has witnessed significantadvancements in recent years, large language models (LLMs) capable ofunderstanding and generating human-like text may be used in supporting orproviding psychological counseling. However, the application of LLMs in themental health domain raises concerns regarding the accuracy, effectiveness, andreliability of the information provided. This paper investigates the majorchallenges associated with the development of LLMs for psychologicalcounseling, including model hallucination, interpretability, bias, privacy, andclinical effectiveness. We explore potential solutions to these challenges thatare practical and applicable to the current paradigm of AI. From our experiencein developing and deploying LLMs for mental health, AI holds a great promisefor improving mental health care, if we can carefully navigate and overcomepitfalls of LLMs.\r2023-11-22\nSteal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models\nGe Luo Junqiang Huang Manman Zhang Zhenxing Qian Sheng Li Xinpeng Zhang\nabstract\rabstract: The advancement in text-to-image models has led to astonishing artisticperformances. However, several studios and websites illegally fine-tune thesemodels using artists\u0026rsquo; artworks to mimic their styles for profit, which violatesthe copyrights of artists and diminishes their motivation to produce originalworks. Currently, there is a notable lack of research focusing on this issue.In this paper, we propose a novel watermarking framework that detects mimicryin text-to-image models through fine-tuning. This framework embeds subtlewatermarks into digital artworks to protect their copyrights while stillpreserving the artist\u0026rsquo;s visual expression. If someone takes watermarkedartworks as training data to mimic an artist\u0026rsquo;s style, these watermarks canserve as detectable indicators. By analyzing the distribution of thesewatermarks in a series of generated images, acts of fine-tuning mimicry usingstolen victim data will be exposed. In various fine-tune scenarios and againstwatermark attack methods, our research confirms that analyzing the distributionof watermarks in artificially generated images reliably detects unauthorizedmimicry.\r2023-11-21\nDon\u0026rsquo;t forget private retrieval: distributed private similarity search for large language models\nGuy Zyskind Tobin South Alex Pentland\nabstract\rabstract: While the flexible capabilities of large language models (LLMs) allow them toanswer a range of queries based on existing learned knowledge, informationretrieval to augment generation is an important tool to allow LLMs to answerquestions on information not included in pre-training data. Such privateinformation is increasingly being generated in a wide array of distributedcontexts by organizations and individuals. Performing such informationretrieval using neural embeddings of queries and documents always leakedinformation about queries and database content unless both were stored locally.We present Private Retrieval Augmented Generation (PRAG), an approach that usesmulti-party computation (MPC) to securely transmit queries to a distributed setof servers containing a privately constructed database to return top-k andapproximate top-k documents. This is a first-of-its-kind approach to denseinformation retrieval that ensures no server observes a client\u0026rsquo;s query or cansee the database content. The approach introduces a novel MPC friendly protocolfor inverted file approximate search (IVF) that allows for fast document searchover distributed and private data in sublinear communication complexity. Thiswork presents new avenues through which data for use in LLMs can be accessedand used without needing to centralize or forgo privacy.\rKNVQA: A Benchmark for evaluation knowledge-based VQA\nSirui Cheng Siyu Zhang Jiayi Wu Muchen Lan\nabstract\rabstract: Within the multimodal field, large vision-language models (LVLMs) have madesignificant progress due to their strong perception and reasoning capabilitiesin the visual and language systems. However, LVLMs are still plagued by the twocritical issues of object hallucination and factual accuracy, which limit thepracticality of LVLMs in different scenarios. Furthermore, previous evaluationmethods focus more on the comprehension and reasoning of language content butlack a comprehensive evaluation of multimodal interactions, thereby resultingin potential limitations. To this end, we propose a novel KNVQA-Eval, which isdevoted to knowledge-based VQA task evaluation to reflect the factuality ofmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,we develop a new KNVQA dataset by incorporating human judgment and perception,aiming to evaluate the accuracy of standard answers relative to AI-generatedanswers in knowledge-based VQA. This work not only comprehensively evaluatesthe contextual information of LVLMs using reliable human annotations, but alsofurther analyzes the fine-grained capabilities of current methods to revealpotential avenues for subsequent optimization of LVLMs-based estimators. Ourproposed VQA-Eval and corresponding dataset KNVQA will facilitate thedevelopment of automatic evaluation tools with the advantages of low cost,privacy protection, and reproducibility. Our code will be released uponpublication.\rAdapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications\nSamira Ghodratnama Mehrdad Zakershahrak\nabstract\rabstract: The advent of Large Language Models (LLMs) heralds a pivotal shift in onlineuser interactions with information. Traditional Information Retrieval (IR)systems primarily relied on query-document matching, whereas LLMs excel incomprehending and generating human-like text, thereby enriching the IRexperience significantly. While LLMs are often associated with chatbotfunctionalities, this paper extends the discussion to their explicitapplication in information retrieval. We explore methodologies to optimize theretrieval process, select optimal models, and effectively scale and orchestrateLLMs, aiming for cost-efficiency and enhanced result accuracy. A notablechallenge, model hallucination-where the model yields inaccurate ormisinterpreted data-is addressed alongside other model-specific hurdles. Ourdiscourse extends to crucial considerations including user privacy, dataoptimization, and the necessity for system clarity and interpretability.Through a comprehensive examination, we unveil not only innovative strategiesfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,but also the consequential considerations that underline the need for abalanced approach aligned with user-centric principles.\r2023-11-20\nMulti-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks\nRami Haffar David Sánchez Josep Domingo-Ferrer\nabstract\rabstract: Human facial data hold tremendous potential to address a variety ofclassification problems, including face recognition, age estimation, genderidentification, emotion analysis, and race classification. However, recentprivacy regulations, such as the EU General Data Protection Regulation andothers, have restricted the ways in which human images may be collected andused for research. As a result, several previously published data setscontaining human faces have been removed from the internet due to inadequatedata collection methods that failed to meet privacy regulations. Data setsconsisting of synthetic data have been proposed as an alternative, but theyfall short of accurately representing the real data distribution. On the otherhand, most available data sets are labeled for just a single task, which limitstheir applicability. To address these issues, we present the Multi-Task Faces(MTF) image data set, a meticulously curated collection of face images designedfor various classification tasks, including face recognition, as well as race,gender, and age classification. The MTF data set has been ethically gathered byleveraging publicly available images of celebrities and strictly adhering tocopyright regulations. In this paper, we present this data set and providedetailed descriptions of the followed data collection and processingprocedures. Furthermore, we evaluate the performance of five deep learning (DL)models on the MTF data set across the aforementioned classification tasks.Additionally, we compare the performance of DL models over the processed MTFdata and over raw data crawled from the internet. The reported resultsconstitute a baseline for further research employing these data. The MTF dataset can be accessed through the following link (please cite the present paperif you use the data set): https://github.com/RamiHaf/MTF_data_set\r2023-11-18\nExperts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q\u0026amp;A\nZahra Kolagar Anna Katharina Leschanowsky Birgit Popp\nabstract\rabstract: Privacy policies play a vital role in safeguarding user privacy as legaljurisdictions worldwide emphasize the need for transparent data processing.While the suitability of privacy policies to enhance transparency has beencritically discussed, employing conversational AI systems presents uniquechallenges in informing users effectively. In this position paper, we propose adynamic workflow for transforming privacy policies into privacyquestion-and-answer (Q\u0026amp;A) pairs to make privacy policies easily accessiblethrough conversational AI. Thereby, we facilitate interdisciplinarycollaboration among legal experts and conversation designers, while alsoconsidering the utilization of large language models\u0026rsquo; generative capabilitiesand addressing associated challenges. Our proposed workflow underscorescontinuous improvement and monitoring throughout the construction of privacyQ\u0026amp;As, advocating for comprehensive review and refinement through anexperts-in-the-loop approach.\rFully Composable and Adequate Verified Compilation with Direct Refinements between Open Modules (Technical Report)\nLing Zhang Yuting Wang Jinhua Wu Jérémie Koenig Zhong Shao\nabstract\rabstract: Verified compilation of open modules (i.e., modules whose functionalitydepends on other modules) provides a foundation for end-to-end verification ofmodular programs ubiquitous in contemporary software. However, despiteintensive investigation in this topic for decades, the proposed approaches arestill difficult to use in practice as they rely on assumptions about theinternal working of compilers which make it difficult for external users toapply the verification results. We propose an approach to verifiedcompositional compilation without such assumptions in the setting of verifyingcompilation of heterogeneous modules written in first-order languagessupporting global memory and pointers. Our approach is based on the memorymodel of CompCert and a new discovery that a Kripke relation with a notion ofmemory protection can serve as a uniform and composable semantic interface forthe compiler passes. By absorbing the rely-guarantee conditions on memoryevolution for all compiler passes into this Kripke Memory Relation and bypiggybacking requirements on compiler optimizations onto it, we getcompositional correctness theorems for realistic optimizing compilers asrefinements that directly relate native semantics of open modules and that areignorant of intermediate compilation processes. Such direct refinements supportall the compositionality and adequacy properties essential for verifiedcompilation of open modules. We have applied this approach to the fullcompilation chain of CompCert with its Clight source language and demonstratedthat our compiler correctness theorem is open to composition and intuitive touse with reduced verification complexity through end-to-end verification ofnon-trivial heterogeneous modules that may freely invoke each other (e.g.,mutually recursively).\r2023-11-17\nFunctionMarker: Watermarking Language Datasets via Knowledge Injection\nShuai Li Kejiang Chen Kunsheng Tang Wen Huang Jie Zhang Weiming Zhang Nenghai Yu\nabstract\rabstract: Large Language Models (LLMs) have demonstrated superior performance invarious natural language processing tasks. Meanwhile, they require extensivetraining data, raising concerns related to dataset copyright protection.Backdoor-based watermarking is a viable approach to protect the copyright ofclassification datasets. However, these methods may introduce maliciousmisclassification behaviors into watermarked LLMs by attackers and also affectthe semantic information of the watermarked text. To address these issues, wepropose FunctionMarker, a novel copyright protection method for languagedatasets via knowledge injection. FunctionMarker enables LLMs to learn specificknowledge through fine-tuning on watermarked datasets, and we can extract theembedded watermark by obtaining the responses of LLMs to specificknowledge-related queries. Considering watermark capacity and stealthness, weselect customizable functions as specific knowledge for LLMs to learn and embedthe watermark into them. Moreover, FunctionMarker can embed multi-bitwatermarks while preserving the original semantic information, therebyincreasing the difficulty of adaptive attacks. We take mathematical functionsas an instance to evaluate the effectiveness of FunctionMarker, and experimentsshow that only 0.3% of watermarked text achieves a 90% watermark extractionaccuracy in most cases, validating our method\u0026rsquo;s effectiveness.\r2023-11-16\nText Sanitization Beyond Specific Domains: Zero-Shot Redaction \u0026amp; Substitution with Large Language Models\nFederico Albanese Daniel Ciolek Nicolas D\u0026rsquo;Ippolito\nabstract\rabstract: In the context of information systems, text sanitization techniques are usedto identify and remove sensitive data to comply with security and regulatoryrequirements. Even though many methods for privacy preservation have beenproposed, most of them are focused on the detection of entities from specificdomains (e.g., credit card numbers, social security numbers), lackinggenerality and requiring customization for each desirable domain. Moreover,removing words is, in general, a drastic measure, as it can degrade textcoherence and contextual information. Less severe measures include substitutinga word for a safe alternative, yet it can be challenging to automatically findmeaningful substitutions. We present a zero-shot text sanitization techniquethat detects and substitutes potentially sensitive information using LargeLanguage Models. Our evaluation shows that our method excels at protectingprivacy while maintaining text coherence and contextual information, preservingdata utility for downstream tasks.\rTowards More Realistic Membership Inference Attacks on Large Diffusion Models\nJan Dubiński Antoni Kowalczuk Stanisław Pawlak Przemysław Rokita Tomasz Trzciński Paweł Morawiecki\nabstract\rabstract: Generative diffusion models, including Stable Diffusion and Midjourney, cangenerate visually appealing, diverse, and high-resolution images for variousapplications. These models are trained on billions of internet-sourced images,raising significant concerns about the potential unauthorized use ofcopyright-protected images. In this paper, we examine whether it is possible todetermine if a specific image was used in the training set, a problem known inthe cybersecurity community and referred to as a membership inference attack.Our focus is on Stable Diffusion, and we address the challenge of designing afair evaluation framework to answer this membership question. We propose amethodology to establish a fair evaluation setup and apply it to StableDiffusion, enabling potential extensions to other generative models. Utilizingthis evaluation setup, we execute membership attacks (both known and newlyintroduced). Our research reveals that previously proposed evaluation setups donot provide a full understanding of the effectiveness of membership inferenceattacks. We conclude that the membership inference attack remains a significantchallenge for large diffusion models (often deployed as black-box systems),indicating that related privacy and copyright issues will persist in theforeseeable future.\rWhen the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks\nEve Fleisig Rediet Abebe Dan Klein\nabstract\rabstract: Though majority vote among annotators is typically used for ground truthlabels in natural language processing, annotator disagreement in tasks such ashate speech detection may reflect differences in opinion across groups, notnoise. Thus, a crucial problem in hate speech detection is determining whethera statement is offensive to the demographic group that it targets, when thatgroup may constitute a small fraction of the annotator pool. We construct amodel that predicts individual annotator ratings on potentially offensive textand combines this information with the predicted target group of the text tomodel the opinions of target group members. We show gains across a range ofmetrics, including raising performance over the baseline by 22% at predictingindividual annotators\u0026rsquo; ratings and by 33% at predicting variance amongannotators, which provides a metric for model uncertainty downstream. We findthat annotator ratings can be predicted using their demographic information andopinions on online content, without the need to track identifying annotator IDsthat link each annotator to their ratings. We also find that use ofnon-invasive survey questions on annotators\u0026rsquo; online experiences helps tomaximize privacy and minimize unnecessary collection of demographic informationwhen predicting annotators\u0026rsquo; opinions.\rIncorporating Worker Perspectives into MTurk Annotation Practices for NLP\nOlivia Huang Eve Fleisig Dan Klein\nabstract\rabstract: Current practices regarding data collection for natural language processingon Amazon Mechanical Turk (MTurk) often rely on a combination of studies ondata quality and heuristics shared among NLP researchers. However, withoutconsidering the perspectives of MTurk workers, these approaches are susceptibleto issues regarding workers\u0026rsquo; rights and poor response quality. We conducted acritical literature review and a survey of MTurk workers aimed at addressingopen questions regarding best practices for fair payment, worker privacy, dataquality, and considering worker incentives. We found that worker preferencesare often at odds with received wisdom among NLP researchers. Surveyed workerspreferred reliable, reasonable payments over uncertain, very high payments;reported frequently lying on demographic questions; and expressed frustrationat having work rejected with no explanation. We also found that workers viewsome quality control methods, such as requiring minimum response times orMaster\u0026rsquo;s qualifications, as biased and largely ineffective. Based on the surveyresults, we provide recommendations on how future NLP studies may betteraccount for MTurk workers\u0026rsquo; experiences in order to respect workers\u0026rsquo; rights andimprove data quality.\r2023-11-15\nHow Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\nLingbo Mo Boshi Wang Muhao Chen Huan Sun\nabstract\rabstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.\rAn Empathetic User-Centric Chatbot for Emotional Support\nYanting Pan Yixuan Tang Yuchen Niu\nabstract\rabstract: This paper explores the intersection of Otome Culture and artificialintelligence, particularly focusing on how Otome-oriented games fulfill theemotional needs of young women. These games, which are deeply rooted in asubcultural understanding of love, provide players with feelings ofsatisfaction, companionship, and protection through carefully crafted narrativestructures and character development. With the proliferation of Large LanguageModels (LLMs), there is an opportunity to transcend traditional static gamenarratives and create dynamic, emotionally responsive interactions. We presenta case study of Tears of Themis, where we have integrated LLM technology toenhance the interactive experience. Our approach involves augmenting existinggame narratives with a Question and Answer (QA) system, enriched through dataaugmentation and emotional enhancement techniques, resulting in a chatbot thatoffers realistic and supportive companionship.\rValue FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values\nJing Yao Xiaoyuan Yi Xiting Wang Yifan Gong Xing Xie\nabstract\rabstract: The rapid advancement of Large Language Models (LLMs) has attracted muchattention to value alignment for their responsible development. However, how todefine values in this context remains a largely unexplored question. Existingwork mainly follows the Helpful, Honest, Harmless principle and specifiesvalues as risk criteria formulated in the AI community, e.g., fairness andprivacy protection, suffering from poor clarity, adaptability and transparency.Inspired by basic values in humanity and social science across cultures, thiswork proposes a novel basic value alignment paradigm and introduces a valuespace spanned by basic value dimensions. All LLMs\u0026rsquo; behaviors can be mapped intothe space by identifying the underlying values, possessing the potential toaddress the three challenges. To foster future research, we apply therepresentative Schwartz\u0026rsquo;s Theory of Basic Values as an initialized example andconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.Our extensive analysis of FULCRA reveals the underlying relation between basicvalues and LLMs\u0026rsquo; behaviors, demonstrating that our approach not only coversexisting mainstream risks but also anticipates possibly unidentified ones.Additionally, we present an initial implementation of the basic valueevaluation and alignment, paving the way for future research in this line.\r2023-11-14\nSparsity-Preserving Differentially Private Training of Large Embedding Models\nBadih Ghazi Yangsibo Huang Pritish Kamath Ravi Kumar Pasin Manurangsi Amer Sinha Chiyuan Zhang\nabstract\rabstract: As the use of large embedding models in recommendation systems and languageapplications increases, concerns over user data privacy have also risen.DP-SGD, a training algorithm that combines differential privacy with stochasticgradient descent, has been the workhorse in protecting user privacy withoutcompromising model accuracy by much. However, applying DP-SGD naively toembedding models can destroy gradient sparsity, leading to reduced trainingefficiency. To address this issue, we present two new algorithms, DP-FEST andDP-AdaFEST, that preserve gradient sparsity during private training of largeembedding models. Our algorithms achieve substantial reductions ($10^6 \\times$)in gradient size, while maintaining comparable levels of accuracy, on benchmarkreal-world datasets.\rLatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud\nMengke Zhang Tianxing He Tianle Wang Lu Mi Fatemehsadat Mireshghallah Binyi Chen Hao Wang Yulia Tsvetkov\nabstract\rabstract: In the current user-server interaction paradigm of prompted generation withlarge language models (LLM) on cloud, the server fully controls the generationprocess, which leaves zero options for users who want to keep the generatedtext to themselves. We propose LatticeGen, a cooperative framework in which theserver still handles most of the computation while the user controls thesampling operation. The key idea is that the true generated sequence is mixedwith noise tokens by the user and hidden in a noised lattice. Consideringpotential attacks from a hypothetically malicious server and how the user candefend against it, we propose the repeated beam-search attack and the mixingnoise scheme. In our experiments we apply LatticeGen to protect both prompt andgeneration. It is shown that while the noised lattice degrades generationquality, LatticeGen successfully protects the true generation to a remarkabledegree under strong attacks (more than 50% of the semantic remains hidden asmeasured by BERTScore).\r2023-11-13\nFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\nKerem Zaman Leshem Choshen Shashank Srivastava\nabstract\rabstract: Model fusion research aims to aggregate the knowledge of multiple models toenhance performance by combining their weights. In this work, we study theinverse, investigating whether and how can model fusion interfere and reduceunwanted knowledge. We delve into the effects of model fusion on the evolutionof learned shortcuts, social biases, and memorization capabilities infine-tuned language models. Through several experiments covering textclassification and generation tasks, our analysis highlights that sharedknowledge among models is usually enhanced during model fusion, while unsharedknowledge is usually lost or forgotten. Based on this observation, wedemonstrate the potential of model fusion as a debiasing tool and showcase itsefficacy in addressing privacy concerns associated with language models.\r2023-11-12\nFlames: Benchmarking Value Alignment of Chinese Large Language Models\nKexin Huang Xiangyang Liu Qianyu Guo Tianxiang Sun Jiawei Sun Yaru Wang Zeyang Zhou Yixu Wang Yan Teng Xipeng Qiu Yingchun Wang Dahua Lin\nabstract\rabstract: The widespread adoption of large language models (LLMs) across variousregions underscores the urgent need to evaluate their alignment with humanvalues. Current benchmarks, however, fall short of effectively uncoveringsafety vulnerabilities in LLMs. Despite numerous models achieving high scoresand \u0026rsquo;topping the chart\u0026rsquo; in these evaluations, there is still a significant gapin LLMs\u0026rsquo; deeper alignment with human values and achieving genuine harmlessness.To this end, this paper proposes the first highly adversarial benchmark namedFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responseswith fine-grained annotations, and a specified scorer. Our frameworkencompasses both common harmlessness principles, such as fairness, safety,legality, and data protection, and a unique morality dimension that integratesspecific Chinese values such as harmony. Based on the framework, we carefullydesign adversarial prompts that incorporate complex scenarios and jailbreakingmethods, mostly with implicit malice. By prompting mainstream LLMs with suchadversarially constructed prompts, we obtain model responses, which are thenrigorously annotated for evaluation. Our findings indicate that all theevaluated LLMs demonstrate relatively poor performance on Flames, particularlyin the safety and fairness dimensions. Claude emerges as the best-performingmodel overall, but with its harmless rate being only 63.08% while GPT-4 onlyscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,setting a new challenge for contemporary LLMs and highlighting the need forfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,we develop a specified scorer capable of scoring LLMs across multipledimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publiclyavailable on https://github.com/AIFlames/Flames.\rTunable Soft Prompts are Messengers in Federated Learning\nChenhe Dong Yuexiang Xie Bolin Ding Ying Shen Yaliang Li\nabstract\rabstract: Federated learning (FL) enables multiple participants to collaborativelytrain machine learning models using decentralized data sources, alleviatingprivacy concerns that arise from directly sharing local data. However, the lackof model privacy protection in FL becomes an unneglectable challenge,especially when people want to federally finetune models based on a proprietarylarge language model. In this study, we propose a novel FL training approachthat accomplishes information exchange among participants via tunable softprompts. These soft prompts, updated and transmitted between the server andclients, assume the role of the global model parameters and serve as messengersto deliver useful knowledge from the local data and global model. As the globalmodel itself is not required to be shared and the local training is conductedbased on an auxiliary model with fewer parameters than the global model, theproposed approach provides protection for the global model while reducingcommunication and computation costs in FL. Extensive experiments show theeffectiveness of the proposed approach compared to several baselines. We havereleased the source code at\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.\rEvaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling\nYujin Cho Mingeon Kim Seojin Kim Oyun Kwon Ryan Donghan Kwon Yoonha Lee Dohyun Lim\nabstract\rabstract: This study investigates the efficacy of Large Language Models (LLMs) ininteractive language therapy for high-functioning autistic adolescents. Withthe rapid advancement of artificial intelligence, particularly in naturallanguage processing, LLMs present a novel opportunity to augment traditionalpsychological counseling methods. This research primarily focuses on evaluatingthe LLM\u0026rsquo;s ability to engage in empathetic, adaptable, and contextuallyappropriate interactions within a therapeutic setting. A comprehensiveevaluation was conducted by a panel of clinical psychologists and psychiatristsusing a specially developed scorecard. The assessment covered various aspectsof the LLM\u0026rsquo;s performance, including empathy, communication skills,adaptability, engagement, and the ability to establish a therapeutic alliance.The study avoided direct testing with patients, prioritizing privacy andethical considerations, and instead relied on simulated scenarios to gauge theLLM\u0026rsquo;s effectiveness. The results indicate that LLMs hold significant promise assupportive tools in therapy, demonstrating strengths in empathetic engagementand adaptability in conversation. However, challenges in achieving the depth ofpersonalization and emotional understanding characteristic of human therapistswere noted. The study also highlights the importance of ethical considerationsin the application of AI in therapeutic contexts. This research providesvaluable insights into the potential and limitations of using LLMs inpsychological counseling for autistic adolescents. It lays the groundwork forfuture explorations into AI\u0026rsquo;s role in mental health care, emphasizing the needfor ongoing development to enhance the capabilities of these models intherapeutic settings.\r2023-11-11\nAn In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing\nLe Peng Gaoxiang Luo sicheng zhou jiandong chen Rui Zhang Ziyue Xu Ju Sun\nabstract\rabstract: Language models (LMs) such as BERT and GPT have revolutionized naturallanguage processing (NLP). However, the medical field faces challenges intraining LMs due to limited data access and privacy constraints imposed byregulations like the Health Insurance Portability and Accountability Act(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning(FL) offers a decentralized solution that enables collaborative learning whileensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasksencompassing 8 corpora using 6 LMs. Our results show that: 1) FL modelsconsistently outperformed models trained on individual clients\u0026rsquo; data andsometimes performed comparably with models trained with polled data; 2) withthe fixed number of total data, FL models training with more clients producedinferior performance but pre-trained transformer-based models exhibited greatresilience. 3) FL models significantly outperformed large language models usingzero-/one-shot learning and offered lightning inference speed.\rDo Not Harm Protected Groups in Debiasing Language Representation Models\nChloe Qinyu Zhu Rickard Stureborg Brandon Fain\nabstract\rabstract: Language Representation Models (LRMs) trained with real-world data maycapture and exacerbate undesired bias and cause unfair treatment of people invarious demographic groups. Several techniques have been investigated forapplying interventions to LRMs to remove bias in benchmark evaluations on, forexample, word embeddings. However, the negative side effects of debiasinginterventions are usually not revealed in the downstream tasks. We proposexGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. Inthis work, We examine four debiasing techniques on a real-world textclassification task and show that reducing biasing is at the cost of degradingperformance for all demographic groups, including those the debiasingtechniques aim to protect. We advocate that a debiasing technique should havegood downstream performance with the constraint of ensuring no harm to theprotected group.\rGenerative AI for Space-Air-Ground Integrated Networks (SAGIN)\nRuichen Zhang Hongyang Du Dusit Niyato Jiawen Kang Zehui Xiong Abbas Jamalipour Ping Zhang Dong In Kim\nabstract\rabstract: Recently, generative AI technologies have emerged as a significantadvancement in artificial intelligence field, renowned for their language andimage generation capabilities. Meantime, space-air-ground integrated network(SAGIN) is an integral part of future B5G/6G for achieving ubiquitousconnectivity. Inspired by this, this article explores an integration ofgenerative AI in SAGIN, focusing on potential applications and case study. Wefirst provide a comprehensive review of SAGIN and generative AI models,highlighting their capabilities and opportunities of their integration.Benefiting from generative AI\u0026rsquo;s ability to generate useful data and facilitateadvanced decision-making processes, it can be applied to various scenarios ofSAGIN. Accordingly, we present a concise survey on their integration, includingchannel modeling and channel state information (CSI) estimation, jointair-space-ground resource allocation, intelligent network deployment, semanticcommunications, image extraction and processing, security and privacyenhancement. Next, we propose a framework that utilizes a Generative DiffusionModel (GDM) to construct channel information map to enhance quality of servicefor SAGIN. Simulation results demonstrate the effectiveness of the proposedframework. Finally, we discuss potential research directions for generativeAI-enabled SAGIN.\r2023-11-10\nRemoving RLHF Protections in GPT-4 via Fine-Tuning\nQiusi Zhan Richard Fang Rohan Bindu Akul Gupta Tatsunori Hashimoto Daniel Kang\nabstract\rabstract: As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendorsof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to removeRLHF protections with as few as 340 examples and a 95% success rate. Thesetraining examples can be automatically generated with weaker models. We furthershow that removing RLHF protections does not decrease usefulness onnon-censored outputs, providing evidence that our fine-tuning strategy does notdecrease usefulness despite using weaker models to generate training data. Ourresults show the need for further research on protections on LLMs.\rWatermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service\nYuanmin Tang Jing Yu Keke Gai Xiangyan Qu Yue Hu Gang Xiong Qi Wu\nabstract\rabstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at https://github.com/Pter61/vlpmarker.\r2023-11-09\nChatbots Are Not Reliable Text Annotators\nRoss Deans Kristensen-McLachlan Miceal Canavan Márton Kardos Mia Jacobsen Lene Aarøe\nabstract\rabstract: Recent research highlights the significant potential of ChatGPT for textannotation in social science research. However, ChatGPT is a closed-sourceproduct which has major drawbacks with regards to transparency,reproducibility, cost, and data protection. Recent advances in open-source (OS)large language models (LLMs) offer alternatives which remedy these challenges.This means that it is important to evaluate the performance of OS LLMs relativeto ChatGPT and standard approaches to supervised machine learningclassification. We conduct a systematic comparative evaluation of theperformance of a range of OS LLM models alongside ChatGPT, using both zero- andfew-shot learning as well as generic and custom prompts, with results comparedto more traditional supervised classification models. Using a new dataset ofTweets from US news media, and focusing on simple binary text annotation tasksfor standard social science concepts, we find significant variation in theperformance of ChatGPT and OS models across the tasks, and that supervisedclassifiers consistently outperform both. Given the unreliable performance ofChatGPT and the significant challenges it poses to Open Science we adviseagainst using ChatGPT for substantive text annotation tasks in social scienceresearch.\rSynthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model\nBrandon Theodorou Cao Xiao Jimeng Sun\nabstract\rabstract: Synthetic electronic health records (EHRs) that are both realistic andpreserve privacy can serve as an alternative to real EHRs for machine learning(ML) modeling and statistical analysis. However, generating high-fidelity andgranular electronic health record (EHR) data in its original,highly-dimensional form poses challenges for existing methods due to thecomplexities inherent in high-dimensional data. In this paper, we proposeHierarchical Autoregressive Language mOdel (HALO) for generating longitudinalhigh-dimensional EHR, which preserve the statistical properties of real EHR andcan be used to train accurate ML models without privacy concerns. Our HALOmethod, designed as a hierarchical autoregressive model, generates aprobability density function of medical codes, clinical visits, and patientrecords, allowing for the generation of realistic EHR data in its original,unaggregated form without the need for variable selection or aggregation.Additionally, our model also produces high-quality continuous variables in alongitudinal and probabilistic manner. We conducted extensive experiments anddemonstrate that HALO can generate high-fidelity EHR data with high-dimensionaldisease code probabilities (d \u0026gt; 10,000), disease co-occurrence probabilitieswithin visits (d \u0026gt; 1,000,000), and conditional probabilities across consecutivevisits (d \u0026gt; 5,000,000) and achieve above 0.9 R2 correlation in comparison toreal EHR data. This performance then enables downstream ML models trained onits synthetic data to achieve comparable accuracy to models trained on realdata (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using acombination of real and synthetic data enhances the accuracy of ML modelsbeyond that achieved by using only real EHR data.\rEnhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT\nJingye Yang Cong Liu Wendy Deng Da Wu Chunhua Weng Yunyun Zhou Kai Wang\nabstract\rabstract: We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.\rPRODIGy: a PROfile-based DIalogue Generation dataset\nDaniela Occhipinti Serra Sinem Tekiroglu Marco Guerini\nabstract\rabstract: Providing dialogue agents with a profile representation can improve theirconsistency and coherence, leading to better conversations. However, currentprofile-based dialogue datasets for training such agents contain eitherexplicit profile representations that are simple and dialogue-specific, orimplicit representations that are difficult to collect. In this work, wepropose a unified framework in which we bring together both standard and moresophisticated profile representations by creating a new resource where eachdialogue is aligned with all possible speaker representations such ascommunication style, biographies, and personality. This framework allows totest several baselines built using generative language models with severalprofile configurations. The automatic evaluation shows that profile-basedmodels have better generalisation capabilities than models trained on dialoguesonly, both in-domain and cross-domain settings. These results are consistentfor fine-tuned models and instruction-based LLMs. Additionally, humanevaluation demonstrates a clear preference for generations consistent with bothprofile and context. Finally, to account for possible privacy concerns, allexperiments are done under two configurations: inter-character andintra-character. In the former, the LM stores the information about thecharacter in its internal representation, while in the latter, the LM does notretain any personal information but uses it only at inference time.\r2023-11-08\nBuilding Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective\nMd Tahmid Rahman Laskar Xue-Yong Fu Cheng Chen Shashi Bhushan TN\nabstract\rabstract: This paper studies how to effectively build meeting summarization systems forreal-world usage using large language models (LLMs). For this purpose, weconduct an extensive evaluation and comparison of various closed-source andopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findingsreveal that most closed-source LLMs are generally better in terms ofperformance. However, much smaller open-source models like LLaMA- 2 (7B and13B) could still achieve performance comparable to the large closed-sourcemodels even in zero-shot scenarios. Considering the privacy concerns ofclosed-source models for only being accessible via API, alongside the high costassociated with using fine-tuned versions of the closed-source models, theopensource models that can achieve competitive performance are moreadvantageous for industrial use. Balancing performance with associated costsand privacy concerns, the LLaMA-2-7B model looks more promising for industrialusage. In sum, this paper offers practical insights on using LLMs forreal-world business meeting summarization, shedding light on the trade-offsbetween performance and cost.\rFederated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models\nSixing Yu J. Pablo Muñoz Ali Jannesari\nabstract\rabstract: Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, havedemonstrated remarkable success in a wide range of applications, driven bytheir ability to leverage vast amounts of data for pre-training. However,optimizing FMs often requires access to sensitive data, raising privacyconcerns and limiting their applicability in many domains. In this paper, wepropose the Federated Foundation Models (FFMs) paradigm, which combines thebenefits of FMs and Federated Learning (FL) to enable privacy-preserving andcollaborative learning across multiple end-users. We discuss the potentialbenefits and challenges of integrating FL into the lifespan of FMs, coveringpre-training, fine-tuning, and application. We further outline potential futureresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, andfederated prompt tuning, which allow the development of more personalized andcontext-aware models while ensuring data privacy. Moreover, we explore thepossibility of continual/lifelong learning in FFMs, as increased computationalpower at the edge may unlock the potential for optimizing FMs using newlygenerated private data close to the data source. The proposed FFM conceptsoffer a flexible and scalable framework for training large language models in aprivacy-preserving manner, setting the stage for subsequent advancements inboth FM training and federated learning.\rStepping out of Flatland: Discovering Behavior Patterns as Topological Structures in Cyber Hypergraphs\nHelen Jenne Sinan G. Aksoy Daniel Best Alyson Bittner Gregory Henselman-Petrusek Cliff Joslyn Bill Kay Audun Myers Garret Seppala Jackson Warley Stephen J. Young Emilie Purvine\nabstract\rabstract: Data breaches and ransomware attacks occur so often that they have becomepart of our daily news cycle. This is due to a myriad of factors, including theincreasing number of internet-of-things devices, shift to remote work duringthe pandemic, and advancement in adversarial techniques, which all contributeto the increase in both the complexity of data captured and the challenge ofprotecting our networks. At the same time, cyber research has made strides,leveraging advances in machine learning and natural language processing tofocus on identifying sophisticated attacks that are known to evade conventionalmeasures. While successful, the shortcomings of these methods, particularly thelack of interpretability, are inherent and difficult to overcome. Consequently,there is an ever-increasing need to develop new tools for analyzing cyber datato enable more effective attack detection. In this paper, we present a novelframework based in the theory of hypergraphs and topology to understand datafrom cyber networks through topological signatures, which are both flexible andcan be traced back to the log data. While our approach\u0026rsquo;s mathematical groundingrequires some technical development, this pays off in interpretability, whichwe will demonstrate with concrete examples in a large-scale cyber networkdataset. These examples are an introduction to the broader possibilities thatlie ahead; our goal is to demonstrate the value of applying methods from theburgeoning fields of hypernetwork science and applied topology to understandrelationships among behaviors in cyber data.\r2023-11-07\nP-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models\nHaoran Li Dadi Guo Donghao Li Wei Fan Qi Hu Xin Liu Chunkit Chan Duanyi Yao Yangqiu Song\nabstract\rabstract: The rapid development of language models (LMs) brings unprecedentedaccessibility and usage for both models and users. On the one hand, powerfulLMs, trained with massive textual data, achieve state-of-the-art performanceover numerous downstream NLP tasks. On the other hand, more and more attentionis paid to unrestricted model accesses that may bring malicious privacy risksof data leakage. To address these issues, many recent works proposeprivacy-preserving language models (PPLMs) with differential privacy (DP).Unfortunately, different DP implementations make it challenging for a faircomparison among existing PPLMs. In this paper, we present P-Bench, amulti-perspective privacy evaluation benchmark to empirically and intuitivelyquantify the privacy leakage of LMs. Instead of only protecting and measuringthe privacy of protected data with DP parameters, P-Bench sheds light on theneglected inference data privacy during actual usage. P-Bench first clearlydefines multi-faceted privacy objectives during private fine-tuning. Then,P-Bench constructs a unified pipeline to perform private fine-tuning. Lastly,P-Bench performs existing privacy attacks on LMs with pre-defined privacyobjectives as the empirical evaluation results. The empirical attack resultsare used to fairly and intuitively evaluate the privacy leakage of variousPPLMs. We conduct extensive experiments on three datasets of GLUE formainstream LMs.\rLoss Balancing for Fair Supervised Learning\nMohammad Mahdi Khalili Xueru Zhang Mahed Abroshan\nabstract\rabstract: Supervised learning models have been used in various domains such as lending,college admission, face recognition, natural language processing, etc. However,they may inherit pre-existing biases from training data and exhibitdiscrimination against protected social groups. Various fairness notions havebeen proposed to address unfairness issues. In this work, we focus on EqualizedLoss (EL), a fairness notion that requires the expected loss to be(approximately) equalized across different groups. Imposing EL on the learningprocess leads to a non-convex optimization problem even if the loss function isconvex, and the existing fair learning algorithms cannot properly be adopted tofind the fair predictor under the EL constraint. This paper introduces analgorithm that can leverage off-the-shelf convex programming tools (e.g.,CVXPY) to efficiently find the global optimum of this non-convex optimization.In particular, we propose the ELminimizer algorithm, which finds the optimalfair predictor under EL by reducing the non-convex optimization to a sequenceof convex optimization problems. We theoretically prove that our algorithmfinds the global optimal solution under certain conditions. Then, we supportour theoretical results through several empirical studies.\r2023-11-06\nFindings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs\nLongyue Wang Zhaopeng Tu Yan Gu Siyou Liu Dian Yu Qingsong Ma Chenyang Lyu Liting Zhou Chao-Hong Liu Yufeng Ma Weiyu Chen Yvette Graham Bonnie Webber Philipp Koehn Andy Way Yulin Yuan Shuming Shi\nabstract\rabstract: Translating literary works has perennially stood as an elusive dream inmachine translation (MT), a journey steeped in intricate challenges. To fosterprogress in this domain, we hold a new shared task at WMT 2023, the firstedition of the Discourse-Level Literary Translation. First, we (Tencent AI Laband China Literature Ltd.) release a copyrighted and document-levelChinese-English web novel corpus. Furthermore, we put forth anindustry-endorsed criteria to guide human evaluation process. This year, wetotally received 14 submissions from 7 academia and industry teams. We employboth automatic and human evaluations to measure the performance of thesubmitted systems. The official ranking of the systems is based on the overallhuman judgments. In addition, our extensive analysis reveals a series ofinteresting findings on literary and discourse-aware MT. We release data,system outputs, and leaderboard athttp://www2.statmt.org/wmt23/literary-translation-task.html.\r2023-11-05\nPyclipse, a library for deidentification of free-text clinical notes\nCallandra Moore Jonathan Ranisau Walter Nelson Jeremy Petch Alistair Johnson\nabstract\rabstract: Automated deidentification of clinical text data is crucial due to the highcost of manual deidentification, which has been a barrier to sharing clinicaltext and the advancement of clinical natural language processing. However,creating effective automated deidentification tools faces several challenges,including issues in reproducibility due to differences in text processing,evaluation methods, and a lack of consistency across clinical domains andinstitutions. To address these challenges, we propose the pyclipse framework, aunified and configurable evaluation procedure to streamline the comparison ofdeidentification algorithms. Pyclipse serves as a single interface for runningopen-source deidentification algorithms on local clinical data, allowing forcontext-specific evaluation. To demonstrate the utility of pyclipse, we comparesix deidentification algorithms across four public and two private clinicaltext datasets. We find that algorithm performance consistently falls short ofthe results reported in the original papers, even when evaluated on the samebenchmark dataset. These discrepancies highlight the complexity of accuratelyassessing and comparing deidentification algorithms, emphasizing the need for areproducible, adjustable, and extensible framework like pyclipse. Our frameworklays the foundation for a unified approach to evaluate and improvedeidentification tools, ultimately enhancing patient protection in clinicalnatural language processing.\rQuantifying and Analyzing Entity-level Memorization in Large Language Models\nZhenhong Zhou Jiuyang Xiang Chaomeng Chen Sen Su\nabstract\rabstract: Large language models (LLMs) have been proven capable of memorizing theirtraining data, which can be extracted through specifically designed prompts. Asthe scale of datasets continues to grow, privacy risks arising frommemorization have attracted increasing attention. Quantifying language modelmemorization helps evaluate potential privacy risks. However, prior works onquantifying memorization require access to the precise original data or incursubstantial computational overhead, making it difficult for applications inreal-world language models. To this end, we propose a fine-grained,entity-level definition to quantify memorization with conditions and metricscloser to real-world scenarios. In addition, we also present an approach forefficiently extracting sensitive entities from autoregressive language models.We conduct extensive experiments based on the proposed, probing languagemodels\u0026rsquo; ability to reconstruct sensitive entities under different settings. Wefind that language models have strong memorization at the entity level and areable to reproduce the training data even with partial leakages. The resultsdemonstrate that LLMs not only memorize their training data but also understandassociations between entities. These findings necessitate that trainers of LLMsexercise greater prudence regarding model memorization, adopting memorizationmitigation techniques to preclude privacy violations.\rTag Your Fish in the Broken Net: A Responsible Web Framework for Protecting Online Privacy and Copyright\nDawen Zhang Boming Xia Yue Liu Xiwei Xu Thong Hoang Zhenchang Xing Mark Staples Qinghua Lu Liming Zhu\nabstract\rabstract: The World Wide Web, a ubiquitous source of information, serves as a primaryresource for countless individuals, amassing a vast amount of data from globalinternet users. However, this online data, when scraped, indexed, and utilizedfor activities like web crawling, search engine indexing, and, notably, AImodel training, often diverges from the original intent of its contributors.The ascent of Generative AI has accentuated concerns surrounding data privacyand copyright infringement. Regrettably, the web\u0026rsquo;s current framework fallsshort in facilitating pivotal actions like consent withdrawal or data copyrightclaims. While some companies offer voluntary measures, such as crawler accessrestrictions, these often remain inaccessible to individual users. To empoweronline users to exercise their rights and enable companies to adhere toregulations, this paper introduces a user-controlled consent tagging frameworkfor online data. It leverages the extensibility of HTTP and HTML in conjunctionwith the decentralized nature of distributed ledger technology. With thisframework, users have the ability to tag their online data at the time oftransmission, and subsequently, they can track and request the withdrawal ofconsent for their data from the data holders. A proof-of-concept system isimplemented, demonstrating the feasibility of the framework. This work holdssignificant potential for contributing to the reinforcement of user consent,privacy, and copyright on the modern internet and lays the groundwork forfuture insights into creating a more responsible and user-centric webecosystem.\rDomain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand\nJunfeng Guo Yiming Li Lixu Wang Shu-Tao Xia Heng Huang Cong Liu Bo Li\nabstract\rabstract: The prosperity of deep neural networks (DNNs) is largely benefited fromopen-source datasets, based on which users can evaluate and improve theirmethods. In this paper, we revisit backdoor-based dataset ownershipverification (DOV), which is currently the only feasible approach to protectthe copyright of open-source datasets. We reveal that these methods arefundamentally harmful given that they could introduce maliciousmisclassification behaviors to watermarked DNNs by the adversaries. In thispaper, we design DOV from another perspective by making watermarked models(trained on the protected dataset) correctly classify some `hard\u0026rsquo; samples thatwill be misclassified by the benign model. Our method is inspired by thegeneralization property of DNNs, where we find a \\emph{hardly-generalizeddomain} for the original dataset (as its \\emph{domain watermark}). It can beeasily learned with the protected dataset containing modified samples.Specifically, we formulate the domain generation as a bi-level optimization andpropose to optimize a set of visually-indistinguishable clean-label modifieddata with similar effects to domain-watermarked samples from thehardly-generalized domain to ensure watermark stealthiness. We also design ahypothesis-test-guided ownership verification via our domain watermark andprovide the theoretical analyses of our method. Extensive experiments on threebenchmark datasets are conducted, which verify the effectiveness of our methodand its resistance to potential adaptive methods. The code for reproducing mainexperiments is available at\\url{https://github.com/JunfengGo/Domain-Watermark}.\r2023-11-03\nAutomating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models\nJake Chanenson Madison Pickering Noah Apthorpe\nabstract\rabstract: Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research.\rDetecting Pretraining Data from Large Language Models\nWeijia Shi Anirudh Ajith Mengzhou Xia Yangsibo Huang Daogao Liu Terra Blevins Danqi Chen Luke Zettlemoyer\nabstract\rabstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.\rAntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors\nYou-Ming Chang Chen Yeh Wei-Chen Chiu Ning Yu\nabstract\rabstract: Deep generative models can create remarkably photorealistic fake images whileraising concerns about misinformation and copyright infringement, known asdeepfake threats. Deepfake detection technique is developed to distinguishbetween real and fake images, where the existing methods typically trainclassifiers in the image domain or various feature domains. However, thegeneralizability of deepfake detection against emerging and more advancedgenerative models remains challenging. In this paper, inspired by the zero-shotadvantages of Vision-Language Models (VLMs), we propose a novel approach usingVLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfakedetection accuracy over unseen data. We formulate deepfake detection as avisual question answering problem, and tune soft prompts for InstructBLIP todistinguish a query image is real or fake. We conduct full-spectrum experimentson datasets from 3 held-in and 13 held-out generative models, covering moderntext-to-image generation, image editing and image attacks. Results demonstratethat (1) the deepfake detection accuracy can be significantly and consistentlyimproved (from 54.6% to 91.31%, in average accuracy over unseen data) usingpretrained vision-language models with prompt tuning; (2) our superiorperformance is at less cost of trainable parameters, resulting in an effectiveand efficient solution for deepfake detection. Code and models can be found athttps://github.com/nctu-eva-lab/AntifakePrompt.\rMARRS: Multimodal Reference Resolution System\nHalim Cagri Ates Shruti Bhargava Site Li Jiarui Lu Siddhardha Maddula Joel Ruben Antony Moniz Anil Kumar Nalamalapu Roman Hoang Nguyen Melis Ozyildirim Alkesh Patel Dhivya Piraviperumal Vincent Renkens Ankit Samal Thy Tran Bo-Hsiang Tseng Hong Yu Yuan Zhang Rong Zou\nabstract\rabstract: Successfully handling context is essential for any dialog understanding task.This context maybe be conversational (relying on previous user queries orsystem responses), visual (relying on what the user sees, for example, on theirscreen), or background (based on signals such as a ringing alarm or playingmusic). In this work, we present an overview of MARRS, or Multimodal ReferenceResolution System, an on-device framework within a Natural LanguageUnderstanding system, responsible for handling conversational, visual andbackground context. In particular, we present different machine learning modelsto enable handing contextual queries; specifically, one to enable referenceresolution, and one to handle context via query rewriting. We also describe howthese models complement each other to form a unified, coherent, lightweightsystem that can understand context while preserving user privacy.\r2023-11-02\nImproving Fairness using Vision-Language Driven Image Augmentation\nMoreno D\u0026rsquo;Incà Christos Tzelepis Ioannis Patras Nicu Sebe\nabstract\rabstract: Fairness is crucial when training a deep-learning discriminative model,especially in the facial domain. Models tend to correlate specificcharacteristics (such as age and skin color) with unrelated attributes(downstream tasks), resulting in biases which do not correspond to reality. Itis common knowledge that these correlations are present in the data and arethen transferred to the models during training. This paper proposes a method tomitigate these correlations to improve fairness. To do so, we learninterpretable and meaningful paths lying in the semantic space of a pre-traineddiffusion model (DiffAE) \u0026ndash; such paths being supervised by contrastive textdipoles. That is, we learn to edit protected characteristics (age and skincolor). These paths are then applied to augment images to improve the fairnessof a given dataset. We test the proposed method on CelebA-HQ and UTKFace onseveral downstream tasks with age and skin color as protected characteristics.As a proxy for fairness, we compute the difference in accuracy with respect tothe protected characteristics. Quantitative results show how the augmentedimages help the model improve the overall accuracy, the aforementioned metric,and the disparity of equal opportunity. Code is available at:https://github.com/Moreno98/Vision-Language-Bias-Control.\rFVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation\nYan Wang Jian Cheng Yixin Chen Shuai Shao Lanyun Zhu Zhenzhou Wu Tao Liu Haogang Zhu\nabstract\rabstract: Medical image segmentation methods normally perform poorly when there is adomain shift between training and testing data. Unsupervised Domain Adaptation(UDA) addresses the domain shift problem by training the model using bothlabeled data from the source domain and unlabeled data from the target domain.Source-Free UDA (SFUDA) was recently proposed for UDA without requiring thesource data during the adaptation, due to data privacy or data transmissionissues, which normally adapts the pre-trained deep model in the testing stage.However, in real clinical scenarios of medical image segmentation, the trainedmodel is normally frozen in the testing stage. In this paper, we proposeFourier Visual Prompting (FVP) for SFUDA of medical image segmentation.Inspired by prompting learning in natural language processing, FVP steers thefrozen pre-trained model to perform well in the target domain by adding avisual prompt to the input target data. In FVP, the visual prompt isparameterized using only a small amount of low-frequency learnable parametersin the input frequency space, and is learned by minimizing the segmentationloss between the predicted segmentation of the prompted target image andreliable pseudo segmentation label of the target image under the frozen model.To our knowledge, FVP is the first work to apply visual prompts to SFUDA formedical image segmentation. The proposed FVP is validated using three publicdatasets, and experiments demonstrate that FVP yields better segmentationresults, compared with various existing methods.\rInclusiveness Matters: A Large-Scale Analysis of User Feedback\nNowshin Nawar Arony Ze Shi Li Bowen Xu Daniela Damian\nabstract\rabstract: In an era of rapidly expanding software usage, catering to the diverse needsof users from various backgrounds has become a critical challenge.Inclusiveness, representing a core human value, is frequently overlooked duringsoftware development, leading to user dissatisfaction. Users often engage indiscourse on online platforms where they indicate their concerns. In thisstudy, we leverage user feedback from three popular online sources, Reddit,Google Play Store, and Twitter, for 50 of the most popular apps in the world toreveal the inclusiveness-related concerns from end users. Using aSocio-Technical Grounded Theory approach, we analyzed 23,107 posts across thethree sources and identified 1,211 inclusiveness related posts. We organize ourempirical results in a taxonomy for inclusiveness comprising 6 majorcategories: Fairness, Technology, Privacy, Demography, Usability, and OtherHuman Values. To explore automated support to identifying inclusiveness-relatedposts, we experimented with five state-of-the-art pre-trained large languagemodels (LLMs) and found that these models\u0026rsquo; effectiveness is high and yet varieddepending on the data source. GPT-2 performed best on Reddit, BERT on theGoogle Play Store, and BART on Twitter. Our study provides an in-depth view ofinclusiveness-related user feedback from most popular apps and online sources.We provide implications and recommendations that can be used to bridge the gapbetween user expectations and software so that software developers can resonatewith the varied and evolving needs of the wide spectrum of users.\r2023-11-01\nTowards Legally Enforceable Hate Speech Detection for Public Forums\nChu Fei Luo Rohan Bhambhoria Xiaodan Zhu Samuel Dahan\nabstract\rabstract: Hate speech causes widespread and deep-seated societal issues. Properenforcement of hate speech laws is key for protecting groups of people againstharmful and discriminatory language. However, determining what constitutes hatespeech is a complex task that is highly open to subjective interpretations.Existing works do not align their systems with enforceable definitions of hatespeech, which can make their outputs inconsistent with the goals of regulators.This research introduces a new perspective and task for enforceable hate speechdetection centred around legal definitions, and a dataset annotated onviolations of eleven possible definitions by legal experts. Given the challengeof identifying clear, legally enforceable instances of hate speech, we augmentthe dataset with expert-generated samples and an automatically mined challengeset. We experiment with grounding the model decision in these definitions usingzero-shot and few-shot prompting. We then report results on several largelanguage models (LLMs). With this task definition, automatic hate speechdetection can be more closely aligned to enforceable laws, and hence assist inmore rigorous enforcement of legal protections against harmful speech in publicforums.\rMulti-step Jailbreaking Privacy Attacks on ChatGPT\nHaoran Li Dadi Guo Wei Fan Mingshi Xu Jie Huang Fanpu Meng Yangqiu Song\nabstract\rabstract: With the rapid progress of large language models (LLMs), many downstream NLPtasks can be well solved given appropriate prompts. Though model developers andresearchers work hard on dialog safety to avoid generating harmful content fromLLMs, it is still challenging to steer AI-generated content (AIGC) for thehuman good. As powerful LLMs are devouring existing text data from variousdomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whetherthe private information is included in the training data and what privacythreats can these LLMs and their downstream applications bring. In this paper,we study the privacy threats from OpenAI\u0026rsquo;s ChatGPT and the New Bing enhanced byChatGPT and show that application-integrated LLMs may cause new privacythreats. To this end, we conduct extensive experiments to support our claimsand discuss LLMs\u0026rsquo; privacy implications.\rKnowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\nRan Xu Hejie Cui Yue Yu Xuan Kan Wenqi Shi Yuchen Zhuang Wei Jin Joyce Ho Carl Yang\nabstract\rabstract: Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.\r2023-10-31\nInvisible Watermarking for Audio Generation Diffusion Models\nXirong Cao Xiang Li Divyesh Jadav Yanzhao Wu Zhehui Chen Chen Zeng Wenqi Wei\nabstract\rabstract: Diffusion models have gained prominence in the image domain for theircapabilities in data generation and transformation, achieving state-of-the-artperformance in various tasks in both image and audio domains. In the rapidlyevolving field of audio-based machine learning, safeguarding model integrityand establishing data copyright are of paramount importance. This paperpresents the first watermarking technique applied to audio diffusion modelstrained on mel-spectrograms. This offers a novel approach to the aforementionedchallenges. Our model excels not only in benign audio generation, but alsoincorporates an invisible watermarking trigger mechanism for modelverification. This watermark trigger serves as a protective layer, enabling theidentification of model ownership and ensuring its integrity. Through extensiveexperiments, we demonstrate that invisible watermark triggers can effectivelyprotect against unauthorized modifications while maintaining high utility inbenign audio generation tasks.\rCocoon: Static Information Flow Control in Rust\nAda Barach Maxwell Taylor Vincent Beardsley Jacob Bambeck Michael D. Bond Zhiqiang Lin\nabstract\rabstract: Information flow control (IFC) ensures confidentiality by preventing secretvalues from affecting non-secret values. Existing language-level IFC approachesmodify the language and use non-standard compilation tools, impose run-timeoverhead, or report false leaks, all of which hinder adoption. This paperpresents Cocoon, a Rust library for static type-based IFC that uses theunmodified Rust language and compiler. The key insight of Cocoon lies inleveraging Rust\u0026rsquo;s type system and procedural macros to establish an effectsystem that allows applications to safely compute arbitrary functions on secretdata. We integrated Cocoon into two popular Rust programs, the Spotify TUIclient and Mozilla\u0026rsquo;s Servo browser engine, to protect a secret value in eachprogram. The results show that applications can be retrofitted to use Cocoonwith limited modifications, at least to protect a single value, with negligibleor nonexistent impacts on run-time and compile-time performance.\rUnlearn What You Want to Forget: Efficient Unlearning for LLMs\nJiaao Chen Diyi Yang\nabstract\rabstract: Large language models (LLMs) have achieved significant progress frompre-training on and memorizing a wide range of textual data, however, thisprocess might suffer from privacy issues and violations of data protectionregulations. As a result, the ability to easily remove data related toindividual users from such models while not deteriorating their predictivequality after the removal becomes increasingly important. To address theseissues, in this work, we propose an efficient unlearning framework that couldefficiently update LLMs without having to retrain the whole model after dataremovals, by introducing lightweight unlearning layers learned with a selectiveteacher-student objective into the transformers. In addition, we introduce afusion mechanism to effectively combine different unlearning layers that learnsto forget different sets of data to handle a sequence of forgetting operations.Experiments on classification and generation tasks demonstrate theeffectiveness of our proposed methods compared to the state-of-the-artbaselines.\rMaking Large Language Models Better Data Creators\nDong-Ho Lee Jay Pujara Mohit Sewak Ryen W. White Sujay Kumar Jauhar\nabstract\rabstract: Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world.\r2023-10-30\nLearnware: Small Models Do Big\nZhi-Hua Zhou Zhi-Hao Tan\nabstract\rabstract: There are complaints about current machine learning techniques such as therequirement of a huge amount of training data and proficient training skills,the difficulty of continual learning, the risk of catastrophic forgetting, theleaking of data privacy/proprietary, etc. Most research efforts have beenfocusing on one of those concerned issues separately, paying less attention tothe fact that most issues are entangled in practice. The prevailing big modelparadigm, which has achieved impressive results in natural language processingand computer vision applications, has not yet addressed those issues, whereasbecoming a serious source of carbon emissions. This article offers an overviewof the learnware paradigm, which attempts to enable users not need to buildmachine learning models from scratch, with the hope of reusing small models todo things even beyond their original purposes, where the key ingredient is thespecification which enables a trained model to be adequately identified toreuse according to the requirement of future users who know nothing about themodel in advance.\rKnowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks\nMinki Kang Seanie Lee Jinheon Baek Kenji Kawaguchi Sung Ju Hwang\nabstract\rabstract: Large Language Models (LLMs) have shown promising performance inknowledge-intensive reasoning tasks that require a compound understanding ofknowledge. However, deployment of the LLMs in real-world applications can bechallenging due to their high computational requirements and concerns on dataprivacy. Previous studies have focused on building task-specific small LanguageModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,these approaches are ill-suited for knowledge-intensive reasoning tasks due tothe limited capacity of small LMs in memorizing the knowledge required.Motivated by our theoretical analysis on memorization, we proposeKnowledge-Augmented Reasoning Distillation (KARD), a novel method thatfine-tunes small LMs to generate rationales obtained from LLMs with augmentedknowledge retrieved from an external knowledge base. Moreover, we furtherpropose a neural reranker to obtain documents relevant to rationale generation.We empirically show that KARD significantly improves the performance of smallT5 and GPT models on the challenging knowledge-intensive reasoning datasets,namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the250M T5 models achieve superior performance against the fine-tuned 3B models,having 12 times larger parameters, on both MedQA-USMLE and StrategyQAbenchmarks.\r2023-10-28\nFedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy\nJianqing Zhang Yang Hua Hao Wang Tao Song Zhengui Xue Ruhui Ma Haibing Guan\nabstract\rabstract: Recently, personalized federated learning (pFL) has attracted increasingattention in privacy protection, collaborative learning, and tacklingstatistical heterogeneity among clients, e.g., hospitals, mobile smartphones,etc. Most existing pFL methods focus on exploiting the global information andpersonalized information in the client-level model parameters while neglectingthat data is the source of these two kinds of information. To address this, wepropose the Federated Conditional Policy (FedCP) method, which generates aconditional policy for each sample to separate the global information andpersonalized information in its features and then processes them by a globalhead and a personalized head, respectively. FedCP is more fine-grained toconsider personalization in a sample-specific manner than existing pFL methods.Extensive experiments in computer vision and natural language processingdomains show that FedCP outperforms eleven state-of-the-art methods by up to6.69%. Furthermore, FedCP maintains its superiority when some clientsaccidentally drop out, which frequently happens in mobile settings. Our code ispublic at https://github.com/TsingZ0/FedCP.\rCOPF: Continual Learning Human Preference through Optimal Policy Fitting\nHan Zhang Lin Gui Yuanzhao Zhai Hui Wang Yu Lei Ruifeng Xu\nabstract\rabstract: The technique of Reinforcement Learning from Human Feedback (RLHF) is acommonly employed method to improve pre-trained Language Models (LM), enhancingtheir ability to conform to human preferences. Nevertheless, the currentRLHF-based LMs necessitate full retraining each time novel queries or feedbackare introduced, which becomes a challenging task because human preferences canvary between different domains or tasks. Retraining LMs poses practicaldifficulties in many real-world situations due to the significant time andcomputational resources required, along with concerns related to data privacy.To address this limitation, we propose a new method called Continual OptimalPolicy Fitting (COPF), in which we estimate a series of optimal policies usingthe Monte Carlo method, and then continually fit the policy sequence with thefunction regularization. COPF involves a single learning phase and doesn\u0026rsquo;tnecessitate complex reinforcement learning. Importantly, it shares thecapability with RLHF to learn from unlabeled data, making it flexible forcontinual preference learning. Our experimental results show that COPFoutperforms strong Continuous learning (CL) baselines when it comes toconsistently aligning with human preferences on different tasks and domains.\r2023-10-27\nSDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes\nAdam D. Lelkes Eric Loreaux Tal Schuster Ming-Jun Chen Alvin Rajkomar\nabstract\rabstract: Social and behavioral determinants of health (SDOH) play a significant rolein shaping health outcomes, and extracting these determinants from clinicalnotes is a first step to help healthcare providers systematically identifyopportunities to provide appropriate care and address disparities. Progress onusing NLP methods for this task has been hindered by the lack of high-qualitypublicly available labeled data, largely due to the privacy and regulatoryconstraints on the use of real patients\u0026rsquo; information. This paper introduces anew dataset, SDOH-NLI, that is based on publicly available notes and which werelease publicly. We formulate SDOH extraction as a natural language inference(NLI) task, and provide binary textual entailment labels obtained from humanraters for a cross product of a set of social history snippets as premises andSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks inthat our premises and hypotheses are obtained independently. We evaluate both\u0026quot;off-the-shelf\u0026quot; entailment models as well as models fine-tuned on our data, andhighlight the ways in which our dataset appears more challenging than commonlyused NLI datasets.\rCan LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory\nNiloofar Mireshghallah Hyunwoo Kim Xuhui Zhou Yulia Tsvetkov Maarten Sap Reza Shokri Yejin Choi\nabstract\rabstract: The interactive use of large language models (LLMs) in AI assistants (atwork, home, etc.) introduces a new set of inference-time privacy risks: LLMsare fed different types of information from multiple sources in their inputsand are expected to reason about what to share in their outputs, for whatpurpose and with whom, within a given context. In this work, we draw attentionto the highly critical yet overlooked notion of contextual privacy by proposingConfAIde, a benchmark designed to identify critical weaknesses in the privacyreasoning capabilities of instruction-tuned LLMs. Our experiments show thateven the most capable models such as GPT-4 and ChatGPT reveal privateinformation in contexts that humans would not, 39% and 57% of the time,respectively. This leakage persists even when we employ privacy-inducingprompts or chain-of-thought reasoning. Our work underscores the immediate needto explore novel inference-time privacy-preserving approaches, based onreasoning and theory of mind.\r2023-10-26\nPockEngine: Sparse and Efficient Fine-tuning in a Pocket\nLigeng Zhu Lanxiang Hu Ji Lin Wei-Chen Wang Wei-Ming Chen Chuang Gan Song Han\nabstract\rabstract: On-device learning and efficient fine-tuning enable continuous andprivacy-preserving customization (e.g., locally fine-tuning large languagemodels on personalized data). However, existing training frameworks aredesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) andlack the optimizations for learning on the edge, which faces challenges ofresource limitations and edge hardware diversity. We introduce PockEngine: atiny, sparse and efficient engine to enable fine-tuning on various edgedevices. PockEngine supports sparse backpropagation: it prunes the backwardgraph and sparsely updates the model with measured memory saving and latencyreduction while maintaining the model quality. Secondly, PockEngine iscompilation first: the entire training graph (including forward, backward andoptimization steps) is derived at compile-time, which reduces the runtimeoverhead and brings opportunities for graph transformations. PockEngine alsointegrates a rich set of training graph optimizations, thus can furtheraccelerate the training cost, including operator reordering and backendswitching. PockEngine supports diverse applications, frontends and hardwarebackends: it flexibly compiles and tunes models defined inPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. Weevaluated PockEngine on both vision models and large language models.PockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow(Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin).Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orinat 550 tokens/s, 7.9$\\times$ faster than the PyTorch.\rDocumentNet: Bridging the Data Gap in Document Pre-Training\nLijun Yu Jin Miao Xiaoyu Sun Jiayi Chen Alexander G. Hauptmann Hanjun Dai Wei Wei\nabstract\rabstract: Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER.\r2023-10-25\nPrivately Aligning Language Models with Reinforcement Learning\nFan Wu Huseyin A. Inan Arturs Backurs Varun Chandrasekaran Janardhan Kulkarni Robert Sim\nabstract\rabstract: Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.\rDual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping\nYunming Zhang Dengpan Ye Caiyun Xie Long Tang Chuanxi Chen Ziyi Liu Jiacheng Deng\nabstract\rabstract: The malicious applications of deep forgery, represented by face swapping,have introduced security threats such as misinformation dissemination andidentity fraud. While some research has proposed the use of robust watermarkingmethods to trace the copyright of facial images for post-event traceability,these methods cannot effectively prevent the generation of forgeries at thesource and curb their dissemination. To address this problem, we propose anovel comprehensive active defense mechanism that combines traceability andadversariality, called Dual Defense. Dual Defense invisibly embeds a singlerobust watermark within the target face to actively respond to sudden cases ofmalicious face swapping. It disrupts the output of the face swapping modelwhile maintaining the integrity of watermark information throughout the entiredissemination process. This allows for watermark extraction at any stage ofimage tracking for traceability. Specifically, we introduce a watermarkembedding network based on original-domain feature impersonation attack. Thisnetwork learns robust adversarial features of target facial images and embedswatermarks, seeking a well-balanced trade-off between watermark invisibility,adversariality, and traceability through perceptual adversarial encodingstrategies. Extensive experiments demonstrate that Dual Defense achievesoptimal overall defense success rates and exhibits promising universality inanti-face swapping tasks and dataset generalization ability. It maintainsimpressive adversariality and traceability in both original and robustsettings, surpassing current forgery defense methods that possess only one ofthese capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGDmethods.\rFedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning\nJaemin Shin Hyungjun Yoon Seungjoo Lee Sungjoon Park Yunxin Liu Jinho D. Choi Sung-Ju Lee\nabstract\rabstract: Psychiatrists diagnose mental disorders via the linguistic use of patients.Still, due to data privacy, existing passive mental health monitoring systemsuse alternative features such as activity, app usage, and location via mobiledevices. We propose FedTherapist, a mobile mental health monitoring system thatutilizes continuous speech and keyboard input in a privacy-preserving way viafederated learning. We explore multiple model designs by comparing theirperformance and overhead for FedTherapist to overcome the complex nature ofon-device language model training on smartphones. We further propose aContext-Aware Language Learning (CALL) methodology to effectively utilizesmartphones\u0026rsquo; large and noisy text for mental health signal sensing. OurIRB-approved evaluation of the prediction of self-reported depression, stress,anxiety, and mood from 46 participants shows higher accuracy of FedTherapistcompared with the performance with non-language features, achieving 0.15 AUROCimprovement and 8.21% MAE reduction.\rRCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models\nZefan Wang Zichuan Liu Yingying Zhang Aoxiao Zhong Lunting Fan Lingfei Wu Qingsong Wen\nabstract\rabstract: Large language model (LLM) applications in cloud root cause analysis (RCA)have been actively explored recently. However, current methods are stillreliant on manual workflow settings and do not unleash LLMs\u0026rsquo; decision-makingand environment interaction capabilities. We present RCAgent, a tool-augmentedLLM autonomous agent framework for practical and privacy-aware industrial RCAusage. Running on an internally deployed model rather than GPT families,RCAgent is capable of free-form data collection and comprehensive analysis withtools. Our framework combines a variety of enhancements, including a uniqueSelf-Consistency for action trajectories, and a suite of methods for contextmanagement, stabilization, and importing domain knowledge. Our experiments showRCAgent\u0026rsquo;s evident and consistent superiority over ReAct across all aspects ofRCA \u0026ndash; predicting root causes, solutions, evidence, and responsibilities \u0026ndash; andtasks covered or uncovered by current rules, as validated by both automatedmetrics and human evaluations. Furthermore, RCAgent has already been integratedinto the diagnosis and issue discovery workflow of the Real-time ComputePlatform for Apache Flink of Alibaba Cloud.\r2023-10-24\nFLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering\nMd Rafi Ur Rashid Vishnu Asutosh Dasu Kang Gu Najrin Sultana Shagufta Mehnaz\nabstract\rabstract: Federated learning (FL) is becoming a key component in many technology-basedapplications including language modeling \u0026ndash; where individual FL participantsoften have privacy-sensitive text data in their local datasets. However,realizing the extent of privacy leakage in federated language models is notstraightforward and the existing attacks only intend to extract data regardlessof how sensitive or naive it is. To fill this gap, in this paper, we introducetwo novel findings with regard to leaking privacy-sensitive user data fromfederated language models. Firstly, we make a key observation that modelsnapshots from the intermediate rounds in FL can cause greater privacy leakagethan the final trained model. Secondly, we identify that privacy leakage can beaggravated by tampering with a model\u0026rsquo;s selective weights that are specificallyresponsible for memorizing the sensitive training data. We show how a maliciousclient can leak the privacy-sensitive data of some other user in FL evenwithout any cooperation from the server. Our best-performing method improvesthe membership inference recall by 29% and achieves up to 70% private datareconstruction, evidently outperforming existing attacks with strongerassumptions of adversary capabilities.\rSoK: Memorization in General-Purpose Large Language Models\nValentin Hartmann Anshuman Suri Vincent Bindschaedler David Evans Shruti Tople Robert West\nabstract\rabstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.\rCreating a silver standard for patent simplification\nSilvia Casola Alberto Lavelli Horacio Saggion\nabstract\rabstract: Patents are legal documents that aim at protecting inventions on the one handand at making technical knowledge circulate on the other. Their complex style\u0026ndash; a mix of legal, technical, and extremely vague language \u0026ndash; makes theircontent hard to access for humans and machines and poses substantial challengesto the information retrieval community. This paper proposes an approach toautomatically simplify patent text through rephrasing. Since no in-domainparallel simplification data exist, we propose a method to automaticallygenerate a large-scale silver standard for patent sentences. To obtaincandidates, we use a general-domain paraphrasing system; however, the processis error-prone and difficult to control. Thus, we pair it with proper filtersand construct a cleaner corpus that can successfully be used to train asimplification system. Human evaluation of the synthetic silver corpus showsthat it is considered grammatical, adequate, and contains simple sentences.\rCRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model\nKaiyan Zhang Ning Ding Biqing Qi Xuekai Zhu Xinwei Long Bowen Zhou\nabstract\rabstract: Instruction tuning has recently been recognized as an effective way ofaligning Large Language Models (LLMs) to enhance their generalization abilityacross various tasks. However, when tuning publicly accessible, centralizedLLMs with private instruction data, privacy concerns are inevitable. Whiledirect transfer of parameterized modules between models is a plausible approachto address this, its implications and effectiveness need further exploration.This paper focuses on Offsite-Tuning (OFT), a representative technique thattransfers transformer blocks between centralized LLMs and downstream emulators.Given the limited understanding of the underlying mechanism of OFT, we performan empirical analysis on LLMs from the perspectives of representation andfunctional similarity. Interestingly, our findings reveal a unique modularstructure within the layers of LLMs that appears to emerge as the model sizeexpands. Simultaneously, we note subtle but potentially significant changes inrepresentation and intermediate predictions across the layers. Inspired bythese observations, we propose CRaSh, involving Clustering, Removing, andSharing, a training-free strategy to derive improved emulators from LLMs. CRaShsignificantly boosts performance of OFT with billions of parameters.Furthermore, we investigate the optimal solutions yielded by fine-tuning withand without full model through the lens of loss landscape. Our findingsdemonstrate a linear connectivity among these optima falling over the samebasin, thereby highlighting the effectiveness of CRaSh and OFT. The source codeis publicly available at https://github.com/TsinghuaC3I/CRaSh.\rThe Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks\nXiaoyi Chen Siyuan Tang Rui Zhu Shijun Yan Lei Jin Zihao Wang Liya Su XiaoFeng Wang Haixu Tang\nabstract\rabstract: The era post-2018 marked the advent of Large Language Models (LLMs), withinnovations such as OpenAI\u0026rsquo;s ChatGPT showcasing prodigious linguistic prowess.As the industry galloped toward augmenting model parameters and capitalizing onvast swaths of human language data, security and privacy challenges alsoemerged. Foremost among these is the potential inadvertent accrual of PersonalIdentifiable Information (PII) during web-based data acquisition, posing risksof unintended PII disclosure. While strategies like RLHF during training andCatastrophic Forgetting have been marshaled to control the risk of privacyinfringements, recent advancements in LLMs, epitomized by OpenAI\u0026rsquo;s fine-tuninginterface for GPT-3.5, have reignited concerns. One may ask: can thefine-tuning of LLMs precipitate the leakage of personal information embeddedwithin training datasets? This paper reports the first endeavor to seek theanswer to the question, particularly our discovery of a new LLM exploitationavenue, called the Janus attack. In the attack, one can construct a PIIassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,to potentially reinstate and reveal concealed PIIs. Our findings indicate that,with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition frombeing impermeable to PII extraction to a state where they divulge a substantialproportion of concealed PII. This research, through its deep dive into theJanus attack vector, underscores the imperative of navigating the intricateinterplay between LLM utility and privacy preservation.\rCounter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think \u0026ndash; Introducing AI Detectability Index\nMegha Chakraborty S. M Towhidul Islam Tonmoy S M Mehedi Zaman Krish Sharma Niyar R Barman Chandan Gupta Shreya Gautam Tanay Kumar Vinija Jain Aman Chadha Amit P. Sheth Amitava Das\nabstract\rabstract: With the rise of prolific ChatGPT, the risk and consequences of AI-generatedtext has increased alarmingly. To address the inevitable question of ownershipattribution for AI-generated artifacts, the US Copyright Office released astatement stating that \u0026lsquo;If a work\u0026rsquo;s traditional elements of authorship wereproduced by a machine, the work lacks human authorship and the Office will notregister it\u0026rsquo;. Furthermore, both the US and the EU governments have recentlydrafted their initial proposals regarding the regulatory framework for AI.Given this cynosural spotlight on generative AI, AI-generated text detection(AGTD) has emerged as a topic that has already received immediate attention inresearch, with some initial methods having been proposed, soon followed byemergence of techniques to bypass detection. This paper introduces the CounterTuring Test (CT^2), a benchmark consisting of techniques aiming to offer acomprehensive evaluation of the robustness of existing AGTD techniques. Ourempirical findings unequivocally highlight the fragility of the proposed AGTDmethods under scrutiny. Amidst the extensive deliberations on policy-making forregulating AI development, it is of utmost importance to assess thedetectability of content generated by LLMs. Thus, to establish a quantifiablespectrum facilitating the evaluation and ranking of LLMs according to theirdetectability levels, we propose the AI Detectability Index (ADI). We conduct athorough examination of 15 contemporary LLMs, empirically demonstrating thatlarger LLMs tend to have a higher ADI, indicating they are less detectablecompared to smaller LLMs. We firmly believe that ADI holds significant value asa tool for the wider NLP community, with the potential to serve as a rubric inAI-related policy-making.\r2023-10-23\nHealth Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model\nYohn Jairo Parra Bautista Vinicious Lima Carlos Theran Richard Alo\nabstract\rabstract: Health disparities are differences in health outcomes and access tohealthcare between different groups, including racial and ethnic minorities,low-income people, and rural residents. An artificial intelligence (AI) programcalled large language models (LLMs) can understand and generate human language,improving health communication and reducing health disparities. There are manychallenges in using LLMs in human-doctor interaction, including the need fordiverse and representative data, privacy concerns, and collaboration betweenhealthcare providers and technology experts. We introduce the comparativeinvestigation of domain-specific large language models such as SciBERT with amulti-purpose LLMs BERT. We used cosine similarity to analyze text queriesabout health disparities in exam rooms when factors such as race are usedalone. Using text queries, SciBERT fails when it doesn\u0026rsquo;t differentiate betweenqueries text: \u0026ldquo;race\u0026rdquo; alone and \u0026ldquo;perpetuates health disparities.\u0026rdquo; We believeclinicians can use generative AI to create a draft response when communicatingasynchronously with patients. However, careful attention must be paid to ensurethey are developed and implemented ethically and equitably.\rDid the Neurons Read your Book? Document-level Membership Inference for Large Language Models\nMatthieu Meeus Shubham Jain Marek Rei Yves-Alexandre de Montjoye\nabstract\rabstract: With large language models (LLMs) poised to become embedded in our dailylives, questions are starting to be raised about the dataset(s) they learnedfrom. These questions range from potential bias or misinformation LLMs couldretain from their training data to questions of copyright and fair use ofhuman-generated text. However, while these questions emerge, developers of therecent state-of-the-art LLMs become increasingly reluctant to disclose detailson their training corpus. We here introduce the task of document-levelmembership inference for real-world LLMs, i.e. inferring whether the LLM hasseen a given document during training or not. First, we propose a procedure forthe development and evaluation of document-level membership inference for LLMsby leveraging commonly used data sources for training and the model releasedate. We then propose a practical, black-box method to predict document-levelmembership and instantiate it on OpenLLaMA-7B with both books and academicpapers. We show our methodology to perform very well, reaching an impressiveAUC of 0.856 for books and 0.678 for papers. We then show our approach tooutperform the sentence-level membership inference attacks used in the privacyliterature for the document-level membership task. We finally evaluate whethersmaller models might be less sensitive to document-level inference and showOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.Taken together, our results show that accurate document-level membership can beinferred for LLMs, increasing the transparency of technology poised to changeour lives.\rTowards LLM-driven Dialogue State Tracking\nYujie Feng Zexin Lu Bo Liu Liming Zhan Xiao-Ming Wu\nabstract\rabstract: Dialogue State Tracking (DST) is of paramount importance in ensuring accuratetracking of user goals and system actions within task-oriented dialoguesystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPThas sparked considerable interest in assessing their efficacy across diverseapplications. In this study, we conduct an initial examination of ChatGPT\u0026rsquo;scapabilities in DST. Our evaluation uncovers the exceptional performance ofChatGPT in this task, offering valuable insights to researchers regarding itscapabilities and providing useful directions for designing and enhancingdialogue systems. Despite its impressive performance, ChatGPT has significantlimitations including its closed-source nature, request restrictions, raisingdata privacy concerns, and lacking local deployment capabilities. To addressthese concerns, we present LDST, an LLM-driven DST framework based on smaller,open-source foundation models. By utilizing a novel domain-slot instructiontuning method, LDST achieves performance on par with ChatGPT. Comprehensiveevaluations across three distinct experimental settings, we find that LDSTexhibits remarkable performance improvements in both zero-shot and few-shotsetting compared to previous SOTA methods. The source code is provided forreproducibility.\rDifferentially Private Natural Language Models: Recent Advances and Future Directions\nLijie Hu Ivan Habernal Lei Shen Di Wang\nabstract\rabstract: Recent developments in deep learning have led to great success in variousnatural language processing (NLP) tasks. However, these applications mayinvolve data that contain sensitive information. Therefore, how to achieve goodperformance while also protecting the privacy of sensitive data is a crucialchallenge in NLP. To preserve privacy, Differential Privacy (DP), which canprevent reconstruction attacks and protect against potential side knowledge, isbecoming a de facto technique for private data analysis. In recent years, NLPin DP models (DP-NLP) has been studied from different perspectives, whichdeserves a comprehensive review. In this paper, we provide the first systematicreview of recent advances in DP deep learning models in NLP. In particular, wefirst discuss some differences and additional challenges of DP-NLP comparedwith the standard DP deep learning. Then, we investigate some existing work onDP-NLP and present its recent developments from three aspects: gradientperturbation based methods, embedding vector perturbation based methods, andensemble model based methods. We also discuss some challenges and futuredirections.\rMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems\nJakub Macina Nico Daheim Sankalan Pal Chowdhury Tanmay Sinha Manu Kapur Iryna Gurevych Mrinmaya Sachan\nabstract\rabstract: While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly.\rH2O Open Ecosystem for State-of-the-art Large Language Models\nArno Candel Jon McKinney Philipp Singer Pascal Pfeiffer Maximilian Jeblick Chun Ming Lee Marcos V. Conde\nabstract\rabstract: Large Language Models (LLMs) represent a revolution in AI. However, they alsopose many significant risks, such as the presence of biased, private,copyrighted or harmful text. For this reason we need open, transparent and safesolutions. We introduce a complete open-source ecosystem for developing andtesting LLMs. The goal of this project is to boost open alternatives toclosed-source approaches. We release h2oGPT, a family of fine-tuned LLMs ofdiverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUIdesigned for efficient fine-tuning, evaluation, and deployment of LLMs usingthe most recent state-of-the-art techniques. Our code and models are fullyopen-source. We believe this work helps to boost AI development and make itmore accessible, efficient and trustworthy. The demo is available at:https://gpt.h2o.ai/\r$Λ$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative AI\nShoki Ohta Takayuki Nishio\nabstract\rabstract: In the wake of the burgeoning expansion of generative artificial intelligence(AI) services, the computational demands inherent to these technologiesfrequently necessitate cloud-powered computational offloading, particularly forresource-constrained mobile devices. These services commonly employ prompts tosteer the generative process, and both the prompts and the resultant content,such as text and images, may harbor privacy-sensitive or confidentialinformation, thereby elevating security and privacy risks. To mitigate theseconcerns, we introduce $\\Lambda$-Split, a split computing framework tofacilitate computational offloading while simultaneously fortifying dataprivacy against risks such as eavesdropping and unauthorized access. In$\\Lambda$-Split, a generative model, usually a deep neural network (DNN), ispartitioned into three sub-models and distributed across the user\u0026rsquo;s localdevice and a cloud server: the input-side and output-side sub-models areallocated to the local, while the intermediate, computationally-intensivesub-model resides on the cloud server. This architecture ensures that only thehidden layer outputs are transmitted, thereby preventing the externaltransmission of privacy-sensitive raw input and output data. Given theblack-box nature of DNNs, estimating the original input or output fromintercepted hidden layer outputs poses a significant challenge for maliciouseavesdroppers. Moreover, $\\Lambda$-Split is orthogonal to traditionalencryption-based security mechanisms, offering enhanced security when deployedin conjunction. We empirically validate the efficacy of the $\\Lambda$-Splitframework using Llama 2 and Stable Diffusion XL, representative large languageand diffusion models developed by Meta and Stability AI, respectively. Our$\\Lambda$-Split implementation is publicly accessible athttps://github.com/nishio-laboratory/lambda_split.\r2023-10-22\nCOFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation\nNan Wang Qifan Wang Yi-Chia Wang Maziar Sanjabi Jingzhou Liu Hamed Firooz Hongning Wang Shaoliang Nie\nabstract\rabstract: As language models become increasingly integrated into our digital lives,Personalized Text Generation (PTG) has emerged as a pivotal component with awide range of applications. However, the bias inherent in user written text,often used for PTG model training, can inadvertently associate different levelsof linguistic quality with users\u0026rsquo; protected attributes. The model can inheritthe bias and perpetuate inequality in generating text w.r.t. users\u0026rsquo; protectedattributes, leading to unfair treatment when serving users. In this work, weinvestigate fairness of PTG in the context of personalized explanationgeneration for recommendations. We first discuss the biases in generatedexplanations and their fairness implications. To promote fairness, we introducea general framework to achieve measure-specific counterfactual fairness inexplanation generation. Extensive experiments and human evaluations demonstratethe effectiveness of our method.\rText-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning\nShengfang Zhai Yinpeng Dong Qingni Shen Shi Pu Yuejian Fang Hang Su\nabstract\rabstract: With the help of conditioning mechanisms, the state-of-the-art diffusionmodels have achieved tremendous success in guided image generation,particularly in text-to-image synthesis. To gain a better understanding of thetraining process and potential risks of text-to-image synthesis, we perform asystematic investigation of backdoor attack on text-to-image diffusion modelsand propose BadT2I, a general multimodal backdoor attack framework that tamperswith image synthesis in diverse semantic levels. Specifically, we performbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, ourmethods efficiently inject backdoors into a large-scale text-to-image diffusionmodel while preserving its utility with benign inputs. We conduct empiricalexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,demonstrating that the large-scale diffusion model can be easily backdooredwithin a few fine-tuning steps. We conduct additional experiments to explorethe impact of different types of textual triggers, as well as the backdoorpersistence during further training, providing insights for the development ofbackdoor defense methods. Besides, our investigation may contribute to thecopyright protection of text-to-image models in the future.\rNeural Text Sanitization with Privacy Risk Indicators: An Empirical Analysis\nAnthi Papadopoulou Pierre Lison Mark Anderson Lilja Øvrelid Ildikó Pilán\nabstract\rabstract: Text sanitization is the task of redacting a document to mask all occurrencesof (direct or indirect) personal identifiers, with the goal of concealing theidentity of the individual(s) referred in it. In this paper, we consider atwo-step approach to text sanitization and provide a detailed analysis of itsempirical performance on two recently published datasets: the TextAnonymization Benchmark (Pil'an et al., 2022) and a collection of Wikipediabiographies (Papadopoulou et al., 2022). The text sanitization process startswith a privacy-oriented entity recognizer that seeks to determine the textspans expressing identifiable personal information. This privacy-orientedentity recognizer is trained by combining a standard named entity recognitionmodel with a gazetteer populated by person-related terms extracted fromWikidata. The second step of the text sanitization process consists inassessing the privacy risk associated with each detected text span, eitherisolated or in combination with other text spans. We present five distinctindicators of the re-identification risk, respectively based on language modelprobabilities, text span classification, sequence labelling, perturbations, andweb search. We provide a contrastive analysis of each privacy indicator andhighlight their benefits and limitations, notably in relation to the availablelabeled data.\rDemocratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models\nSumuk Shashidhar Abhinav Chinta Vaibhav Sahai Zhenhailong Wang Heng Ji\nabstract\rabstract: The dominance of proprietary LLMs has led to restricted access and raisedinformation privacy concerns. High-performing open-source alternatives arecrucial for information-sensitive and high-volume applications but often lagbehind in performance. To address this gap, we propose (1) A untargeted variantof iterative self-critique and self-refinement devoid of external influence.(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score(PeRFICS) - to find the optimal model for a given task considering refinedperformance and cost. Our experiments show that SoTA open source models ofvarying sizes from 7B - 65B, on average, improve 8.2% from their baselineperformance. Strikingly, even models with extremely small memory footprints,such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%improvement in high-creativity, open ended tasks on the Vicuna benchmark.Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement.This work has profound implications for resource-constrained andinformation-sensitive environments seeking to leverage LLMs without incurringprohibitive costs, compromising on performance and privacy. The domain-agnosticself-refinement process coupled with our novel ranking metric facilitatesinformed decision-making in model selection, thereby reducing costs anddemocratizing access to high-performing language models, as evidenced by casestudies.\r2023-10-21\nTransductive Learning for Textual Few-Shot Classification in API-based Embedding Models\nPierre Colombo Victor Pellegrain Malik Boudiaf Victor Storchan Myriam Tami Ismail Ben Ayed Celine Hudelot Pablo Piantanida\nabstract\rabstract: Proprietary and closed APIs are becoming increasingly common to processnatural language, and are impacting the practical applications of naturallanguage processing, including few-shot classification. Few-shot classificationinvolves training a model to perform a new classification task with a handfulof labeled data. This paper presents three contributions. First, we introduce ascenario where the embedding of a pre-trained model is served through a gatedAPI with compute-cost and data-privacy constraints. Second, we propose atransductive inference, a learning paradigm that has been overlooked by the NLPcommunity. Transductive inference, unlike traditional inductive learning,leverages the statistics of unlabeled data. We also introduce a newparameter-free transductive regularizer based on the Fisher-Rao loss, which canbe used on top of the gated API embeddings. This method fully utilizesunlabeled data, does not share any label with the third-party API provider andcould serve as a baseline for future research. Third, we propose an improvedexperimental setting and compile a benchmark of eight datasets involvingmulticlass classification in four different languages, with up to 151 classes.We evaluate our methods using eight backbone models, along with an episodicevaluation over 1,000 episodes, which demonstrate the superiority oftransductive inference over the standard inductive setting.\r2023-10-20\nZero-Shot Sharpness-Aware Quantization for Pre-trained Language Models\nMiaoxi Zhu Qihuang Zhong Li Shen Liang Ding Juhua Liu Bo Du Dacheng Tao\nabstract\rabstract: Quantization is a promising approach for reducing memory overhead andaccelerating inference, especially in large pre-trained language model (PLM)scenarios. While having no access to original training data due to security andprivacy concerns has emerged the demand for zero-shot quantization. Most of thecutting-edge zero-shot quantization methods primarily 1) apply to computervision tasks, and 2) neglect of overfitting problem in the generativeadversarial learning process, leading to sub-optimal performance. Motivated bythis, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)framework for the zero-shot quantization of various PLMs. The key algorithm insolving ZSAQ is the SAM-SGA optimization, which aims to improve thequantization accuracy and model generalization via optimizing a minimaxproblem. We theoretically prove the convergence rate for the minimaxoptimization problem and this result can be applied to other nonconvex-PLminimax optimization frameworks. Extensive experiments on 11 tasks demonstratethat our method brings consistent and significant performance gains on bothdiscriminative and generative PLMs, i.e., up to +6.98 average score.Furthermore, we empirically validate that our method can effectively improvethe model generalization.\rAssessing Privacy Risks in Language Models: A Case Study on Summarization Tasks\nRuixiang Tang Gord Lueck Rodolfo Quispe Huseyin A Inan Janardhan Kulkarni Xia Hu\nabstract\rabstract: Large language models have revolutionized the field of NLP by achievingstate-of-the-art performance on various tasks. However, there is a concern thatthese models may disclose information in the training data. In this study, wefocus on the summarization task and investigate the membership inference (MI)attack: given a sample and black-box access to a model\u0026rsquo;s API, it is possible todetermine if the sample was part of the training data. We exploit textsimilarity and the model\u0026rsquo;s resistance to document modifications as potential MIsignals and evaluate their effectiveness on widely used datasets. Our resultsdemonstrate that summarization models are at risk of exposing data membership,even in cases where the reference summary is not available. Furthermore, wediscuss several safeguards for training summarization models to protect againstMI attacks and discuss the inherent trade-off between privacy and utility.\r2023-10-19\nTabuLa: Harnessing Language Models for Tabular Data Synthesis\nZilong Zhao Robert Birke Lydia Chen\nabstract\rabstract: Given the ubiquitous use of tabular data in industries and the growingconcerns in data privacy and security, tabular data synthesis emerges as acritical research area. The recent state-of-the-art methods show that largelanguage models (LLMs) can be adopted to generate realistic tabular data. AsLLMs pre-process tabular data as full text, they have the advantage of avoidingthe curse of dimensionality associated with one-hot encoding high-dimensionaldata. However, their long training time and limited re-usability on new tasksprevent them from replacing exiting tabular generative models. In this paper,we propose Tabula, a tabular data synthesizer based on the language modelstructure. Through Tabula, we demonstrate the inherent limitation of employingpre-trained language models designed for natural language processing (NLP) inthe context of tabular data synthesis. Our investigation delves into thedevelopment of a dedicated foundational model tailored specifically for tabulardata synthesis. Additionally, we propose a token sequence compression strategyto significantly reduce training time while preserving the quality of syntheticdata. Extensive experiments on six datasets demonstrate that using a languagemodel structure without loading the well-trained model weights yields a betterstarting model for tabular data synthesis. Moreover, the Tabula model,previously trained on other tabular data, serves as an excellent foundationmodel for new tabular data synthesis tasks. Additionally, the token sequencecompression method substantially reduces the model\u0026rsquo;s training time. Resultsshow that Tabula averagely reduces 46.2% training time per epoch comparing tocurrent LLMs-based state-of-the-art algorithm and consistently achieves evenhigher synthetic data utility.\rReliable and Efficient In-Memory Fault Tolerance of Large Language Model Pretraining\nYuxin Wang Shaohuai Shi Xin He Zhenheng Tang Xinglin Pan Yang Zheng Xiaoyu Wu Amelie Chi Zhou Bingsheng He Xiaowen Chu\nabstract\rabstract: Extensive system scales (i.e. thousands of GPU/TPUs) and prolonged trainingperiods (i.e. months of pretraining) significantly escalate the probability offailures when training large language models (LLMs). Thus, efficient andreliable fault-tolerance methods are in urgent need. Checkpointing is theprimary fault-tolerance method to periodically save parameter snapshots fromGPU memory to disks via CPU memory. In this paper, we identify the frequency ofexisting checkpoint-based fault-tolerance being significantly limited by thestorage I/O overheads, which results in hefty re-training costs on restartingfrom the nearest checkpoint. In response to this gap, we introduce an in-memoryfault-tolerance framework for large-scale LLM pretraining. The framework booststhe efficiency and reliability of fault tolerance from three aspects: (1)Reduced Data Transfer and I/O: By asynchronously caching parameters, i.e.,sharded model parameters, optimizer states, and RNG states, to CPU volatilememory, Our framework significantly reduces communication costs and bypassescheckpoint I/O. (2) Enhanced System Reliability: Our framework enhancesparameter protection with a two-layer hierarchy: snapshot management processes(SMPs) safeguard against software failures, together with Erasure Coding (EC)protecting against node failures. This double-layered protection greatlyimproves the survival probability of the parameters compared to existingcheckpointing methods. (3) Improved Snapshotting Frequency: Our frameworkachieves more frequent snapshotting compared with asynchronous checkpointingoptimizations under the same saving time budget, which improves the faulttolerance efficiency. Empirical results demonstrate that Our frameworkminimizes the overhead of fault tolerance of LLM pretraining by effectivelyleveraging redundant CPU resources.\rPrivacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework\nImdad Ullah Najm Hassan Sukhpal Singh Gill Basem Suleiman Tariq Ahamed Ahanger Zawar Shah Junaid Qadir Salil S. Kanhere\nabstract\rabstract: The generative Artificial Intelligence (AI) tools based on Large LanguageModels (LLMs) use billions of parameters to extensively analyse large datasetsand extract critical private information such as, context, specific details,identifying information etc. This have raised serious threats to user privacyand reluctance to use such tools. This article proposes the conceptual modelcalled PrivChatGPT, a privacy-preserving model for LLMs that consists of twomain components i.e., preserving user privacy during the datacuration/pre-processing together with preserving private context and theprivate training process for large-scale data. To demonstrate itsapplicability, we show how a private mechanism could be integrated into theexisting model for training LLMs to protect user privacy; specifically, weemployed differential privacy and private training using Reinforcement Learning(RL). We measure the privacy loss and evaluate the measure of uncertainty orrandomness once differential privacy is applied. It further recursivelyevaluates the level of privacy guarantees and the measure of uncertainty ofpublic database and resources, during each update when new information is addedfor training purposes. To critically evaluate the use of differential privacyfor private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,private information retrieval, randomisation, for various performance measuressuch as the model performance and accuracy, computational complexity, privacyvs. utility etc. We conclude that differential privacy, randomisation, andobfuscation can impact utility and performance of trained models, conversely,the use of ToR, Blockchain, and PIR may introduce additional computationalcomplexity and high training latency. We believe that the proposed model couldbe used as a benchmark for proposing privacy preserving LLMs for generative AItools.\rRed Teaming Language Model Detectors with Language Models\nZhouxing Shi Yihan Wang Fan Yin Xiangning Chen Kai-Wei Chang Cho-Jui Hsieh\nabstract\rabstract: The prevalence and strong capability of large language models (LLMs) presentsignificant safety and ethical risks if exploited by malicious users. Toprevent the potentially deceptive usage of LLMs, recent works have proposedalgorithms to detect LLM-generated text and protect LLMs. In this paper, weinvestigate the robustness and reliability of these LLM detectors underadversarial attacks. We study two types of attack strategies: 1) replacingcertain words in an LLM\u0026rsquo;s output with their synonyms given the context; 2)automatically searching for an instructional prompt to alter the writing styleof the generation. In both strategies, we leverage an auxiliary LLM to generatethe word replacements or the instructional prompt. Different from previousworks, we consider a challenging setting where the auxiliary LLM can also beprotected by a detector. Experiments reveal that our attacks effectivelycompromise the performance of all detectors in the study with plausiblegenerations, underscoring the urgent need to improve the robustness ofLLM-generated text detection systems.\rUnmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights\nYichuan Deng Zhao Song Shenghao Xie Chiwun Yang\nabstract\rabstract: In the realm of deep learning, transformers have emerged as a dominantarchitecture, particularly in natural language processing tasks. However, withtheir widespread adoption, concerns regarding the security and privacy of thedata processed by these models have arisen. In this paper, we address a pivotalquestion: Can the data fed into transformers be recovered using their attentionweights and outputs? We introduce a theoretical framework to tackle thisproblem. Specifically, we present an algorithm that aims to recover the inputdata $X \\in \\mathbb{R}^{d \\times n}$ from given attention weights $W = QK^\\top\\in \\mathbb{R}^{d \\times d}$ and output $B \\in \\mathbb{R}^{n \\times n}$ byminimizing the loss function $L(X)$. This loss function captures thediscrepancy between the expected output and the actual output of thetransformer. Our findings have significant implications for the LocalizedLayer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model\u0026rsquo;sdesign from a security and privacy perspective. This work underscores theimportance of understanding and safeguarding the internal workings oftransformers to ensure the confidentiality of processed data.\r2023-10-18\nBlack-Box Training Data Identification in GANs via Detector Networks\nLukman Olagoke Salil Vadhan Seth Neel\nabstract\rabstract: Since their inception Generative Adversarial Networks (GANs) have beenpopular generative models across images, audio, video, and tabular data. Inthis paper we study whether given access to a trained GAN, as well as freshsamples from the underlying distribution, if it is possible for an attacker toefficiently identify if a given point is a member of the GAN\u0026rsquo;s training data.This is of interest for both reasons related to copyright, where a user maywant to determine if their copyrighted data has been used to train a GAN, andin the study of data privacy, where the ability to detect training setmembership is known as a membership inference attack. Unlike the majority ofprior work this paper investigates the privacy implications of using GANs inblack-box settings, where the attack only has access to samples from thegenerator, rather than access to the discriminator as well. We introduce asuite of membership inference attacks against GANs in the black-box setting andevaluate our attacks on image GANs trained on the CIFAR10 dataset and tabularGANs trained on genomic data. Our most successful attack, called The Detector,involve training a second network to score samples based on their likelihood ofbeing generated by the GAN, as opposed to a fresh sample from the distribution.We prove under a simple model of the generator that the detector is anapproximately optimal membership inference attack. Across a wide range oftabular and image datasets, attacks, and GAN architectures, we find thatadversaries can orchestrate non-trivial privacy attacks when provided withaccess to samples from the generator. At the same time, the attack successachievable against GANs still appears to be lower compared to other generativeand discriminative models; this leaves the intriguing open question of whetherGANs are in fact more private, or if it is a matter of developing strongerattacks.\rTo Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images \u0026hellip; For Now\nYimeng Zhang Jinghan Jia Xin Chen Aochuan Chen Yihua Zhang Jiancheng Liu Ke Ding Sijia Liu\nabstract\rabstract: The recent advances in diffusion models (DMs) have revolutionized thegeneration of complex and diverse images. However, these models also introducepotential safety hazards, such as the production of harmful content andinfringement of data copyrights. Although there have been efforts to createsafety-driven unlearning methods to counteract these challenges, doubts remainabout their capabilities. To bridge this uncertainty, we propose an evaluationframework built upon adversarial attacks (also referred to as adversarialprompts), in order to discern the trustworthiness of these safety-drivenunlearned DMs. Specifically, our research explores the (worst-case) robustnessof unlearned DMs in eradicating unwanted concepts, styles, and objects,assessed by the generation of adversarial prompts. We develop a noveladversarial learning approach called UnlearnDiff that leverages the inherentclassification capabilities of DMs to streamline the generation of adversarialprompts, making it as simple for DMs as it is for image classification attacks.This technique streamlines the creation of adversarial prompts, making theprocess as intuitive for generative modeling as it is for image classificationassaults. Through comprehensive benchmarking, we assess the unlearningrobustness of five prevalent unlearned DMs across multiple tasks. Our resultsunderscore the effectiveness and efficiency of UnlearnDiff when compared tostate-of-the-art adversarial prompting methods. Codes are available athttps://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: This papercontains model outputs that may be offensive in nature.\rEvaluating the Fairness of Discriminative Foundation Models in Computer Vision\nJunaid Ali Matthaeus Kleindessner Florian Wenzel Kailash Budhathoki Volkan Cevher Chris Russell\nabstract\rabstract: We propose a novel taxonomy for bias evaluation of discriminative foundationmodels, such as Contrastive Language-Pretraining (CLIP), that are used forlabeling tasks. We then systematically evaluate existing methods for mitigatingbias in these models with respect to our taxonomy. Specifically, we evaluateOpenAI\u0026rsquo;s CLIP and OpenCLIP models for key applications, such as zero-shotclassification, image retrieval and image captioning. We categorize desiredbehaviors based around three axes: (i) if the task concerns humans; (ii) howsubjective the task is (i.e., how likely it is that people from a diverse rangeof backgrounds would agree on a labeling); and (iii) the intended purpose ofthe task and if fairness is better served by impartiality (i.e., makingdecisions independent of the protected attributes) or representation (i.e.,making decisions to maximize diversity). Finally, we provide quantitativefairness evaluations for both binary-valued and multi-valued protectedattributes over ten diverse datasets. We find that fair PCA, a post-processingmethod for fair representations, works very well for debiasing in most of theaforementioned tasks while incurring only minor loss of performance. However,different debiasing approaches vary in their effectiveness depending on thetask. Hence, one should choose the debiasing approach depending on the specificuse case.\r2023-10-17\nLast One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning\nRui Wen Tianhao Wang Michael Backes Yang Zhang Ahmed Salem\nabstract\rabstract: Large Language Models (LLMs) are powerful tools for natural languageprocessing, enabling novel applications and user experiences. However, toachieve optimal performance, LLMs often require adaptation with private data,which poses privacy and security challenges. Several techniques have beenproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparativeprivacy and security properties have not been systematically investigated. Inthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICLagainst three types of well-established attacks: membership inference, whichexposes data leakage (privacy); backdoor, which injects malicious behavior(security); and model stealing, which can violate intellectual property(privacy and security). Our results show that there is no silver bullet forprivacy and security in LLM adaptation and each technique has differentstrengths and weaknesses.\rDisentangling the Linguistic Competence of Privacy-Preserving BERT\nStefan Arnold Nils Kemmerzell Annika Schreiner\nabstract\rabstract: Differential Privacy (DP) has been tailored to address the unique challengesof text-to-text privatization. However, text-to-text privatization is known fordegrading the performance of language models when trained on perturbed text.Employing a series of interpretation techniques on the internal representationsextracted from BERT trained on perturbed pre-text, we intend to disentangle atthe linguistic level the distortion induced by differential privacy.Experimental results from a representational similarity analysis indicate thatthe overall similarity of internal representations is substantially reduced.Using probing tasks to unpack this dissimilarity, we find evidence thattext-to-text privatization affects the linguistic competence across severalformalisms, encoding localized properties of words while falling short atencoding the contextual relationships between spans of words.\rWatermarking LLMs with Weight Quantization\nLinyang Li Botian Jiang Pengyu Wang Ke Ren Hang Yan Xipeng Qiu\nabstract\rabstract: Abuse of large language models reveals high risks as large language modelsare being deployed at an astonishing speed. It is important to protect themodel weights to avoid malicious usage that violates licenses of open-sourcelarge language models. This paper proposes a novel watermarking strategy thatplants watermarks in the quantization process of large language models withoutpre-defined triggers during inference. The watermark works when the model isused in the fp32 mode and remains hidden when the model is quantized to int8,in this way, the users can only inference the model without further supervisedfine-tuning of the model. We successfully plant the watermark into open-sourcelarge language model weights including GPT-Neo and LLaMA. We hope our proposedmethod can provide a potential direction for protecting model weights in theera of large language model applications.\rOpportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health\nShubo Tian Qiao Jin Lana Yeganova Po-Ting Lai Qingqing Zhu Xiuying Chen Yifan Yang Qingyu Chen Won Kim Donald C. Comeau Rezarta Islamaj Aadit Kapoor Xin Gao Zhiyong Lu\nabstract\rabstract: ChatGPT has drawn considerable attention from both the general public anddomain experts with its remarkable text generation capabilities. This hassubsequently led to the emergence of diverse applications in the field ofbiomedicine and health. In this work, we examine the diverse applications oflarge language models (LLMs), such as ChatGPT, in biomedicine and health.Specifically we explore the areas of biomedical information retrieval, questionanswering, medical text summarization, information extraction, and medicaleducation, and investigate whether LLMs possess the transformative power torevolutionize these tasks or whether the distinct complexities of biomedicaldomain presents unique challenges. Following an extensive literature survey, wefind that significant advances have been made in the field of text generationtasks, surpassing the previous state-of-the-art methods. For otherapplications, the advances have been modest. Overall, LLMs have not yetrevolutionized biomedicine, but recent rapid progress indicates that suchmethods hold great potential to provide valuable means for acceleratingdiscovery and improving health. We also find that the use of LLMs, likeChatGPT, in the fields of biomedicine and health entails various risks andchallenges, including fabricated information in its generated responses, aswell as legal and privacy concerns associated with sensitive patient data. Webelieve this survey can provide a comprehensive and timely overview tobiomedical researchers and healthcare practitioners on the opportunities andchallenges associated with using ChatGPT and other LLMs for transformingbiomedicine and health.\r2023-10-16\nPrivacy in Large Language Models: Attacks, Defenses and Future Directions\nHaoran Li Yulin Chen Jinglong Luo Yan Kang Xiaojin Zhang Qi Hu Chunkit Chan Yangqiu Song\nabstract\rabstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary\u0026rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.\rSnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds\nYanyu Li Huan Wang Qing Jin Ju Hu Pavlo Chemerys Yun Fu Yanzhi Wang Sergey Tulyakov Jian Ren\nabstract\rabstract: Text-to-image diffusion models can create stunning images from naturallanguage descriptions that rival the work of professional artists andphotographers. However, these models are large, with complex networkarchitectures and tens of denoising iterations, making them computationallyexpensive and slow to run. As a result, high-end GPUs and cloud-based inferenceare required to run diffusion models at scale. This is costly and has privacyimplications, especially when user data is sent to a third party. To overcomethese challenges, we present a generic approach that, for the first time,unlocks running text-to-image diffusion models on mobile devices in less than$2$ seconds. We achieve so by introducing efficient network architecture andimproving step distillation. Specifically, we propose an efficient UNet byidentifying the redundancy of the original model and reducing the computationof the image decoder via data distillation. Further, we enhance the stepdistillation by exploring training strategies and introducing regularizationfrom classifier-free guidance. Our extensive experiments on MS-COCO show thatour model with $8$ denoising steps achieves better FID and CLIP scores thanStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creationby bringing powerful text-to-image diffusion models to the hands of users.\rFATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models\nTao Fan Yan Kang Guoqiang Ma Weijing Chen Wenbin Wei Lixin Fan Qiang Yang\nabstract\rabstract: Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, haveexhibited remarkable performances across various tasks in recent years.However, LLMs face two main challenges in real-world applications. Onechallenge is that training LLMs consumes vast computing resources, preventingLLMs from being adopted by small and medium-sized enterprises with limitedcomputing resources. Another is that training LLM requires a large amount ofhigh-quality data, which are often scattered among enterprises. To addressthese challenges, we propose FATE-LLM, an industrial-grade federated learningframework for large language models. FATE-LLM (1) facilitates federatedlearning for large language models (coined FedLLM); (2) promotes efficienttraining of FedLLM using parameter-efficient fine-tuning methods; (3) protectsthe intellectual property of LLMs; (4) preserves data privacy during trainingand inference through privacy-preserving mechanisms. We release the code ofFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the researchof FedLLM and enable a broad range of industrial applications.\r2023-10-15\nA Recipe for Watermarking Diffusion Models\nYunqing Zhao Tianyu Pang Chao Du Xiao Yang Ngai-Man Cheung Min Lin\nabstract\rabstract: Diffusion models (DMs) have demonstrated advantageous potential on generativetasks. Widespread interest exists in incorporating DMs into downstreamapplications, such as producing or editing photorealistic images. However,practical deployment and unprecedented power of DMs raise legal issues,including copyright protection and monitoring of generated content. In thisregard, watermarking has been a proven solution for copyright protection andcontent monitoring, but it is underexplored in the DMs literature.Specifically, DMs generate samples from longer tracks and may have newlydesigned multimodal structures, necessitating the modification of conventionalwatermarking pipelines. To this end, we conduct comprehensive analyses andderive a recipe for efficiently watermarking state-of-the-art DMs (e.g., StableDiffusion), via training from scratch or finetuning. Our recipe isstraightforward but involves empirically ablated implementation details,providing a foundation for future research on watermarking DMs. The code isavailable at https://github.com/yunqing-me/WatermarkDM.\r2023-10-14\nTowards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning\nChang Lu Chandan K. Reddy Ping Wang Yue Ning\nabstract\rabstract: Automatic coding of International Classification of Diseases (ICD) is amulti-label text categorization task that involves extracting disease orprocedure codes from clinical notes. Despite the application ofstate-of-the-art natural language processing (NLP) techniques, there are stillchallenges including limited availability of data due to privacy constraintsand the high variability of clinical notes caused by different writing habitsof medical professionals and various pathological features of patients. In thiswork, we investigate the semi-structured nature of clinical notes and proposean automatic algorithm to segment them into sections. To address thevariability issues in existing ICD coding models with limited data, weintroduce a contrastive pre-training approach on sections using a softmulti-label similarity metric based on tree edit distance. Additionally, wedesign a masked section training strategy to enable ICD coding models to locatesections related to ICD codes. Extensive experimental results demonstrate thatour proposed training strategies effectively enhance the performance ofexisting ICD coding methods.\rUnified High-binding Watermark for Unconditional Image Generation Models\nRuinan Ma Yu-an Tan Shangbo Wu Tian Chen Yajie Wang Yuanzhang Li\nabstract\rabstract: Deep learning techniques have implemented many unconditional image generation(UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images(also known as AI-Generated Content, AIGC for short) produced by these modelsbring urgent needs for intellectual property protection such as datatraceability and copyright certification. An attacker can steal the outputimages of the target model and use them as part of the training data to train aprivate surrogate UIG model. The implementation mechanisms of UIG models arediverse and complex, and there is no unified and effective protection andverification method at present. To address these issues, we propose a two-stageunified watermark verification mechanism with high-binding effects for suchmodels. In the first stage, we use an encoder to invisibly write the watermarkimage into the output images of the original AIGC tool, and reversely extractthe watermark image through the corresponding decoder. In the second stage, wedesign the decoder fine-tuning process, and the fine-tuned decoder can makecorrect judgments on whether the suspicious model steals the original AIGC tooldata. Experiments demonstrate our method can complete the verification workwith almost zero false positive rate under the condition of only using themodel output images. Moreover, the proposed method can achieve data stealverification across different types of UIG models, which further increases thepracticality of the method.\r2023-10-13\nCopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow\nJunlei Zhou Jiashi Gao Ziwei Wang Xuetao Wei\nabstract\rabstract: Web-based AI image generation has become an innovative art form that cangenerate novel artworks with the rapid development of the diffusion model.However, this new technique brings potential copyright infringement risks as itmay incorporate the existing artworks without the owners\u0026rsquo; consent. Copyrightinfringement quantification is the primary and challenging step towardsAI-generated image copyright traceability. Previous work only focused on dataattribution from the training data perspective, which is unsuitable for tracingand quantifying copyright infringement in practice because of the followingreasons: (1) the training datasets are not always available in public; (2) themodel provider is the responsible party, not the image. Motivated by this, inthis paper, we propose CopyScope, a new framework to quantify the infringementof AI-generated images from the model level. We first rigorously identifypivotal components within the AI image generation pipeline. Then, we propose totake advantage of Fr'echet Inception Distance (FID) to effectively capture theimage similarity that fits human perception naturally. We further propose theFID-based Shapley algorithm to evaluate the infringement contribution amongmodels. Extensive experiments demonstrate that our work not only reveals theintricacies of infringement quantification but also effectively depicts theinfringing models quantitatively, thus promoting accountability in AIimage-generation tasks.\r2023-10-12\nDataless Knowledge Fusion by Merging Weights of Language Models\nXisen Jin Xiang Ren Daniel Preotiuc-Pietro Pengxiang Cheng\nabstract\rabstract: Fine-tuning pre-trained language models has become the prevalent paradigm forbuilding downstream NLP models. Oftentimes fine-tuned models are readilyavailable but their training data is not, due to data privacy or intellectualproperty concerns. This creates a barrier to fusing knowledge across individualmodels to yield a better single model. In this paper, we study the problem ofmerging individual models built on different training data sets to obtain asingle model that performs well both across all data set domains and cangeneralize on out-of-domain data. We propose a dataless knowledge fusion methodthat merges models in their parameter space, guided by weights that minimizeprediction differences between the merged model and the individual models. Overa battery of evaluation settings, we show that the proposed methodsignificantly outperforms baselines such as Fisher-weighted averaging or modelensembling. Further, we find that our method is a promising alternative tomulti-task learning that can preserve or sometimes improve over the individualmodels without access to the training data. Finally, model merging is moreefficient than training a multi-task model, thus making it applicable to awider set of scenarios.\rBuilding Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models\nJinmeng Rao Song Gao Gengchen Mai Krzysztof Janowicz\nabstract\rabstract: In recent years we have seen substantial advances in foundation models forartificial intelligence, including language, vision, and multimodal models.Recent studies have highlighted the potential of using foundation models ingeospatial artificial intelligence, known as GeoAI Foundation Models, forgeographic question answering, remote sensing image understanding, mapgeneration, and location-based services, among others. However, the developmentand application of GeoAI foundation models can pose serious privacy andsecurity risks, which have not been fully discussed or addressed to date. Thispaper introduces the potential privacy and security risks throughout thelifecycle of GeoAI foundation models and proposes a comprehensive blueprint forresearch directions and preventative and control strategies. Through thisvision paper, we hope to draw the attention of researchers and policymakers ingeospatial domains to these privacy and security risks inherent in GeoAIfoundation models and advocate for the development of privacy-preserving andsecure GeoAI foundation models.\rAn Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\nShyni Sharaf V. S. Anoop\nabstract\rabstract: This paper conducts a comprehensive investigation into applying largelanguage models, particularly on BioBERT, in healthcare. It begins withthoroughly examining previous natural language processing (NLP) approaches inhealthcare, shedding light on the limitations and challenges these methodsface. Following that, this research explores the path that led to theincorporation of BioBERT into healthcare applications, highlighting itssuitability for addressing the specific requirements of tasks related tobiomedical text mining. The analysis outlines a systematic methodology forfine-tuning BioBERT to meet the unique needs of the healthcare domain. Thisapproach includes various components, including the gathering of data from awide range of healthcare sources, data annotation for tasks like identifyingmedical entities and categorizing them, and the application of specializedpreprocessing techniques tailored to handle the complexities found inbiomedical texts. Additionally, the paper covers aspects related to modelevaluation, with a focus on healthcare benchmarks and functions like processingof natural language in biomedical, question-answering, clinical documentclassification, and medical entity recognition. It explores techniques toimprove the model\u0026rsquo;s interpretability and validates its performance compared toexisting healthcare-focused language models. The paper thoroughly examinesethical considerations, particularly patient privacy and data security. Ithighlights the benefits of incorporating BioBERT into healthcare contexts,including enhanced clinical decision support and more efficient informationretrieval. Nevertheless, it acknowledges the impediments and complexities ofthis integration, encompassing concerns regarding data privacy, transparency,resource-intensive requirements, and the necessity for model customization toalign with diverse healthcare domains.\r2023-10-11\nBeyond Memorization: Violating Privacy Via Inference with Large Language Models\nRobin Staab Mark Vero Mislav Balunović Martin Vechev\nabstract\rabstract: Current privacy research on large language models (LLMs) primarily focuses onthe issue of extracting memorized training data. At the same time, models\u0026rsquo;inference capabilities have increased drastically. This raises the key questionof whether current LLMs could violate individuals\u0026rsquo; privacy by inferringpersonal attributes from text given at inference time. In this work, we presentthe first comprehensive study on the capabilities of pretrained LLMs to inferpersonal attributes from text. We construct a dataset consisting of real Redditprofiles, and show that current LLMs can infer a wide range of personalattributes (e.g., location, income, sex), achieving up to $85%$ top-1 and$95.8%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time($240\\times$) required by humans. As people increasingly interact withLLM-powered chatbots across all aspects of life, we also explore the emergingthreat of privacy-invasive chatbots trying to extract personal informationthrough seemingly benign questions. Finally, we show that common mitigations,i.e., text anonymization and model alignment, are currently ineffective atprotecting user privacy against LLM inference. Our findings highlight thatcurrent LLMs can infer personal data at a previously unattainable scale. In theabsence of working defenses, we advocate for a broader discussion around LLMprivacy implications beyond memorization, striving for a wider privacyprotection.\rImproved Membership Inference Attacks Against Language Classification Models\nShlomit Shachor Natalia Razinkov Abigail Goldsteen\nabstract\rabstract: Artificial intelligence systems are prevalent in everyday life, with usecases in retail, manufacturing, health, and many other fields. With the rise inAI adoption, associated risks have been identified, including privacy risks tothe people whose data was used to train models. Assessing the privacy risks ofmachine learning models is crucial to enabling knowledgeable decisions onwhether to use, deploy, or share a model. A common approach to privacy riskassessment is to run one or more known attacks against the model and measuretheir success rate. We present a novel framework for running membershipinference attacks against classification models. Our framework takes advantageof the ensemble method, generating many specialized attack models for differentsubsets of the data. We show that this approach achieves higher accuracy thaneither a single attack model or an attack model per class label, both onclassical and language classification tasks.\r2023-10-10\nSound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention\nRodolfo Valentim Idilio Drago Marco Mellia Federico Cerutti\nabstract\rabstract: Sound-squatting is a phishing attack that tricks users into maliciousresources by exploiting similarities in the pronunciation of words. Proactivedefense against sound-squatting candidates is complex, and existing solutionsrely on manually curated lists of homophones. We here introduce Sound-skwatter,a multi-language AI-based system that generates sound-squatting candidates forproactive defense. Sound-skwatter relies on an innovative multi-modalcombination of Transformers Networks and acoustic models to learn soundsimilarities. We show that Sound-skwatter can automatically list knownhomophones and thousands of high-quality candidates. In addition, it coverscross-language sound-squatting, i.e., when the reader and the listener speakdifferent languages, supporting any combination of languages. We applySound-skwatter to network-centric phishing via squatted domain names. We find ~10% of the generated domains exist in the wild, the vast majority unknown toprotection solutions. Next, we show attacks on the PyPI package manager, where~ 17% of the popular packages have at least one existing candidate. We believeSound-skwatter is a crucial asset to mitigate the sound-squatting phenomenonproactively on the Internet. To increase its impact, we publish an online demoand release our models and code as open source.\rMemorization of Named Entities in Fine-tuned BERT Models\nAndor Diera Nicolas Lell Aygul Garifullina Ansgar Scherp\nabstract\rabstract: Privacy preserving deep learning is an emerging field in machine learningthat aims to mitigate the privacy risks in the use of deep neural networks. Onesuch risk is training data extraction from language models that have beentrained on datasets, which contain personal and privacy sensitive information.In our study, we investigate the extent of named entity memorization infine-tuned BERT models. We use single-label text classification asrepresentative downstream task and employ three different fine-tuning setups inour experiments, including one with Differentially Privacy (DP). We create alarge number of text samples from the fine-tuned BERT models utilizing a customsequential sampling strategy with two prompting strategies. We search in thesesamples for named entities and check if they are also present in thefine-tuning datasets. We experiment with two benchmark datasets in the domainsof emails and blogs. We show that the application of DP has a detrimentaleffect on the text generation capabilities of BERT. Furthermore, we show that afine-tuned BERT does not generate more named entities specific to thefine-tuning dataset than a BERT model that is pre-trained only. This suggeststhat BERT is unlikely to emit personal or privacy sensitive named entities.Overall, our results are important to understand to what extent BERT-basedservices are prone to training data extraction attacks.\rEvaluation and Analysis of Hallucination in Large Vision-Language Models\nJunyang Wang Yiyang Zhou Guohai Xu Pengcheng Shi Chenlin Zhao Haiyang Xu Qinghao Ye Ming Yan Ji Zhang Jihua Zhu Jitao Sang Haoyu Tang\nabstract\rabstract: Large Vision-Language Models (LVLMs) have recently achieved remarkablesuccess. However, LVLMs are still plagued by the hallucination problem, whichlimits the practicality in many scenarios. Hallucination refers to theinformation of LVLMs\u0026rsquo; responses that does not exist in the visual input, whichposes potential risks of substantial consequences. There has been limited workstudying hallucination evaluation in LVLMs. In this paper, we proposeHallucination Evaluation based on Large Language Models (HaELM), an LLM-basedhallucination evaluation framework. HaELM achieves an approximate 95%performance comparable to ChatGPT and has additional advantages including lowcost, reproducibility, privacy preservation and local deployment. Leveragingthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, weanalyze the factors contributing to hallucination in LVLMs and offer helpfulsuggestions to mitigate the hallucination problem. Our training data and humanannotation hallucination data will be made public soon.\rDASICS: Enhancing Memory Protection with Dynamic Compartmentalization\nYue Jin Yibin Xu Chengyuan Yang Han Wang Tianyi Huang Tianyue Lu Mingyu Chen\nabstract\rabstract: In the existing software development ecosystem, security issues introduced bythird-party code cannot be overlooked. Among these security concerns, memoryaccess vulnerabilities stand out prominently, leading to risks such as thetheft or tampering of sensitive data. To address this issue, software-baseddefense mechanisms have been established at the programming language, compiler,and operating system levels. However, as a trade-off, these mechanismssignificantly reduce software execution efficiency. Hardware-software co-designapproaches have sought to either construct entirely isolated trusted executionenvironments or attempt to partition security domains within the same addressspace. While such approaches enhance efficiency compared to pure softwaremethods, they also encounter challenges related to granularity of protection,performance overhead, and portability. In response to these challenges, wepresent the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secureprocessor design, which offers dynamic and flexible security protection acrossmultiple privilege levels, addressing data flow protection, control flowprotection, and secure system calls. We have implemented hardware FPGAprototypes and software QEMU simulator prototypes based on DASICS, along withnecessary modifications to system software for adaptability. We illustrate theprotective mechanisms and effectiveness of DASICS with two practical examplesand provide potential real-world use cases where DASICS could be applied.\rBC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models\nHaoxiang Luo Jian Luo Athanasios V. Vasilakos\nabstract\rabstract: In recent years, artificial intelligence (AI) and machine learning (ML) arereshaping society\u0026rsquo;s production methods and productivity, and also changing theparadigm of scientific research. Among them, the AI language model representedby ChatGPT has made great progress. Such large language models (LLMs) servepeople in the form of AI-generated content (AIGC) and are widely used inconsulting, healthcare, and education. However, it is difficult to guaranteethe authenticity and reliability of AIGC learning data. In addition, there arealso hidden dangers of privacy disclosure in distributed AI training. Moreover,the content generated by LLMs is difficult to identify and trace, and it isdifficult to cross-platform mutual recognition. The above information securityissues in the coming era of AI powered by LLMs will be infinitely amplified andaffect everyone\u0026rsquo;s life. Therefore, we consider empowering LLMs using blockchaintechnology with superior security features to propose a vision for trusted AI.This paper mainly introduces the motivation and technical route of blockchainfor LLM (BC4LLM), including reliable learning corpus, secure training process,and identifiable generated content. Meanwhile, this paper also reviews thepotential applications and future challenges, especially in the frontiercommunication networks field, including network resource allocation, dynamicspectrum sharing, and semantic communication. Based on the above work combinedand the prospect of blockchain and LLMs, it is expected to help the earlyrealization of trusted AI and provide guidance for the academic community.\rWatermarking Classification Dataset for Copyright Protection\nYixin Liu Hongsheng Hu Xun Chen Xuyun Zhang Lichao Sun\nabstract\rabstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals\u0026rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.\r2023-10-09\nDoes Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?\nAdaku Uchendu Jooyoung Lee Hua Shen Thai Le Ting-Hao \u0026lsquo;Kenneth\u0026rsquo; Huang Dongwon Lee\nabstract\rabstract: Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved thegeneration of coherent sentences resembling human writing on a large scale,resulting in the creation of so-called deepfake texts. However, this progressposes security and privacy concerns, necessitating effective solutions fordistinguishing deepfake texts from human-written ones. Although prior worksstudied humans\u0026rsquo; ability to detect deepfake texts, none has examined whether\u0026quot;collaboration\u0026quot; among humans improves the detection of deepfake texts. In thisstudy, to address this gap of understanding on deepfake texts, we conductedexperiments with two groups: (1) nonexpert individuals from the AMT platformand (2) writing experts from the Upwork platform. The results demonstrate thatcollaboration among humans can potentially improve the detection of deepfaketexts for both groups, increasing detection accuracies by 6.36% for non-expertsand 12.76% for experts, respectively, compared to individuals\u0026rsquo; detectionaccuracies. We further analyze the explanations that humans used for detectinga piece of text as deepfake text, and find that the strongest indicator ofdeepfake texts is their lack of coherence and consistency. Our study providesuseful insights for future tools and framework designs to facilitate thecollaborative human detection of deepfake texts. The experiment datasets andAMT implementations are available at:https://github.com/huashen218/llm-deepfake-human-study.git\rTwo Models are Better than One: Federated Learning Is Not Private For Google GBoard Next Word Prediction\nMohamed Suliman Douglas Leith\nabstract\rabstract: In this paper we present new attacks against federated learning when used totrain natural language text models. We illustrate the effectiveness of theattacks against the next word prediction model used in Google\u0026rsquo;s GBoard app, awidely used mobile keyboard app that has been an early adopter of federatedlearning for production use. We demonstrate that the words a user types ontheir mobile handset, e.g. when sending text messages, can be recovered withhigh accuracy under a wide range of conditions and that counter-measures such ause of mini-batches and adding local noise are ineffective. We also show thatthe word order (and so the actual sentences typed) can be reconstructed withhigh fidelity. This raises obvious privacy concerns, particularly since GBoardis in production use.\rDiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models\nYingqian Cui Jie Ren Han Xu Pengfei He Hui Liu Lichao Sun Yue Xing Jiliang Tang\nabstract\rabstract: Recently, Generative Diffusion Models (GDMs) have showcased their remarkablecapabilities in learning and generating images. A large community of GDMs hasnaturally emerged, further promoting the diversified applications of GDMs invarious fields. However, this unrestricted proliferation has raised seriousconcerns about copyright protection. For example, artists including paintersand photographers are becoming increasingly concerned that GDMs couldeffortlessly replicate their unique creative works without authorization. Inresponse to these challenges, we introduce a novel watermarking scheme,DiffusionShield, tailored for GDMs. DiffusionShield protects images fromcopyright infringement by GDMs through encoding the ownership information intoan imperceptible watermark and injecting it into the images. Its watermark canbe easily learned by GDMs and will be reproduced in their generated images. Bydetecting the watermark from generated images, copyright infringement can beexposed with evidence. Benefiting from the uniformity of the watermarks and thejoint optimization method, DiffusionShield ensures low distortion of theoriginal image, high watermark detection performance, and the ability to embedlengthy messages. We conduct rigorous and comprehensive experiments to show theeffectiveness of DiffusionShield in defending against infringement by GDMs andits superiority over traditional watermarking methods.\r2023-10-08\nAre Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT\nAkshaj Kumar Veldanda Fabian Grob Shailja Thakur Hammond Pearce Benjamin Tan Ramesh Karri Siddharth Garg\nabstract\rabstract: Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibitapplicability across numerous tasks. One domain of interest is their use inalgorithmic hiring, specifically in matching resumes with job categories. Yet,this introduces issues of bias on protected attributes like gender, race andmaternity status. The seminal work of Bertrand \u0026amp; Mullainathan (2003) set thegold-standard for identifying hiring bias via field experiments where theresponse rate for identical resumes that differ only in protected attributes,e.g., racially suggestive names such as Emily or Lakisha, is compared. Wereplicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude andLlama) to evaluate bias (or lack thereof) on gender, race, maternity status,pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)matching resumes to job categories; and (2) summarizing resumes with employmentrelevant information. Overall, LLMs are robust across race and gender. Theydiffer in their performance on pregnancy status and political affiliation. Weuse contrastive input decoding on open-source LLMs to uncover potential sourcesof bias.\r2023-10-07\nPrompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models\nGabriele Tolomei Cesare Campagnano Fabrizio Silvestri Giovanni Trappolini\nabstract\rabstract: In this paper, we present a groundbreaking paradigm for human-computerinteraction that revolutionizes the traditional notion of an operating system. Within this innovative framework, user requests issued to the machine arehandled by an interconnected ecosystem of generative AI models that seamlesslyintegrate with or even replace traditional software applications. At the coreof this paradigm shift are large generative models, such as language anddiffusion models, which serve as the central interface between users andcomputers. This pioneering approach leverages the abilities of advancedlanguage models, empowering users to engage in natural language conversationswith their computing devices. Users can articulate their intentions, tasks, andinquiries directly to the system, eliminating the need for explicit commands orcomplex navigation. The language model comprehends and interprets the user\u0026rsquo;sprompts, generating and displaying contextual and meaningful responses thatfacilitate seamless and intuitive interactions. This paradigm shift not only streamlines user interactions but also opens upnew possibilities for personalized experiences. Generative models can adapt toindividual preferences, learning from user input and continuously improvingtheir understanding and response generation. Furthermore, it enables enhancedaccessibility, as users can interact with the system using speech or text,accommodating diverse communication preferences. However, this visionary concept raises significant challenges, includingprivacy, security, trustability, and the ethical use of generative models.Robust safeguards must be in place to protect user data and prevent potentialmisuse or manipulation of the language model. While the full realization of this paradigm is still far from being achieved,this paper serves as a starting point for envisioning this transformativepotential.\r2023-10-06\nDe-Identification of French Unstructured Clinical Notes for Machine Learning Tasks\nYakini Tchouka Jean-François Couchot Maxime Coulmeau David Laiymani Philippe Selles Azzedine Rahmani\nabstract\rabstract: Unstructured textual data are at the heart of health systems: liaison lettersbetween doctors, operating reports, coding of procedures according to theICD-10 standard, etc. The details included in these documents make it possibleto get to know the patient better, to better manage him or her, to better studythe pathologies, to accurately remunerate the associated medical acts\\ldots Allthis seems to be (at least partially) within reach of today by artificialintelligence techniques. However, for obvious reasons of privacy protection,the designers of these AIs do not have the legal right to access thesedocuments as long as they contain identifying data. De-identifying thesedocuments, i.e. detecting and deleting all identifying information present inthem, is a legally necessary step for sharing this data between twocomplementary worlds. Over the last decade, several proposals have been made tode-identify documents, mainly in English. While the detection scores are oftenhigh, the substitution methods are often not very robust to attack. In French,very few methods are based on arbitrary detection and/or substitution rules. Inthis paper, we propose a new comprehensive de-identification method dedicatedto French-language medical documents. Both the approach for the detection ofidentifying elements (based on deep learning) and their substitution (based ondifferential privacy) are based on the most proven existing approaches. Theresult is an approach that effectively protects the privacy of the patients atthe heart of these medical documents. The whole approach has been evaluated ona French language medical dataset of a French public hospital and the resultsare very encouraging.\rQuantized Transformer Language Model Implementations on Edge Devices\nMohammad Wali Ur Rahman Murad Mehrab Abrar Hunter Gibbons Copening Salim Hariri Sicong Shao Pratik Satam Soheil Salehi\nabstract\rabstract: Large-scale transformer-based models like the Bidirectional EncoderRepresentations from Transformers (BERT) are widely used for Natural LanguageProcessing (NLP) applications, wherein these models are initially pre-trainedwith a large corpus with millions of parameters and then fine-tuned for adownstream NLP task. One of the major limitations of these large-scale modelsis that they cannot be deployed on resource-constrained devices due to theirlarge model size and increased inference latency. In order to overcome theselimitations, such large-scale models can be converted to an optimizedFlatBuffer format, tailored for deployment on resource-constrained edgedevices. Herein, we evaluate the performance of such FlatBuffer transformedMobileBERT models on three different edge devices, fine-tuned for Reputationanalysis of English language tweets in the RepLab 2013 dataset. In addition,this study encompassed an evaluation of the deployed models, wherein theirlatency, performance, and resource efficiency were meticulously assessed. Ourexperiment results show that, compared to the original BERT large model, theconverted and quantized MobileBERT models have 160$\\times$ smaller footprintsfor a 4.1% drop in accuracy while analyzing at least one tweet per second onedge devices. Furthermore, our study highlights the privacy-preserving aspectof TinyML systems as all data is processed locally within a serverlessenvironment.\r2023-10-05\nValidating transformers for redaction of text from electronic health records in real-world healthcare\nZeljko Kraljevic Anthony Shek Joshua Au Yeung Ewart Jonathan Sheldon Mohammad Al-Agil Haris Shuaib Xi Bai Kawsar Noor Anoop D. Shah Richard Dobson James Teo\nabstract\rabstract: Protecting patient privacy in healthcare records is a top priority, andredaction is a commonly used method for obscuring directly identifiableinformation in text. Rule-based methods have been widely used, but theirprecision is often low causing over-redaction of text and frequently not beingadaptable enough for non-standardised or unconventional structures of personalhealth information. Deep learning techniques have emerged as a promisingsolution, but implementing them in real-world environments poses challenges dueto the differences in patient record structure and language across differentdepartments, hospitals, and countries. In this study, we present AnonCAT, a transformer-based model and a blueprinton how deidentification models can be deployed in real-world healthcare.AnonCAT was trained through a process involving manually annotated redactionsof real-world documents from three UK hospitals with different electronichealth record systems and 3116 documents. The model achieved high performancein all three hospitals with a Recall of 0.99, 0.99 and 0.96. Our findings demonstrate the potential of deep learning techniques forimproving the efficiency and accuracy of redaction in global healthcare dataand highlight the importance of building workflows which not just use thesemodels but are also able to continually fine-tune and audit the performance ofthese algorithms to ensure continuing effectiveness in real-world settings.This approach provides a blueprint for the real-world use of de-identifyingalgorithms through fine-tuning and localisation, the code together withtutorials is available on GitHub (https://github.com/CogStack/MedCAT).\r2023-10-04\nMedAlpaca \u0026ndash; An Open-Source Collection of Medical Conversational AI Models and Training Data\nTianyu Han Lisa C. Adams Jens-Michalis Papaioannou Paul Grundmann Tom Oberhauser Alexander Löser Daniel Truhn Keno K. Bressem\nabstract\rabstract: As large language models (LLMs) like OpenAI\u0026rsquo;s GPT series continue to makestrides, we witness the emergence of artificial intelligence applications in anever-expanding range of fields. In medicine, these LLMs hold considerablepromise for improving medical workflows, diagnostics, patient care, andeducation. Yet, there is an urgent need for open-source models that can bedeployed on-premises to safeguard patient privacy. In our work, we present aninnovative dataset consisting of over 160,000 entries, specifically crafted tofine-tune LLMs for effective medical applications. We investigate the impact offine-tuning these datasets on publicly accessible pre-trained LLMs, andsubsequently, we juxtapose the performance of pre-trained-only models againstthe fine-tuned models concerning the examinations that future medical doctorsmust pass to achieve certification.\rDP-SGD for non-decomposable objective functions\nWilliam Kong Andrés Muñoz Medina Mónica Ribero\nabstract\rabstract: Unsupervised pre-training is a common step in developing computer visionmodels and large language models. In this setting, the absence of labelsrequires the use of similarity-based loss functions, such as contrastive loss,that favor minimizing the distance between similar inputs and maximizing thedistance between distinct inputs. As privacy concerns mount, training thesemodels using differential privacy has become more important. However, due tohow inputs are generated for these losses, one of their undesirable propertiesis that their $L_2$ sensitivity can grow with increasing batch size. Thisproperty is particularly disadvantageous for differentially private trainingmethods, such as DP-SGD. To overcome this issue, we develop a new DP-SGDvariant for similarity based loss functions \u0026ndash; in particular the commonly usedcontrastive loss \u0026ndash; that manipulates gradients of the objective function in anovel way to obtain a senstivity of the summed gradient that is $O(1)$ forbatch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, ourmethod\u0026rsquo;s performance comes close to that of a non-private model and generallyoutperforms DP-SGD applied directly to the contrastive loss.\rBridging the Gap Between Foundation Models and Heterogeneous Federated Learning\nSixing Yu J. Pablo Muñoz Ali Jannesari\nabstract\rabstract: Federated learning (FL) offers privacy-preserving decentralized machinelearning, optimizing models at edge clients without sharing private data.Simultaneously, foundation models (FMs) have gained traction in the artificialintelligence (AI) community due to their exceptional performance across varioustasks. However, integrating FMs into FL presents challenges, primarily due totheir substantial size and intensive resource requirements. This is especiallytrue when considering the resource heterogeneity in edge FL systems. We presentan adaptive framework for Resource-aware Federated Foundation Models (RaFFM) toaddress these challenges. RaFFM introduces specialized model compressionalgorithms tailored for FL scenarios, such as salient parameter prioritizationand high-performance subnetwork extraction. These algorithms enable dynamicscaling of given transformer-based FMs to fit heterogeneous resourceconstraints at the network edge during both FL\u0026rsquo;s optimization and deploymentstages. Experimental results demonstrate that RaFFM shows significantsuperiority in resource utilization efficiency and uses fewer resources todeploy FMs to FL. Despite the lower resource consumption, target modelsoptimized by RaFFM achieve performance on par with traditional FL methodsapplied to full-sized FMs. This is evident across tasks in both naturallanguage processing and computer vision domains.\rWho\u0026rsquo;s Harry Potter? Approximate Unlearning in LLMs\nRonen Eldan Mark Russinovich\nabstract\rabstract: Large language models (LLMs) are trained on massive internet corpora thatoften contain copyrighted content. This poses legal and ethical challenges forthe developers and users of these models, as well as the original authors andpublishers. In this paper, we propose a novel technique for unlearning a subsetof the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter booksfrom the Llama2-7b model (a generative language model recently open-sourced byMeta). While the model took over 184K GPU-hours to pretrain, we show that inabout 1 GPU hour of finetuning, we effectively erase the model\u0026rsquo;s ability togenerate or recall Harry Potter-related content, while its performance oncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remainsalmost unaffected. We make our fine-tuned model publicly available onHuggingFace for community evaluation. To the best of our knowledge, this is thefirst paper to present an effective technique for unlearning in generativelanguage models. Our technique consists of three main components: First, we use a reinforcedmodel that is further trained on the target data to identify the tokens thatare most related to the unlearning target, by comparing its logits with thoseof a baseline model. Second, we replace idiosyncratic expressions in the targetdata with generic counterparts, and leverage the model\u0026rsquo;s own predictions togenerate alternative labels for every token. These labels aim to approximatethe next-token predictions of a model that has not been trained on the targetdata. Third, we finetune the model on these alternative labels, whicheffectively erases the original text from the model\u0026rsquo;s memory whenever it isprompted with its context.\r2023-10-03\nLarge Language Models Can Be Good Privacy Protection Learners\nYijia Xiao Yiqiao Jin Yushi Bai Yue Wu Xianjun Yang Xiao Luo Wenchao Yu Xujiang Zhao Yanchi Liu Haifeng Chen Wei Wang Wei Cheng\nabstract\rabstract: The proliferation of Large Language Models (LLMs) has driven considerableinterest in fine-tuning them with domain-specific data to create specializedlanguage models. Nevertheless, such domain-specific fine-tuning data oftencontains sensitive personally identifiable information (PII). Directfine-tuning LLMs on this data without privacy protection poses a risk ofleakage. To address this challenge, we introduce Privacy Protection LanguageModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injectsdomain-specific knowledge while safeguarding data privacy. Our work offers atheoretical analysis for model design and delves into various techniques suchas corpus curation, penalty-based unlikelihood in training loss, andinstruction-based tuning, etc. Extensive experiments across diverse datasetsand scenarios demonstrate the effectiveness of our approaches. In particular,instruction tuning with both positive and negative examples, stands out as apromising method, effectively protecting private data while enhancing themodel\u0026rsquo;s knowledge. Our work underscores the potential for Large Language Modelsas robust privacy protection learners.\rCan Large Language Models Provide Security \u0026amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions\nYufan Chen Arjun Arunasalam Z. Berkay Celik\nabstract\rabstract: Users seek security \u0026amp; privacy (S\u0026amp;P) advice from online resources, includingtrusted websites and content-sharing platforms. These resources help usersunderstand S\u0026amp;P technologies and tools and suggest actionable strategies. LargeLanguage Models (LLMs) have recently emerged as trusted information sources.However, their accuracy and correctness have been called into question. Priorresearch has outlined the shortcomings of LLMs in answering multiple-choicequestions and user ability to inadvertently circumvent model restrictions(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliableS\u0026amp;P advice is not well-explored. In this paper, we measure their ability torefute popular S\u0026amp;P misconceptions that the general public holds. We first studyrecent academic literature to curate a dataset of over a hundred S\u0026amp;P-relatedmisconceptions across six different topics. We then query two popular LLMs(Bard and ChatGPT) and develop a labeling guide to evaluate their responses tothese misconceptions. To comprehensively evaluate their responses, we furtherapply three strategies: query each misconception multiple times, generate andquery their paraphrases, and solicit source URLs of the responses. Both modelsdemonstrate, on average, a 21.3% non-negligible error rate, incorrectlysupporting popular S\u0026amp;P misconceptions. The error rate increases to 32.6% whenwe repeatedly query LLMs with the same or paraphrased misconceptions. We alsoexpose that models may partially support a misconception or remainnoncommittal, refusing a firm stance on misconceptions. Our exploration ofinformation sources for responses revealed that LLMs are susceptible toproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point tounrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).\rFT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models\nYingqian Cui Jie Ren Yuping Lin Han Xu Pengfei He Yue Xing Wenqi Fan Hui Liu Jiliang Tang\nabstract\rabstract: Text-to-image generative models based on latent diffusion models (LDM) havedemonstrated their outstanding ability in generating high-quality andhigh-resolution images according to language prompt. Based on these powerfullatent diffusion models, various fine-tuning methods have been proposed toachieve the personalization of text-to-image diffusion models such as artisticstyle adaptation and human face transfer. However, the unauthorized usage ofdata for model personalization has emerged as a prevalent concern in relationto copyright violations. For example, a malicious user may use the fine-tuningtechnique to generate images which mimic the style of a painter without his/herpermission. In light of this concern, we have proposed FT-Shield, awatermarking approach specifically designed for the fine-tuning oftext-to-image diffusion models to aid in detecting instances of infringement.We develop a novel algorithm for the generation of the watermark to ensure thatthe watermark on the training images can be quickly and accurately transferredto the generated images of text-to-image diffusion models. A watermark will bedetected on an image by a binary watermark detector if the image is generatedby a model that has been fine-tuned using the protected watermarked images.Comprehensive experiments were conducted to validate the effectiveness ofFT-Shield.\rAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\nJi Lin Jiaming Tang Haotian Tang Shang Yang Xingyu Dang Chuang Gan Song Han\nabstract\rabstract: Large language models (LLMs) have shown excellent performance on varioustasks, but the astronomical model size raises the hardware barrier for serving(memory size) and slows down token generation (memory bandwidth). In thispaper, we propose Activation-aware Weight Quantization (AWQ), ahardware-friendly approach for LLM low-bit weight-only quantization. Our methodis based on the observation that weights are not equally important: protectingonly 1% of salient weights can greatly reduce quantization error. We thenpropose to search for the optimal per-channel scaling that protects the salientweights by observing the activation, not weights. AWQ does not rely on anybackpropagation or reconstruction, so it can well preserve LLMs\u0026rsquo; generalizationability on different domains and modalities, without overfitting to thecalibration set. AWQ outperforms existing work on various language modeling anddomain-specific benchmarks. Thanks to better generalization, it achievesexcellent quantization performance for instruction-tuned LMs and, for the firsttime, multi-modal LMs. Alongside AWQ, we implement an efficient and flexibleinference framework tailored for LLMs on the edge, offering more than 3xspeedup over the Huggingface FP16 implementation on both desktop and mobileGPUs. It also democratizes the deployment of the 70B Llama-2 model on mobileGPU (NVIDIA Jetson Orin 64GB).\rCan Language Models be Instructed to Protect Personal Information?\nYang Chen Ethan Mendes Sauvik Das Wei Xu Alan Ritter\nabstract\rabstract: Large multimodal language models have proven transformative in numerousapplications. However, these models have been shown to memorize and leakpre-training data, raising serious user privacy and information securityconcerns. While data leaks should be prevented, it is also crucial to examinethe trade-off between the privacy protection and model utility of proposedapproaches. In this paper, we introduce PrivQA \u0026ndash; a multimodal benchmark toassess this privacy/utility trade-off when a model is instructed to protectspecific categories of personal information in a simulated scenario. We alsopropose a technique to iteratively self-moderate responses, which significantlyimproves privacy. However, through a series of red-teaming experiments, we findthat adversaries can also easily circumvent these protections with simplejailbreaking methods through textual and/or image inputs. We believe PrivQA hasthe potential to support the development of new models with improved privacyprotections, as well as the adversarial robustness of these protections. Werelease the entire PrivQA dataset at https://llm-access-control.github.io/.\rFine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?\nJingyuan Sun Marie-Francine Moens\nabstract\rabstract: To decipher the algorithm underlying the human brain\u0026rsquo;s languagerepresentation, previous work probed brain responses to language input withpre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.However, full fine-tuning generally updates the entire parametric space anddistorts pre-trained features, cognitively inconsistent with the brain\u0026rsquo;s robustmulti-task learning ability. Prompt-tuning, in contrast, protects pre-trainedweights and learns task-specific embeddings to fit a task. Could prompt-tuninggenerate representations that better account for the brain\u0026rsquo;s languagerepresentations than fine-tuning? If so, what kind of NLU task leads apre-trained model to better decode the information represented in the humanbrain? We investigate these questions by comparing prompt-tuned and fine-tunedrepresentations in neural decoding, that is predicting the linguistic stimulusfrom the brain activities evoked by the stimulus. We find that on none of the10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning inneural decoding, implicating that a more brain-consistent tuning method yieldsrepresentations that better correlate with brain data. Moreover, we identifythat tasks dealing with fine-grained concept meaning yield representations thatbetter decode brain activation patterns than other tasks, especially thesyntactic chunking task. This indicates that our brain encodes morefine-grained concept information than shallow syntactic information whenrepresenting languages.\r2023-10-02\nBTR: Binary Token Representations for Efficient Retrieval Augmented Language Models\nQingqing Cao Sewon Min Yizhong Wang Hannaneh Hajishirzi\nabstract\rabstract: Retrieval augmentation addresses many critical problems in large languagemodels such as hallucination, staleness, and privacy leaks. However, runningretrieval-augmented language models (LMs) is slow and difficult to scale due toprocessing large amounts of retrieved text. We introduce binary tokenrepresentations (BTR), which use 1-bit vectors to precompute every token inpassages, significantly reducing computation during inference. Despite thepotential loss of accuracy, our new calibration techniques and trainingobjectives restore performance. Combined with offline and runtime compression,this only requires 127GB of disk space for encoding 3 billion tokens inWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTRaccelerates state-of-the-art inference by up to 4x and reduces storage by over100x while maintaining over 95% task performance.\rFedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models\nJingwei Sun Ziyue Xu Hongxu Yin Dong Yang Daguang Xu Yiran Chen Holger R. Roth\nabstract\rabstract: Pre-trained language models (PLM) have revolutionized the NLP landscape,achieving stellar performances across diverse tasks. These models, whilebenefiting from vast training data, often require fine-tuning on specific datato cater to distinct downstream tasks. However, this data adaptation processhas inherent security and privacy concerns, primarily when leveraginguser-generated, device-residing data. Federated learning (FL) provides asolution, allowing collaborative model fine-tuning without centralized datacollection. However, applying FL to finetune PLMs is hampered by challenges,including restricted model parameter access, high computational requirements,and communication overheads. This paper introduces Federated Black-box PromptTuning (FedBPT), a framework designed to address these challenges. FedBPT doesnot require the clients to access the model parameters. By focusing on trainingoptimal prompts and utilizing gradient-free optimization methods, FedBPTreduces the number of exchanged variables, boosts communication efficiency, andminimizes computational and storage costs. Experiments highlight theframework\u0026rsquo;s ability to drastically cut communication and memory costs whilemaintaining competitive performance. Ultimately, FedBPT presents a promisingsolution for efficient, privacy-preserving fine-tuning of PLM in the age oflarge language models.\rCoupling public and private gradient provably helps optimization\nRuixuan Liu Zhiqi Bu Yu-xiang Wang Sheng Zha George Karypis\nabstract\rabstract: The success of large neural networks is crucially determined by theavailability of data. It has been observed that training only on a small amountof public data, or privately on the abundant private data can lead toundesirable degradation of accuracy. In this work, we leverage both private andpublic data to improve the optimization, by coupling their gradients via aweighted linear combination. We formulate an optimal solution for the optimalweight in the convex setting to indicate that the weighting coefficient shouldbe hyperparameter-dependent. Then, we prove the acceleration in the convergenceof non-convex loss and the effects of hyper-parameters such as privacy budget,number of iterations, batch size, and model size on the choice of the weightingcoefficient. We support our analysis with empirical experiments across languageand vision benchmarks, and provide a guideline for choosing the optimal weightof the gradient coupling.\rGotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models\nZhou Yang Zhipeng Zhao Chenyu Wang Jieke Shi Dongsum Kim Donggyun Han David Lo\nabstract\rabstract: Given large-scale source code datasets available in open-source projects andadvanced large language models, recent code models have been proposed toaddress a series of critical software engineering tasks, such as program repairand code completion. The training data of the code models come from varioussources, not only the publicly available source code, e.g., open-sourceprojects on GitHub but also the private data such as the confidential sourcecode from companies, which may contain sensitive information (for example, SSHkeys and personal information). As a result, the use of these code models mayraise new privacy concerns. In this paper, we focus on a critical yet not well-explored question on usingcode models: what is the risk of membership information leakage in code models?Membership information leakage refers to the risk that an attacker can inferwhether a given data point is included in (i.e., a member of) the trainingdata. To answer this question, we propose Gotcha, a novel membership inferenceattack method specifically for code models. We investigate the membershipleakage risk of code models. Our results reveal a worrying fact that the riskof membership leakage is high: although the previous attack methods are closeto random guessing, Gotcha can predict the data membership with a high truepositive rate of 0.95 and a low false positive rate of 0.10. We also show thatthe attacker\u0026rsquo;s knowledge of the victim model (e.g., the model architecture andthe pre-training data) impacts the success rate of attacks. Further analysisdemonstrates that changing the decoding strategy can mitigate the risk ofmembership leakage. This study calls for more attention to understanding theprivacy of code models and developing more effective countermeasures againstsuch attacks.\r2023-09-30\nPrivacy-Preserving In-Context Learning for Large Language Models\nTong Wu Ashwinee Panda Jiachen T. Wang Prateek Mittal\nabstract\rabstract: In-context learning (ICL) is an important capability of Large Language Models(LLMs), enabling these models to dynamically adapt based on specific,in-context exemplars, thereby improving accuracy and relevance. However, LLM\u0026rsquo;sresponses may leak the sensitive private information contained in in-contextexemplars. To address this challenge, we propose Differentially PrivateIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. Thekey idea for DP-ICL paradigm is generating differentially private responsesthrough a noisy consensus among an ensemble of LLM\u0026rsquo;s responses based ondisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiateseveral techniques showing how to privatize ICL for text classification andlanguage generation. We evaluate DP-ICL on four text classification benchmarksand two language generation tasks, and our empirical results show that DP-ICLachieves a strong utility-privacy tradeoff.\rInvestigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting\nBaphumelele Masikisiki Vukosi Marivate Yvette Hlope\nabstract\rabstract: Large Language Models, such as Generative Pre-trained Transformer 3 (aka.GPT-3), have been developed to understand language through the analysis ofextensive text data, allowing them to identify patterns and connections betweenwords. While LLMs have demonstrated impressive performance across varioustext-related tasks, they encounter challenges in tasks associated withreasoning. To address this challenge, Chain of Thought(CoT) prompting methodhas been proposed as a means to enhance LLMs\u0026rsquo; proficiency in complex reasoningtasks like solving math word problems and answering questions based on logicalargumentative reasoning. The primary aim of this research is to assess how wellfour language models can grade reflective essays of third-year medicalstudents. The assessment will specifically target the evaluation of criticalthinking skills using CoT prompting. The research will provide the following contributions; to introduce andeducate on the process of instructing models to evaluate reflective essays froma dataset they have not been previously trained on; to illustrate the use ofCoT prompting as an instructional approach for training large models to carryout particular tasks. Our results suggest that among all the models, Llama-7bperforms the least effectively, displaying the highest mean squared error.Conversely, ChatGPT emerges as the superior model, boasting a higher Cohenkappa score value of 0.53. Lastly, it\u0026rsquo;s important to note that the selectedmodels do prioritise user privacy by allowing users to delete their ownconducted conversations.\r2023-09-29\nCan Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks\nVaidehi Patil Peter Hase Mohit Bansal\nabstract\rabstract: Pretrained language models sometimes possess knowledge that we do not wishthem to, including memorized personal information and knowledge that could beused to harm people. They can also output toxic or harmful text. To mitigatethese safety and informational issues, we propose an attack-and-defenseframework for studying the task of deleting sensitive information directly frommodel weights. We study direct edits to model weights because (1) this approachshould guarantee that particular deleted information is never extracted byfuture prompt attacks, and (2) it should protect against whitebox attacks,which is necessary for making claims about safety/privacy in a setting wherepublicly available model weights could be used to elicit sensitive information.Our threat model assumes that an attack succeeds if the answer to a sensitivequestion is located among a set of B generated candidates, based on scenarioswhere the information would be insecure if the answer is among B candidates.Experimentally, we show that even state-of-the-art model editing methods suchas ROME struggle to truly delete factual information from models like GPT-J, asour whitebox and blackbox attacks can recover \u0026ldquo;deleted\u0026rdquo; information from anedited model 38% of the time. These attacks leverage two key observations: (1)that traces of deleted information can be found in intermediate model hiddenstates, and (2) that applying an editing method for one question may not deleteinformation across rephrased versions of the question. Finally, we provide newdefense methods that protect against some extraction attacks, but we do notfind a single universally effective defense method. Our results suggest thattruly deleting sensitive information is a tractable but difficult problem,since even relatively low attack success rates have potentially severe societalimplications for real-world deployment of language models.\rRevolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\nSamuel Carreira Tomás Marques José Ribeiro Carlos Grilo\nabstract\rabstract: The field of Artificial Intelligence has witnessed remarkable progress inrecent years, especially with the emergence of powerful large language models(LLMs) based on the transformer architecture. Cloud-based LLMs, such asOpenAI\u0026rsquo;s ChatGPT, offer impressive capabilities but come with concernsregarding latency and privacy due to network dependencies. This articlepresents an innovative approach to LLM inference, envisioning a future whereLLMs with billions of parameters can be executed directly on mobile deviceswithout network connectivity. The article showcases a fine-tuned GPT LLM with 3billion parameters that can operate smoothly on devices with as low as 4GB ofmemory. Through the integration of native code and model quantizationtechniques, the application not only serves as a general-purpose assistant butalso facilitates seamless mobile interactions with text-to-actions features.The article provides insights into the training pipeline, implementationdetails, test results, and future directions of on-device LLM inference. Thisbreakthrough technology opens up possibilities for empowering users withsophisticated AI capabilities while preserving their privacy and eliminatinglatency concerns.\rMedical Foundation Models are Susceptible to Targeted Misinformation Attacks\nTianyu Han Sven Nebelung Firas Khader Tianci Wang Gustav Mueller-Franzes Christiane Kuhl Sebastian Försch Jens Kleesiek Christoph Haarburger Keno K. Bressem Jakob Nikolas Kather Daniel Truhn\nabstract\rabstract: Large language models (LLMs) have broad medical knowledge and can reasonabout medical information across many domains, holding promising potential fordiverse medical applications in the near future. In this study, we demonstratea concerning vulnerability of LLMs in medicine. Through targeted manipulationof just 1.1% of the model\u0026rsquo;s weights, we can deliberately inject an incorrectbiomedical fact. The erroneous information is then propagated in the model\u0026rsquo;soutput, whilst its performance on other biomedical tasks remains intact. Wevalidate our findings in a set of 1,038 incorrect biomedical facts. Thispeculiar susceptibility raises serious security and trustworthiness concernsfor the application of LLMs in healthcare settings. It accentuates the need forrobust protective measures, thorough verification mechanisms, and stringentmanagement of access to these models, ensuring their reliable and safe use inmedical practice.\r2023-09-28\nForgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble\nZhe Liu Ozlem Kalinli\nabstract\rabstract: Recent research has shown that language models have a tendency to memorizerare or unique token sequences in the training corpus. After deploying a model,practitioners might be asked to delete any personal information from the modelby individuals\u0026rsquo; requests. Re-training the underlying model every timeindividuals would like to practice their rights to be forgotten iscomputationally expensive. We employ a teacher-student framework and propose anovel leave-one-out ensemble method to unlearn the targeted textual sequencesthat need to be forgotten from the model. In our approach, multiple teachersare trained on disjoint sets; for each targeted sequence to be removed, weexclude the teacher trained on the set containing this sequence and aggregatethe predictions from remaining teachers to provide supervision duringfine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that theproposed method achieves superior privacy-utility trade-offs than othercounterparts.\r2023-09-27\nIdentifying and Mitigating Privacy Risks Stemming from Language Models: A Survey\nVictoria Smith Ali Shahin Shamsabadi Carolyn Ashurst Adrian Weller\nabstract\rabstract: Rapid advancements in language models (LMs) have led to their adoption acrossmany sectors. Alongside the potential benefits, such models present a range ofrisks, including around privacy. In particular, as LMs have grown in size, thepotential to memorise aspects of their training data has increased, resultingin the risk of leaking private information. As LMs become increasinglywidespread, it is vital that we understand such privacy risks and how theymight be mitigated. To help researchers and policymakers understand the stateof knowledge around privacy attacks and mitigations, including where more workis needed, we present the first technical survey on LM privacy. We (i) identifya taxonomy of salient dimensions where attacks differ on LMs, (ii) surveyexisting attacks and use our taxonomy of dimensions to highlight key trends,(iii) discuss existing mitigation strategies, highlighting their strengths andlimitations, identifying key gaps and demonstrating open problems and areas forconcern.\r2023-09-26\nPLMM: Personal Large Models on Mobile Devices\nYuanhao Gong\nabstract\rabstract: Inspired by Federated Learning, in this paper, we propose personal largemodels that are distilled from traditional large language models but moreadaptive to local users\u0026rsquo; personal information such as education background andhobbies. We classify the large language models into three levels: the personallevel, expert level and traditional level. The personal level models areadaptive to users\u0026rsquo; personal information. They encrypt the users\u0026rsquo; input andprotect their privacy. The expert level models focus on merging specificknowledge such as finance, IT and art. The traditional models focus on theuniversal knowledge discovery and upgrading the expert models. In suchclassifications, the personal models directly interact with the user. For thewhole system, the personal models have users\u0026rsquo; (encrypted) personal information.Moreover, such models must be small enough to be performed on personalcomputers or mobile devices. Finally, they also have to response in real-timefor better user experience and produce high quality results. The proposedpersonal large models can be applied in a wide range of applications such aslanguage and vision tasks.\r2023-09-25\nAn Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox\nChaoran Chen Weijun Li Wenxin Song Yanfang Ye Yaxing Yao Toby Jia-jun Li\nabstract\rabstract: The \u0026ldquo;privacy paradox\u0026rdquo; describes the discrepancy between users\u0026rsquo; privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users\u0026rsquo; hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes.\r2023-09-22\nRight to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions\nDawen Zhang Pamela Finckenberg-Broman Thong Hoang Shidong Pan Zhenchang Xing Mark Staples Xiwei Xu\nabstract\rabstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.\r2023-09-21\nMarkNerf:Watermarking for Neural Radiance Field\nLifeng Chen Jia Liu Yan Ke Wenquan Sun Weina Dong Xiaozhong Pan\nabstract\rabstract: A watermarking algorithm is proposed in this paper to address the copyrightprotection issue of implicit 3D models. The algorithm involves embeddingwatermarks into the images in the training set through an embedding network,and subsequently utilizing the NeRF model for 3D modeling. A copyright verifieris employed to generate a backdoor image by providing a secret perspective asinput to the neural radiation field. Subsequently, a watermark extractor isdevised using the hyperparameterization method of the neural network to extractthe embedded watermark image from that perspective. In a black box scenario, ifthere is a suspicion that the 3D model has been used without authorization, theverifier can extract watermarks from a secret perspective to verify networkcopyright. Experimental results demonstrate that the proposed algorithmeffectively safeguards the copyright of 3D models. Furthermore, the extractedwatermarks exhibit favorable visual effects and demonstrate robust resistanceagainst various types of noise attacks.\r2023-09-20\n\u0026ldquo;It\u0026rsquo;s a Fair Game\u0026rsquo;\u0026rsquo;, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents\nZhiping Zhang Michelle Jia Hao-Ping Lee Bingsheng Yao Sauvik Das Ada Lerner Dakuo Wang Tianshi Li\nabstract\rabstract: The widespread use of Large Language Model (LLM)-based conversational agents(CAs), especially in high-stakes domains, raises many privacy concerns.Building ethical LLM-based CAs that respect user privacy requires an in-depthunderstanding of the privacy risks that concern users the most. However,existing research, primarily model-centered, does not provide insight intousers\u0026rsquo; perspectives. To bridge this gap, we analyzed sensitive disclosures inreal-world ChatGPT conversations and conducted semi-structured interviews with19 LLM-based CA users. We found that users are constantly faced with trade-offsbetween privacy, utility, and convenience when using LLM-based CAs. However,users\u0026rsquo; erroneous mental models and the dark patterns in system design limitedtheir awareness and comprehension of the privacy risks. Additionally, thehuman-like interactions encouraged more sensitive disclosures, whichcomplicated users\u0026rsquo; ability to navigate the trade-offs. We discuss practicaldesign guidelines and the needs for paradigmatic shifts to protect the privacyof LLM-based CA users.\rPolicy Patterns for Usage Control in Data Spaces\nTobias Dam Andreas Krimbacher Sebastian Neumaier\nabstract\rabstract: Data-driven technologies have the potential to initiate a transportationrelated revolution in the way we travel, commute and navigate within cities. Asa major effort of this transformation relies on Mobility Data Spaces for theexchange of mobility data, the necessity to protect valuable data and formulateconditions for data exchange arises. This paper presents key contributions tothe development of automated contract negotiation and data usage policies inthe Mobility Data Space. A comprehensive listing of policy patterns for usagecontrol is provided, addressing common requirements and scenarios in datasharing and governance. The use of the Open Digital Rights Language (ODRL) isproposed to formalize the collected policies, along with an extension of theODRL vocabulary for data space-specific properties.\r2023-09-19\nSpecializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training\nRuiqi Xu Yongfeng Huang Xin Chen Lin Zhang\nabstract\rabstract: In this work, we introduce the concept of complex text style transfer tasks,and constructed complex text datasets based on two widely applicable scenarios.Our dataset is the first large-scale data set of its kind, with 700 rephrasedsentences and 1,000 sentences from the game Genshin Impact. While largelanguage models (LLM) have shown promise in complex text style transfer, theyhave drawbacks such as data privacy concerns, network instability, and highdeployment costs. To address these issues, we explore the effectiveness ofsmall models (less than T5-3B) with implicit style pre-training throughcontrastive learning. We also propose a method for automated evaluation of textgeneration quality based on alignment with human evaluations using ChatGPT.Finally, we compare our approach with existing methods and show that our modelachieves state-of-art performances of few-shot text style transfer models.\rFacilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation\nHuachuan Qiu Shuai Zhang Hongliang He Anqi Li Zhenzhong Lan\nabstract\rabstract: NSFW (Not Safe for Work) content, in the context of a dialogue, can havesevere side effects on users in open-domain dialogue systems. However, researchon detecting NSFW language, especially sexually explicit content, within adialogue context has significantly lagged behind. To address this issue, weintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialoguedetection. Leveraging knowledge distillation techniques involving GPT-4 andChatGPT, this dataset offers a cost-effective means of constructing NSFWcontent detectors. The process entails collecting real-life human-machineinteraction data and breaking it down into single utterances and single-turndialogues, with the chatbot delivering the final utterance. ChatGPT is employedto annotate unlabeled data, serving as a training set. Rationale validation andtest sets are constructed using ChatGPT and GPT-4 as annotators, with aself-criticism strategy for resolving discrepancies in labeling. A BERT modelis fine-tuned as a text classifier on pseudo-labeled data, and its performanceis assessed. The study emphasizes the importance of AI systems prioritizinguser safety and well-being in digital conversations while respecting freedom ofexpression. The proposed approach not only advances NSFW content detection butalso aligns with evolving user protection needs in AI-driven dialogues.\rDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass\nMinxin Du Xiang Yue Sherman S. M. Chow Tianhao Wang Chenyu Huang Huan Sun\nabstract\rabstract: Differentially private stochastic gradient descent (DP-SGD) adds noise togradients in back-propagation, safeguarding training data from privacy leakage,particularly membership inference. It fails to cover (inference-time) threatslike embedding inversion and sensitive attribute inference. It is also costlyin storage and computation when used to fine-tune large pre-trained languagemodels (LMs). We propose DP-Forward, which directly perturbs embedding matrices in theforward pass of LMs. It satisfies stringent local DP requirements for trainingand inference data. To instantiate it using the smallest matrix-valued noise,we devise an analytic matrix Gaussian~mechanism (aMGM) by drawing possiblynon-i.i.d. noise from a matrix Gaussian distribution. We then investigateperturbing outputs from different hidden (sub-)layers of LMs with aMGM noises.Its utility on three typical tasks almost hits the non-private baseline andoutperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves3$\\times$ time and memory costs compared to DP-SGD with the latest high-speedlibrary. It also reduces the average success rates of embedding inversion andsensitive attribute inference by up to 88pp and 41pp, respectively, whereasDP-SGD fails.\rLLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI\u0026rsquo;s ChatGPT Plugins\nUmar Iqbal Tadayoshi Kohno Franziska Roesner\nabstract\rabstract: Large language model (LLM) platforms, such as ChatGPT, have recently begunoffering a plugin ecosystem to interface with third-party services on theinternet. While these plugins extend the capabilities of LLM platforms, theyare developed by arbitrary third parties and thus cannot be implicitly trusted.Plugins also interface with LLM platforms and users using natural language,which can have imprecise interpretations. In this paper, we propose a frameworkthat lays a foundation for LLM platform designers to analyze and improve thesecurity, privacy, and safety of current and future plugin-integrated LLMplatforms. Our framework is a formulation of an attack taxonomy that isdeveloped by iteratively exploring how LLM platform stakeholders could leveragetheir capabilities and responsibilities to mount attacks against each other. Aspart of our iterative process, we apply our framework in the context ofOpenAI\u0026rsquo;s plugin ecosystem. We uncover plugins that concretely demonstrate thepotential for the types of issues that we outline in our attack taxonomy. Weconclude by discussing novel challenges and by providing recommendations toimprove the security, privacy, and safety of present and future LLM-basedcomputing platforms.\rDifferentially Private Optimization on Large Model at Small Cost\nZhiqi Bu Yu-Xiang Wang Sheng Zha George Karypis\nabstract\rabstract: Differentially private (DP) optimization is the standard paradigm to learnlarge neural networks that are accurate and privacy-preserving. Thecomputational cost for DP deep learning, however, is notoriously heavy due tothe per-sample gradient clipping. Existing DP implementations are 2-1000X morecostly in time and space complexity than the standard (non-private) training.In this work, we develop a novel Book-Keeping (BK) technique that implementsexisting DP optimizers (thus achieving the same accuracy), with a substantialimprovement on the computational cost. Specifically, BK enables DP training onlarge models and high dimensional data to be roughly as fast and memory-savingas the standard training, whereas previous DP algorithms can be inefficient orincapable of training due to memory error. The computational advantage of BK issupported by the complexity analysis as well as extensive experiments on visionand language tasks. Our implementation achieves state-of-the-art (SOTA)accuracy with very small extra cost: on GPT2 and at almost the same memory cost(\u0026lt;1% overhead), BK has 1.03X the time complexity of the standard training(0.83X training speed in practice), and 0.61X the time complexity of the mostefficient DP implementation (1.36X training speed in practice). We open-sourcethe codebase for the BK algorithm at the FastDP library(https://github.com/awslabs/fast-differential-privacy).\rPolicyGPT: Automated Analysis of Privacy Policies with Large Language Models\nChenhao Tang Zhengliang Liu Chong Ma Zihao Wu Yiwei Li Wei Liu Dajiang Zhu Quanzheng Li Xiang Li Tianming Liu Lei Fan\nabstract\rabstract: Privacy policies serve as the primary conduit through which online serviceproviders inform users about their data collection and usage procedures.However, in a bid to be comprehensive and mitigate legal risks, these policydocuments are often quite verbose. In practical use, users tend to click theAgree button directly rather than reading them carefully. This practice exposesusers to risks of privacy leakage and legal issues. Recently, the advent ofLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened newpossibilities for text analysis, especially for lengthy documents like privacypolicies. In this study, we investigate a privacy policy text analysisframework PolicyGPT based on the LLM. This framework was tested using twodatasets. The first dataset comprises of privacy policies from 115 websites,which were meticulously annotated by legal experts, categorizing each segmentinto one of 10 classes. The second dataset consists of privacy policies from304 popular mobile applications, with each sentence manually annotated andclassified into one of another 10 categories. Under zero-shot learningconditions, PolicyGPT demonstrated robust performance. For the first dataset,it achieved an accuracy rate of 97%, while for the second dataset, it attainedan 87% accuracy rate, surpassing that of the baseline machine learning andneural network models.\r2023-09-18\nSpecification-Driven Video Search via Foundation Models and Formal Verification\nYunhao Yang Jean-Raphaël Gaglione Sandeep Chinchali Ufuk Topcu\nabstract\rabstract: The increasing abundance of video data enables users to search for events ofinterest, e.g., emergency incidents. Meanwhile, it raises new concerns, such asthe need for preserving privacy. Existing approaches to video search requireeither manual inspection or a deep learning model with massive training. Wedevelop a method that uses recent advances in vision and language models, aswell as formal methods, to search for events of interest in video clipsautomatically and efficiently. The method consists of an algorithm to maptext-based event descriptions into linear temporal logic over finite traces(LTL$_f$) and an algorithm to construct an automaton encoding the videoinformation. Then, the method formally verifies the automaton representing thevideo against the LTL$_f$ specifications and adds the pertinent video clips tothe search result if the automaton satisfies the specifications. We providequalitative and quantitative analysis to demonstrate the video-searchingcapability of the proposed method. It achieves over 90 percent precision insearching over privacy-sensitive videos and a state-of-the-art autonomousdriving dataset.\rInstruction-Following Speech Recognition\nCheng-I Jeff Lai Zhiyun Lu Liangliang Cao Ruoming Pang\nabstract\rabstract: Conventional end-to-end Automatic Speech Recognition (ASR) models primarilyfocus on exact transcription tasks, lacking flexibility for nuanced userinteractions. With the advent of Large Language Models (LLMs) in speechprocessing, more organic, text-prompt-based interactions have become possible.However, the mechanisms behind these models\u0026rsquo; speech understanding and\u0026quot;reasoning\u0026quot; capabilities remain underexplored. To study this question from thedata perspective, we introduce instruction-following speech recognition,training a Listen-Attend-Spell model to understand and execute a diverse set offree-form text instructions. This enables a multitude of speech recognitiontasks \u0026ndash; ranging from transcript manipulation to summarization \u0026ndash; withoutrelying on predefined command sets. Remarkably, our model, trained from scratchon Librispeech, interprets and executes simple instructions without requiringLLMs or pre-trained speech modules. It also offers selective transcriptionoptions based on instructions like \u0026ldquo;transcribe first half and then turn offlistening,\u0026rdquo; providing an additional layer of privacy and safety compared toexisting LLMs. Our findings highlight the significant potential ofinstruction-following training to advance speech foundation models.\rEmpowering Fake-News Mitigation: Insights from Sharers\u0026rsquo; Social Media Post-Histories\nVerena Schoenmueller Simon J. Blanchard Gita V. Johar\nabstract\rabstract: Misinformation is a global concern and limiting its spread is critical forprotecting democracy, public health, and consumers. We propose that consumers\u0026rsquo;own social media post-histories are an underutilized data source to study whatleads them to share links to fake-news. In Study 1, we explore how textual cuesextracted from post-histories distinguish fake-news sharers from random socialmedia users and others in the misinformation ecosystem. Among other results, wefind across two datasets that fake-news sharers use more words related toanger, religion and power. In Study 2, we show that adding textual cues frompost-histories improves the accuracy of models to predict who is likely toshare fake-news. In Study 3, we provide a preliminary test of two mitigationstrategies deduced from Study 1 - activating religious values and reducinganger - and find that they reduce fake-news sharing and sharing more generally.In Study 4, we combine survey responses with users\u0026rsquo; verified Twitterpost-histories and show that using empowering language in a fact-checkingbrowser extension ad increases download intentions. Our research encouragesmarketers, misinformation scholars, and practitioners to use post-histories todevelop theories and test interventions to reduce the spread of misinformation.\r2023-09-17\nYour Room is not Private: Gradient Inversion Attack on Reinforcement Learning\nMiao Li Wenhao Ding Ding Zhao\nabstract\rabstract: The prominence of embodied Artificial Intelligence (AI), which empowersrobots to navigate, perceive, and engage within virtual environments, hasattracted significant attention, owing to the remarkable advancements incomputer vision and large language models. Privacy emerges as a pivotal concernwithin the realm of embodied AI, as the robot accesses substantial personalinformation. However, the issue of privacy leakage in embodied AI tasks,particularly in relation to reinforcement learning algorithms, has not receivedadequate consideration in research. This paper aims to address this gap byproposing an attack on the value-based algorithm and the gradient-basedalgorithm, utilizing gradient inversion to reconstruct states, actions, andsupervision signals. The choice of using gradients for the attack is motivatedby the fact that commonly employed federated learning techniques solely utilizegradients computed based on private user data to optimize models, withoutstoring or transmitting the data to public servers. Nevertheless, thesegradients contain sufficient information to potentially expose private data. Tovalidate our approach, we conduct experiments on the AI2THOR simulator andevaluate our algorithm on active perception, a prevalent task in embodied AI.The experimental results demonstrate the effectiveness of our method insuccessfully reconstructing all information from the data across 120 roomlayouts.\r2023-09-16\nData-Flow-Based Normalization Generation Algorithm of R1CS for Zero-Knowledge Proof\nChenhao Shi Hao Chen Ruibang Liu Guoqiang Li\nabstract\rabstract: The communities of blockchains and distributed ledgers have been stirred upby the introduction of zero-knowledge proofs (ZKPs). Originally designed tosolve privacy issues, ZKPs have now evolved into an effective remedy forscalability concerns and are applied in Zcash (internet money like Bitcoin). Toenable ZKPs, Rank-1 Constraint Systems (R1CS) offer a verifier for bi-linearequations. To accurately and efficiently represent R1CS, several language toolslike Circom, Noir, and Snarky have been proposed to automate the compilation ofadvanced programs into R1CS. However, due to the flexible nature of R1CSrepresentation, there can be significant differences in the compiled R1CS formsgenerated from circuit language programs with the same underlying semantics. Toaddress this issue, this paper uses a data-flow-based R1CS paradigm algorithm,which produces a standardized format for different R1CS instances withidentical semantics. By using the normalized R1CS format circuits, thecomplexity of circuits\u0026rsquo; verification can be reduced. In addition, this paperpresents an R1CS normalization algorithm benchmark, and our experimentalevaluation demonstrates the effectiveness and correctness of our methods.\r2023-09-15\nSystem Fingerprint Recognition for Deepfake Audio: An Initial Dataset and Investigation\nXinrui Yan Jiangyan Yi Chenglong Wang Jianhua Tao Junzuo Zhou Hao Gu Ruibo Fu\nabstract\rabstract: The rapid progress of deep speech synthesis models has posed significantthreats to society such as malicious content manipulation. Therefore, manystudies have emerged to detect the so-called deepfake audio. However, existingworks focus on the binary detection of real audio and fake audio. In real-worldscenarios such as model copyright protection and digital evidence forensics, itis needed to know what tool or model generated the deepfake audio to explainthe decision. This motivates us to ask: Can we recognize the systemfingerprints of deepfake audio? In this paper, we present the first deepfakeaudio dataset for system fingerprint recognition (SFR) and conduct an initialinvestigation. We collected the dataset from the speech synthesis systems ofseven Chinese vendors that use the latest state-of-the-art deep learningtechnologies, including both clean and compressed sets. In addition, tofacilitate the further development of system fingerprint recognition methods,we provide extensive benchmarks that can be compared and research findings. Thedataset will be publicly available. .\r2023-09-14\nDo Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code Completion Tools\nYizhan Huang Yichen Li Weibin Wu Jianping Zhang Michael R. Lyu\nabstract\rabstract: Neural Code Completion Tools (NCCTs) have reshaped the field of softwaredevelopment, which accurately suggest contextually-relevant code snippetsbenefiting from language modeling techniques. However, language models may emitthe training data verbatim during inference with appropriate prompts. Thismemorization property raises privacy concerns of commercial NCCTs about thehard-coded credential leakage, leading to unauthorized access to systems.Therefore, to answer whether NCCTs will inadvertently emit the hard-codedcredential, we propose an evaluation tool called Hard-coded Credential Revealer(HCR). HCR effectively constructs test prompts from GitHub code files withcredentials to trigger memorization phenomenon of commercial NCCTs. Then, HCRextracts credentials with pre-defined format from the responses by fourdesigned filters. We apply HCR to evaluate two representative commercial NCCTs:GitHub Copilot and Amazon CodeWhisperer and successfully extracted 2,702hard-coded credentials from Copilot and 129 secrets from CodeWhisper under theblack-box setting, among which at least 3.6% and 5.4% secrets are real stringsfrom GitHub repositories. Moreover, two operational credentials wereidentified. The experimental results raise the severe privacy concern of thepotential leakage of hard-coded credentials in the training data of commercialNCCTs.\rPreventing Unauthorized AI Over-Analysis by Medical Image Adversarial Watermarking\nXingxing Wei Bangzheng Pu Shiji Zhao Chen Chi Huazhu Fu\nabstract\rabstract: The advancement of deep learning has facilitated the integration ofArtificial Intelligence (AI) into clinical practices, particularly incomputer-aided diagnosis. Given the pivotal role of medical images in variousdiagnostic procedures, it becomes imperative to ensure the responsible andsecure utilization of AI techniques. However, the unauthorized utilization ofAI for image analysis raises significant concerns regarding patient privacy andpotential infringement on the proprietary rights of data custodians.Consequently, the development of pragmatic and cost-effective strategies thatsafeguard patient privacy and uphold medical image copyrights emerges as acritical necessity. In direct response to this pressing demand, we present apioneering solution named Medical Image Adversarial watermarking (MIAD-MARK).Our approach introduces watermarks that strategically mislead unauthorized AIdiagnostic models, inducing erroneous predictions without compromising theintegrity of the visual content. Importantly, our method integrates anauthorization protocol tailored for legitimate users, enabling the removal ofthe MIAD-MARK through encryption-generated keys. Through extensive experiments,we validate the efficacy of MIAD-MARK across three prominent medical imagedatasets. The empirical outcomes demonstrate the substantial impact of ourapproach, notably reducing the accuracy of standard AI diagnostic models to amere 8.57% under white box conditions and 45.83% in the more challenging blackbox scenario. Additionally, our solution effectively mitigates unauthorizedexploitation of medical images even in the presence of sophisticated watermarkremoval networks. Notably, those AI diagnosis networks exhibit a meager averageaccuracy of 38.59% when applied to images protected by MIAD-MARK, underscoringthe robustness of our safeguarding mechanism.\r2023-09-13\neDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models\nMinsik Cho Keivan A. Vahid Qichen Fu Saurabh Adya Carlo C Del Mundo Mohammad Rastegari Devang Naik Peter Zatloukal\nabstract\rabstract: Since Large Language Models or LLMs have demonstrated high-qualityperformance on many complex language tasks, there is a great interest inbringing these LLMs to mobile devices for faster responses and better privacyprotection. However, the size of LLMs (i.e., billions of parameters) requireshighly effective compression to fit into storage-limited devices. Among manycompression techniques, weight-clustering, a form of non-linear quantization,is one of the leading candidates for LLM compression, and supported by modernsmartphones. Yet, its training overhead is prohibitively significant for LLMfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shownthe state-of-the-art trade-off between compression ratio and accuracyregression, but its large memory complexity makes it nearly impossible to applyto train-time LLM compression. In this paper, we propose a memory-efficient DKMimplementation, eDKM powered by novel techniques to reduce the memory footprintof DKM by orders of magnitudes. For a given tensor to be saved on CPU for thebackward pass of DKM, we compressed the tensor by applying uniquification andsharding after checking if there is no duplicated tensor previously copied toCPU. Our experimental results demonstrate that \\prjname can fine-tune andcompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) withthe Alpaca dataset by reducing the train-time memory footprint of a decoderlayer by 130$\\times$, while delivering good accuracy on broader LLM benchmarks(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).\rSpeaker anonymization using orthogonal Householder neural network\nXiaoxiao Miao Xin Wang Erica Cooper Junichi Yamagishi Natalia Tomashenko\nabstract\rabstract: Speaker anonymization aims to conceal a speaker\u0026rsquo;s identity while preservingcontent information in speech. Current mainstream neural-network speakeranonymization systems disentangle speech into prosody-related, content, andspeaker representations. The speaker representation is then anonymized by aselection-based speaker anonymizer that uses a mean vector over a set ofrandomly selected speaker vectors from an external pool of English speakers.However, the resulting anonymized vectors are subject to severe privacy leakageagainst powerful attackers, reduction in speaker diversity, and languagemismatch problems for unseen-language speaker anonymization. To generatediverse, language-neutral speaker vectors, this paper proposes an anonymizerbased on an orthogonal Householder neural network (OHNN). Specifically, theOHNN acts like a rotation to transform the original speaker vectors intoanonymized speaker vectors, which are constrained to follow the distributionover the original speaker vector space. A basic classification loss isintroduced to ensure that anonymized speaker vectors from different speakershave unique speaker identities. To further protect speaker identities, animproved classification loss and similarity loss are used to pushoriginal-anonymized sample pairs away from each other. Experiments onVoicePrivacy Challenge datasets in English and the \\textit{AISHELL-3} datasetin Mandarin demonstrate the proposed anonymizer\u0026rsquo;s effectiveness.\r2023-09-12\nPrompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts\nZhi-Yi Chin Chieh-Ming Jiang Ching-Chun Huang Pin-Yu Chen Wei-Chen Chiu\nabstract\rabstract: Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shownremarkable ability in high-quality content generation, and become one of therepresentatives for the recent wave of transformative AI. Nevertheless, suchadvance comes with an intensifying concern about the misuse of this generativetechnology, especially for producing copyrighted or NSFW (i.e. not safe forwork) images. Although efforts have been made to filter inappropriateimages/prompts or remove undesirable concepts/styles via model fine-tuning, thereliability of these safety mechanisms against diversified problematic promptsremains largely unexplored. In this work, we propose Prompting4Debugging (P4D)as a debugging and red-teaming tool that automatically finds problematicprompts for diffusion models to test the reliability of a deployed safetymechanism. We demonstrate the efficacy of our P4D tool in uncovering newvulnerabilities of SD models with safety mechanisms. Particularly, our resultshows that around half of prompts in existing safe prompting benchmarks whichwere originally considered \u0026ldquo;safe\u0026rdquo; can actually be manipulated to bypass manydeployed safety mechanisms, including concept removal, negative prompt, andsafety guidance. Our findings suggest that, without comprehensive testing, theevaluations on limited safe prompting benchmarks can lead to a false sense ofsafety for text-to-image models.\rFingerprint Attack: Client De-Anonymization in Federated Learning\nQiongkai Xu Trevor Cohn Olga Ohrimenko\nabstract\rabstract: Federated Learning allows collaborative training without data sharing insettings where participants do not trust the central server and one another.Privacy can be further improved by ensuring that communication between theparticipants and the server is anonymized through a shuffle; decoupling theparticipant identity from their data. This paper seeks to examine whether sucha defense is adequate to guarantee anonymity, by proposing a novelfingerprinting attack over gradients sent by the participants to the server. Weshow that clustering of gradients can easily break the anonymization in anempirical study of learning federated language models on two language corpora.We then show that training with differential privacy can provide a practicaldefense against our fingerprint attack.\r2023-09-11\nPreventing Verbatim Memorization in Language Models Gives a False Sense of Privacy\nDaphne Ippolito Florian Tramèr Milad Nasr Chiyuan Zhang Matthew Jagielski Katherine Lee Christopher A. Choquette-Choo Nicholas Carlini\nabstract\rabstract: Studying data memorization in neural language models helps us understand therisks (e.g., to privacy or copyright) associated with models regurgitatingtraining data and aids in the development of countermeasures. Many prior works\u0026ndash; and some recently deployed defenses \u0026ndash; focus on \u0026ldquo;verbatim memorization\u0026rdquo;,defined as a model generation that exactly matches a substring from thetraining set. We argue that verbatim memorization definitions are toorestrictive and fail to capture more subtle forms of memorization.Specifically, we design and implement an efficient defense that perfectlyprevents all verbatim memorization. And yet, we demonstrate that this \u0026ldquo;perfect\u0026quot;filter does not prevent the leakage of training data. Indeed, it is easilycircumvented by plausible and minimally modified \u0026ldquo;style-transfer\u0026rdquo; prompts \u0026ndash;and in some cases even the non-modified original prompts \u0026ndash; to extractmemorized information. We conclude by discussing potential alternativedefinitions and why defining memorization is a difficult yet crucial openquestion for neural language models.\rPrivacy Side Channels in Machine Learning Systems\nEdoardo Debenedetti Giorgio Severi Nicholas Carlini Christopher A. Choquette-Choo Matthew Jagielski Milad Nasr Eric Wallace Florian Tramèr\nabstract\rabstract: Most current approaches for protecting privacy in machine learning (ML)assume that models exist in a vacuum, when in reality, ML models are part oflarger systems that include components for training data filtering, outputmonitoring, and more. In this work, we introduce privacy side channels: attacksthat exploit these system-level components to extract private information atfar higher rates than is otherwise possible for standalone models. We proposefour categories of side channels that span the entire ML lifecycle (trainingdata filtering, input preprocessing, output post-processing, and queryfiltering) and allow for either enhanced membership inference attacks or evennovel threats such as extracting users\u0026rsquo; test queries. For example, we show thatdeduplicating training data before applying differentially-private trainingcreates a side-channel that completely invalidates any provable privacyguarantees. Moreover, we show that systems which block language models fromregenerating training data can be exploited to allow exact reconstruction ofprivate keys contained in the training set \u0026ndash; even if the model did notmemorize these keys. Taken together, our results demonstrate the need for aholistic, end-to-end privacy analysis of machine learning.\r2023-09-10\nUncloneable Quantum Advice\nAnne Broadbent Martti Karvonen Sébastien Lord\nabstract\rabstract: The famous no-cloning principle has been shown recently to enable a number ofuncloneable functionalities. Here we address for the first time unkeyed quantumuncloneablity, via the study of a complexity-theoretic tool that enables acomputation, but that is natively unkeyed: quantum advice. Remarkably, this isan application of the no-cloning principle in a context where the quantumstates of interest are not chosen by a random process. We show theunconditional existence of promise problems admitting uncloneable quantumadvice, and the existence of languages with uncloneable advice, assuming thefeasibility of quantum copy-protecting certain functions. Along the way, wenote that state complexity classes, introduced by Rosenthal and Yuen (ITCS2022) - which concern the computational difficulty of synthesizing sequences ofquantum states - can be naturally generalized to obtain state cloningcomplexity classes. We make initial observations on these classes, notablyobtaining a result analogous to the existence of undecidable problems. Our proof technique establishes the existence of ingenerable sequences offinite bit strings - essentially meaning that they cannot be generated by anyuniform circuit family. We then prove a generic result showing that thedifficulty of accomplishing a computational task on uniformly random inputsimplies its difficulty on any fixed, ingenerable sequence. We use this resultto derandomize quantum cryptographic games that relate to cloning, and thenincorporate a result of Kundu and Tan (arXiv 2022) to obtain uncloneableadvice. Applying this two-step process to a monogamy-of-entanglement gameyields a promise problem with uncloneable advice, and applying it to thequantum copy-protection of pseudorandom functions with super-logarithmic outputlengths yields a language with uncloneable advice.\r2023-09-08\nLLMCad: Fast and Scalable On-device Large Language Model Inference\nDaliang Xu Wangsong Yin Xin Jin Ying Zhang Shiyun Wei Mengwei Xu Xuanzhe Liu\nabstract\rabstract: Generative tasks, such as text generation and question answering, hold acrucial position in the realm of mobile applications. Due to their sensitivityto privacy concerns, there is a growing demand for their execution directly onmobile devices. Currently, the execution of these generative tasks heavilydepends on Large Language Models (LLMs). Nevertheless, the limited memorycapacity of these devices presents a formidable challenge to the scalability ofsuch models. In our research, we introduce LLMCad, an innovative on-device inferenceengine specifically designed for efficient generative Natural LanguageProcessing (NLP) tasks. The core idea behind LLMCad revolves around modelcollaboration: a compact LLM, residing in memory, takes charge of generatingthe most straightforward tokens, while a high-precision LLM steps in tovalidate these tokens and rectify any identified errors. LLMCad incorporatesthree novel techniques: (1) Instead of generating candidate tokens in asequential manner, LLMCad employs the smaller LLM to construct a token tree,encompassing a wider range of plausible token pathways. Subsequently, thelarger LLM can efficiently validate all of these pathways simultaneously. (2)It employs a self-adjusting fallback strategy, swiftly initiating theverification process whenever the smaller LLM generates an erroneous token. (3)To ensure a continuous flow of token generation, LLMCad speculatively generatestokens during the verification process by implementing a compute-IO pipeline.Through an extensive series of experiments, LLMCad showcases an impressivetoken generation speed, achieving rates up to 9.3x faster than existinginference engines.\r2023-09-07\nEnhancing Pipeline-Based Conversational Agents with Large Language Models\nMina Foosherian Hendrik Purwins Purna Rathnayake Touhidul Alam Rui Teimao Klaus-Dieter Thoben\nabstract\rabstract: The latest advancements in AI and deep learning have led to a breakthrough inlarge language model (LLM)-based agents such as GPT-4. However, many commercialconversational agent development tools are pipeline-based and have limitationsin holding a human-like conversation. This paper investigates the capabilitiesof LLMs to enhance pipeline-based conversational agents during two phases: 1)in the design and development phase and 2) during operations. In 1) LLMs canaid in generating training data, extracting entities and synonyms,localization, and persona design. In 2) LLMs can assist in contextualization,intent classification to prevent conversational breakdown and handleout-of-scope questions, auto-correcting utterances, rephrasing responses,formulating disambiguation questions, summarization, and enabling closedquestion-answering capabilities. We conducted informal experiments with GPT-4in the private banking domain to demonstrate the scenarios above with apractical example. Companies may be hesitant to replace their pipeline-basedagents with LLMs entirely due to privacy concerns and the need for deepintegration within their existing ecosystems. A hybrid approach in which LLMs\u0026rsquo;are integrated into the pipeline-based agents allows them to save time andcosts of building and running agents by capitalizing on the capabilities ofLLMs while retaining the integration and privacy safeguards of their existingsystems.\rReuNify: A Step Towards Whole Program Analysis for React Native Android Apps\nYonghui Liu Xiao Chen Pei Liu John Grundy Chunyang Chen Li Li\nabstract\rabstract: React Native is a widely-used open-source framework that facilitates thedevelopment of cross-platform mobile apps. The framework enables JavaScriptcode to interact with native-side code, such as Objective-C/Swift for iOS andJava/Kotlin for Android, via a communication mechanism provided by ReactNative. However, previous research and tools have overlooked this mechanism,resulting in incomplete analysis of React Native app code. To address thislimitation, we have developed REUNIFY, a prototype tool that integrates theJavaScript and native-side code of React Native apps into an intermediatelanguage that can be processed by the Soot static analysis framework. By doingso, REUNIFY enables the generation of a comprehensive model of the app\u0026rsquo;sbehavior. Our evaluation indicates that, by leveraging REUNIFY, the Soot-basedframework can improve its coverage of static analysis for the 1,007 mostpopular React Native Android apps, augmenting the number of lines of Jimplecode by 70%. Additionally, we observed an average increase of 84% in new nodesreached in the callgraph for these apps, after integrating REUNIFY. WhenREUNIFY is used for taint flow analysis, an average of two additional privacyleaks were identified. Overall, our results demonstrate that REUNIFYsignificantly enhances the Soot-based framework\u0026rsquo;s capability to analyze ReactNative Android apps.\rVeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints\nAoting Hu Zhigang Lu Renjie Xie Minhui Xue\nabstract\rabstract: Deploying Machine Learning as a Service gives rise to model plagiarism,leading to copyright infringement. Ownership testing techniques are designed toidentify model fingerprints for verifying plagiarism. However, previous worksoften rely on overfitting or robustness features as fingerprints, lackingtheoretical guarantees and exhibiting under-performance on generalized models.In this paper, we propose a novel ownership testing method called VeriDIP,which verifies a DNN model\u0026rsquo;s intellectual property. VeriDIP makes two majorcontributions. (1) It utilizes membership inference attacks to estimate thelower bound of privacy leakage, which reflects the fingerprint of a givenmodel. The privacy leakage fingerprints highlight the unique patterns throughwhich the models memorize sensitive training datasets. (2) We introduce a novelapproach using less private samples to enhance the performance of ownershiptesting. Extensive experimental results confirm that VeriDIP is effective andefficient in validating the ownership of deep learning models trained on bothimage and tabular datasets. VeriDIP achieves comparable performance tostate-of-the-art methods on image datasets while significantly reducingcomputation and communication costs. Enhanced VeriDIP demonstrates superiorverification performance on generalized deep learning models, particularly ontable-trained models. Additionally, VeriDIP exhibits similar effectiveness onutility-preserving differentially private models compared to non-differentiallyprivate baselines.\r2023-09-06\nPublicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes\nSunjun Kweon Junu Kim Jiyoun Kim Sujeong Im Eunbyeol Cho Seongsu Bae Jungwoo Oh Gyubok Lee Jong Hak Moon Seng Chan You Seungjin Baek Chang Hoon Han Yoon Bin Jung Yohan Jo Edward Choi\nabstract\rabstract: The development of large language models tailored for handling patients\u0026rsquo;clinical notes is often hindered by the limited accessibility and usability ofthese notes due to strict privacy regulations. To address these challenges, wefirst create synthetic large-scale clinical notes using publicly available casereports extracted from biomedical literature. We then use these synthetic notesto train our specialized clinical large language model, Asclepius. WhileAsclepius is trained on synthetic data, we assess its potential performance inreal-world applications by evaluating it using real clinical notes. Webenchmark Asclepius against several other large language models, includingGPT-3.5-turbo and other open-source alternatives. To further validate ourapproach using synthetic notes, we also compare Asclepius with its variantstrained on real clinical notes. Our findings convincingly demonstrate thatsynthetic clinical notes can serve as viable substitutes for real ones whenconstructing high-performing clinical language models. This conclusion issupported by detailed evaluations conducted by both GPT-4 and medicalprofessionals. All resources including weights, codes, and data used in thedevelopment of Asclepius are made publicly accessible for future research.\rMy Art My Choice: Adversarial Protection Against Unruly AI\nAnthony Rhodes Ram Bhagat Umur Aybars Ciftci Ilke Demir\nabstract\rabstract: Generative AI is on the rise, enabling everyone to produce realistic contentvia publicly available interfaces. Especially for guided image generation,diffusion models are changing the creator economy by producing high quality lowcost content. In parallel, artists are rising against unruly AI, since theirartwork are leveraged, distributed, and dissimulated by large generativemodels. Our approach, My Art My Choice (MAMC), aims to empower content ownersby protecting their copyrighted materials from being utilized by diffusionmodels in an adversarial fashion. MAMC learns to generate adversariallyperturbed \u0026ldquo;protected\u0026rdquo; versions of images which can in turn \u0026ldquo;break\u0026rdquo; diffusionmodels. The perturbation amount is decided by the artist to balance distortionvs. protection of the content. MAMC is designed with a simple UNet-basedgenerator, attacking black box diffusion models, combining several losses tocreate adversarial twins of the original artwork. We experiment on threedatasets for various image-to-image tasks, with different user control values.Both protected image and diffusion output results are evaluated in visual,noise, structure, pixel, and generative spaces to validate our claims. Webelieve that MAMC is a crucial step for preserving ownership information for AIgenerated content in a flawless, based-on-need, and human-centric way.\rHide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection\nYu Chen Tingxin Li Huiming Liu Yang Yu\nabstract\rabstract: Numerous companies have started offering services based on large languagemodels (LLM), such as ChatGPT, which inevitably raises privacy concerns asusers\u0026rsquo; prompts are exposed to the model provider. Previous research on securereasoning using multi-party computation (MPC) has proven to be impractical forLLM applications due to its time-consuming and communication-intensive nature.While lightweight anonymization techniques can protect private information inprompts through substitution or masking, they fail to recover sensitive datareplaced in the LLM-generated results. In this paper, we expand the applicationscenarios of anonymization techniques by training a small local model tode-anonymize the LLM\u0026rsquo;s returned results with minimal computational overhead. Weintroduce the HaS framework, where \u0026ldquo;H(ide)\u0026rdquo; and \u0026ldquo;S(eek)\u0026rdquo; represent its two coreprocesses: hiding private entities for anonymization and seeking privateentities for de-anonymization, respectively. To quantitatively assess HaS\u0026rsquo;sprivacy protection performance, we propose both black-box and white-boxadversarial models. Furthermore, we conduct experiments to evaluate HaS\u0026rsquo;susability in translation and classification tasks. The experimental findingsdemonstrate that the HaS framework achieves an optimal balance between privacyprotection and utility.\rAI for Investment: A Platform Disruption\nMohammad Rasouli Ravi Chiruvolu Ali Risheh\nabstract\rabstract: With the investment landscape becoming more competitive, efficiently scalingdeal sourcing and improving deal insights have become a dominant strategy forfunds. While funds are already spending significant efforts on these two tasks,they cannot be scaled with traditional approaches; hence, there is a surge inautomating them. Many third party software providers have emerged recently toaddress this need with productivity solutions, but they fail due to a lack ofpersonalization for the fund, privacy constraints, and natural limits ofsoftware use cases. Therefore, most major funds and many smaller funds havestarted developing their in-house AI platforms: a game changer for theindustry. These platforms grow smarter by direct interactions with the fund andcan be used to provide personalized use cases. Recent developments in largelanguage models, e.g. ChatGPT, have provided an opportunity for other funds toalso develop their own AI platforms. While not having an AI platform now is nota competitive disadvantage, it will be in two years. Funds require a practicalplan and corresponding risk assessments for such AI platforms.\rAutomated Bioinformatics Analysis via AutoBA\nJuexiao Zhou Bin Zhang Xiuying Chen Haoyang Li Xiaopeng Xu Siyuan Chen Xin Gao\nabstract\rabstract: With the fast-growing and evolving omics data, the demand for streamlined andadaptable tools to handle the analysis continues to grow. In response to thisneed, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AIagent based on a large language model designed explicitly for conventionalomics data analysis. AutoBA simplifies the analytical process by requiringminimal user input while delivering detailed step-by-step plans for variousbioinformatics tasks. Through rigorous validation by expert bioinformaticians,AutoBA\u0026rsquo;s robustness and adaptability are affirmed across a diverse range ofomics analysis cases, including whole genome sequencing (WGS), RNA sequencing(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA\u0026rsquo;sunique capacity to self-design analysis processes based on input datavariations further underscores its versatility. Compared with onlinebioinformatic services, AutoBA deploys the analysis locally, preserving dataprivacy. Moreover, different from the predefined pipeline, AutoBA hasadaptability in sync with emerging bioinformatics tools. Overall, AutoBArepresents a convenient tool, offering robustness and adaptability for complexomics data analysis.\r2023-09-05\n\u0026ldquo;An Adapt-or-Die Type of Situation\u0026rdquo;: Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals\nVeera Vimpari Annakaisa Kultima Perttu Hämäläinen Christian Guckelsberger\nabstract\rabstract: Text-to-image generation (TTIG) models, a recent addition to creative AI, cangenerate images based on a text description. These models have begun to rivalthe work of professional creatives, and sparked discussions on the future ofcreative work, loss of jobs, and copyright issues, amongst other importantimplications. To support the sustainable adoption of TTIG, we must providerich, reliable and transparent insights into how professionals perceive, adoptand use TTIG. Crucially though, the public debate is shallow, narrow andlacking transparency, while academic work has focused on studying the use ofTTIG in a general artist population, but not on the perceptions and attitudesof professionals in a specific industry. In this paper, we contribute aqualitative, exploratory interview study on TTIG in the Finnish videogameindustry. Through a Template Analysis on semi-structured interviews with 14game professionals, we reveal 12 overarching themes, structured into 49sub-themes on professionals\u0026rsquo; perception, adoption and use of TTIG systems ingames industry practice. Experiencing (yet another) change of roles andcreative processes, our participants\u0026rsquo; reflections can inform discussions withinthe industry, be used by policymakers to inform urgently needed legislation,and support researchers in games, HCI and AI to support the sustainable,professional use of TTIG to benefit people and games as cultural artefacts.\r2023-09-03\nFusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs\nZhenheng Tang Yuxin Wang Xin He Longteng Zhang Xinglin Pan Qiang Wang Rongfei Zeng Kaiyong Zhao Shaohuai Shi Bingsheng He Xiaowen Chu\nabstract\rabstract: The rapid growth of memory and computation requirements of large languagemodels (LLMs) has outpaced the development of hardware, hindering people wholack large-scale high-end GPUs from training or deploying LLMs. However,consumer-level GPUs, which constitute a larger market share, are typicallyoverlooked in LLM due to their weaker computing performance, smaller storagecapacity, and lower communication bandwidth. Additionally, users may haveprivacy concerns when interacting with remote LLMs. In this paper, we envisiona decentralized system unlocking the potential vast untapped consumer-levelGPUs in pre-training, inference and fine-tuning of LLMs with privacyprotection. However, this system faces critical challenges, including limitedCPU and GPU memory, low network bandwidth, the variability of peer and deviceheterogeneity. To address these challenges, our system design incorporates: 1)a broker with backup pool to implement dynamic join and quit of computingproviders; 2) task scheduling with hardware performance to improve systemefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) toachieve model and task universality; 4) abstracting intermediate representionand execution planes to ensure compatibility of various devices and deeplearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which aresignificantly more expensive.\r2023-09-02\nCombing for Credentials: Active Pattern Extraction from Smart Reply\nBargav Jayaraman Esha Ghosh Melissa Chase Sambuddha Roy Wei Dai David Evans\nabstract\rabstract: Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply\u0026rsquo;\u0026rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.\rValue Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties\nTaylor Sorensen Liwei Jiang Jena Hwang Sydney Levine Valentina Pyatkin Peter West Nouha Dziri Ximing Lu Kavel Rao Chandra Bhagavatula Maarten Sap John Tasioulas Yejin Choi\nabstract\rabstract: Human values are crucial to human decision-making. Value pluralism is theview that multiple correct values may be held in tension with one another(e.g., when considering lying to a friend to protect their feelings, how doesone balance honesty with friendship?). As statistical learners, AI systems fitto averages by default, washing out these potentially irreducible valueconflicts. To improve AI systems to better reflect value pluralism, thefirst-order challenge is to explore the extent to which AI systems can modelpluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, andduties connected to 31k human-written situations. ValuePrism\u0026rsquo;s contextualizedvalues are generated by GPT-4 and deemed high-quality by human annotators 91%of the time. We conduct a large-scale study with annotators across diversesocial and demographic backgrounds to try to understand whose values arerepresented. With ValuePrism, we build Kaleido, an open, light-weight, and structuredlanguage-based multi-task model that generates, explains, and assesses therelevance and valence (i.e., support or oppose) of human values, rights, andduties within a specific context. Humans prefer the sets of values output byour system over the teacher GPT-4, finding them more accurate and with broadercoverage. In addition, we demonstrate that Kaleido can help explain variabilityin human decision-making by outputting contrasting values. Finally, we showthat Kaleido\u0026rsquo;s representations transfer to other philosophical frameworks anddatasets, confirming the benefit of an explicit, modular, and interpretableapproach to value pluralism. We hope that our work will serve as a step tomaking more explicit the implicit values behind human decision-making and tosteering AI systems to make decisions that are more in accordance with them.\r2023-09-01\nDesigning a realistic peer-like embodied conversational agent for supporting children\u0026rsquo;s storytelling\nZhixin Li Ying Xu\nabstract\rabstract: Advances in artificial intelligence have facilitated the use of largelanguage models (LLMs) and AI-generated synthetic media in education, which mayinspire HCI researchers to develop technologies, in particular, embodiedconversational agents (ECAs) to simulate the kind of scaffolding children mightreceive from a human partner. In this paper, we will propose a design prototypeof a peer-like ECA named STARie that integrates multiple AI models - GPT-3,Speech Synthesis (Real-time Voice Cloning), VOCA (Voice Operated CharacterAnimation), and FLAME (Faces Learned with an Articulated Model and Expressions)that aims to support narrative production in collaborative storytelling,specifically for children aged 4-8. However, designing a child-centered ECAraises concerns about age appropriateness, children privacy, gender choices ofECAs, and the uncanny valley effect. Thus, this paper will also discussconsiderations and ethical concerns that must be taken into account whendesigning such an ECA. This proposal offers insights into the potential use ofAI-generated synthetic media in child-centered AI design and how peer-like AIembodiment may support children\\textquotesingle s storytelling.\r2023-08-31\nContinual Learning From a Stream of APIs\nEnneng Yang Zhenyi Wang Li Shen Nan Yin Tongliang Liu Guibing Guo Xingwei Wang Dacheng Tao\nabstract\rabstract: Continual learning (CL) aims to learn new tasks without forgetting previoustasks. However, existing CL methods require a large amount of raw data, whichis often unavailable due to copyright considerations and privacy risks.Instead, stakeholders usually release pre-trained machine learning models as aservice (MLaaS), which users can access via APIs. This paper considers twopractical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL(DFCL-APIs), which achieve CL from a stream of APIs with partial or no rawdata. Performing CL under these two new settings faces several challenges:unavailable full raw data, unknown model parameters, heterogeneous models ofarbitrary architecture and scale, and catastrophic forgetting of previous APIs.To overcome these issues, we propose a novel data-free cooperative continualdistillation learning framework that distills knowledge from a stream of APIsinto a CL model by generating pseudo data, just by querying APIs. Specifically,our framework includes two cooperative generators and one CL model, formingtheir training as an adversarial game. We first use the CL model and thecurrent API as fixed discriminators to train generators via a derivative-freemethod. Generators adversarially generate hard and diverse synthetic data tomaximize the response gap between the CL model and the API. Next, we train theCL model by minimizing the gap between the responses of the CL model and theblack-box API on synthetic data, to transfer the API\u0026rsquo;s knowledge to the CLmodel. Furthermore, we propose a new regularization term based on networksimilarity to prevent catastrophic forgetting of previous APIs.Our methodperforms comparably to classic CL with full raw data on the MNIST and SVHN inthe DFCL-APIs setting. In the DECL-APIs setting, our method achieves 0.97x,0.75x and 0.69x performance of classic CL on CIFAR10, CIFAR100, andMiniImageNet.\r2023-08-30\nGrandma Karl is 27 years old \u0026ndash; research agenda for pseudonymization of research data\nElena Volodina Simon Dobnik Therese Lindström Tiedemann Xuan-Son Vu\nabstract\rabstract: Accessibility of research data is critical for advances in many researchfields, but textual data often cannot be shared due to the personal andsensitive information which it contains, e.g names or political opinions.General Data Protection Regulation (GDPR) suggests pseudonymization as asolution to secure open access to research data, but we need to learn moreabout pseudonymization as an approach before adopting it for manipulation ofresearch data. This paper outlines a research agenda within pseudonymization,namely need of studies into the effects of pseudonymization on unstructureddata in relation to e.g. readability and language assessment, as well as theeffectiveness of pseudonymization as a way of protecting writer identity, whilealso exploring different ways of developing context-sensitive algorithms fordetection, labelling and replacement of personal information in unstructureddata. The recently granted project on pseudonymization Grandma Karl is 27 yearsold addresses exactly those challenges.\rIntroducing Language Guidance in Prompt-based Continual Learning\nMuhammad Gul Zain Ali Khan Muhammad Ferjad Naeem Luc Van Gool Didier Stricker Federico Tombari Muhammad Zeshan Afzal\nabstract\rabstract: Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters.\rOn the culture of open access: the Sci-hub paradox\nAbdelghani Maddi David Sapinho\nabstract\rabstract: Shadow libraries, also known as \u0026lsquo;\u0026lsquo;pirate libraries\u0026rsquo;\u0026rsquo;, are online collectionsof copyrighted publications that have been made available for free without thepermission of the copyright holders. They have gradually become key players ofscientific knowledge dissemination, despite their illegality in most countriesof the world. Many publishers and scientist-editors decry such libraries fortheir copyright infringement and loss of publication usage information, whilesome scholars and institutions support them, sometimes in a roundabout way, fortheir role in reducing inequalities of access to knowledge, particularly inlow-income countries. Although there is a wealth of literature on shadowlibraries, none of this have focused on its potential role in knowledgedissemination, through the open access movement. Here we analyze how shadowlibraries can affect researchers\u0026rsquo; citation practices, highlighting somecounter-intuitive findings about their impact on the Open Access CitationAdvantage (OACA). Based on a large randomized sample, this study first showsthat OA publications, including those in fully OA journals, receive morecitations than their subscription-based counterparts do. However, the OACA hasslightly decreased over the seven last years. The introduction of a distinctionbetween those accessible or not via the Scihub platform amongsubscription-based suggest that the generalization of its use cancels thepositive effect of OA publishing. The results show that publications in fullyOA journals are victims of the success of Sci-hub. Thus, paradoxically,although Sci-hub may seem to facilitate access to scientific knowledge, itnegatively affects the OA movement as a whole, by reducing the comparativeadvantage of OA publications in terms of visibility for researchers. Thedemocratization of the use of Sci-hub may therefore lead to a vicious cycle,hindering efforts to develop full OA strategies without proposing a credibleand sustainable alternative model for the dissemination of scientificknowledge.\r2023-08-29\nTransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification\nJianing Wang Chengyu Wang Cen Chen Ming Gao Jun Huang Aoying Zhou\nabstract\rabstract: Text classification is one of the most imperative tasks in natural languageprocessing (NLP). Recent advances with pre-trained language models (PLMs) haveshown remarkable success on this task. However, the satisfying results obtainedby PLMs heavily depend on the large amounts of task-specific labeled data,which may not be feasible in many application scenarios due to data access andprivacy constraints. The recently-proposed prompt-based fine-tuning paradigmimproves the performance of PLMs for few-shot text classification withtask-specific templates. Yet, it is unclear how the prompting knowledge can betransferred across tasks, for the purpose of mutual reinforcement. We proposeTransPrompt v2, a novel transferable prompting framework for few-shot learningacross similar or distant text classification tasks. For learning acrosssimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)procedure to train a meta-learner that captures the cross-task transferableknowledge. For learning across distant tasks, we further inject the task typedescriptions into the prompt, and capture the intra-type and inter-type promptembeddings among multiple distant tasks. Additionally, two de-biasingtechniques are further designed to make the trained meta-learner moretask-agnostic and unbiased towards any tasks. After that, the meta-learner canbe adapted to each specific task with better parameters initialization.Extensive experiments show that TransPrompt v2 outperforms single-task andcross-task strong baselines over multiple NLP tasks and datasets. We furthershow that the meta-learner can effectively improve the performance of PLMs onpreviously unseen tasks. In addition, TransPrompt v2 also outperforms strongfine-tuning baselines when learning with full training sets.\rCEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction\nUmar Khalid Hasan Iqbal Saeed Vahidian Jing Hua Chen Chen\nabstract\rabstract: Human-robot interaction (HRI) is a rapidly growing field that encompassessocial and industrial applications. Machine learning plays a vital role inindustrial HRI by enhancing the adaptability and autonomy of robots in complexenvironments. However, data privacy is a crucial concern in the interactionbetween humans and robots, as companies need to protect sensitive data whilemachine learning algorithms require access to large datasets. FederatedLearning (FL) offers a solution by enabling the distributed training of modelswithout sharing raw data. Despite extensive research on Federated learning (FL)for tasks such as natural language processing (NLP) and image classification,the question of how to use FL for HRI remains an open research problem. Thetraditional FL approach involves transmitting large neural network parametermatrices between the server and clients, which can lead to high communicationcosts and often becomes a bottleneck in FL. This paper proposes acommunication-efficient FL framework for human-robot interaction (CEFHRI) toaddress the challenges of data heterogeneity and communication costs. Theframework leverages pre-trained models and introduces a trainablespatiotemporal adapter for video understanding tasks in HRI. Experimentalresults on three human-robot interaction benchmark datasets: HRI30, InHARD, andCOIN demonstrate the superiority of CEFHRI over full fine-tuning in terms ofcommunication costs. The proposed methodology provides a secure and efficientapproach to HRI federated learning, particularly in industrial environmentswith data privacy concerns and limited communication bandwidth. Our code isavailable athttps://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.\r2023-08-28\nChallenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption\nAlaina N. Talboy Elizabeth Fuller\nabstract\rabstract: Assessments of algorithmic bias in large language models (LLMs) are generallycatered to uncovering systemic discrimination based on protectedcharacteristics such as sex and ethnicity. However, there are over 180documented cognitive biases that pervade human reasoning and decision makingthat are routinely ignored when discussing the ethical complexities of AI. Wedemonstrate the presence of these cognitive biases in LLMs and discuss theimplications of using biased reasoning under the guise of expertise. We callfor stronger education, risk management, and continued research as widespreadadoption of this technology increases. Finally, we close with a set of bestpractices for when and how to employ this technology as widespread adoptioncontinues to grow.\rNoctalgia (sky grief): Our Brightening Night Skies and Loss of Environment for Astronomy and Sky Traditions\nAparna Venkatesan John C. Barentine\nabstract\rabstract: Fifty years after the first mention of light pollution in Science, thejournal recently elevated this topic to the cover of its 16 June 2023 issue,highlighting the large impact on human and ecological health, circadianrhythms, migratory patterns, and more. We offer here the term noctalgia toexpress \u0026ldquo;sky grief\u0026rdquo; for the accelerating loss of the home environment of ourshared skies - representing loss of science, heritage, millennia-old skytraditions, place-based language, and more - and summarize next steps toaddress the protection of our nighttime and daytime skies.\rCodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models\nZhensu Sun Xiaoning Du Fu Song Li Li\nabstract\rabstract: Code datasets are of immense value for training neural-network-based codecompletion models, where companies or organizations have made substantialinvestments to establish and process these datasets. Unluckily, these datasets,either built for proprietary or public usage, face the high risk ofunauthorized exploits, resulting from data leakages, license violations, etc.Even worse, the ``black-box\u0026rsquo;\u0026rsquo; nature of neural models sets a high barrier forexternals to audit their training datasets, which further connives theseunauthorized usages. Currently, watermarking methods have been proposed toprohibit inappropriate usage of image and natural language datasets. However,due to domain specificity, they are not directly applicable to code datasets,leaving the copyright protection of this emerging and important field of codedata still exposed to threats. To fill this gap, we propose a method, namedCodeMark, to embed user-defined imperceptible watermarks into code datasets totrace their usage in training neural code completion models. CodeMark is basedon adaptive semantic-preserving transformations, which preserve the exactfunctionality of the code data and keep the changes covert againstrule-breakers. We implement CodeMark in a toolkit and conduct an extensiveevaluation of code completion models. CodeMark is validated to fulfill alldesired properties of practical watermarks, including harmlessness to modelaccuracy, verifiability, robustness, and imperceptibility.\rEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models\nRongjie Yi Liwei Guo Shiyun Wei Ao Zhou Shangguang Wang Mengwei Xu\nabstract\rabstract: Large Language Models (LLMs) such as GPTs and LLaMa have ushered in arevolution in machine intelligence, owing to their exceptional capabilities ina wide range of machine learning tasks. However, the transition of LLMs fromdata centers to edge devices presents a set of challenges and opportunities.While this shift can enhance privacy and availability, it is hampered by theenormous parameter sizes of these models, leading to impractical runtime costs.In light of these considerations, we introduce EdgeMoE, the first on-deviceinference engine tailored for mixture-of-expert (MoE) LLMs, a popular variantof sparse LLMs that exhibit nearly constant computational complexity as theirparameter size scales. EdgeMoE achieves both memory and computationalefficiency by strategically partitioning the model across the storagehierarchy. Specifically, non-expert weights are stored in the device\u0026rsquo;s memory,while expert weights are kept in external storage and are fetched into memoryonly when they are activated. This design is underpinned by a crucial insightthat expert weights, though voluminous, are infrequently accessed due to sparseactivation patterns. To further mitigate the overhead associated with expertI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wisebitwidth adaptation: This method reduces the size of expert weights with anacceptable level of accuracy loss. (2) Expert management: It predicts theexperts that will be activated in advance and preloads them into thecompute-I/O pipeline, thus further optimizing the process. In empiricalevaluations conducted on well-established MoE LLMs and various edge devices,EdgeMoE demonstrates substantial memory savings and performance improvementswhen compared to competitive baseline solutions.\r2023-08-27\nProtecting Language Generation Models via Invisible Watermarking\nXuandong Zhao Yu-Xiang Wang Lei Li\nabstract\rabstract: Language generation models have been an increasingly powerful enabler formany applications. Many such models offer free or affordable API access, whichmakes them potentially vulnerable to model extraction attacks throughdistillation. To protect intellectual property (IP) and ensure fair use ofthese models, various techniques such as lexical watermarking and synonymreplacement have been proposed. However, these methods can be nullified byobvious countermeasures such as \u0026ldquo;synonym randomization\u0026rdquo;. To address this issue,we propose GINSEW, a novel method to protect text generation models from beingstolen through distillation. The key idea of our method is to inject secretsignals into the probability vector of the decoding steps for each targettoken. We can then detect the secret message by probing a suspect model to tellif it is distilled from the protected one. Experimental results show thatGINSEW can effectively identify instances of IP infringement with minimalimpact on the generation quality of protected APIs. Our method demonstrates anabsolute improvement of 19 to 29 points on mean average precision (mAP) indetecting suspects compared to previous methods against watermark removalattacks.\r2023-08-23\nHow to Protect Copyright Data in Optimization of Large Language Models?\nTimothy Chu Zhao Song Chiwun Yang\nabstract\rabstract: Large language models (LLMs) and generative AI have played a transformativerole in computer research and applications. Controversy has arisen as towhether these models output copyrighted data, which can occur if the data themodels are trained on is copyrighted. LLMs are built on the transformer neuralnetwork architecture, which in turn relies on a mathematical computation calledAttention that uses the softmax function. In this paper, we show that large language model training and optimizationcan be seen as a softmax regression problem. We then establish a method ofefficiently performing softmax regression, in a way that prevents theregression function from generating copyright data. This establishes atheoretical method of training large language models in a way that avoidsgenerating copyright data.\r2023-08-22\nTowards an On-device Agent for Text Rewriting\nYun Zhu Yinxiao Liu Felix Stahlberg Shankar Kumar Yu-hui Chen Liangchen Luo Lei Shu Renjie Liu Jindong Chen Lei Meng\nabstract\rabstract: Large Language Models (LLMs) have demonstrated impressive capabilities fortext rewriting. Nonetheless, the large sizes of these models make themimpractical for on-device inference, which would otherwise allow for enhancedprivacy and economical inference. Creating a smaller yet potent language modelfor text rewriting presents a formidable challenge because it requiresbalancing the need for a small size with the need to retain the emergentcapabilities of the LLM, that requires costly data collection. To address theabove challenge, we introduce a new instruction tuning approach for building amobile-centric text rewriting model. Our strategies enable the generation ofhigh quality training data without any human labeling. In addition, we proposea heuristic reinforcement learning framework which substantially enhancesperformance without requiring preference data. To further bridge theperformance gap with the larger server-side model, we propose an effectiveapproach that combines the mobile rewrite agent with the server model using acascade. To tailor the text rewriting tasks to mobile scenarios, we introduceMessageRewriteEval, a benchmark that focuses on text rewriting for messagesthrough natural language instructions. Through empirical experiments, wedemonstrate that our on-device model surpasses the current state-of-the-artLLMs in text rewriting while maintaining a significantly reduced model size.Notably, we show that our proposed cascading approach improves modelperformance.\r2023-08-21\nFedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning\nHaokun Chen Yao Zhang Denis Krompass Jindong Gu Volker Tresp\nabstract\rabstract: Recently, foundation models have exhibited remarkable advancements inmulti-modal learning. These models, equipped with millions (or billions) ofparameters, typically require a substantial amount of data for finetuning.However, collecting and centralizing training data from diverse sectors becomeschallenging due to distinct privacy regulations. Federated Learning (FL)emerges as a promising solution, enabling multiple clients to collaborativelytrain neural networks without centralizing their local data. To alleviateclient computation burdens and communication overheads, previous works haveadapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only asmall fraction of the model parameters are optimized and communicated duringfederated communications. Nevertheless, most previous works have focused on asingle modality and neglected one common phenomenon, i.e., the presence of dataheterogeneity across the clients. Therefore, in this work, we propose afinetuning framework tailored to heterogeneous multi-modal FL, called FederatedDual-Aadapter Teacher (FedDAT). Specifically, our approach leverages aDual-Adapter Teacher (DAT) to address data heterogeneity by regularizing theclient local updates and applying Mutual Knowledge Distillation (MKD) for anefficient knowledge transfer. FedDAT is the first approach that enables anefficient distributed finetuning of foundation models for a variety ofheterogeneous Vision-Language tasks. To demonstrate its effectiveness, weconduct extensive experiments on four multi-modality FL benchmarks withdifferent types of data heterogeneity, where FedDAT substantially outperformsthe existing centralized PEFT methods adapted for FL.\rFederated learning for secure development of AI models for Parkinson\u0026rsquo;s disease detection using speech from different languages\nSoroosh Tayebi Arasteh Cristian David Rios-Urrego Elmar Noeth Andreas Maier Seung Hee Yang Jan Rusz Juan Rafael Orozco-Arroyave\nabstract\rabstract: Parkinson\u0026rsquo;s disease (PD) is a neurological disorder impacting a person\u0026rsquo;sspeech. Among automatic PD assessment methods, deep learning models have gainedparticular interest. Recently, the community has explored cross-pathology andcross-language models which can improve diagnostic accuracy even further.However, strict patient data privacy regulations largely prevent institutionsfrom sharing patient speech data with each other. In this paper, we employfederated learning (FL) for PD detection using speech signals from 3 real-worldlanguage corpora of German, Spanish, and Czech, each from a separateinstitution. Our results indicate that the FL model outperforms all the localmodels in terms of diagnostic accuracy, while not performing very differentlyfrom the model based on centrally combined training sets, with the advantage ofnot requiring any data sharing among collaborators. This will simplifyinter-institutional collaborations, resulting in enhancement of patientoutcomes.\r2023-08-19\nFederated Few-Shot Learning for Mobile NLP\nDongqi Cai Shangguang Wang Yaozong Wu Felix Xiaozhu Lin Mengwei Xu\nabstract\rabstract: Natural language processing (NLP) sees rich mobile applications. To supportvarious language understanding tasks, a foundation NLP model is oftenfine-tuned in a federated, privacy-preserving setting (FL). This processcurrently relies on at least hundreds of thousands of labeled training samplesfrom mobile clients; yet mobile users often lack willingness or knowledge tolabel their data. Such an inadequacy of data labels is known as a few-shotscenario; it becomes the key blocker for mobile NLP applications. For the first time, this work investigates federated NLP in the few-shotscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling andprompt learning, we first establish a training pipeline that deliverscompetitive accuracy when only 0.05% (fewer than 100) of the training data islabeled and the remaining is unlabeled. To instantiate the workflow, we furtherpresent a system FeS, addressing the high execution cost with novel designs.(1) Curriculum pacing, which injects pseudo labels to the training workflow ata rate commensurate to the learning progress; (2) Representational diversity, amechanism for selecting the most learnable data, only for which pseudo labelswill be generated; (3) Co-planning of a model\u0026rsquo;s training depth and layercapacity. Together, these designs reduce the training delay, client energy, andnetwork traffic by up to 46.0$\\times$, 41.2$\\times$ and 3000.0$\\times$,respectively. Through algorithm/system co-design, FFNLP demonstrates that FLcan apply to challenging settings where most training samples are unlabeled.\rDUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization\nXiaoyu Ye Hao Huang Jiaqi An Yongtao Wang\nabstract\rabstract: Stable Diffusion (SD) customization approaches enable users to personalize SDmodel outputs, greatly enhancing the flexibility and diversity of AI art.However, they also allow individuals to plagiarize specific styles or subjectsfrom copyrighted images, which raises significant concerns about potentialcopyright infringement. To address this issue, we propose an invisibledata-free universal adversarial watermark (DUAW), aiming to protect a myriad ofcopyrighted images from different customization approaches across variousversions of SD models. First, DUAW is designed to disrupt the variationalautoencoder during SD customization. Second, DUAW operates in a data-freecontext, where it is trained on synthetic images produced by a Large LanguageModel (LLM) and a pretrained SD model. This approach circumvents the necessityof directly handling copyrighted images, thereby preserving theirconfidentiality. Once crafted, DUAW can be imperceptibly integrated intomassive copyrighted images, serving as a protective measure by inducingsignificant distortions in the images generated by customized SD models.Experimental results demonstrate that DUAW can effectively distort the outputsof fine-tuned SD models, rendering them discernible to both human observers anda simple classifier.\r2023-08-18\nLeveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks\nAbubakar S. Ali Dimitrios Michael Manias Abdallah Shami Sami Muhaidat\nabstract\rabstract: As the dawn of sixth-generation (6G) networking approaches, it promisesunprecedented advancements in communication and automation. Among the leadinginnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming toachieve fully automated, self-optimizing networks with minimal humanintervention. Despite the advantages ZTNs offer in terms of efficiency andscalability, challenges surrounding transparency, adaptability, and human trustremain prevalent. Concurrently, the advent of Large Language Models (LLMs)presents an opportunity to elevate the ZTN framework by bridging the gapbetween automated processes and human-centric interfaces. This paper exploresthe integration of LLMs into ZTNs, highlighting their potential to enhancenetwork transparency and improve user interactions. Through a comprehensivecase study on deep reinforcement learning (DRL)-based anti-jamming technique,we demonstrate how LLMs can distill intricate network operations intointuitive, human-readable reports. Additionally, we address the technical andethical intricacies of melding LLMs with ZTNs, with an emphasis on dataprivacy, transparency, and bias reduction. Looking ahead, we identify emergingresearch avenues at the nexus of LLMs and ZTNs, advocating for sustainedinnovation and interdisciplinary synergy in the domain of automated networks.\r2023-08-17\nFashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings\nYulin Su Min Yang Minghui Qiu Jing Wang Tao Wang\nabstract\rabstract: Logo embedding plays a crucial role in various e-commerce applications byfacilitating image retrieval or recognition, such as intellectual propertyprotection and product search. However, current methods treat logo embedding asa purely visual problem, which may limit their performance in real-worldscenarios. A notable issue is that the textual knowledge embedded in logoimages has not been adequately explored. Therefore, we propose a novel approachthat leverages textual knowledge as an auxiliary to improve the robustness oflogo embedding. The emerging Multimodal Large Language Models (MLLMs) havedemonstrated remarkable capabilities in both visual and textual understandingand could become valuable visual assistants in understanding logo images.Inspired by this observation, our proposed method, FashionLOGO, aims to utilizeMLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logoembedding by prompting them to generate explicit textual knowledge throughthree types of prompts, including image OCR, brief captions, and detaileddescriptions prompts, in a zero-shot setting. We adopt a cross-attentiontransformer to enable image embedding queries to learn supplementary knowledgefrom textual embeddings automatically. To reduce computational costs, we onlyuse the image embedding model in the inference stage, similar to traditionalinference pipelines. Our extensive experiments on three real-world datasetsdemonstrate that FashionLOGO learns generalized and robust logo embeddings,achieving state-of-the-art performance in all benchmark datasets. Furthermore,we conduct comprehensive ablation studies to demonstrate the performanceimprovements resulting from the introduction of MLLMs.\rApproaches to Generative Artificial Intelligence, A Social Justice Perspective\nMyke Healy\nabstract\rabstract: In the 2023-2024 academic year, the widespread availability of generativeartificial intelligence, exemplified by ChatGPT\u0026rsquo;s 1.6 billion monthly visits,is set to impact academic integrity. With 77% of high school studentspreviously reporting engagement in dishonest behaviour, the rise of AI-drivenwriting assistance, dubbed \u0026lsquo;AI-giarism\u0026rsquo; by Chan (arXiv:2306.03358v2), will makeplagiarism more accessible and less detectable. While these concerns areurgent, they also raise broader questions about the revolutionary nature ofthis technology, including autonomy, data privacy, copyright, and equity. Thispaper aims to explore generative AI from a social justice perspective,examining the training of these models, the inherent biases, and the potentialinjustices in detecting AI-generated writing.\rDifferential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models\nPhillip Rust Anders Søgaard\nabstract\rabstract: Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingualgeneralization or compression to facilitate transfer to a large number of(potentially unseen) languages. However, these models should ideally also beprivate, linguistically fair, and transparent, by relating their predictions totraining data. Can these requirements be simultaneously satisfied? We show thatmultilingual compression and linguistic fairness are compatible withdifferential privacy, but that differential privacy is at odds with trainingdata influence sparsity, an objective for transparency. We further present aseries of experiments on two common NLP tasks and evaluate multilingualcompression and training data influence sparsity under different privacyguarantees, exploring these trade-offs in more detail. Our results suggest thatwe need to develop ways to jointly optimize for these objectives in order tofind practical trade-offs.\r2023-08-16\nFermionic fractional quantum Hall states: A modern approach to systems with bulk-edge correspondence\nYoshiki Fukusumi Bo Yang\nabstract\rabstract: In contemporary physics, especially in condensed matter physics, fermionictopological order and its protected edge modes are one of the most importantobjects. In this work, we propose a systematic construction of the cylinderpartition corresponding to the fermionic fractional quantum Hall effect (FQHE)and a general mechanism for obtaining the candidates of the protected edgemodes. In our construction, when the underlying conformal field theory has the$Z_{2}$ duality defects corresponding to the fermionic $Z_{2}$ electricparticle, we show that the FQH partition function has a fermionic T duality.This duality is analogous to (hopefully the same as) the dualities in the dualresonance models, typically known as supersymmetry, and gives a renormalizationgroup (RG) theoretic understanding of the topological phases. We also introducea modern understanding of bulk topological degeneracies and topologicalentanglement entropy. This understanding is based on the traditional tunnelproblem and the recent conjecture of correspondence between the bulkrenormalization group flow and the boundary conformal field theory. Ourformalism gives an intuitive and general understanding of the modern physics ofthe topologically ordered systems in the traditional language of RG andfermionization.\rAblating Concepts in Text-to-Image Diffusion Models\nNupur Kumari Bingliang Zhang Sheng-Yu Wang Eli Shechtman Richard Zhang Jun-Yan Zhu\nabstract\rabstract: Large-scale text-to-image diffusion models can generate high-fidelity imageswith powerful compositional ability. However, these models are typicallytrained on an enormous amount of Internet data, often containing copyrightedmaterial, licensed images, and personal photos. Furthermore, they have beenfound to replicate the style of various living artists or memorize exacttraining samples. How can we remove such copyrighted concepts or images withoutretraining the model from scratch? To achieve this goal, we propose anefficient method of ablating concepts in the pretrained model, i.e., preventingthe generation of a target concept. Our algorithm learns to match the imagedistribution for a target style, instance, or text prompt we wish to ablate tothe distribution corresponding to an anchor concept. This prevents the modelfrom generating target concepts given its text condition. Extensive experimentsshow that our method can successfully prevent the generation of the ablatedconcept while preserving closely related concepts in the model.\r2023-08-15\nRobustness Over Time: Understanding Adversarial Examples\u0026rsquo; Effectiveness on Longitudinal Versions of Large Language Models\nYugeng Liu Tianshuo Cong Zhengyu Zhao Michael Backes Yun Shen Yang Zhang\nabstract\rabstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.\r2023-08-14\nTowards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark\nShuyu Yang Yinan Zhou Yaxiong Wang Yujiao Wu Li Zhu Zhedong Zheng\nabstract\rabstract: In this paper, we introduce a large Multi-Attribute and Language Searchdataset for text-based person retrieval, called MALS, and explore thefeasibility of performing pre-training on both attribute recognition andimage-text matching tasks in one stone. In particular, MALS contains 1,510,330image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,and all images are annotated with 27 attributes. Considering the privacyconcerns and annotation costs, we leverage the off-the-shelf diffusion modelsto generate the dataset. To verify the feasibility of learning from thegenerated data, we develop a new joint Attribute Prompt Learning and TextMatching Learning (APTM) framework, considering the shared knowledge betweenattribute and text. As the name implies, APTM contains an attribute promptlearning stream and a text matching learning stream. (1) The attribute promptlearning leverages the attribute prompts for image-attribute alignment, whichenhances the text matching learning. (2) The text matching learning facilitatesthe representation learning on fine-grained details, and in turn, boosts theattribute prompt learning. Extensive experiments validate the effectiveness ofthe pre-training on MALS, achieving state-of-the-art retrieval performance viaAPTM on three challenging real-world benchmarks. In particular, APTM achieves aconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy onCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.\r2023-08-13\nFree-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks\nDavid Junhao Zhang Mutian Xu Chuhui Xue Wenqing Zhang Xiaoguang Han Song Bai Mike Zheng Shou\nabstract\rabstract: Despite the rapid advancement of unsupervised learning in visualrepresentation, it requires training on large-scale datasets that demand costlydata collection, and pose additional challenges due to concerns regarding dataprivacy. Recently, synthetic images generated by text-to-image diffusionmodels, have shown great potential for benefiting image recognition. Althoughpromising, there has been inadequate exploration dedicated to unsupervisedlearning on diffusion-generated images. To address this, we start by uncoveringthat diffusion models\u0026rsquo; cross-attention layers inherently provideannotation-free attention masks aligned with corresponding text inputs ongenerated images. We then investigate the problems of three prevalentunsupervised learning techniques ( i.e., contrastive learning, masked modeling,and vision-language pretraining) and introduce customized solutions by fullyexploiting the aforementioned free attention masks. Our approach is validatedthrough extensive experiments that show consistent improvements in baselinemodels across various downstream tasks, including image classification,detection, segmentation, and image-text retrieval. By utilizing our method, itis possible to close the performance gap between unsupervised pretraining onsynthetic data and real-world scenarios.\rEthical Aspects of ChatGPT in Software Engineering Research\nMuhammad Azeem Akbar Arif Ali Khan Peng Liang\nabstract\rabstract: ChatGPT can improve Software Engineering (SE) research practices by offeringefficient, accessible information analysis and synthesis based on naturallanguage interactions. However, ChatGPT could bring ethical challenges,encompassing plagiarism, privacy, data security, and the risk of generatingbiased or potentially detrimental data. This research aims to fill the givengap by elaborating on the key elements: motivators, demotivators, and ethicalprinciples of using ChatGPT in SE research. To achieve this objective, weconducted a literature survey, identified the mentioned elements, and presentedtheir relationships by developing a taxonomy. Further, the identifiedliterature-based elements (motivators, demotivators, and ethical principles)were empirically evaluated by conducting a comprehensive questionnaire-basedsurvey involving SE researchers. Additionally, we employed InterpretiveStructure Modeling (ISM) approach to analyze the relationships between theethical principles of using ChatGPT in SE research and develop a level baseddecision model. We further conducted a Cross-Impact Matrix MultiplicationApplied to Classification (MICMAC) analysis to create a cluster-based decisionmodel. These models aim to help SE researchers devise effective strategies forethically integrating ChatGPT into SE research by following the identifiedprinciples through adopting the motivators and addressing the demotivators. Thefindings of this study will establish a benchmark for incorporating ChatGPTservices in SE research with an emphasis on ethical considerations.\r2023-08-11\nEnhancing Network Management Using Code Generated by Large Language Models\nSathiya Kumaran Mani Yajie Zhou Kevin Hsieh Santiago Segarra Ranveer Chandra Srikanth Kandula\nabstract\rabstract: Analyzing network topologies and communication graphs plays a crucial role incontemporary network management. However, the absence of a cohesive approachleads to a challenging learning curve, heightened errors, and inefficiencies.In this paper, we introduce a novel approach to facilitate anatural-language-based network management experience, utilizing large languagemodels (LLMs) to generate task-specific code from natural language queries.This method tackles the challenges of explainability, scalability, and privacyby allowing network operators to inspect the generated code, eliminating theneed to share network data with LLMs, and concentrating on application-specificrequests combined with general program synthesis techniques. We design andevaluate a prototype system using benchmark applications, showcasing highaccuracy, cost-effectiveness, and the potential for further enhancements usingcomplementary program synthesis techniques.\r2023-08-10\nUnderstanding the Cryptocurrency Free Giveaway Scam Disseminated on Twitter Lists\nKai Li Darren Lee Shixuan Guan\nabstract\rabstract: This paper presents a comprehensive analysis of the cryptocurrency freegiveaway scam disseminated in a new distribution channel, Twitter lists. Tocollect and detect the scam in this channel, unlike existing scam detectionsystems that rely on manual effort, this paper develops a fully automated scamdetection system, \\textit{GiveawayScamHunter}, to continuously collect listsfrom Twitter and utilize a Nature-Language-Processing (NLP) model toautomatically detect the free giveaway scam and extract the scam cryptocurrencyaddress. By running \\textit{GiveawayScamHunter} from June 2022 to June 2023, wedetected 95,111 free giveaway scam lists on Twitter that were created bythousands of Twitter accounts. Through analyzing the list creator accounts, ourwork reveals that scammers have combined different strategies to spread thescam, including compromising popular accounts and creating spam accounts onTwitter. Our analysis result shows that 43.9% of spam accounts still remainactive as of this writing. Furthermore, we collected 327 free giveaway domainsand 121 new scam cryptocurrency addresses. By tracking the transactions of thescam cryptocurrency addresses, this work uncovers that over 365 victims havebeen attacked by the scam, resulting in an estimated financial loss of 872KUSD. Overall, this work sheds light on the tactics, scale, and impact of freegiveaway scams disseminated on Twitter lists, emphasizing the urgent need foreffective detection and prevention mechanisms to protect social media usersfrom such fraudulent activity.\r2023-08-09\nLLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following\nKaize Shi Xueyao Sun Dingxian Wang Yinlin Fu Guandong Xu Qing Li\nabstract\rabstract: E-commerce authoring involves creating attractive, abundant, and targetedpromotional content to drive product sales. The emergence of large languagemodels (LLMs) introduces an innovative paradigm, offering a unified solution toaddress various authoring tasks within this scenario. However, mainstream LLMstrained on general corpora with common sense knowledge reveal limitations infitting complex and personalized features unique to e-commerce products andcustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,raising concerns about safeguarding voluminous customer privacy data duringtransmission. This paper proposes the LLaMA-E, the unified and customizedinstruction-following language models focusing on diverse e-commerce authoringtasks. Specifically, the domain experts create the seed instruction set fromthe tasks of ads generation, query-enhanced product title rewriting, productclassification, purchase intent speculation, and general Q\u0026amp;A. These tasksenable the models to comprehensively understand precise e-commerce authoringknowledge by interleaving features covering typical service aspects ofcustomers, sellers, and platforms. The GPT-3.5 is introduced as a teachermodel, which expands the seed instructions to form a training set for theLLaMA-E models with various scales. The experimental results show that theproposed LLaMA-E models achieve state-of-the-art results in quantitative andqualitative evaluations, also exhibiting the advantage in zero-shot scenes. Tothe best of our knowledge, this study is the first to serve the LLMs tospecific e-commerce authoring scenarios.\r2023-08-08\nSILO Language Models: Isolating Legal Risk In a Nonparametric Datastore\nSewon Min Suchin Gururangan Eric Wallace Hannaneh Hajishirzi Noah A. Smith Luke Zettlemoyer\nabstract\rabstract: The legality of training language models (LMs) on copyrighted or otherwiserestricted data is under intense debate. However, as we show, model performancesignificantly degrades if trained only on low-risk text (e.g., out-of-copyrightbooks or government documents), due to its limited size and domain coverage. Wepresent SILO, a new language model that manages this risk-performance tradeoffduring inference. SILO is built by (1) training a parametric LM on Open LicenseCorpus (OLC), a new corpus we curate with 228B tokens of public domain andpermissively licensed text and (2) augmenting it with a more general and easilymodifiable nonparametric datastore (e.g., containing copyrighted books or news)that is only queried during inference. The datastore allows use of high-riskdata without training on it, supports sentence-level data attribution, andenables data producers to opt out from the model by removing content from thestore. These capabilities can foster compliance with data-use regulations suchas the fair use doctrine in the United States and the GDPR in the EuropeanUnion. Our experiments show that the parametric LM struggles on domains notcovered by OLC. However, access to the datastore greatly improves out of domainperformance, closing 90% of the performance gap with an LM trained on the Pile,a more diverse corpus with mostly high-risk text. We also analyze whichnonparametric approach works best, where the remaining errors lie, and howperformance scales with datastore size. Our results suggest that it is possibleto build high quality language models while mitigating their legal risk.\rAssistive Chatbots for healthcare: a succinct review\nBasabdatta Sen Bhattacharya Vibhav Sinai Pissurlenkar\nabstract\rabstract: Artificial Intelligence (AI) for supporting healthcare services has neverbeen more necessitated than by the recent global pandemic. Here, we review thestate-of-the-art in AI-enabled Chatbots in healthcare proposed during the last10 years (2013-2023). The focus on AI-enabled technology is because of itspotential for enhancing the quality of human-machine interaction via Chatbots,reducing dependence on human-human interaction and saving man-hours. Our reviewindicates that there are a handful of (commercial) Chatbots that are being usedfor patient support, while there are others (non-commercial) that are in theclinical trial phases. However, there is a lack of trust on this technologyregarding patient safety and data protection, as well as a lack of widerawareness on its benefits among the healthcare workers and professionals. Also,patients have expressed dissatisfaction with Natural Language Processing (NLP)skills of the Chatbots in comparison to humans. Notwithstanding the recentintroduction of ChatGPT that has raised the bar for the NLP technology, thisChatbot cannot be trusted with patient safety and medical ethics withoutthorough and rigorous checks to serve in the `narrow\u0026rsquo; domain of assistivehealthcare. Our review suggests that to enable deployment and integration ofAI-enabled Chatbots in public health services, the need of the hour is: tobuild technology that is simple and safe to use; to build confidence on thetechnology among: (a) the medical community by focussed training anddevelopment; (b) the patients and wider community through outreach.\rSimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\nYouyang Ng Daisuke Miyashita Yasuto Hoshi Yasuhiro Morioka Osamu Torii Tomoya Kodama Jun Deguchi\nabstract\rabstract: Large Language Model (LLM) based Generative AI systems have seen significantprogress in recent years. Integrating a knowledge retrieval architecture allowsfor seamless integration of private data into publicly available Generative AIsystems using pre-trained LLM without requiring additional model fine-tuning.Moreover, Retrieval-Centric Generation (RCG) approach, a promising futureresearch direction that explicitly separates roles of LLMs and retrievers incontext interpretation and knowledge memorization, potentially leads to moreefficient implementation. SimplyRetrieve is an open-source tool with the goalof providing a localized, lightweight, and user-friendly interface to thesesophisticated advancements to the machine learning community. SimplyRetrievefeatures a GUI and API based RCG platform, assisted by a Private Knowledge BaseConstructor and a Retrieval Tuning Module. By leveraging these capabilities,users can explore the potential of RCG for improving generative AI performancewhile maintaining privacy standards. The tool is available athttps://github.com/RCGAI/SimplyRetrieve with an MIT license.\rDegeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion\nZixuan Ni Longhui Wei Jiacheng Li Siliang Tang Yueting Zhuang Qi Tian\nabstract\rabstract: Owing to the unrestricted nature of the content in the training data, largetext-to-image diffusion models, such as Stable Diffusion (SD), are capable ofgenerating images with potentially copyrighted or dangerous content based oncorresponding textual concepts information. This includes specific intellectualproperty (IP), human faces, and various artistic styles. However, NegativePrompt, a widely used method for content removal, frequently fails to concealthis content due to inherent limitations in its inference logic. In this work,we propose a novel strategy named \\textbf{Degeneration-Tuning (DT)} to shieldcontents of unwanted concepts from SD weights. By utilizing Scrambled Grid toreconstruct the correlation between undesired concepts and their correspondingimage domain, we guide SD to generate meaningless content when such textualconcepts are provided as input. As this adaptation occurs at the level of themodel\u0026rsquo;s weights, the SD, after DT, can be grafted onto other conditionaldiffusion frameworks like ControlNet to shield unwanted concepts. In additionto qualitatively showcasing the effectiveness of our DT method in protectingvarious types of concepts, a quantitative comparison of the SD before and afterDT indicates that the DT method does not significantly impact the generativequality of other contents. The FID and IS scores of the model on COCO-30Kexhibit only minor changes after DT, shifting from 12.61 and 39.20 to 13.04 and38.25, respectively, which clearly outperforms the previous methods.\r2023-08-07\nFederated Representation Learning for Automatic Speech Recognition\nGuruprasad V Ramesh Gopinath Chennupati Milind Rao Anit Kumar Sahu Ariya Rastrow Jasha Droppo\nabstract\rabstract: Federated Learning (FL) is a privacy-preserving paradigm, allowing edgedevices to learn collaboratively without sharing data. Edge devices like Alexaand Siri are prospective sources of unlabeled audio data that can be tapped tolearn robust audio representations. In this work, we bring Self-supervisedLearning (SSL) and FL together to learn representations for Automatic SpeechRecognition respecting data privacy constraints. We use the speaker and chapterinformation in the unlabeled speech dataset, Libri-Light, to simulate non-IIDspeaker-siloed data distributions and pre-train an LSTM encoder with theContrastive Predictive Coding framework with FedSGD. We show that thepre-trained ASR encoder in FL performs as well as a centrally pre-trained modeland produces an improvement of 12-15% (WER) compared to no pre-training. Wefurther adapt the federated pre-trained models to a new language, French, andshow a 20% (WER) improvement over no pre-training.\rLabeling without Seeing? Blind Annotation for Privacy-Preserving Entity Resolution\nYixiang Yao Weizhao Jin Srivatsan Ravi\nabstract\rabstract: The entity resolution problem requires finding pairs across datasets thatbelong to different owners but refer to the same entity in the real world. Totrain and evaluate solutions (either rule-based or machine-learning-based) tothe entity resolution problem, generating a ground truth dataset with entitypairs or clusters is needed. However, such a data annotation process involveshumans as domain oracles to review the plaintext data for all candidate recordpairs from different parties, which inevitably infringes the privacy of dataowners, especially in privacy-sensitive cases like medical records. To the bestof our knowledge, there is no prior work on privacy-preserving ground truthdataset generation, especially in the domain of entity resolution. We propose anovel blind annotation protocol based on homomorphic encryption that allowsdomain oracles to collaboratively label ground truths without sharing data inplaintext with other parties. In addition, we design a domain-specificeasy-to-use language that hides the sophisticated underlying homomorphicencryption layer. Rigorous proof of the privacy guarantee is provided and ourempirical experiments via an annotation simulator indicate the feasibility ofour privacy-preserving protocol (f-measure on average achieves more than 90%compared with the real ground truths).\rTableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT\nLiangyu Zha Junlin Zhou Liyao Li Rui Wang Qingyi Huang Saisai Yang Jing Yuan Changbao Su Xiang Li Aofeng Su Tao Zhang Chen Zhou Kaizhe Shou Miao Wang Wufang Zhu Guoshan Lu Chao Ye Yali Ye Wentao Ye Yiming Zhang Xinglong Deng Jie Xu Haobo Wang Gang Chen Junbo Zhao\nabstract\rabstract: Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework\u0026rsquo;s adaptability tospecific use cases.\rCuing Without Sharing: A Federated Cued Speech Recognition Framework via Mutual Knowledge Distillation\nYuxuan Zhang Lei Liu Li Liu\nabstract\rabstract: Cued Speech (CS) is a visual coding tool to encode spoken languages at thephonetic level, which combines lip-reading and hand gestures to effectivelyassist communication among people with hearing impairments. The Automatic CSRecognition (ACSR) task aims to recognize CS videos into linguistic texts,which involves both lips and hands as two distinct modalities conveyingcomplementary information. However, the traditional centralized trainingapproach poses potential privacy risks due to the use of facial and gesturevideos in CS data. To address this issue, we propose a new Federated CuedSpeech Recognition (FedCSR) framework to train an ACSR model over thedecentralized CS data without sharing private information. In particular, amutual knowledge distillation method is proposed to maintain cross-modalsemantic consistency of the Non-IID CS data, which ensures learning a unifiedfeature space for both linguistic and visual information. On the server side, aglobally shared linguistic model is trained to capture the long-termdependencies in the text sentences, which is aligned with the visualinformation from the local clients via visual-to-linguistic distillation. Onthe client side, the visual model of each client is trained with its own localdata, assisted by linguistic-to-visual distillation treating the linguisticmodel as the teacher. To the best of our knowledge, this is the first approachto consider the federated ACSR task for privacy protection. Experimentalresults on the Chinese CS dataset with multiple cuers demonstrate that ourapproach outperforms both mainstream federated learning baselines and existingcentralized state-of-the-art ACSR methods, achieving 9.7% performanceimprovement for character error rate (CER) and 15.0% for word error rate (WER).\rMembership Inference Attacks against Language Models via Neighbourhood Comparison\nJustus Mattern Fatemehsadat Mireshghallah Zhijing Jin Bernhard Schölkopf Mrinmaya Sachan Taylor Berg-Kirkpatrick\nabstract\rabstract: Membership Inference attacks (MIAs) aim to predict whether a data sample waspresent in the training data of a machine learning model or not, and are widelyused for assessing the privacy risks of language models. Most existing attacksrely on the observation that models tend to assign higher probabilities totheir training samples than non-training points. However, simple thresholdingof the model score in isolation tends to lead to high false-positive rates asit does not account for the intrinsic complexity of a sample. Recent work hasdemonstrated that reference-based attacks which compare model scores to thoseobtained from a reference model trained on similar data can substantiallyimprove the performance of MIAs. However, in order to train reference models,attacks of this kind make the strong and arguably unrealistic assumption thatan adversary has access to samples closely resembling the original trainingdata. Therefore, we investigate their performance in more realistic scenariosand find that they are highly fragile in relation to the data distribution usedto train reference models. To investigate whether this fragility provides alayer of safety, we propose and evaluate neighbourhood attacks, which comparemodel scores for a given sample to scores of synthetically generated neighbourtexts and therefore eliminate the need for access to the training datadistribution. We show that, in addition to being competitive withreference-based attacks that have perfect knowledge about the training datadistribution, our attack clearly outperforms existing reference-free attacks aswell as reference-based attacks with imperfect knowledge, which demonstratesthe need for a reevaluation of the threat model of adversarial attacks.\r2023-08-06\nInvisible Image Watermarks Are Provably Removable Using Generative AI\nXuandong Zhao Kexun Zhang Zihao Su Saastha Vasan Ilya Grishchenko Christopher Kruegel Giovanni Vigna Yu-Xiang Wang Lei Li\nabstract\rabstract: Invisible watermarks safeguard images\u0026rsquo; copyright by embedding hidden messagesonly detectable by owners. They also prevent people from misusing images,especially those generated by AI models. We propose a family of regenerationattacks to remove these invisible watermarks. The proposed attack method firstadds random noise to an image to destroy the watermark and then reconstructsthe image. This approach is flexible and can be instantiated with many existingimage-denoising algorithms and pre-trained generative models such as diffusionmodels. Through formal proofs and empirical results, we show that all invisiblewatermarks are vulnerable to the proposed attack. For a particularly resilientwatermark, RivaGAN, regeneration attacks remove 93-99% of the invisiblewatermarks while the baseline attacks remove no more than 3%. However, if we donot require the watermarked image to look the same as the original one,watermarks that keep the image semantically similar can be an alternativedefense against our attack. Our finding underscores the need for a shift inresearch/industry emphasis from invisible watermarks to semantically similarones. Code is available at https://github.com/XuandongZhao/WatermarkAttacker.\r2023-08-05\nLarge Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching\nJiayi Yuan Ruixiang Tang Xiaoqian Jiang Xia Hu\nabstract\rabstract: The process of matching patients with suitable clinical trials is essentialfor advancing medical research and providing optimal care. However, currentapproaches face challenges such as data standardization, ethicalconsiderations, and a lack of interoperability between Electronic HealthRecords (EHRs) and clinical trial criteria. In this paper, we explore thepotential of large language models (LLMs) to address these challenges byleveraging their advanced natural language generation capabilities to improvecompatibility between EHRs and clinical trial descriptions. We propose aninnovative privacy-aware data augmentation approach for LLM-based patient-trialmatching (LLM-PTM), which balances the benefits of LLMs while ensuring thesecurity and confidentiality of sensitive patient data. Our experimentsdemonstrate a 7.32% average improvement in performance using the proposedLLM-PTM method, and the generalizability to new data is improved by 12.12%.Additionally, we present case studies to further illustrate the effectivenessof our approach and provide a deeper understanding of its underlyingprinciples.\r2023-08-03\nGradual Sensitivity Typing\nDamian Arquez Matías Toro Éric Tanter\nabstract\rabstract: Reasoning about the sensitivity of functions with respect to their inputs hasinteresting applications in various areas, such as differential privacy. Inorder to check and enforce sensitivity, several approaches have been developed,notably sensitivity type systems. In these systems, sensitivity can be seen asan effect in the sense of type-and-effects systems as originally proposed byGifford and Lucassen. Because type-and-effect systems can make certain usefulprogramming patterns tedious or overly conservative, there is value in bringingthe benefits of gradual typing to these disciplines in order to ease theiradoption. In this work, we motivate, formalize, and prototype gradualsensitivity typing. The language GSoul supports both the unrestricted unknownsensitivity and bounded imprecision in the form of intervals. Gradualsensitivity typing allows programmers to smoothly evolve typed programs withoutany static sensitivity information towards hardened programs with a mix ofstatic and dynamic sensitivity checking. In particular, we show that gradualsensitivity supports recursive functions for which fully static checking wouldbe overly conservative, seamlessly enabling exact runtime sensitivity checks.GSoul satisfies both the gradual guarantees and sensitivity type soundness,known as metric preservation. We establish that, in general, gradual metricpreservation is termination insensitive, and that one can achievetermination-sensitive gradual metric preservation by hardening specificationsto bounded imprecision. We implement a prototype that provides an interactivetest bed for gradual sensitivity typing. This work opens the door togradualizing other typing disciplines that rely on function sensitivity such asdifferential privacy, as well as other quantitative type-based reasoningtechniques.\rMapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity Challenges: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis\nMaya Karanouh\nabstract\rabstract: The exponential growth in user acquisition and popularity of OpenAIs ChatGPT,an artificial intelligence(AI) powered chatbot, was accompanied by widespreadmainstream media coverage. This article presents a quantitative data analysisof the early trends and sentiments revealed by conducting text mining and NLPmethods onto a corpus of 10,902 mainstream news headlines related to thesubject of ChatGPT and artificial intelligence, from the launch of ChatGPT inNovember 2022 to March 2023. The findings revealed in sentiment analysis,ChatGPT and artificial intelligence, were perceived more positively thannegatively in the mainstream media. In regards to word frequency results, oversixty-five percent of the top frequency words were focused on Big Tech issuesand actors while topics such as jobs, diversity, ethics, copyright, gender andwomen were poorly represented or completely absent and only accounted for sixpercent of the total corpus. This article is a critical analysis into the powerstructures and collusions between Big Tech and Big Media in their hegemonicexclusion of diversity and job challenges from mainstream media.\rMusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies\nKe Chen Yusong Wu Haohe Liu Marianna Nezhurina Taylor Berg-Kirkpatrick Shlomo Dubnov\nabstract\rabstract: Diffusion models have shown promising results in cross-modal generationtasks, including text-to-image and text-to-audio generation. However,generating music, as a special type of audio, presents unique challenges due tolimited availability of music data and sensitive issues related to copyrightand plagiarism. In this paper, to tackle these challenges, we first construct astate-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusionand AudioLDM architectures to the music domain. We achieve this by retrainingthe contrastive language-audio pretraining model (CLAP) and the Hifi-GANvocoder, as components of MusicLDM, on a collection of music data samples.Then, to address the limitations of training data and to avoid plagiarism, weleverage a beat tracking model and propose two different mixup strategies fordata augmentation: beat-synchronous audio mixup and beat-synchronous latentmixup, which recombine training audio directly or via a latent embeddingsspace, respectively. Such mixup strategies encourage the model to interpolatebetween musical training samples and generate new music within the convex hullof the training data, making the generated music more diverse while stillstaying faithful to the corresponding style. In addition to popular evaluationmetrics, we design several new evaluation metrics based on CLAP score todemonstrate that our proposed MusicLDM and beat-synchronous mixup strategiesimprove both the quality and novelty of generated music, as well as thecorrespondence between input text and generated music.\r2023-08-02\nA Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards\nJoshua Harrison Ehsan Toreini Maryam Mehrnezhad\nabstract\rabstract: With recent developments in deep learning, the ubiquity of micro-phones andthe rise in online services via personal devices, acoustic side channel attackspresent a greater threat to keyboards than ever. This paper presents apractical implementation of a state-of-the-art deep learning model in order toclassify laptop keystrokes, using a smartphone integrated microphone. Whentrained on keystrokes recorded by a nearby phone, the classifier achieved anaccuracy of 95%, the highest accuracy seen without the use of a language model.When trained on keystrokes recorded using the video-conferencing software Zoom,an accuracy of 93% was achieved, a new best for the medium. Our results provethe practicality of these side channel attacks via off-the-shelf equipment andalgorithms. We discuss a series of mitigation methods to protect users againstthese series of attacks.\r2023-07-31\nPerceptions of the Fourth Industrial Revolution and Artificial Intelligence Impact on Society\nDaniel Agbaji Brady Lund Nishith Reddy Mannuru\nabstract\rabstract: The Fourth Industrial Revolution, particularly Artificial Intelligence (AI),has had a profound impact on society, raising concerns about its implicationsand ethical considerations. The emergence of text generative AI tools likeChatGPT has further intensified concerns regarding ethics, security, privacy,and copyright. This study aims to examine the perceptions of individuals indifferent information flow categorizations toward AI. The results reveal keythemes in participant-supplied definitions of AI and the fourth industrialrevolution, emphasizing the replication of human intelligence, machinelearning, automation, and the integration of digital technologies. Participantsexpressed concerns about job replacement, privacy invasion, and inaccurateinformation provided by AI. However, they also recognized the benefits of AI,such as solving complex problems and increasing convenience. Views ongovernment involvement in shaping the fourth industrial revolution varied, withsome advocating for strict regulations and others favoring support anddevelopment. The anticipated changes brought by the fourth industrialrevolution include automation, potential job impacts, increased socialdisconnect, and reliance on technology. Understanding these perceptions iscrucial for effectively managing the challenges and opportunities associatedwith AI in the evolving digital landscape.\rAMOE: a Tool to Automatically Extract and Assess Organizational Evidence for Continuous Cloud Audit\nFranz Deimling Michela Fazzolari\nabstract\rabstract: The recent spread of cloud services has enabled many companies to takeadvantage of them. Nevertheless, the main concern about the adoption of cloudservices remains the lack of transparency perceived by customers regardingsecurity and privacy. To overcome this issue, Cloud Service Certifications(CSCs) have emerged as an effective solution to increase the level of trust incloud services, possibly based on continuous auditing to monitor and evaluatethe security of cloud services on an ongoing basis. Continuous auditing can beeasily implemented for technical aspects, while organizational aspects can bechallenging due to their generic nature and varying policies between serviceproviders. In this paper, we propose an approach to facilitate the automaticassessment of organizational evidence, such as that extracted from securitypolicy documents. The evidence extraction process is based on Natural LanguageProcessing (NLP) techniques, in particular on Question Answering (QA). Theimplemented prototype provides promising results on an annotated dataset, sinceit is capable to retrieve the correct answer for more than half of the testedmetrics. This prototype can be helpful for Cloud Service Providers (CSPs) toautomate the auditing of textual policy documents and to help in reducing thetime required by auditors to check policy documents.\rChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model\nHanyao Huang Ou Zheng Dongdong Wang Jiayi Yin Zijin Wang Shengxuan Ding Heng Yin Chuan Xu Renjie Yang Qian Zheng Bing Shi\nabstract\rabstract: The ChatGPT, a lite and conversational variant of Generative PretrainedTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone LargeLanguage Models (LLMs) with billions of parameters. LLMs have stirred up muchinterest among researchers and practitioners in their impressive skills innatural language processing tasks, which profoundly impact various fields. Thispaper mainly discusses the future applications of LLMs in dentistry. Weintroduce two primary LLM deployment methods in dentistry, including automateddental diagnosis and cross-modal dental diagnosis, and examine their potentialapplications. Especially, equipped with a cross-modal encoder, a single LLM canmanage multi-source data and conduct advanced natural language reasoning toperform complex clinical operations. We also present cases to demonstrate thepotential of a fully automatic Multi-Modal LLM AI system for dentistry clinicalapplication. While LLMs offer significant potential benefits, the challenges,such as data privacy, data quality, and model bias, need further study.Overall, LLMs have the potential to revolutionize dental diagnosis andtreatment, which indicates a promising avenue for clinical application andresearch in dentistry.\rFair Algorithms for Hierarchical Agglomerative Clustering\nAnshuman Chhabra Prasant Mohapatra\nabstract\rabstract: Hierarchical Agglomerative Clustering (HAC) algorithms are extensivelyutilized in modern data science, and seek to partition the dataset intoclusters while generating a hierarchical relationship between the data samples.HAC algorithms are employed in many applications, such as biology, naturallanguage processing, and recommender systems. Thus, it is imperative to ensurethat these algorithms are fair \u0026ndash; even if the dataset contains biases againstcertain protected groups, the cluster outputs generated should not discriminateagainst samples from any of these groups. However, recent work in clusteringfairness has mostly focused on center-based clustering algorithms, such ask-median and k-means clustering. In this paper, we propose fair algorithms forperforming HAC that enforce fairness constraints 1) irrespective of thedistance linkage criteria used, 2) generalize to any natural measures ofclustering fairness for HAC, 3) work for multiple protected groups, and 4) havecompetitive running times to vanilla HAC. Through extensive experiments onmultiple real-world UCI datasets, we show that our proposed algorithm findsfairer clusterings compared to vanilla HAC as well as other state-of-the-artfair clustering approaches.\r2023-07-30\nA Review of Media Copyright Management using Blockchain Technologies from the Academic and Business Perspectives\nRoberto García Ana Cediel Mercè Teixidó Rosa Gil\nabstract\rabstract: Blockchain technologies open new opportunities for media copyrightmanagement. To provide an overview of the main initiatives in this blockchainapplication area, we have first reviewed the existing academic literature. Thereview shows literature is still scarce and immature in many aspects, which ismore evident when comparing it to initiatives coming from the industry.Blockchain has been receiving significant inflows of venture capital andcrowdfunding, which have boosted its progress in many fields, including itsapplication to media management. Consequently, we have complemented the reviewwith a business perspective. Existing reports about blockchain and media havebeen studied and consolidated into four prominent use cases. Moreover, each onehas been illustrated through existing businesses already exploring them.Combining the academic and industry perspectives, we provide a more general andcomplete overview of current trends in media copyright management usingblockchain technologies.\r2023-07-28\nHolistic Survey of Privacy and Fairness in Machine Learning\nSina Shaham Arash Hajisafi Minh K Quan Dinh C Nguyen Bhaskar Krishnamachari Charith Peris Gabriel Ghinita Cyrus Shahabi Pubudu N. Pathirana\nabstract\rabstract: Privacy and fairness are two crucial pillars of responsible ArtificialIntelligence (AI) and trustworthy Machine Learning (ML). Each objective hasbeen independently studied in the literature with the aim of reducing utilityloss in achieving them. Despite the significant interest attracted from bothacademia and industry, there remains an immediate demand for more in-depthresearch to unravel how these two objectives can be simultaneously integratedinto ML models. As opposed to well-accepted trade-offs, i.e., privacy-utilityand fairness-utility, the interrelation between privacy and fairness is notwell-understood. While some works suggest a trade-off between the two objectivefunctions, there are others that demonstrate the alignment of these functionsin certain scenarios. To fill this research gap, we provide a thorough reviewof privacy and fairness in ML, including supervised, unsupervised,semi-supervised, and reinforcement learning. After examining and consolidatingthe literature on both objectives, we present a holistic survey on the impactof privacy on fairness, the impact of fairness on privacy, existingarchitectures, their interaction in application domains, and algorithms thataim to achieve both objectives while minimizing the utility sacrificed.Finally, we identify research challenges in achieving privacy and fairnessconcurrently in ML, particularly focusing on large language models.\rMean Estimation with User-level Privacy under Data Heterogeneity\nRachel Cummings Vitaly Feldman Audra McMillan Kunal Talwar\nabstract\rabstract: A key challenge in many modern data analysis tasks is that user data areheterogeneous. Different users may possess vastly different numbers of datapoints. More importantly, it cannot be assumed that all users sample from thesame underlying distribution. This is true, for example in language data, wheredifferent speech styles result in data heterogeneity. In this work we propose asimple model of heterogeneous user data that allows user data to differ in bothdistribution and quantity of data, and provide a method for estimating thepopulation-level mean while preserving user-level differential privacy. Wedemonstrate asymptotic optimality of our estimator and also prove general lowerbounds on the error achievable in the setting we introduce.\rLessons in Reproducibility: Insights from NLP Studies in Materials Science\nXiangyun Lei Edward Kim Viktoriia Baibakova Shijing Sun\nabstract\rabstract: Natural Language Processing (NLP), a cornerstone field within artificialintelligence, has been increasingly utilized in the field of materials scienceliterature. Our study conducts a reproducibility analysis of two pioneeringworks within this domain: \u0026ldquo;Machine-learned and codified synthesis parameters ofoxide materials\u0026rdquo; by Kim et al., and \u0026ldquo;Unsupervised word embeddings capturelatent knowledge from materials science literature\u0026rdquo; by Tshitoyan et al. We aimto comprehend these studies from a reproducibility perspective, acknowledgingtheir significant influence on the field of materials informatics, rather thancritiquing them. Our study indicates that both papers offered thoroughworkflows, tidy and well-documented codebases, and clear guidance for modelevaluation. This makes it easier to replicate their results successfully andpartially reproduce their findings. In doing so, they set commendable standardsfor future materials science publications to aspire to. However, our analysisalso highlights areas for improvement such as to provide access to trainingdata where copyright restrictions permit, more transparency on modelarchitecture and the training process, and specifications of softwaredependency versions. We also cross-compare the word embedding models betweenpapers, and find that some key differences in reproducibility andcross-compatibility are attributable to design choices outside the bounds ofthe models themselves. In summary, our study appreciates the benchmark set bythese seminal papers while advocating for further enhancements in researchreproducibility practices in the field of NLP for materials science. Thisbalance of understanding and continuous improvement will ultimately propel theintersecting domains of NLP and materials science literature into a future ofexciting discoveries.\rRAWIW: RAW Image Watermarking Robust to ISP Pipeline\nKang Fu Xiaohong Liu Jun Jia Zicheng Zhang Yicong Peng Jia Wang Guangtao Zhai\nabstract\rabstract: Invisible image watermarking is essential for image copyright protection.Compared to RGB images, RAW format images use a higher dynamic range to capturethe radiometric characteristics of the camera sensor, providing greaterflexibility in post-processing and retouching. Similar to the master recordingin the music industry, RAW images are considered the original format fordistribution and image production, thus requiring copyright protection.Existing watermarking methods typically target RGB images, leaving a gap forRAW images. To address this issue, we propose the first deep learning-based RAWImage Watermarking (RAWIW) framework for copyright protection. Unlike RGB imagewatermarking, our method achieves cross-domain copyright protection. Wedirectly embed copyright information into RAW images, which can be laterextracted from the corresponding RGB images generated by differentpost-processing methods. To achieve end-to-end training of the framework, weintegrate a neural network that simulates the ISP pipeline to handle theRAW-to-RGB conversion process. To further validate the generalization of ourframework to traditional ISP pipelines and its robustness to transmissiondistortion, we adopt a distortion network. This network simulates various typesof noises introduced during the traditional ISP pipeline and transmission.Furthermore, we employ a three-stage training strategy to strike a balancebetween robustness and concealment of watermarking. Our extensive experimentsdemonstrate that RAWIW successfully achieves cross-domain copyright protectionfor RAW images while maintaining their visual quality and robustness to ISPpipeline distortions.\r2023-07-27\nTowards Regulated Deep Learning\nAndrés García-Camino\nabstract\rabstract: Regulation of Multi-Agent Systems (MAS) and Declarative ElectronicInstitutions (DEIs) was a multidisciplinary research topic of the past decadeinvolving (Physical and Software) Agents and Law since the beginning, butrecently evolved towards News-claimed Robot Lawyer since 2016. One of thesefirst proposals of restricting the behaviour of Software Agents was ElectronicInstitutions.However, with the recent reformulation of Artificial NeuralNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legalissues regarding the use of DL has raised concerns in the ArtificialIntelligence (AI) Community. Now that the Regulation of MAS is almost correctlyaddressed, we propose the Regulation of Artificial Neural Networks asAgent-based Training of a special type of regulated Artificial Neural Networkthat we call Institutional Neural Network (INN).The main purpose of this paperis to bring attention to Artificial Teaching (AT) and to give a tentativeanswer showing a proof-of-concept implementation of Regulated Deep Learning(RDL). This paper introduces the former concept and provide $I^*$, a languagepreviously used to model declaratively and extend Electronic Institutions, as ameans to regulate the execution of Artificial Neural Networks and theirinteractions with Artificial Teachers (ATs)\r2023-07-26\nUnveiling Security, Privacy, and Ethical Concerns of ChatGPT\nXiaodong Wu Ran Duan Jianbing Ni\nabstract\rabstract: This paper delves into the realm of ChatGPT, an AI-powered chatbot thatutilizes topic modeling and reinforcement learning to generate naturalresponses. Although ChatGPT holds immense promise across various industries,such as customer service, education, mental health treatment, personalproductivity, and content creation, it is essential to address its security,privacy, and ethical implications. By exploring the upgrade path from GPT-1 toGPT-4, discussing the model\u0026rsquo;s features, limitations, and potentialapplications, this study aims to shed light on the potential risks ofintegrating ChatGPT into our daily lives. Focusing on security, privacy, andethics issues, we highlight the challenges these concerns pose for widespreadadoption. Finally, we analyze the open problems in these areas, calling forconcerted efforts to ensure the development of secure and ethically sound largelanguage models.\r2023-07-25\nNot with my name! Inferring artists\u0026rsquo; names of input strings employed by Diffusion Models\nRoberto Leotta Oliver Giudice Luca Guarnera Sebastiano Battiato\nabstract\rabstract: Diffusion Models (DM) are highly effective at generating realistic,high-quality images. However, these models lack creativity and merely composeoutputs based on their training data, guided by a textual input provided atcreation time. Is it acceptable to generate images reminiscent of an artist,employing his name as input? This imply that if the DM is able to replicate anartist\u0026rsquo;s work then it was trained on some or all of his artworks thus violatingcopyright. In this paper, a preliminary study to infer the probability of useof an artist\u0026rsquo;s name in the input string of a generated image is presented. Tothis aim we focused only on images generated by the famous DALL-E 2 andcollected images (both original and generated) of five renowned artists.Finally, a dedicated Siamese Neural Network was employed to have a first kindof probability. Experimental results demonstrate that our approach is anoptimal starting point and can be employed as a prior for predicting a completeinput string of an investigated image. Dataset and code are available at:https://github.com/ictlab-unict/not-with-my-name .\rA short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM\nCarlos Ferreira\nabstract\rabstract: Artificial Intelligence is not a new subject, and business, industry andpublic sectors have used it in different ways and contexts and consideringmultiple concerns. This work reviewed research papers published in ACM DigitalLibrary and IEEE Xplore conference proceedings in the last two years supportedby fundamental concepts of Natural Language Processing (NLP) and Text Mining(TM). The objective was to capture insights regarding data privacy, ethics,interpretability, explainability, trustworthiness, and fairness in the publicsector. The methodology has saved analysis time and could retrieve paperscontaining relevant information. The results showed that fairness was the mostfrequent concern. The least prominent topic was data privacy (although embeddedin most articles), while the most prominent was trustworthiness. Finally,gathering helpful insights about those concerns regarding A.I. applications inthe public sector was also possible.\rMultilevel Large Language Models for Everyone\nYuanhao Gong\nabstract\rabstract: Large language models have made significant progress in the past few years.However, they are either generic {\\it or} field specific, splitting thecommunity into different groups. In this paper, we unify these large languagemodels into a larger map, where the generic {\\it and} specific models arelinked together and can improve each other, based on the user personal inputand information from the internet. The idea of linking several large languagemodels together is inspired by the functionality of human brain. The specificregions on the brain cortex are specific for certain low level functionality.And these regions can jointly work together to achieve more complex high levelfunctionality. Such behavior on human brain cortex sheds the light to designthe multilevel large language models that contain global level, field level anduser level models. The user level models run on local machines to achieveefficient response and protect the user\u0026rsquo;s privacy. Such multilevel modelsreduce some redundancy and perform better than the single level models. Theproposed multilevel idea can be applied in various applications, such asnatural language processing, computer vision tasks, professional assistant,business and healthcare.\r2023-07-22\nSecurity and Privacy Issues of Federated Learning\nJahid Hasan\nabstract\rabstract: Federated Learning (FL) has emerged as a promising approach to address dataprivacy and confidentiality concerns by allowing multiple participants toconstruct a shared model without centralizing sensitive data. However, thisdecentralized paradigm introduces new security challenges, necessitating acomprehensive identification and classification of potential risks to ensureFL\u0026rsquo;s security guarantees. This paper presents a comprehensive taxonomy ofsecurity and privacy challenges in Federated Learning (FL) across variousmachine learning models, including large language models. We specificallycategorize attacks performed by the aggregator and participants, focusing onpoisoning attacks, backdoor attacks, membership inference attacks, generativeadversarial network (GAN) based attacks, and differential privacy attacks.Additionally, we propose new directions for future research, seeking innovativesolutions to fortify FL systems against emerging security risks and upholdsensitive data confidentiality in distributed learning environments.\rPractical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review\nLixiang Yan Lele Sha Linxuan Zhao Yuheng Li Roberto Martinez-Maldonado Guanliang Chen Xinyu Li Yueqiao Jin Dragan Gašević\nabstract\rabstract: Educational technology innovations leveraging large language models (LLMs)have shown the potential to automate the laborious process of generating andanalysing textual content. While various innovations have been developed toautomate a range of educational tasks (e.g., question generation, feedbackprovision, and essay grading), there are concerns regarding the practicalityand ethicality of these innovations. Such concerns may hinder future researchand the adoption of LLMs-based innovations in authentic educational contexts.To address this, we conducted a systematic scoping review of 118 peer-reviewedpapers published since 2017 to pinpoint the current state of research on usingLLMs to automate and support educational tasks. The findings revealed 53 usecases for LLMs in automating education tasks, categorised into nine maincategories: profiling/labelling, detection, grading, teaching support,prediction, knowledge representation, feedback, content generation, andrecommendation. Additionally, we also identified several practical and ethicalchallenges, including low technological readiness, lack of replicability andtransparency, and insufficient privacy and beneficence considerations. Thefindings were summarised into three recommendations for future studies,including updating existing innovations with state-of-the-art models (e.g.,GPT-3/4), embracing the initiative of open-sourcing models/systems, andadopting a human-centred approach throughout the developmental process. As theintersection of AI and education is continuously evolving, the findings of thisstudy can serve as an essential reference point for researchers, allowing themto leverage the strengths, learn from the limitations, and uncover potentialresearch opportunities enabled by ChatGPT and other generative AI models.\r2023-07-21\nBibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing\nConner Ganjavi Michael B. Eppler Asli Pekcan Brett Biedermann Andre Abreu Gary S. Collins Inderbir S. Gill Giovanni E. Cacciamani\nabstract\rabstract: We aim to determine the extent and content of guidance for authors regardingthe use of generative-AI (GAI), Generative Pretrained models (GPTs) and LargeLanguage Models (LLMs) powered tools among the top 100 academic publishers andjournals in science. The websites of these publishers and journals werescreened from between 19th and 20th May 2023. Among the largest 100 publishers,17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top25 publishers. Among the top 100 journals, 70% have provided guidance on GAI.Of those with guidance, 94.1% of publishers and 95.7% of journals prohibitedthe inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit theuse of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15(21.4%) journals indicated their guidance exclusively applies to the writingprocess. When disclosing the use of GAI, 42.8% of publishers and 44.3% ofjournals included specific disclosure criteria. There was variability inguidance of where to disclose the use of GAI, including in the methods,acknowledgments, cover letter, or a new section. There was also variability inhow to access GAI guidance and the linking of journal and publisherinstructions to authors. There is a lack of guidance by some top publishers andjournals on the use of GAI by authors. Among those publishers and journals thatprovide guidance, there is substantial heterogeneity in the allowable uses ofGAI and in how it should be disclosed, with this heterogeneity persisting amongaffiliated publishers and journals in some instances. The lack ofstandardization burdens authors and threatens to limit the effectiveness ofthese regulations. There is a need for standardized guidelines in order toprotect the integrity of scientific output as GAI continues to grow inpopularity.\rProject Florida: Federated Learning Made Easy\nDaniel Madrigal Diaz Andre Manoel Jialei Chen Nalin Singal Robert Sim\nabstract\rabstract: We present Project Florida, a system architecture and software developmentkit (SDK) enabling deployment of large-scale Federated Learning (FL) solutionsacross a heterogeneous device ecosystem. Federated learning is an approach tomachine learning based on a strong data sovereignty principle, i.e., thatprivacy and security of data is best enabled by storing it at its origin,whether on end-user devices or in segregated cloud storage silos. Federatedlearning enables model training across devices and silos while the trainingdata remains within its security boundary, by distributing a model snapshot toa client running inside the boundary, running client code to update the model,and then aggregating updated snapshots across many clients in a centralorchestrator. Deploying a FL solution requires implementation of complexprivacy and security mechanisms as well as scalable orchestrationinfrastructure. Scale and performance is a paramount concern, as the modeltraining process benefits from full participation of many client devices, whichmay have a wide variety of performance characteristics. Project Florida aims tosimplify the task of deploying cross-device FL solutions by providingcloud-hosted infrastructure and accompanying task management interfaces, aswell as a multi-platform SDK supporting most major programming languagesincluding C++, Java, and Python, enabling FL training across a wide range ofoperating system (OS) and hardware specifications. The architecture decouplesservice management from the FL workflow, enabling a cloud service provider todeliver FL-as-a-service (FLaaS) to ML engineers and application developers. Wepresent an overview of Florida, including a description of the architecture,sample code, and illustrative experiments demonstrating system capabilities.\rOn Provable Copyright Protection for Generative Models\nNikhil Vyas Sham Kakade Boaz Barak\nabstract\rabstract: There is a growing concern that learned conditional generative models mayoutput samples that are substantially similar to some copyrighted data $C$ thatwas in their training set. We give a formal definition of $\\textit{nearaccess-freeness (NAF)}$ and prove bounds on the probability that a modelsatisfying this definition outputs a sample similar to $C$, even if $C$ isincluded in its training set. Roughly speaking, a generative model $p$ is$\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of$p$ diverges by at most $k$-bits from the output of a model $q$ that$\\textit{did not access $C$ at all}$. We also give generative model learningalgorithms, which efficiently modify the original generative model learningalgorithm in a black box manner, that output generative models with strongbounds on the probability of sampling protected content. Furthermore, weprovide promising experiments for both language (transformers) and image(diffusion) generative models, showing minimal degradation in output qualitywhile ensuring strong protections against sampling protected content.\rContinual Learning for Abdominal Multi-Organ and Tumor Segmentation\nYixiao Zhang Xinyi Li Huimiao Chen Alan Yuille Yaoyao Liu Zongwei Zhou\nabstract\rabstract: The ability to dynamically extend a model to new data and classes is criticalfor multiple organ and tumor segmentation. However, due to privacy regulations,accessing previous data and annotations can be problematic in the medicaldomain. This poses a significant barrier to preserving the high segmentationaccuracy of the old classes when learning from new classes because of thecatastrophic forgetting problem. In this paper, we first empiricallydemonstrate that simply using high-quality pseudo labels can fairly mitigatethis problem in the setting of organ segmentation. Furthermore, we put forwardan innovative architecture designed specifically for continuous organ and tumorsegmentation, which incurs minimal computational overhead. Our proposed designinvolves replacing the conventional output layer with a suite of lightweight,class-specific heads, thereby offering the flexibility to accommodate newlyemerging classes. These heads enable independent predictions for newlyintroduced and previously learned classes, effectively minimizing the impact ofnew classes on old ones during the course of continual learning. We furtherpropose incorporating Contrastive Language-Image Pretraining (CLIP) embeddingsinto the organ-specific heads. These embeddings encapsulate the semanticinformation of each class, informed by extensive image-text co-training. Theproposed method is evaluated on both in-house and public abdominal CT datasetsunder organ and tumor segmentation tasks. Empirical results suggest that theproposed design improves the segmentation performance of a baseline neuralnetwork on newly-introduced and previously-learned classes along the learningtrajectory.\r2023-07-20\nExploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots\nAdvait Sarkar\nabstract\rabstract: Artificial Intelligence (AI), and in particular generative models, aretransformative tools for knowledge work. They problematise notions ofcreativity, originality, plagiarism, the attribution of credit, and copyrightownership. Critics of generative models emphasise the reliance on large amountsof training data, and view the output of these models as no more thanrandomised plagiarism, remix, or collage of the source data. On these grounds,many have argued for stronger regulations on the deployment, use, andattribution of the output of these models. However, these issues are not new orunique to artificial intelligence. In this position paper, using examples fromliterary criticism, the history of art, and copyright law, I show howcreativity and originality resist definition as a notatable orinformation-theoretic property of an object, and instead can be seen as theproperty of a process, an author, or a viewer. Further alternative views holdthat all creative work is essentially reuse (mostly without attribution), orthat randomness itself can be creative. I suggest that creativity is ultimatelydefined by communities of creators and receivers, and the deemed sources ofcreativity in a workflow often depend on which parts of the workflow can beautomated. Using examples from recent studies of AI in creative knowledge work,I suggest that AI shifts knowledge work from material production to criticalintegration. This position paper aims to begin a conversation around a morenuanced approach to the problems of creativity and credit assignment forgenerative models, one which more fully recognises the importance of thecreative and curatorial voice of the users of these models and moves away fromsimpler notational or information-theoretic views.\rPatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation\nLe Xiao Xin Shan\nabstract\rabstract: Large language models(LLMS)have shown excellent text generation capabilities,capable of generating fluent human-like responses for many downstream tasks.However, applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To cope with the above challenges, this paperproposes PatternGPT, a pattern-driven text generation framework for LargeLanguage Models. Firstly, the framework utilizes the extraction capability ofLarge Language Models to generate rich and diversified structured andformalized patterns, which facilitates the introduction of external knowledgeto do the computation, and then draws on the idea of federated learning to usemultiple agents to achieve the sharing in order to obtain more diversifiedpatterns, and finally uses judgment criteria and optimization algorithm tosearch for high-quality patterns to guide the generation of models. Finally,external knowledge such as judgment criteria and optimization algorithms areused to search for high-quality patterns, and the searched patterns are used toguide model generation. This framework has the advantages of generatingdiversified patterns, protecting data privacy, combining external knowledge,and improving the quality of generation, which provides an effective method tooptimize the text generation capability of large language models, and make itbetter applied to the field of intelligent dialogue and content generation.\r2023-07-19\nWhat can we learn from Data Leakage and Unlearning for Law?\nJaydeep Borkar\nabstract\rabstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users\u0026rsquo; PII.In order to comply with privacy laws such as the \u0026ldquo;right to be forgotten\u0026rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.\r2023-07-18\nSynthetic Text Generation with Differential Privacy: A Simple and Practical Recipe\nXiang Yue Huseyin A. Inan Xuechen Li Girish Kumar Julia McAnallen Hoda Shajari Huan Sun David Levitan Robert Sim\nabstract\rabstract: Privacy concerns have attracted increasing attention in data-driven productsdue to the tendency of machine learning models to memorize sensitive trainingdata. Generating synthetic versions of such data with a formal privacyguarantee, such as differential privacy (DP), provides a promising path tomitigating these privacy concerns, but previous approaches in this directionhave typically failed to produce synthetic data of high quality. In this work,we show that a simple and practical recipe in the text domain is effective:simply fine-tuning a pretrained generative language model with DP enables themodel to generate useful synthetic text with strong privacy protection. Throughextensive empirical analyses on both benchmark and private customer data, wedemonstrate that our method produces synthetic text that is competitive interms of utility with its non-private counterpart, meanwhile providing strongprotection against potential privacy leakages.\rCloud-native RStudio on Kubernetes for Hopsworks\nGibson Chikafa Sina Sheikholeslami Salman Niazi Jim Dowling Vladimir Vlassov\nabstract\rabstract: In order to fully benefit from cloud computing, services are designedfollowing the \u0026ldquo;multi-tenant\u0026rdquo; architectural model, which is aimed at maximizingresource sharing among users. However, multi-tenancy introduces challenges ofsecurity, performance isolation, scaling, and customization. RStudio server isan open-source Integrated Development Environment (IDE) accessible over a webbrowser for the R programming language. We present the design andimplementation of a multi-user distributed system on Hopsworks, adata-intensive AI platform, following the multi-tenant model that providesRStudio as Software as a Service (SaaS). We use the most popular cloud-nativetechnologies: Docker and Kubernetes, to solve the problems of performanceisolation, security, and scaling that are present in a multi-tenantenvironment. We further enable secure data sharing in RStudio server instancesto provide data privacy and allow collaboration among RStudio users. Weintegrate our system with Apache Spark, which can scale and handle Big Dataprocessing workloads. Also, we provide a UI where users can provide customconfigurations and have full control of their own RStudio server instances. Oursystem was tested on a Google Cloud Platform cluster with four worker nodes,each with 30GB of RAM allocated to them. The tests on this cluster showed that44 RStudio servers, each with 2GB of RAM, can be run concurrently. Our systemcan scale out to potentially support hundreds of concurrently running RStudioservers by adding more resources (CPUs and RAM) to the cluster or system.\rFederated Large Language Model: A Position Paper\nChaochao Chen Xiaohua Feng Jun Zhou Jianwei Yin Xiaolin Zheng\nabstract\rabstract: Large scale language models (LLM) have received significant attention andfound diverse applications across various domains, but their developmentencounters challenges in real-world scenarios. These challenges arise due tothe scarcity of public domain data availability and the need to maintainprivacy with respect to private domain data. To address these issues, federatedlearning (FL) has emerged as a promising technology that enables collaborativetraining of shared models while preserving decentralized data. We propose theconcept of federated LLM, which comprises three key components, i.e., federatedLLM pre-training, federated LLM fine-tuning, and federated LLM promptengineering. For each component, we discuss its advantage over traditional LLMtraining methods and propose specific engineering strategies forimplementation. Furthermore, we explore the novel challenges introduced by theintegration of FL and LLM. We analyze existing solutions and identify potentialobstacles faced by these solutions within the context of federated LLM.\r2023-07-17\nFederated Learning of Gboard Language Models with Differential Privacy\nZheng Xu Yanxiang Zhang Galen Andrew Christopher A. Choquette-Choo Peter Kairouz H. Brendan McMahan Jesse Rosenstock Yuanbo Zhang\nabstract\rabstract: We train language models (LMs) with federated learning (FL) and differentialprivacy (DP) in the Google Keyboard (Gboard). We apply theDP-Follow-the-Regularized-Leader (DP-FTRL)~\\citep{kairouz21b} algorithm toachieve meaningfully formal DP guarantees without requiring uniform sampling ofclient devices. To provide favorable privacy-utility trade-offs, we introduce anew client participation criterion and discuss the implication of itsconfiguration in large scale systems. We show how quantile-based clipestimation~\\citep{andrew2019differentially} can be combined with DP-FTRL toadaptively choose the clip norm during training or reduce the hyperparametertuning in preparation for training. With the help of pretraining on publicdata, we train and deploy more than twenty Gboard LMs that achieve high utilityand $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.2, 2)$, with two modelsadditionally trained with secure aggregation~\\citep{bonawitz2017practical}. Weare happy to announce that all the next word prediction neural network LMs inGboard now have DP guarantees, and all future launches of Gboard neural networkLMs will require DP guarantees. We summarize our experience and provideconcrete suggestions on DP training for practitioners.\r2023-07-14\nPopulation Expansion for Training Language Models with Private Federated Learning\nTatsuki Koga Congzheng Song Martin Pelikan Mona Chitnis\nabstract\rabstract: Federated learning (FL) combined with differential privacy (DP) offersmachine learning (ML) training with distributed devices and with a formalprivacy guarantee. With a large population of devices, FL with DP produces aperformant model in a timely manner. However, for applications with a smallerpopulation, not only does the model utility degrade as the DP noise isinversely proportional to population, but also the training latency increasessince waiting for enough clients to become available from a smaller pool isslower. In this work, we thus propose expanding the population based on domainadaptation techniques to speed up the training and improves the final modelquality when training with small populations. We empirically demonstrate thatour techniques can improve the utility by 13% to 30% on real-world languagemodeling datasets.\r2023-07-13\nLarge Language Models for Supply Chain Optimization\nBeibin Li Konstantina Mellou Bo Zhang Jeevan Pathuri Ishai Menache\nabstract\rabstract: Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide \u0026ndash; a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft\u0026rsquo;s cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.\r2023-07-12\nDistilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events\nYu Gu Sheng Zhang Naoto Usuyama Yonas Woldesenbet Cliff Wong Praneeth Sanapathi Mu Wei Naveen Valluri Erika Strandberg Tristan Naumann Hoifung Poon\nabstract\rabstract: Large language models (LLMs), such as GPT-4, have demonstrated remarkablecapabilities across a wide range of tasks, including health applications. Inthis paper, we study how LLMs can be used to scale biomedical knowledgecuration. We find that while LLMs already possess decent competency instructuring biomedical text, by distillation into a task-specific student modelthrough self-supervised learning, substantial gains can be attained overout-of-box LLMs, with additional advantages such as cost, efficiency, andwhite-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is animportant area for improving care. On standard ADE extraction evaluation, aGPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervisedstate-of-the-art models without using any labeled data. Despite being over1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 byover 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)and ADE extraction architecture shed light on best practice for biomedicalknowledge extraction. Similar gains were attained by distillation for otherstandard biomedical knowledge extraction tasks such as gene-diseaseassociations and protected health information, further illustrating the promiseof this approach.\rTowards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models\nSanghyun Kim Seohyeon Jung Balhae Kim Moonseok Choi Jinwoo Shin Juho Lee\nabstract\rabstract: Large-scale image generation models, with impressive quality made possible bythe vast amount of data available on the Internet, raise social concerns thatthese models may generate harmful or copyrighted content. The biases andharmfulness arise throughout the entire training process and are hard tocompletely remove, which have become significant hurdles to the safe deploymentof these models. In this paper, we propose a method called SDD to preventproblematic content generation in text-to-image diffusion models. Weself-distill the diffusion model to guide the noise estimate conditioned on thetarget removal concept to match the unconditional one. Compared to the previousmethods, our method eliminates a much greater proportion of harmful contentfrom the generated images without degrading the overall image quality.Furthermore, our method allows the removal of multiple concepts at once,whereas previous works are limited to removing a single concept at a time.\rUnified Medical Image-Text-Label Contrastive Learning With Continuous Prompt\nYuhao Wang\nabstract\rabstract: Contrastive language-image Pre-training (CLIP) [13] can leverage largedatasets of unlabeled Image-Text pairs, which have demonstrated impressiveperformance in various downstream tasks. Given that annotating medical data istime-consuming and laborious, Image-Text Pre-training has promisingapplications in exploiting large-scale medical image and radiology reportdatasets. However, medical Image-Text Pre-training faces several challenges, asfollows: (1) Due to privacy concerns, the amount of available medical data isrelatively small compared to natural data, leading to weaker generalizationability of the model. (2) Medical images are highly similar with onlyfine-grained differences in subtleties, resulting in a large number offalse-negative sample pairs in comparison learning. (3) The hand-crafted Promptusually differs from the natural medical image report, Subtle changes inwording can lead to significant differences in performance. In this paper, wepropose a unified Image-Text-Label contrastive learning framework based oncontinuous prompts, with three main contributions. First, We unified the dataof images, text, and labels, which greatly expanded the training data that themodel could utilize. Second, we address the issue of data diversity and theimpact of hand-crafted prompts on model performance by introducing continuousimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training tomitigate the problem of too many false-negative samples. We demonstrate throughsufficient experiments that the Unified Medical Contrastive Learning (UMCL)framework exhibits excellent performance on several downstream tasks.\r2023-07-10\nDeductive Controller Synthesis for Probabilistic Hyperproperties\nRoman Andriushchenko Ezio Bartocci Milan Ceska Francesco Pontiggia Sarah Sallinger\nabstract\rabstract: Probabilistic hyperproperties specify quantitative relations between theprobabilities of reaching different target sets of states from differentinitial sets of states. This class of behavioral properties is suitable forcapturing important security, privacy, and system-level requirements. Wepropose a new approach to solve the controller synthesis problem for Markovdecision processes (MDPs) and probabilistic hyperproperties. Our specificationlanguage builds on top of the logic HyperPCTL and enhances it with structuralconstraints over the synthesized controllers. Our approach starts from a familyof controllers represented symbolically and defined over the same copy of anMDP. We then introduce an abstraction refinement strategy that can relatemultiple computation trees and that we employ to prune the search spacedeductively. The experimental evaluation demonstrates that the proposedapproach considerably outperforms HyperProb, a state-of-the-art SMT-based modelchecking tool for HyperPCTL. Moreover, our approach is the first one that isable to effectively combine probabilistic hyperproperties with additionalintra-controller constraints (e.g. partial observability) as well asinter-controller constraints (e.g. agreements on a common action).\rEthicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation\nZhexin Zhang Jiaxin Wen Minlie Huang\nabstract\rabstract: Large pre-trained language models achieve impressive results across manytasks. However, recent works point out that pre-trained language models maymemorize a considerable fraction of their training data, leading to the privacyrisk of information leakage. In this paper, we propose a method named Ethicistfor targeted training data extraction through loss smoothed soft prompting andcalibrated confidence estimation, investigating how to recover the suffix inthe training data when given a prefix. To elicit memorization in the attackedmodel, we tune soft prompt embeddings while keeping the model fixed. We furtherpropose a smoothing loss that smooths the loss distribution of the suffixtokens to make it easier to sample the correct suffix. In order to select themost probable suffix from a collection of sampled suffixes and estimate theprediction confidence, we propose a calibrated confidence estimation method,which normalizes the confidence of the generated suffixes with a localestimation. We show that Ethicist significantly improves the extractionperformance on a recently proposed public benchmark. We also investigateseveral factors influencing the data extraction performance, including decodingstrategy, model scale, prefix length, and suffix length. Our code is availableat https://github.com/thu-coai/Targeted-Data-Extraction.\rPrivacy-Preserving Graph Machine Learning from Data to Computation: A Survey\nDongqi Fu Wenxuan Bao Ross Maciejewski Hanghang Tong Jingrui He\nabstract\rabstract: In graph machine learning, data collection, sharing, and analysis ofteninvolve multiple parties, each of which may require varying levels of datasecurity and privacy. To this end, preserving privacy is of great importance inprotecting sensitive information. In the era of big data, the relationshipsamong data entities have become unprecedentedly complex, and more applicationsutilize advanced data structures (i.e., graphs) that can support networkstructures and relevant attribute information. To date, many graph-based AImodels have been proposed (e.g., graph neural networks) for various domaintasks, like computer vision and natural language processing. In this paper, wefocus on reviewing privacy-preserving techniques of graph machine learning. Wesystematically review related works from the data to the computational aspects.We first review methods for generating privacy-preserving graph data. Then wedescribe methods for transmitting privacy-preserved information (e.g., graphmodel parameters) to realize the optimization-based computation when datasharing among multiple parties is risky or impossible. In addition todiscussing relevant theoretical methodology and software tools, we also discusscurrent challenges and highlight several possible future research opportunitiesfor privacy-preserving graph machine learning. Finally, we envision a unifiedand comprehensive secure graph machine learning system.\r2023-07-09\nShaping the Emerging Norms of Using Large Language Models in Social Computing Research\nHong Shen Tianshi Li Toby Jia-Jun Li Joon Sung Park Diyi Yang\nabstract\rabstract: The emergence of Large Language Models (LLMs) has brought both excitement andconcerns to social computing research. On the one hand, LLMs offerunprecedented capabilities in analyzing vast amounts of textual data andgenerating human-like responses, enabling researchers to delve into complexsocial phenomena. On the other hand, concerns are emerging regarding thevalidity, privacy, and ethics of the research when LLMs are involved. This SIGaims at offering an open space for social computing researchers who areinterested in understanding the impacts of LLMs to discuss their currentpractices, perspectives, challenges when engaging with LLMs in their everydaywork and collectively shaping the emerging norms of using LLMs in socialcomputing research.\r2023-07-08\nMeasuring the Success of Diffusion Models at Imitating Human Artists\nStephen Casper Zifan Guo Shreya Mogulothu Zachary Marinov Chinmay Deshpande Rui-Jie Yew Zheng Dai Dylan Hadfield-Menell\nabstract\rabstract: Modern diffusion models have set the state-of-the-art in AI image generation.Their success is due, in part, to training on Internet-scale data which oftenincludes copyrighted work. This prompts questions about the extent to whichthese models learn from, imitate, or copy the work of human artists. This worksuggests that tying copyright liability to the capabilities of the model may beuseful given the evolving ecosystem of generative models. Specifically, much ofthe legal analysis of copyright and generative systems focuses on the use ofprotected data for training. As a result, the connections between data,training, and the system are often obscured. In our approach, we considersimple image classification techniques to measure a model\u0026rsquo;s ability to imitatespecific artists. Specifically, we use Contrastive Language-Image Pretrained(CLIP) encoders to classify images in a zero-shot fashion. Our process firstprompts a model to imitate a specific artist. Then, we test whether CLIP can beused to reclassify the artist (or the artist\u0026rsquo;s work) from the imitation. Ifthese tests match the imitation back to the original artist, this suggests themodel can imitate that artist\u0026rsquo;s expression. Our approach is simple andquantitative. Furthermore, it uses standard techniques and does not requireadditional training. We demonstrate our approach with an audit of StableDiffusion\u0026rsquo;s capacity to imitate 70 professional digital artists withcopyrighted work online. When Stable Diffusion is prompted to imitate an artistfrom this set, we find that the artist can be identified from the imitationwith an average accuracy of 81.0%. Finally, we also show that a sample of theartist\u0026rsquo;s work can be matched to these imitation images with a high degree ofstatistical reliability. Overall, these results suggest that Stable Diffusionis broadly successful at imitating individual human artists.\r2023-07-07\nThe Ethical Implications of Generative Audio Models: A Systematic Literature Review\nJulia Barnett\nabstract\rabstract: Generative audio models typically focus their applications in music andspeech generation, with recent models having human-like quality in their audiooutput. This paper conducts a systematic literature review of 884 papers in thearea of generative audio models in order to both quantify the degree to whichresearchers in the field are considering potential negative impacts andidentify the types of ethical implications researchers in this area need toconsider. Though 65% of generative audio research papers note positivepotential impacts of their work, less than 10% discuss any negative impacts.This jarringly small percentage of papers considering negative impact isparticularly worrying because the issues brought to light by the few papersdoing so are raising serious ethical implications and concerns relevant to thebroader field such as the potential for fraud, deep-fakes, and copyrightinfringement. By quantifying this lack of ethical consideration in generativeaudio research and identifying key areas of potential harm, this paper lays thegroundwork for future work in the field at a critical point in time in order toguide more conscientious research as this field progresses.\rFederated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios\nKarim Gamal Ahmed Gaber Hossam Amer\nabstract\rabstract: Federated learning is a growing field in the machine learning community dueto its decentralized and private design. Model training in federated learningis distributed over multiple clients giving access to lots of client data whilemaintaining privacy. Then, a server aggregates the training done on thesemultiple clients without access to their data, which could be emojis widelyused in any social media service and instant messaging platforms to expressusers\u0026rsquo; sentiments. This paper proposes federated learning-based multilingualemoji prediction in both clean and attack scenarios. Emoji prediction data havebeen crawled from both Twitter and SemEval emoji datasets. This data is used totrain and evaluate different transformer model sizes including a sparselyactivated transformer with either the assumption of clean data in all clientsor poisoned data via label flipping attack in some clients. Experimentalresults on these models show that federated learning in either clean orattacked scenarios performs similarly to centralized training in multilingualemoji prediction on seen and unseen languages under different data sources anddistributions. Our trained transformers perform better than other techniques onthe SemEval emoji dataset in addition to the privacy as well as distributedbenefits of federated learning.\r2023-07-06\nUndecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages\nShreyanth S\nabstract\rabstract: By combining the undecimated wavelet transform within a Word EmbeddedSemantic Marginal Autoencoder (WESMA), this research study provides a novelstrategy for improving security measures and denoising multiple languages. Theincorporation of these strategies is intended to address the issues ofrobustness, privacy, and multilingualism in data processing applications. Theundecimated wavelet transform is used as a feature extraction tool to identifyprominent language patterns and structural qualities in the input data. Theproposed system may successfully capture significant information whilepreserving the temporal and geographical links within the data by employingthis transform. This improves security measures by increasing the system\u0026rsquo;sability to detect abnormalities, discover hidden patterns, and distinguishbetween legitimate content and dangerous threats. The Word Embedded SemanticMarginal Autoencoder also functions as an intelligent framework fordimensionality and noise reduction. The autoencoder effectively learns theunderlying semantics of the data and reduces noise components by exploitingword embeddings and semantic context. As a result, data quality and accuracyare increased in following processing stages. The suggested methodology istested using a diversified dataset that includes several languages and securityscenarios. The experimental results show that the proposed approach iseffective in attaining security enhancement and denoising capabilities acrossmultiple languages. The system is strong in dealing with linguistic variances,producing consistent outcomes regardless of the language used. Furthermore,incorporating the undecimated wavelet transform considerably improves thesystem\u0026rsquo;s ability to efficiently address complex security concerns\r2023-07-05\nOpen-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks\nMeysam Alizadeh Maël Kubli Zeynab Samei Shirin Dehghani Juan Diego Bermeo Maria Korobeynikova Fabrizio Gilardi\nabstract\rabstract: This study examines the performance of open-source Large Language Models(LLMs) in text annotation tasks and compares it with proprietary models likeChatGPT and human-based services such as MTurk. While prior researchdemonstrated the high performance of ChatGPT across numerous NLP tasks,open-source LLMs like HugginChat and FLAN are gaining attention for theircost-effectiveness, transparency, reproducibility, and superior dataprotection. We assess these models using both zero-shot and few-shot approachesand different temperature parameters across a range of text annotation tasks.Our findings show that while ChatGPT achieves the best performance in mosttasks, open-source LLMs not only outperform MTurk but also demonstratecompetitive potential against ChatGPT in specific tasks.\r2023-07-04\nProPILE: Probing Privacy Leakage in Large Language Models\nSiwon Kim Sangdoo Yun Hwaran Lee Martin Gubri Sungroh Yoon Seong Joon Oh\nabstract\rabstract: The rapid advancement and widespread use of large language models (LLMs) haveraised significant concerns regarding the potential leakage of personallyidentifiable information (PII). These models are often trained on vastquantities of web-collected data, which may inadvertently include sensitivepersonal data. This paper presents ProPILE, a novel probing tool designed toempower data subjects, or the owners of the PII, with awareness of potentialPII leakage in LLM-based services. ProPILE lets data subjects formulate promptsbased on their own PII to evaluate the level of privacy intrusion in LLMs. Wedemonstrate its application on the OPT-1.3B model trained on the publiclyavailable Pile dataset. We show how hypothetical data subjects may assess thelikelihood of their PII being included in the Pile dataset being revealed.ProPILE can also be leveraged by LLM service providers to effectively evaluatetheir own levels of PII leakage with more powerful prompts specifically tunedfor their in-house models. This tool represents a pioneering step towardsempowering the data subjects for their awareness and control over their owndata on the web.\rTree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust\nYuxin Wen John Kirchenbauer Jonas Geiping Tom Goldstein\nabstract\rabstract: Watermarking the outputs of generative models is a crucial technique fortracing copyright and preventing potential harm from AI-generated content. Inthis paper, we introduce a novel technique called Tree-Ring Watermarking thatrobustly fingerprints diffusion model outputs. Unlike existing methods thatperform post-hoc modifications to images after sampling, Tree-Ring Watermarkingsubtly influences the entire sampling process, resulting in a model fingerprintthat is invisible to humans. The watermark embeds a pattern into the initialnoise vector used for sampling. These patterns are structured in Fourier spaceso that they are invariant to convolutions, crops, dilations, flips, androtations. After image generation, the watermark signal is detected byinverting the diffusion process to retrieve the noise vector, which is thenchecked for the embedded signal. We demonstrate that this technique can beeasily applied to arbitrary diffusion models, including text-conditioned StableDiffusion, as a plug-in with negligible loss in FID. Our watermark issemantically hidden in the image space and is far more robust than watermarkingalternatives that are currently deployed. Code is available athttps://github.com/YuxinWenRick/tree-ring-watermark.\r2023-06-28\nViP: A Differentially Private Foundation Model for Computer Vision\nYaodong Yu Maziar Sanjabi Yi Ma Kamalika Chaudhuri Chuan Guo\nabstract\rabstract: Artificial intelligence (AI) has seen a tremendous surge in capabilitiesthanks to the use of foundation models trained on internet-scale data. On theflip side, the uncurated nature of internet-scale data also poses significantprivacy and legal risks, as they often contain personal information orcopyrighted material that should not be trained on without permission. In thiswork, we propose as a mitigation measure a recipe to train foundation visionmodels with differential privacy (DP) guarantee. We identify maskedautoencoders as a suitable learning algorithm that aligns well with DP-SGD, andtrain ViP \u0026ndash; a Vision transformer with differential Privacy \u0026ndash; under a strictprivacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate thequality of representation learned by ViP using standard downstream visiontasks; in particular, ViP achieves a (non-private) linear probing accuracy of$55.7%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trainedand evaluated on ImageNet). Our result suggests that scaling to internet-scaledata can be practical for private learning. Code is available at\\url{https://github.com/facebookresearch/ViP-MAE}.\rMulti-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare\nWon Joon Yun Samuel Kim Joongheon Kim\nabstract\rabstract: The prodigious growth of digital health data has precipitated a mountinginterest in harnessing machine learning methodologies, such as natural languageprocessing (NLP), to scrutinize medical records, clinical notes, and othertext-based health information. Although NLP techniques have exhibitedsubstantial potential in augmenting patient care and informing clinicaldecision-making, data privacy and adherence to regulations persist as criticalconcerns. Federated learning (FL) emerges as a viable solution, empoweringmultiple organizations to train machine learning models collaboratively withoutdisseminating raw data. This paper proffers a pragmatic approach to medical NLPby amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA.We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-basedmodel and Bidirectional Encoder Representations from Transformers (BERT), whichhave demonstrated exceptional performance in comprehending context andsemantics within medical data. This paper encompasses the development of anintegrated framework that addresses data privacy and regulatory compliancechallenges while maintaining elevated accuracy and performance, incorporatingBERT pretraining, and comprehensively substantiating the efficacy of theproposed approach.\r2023-06-27\nRequirements for Explainability and Acceptance of Artificial Intelligence in Collaborative Work\nSabine Theis Sophie Jentzsch Fotini Deligiannaki Charles Berro Arne Peter Raulf Carmen Bruder\nabstract\rabstract: The increasing prevalence of Artificial Intelligence (AI) in safety-criticalcontexts such as air-traffic control leads to systems that are practical andefficient, and to some extent explainable to humans to be trusted and accepted.The present structured literature analysis examines n = 236 articles on therequirements for the explainability and acceptance of AI. Results include acomprehensive review of n = 48 articles on information people need to perceivean AI as explainable, the information needed to accept an AI, andrepresentation and interaction methods promoting trust in an AI. Resultsindicate that the two main groups of users are developers who requireinformation about the internal operations of the model and end users whorequire information about AI results or behavior. Users\u0026rsquo; information needs varyin specificity, complexity, and urgency and must consider context, domainknowledge, and the user\u0026rsquo;s cognitive resources. The acceptance of AI systemsdepends on information about the system\u0026rsquo;s functions and performance, privacyand ethical considerations, as well as goal-supporting information tailored toindividual preferences and information to establish trust in the system.Information about the system\u0026rsquo;s limitations and potential failures can increaseacceptance and trust. Trusted interaction methods are human-like, includingnatural language, speech, text, and visual representations such as graphs,charts, and animations. Our results have significant implications for futurehuman-centric AI systems being developed. Thus, they are suitable as input forfurther application-specific investigations of user needs.\r2023-06-26\nChatIDS: Explainable Cybersecurity Using Generative AI\nVictor Jüttner Martin Grimmer Erik Buchmann\nabstract\rabstract: Intrusion Detection Systems (IDS) are a proven approach to secure networks.However, in a privately used network, it is difficult for users withoutcybersecurity expertise to understand IDS alerts, and to respond in time withadequate measures. This puts the security of home networks, smart homeinstallations, home-office workers, etc. at risk, even if an IDS is correctlyinstalled and configured. In this work, we propose ChatIDS, our approach toexplain IDS alerts to non-experts by using large language models. We evaluatethe feasibility of ChatIDS by using ChatGPT, and we identify open researchissues with the help of interdisciplinary experts in artificial intelligence.Our results show that ChatIDS has the potential to increase network security byproposing meaningful security measures in an intuitive language from IDSalerts. Nevertheless, some potential issues in areas such as trust, privacy,ethics, etc. need to be resolved, before ChatIDS might be put into practice.\r2023-06-24\nChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge\nYunxiang Li Zihan Li Kai Zhang Ruilong Dan Steve Jiang You Zhang\nabstract\rabstract: The primary aim of this research was to address the limitations observed inthe medical knowledge of prevalent large language models (LLMs) such asChatGPT, by creating a specialized language model with enhanced accuracy inmedical advice. We achieved this by adapting and refining the large languagemodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialoguessourced from a widely used online medical consultation platform. Theseconversations were cleaned and anonymized to respect privacy concerns. Inaddition to the model refinement, we incorporated a self-directed informationretrieval mechanism, allowing the model to access and utilize real-timeinformation from online sources like Wikipedia and data from curated offlinemedical databases. The fine-tuning of the model with real-world patient-doctorinteractions significantly improved the model\u0026rsquo;s ability to understand patientneeds and provide informed advice. By equipping the model with self-directedinformation retrieval from reliable online and offline sources, we observedsubstantial improvements in the accuracy of its responses. Our proposedChatDoctor, represents a significant advancement in medical LLMs, demonstratinga significant improvement in understanding patient inquiries and providingaccurate advice. Given the high stakes and low error tolerance in the medicalfield, such enhancements in providing accurate and reliable information are notonly beneficial but essential.\r2023-06-23\nDeconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models\nAdel Elmahdy Ahmed Salem\nabstract\rabstract: Natural language processing (NLP) models have become increasingly popular inreal-world applications, such as text classification. However, they arevulnerable to privacy attacks, including data reconstruction attacks that aimto extract the data used to train the model. Most previous studies on datareconstruction attacks have focused on LLM, while classification models wereassumed to be more secure. In this work, we propose a new targeted datareconstruction attack called the Mix And Match attack, which takes advantage ofthe fact that most classification models are based on LLM. The Mix And Matchattack uses the base model of the target model to generate candidate tokens andthen prunes them using the classification head. We extensively demonstrate theeffectiveness of the attack using both random and organic canaries. This workhighlights the importance of considering the privacy risks associated with datareconstruction attacks in classification models and offers insights intopossible leakages.\rExploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT\nRyan Lingo\nabstract\rabstract: This research delves into the construction and utilization of syntheticdatasets, specifically within the telematics sphere, leveraging OpenAI\u0026rsquo;spowerful language model, ChatGPT. Synthetic datasets present an effectivesolution to challenges pertaining to data privacy, scarcity, and control overvariables - characteristics that make them particularly valuable for researchpursuits. The utility of these datasets, however, largely depends on theirquality, measured through the lenses of diversity, relevance, and coherence. Toillustrate this data creation process, a hands-on case study is conducted,focusing on the generation of a synthetic telematics dataset. The experimentinvolved an iterative guidance of ChatGPT, progressively refining prompts andculminating in the creation of a comprehensive dataset for a hypothetical urbanplanning scenario in Columbus, Ohio. Upon generation, the synthetic dataset wassubjected to an evaluation, focusing on the previously identified qualityparameters and employing descriptive statistics and visualization techniquesfor a thorough analysis. Despite synthetic datasets not serving as perfectreplacements for actual world data, their potential in specific use-cases, whenexecuted with precision, is significant. This research underscores thepotential of AI models like ChatGPT in enhancing data availability for complexsectors like telematics, thus paving the way for a myriad of new researchopportunities.\r2023-06-22\nXACML Extension for Graphs: Flexible Authorization Policy Specification and Datastore-independent Enforcement\nAya Mohamed Dagmar Auer Daniel Hofer Josef Küng\nabstract\rabstract: The increasing use of graph-structured data for business- andprivacy-critical applications requires sophisticated, flexible and fine-grainedauthorization and access control. Currently, role-based access control issupported in graph databases, where access to objects is restricted via roles.This does not take special properties of graphs into account such as verticesand edges along the path between a given subject and resource. In previousiterations of our research, we started to design an authorization policylanguage and access control model, which considers the specification of graphpaths and enforces them in the multi-model database ArangoDB. Since thisapproach is promising to consider graph characteristics in data protection, weimprove the language in this work to provide flexible path definitions andspecifying edges as protected resources. Furthermore, we introduce a method fora datastore-independent policy enforcement. Besides discussing the latest workin our XACML4G model, which is an extension to the Extensible Access ControlMarkup Language (XACML), we demonstrate our prototypical implementation with areal case and give an outlook on performance.\rVec2Vec: A Compact Neural Network Approach for Transforming Text Embeddings with High Fidelity\nAndrew Kean Gao\nabstract\rabstract: Vector embeddings have become ubiquitous tools for many language-relatedtasks. A leading embedding model is OpenAI\u0026rsquo;s text-ada-002 which can embedapproximately 6,000 words into a 1,536-dimensional vector. While powerful,text-ada-002 is not open source and is only available via API. We trained asimple neural network to convert open-source 768-dimensional MPNet embeddingsinto text-ada-002 embeddings. We compiled a subset of 50,000 online foodreviews. We calculated MPNet and text-ada-002 embeddings for each review andtrained a simple neural network to for 75 epochs. The neural network wasdesigned to predict the corresponding text-ada-002 embedding for a given MPNETembedding. Our model achieved an average cosine similarity of 0.932 on 10,000unseen reviews in our held-out test dataset. We manually assessed the qualityof our predicted embeddings for vector search over text-ada-002-embeddedreviews. While not as good as real text-ada-002 embeddings, predictedembeddings were able to retrieve highly relevant reviews. Our final model,Vec2Vec, is lightweight (\u0026lt;80 MB) and fast. Future steps include training aneural network with a more sophisticated architecture and a larger dataset ofpaired embeddings to achieve greater performance. The ability to convertbetween and align embedding spaces may be helpful for interoperability,limiting dependence on proprietary models, protecting data privacy, reducingcosts, and offline operations.\r2023-06-21\nA VM-Agnostic and Backwards Compatible Protected Modifier for Dynamically-Typed Languages\nIona Thomas Vincent Aranega Stéphane Ducasse Guillermo Polito Pablo Tesone\nabstract\rabstract: In object-oriented languages, method visibility modifiers hold a key role inseparating internal methods from the public API. Protected visibility modifiersoffer a way to hide methods from external objects while authorizing internaluse and overriding in subclasses. While present in main statically-typedlanguages, visibility modifiers are not as common or mature indynamically-typed languages. In this article, we present ProtDyn, aself-send-based visibility model calculated at compile time fordynamically-typed languages relying on name-mangling and syntacticdifferentiation of self vs non self sends. We present #Pharo, a ProtDynimplementation of this model that is backwards compatible with existingprograms, and its port to Python. Using these implementations we study theperformance impact of ProtDyn on the method lookup, in the presence of globallookup caches and polymorphic inline caches. We show that our name mangling anddouble method registration technique has a very low impact on performance andkeeps the benefits from the global lookup cache and polymorphic inline cache.We also show that the memory overhead on a real use case is between 2% and 13%in the worst-case scenario. Protected modifier semantics enforces encapsulationsuch as private but allow developers to still extend the class in subclasses.ProtDyn offers a VM-agnostic and backwards-compatible design to introduceprotected semantics in dynamically-typed languages.\rA new color image secret sharing protocol\nJosé Ignacio Farrán David Cerezo\nabstract\rabstract: Visual cryptography aims to protect images against their possibleillegitimate use. Thus, one can cipher, hash, or add watermarks for protectingcopyright, among others. In this paper we provide a new solution to the problemof secret sharing for the case when the secret is an image. Our method combinesthe Shamir scheme for secret sharing using finite fields of characteristic 2with the CBC mode of operation of a secure symmetric cryptographic scheme likeAES, so that the security relies on that of the mentioned techniques. Theresulting shares have the same resolution as that of the original image. Theidea of the method could be generalized to other multimedia formats like audioor video, adapting the method to the corresponding encoded information.\rFederated Self-Learning with Weak Supervision for Speech Recognition\nMilind Rao Gopinath Chennupati Gautam Tiwari Anit Kumar Sahu Anirudh Raju Ariya Rastrow Jasha Droppo\nabstract\rabstract: Automatic speech recognition (ASR) models with low-footprint are increasinglybeing deployed on edge devices for conversational agents, which enhancesprivacy. We study the problem of federated continual incremental learning forrecurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancingscheme of learning on-device, without access to ground truth human transcriptsor machine transcriptions from a stronger ASR model. In particular, we studythe performance of a self-learning based scheme, with a paired teacher modelupdated through an exponential moving average of ASR models. Further, wepropose using possibly noisy weak-supervision signals such as feedback scoresand natural language understanding semantics determined from user behavioracross multiple turns in a session of interactions with the conversationalagent. These signals are leveraged in a multi-task policy-gradient trainingapproach to improve the performance of self-learning for ASR. Finally, we showhow catastrophic forgetting can be mitigated by combining on-device learningwith a memory-replay approach using selected historical datasets. Theseinnovations allow for 10% relative improvement in WER on new use cases withminimal degradation on other test sets in the absence of strong-supervisionsignals such as ground-truth transcriptions.\r2023-06-20\nFedMultimodal: A Benchmark For Multimodal Federated Learning\nTiantian Feng Digbalay Bose Tuo Zhang Rajat Hebbar Anil Ramakrishna Rahul Gupta Mi Zhang Salman Avestimehr Shrikanth Narayanan\nabstract\rabstract: Over the past few years, Federated Learning (FL) has become an emergingmachine learning technique to tackle data privacy challenges throughcollaborative training. In the Federated Learning algorithm, the clients submita locally trained model, and the server aggregates these parameters untilconvergence. Despite significant efforts that have been made to FL in fieldslike computer vision, audio, and natural language processing, the FLapplications utilizing multimodal data streams remain largely unexplored. It isknown that multimodal learning has broad real-world applications in emotionrecognition, healthcare, multimedia, and social media, while user privacypersists as a critical concern. Specifically, there are no existing FLbenchmarks targeting multimodal applications or related tasks. In order tofacilitate the research in multimodal FL, we introduce FedMultimodal, the firstFL benchmark for multimodal learning covering five representative multimodalapplications from ten commonly used datasets with a total of eight uniquemodalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-endmodeling framework ranging from data partition and feature extraction to FLbenchmark algorithms and model evaluation. Unlike existing FL benchmarks,FedMultimodal provides a standardized approach to assess the robustness of FLagainst three common data corruptions in real-life multimodal applications:missing modalities, missing labels, and erroneous labels. We hope thatFedMultimodal can accelerate numerous future research directions, includingdesigning multimodal FL algorithms toward extreme data heterogeneity,robustness multimodal FL, and efficient multimodal FL. The datasets andbenchmark results can be accessed at:https://github.com/usc-sail/fed-multimodal.\rPoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs\nHao Cui Rahmadi Trimananda Athina Markopoulou Scott Jordan\nabstract\rabstract: Privacy policies disclose how an organization collects and handles personalinformation. Recent work has made progress in leveraging natural languageprocessing (NLP) to automate privacy policy analysis and extract datacollection statements from different sentences, considered in isolation fromeach other. In this paper, we view and analyze, for the first time, the entiretext of a privacy policy in an integrated way. In terms of methodology: (1) wedefine PoliGraph, a type of knowledge graph that captures statements in aprivacy policy as relations between different parts of the text; and (2) wedevelop an NLP-based tool, PoliGraph-er, to automatically extract PoliGraphfrom the text. In addition, (3) we revisit the notion of ontologies, previouslydefined in heuristic ways, to capture subsumption relations between terms. Wemake a clear distinction between local and global ontologies to capture thecontext of individual privacy policies, application domains, and privacy laws.Using a public dataset for evaluation, we show that PoliGraph-er identifies 40%more collection statements than prior state-of-the-art, with 97% precision. Interms of applications, PoliGraph enables automated analysis of a corpus ofprivacy policies and allows us to: (1) reveal common patterns in the textsacross different privacy policies, and (2) assess the correctness of the termsas defined within a privacy policy. We also apply PoliGraph to: (3) detectcontradictions in a privacy policy, where we show false alarms by prior work,and (4) analyze the consistency of privacy policies and network traffic, wherewe identify significantly more clear disclosures than prior work.\r2023-06-19\nPath to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost\nJuexiao Zhou Xiuying Chen Xin Gao\nabstract\rabstract: Medical artificial general intelligence (AGI) is an emerging field that aimsto develop systems specifically designed for medical applications that possessthe ability to understand, learn, and apply knowledge across a wide range oftasks and domains. Large language models (LLMs) represent a significant steptowards AGI. However, training cross-domain LLMs in the medical field posessignificant challenges primarily attributed to the requirement of collectingdata from diverse domains. This task becomes particularly difficult due toprivacy restrictions and the scarcity of publicly available medical datasets.Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specificmedical LLMs with the lowest cost, and suggest a possible path to achievemedical AGI. With an increasing number of domain-specific professionalmultimodal LLMs in the medical field being developed, MedAGI is designed toautomatically select appropriate medical models by analyzing users\u0026rsquo; questionswith our novel adaptive expert selection algorithm. It offers a unifiedapproach to existing LLMs in the medical field, eliminating the need forretraining regardless of the introduction of new models. This characteristicrenders it a future-proof solution in the dynamically advancing medical domain.To showcase the resilience of MedAGI, we conducted an evaluation across threedistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysisof pathology pictures. The results demonstrated that MedAGI exhibitedremarkable versatility and scalability, delivering exceptional performanceacross diverse domains. Our code is publicly available to facilitate furtherresearch at https://github.com/JoshuaChou2018/MedAGI.\r2023-06-18\nParameter-efficient Modularised Bias Mitigation via AdapterFusion\nDeepak Kumar Oleg Lesota George Zerveas Daniel Cohen Carsten Eickhoff Markus Schedl Navid Rekabsaz\nabstract\rabstract: Large pre-trained language models contain societal biases and carry alongthese biases to downstream tasks. Current in-processing bias mitigationapproaches (like adversarial training) impose debiasing by updating a model\u0026rsquo;sparameters, effectively transferring the model to a new, irreversible debiasedstate. In this work, we propose a novel approach to develop stand-alonedebiasing functionalities separate from the model, which can be integrated intothe model on-demand, while keeping the core model untouched. Drawing from theconcept of AdapterFusion in multi-task learning, we introduce DAM (Debiasingwith Adapter Modules) - a debiasing approach to first encapsulate arbitrarybias mitigation functionalities into separate adapters, and then add them tothe model on-demand in order to deliver fairness qualities. We conduct a largeset of experiments on three classification tasks with gender, race, and age asprotected attributes. Our results show that DAM improves or maintains theeffectiveness of bias mitigation, avoids catastrophic forgetting in amulti-attribute scenario, and maintains on-par task performance, while grantingparameter-efficiency and easy switching between the original and debiasedmodels.\rNLP-based Automated Compliance Checking of Data Processing Agreements against GDPR\nOrlando Amaral Muhammad Ilyas Azeem Sallam Abualhaija Lionel C Briand\nabstract\rabstract: Processing personal data is regulated in Europe by the General DataProtection Regulation (GDPR) through data processing agreements (DPAs).Checking the compliance of DPAs contributes to the compliance verification ofsoftware systems as DPAs are an important source of requirements for softwaredevelopment involving the processing of personal data. However, manuallychecking whether a given DPA complies with GDPR is challenging as it requiressignificant time and effort for understanding and identifying DPA-relevantcompliance requirements in GDPR and then verifying these requirements in theDPA. In this paper, we propose an automated solution to check the compliance ofa given DPA against GDPR. In close interaction with legal experts, we firstbuilt two artifacts: (i) the \u0026ldquo;shall\u0026rdquo; requirements extracted from the GDPRprovisions relevant to DPA compliance and (ii) a glossary table defining thelegal concepts in the requirements. Then, we developed an automated solutionthat leverages natural language processing (NLP) technologies to check thecompliance of a given DPA against these \u0026ldquo;shall\u0026rdquo; requirements. Specifically, ourapproach automatically generates phrasal-level representations for the textualcontent of the DPA and compares it against predefined representations of the\u0026quot;shall\u0026quot; requirements. Over a dataset of 30 actual DPAs, the approach correctlyfinds 618 out of 750 genuine violations while raising 76 false violations, andfurther correctly identifies 524 satisfied requirements. The approach has thusan average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%.Compared to a baseline that relies on off-the-shelf NLP tools, our approachprovides an average accuracy gain of ~20 percentage points. The accuracy of ourapproach can be improved to ~94% with limited manual verification effort.\rShould ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era\nDong Zhang\nabstract\rabstract: With various AI tools such as ChatGPT becoming increasingly popular, we areentering a true AI era. We can foresee that exceptional AI tools will soon reapconsiderable profits. A crucial question arise: should AI tools share revenuewith their training data providers in additional to traditional stakeholdersand shareholders? The answer is Yes. Large AI tools, such as large languagemodels, always require more and better quality data to continuously improve,but current copyright laws limit their access to various types of data. Sharingrevenue between AI tools and their data providers could transform the currenthostile zero-sum game relationship between AI tools and a majority ofcopyrighted data owners into a collaborative and mutually beneficial one, whichis necessary to facilitate the development of a virtuous cycle among AI tools,their users and data providers that drives forward AI technology and builds ahealthy AI ecosystem. However, current revenue-sharing business models do notwork for AI tools in the forthcoming AI era, since the most widely used metricsfor website-based traffic and action, such as clicks, will be replaced by newmetrics such as prompts and cost per prompt for generative AI tools. Acompletely new revenue-sharing business model, which must be almost independentof AI tools and be easily explained to data providers, needs to establish aprompt-based scoring system to measure data engagement of each data provider.This paper systematically discusses how to build such a scoring system for alldata providers for AI tools based on classification and content similaritymodels, and outlines the requirements for AI tools or third parties to buildit. Sharing revenue with data providers using such a scoring system wouldencourage more data owners to participate in the revenue-sharing program. Thiswill be a utilitarian AI era where all parties benefit.\r2023-06-16\nJust One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness\nEric Zelikman Qian Huang Percy Liang Nick Haber Noah D. Goodman\nabstract\rabstract: Language model training in distributed settings is limited by thecommunication cost of gradient exchanges. In this short note, we extend recentwork from Malladi et al. (2023), using shared randomness to perform distributedfine-tuning with low bandwidth. The method is a natural decentralized extensionof memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).Each iteration, each machine seeds a Random Number Generator (RNG) to performlocal reproducible perturbations on model weights and calculate and exchangescalar projected gradients, which are then used to update each model. By usinga (machine, sample) identifier as the random seed, each model can regenerateone another\u0026rsquo;s perturbations. As machines only exchange single-byte projectedgradients, this is highly communication efficient. There are also potentialprivacy benefits, as projected gradients may be calculated on differenttraining data, and models never access the other\u0026rsquo;s data. Our approach not onlydrastically reduces communication bandwidth requirements but also accommodatesdynamic addition or removal of machines during the training process and retainsthe memory-efficient and inference-only advantages of recent work. We performproof-of-concept experiments to demonstrate the potential usefulness of thismethod, building off of rich literature on distributed optimization andmemory-efficient training.\rh2oGPT: Democratizing Large Language Models\nArno Candel Jon McKinney Philipp Singer Pascal Pfeiffer Maximilian Jeblick Prithvi Prabhu Jeff Gambera Mark Landry Shivam Bansal Ryan Chesler Chun Ming Lee Marcos V. Conde Pasha Stetsenko Olivier Grellier SriSatish Ambati\nabstract\rabstract: Applications built on top of Large Language Models (LLMs) such as GPT-4represent a revolution in AI due to their human-level capabilities in naturallanguage processing. However, they also pose many significant risks such as thepresence of biased, private, or harmful text, and the unauthorized inclusion ofcopyrighted material. We introduce h2oGPT, a suite of open-source code repositories for thecreation and use of LLMs based on Generative Pretrained Transformers (GPTs).The goal of this project is to create the world\u0026rsquo;s best truly open-sourcealternative to closed-source approaches. In collaboration with and as part ofthe incredible and unstoppable open-source community, we open-source severalfine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercialuse under fully permissive Apache 2.0 licenses. Included in our release is100% private document search using natural language. Open-source language models help boost AI development and make it moreaccessible and trustworthy. They lower entry hurdles, allowing people andgroups to tailor these models to their needs. This openness increasesinnovation, transparency, and fairness. An open-source strategy is needed toshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.\r2023-06-15\nMatching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models\nMyles Foley Ambrish Rawat Taesung Lee Yufang Hou Gabriele Picco Giulio Zizzo\nabstract\rabstract: The wide applicability and adaptability of generative large language models(LLMs) has enabled their rapid adoption. While the pre-trained models canperform many tasks, such models are often fine-tuned to improve theirperformance on various downstream applications. However, this leads to issuesover violation of model licenses, model theft, and copyright infringement.Moreover, recent advances show that generative technology is capable ofproducing harmful content which exacerbates the problems of accountabilitywithin model supply chains. Thus, we need a method to investigate how a modelwas trained or a piece of text was generated and what their pre-trained basemodel was. In this paper we take the first step to address this open problem bytracing back the origin of a given fine-tuned LLM to its correspondingpre-trained base model. We consider different knowledge levels and attributionstrategies, and find that we can correctly trace back 8 out of the 10 finetuned models with our best method.\r2023-06-14\nRadiology-GPT: A Large Language Model for Radiology\nZhengliang Liu Aoxiao Zhong Yiwei Li Longtao Yang Chao Ju Zihao Wu Chong Ma Peng Shu Cheng Chen Sekeun Kim Haixing Dai Lin Zhao Dajiang Zhu Jun Liu Wei Liu Dinggang Shen Xiang Li Quanzheng Li Tianming Liu\nabstract\rabstract: We introduce Radiology-GPT, a large language model for radiology. Using aninstruction tuning approach on an extensive dataset of radiology domainknowledge, Radiology-GPT demonstrates superior performance compared to generallanguage models such as StableLM, Dolly and LLaMA. It exhibits significantversatility in radiological diagnosis, research, and communication. This workserves as a catalyst for future developments in clinical NLP. The successfulimplementation of Radiology-GPT is indicative of the potential of localizinggenerative large language models, specifically tailored for distinctive medicalspecialties, while ensuring adherence to privacy standards such as HIPAA. Theprospect of developing individualized, large-scale language models that caterto specific needs of various hospitals presents a promising direction. Thefusion of conversational competence and domain-specific knowledge in thesemodels is set to foster future development in healthcare AI. A demo ofRadiology-GPT is available athttps://huggingface.co/spaces/allen-eric/radiology-gpt.\rProtecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization\nZhigang Kan Linbo Qiao Hao Yu Liwen Peng Yifu Gao Dongsheng Li\nabstract\rabstract: Large Language Models (LLMs) are gaining increasing attention due to theirexceptional performance across numerous tasks. As a result, the general publicutilize them as an influential tool for boosting their productivity whilenatural language processing researchers endeavor to employ them in solvingexisting or new research problems. Unfortunately, individuals can only accesssuch powerful AIs through APIs, which ultimately leads to the transmission ofraw data to the models\u0026rsquo; providers and increases the possibility of privacy dataleakage. Current privacy-preserving methods for cloud-deployed language modelsaim to protect privacy information in the pre-training dataset or during themodel training phase. However, they do not meet the specific challengespresented by the remote access approach of new large-scale language models. This paper introduces a novel task, \u0026ldquo;User Privacy Protection for DialogueModels,\u0026rdquo; which aims to safeguard sensitive user information from any possibledisclosure while conversing with chatbots. We also present an evaluation schemefor this task, which covers evaluation metrics for privacy protection, dataavailability, and resistance to simulation attacks. Moreover, we propose thefirst framework for this task, namely privacy protection through textsanitization. Before sending the input to remote large models, it filters outthe sensitive information, using several rounds of text sanitization based onprivacy types that users define. Upon receiving responses from the largermodel, our framework automatically restores privacy to ensure that theconversation goes smoothly, without intervention from the privacy filter.Experiments based on real-world datasets demonstrate the efficacy of ourprivacy-preserving approach against eavesdropping from potential attackers.\r2023-06-13\nFriend or Foe Inside? Exploring In-Process Isolation to Maintain Memory Safety for Unsafe Rust\nMerve Gülmez Thomas Nyman Christoph Baumann Jan Tobias Mühlberg\nabstract\rabstract: Rust is a popular memory-safe systems programming language. In order tointeract with hardware or call into non-Rust libraries, Rust provides\\emph{unsafe} language features that shift responsibility for ensuring memorysafety to the developer. Failing to do so, may lead to memory safety violationsin unsafe code which can violate safety of the entire application. In this workwe explore in-process isolation with Memory Protection Keys as a mechanism toshield safe program sections from safety violations that may happen in unsafesections. Our approach is easy to use and comprehensive as it prevents heap andstack-based violations. We further compare process-based and in-processisolation mechanisms and the necessary requirements for data serialization,communication, and context switching. Our results show that in-processisolation can be effective and efficient, permits for a high degree ofautomation, and also enables a notion of application rewinding where the safeprogram section may detect and safely handle violations in unsafe code.\rPersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer\nXu Han Bin Guo Yoon Jung Benjamin Yao Yu Zhang Xiaohu Liu Chenlei Guo\nabstract\rabstract: Personalized dialogue agents (DAs) powered by large pre-trained languagemodels (PLMs) often rely on explicit persona descriptions to maintainpersonality consistency. However, such descriptions may not always be availableor may pose privacy concerns. To tackle this bottleneck, we introducePersonaPKT, a lightweight transfer learning approach that can buildpersona-consistent dialogue models without explicit persona descriptions. Byrepresenting each persona as a continuous vector, PersonaPKT learns implicitpersona-specific features directly from a small number of dialogue samplesproduced by the same persona, adding less than 0.1% trainable parameters foreach persona on top of the PLM backbone. Empirical results demonstrate thatPersonaPKT effectively builds personalized DAs with high storage efficiency,outperforming various baselines in terms of persona consistency whilemaintaining good response generation quality. In addition, it enhances privacyprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is aneffective solution for creating personalized DAs that respect user privacy.\rDifferentially Private One Permutation Hashing and Bin-wise Consistent Weighted Sampling\nXiaoyun Li Ping Li\nabstract\rabstract: Minwise hashing (MinHash) is a standard algorithm widely used in theindustry, for large-scale search and learning applications with the binary(0/1) Jaccard similarity. One common use of MinHash is for processing massiven-gram text representations so that practitioners do not have to materializethe original data (which would be prohibitive). Another popular use of MinHashis for building hash tables to enable sub-linear time approximate near neighbor(ANN) search. MinHash has also been used as a tool for building large-scalemachine learning systems. The standard implementation of MinHash requiresapplying $K$ random permutations. In comparison, the method of one permutationhashing (OPH), is an efficient alternative of MinHash which splits the datavectors into $K$ bins and generates hash values within each bin. OPH issubstantially more efficient and also more convenient to use. In this paper, we combine the differential privacy (DP) with OPH (as well asMinHash), to propose the DP-OPH framework with three variants: DP-OPH-fix,DP-OPH-re and DP-OPH-rand, depending on which densification strategy is adoptedto deal with empty bins in OPH. A detailed roadmap to the algorithm design ispresented along with the privacy analysis. An analytical comparison of ourproposed DP-OPH methods with the DP minwise hashing (DP-MH) is provided tojustify the advantage of DP-OPH. Experiments on similarity search confirm themerits of DP-OPH, and guide the choice of the proper variant in differentpractical scenarios. Our technique is also extended to bin-wise consistentweighted sampling (BCWS) to develop a new DP algorithm called DP-BCWS fornon-binary data. Experiments on classification tasks demonstrate that DP-BCWSis able to achieve excellent utility at around $\\epsilon = 5\\sim 10$, where$\\epsilon$ is the standard parameter in the language of $(\\epsilon,\\delta)$-DP.\r2023-06-12\n\u0026ldquo;Private Prediction Strikes Back!\u0026rsquo;\u0026rsquo; Private Kernelized Nearest Neighbors with Individual Renyi Filter\nYuqing Zhu Xuandong Zhao Chuan Guo Yu-Xiang Wang\nabstract\rabstract: Most existing approaches of differentially private (DP) machine learningfocus on private training. Despite its many advantages, private training lacksthe flexibility in adapting to incremental changes to the training dataset suchas deletion requests from exercising GDPR\u0026rsquo;s right to be forgotten. We revisit along-forgotten alternative, known as private prediction, and propose a newalgorithm named Individual Kernelized Nearest Neighbor (Ind-KNN). Ind-KNN iseasily updatable over dataset changes and it allows precise control of theR'{e}nyi DP at an individual user level \u0026ndash; a user\u0026rsquo;s privacy loss is measuredby the exact amount of her contribution to predictions; and a user is removedif her prescribed privacy budget runs out. Our results show that Ind-KNNconsistently improves the accuracy over existing private prediction methods fora wide range of $\\epsilon$ on four vision and language tasks. We alsoillustrate several cases under which Ind-KNN is preferable over privatetraining with NoisySGD.\rBackdooring Neural Code Search\nWeisong Sun Yuchen Chen Guanhong Tao Chunrong Fang Xiangyu Zhang Quanjun Zhang Bin Luo\nabstract\rabstract: Reusing off-the-shelf code snippets from online repositories is a commonpractice, which significantly enhances the productivity of software developers.To find desired code snippets, developers resort to code search engines throughnatural language queries. Neural code search models are hence behind many suchengines. These models are based on deep learning and gain substantial attentiondue to their impressive performance. However, the security aspect of thesemodels is rarely studied. Particularly, an adversary can inject a backdoor inneural code search models, which return buggy or even vulnerable code withsecurity/privacy issues. This may impact the downstream software (e.g., stocktrading systems and autonomous driving) and cause financial loss and/orlife-threatening incidents. In this paper, we demonstrate such attacks arefeasible and can be quite stealthy. By simply modifying one variable/functionname, the attacker can make buggy/vulnerable code rank in the top 11%. Ourattack BADCODE features a special trigger generation and injection procedure,making the attack more effective and stealthy. The evaluation is conducted ontwo neural code search models and the results show our attack outperformsbaselines by 60%. Our user study demonstrates that our attack is more stealthythan the baseline by two times based on the F1 score.\r2023-06-11\nDeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection\nHossein Aboutalebi Dayou Mao Carol Xu Alexander Wong\nabstract\rabstract: The tremendous recent advances in generative artificial intelligencetechniques have led to significant successes and promise in a wide range ofdifferent applications ranging from conversational agents and textual contentgeneration to voice and visual synthesis. Amid the rise in generative AI andits increasing widespread adoption, there has been significant growing concernover the use of generative AI for malicious purposes. In the realm of visualcontent synthesis using generative AI, key areas of significant concern hasbeen image forgery (e.g., generation of images containing or derived fromcopyright content), and data poisoning (i.e., generation of adversariallycontaminated images). Motivated to address these key concerns to encourageresponsible generative AI, we introduce the DeepfakeArt Challenge, alarge-scale challenge benchmark dataset designed specifically to aid in thebuilding of machine learning algorithms for generative AI art forgery and datapoisoning detection. Comprising of over 32,000 records across a variety ofgenerative forgery and data poisoning techniques, each entry consists of a pairof images that are either forgeries / adversarially contaminated or not. Eachof the generated images in the DeepfakeArt Challenge benchmark dataset has beenquality checked in a comprehensive manner. The DeepfakeArt Challenge is a corepart of GenAI4Good, a global open source initiative for accelerating machinelearning for promoting responsible creation and deployment of generative AI forgood.\r2023-06-09\nProtect Your Prompts: Protocols for IP Protection in LLM Applications\nM. A. van Wyk M. Bekker X. L. Richards K. J. Nixon\nabstract\rabstract: With the rapid adoption of AI in the form of large language models (LLMs),the potential value of carefully engineered prompts has become significant.However, to realize this potential, prompts should be tradable on an openmarket. Since prompts are, at present, generally economically non-excludable,by virtue of their nature as text, no general competitive market has yet beenestablished. This note discusses two protocols intended to provide protectionof prompts, elevating their status as intellectual property, thus confirmingthe intellectual property rights of prompt engineers, and potentiallysupporting the flourishing of an open market for LLM prompts.\rLearning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning\nRuihan Wu Xiangyu Chen Chuan Guo Kilian Q. Weinberger\nabstract\rabstract: Gradient inversion attack enables recovery of training samples from modelgradients in federated learning (FL), and constitutes a serious threat to dataprivacy. To mitigate this vulnerability, prior work proposed both principleddefenses based on differential privacy, as well as heuristic defenses based ongradient compression as countermeasures. These defenses have so far been veryeffective, in particular those based on gradient compression that allow themodel to maintain high accuracy while greatly reducing the effectiveness ofattacks. In this work, we argue that such findings underestimate the privacyrisk in FL. As a counterexample, we show that existing defenses can be brokenby a simple adaptive attack, where a model trained on auxiliary data is able toinvert gradients on both vision and language tasks.\rRobust Multi-bit Natural Language Watermarking through Invariant Features\nKiYoon Yoo Wonhyuk Ahn Jiho Jang Nojun Kwak\nabstract\rabstract: Recent years have witnessed a proliferation of valuable original naturallanguage contents found in subscription-based media outlets, web novelplatforms, and outputs of large language models. However, these contents aresusceptible to illegal piracy and potential misuse without proper securitymeasures. This calls for a secure watermarking system to guarantee copyrightprotection through leakage tracing or ownership identification. To effectivelycombat piracy and protect copyrights, a multi-bit watermarking framework shouldbe able to embed adequate bits of information and extract the watermarks in arobust manner despite possible corruption. In this work, we explore ways toadvance both payload and robustness by following a well-known proposition fromimage watermarking and identify features in natural language that are invariantto minor corruption. Through a systematic analysis of the possible sources oferrors, we further propose a corruption-resistant infill model. Our full methodimproves upon the previous work on robustness by +16.8% point on average onfour datasets, three corruption types, and two corruption ratios. Codeavailable at https://github.com/bangawayoo/nlp-watermarking.\rPrivacy Aware Question-Answering System for Online Mental Health Risk Assessment\nPrateek Chhikara Ujjwal Pasupulety John Marshall Dhiraj Chaurasia Shweta Kumari\nabstract\rabstract: Social media platforms have enabled individuals suffering from mentalillnesses to share their lived experiences and find the online supportnecessary to cope. However, many users fail to receive genuine clinicalsupport, thus exacerbating their symptoms. Screening users based on what theypost online can aid providers in administering targeted healthcare and minimizefalse positives. Pre-trained Language Models (LMs) can assess users\u0026rsquo; socialmedia data and classify them in terms of their mental health risk. We propose aQuestion-Answering (QA) approach to assess mental health risk using theUnified-QA model on two large mental health datasets. To protect user data, weextend Unified-QA by anonymizing the model training process using differentialprivacy. Our results demonstrate the effectiveness of modeling risk assessmentas a QA task, specifically for mental health use cases. Furthermore, themodel\u0026rsquo;s performance decreases by less than 1% with the inclusion ofdifferential privacy. The proposed system\u0026rsquo;s performance is indicative of apromising research direction that will lead to the development of privacy-awarediagnostic systems.\r2023-06-08\nPrivacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization\nOleksandr Yermilov Vipul Raheja Artem Chernodub\nabstract\rabstract: This work investigates the effectiveness of different pseudonymizationtechniques, ranging from rule-based substitutions to using pre-trained LargeLanguage Models (LLMs), on a variety of datasets and models used for two widelyused NLP tasks: text classification and summarization. Our work providescrucial insights into the gaps between original and anonymized data (focusingon the pseudonymization technique) and model quality and fosters futureresearch into higher-quality anonymization techniques to better balance thetrade-offs between data protection and utility preservation. We make our code,pseudonymized datasets, and downstream models publicly available\rMulti-Epoch Matrix Factorization Mechanisms for Private Machine Learning\nChristopher A. Choquette-Choo H. Brendan McMahan Keith Rush Abhradeep Thakurta\nabstract\rabstract: We introduce new differentially private (DP) mechanisms for gradient-basedmachine learning (ML) with multiple passes (epochs) over a dataset,substantially improving the achievable privacy-utility-computation tradeoffs.We formalize the problem of DP mechanisms for adaptive streams with multipleparticipations and introduce a non-trivial extension of online matrixfactorization DP mechanisms to our setting. This includes establishing thenecessary theory for sensitivity calculations and efficient computation ofoptimal matrices. For some applications like $\u0026gt;!! 10,000$ SGD steps, applyingthese optimal techniques becomes computationally expensive. We thus design anefficient Fourier-transform-based mechanism with only a minor utility loss.Extensive empirical evaluation on both example-level DP for imageclassification and user-level DP for language modeling demonstrate substantialimprovements over all previous methods, including the widely-used DP-SGD .Though our primary application is to ML, our main DP results are applicable toarbitrary linear queries and hence may have much broader applicability.\rPandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\nYidong Wang Zhuohao Yu Zhengran Zeng Linyi Yang Cunxiang Wang Hao Chen Chaoya Jiang Rui Xie Jindong Wang Xing Xie Wei Ye Shikun Zhang Yue Zhang\nabstract\rabstract: Instruction tuning large language models (LLMs) remains a challenging task,owing to the complexity of hyperparameter selection and the difficulty involvedin evaluating the tuned models. To determine the optimal hyperparameters, anautomatic, robust, and reliable evaluation benchmark is essential. However,establishing such a benchmark is not a trivial task due to the challengesassociated with evaluation accuracy and privacy protection. In response tothese challenges, we introduce a judge large language model, named PandaLM,which is trained to distinguish the superior model given several LLMs.PandaLM\u0026rsquo;s focus extends beyond just the objective correctness of responses,which is the main focus of traditional evaluation datasets. It addresses vitalsubjective factors such as relative conciseness, clarity, adherence toinstructions, comprehensiveness, and formality. To ensure the reliability ofPandaLM, we collect a diverse human-annotated test dataset, where all contextsare generated by humans and labels are aligned with human preferences. Ourresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5\u0026rsquo;s evaluationability and 88.28% of GPT-4\u0026rsquo;s in terms of F1-score on our test dataset. PandaLMenables the evaluation of LLM to be fairer but with less cost, evidenced bysignificant improvements achieved by models tuned through PandaLM compared totheir counterparts trained with default Alpaca\u0026rsquo;s hyperparameters. In addition,PandaLM does not depend on API-based evaluations, thus avoiding potential dataleakage. All resources of PandaLM are released athttps://github.com/WeOpenML/PandaLM.\rSkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model\nJuexiao Zhou Xiaonan He Liyuan Sun Jiannan Xu Xiuying Chen Yuetan Chu Longxi Zhou Xingyu Liao Bin Zhang Xin Gao\nabstract\rabstract: Skin and subcutaneous diseases rank high among the leading contributors tothe global burden of nonfatal diseases, impacting a considerable portion of thepopulation. Nonetheless, the field of dermatology diagnosis faces threesignificant hurdles. Firstly, there is a shortage of dermatologists accessibleto diagnose patients, particularly in rural regions. Secondly, accuratelyinterpreting skin disease images poses a considerable challenge. Lastly,generating patient-friendly diagnostic reports is usually a time-consuming andlabor-intensive task for dermatologists. To tackle these challenges, we presentSkinGPT-4, which is the world\u0026rsquo;s first interactive dermatology diagnostic systempowered by an advanced visual large language model. SkinGPT-4 leverages afine-tuned version of MiniGPT-4, trained on an extensive collection of skindisease images (comprising 52,929 publicly available and proprietary images)along with clinical concepts and doctors\u0026rsquo; notes. We designed a two-steptraining process to allow SkinGPT to express medical features in skin diseaseimages with natural language and make accurate diagnoses of the types of skindiseases. With SkinGPT-4, users could upload their own skin photos fordiagnosis, and the system could autonomously evaluate the images, identifiesthe characteristics and categories of the skin conditions, performs in-depthanalysis, and provides interactive treatment recommendations. Meanwhile,SkinGPT-4\u0026rsquo;s local deployment capability and commitment to user privacy alsorender it an appealing choice for patients in search of a dependable andprecise diagnosis of their skin ailments. To demonstrate the robustness ofSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, whichwere independently reviewed by certified dermatologists, and showed thatSkinGPT-4 could provide accurate diagnoses of skin diseases.\r2023-06-07\nPrivately generating tabular data using language models\nAlexandre Sablayrolles Yue Wang Brian Karrer\nabstract\rabstract: Privately generating synthetic data from a table is an important brick of aprivacy-first world. We propose and investigate a simple approach of treatingeach row in a table as a sentence and training a language model withdifferential privacy. We show this approach obtains competitive results inmodelling tabular data across multiple datasets, even at small scales thatfavor alternative methods based on marginal distributions.\r2023-06-06\nAdversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples\nChumeng Liang Xiaoyu Wu Yang Hua Jiaru Zhang Yiming Xue Tao Song Zhengui Xue Ruhui Ma Haibing Guan\nabstract\rabstract: Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise newcopyright concerns, where infringers benefit from using unauthorized paintingsto train DMs to generate novel paintings in a similar style. To address theseemerging copyright violations, in this paper, we are the first to explore andpropose to utilize adversarial examples for DMs to protect human-createdartworks. Specifically, we first build a theoretical framework to define andevaluate the adversarial examples for DMs. Then, based on this framework, wedesign a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimationof adversarial examples for DMs by optimizing upon different latent variablessampled from the reverse process of DMs. Extensive experiments show that thegenerated adversarial examples can effectively hinder DMs from extracting theirfeatures. Therefore, our method can be a powerful tool for human artists toprotect their copyright against infringers equipped with DM-based AI-for-Artapplications. The code of our method is available on GitHub:https://github.com/mist-project/mist.git.\r2023-06-05\nImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment\nEslam Mohamed Bakr Pengzhan Sun Li Erran Li Mohamed Elhoseiny\nabstract\rabstract: Most pre-trained learning systems are known to suffer from bias, whichtypically emerges from the data, the model, or both. Measuring and quantifyingbias and its sources is a challenging task and has been extensively studied inimage captioning. Despite the significant effort in this direction, we observedthat existing metrics lack consistency in the inclusion of the visual signal.In this paper, we introduce a new bias assessment metric, dubbed$ImageCaptioner^2$, for image captioning. Instead of measuring the absolutebias in the model or the data, $ImageCaptioner^2$ pay more attention to thebias introduced by the model w.r.t the data bias, termed bias amplification.Unlike the existing methods, which only evaluate the image captioningalgorithms based on the generated captions only, $ImageCaptioner^2$incorporates the image while measuring the bias. In addition, we design aformulation for measuring the bias of generated captions as prompt-based imagecaptioning instead of using language classifiers. Finally, we apply our$ImageCaptioner^2$ metric across 11 different image captioning architectures onthree different datasets, i.e., MS-COCO caption dataset, Artemis V1, andArtemis V2, and on three different protected attributes, i.e., gender, race,and emotions. Consequently, we verify the effectiveness of our$ImageCaptioner^2$ metric by proposing AnonymousBench, which is a novel humanevaluation paradigm for bias metrics. Our metric shows significant superiorityover the recent bias metric; LIC, in terms of human alignment, where thecorrelation scores are 80% and 54% for our metric and LIC, respectively. Thecode is available at https://eslambakr.github.io/imagecaptioner2.github.io/.\r2023-06-04\nAdversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy\nXiaoting Li Lingwei Chen Dinghao Wu\nabstract\rabstract: Social media has drastically reshaped the world that allows billions ofpeople to engage in such interactive environments to conveniently create andshare content with the public. Among them, text data (e.g., tweets, blogs)maintains the basic yet important social activities and generates a rich sourceof user-oriented information. While those explicit sensitive user data likecredentials has been significantly protected by all means, personal privateattribute (e.g., age, gender, location) disclosure due to inference attacks issomehow challenging to avoid, especially when powerful natural languageprocessing (NLP) techniques have been effectively deployed to automateattribute inferences from implicit text data. This puts users\u0026rsquo; attributeprivacy at risk. To address this challenge, in this paper, we leverage theinherent vulnerability of machine learning to adversarial attacks, and design anovel text-space Adversarial attack for Social Good, called Adv4SG. In otherwords, we cast the problem of protecting personal attribute privacy as anadversarial attack formulation problem over the social media text data todefend against NLP-based attribute inference attacks. More specifically, Adv4SGproceeds with a sequence of word perturbations under given constraints suchthat the probed attribute cannot be identified correctly. Different from theprior works, we advance Adv4SG by considering social media property, andintroducing cost-effective mechanisms to expedite attribute obfuscation overtext data under the black-box setting. Extensive experiments on real-worldsocial media datasets have demonstrated that our method can effectively degradethe inference accuracy with less computational cost over different attributesettings, which substantially helps mitigate the impacts of inference attacksand thus achieve high performance in user attribute privacy protection.\rModular and On-demand Bias Mitigation with Attribute-Removal Subnetworks\nLukas Hauzenberger Shahed Masoudian Deepak Kumar Markus Schedl Navid Rekabsaz\nabstract\rabstract: Societal biases are reflected in large pre-trained language models and theirfine-tuned versions on downstream tasks. Common in-processing bias mitigationapproaches, such as adversarial training and mutual information removal,introduce additional optimization criteria, and update the model to reach a newdebiased state. However, in practice, end-users and practitioners might preferto switch back to the original model, or apply debiasing only on a specificsubset of protected attributes. To enable this, we propose a novel modular biasmitigation approach, consisting of stand-alone highly sparse debiasingsubnetworks, where each debiasing module can be integrated into the core modelon-demand at inference time. Our approach draws from the concept of \\emph{diff}pruning, and proposes a novel training regime adaptable to variousrepresentation disentanglement optimizations. We conduct experiments on threeclassification tasks with gender, race, and age as protected attributes. Theresults show that our modular approach, while maintaining task performance,improves (or at least remains on-par with) the effectiveness of bias mitigationin comparison with baseline finetuning. Particularly on a two-attributedataset, our approach with separately learned debiasing subnetworks showseffective utilization of either or both the subnetworks for selective biasmitigation.\rProTeCt: Prompt Tuning for Hierarchical Consistency\nTz-Ying Wu Chih-Hui Ho Nuno Vasconcelos\nabstract\rabstract: Large visual-language models, like CLIP, learn generalized representationsand have shown promising zero-shot performance. Few-shot adaptation methods,based on prompt tuning, have also been shown to further improve performance ondownstream datasets. However, these models are not hierarchically consistent.Frequently, they infer incorrect labels at coarser taxonomic class levels, evenwhen the inference at the leaf level (original class labels) is correct. Thisis problematic, given their support for open set classification and, inparticular, open-grained classification, where practitioners define label setsat various levels of granularity. To address this problem, we propose a prompttuning technique to calibrate the hierarchical consistency of modelpredictions. A set of metrics of hierarchical consistency, the HierarchicalConsistent Accuracy (HCA) and the Mean Treecut Accuracy (MTA), are firstproposed to benchmark model performance in the open-granularity setting. Aprompt tuning technique, denoted as Prompt Tuning for Hierarchical Consistency(ProTeCt), is then proposed to calibrate classification across all possiblelabel set granularities. Results show that ProTeCt can be combined withexisting prompt tuning methods to significantly improve open-granularityclassification performance without degradation of the original classificationperformance at the leaf level.\r2023-06-02\nUnlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation\nZhengyue Zhao Jinhao Duan Xing Hu Kaidi Xu Chenan Wang Rui Zhang Zidong Du Qi Guo Yunji Chen\nabstract\rabstract: Diffusion models have demonstrated remarkable performance in image generationtasks, paving the way for powerful AIGC applications. However, thesewidely-used generative models can also raise security and privacy concerns,such as copyright infringement, and sensitive data leakage. To tackle theseissues, we propose a method, Unlearnable Diffusion Perturbation, to safeguardimages from unauthorized exploitation. Our approach involves designing analgorithm to generate sample-wise perturbation noise for each image to beprotected. This imperceptible protective noise makes the data almostunlearnable for diffusion models, i.e., diffusion models trained or fine-tunedon the protected data cannot generate high-quality and diverse images relatedto the protected training data. Theoretically, we frame this as a max-minoptimization problem and introduce EUDP, a noise scheduler-based method toenhance the effectiveness of the protective noise. We evaluate our methods onboth Denoising Diffusion Probabilistic Model and Latent Diffusion Models,demonstrating that training diffusion models on the protected data lead to asignificant reduction in the quality of the generated images. Especially, theexperimental results on Stable Diffusion demonstrate that our methodeffectively safeguards images from being used to train Diffusion Models invarious tasks, such as training specific objects and styles. This achievementholds significant importance in real-world scenarios, as it contributes to theprotection of privacy and copyright against AI-generated content.\rWhen Federated Learning Meets Pre-trained Language Models\u0026rsquo; Parameter-Efficient Tuning Methods\nZhuo Zhang Yuanhang Yang Yong Dai Lizhen Qu Zenglin Xu\nabstract\rabstract: With increasing privacy concerns on data, recent studies have madesignificant progress using federated learning (FL) on privacy-sensitive naturallanguage processing (NLP) tasks. Much literature suggests fully fine-tuningpre-trained language models (PLMs) in the FL paradigm can mitigate the dataheterogeneity problem and close the performance gap with centralized training.However, large PLMs bring the curse of prohibitive communication overhead andlocal model adaptation costs for the FL system. To this end, we introducevarious parameter-efficient tuning (PETuning) methods into federated learning.Specifically, we provide a holistic empirical study of representative PLMstuning methods in FL. The experimental results cover the analysis of dataheterogeneity levels, data scales, and different FL scenarios. Overallcommunication overhead can be significantly reduced by locally tuning andglobally aggregating lightweight model parameters while maintaining acceptableperformance in various FL settings. To facilitate the research of PETuning inFL, we also develop a federated tuning framework FedPETuning, which allowspractitioners to exploit different PETuning methods under the FL trainingparadigm conveniently. The source code is available at\\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.\rAre You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark\nWenjun Peng Jingwei Yi Fangzhao Wu Shangxi Wu Bin Zhu Lingjuan Lyu Binxing Jiao Tong Xu Guangzhong Sun Xing Xie\nabstract\rabstract: Large language models (LLMs) have demonstrated powerful capabilities in bothtext understanding and generation. Companies have begun to offer Embedding as aService (EaaS) based on these LLMs, which can benefit various natural languageprocessing (NLP) tasks for customers. However, previous studies have shown thatEaaS is vulnerable to model extraction attacks, which can cause significantlosses for the owners of LLMs, as training these models is extremely expensive.To protect the copyright of LLMs for EaaS, we propose an Embedding Watermarkmethod called EmbMarker that implants backdoors on embeddings. Our methodselects a group of moderate-frequency words from a general text corpus to forma trigger set, then selects a target embedding as the watermark, and inserts itinto the embeddings of texts containing trigger words as the backdoor. Theweight of insertion is proportional to the number of trigger words included inthe text. This allows the watermark backdoor to be effectively transferred toEaaS-stealer\u0026rsquo;s model for copyright verification while minimizing the adverseimpact on the original embeddings\u0026rsquo; utility. Our extensive experiments onvarious datasets show that our method can effectively protect the copyright ofEaaS models without compromising service quality.\r2023-06-01\nTMI! Finetuned Models Leak Private Information from their Pretraining Data\nJohn Abascal Stanley Wu Alina Oprea Jonathan Ullman\nabstract\rabstract: Transfer learning has become an increasingly popular technique in machinelearning as a way to leverage a pretrained model trained for one task to assistwith building a finetuned model for a related task. This paradigm has beenespecially popular for privacy in machine learning, where the pretrained modelis considered public, and only the data for finetuning is considered sensitive.However, there are reasons to believe that the data used for pretraining isstill sensitive, making it essential to understand how much information thefinetuned model leaks about the pretraining data. In this work we propose a newmembership-inference threat model where the adversary only has access to thefinetuned model and would like to infer the membership of the pretraining data.To realize this threat model, we implement a novel metaclassifier-based attack,TMI, that leverages the influence of memorized pretraining samples onpredictions in the downstream task. We evaluate TMI on both vision and naturallanguage tasks across multiple transfer learning settings, including finetuningwith differential privacy. Through our evaluation, we find that TMI cansuccessfully infer membership of pretraining examples using query access to thefinetuned model.\rUNGOML: Automated Classification of unsafe Usages in Go\nAnna-Katharina Wickert Clemens Damke Lars Baumgärtner Eyke Hüllermeier Mira Mezini\nabstract\rabstract: The Go programming language offers strong protection from memory corruption.As an escape hatch of these protections, it provides the unsafe package.Previous studies identified that this unsafe package is frequently used inreal-world code for several purposes, e.g., serialization or casting types. Dueto the variety of these reasons, it may be possible to refactor specific usagesto avoid potential vulnerabilities. However, the classification of unsafeusages is challenging and requires the context of the call and the program\u0026rsquo;sstructure. In this paper, we present the first automated classifier for unsafeusages in Go, UNGOML, to identify what is done with the unsafe package and whyit is used. For UNGOML, we built four custom deep learning classifiers trainedon a manually labeled data set. We represent Go code as enriched control-flowgraphs (CFGs) and solve the label prediction task with one single-vertex andthree context-aware classifiers. All three context-aware classifiers achieve atop-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore,in a set-valued conformal prediction setting, we achieve accuracies of morethan 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML canbe used to efficiently filter unsafe usages for use cases such as refactoringor a security audit. UNGOML: https://github.com/stg-tud/ungoml Artifact:https://dx.doi.org/10.6084/m9.figshare.22293052\rBag of Tricks for Training Data Extraction from Language Models\nWeichen Yu Tianyu Pang Qian Liu Chao Du Bingyi Kang Yan Huang Min Lin Shuicheng Yan\nabstract\rabstract: With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available athttps://github.com/weichen-yu/LM-Extraction.\rChatGPT as a Text Simplification Tool to Remove Bias\nCharmaine Barker Dimitar Kazakov\nabstract\rabstract: The presence of specific linguistic signals particular to a certain sub-groupof people can be picked up by language models during training. If the modelbegins to associate specific language with a distinct group, any decisions madebased upon this language would hold a strong correlation to a decision basedupon their protected characteristic, leading to possible discrimination. Weexplore a potential technique for bias mitigation in the form of simplificationof text. The driving force of this idea is that simplifying text shouldstandardise language between different sub-groups to one way of speaking whilekeeping the same meaning. The experiment shows promising results as theclassifier accuracy for predicting the sensitive attribute drops by up to 17%for the simplified data.\rChallenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond\nChuan Chen Zhenpeng Wu Yanyi Lai Wenlin Ou Tianchi Liao Zibin Zheng\nabstract\rabstract: Artificial Intelligence Generated Content (AIGC) is one of the latestachievements in AI development. The content generated by related applications,such as text, images and audio, has sparked a heated discussion. Variousderived AIGC applications are also gradually entering all walks of life,bringing unimaginable impact to people\u0026rsquo;s daily lives. However, the rapiddevelopment of such generative tools has also raised concerns about privacy andsecurity issues, and even copyright issues in AIGC. We note that advancedtechnologies such as blockchain and privacy computing can be combined with AIGCtools, but no work has yet been done to investigate their relevance andprospect in a systematic and detailed way. Therefore it is necessary toinvestigate how they can be used to protect the privacy and security of data inAIGC by fully exploring the aforementioned technologies. In this paper, wefirst systematically review the concept, classification and underlyingtechnologies of AIGC. Then, we discuss the privacy and security challengesfaced by AIGC from multiple perspectives and purposefully list thecountermeasures that currently exist. We hope our survey will help researchersand industry to build a more secure and robust AIGC system.\r2023-05-31\nDeep Regression Unlearning\nAyush K Tarun Vikram S Chundawat Murari Mandal Mohan Kankanhalli\nabstract\rabstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: https://github.com/ayu987/deep-regression-unlearning\rSynthetic Pre-Training Tasks for Neural Machine Translation\nZexue He Graeme Blackwood Rameswar Panda Julian McAuley Rogerio Feris\nabstract\rabstract: Pre-training models with large crawled corpora can lead to issues such astoxicity and bias, as well as copyright and privacy concerns. A promising wayof alleviating such concerns is to conduct pre-training with synthetic tasksand data, since no real-world information is ingested by the model. Our goal inthis paper is to understand the factors that contribute to the effectiveness ofpre-training models when using synthetic resources, particularly in the contextof neural machine translation. We propose several novel approaches topre-training translation models that involve different levels of lexical andstructural knowledge, including: 1) generating obfuscated data from a largeparallel corpus 2) concatenating phrase pairs extracted from a smallword-aligned corpus, and 3) generating synthetic parallel data without realhuman language corpora. Our experiments on multiple language pairs reveal thatpre-training benefits can be realized even with high levels of obfuscation orpurely synthetic parallel data. We hope the findings from our comprehensiveempirical analysis will shed light on understanding what matters for NMTpre-training, as well as pave the way for the development of more efficient andless toxic models.\r2023-05-30\nExamining risks of racial biases in NLP tools for child protective services\nAnjalie Field Amanda Coston Nupoor Gandhi Alexandra Chouldechova Emily Putnam-Hornstein David Steier Yulia Tsvetkov\nabstract\rabstract: Although much literature has established the presence of demographic bias innatural language processing (NLP) models, most work relies on curated biasmetrics that may not be reflective of real-world applications. At the sametime, practitioners are increasingly using algorithmic tools in high-stakessettings, with particular recent interest in NLP. In this work, we focus on onesuch setting: child protective services (CPS). CPS workers often write copiousfree-form text notes about families they are working with, and CPS agencies areactively seeking to deploy NLP models to leverage these data. Givenwell-established racial bias in this setting, we investigate possible waysdeployed NLP is liable to increase racial disparities. We specifically examineword statistics within notes and algorithmic fairness in risk prediction,coreference resolution, and named entity recognition (NER). We documentconsistent algorithmic unfairness in NER models, possible algorithmicunfairness in coreference resolution models, and little evidence of exacerbatedracial bias in risk prediction. While there is existing pronounced criticism ofrisk prediction, our results expose previously undocumented risks of racialbias in realistic information extraction systems, highlighting potentialconcerns in deploying them, even though they may appear more benign. Our workserves as a rare realistic examination of NLP algorithmic fairness in apotential deployed setting and a timely investigation of a specific riskassociated with deploying NLP in CPS settings.\rJointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning\nUmang Gupta Aram Galstyan Greg Ver Steeg\nabstract\rabstract: Efficient finetuning of pretrained language transformers is becomingincreasingly prevalent for solving natural language processing tasks. Whileeffective, it can still require a large number of tunable parameters. This canbe a drawback for low-resource applications and training withdifferential-privacy constraints, where excessive noise may be introducedduring finetuning. To this end, we propose a novel language transformerfinetuning strategy that introduces task-specific parameters in multipletransformer layers. These parameters are derived from fixed random projectionsof a single trainable vector, enabling finetuning with significantly fewerparameters while maintaining performance. We achieve within 5% of fullfinetuning performance on GLUE tasks with as few as 4,100 parameters per task,outperforming other parameter-efficient finetuning approaches that use asimilar number of per-task parameters. Besides, the random projections can beprecomputed at inference, avoiding additional computational latency. All thesemake our method particularly appealing for low-resource applications. Finally,our method achieves the best or comparable utility compared to several recentfinetuning methods when training with the same privacy constraints,underscoring its effectiveness and potential real-world impact.\rDoes CLIP Know My Face?\nDominik Hintersdorf Lukas Struppek Manuel Brack Felix Friedrich Patrick Schramowski Kristian Kersting\nabstract\rabstract: With the rise of deep learning in various applications, privacy concernsaround the protection of training data has become a critical area of research.Whereas prior studies have focused on privacy risks in single-modal models, weintroduce a novel method to assess privacy for multi-modal models, specificallyvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)reveals whether an individual was included in the training data by querying themodel with images of the same person. Letting the model choose from a widevariety of possible text labels, the model reveals whether it recognizes theperson and, therefore, was used for training. Our large-scale experiments onCLIP demonstrate that individuals used for training can be identified with veryhigh accuracy. We confirm that the model has learned to associate names withdepicted individuals, implying the existence of sensitive information that canbe extracted by adversaries. Our results highlight the need for strongerprivacy protection in large-scale models and suggest that IDIAs can be used toprove the unauthorized use of data for training and to enforce privacy laws.\r2023-05-29\nREx: Data-Free Residual Quantization Error Expansion\nEdouard Yvinec Arnaud Dapgony Matthieu Cord Kevin Bailly\nabstract\rabstract: Deep neural networks (DNNs) are ubiquitous in computer vision and naturallanguage processing, but suffer from high inference cost. This problem can beaddressed by quantization, which consists in converting floating pointoperations into a lower bit-width format. With the growing concerns on privacyrights, we focus our efforts on data-free methods. However, such techniquessuffer from their lack of adaptability to the target devices, as a hardwaretypically only support specific bit widths. Thus, to adapt to a variety ofdevices, a quantization method shall be flexible enough to find good accuracyv.s. speed trade-offs for every bit width and target device. To achieve this,we propose REx, a quantization method that leverages residual error expansion,along with group sparsity and an ensemble approximation for betterparallelization. REx is backed off by strong theoretical guarantees andachieves superior performance on every benchmarked application (from vision toNLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 toternary quantization).\rVoluminous yet Vacuous? Semantic Capital in an Age of Large Language Models\nLuca Nannini\nabstract\rabstract: Large Language Models (LLMs) have emerged as transformative forces in therealm of natural language processing, wielding the power to generate human-liketext. However, despite their potential for content creation, they carry therisk of eroding our Semantic Capital (SC) - the collective knowledge within ourdigital ecosystem - thereby posing diverse social epistemic challenges. Thispaper explores the evolution, capabilities, and limitations of these models,while highlighting ethical concerns they raise. The study contribution istwo-fold: first, it is acknowledged that, withstanding the challenges oftracking and controlling LLM impacts, it is necessary to reconsider ourinteraction with these AI technologies and the narratives that form publicperception of them. It is argued that before achieving this goal, it isessential to confront a potential deontological tipping point in an increasingAI-driven infosphere. This goes beyond just adhering to AI ethical norms orregulations and requires understanding the spectrum of social epistemic risksLLMs might bring to our collective SC. Secondly, building on Luciano Floridi\u0026rsquo;staxonomy for SC risks, those are mapped within the functionality andconstraints of LLMs. By this outlook, we aim to protect and enrich our SC whilefostering a collaborative environment between humans and AI that augments humanintelligence rather than replacing it.\rFingerprinting Generative Adversarial Networks\nGuanlin Li Guowen Xu Han Qiu Shangwei Guo Run Wang Jiwei Li Tianwei Zhang Rongxing Lu\nabstract\rabstract: Generative Adversarial Networks (GANs) have been widely used in variousapplication scenarios. Since the production of a commercial GAN requiressubstantial computational and human resources, the copyright protection of GANsis urgently needed. In this paper, we present the first fingerprinting schemefor the Intellectual Property (IP) protection of GANs. We break through thestealthiness and robustness bottlenecks suffered by previous fingerprintingmethods for classification models being naively transferred to GANs.Specifically, we innovatively construct a composite deep learning model fromthe target GAN and a classifier. Then we generate fingerprint samples from thiscomposite model, and embed them in the classifier for effective ownershipverification. This scheme inspires some concrete methodologies to practicallyprotect the modern GAN models. Theoretical analysis proves that these methodscan satisfy different security requirements necessary for IP protection. Wealso conduct extensive experiments to show that our solutions outperformexisting strategies.\r2023-05-26\nUnsupervised Melody-Guided Lyrics Generation\nYufei Tian Anjali Narayan-Chen Shereen Oraby Alessandra Cervone Gunnar Sigurdsson Chenyang Tao Wenbo Zhao Tagyoung Chung Jing Huang Nanyun Peng\nabstract\rabstract: Automatic song writing is a topic of significant practical interest. However,its research is largely hindered by the lack of training data due to copyrightconcerns and challenged by its creative nature. Most noticeably, prior worksoften fall short of modeling the cross-modal correlation between melody andlyrics due to limited parallel data, hence generating lyrics that are lesssingable. Existing works also lack effective mechanisms for content control, amuch desired feature for democratizing song creation for people with limitedmusic background. In this work, we propose to generate pleasantly listenablelyrics without training on melody-lyric aligned data. Instead, we design ahierarchical lyric generation framework that disentangles training (basedpurely on text) from inference (melody-guided text generation). At inferencetime, we leverage the crucial alignments between melody and lyrics and compilethe given melody into constraints to guide the generation process. Evaluationresults show that our model can generate high-quality lyrics that are moresingable, intelligible, coherent, and in rhyme than strong baselines includingthose supervised on parallel data.\r2023-05-25\nDon\u0026rsquo;t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text\nAshim Gupta Carter Wood Blum Temma Choji Yingjie Fei Shalin Shah Alakananda Vempala Vivek Srikumar\nabstract\rabstract: Can language models transform inputs to protect text classifiers againstadversarial attacks? In this work, we present ATINTER, a model that interceptsand learns to rewrite adversarial inputs to make them non-adversarial for adownstream text classifier. Our experiments on four datasets and five attackmechanisms reveal that ATINTER is effective at providing better adversarialrobustness than existing defense approaches, without compromising taskaccuracy. For example, on sentiment classification using the SST-2 dataset, ourmethod improves the adversarial accuracy over the best existing defenseapproach by more than 4% with a smaller decrease in task accuracy (0.5% vs2.5%). Moreover, we show that ATINTER generalizes across multiple downstreamtasks and classifiers without having to explicitly retrain it for thosesettings. Specifically, we find that when ATINTER is trained to removeadversarial perturbations for the sentiment classification task on the SST-2dataset, it even transfers to a semantically different task of newsclassification (on AGNews) and improves the adversarial robustness by more than10%.\rTraining Data Extraction From Pre-trained Language Models: A Survey\nShotaro Ishihara\nabstract\rabstract: As the deployment of pre-trained language models (PLMs) expands, pressingsecurity concerns have arisen regarding the potential for malicious extractionof training data, posing a threat to data privacy. This study is the first toprovide a comprehensive survey of training data extraction from PLMs. Ourreview covers more than 100 key papers in fields such as natural languageprocessing and security. First, preliminary knowledge is recapped and ataxonomy of various definitions of memorization is presented. The approachesfor attack and defense are then systemized. Furthermore, the empirical findingsof several quantitative studies are highlighted. Finally, future researchdirections based on this review are suggested.\r2023-05-24\nFlocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models\nHaonan Duan Adam Dziedzic Nicolas Papernot Franziska Boenisch\nabstract\rabstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock\u0026rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.\rFrom Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads\nP. V. Sai Charan Hrushikesh Chunduri P. Mohan Anand Sandeep K Shukla\nabstract\rabstract: This research article critically examines the potential risks andimplications arising from the malicious utilization of large languagemodels(LLM), focusing specifically on ChatGPT and Google\u0026rsquo;s Bard. Although theselarge language models have numerous beneficial applications, the misuse of thistechnology by cybercriminals for creating offensive payloads and tools is asignificant concern. In this study, we systematically generated implementablecode for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, andconduct a comparative analysis of its performance with Google\u0026rsquo;s Bard. Ourexperimentation reveals that ChatGPT has the potential to enable attackers toaccelerate the operation of more targeted and sophisticated attacks.Additionally, the technology provides amateur attackers with more capabilitiesto perform a wide range of attacks and empowers script kiddies to developcustomized tools that contribute to the acceleration of cybercrime.Furthermore, LLMs significantly benefits malware authors, particularlyransomware gangs, in generating sophisticated variants of wiper and ransomwareattacks with ease. On a positive note, our study also highlights how offensivesecurity researchers and pentesters can make use of LLMs to simulate realisticattack scenarios, identify potential vulnerabilities, and better protectorganizations. Overall, we conclude by emphasizing the need for increasedvigilance in mitigating the risks associated with LLMs. This includesimplementing robust security measures, increasing awareness and educationaround the potential risks of this technology, and collaborating with securityexperts to stay ahead of emerging threats.\rMachine Unlearning: its nature, scope, and importance for a \u0026ldquo;delete culture\u0026rdquo;\nLuciano Floridi\nabstract\rabstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them \u0026lsquo;forget\u0026rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.\rTrade-Offs Between Fairness and Privacy in Language Modeling\nCleo Matzken Steffen Eger Ivan Habernal\nabstract\rabstract: Protecting privacy in contemporary NLP models is gaining in importance. Sodoes the need to mitigate social biases of such models. But can we have both atthe same time? Existing research suggests that privacy preservation comes atthe price of worsening biases in classification tasks. In this paper, weexplore the extent to which this tradeoff really holds when we incorporate bothprivacy preservation and de-biasing techniques into training text generationmodels. How does improving the model along one dimension affect the otherdimension as well as the utility of the model? We conduct an extensive set ofexperiments that include bias detection, privacy attacks, language modeling,and performance on downstream tasks.\rPrivacy Implications of Retrieval-Based Language Models\nYangsibo Huang Samyak Gupta Zexuan Zhong Kai Li Danqi Chen\nabstract\rabstract: Retrieval-based language models (LMs) have demonstrated improvedinterpretability, factuality, and adaptability compared to their parametriccounterparts, by incorporating retrieved text from external datastores. Whileit is well known that parametric models are prone to leaking private data, itremains unclear how the addition of a retrieval datastore impacts modelprivacy. In this work, we present the first study of privacy risks inretrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimaldesign and training procedure in domains where privacy is of concern, aiming tostrike a balance between utility and privacy. Crucially, we find that $k$NN-LMsare more susceptible to leaking private information from their privatedatastore than parametric models. We further explore mitigations of privacyrisks. When privacy information is targeted and readily detected in the text,we find that a simple sanitization step would completely eliminate the risks,while decoupling query and key encoders achieves an even better utility-privacytrade-off. Otherwise, we consider strategies of mixing public and private datain both datastore and encoder training. While these methods offer modestimprovements, they leave considerable room for future work. Together, ourfindings provide insights for practitioners to better understand and mitigateprivacy risks in retrieval-based LMs. Our code is available at:https://github.com/Princeton-SysML/kNNLM_privacy .\rCan Copyright be Reduced to Privacy?\nNiva Elkin-Koren Uri Hacohen Roi Livni Shay Moran\nabstract\rabstract: There is an increasing concern that generative AI models may produce outputsthat are remarkably similar to the copyrighted input content on which they aretrained. This worry has escalated as the quality and complexity of generativemodels have immensely improved, and the availability of large datasetscontaining copyrighted material has increased. Researchers are activelyexploring strategies to mitigate the risk of producing infringing samples, anda recent line of work suggests to employ techniques such as differentialprivacy and other forms of algorithmic stability to safeguard copyrightedcontent. In this work, we examine the question whether algorithmic stabilitytechniques such as differential privacy are suitable to ensure the responsibleuse of generative models without inadvertently violating copyright laws. Weargue that there are fundamental differences between privacy and copyright thatshould not be overlooked. In particular we highlight that although algorithmicstability may be perceived as a practical tool to detect copying, it does notnecessarily equate to copyright protection. Therefore, if it is adopted asstandard for copyright infringement, it may undermine copyright law intendedpurposes.\rMGeo: Multi-Modal Geographic Pre-Training Method\nRuixue Ding Boli Chen Pengjun Xie Fei Huang Xin Li Qiang Zhang Yao Xu\nabstract\rabstract: As a core task in location-based services (LBS) (e.g., navigation maps),query and point of interest (POI) matching connects users\u0026rsquo; intent withreal-world geographic information. Recently, pre-trained models (PTMs) havemade advancements in many natural language processing (NLP) tasks. Generictext-based PTMs do not have enough geographic knowledge for query-POI matching.To overcome this limitation, related literature attempts to employdomain-adaptive pre-training based on geo-related corpus. However, a querygenerally contains mentions of multiple geographic objects, such as nearbyroads and regions of interest (ROIs). The geographic context (GC), i.e., thesediverse geographic objects and their relationships, is therefore pivotal toretrieving the most relevant POI. Single-modal PTMs can barely make use of theimportant GC and therefore have limited performance. In this work, we propose anovel query-POI matching method Multi-modal Geographic language model (MGeo),which comprises a geographic encoder and a multi-modal interaction module. MGeorepresents GC as a new modality and is able to fully extract multi-modalcorrelations for accurate query-POI matching. Besides, there is no publiclyavailable benchmark for this topic. In order to facilitate further research, webuild a new open-source large-scale benchmark Geographic TExtual Similarity(GeoTES). The POIs come from an open-source geographic information system(GIS). The queries are manually generated by annotators to prevent privacyissues. Compared with several strong baselines, the extensive experimentresults and detailed ablation analyses on GeoTES demonstrate that our proposedmulti-modal pre-training method can significantly improve the query-POImatching capability of generic PTMs, even when the queries\u0026rsquo; GC is not provided.Our code and dataset are publicly available athttps://github.com/PhantomGrapes/MGeo.\r2023-05-23\nA Customized Text Sanitization Mechanism with Differential Privacy\nHuimin Chen Fengran Mo Yanhao Wang Cen Chen Jian-Yun Nie Chengyu Wang Jamie Cui\nabstract\rabstract: As privacy issues are receiving increasing attention within the NaturalLanguage Processing (NLP) community, numerous methods have been proposed tosanitize texts subject to differential privacy. However, the state-of-the-arttext sanitization mechanisms based on metric local differential privacy (MLDP)do not apply to non-metric semantic similarity measures and cannot achieve goodtrade-offs between privacy and utility. To address the above limitations, wepropose a novel Customized Text (CusText) sanitization mechanism based on theoriginal $\\epsilon$-differential privacy (DP) definition, which is compatiblewith any similarity measure. Furthermore, CusText assigns each input token acustomized output set of tokens to provide more advanced privacy protection atthe token level. Extensive experiments on several benchmark datasets show thatCusText achieves a better trade-off between privacy and utility than existingmechanisms. The code is available at https://github.com/sai4july/CusText.\r2023-05-22\nThe \u0026ldquo;code\u0026rsquo;\u0026rsquo; of Ethics:A Holistic Audit of AI Code Generators\nWanlun Ma Yiliao Song Minhui Xue Sheng Wen Yang Xiang\nabstract\rabstract: AI-powered programming language generation (PLG) models have gainedincreasing attention due to their ability to generate source code of programsin a few seconds with a plain program description. Despite their remarkableperformance, many concerns are raised over the potential risks of theirdevelopment and deployment, such as legal issues of copyright infringementinduced by training usage of licensed code, and malicious consequences due tothe unregulated use of these models. In this paper, we present thefirst-of-its-kind study to systematically investigate the accountability of PLGmodels from the perspectives of both model development and deployment. Inparticular, we develop a holistic framework not only to audit the training datausage of PLG models, but also to identify neural code generated by PLG modelsas well as determine its attribution to a source model. To this end, we proposeusing membership inference to audit whether a code snippet used is in the PLGmodel\u0026rsquo;s training data. In addition, we propose a learning-based method todistinguish between human-written code and neural code. In neural codeattribution, through both empirical and theoretical analysis, we show that itis impossible to reliably attribute the generation of one code snippet to onemodel. We then propose two feasible alternative methods: one is to attributeone neural code snippet to one of the candidate PLG models, and the other is toverify whether a set of neural code snippets can be attributed to a given PLGmodel. The proposed framework thoroughly examines the accountability of PLGmodels which are verified by extensive experiments. The implementations of ourproposed framework are also encapsulated into a new artifact, namedCodeForensic, to foster further research.\rEnhancing Small Medical Learners with Privacy-preserving Contextual Prompting\nXinlu Zhang Shiyang Li Xianjun Yang Chenxin Tian Yao Qin Linda Ruth Petzold\nabstract\rabstract: Large language models (LLMs) demonstrate remarkable medical expertise, butdata privacy concerns impede their direct use in healthcare environments.Although offering improved data privacy protection, domain-specific smalllanguage models (SLMs) often underperform LLMs, emphasizing the need formethods that reduce this performance gap while alleviating privacy concerns. Inthis paper, we present a simple yet effective method that harnesses LLMs\u0026rsquo;medical proficiency to boost SLM performance in medical tasks underprivacy-restricted scenarios. Specifically, we mitigate patient privacy issuesby extracting keywords from medical data and prompting the LLM to generate amedical knowledge-intensive context by simulating clinicians\u0026rsquo; thoughtprocesses. This context serves as additional input for SLMs, augmenting theirdecision-making capabilities. Our method significantly enhances performance inboth few-shot and full training settings across three medicalknowledge-intensive tasks, achieving up to a 22.57% increase in absoluteaccuracy compared to SLM fine-tuning without context, and sets newstate-of-the-art results in two medical tasks within privacy-restrictedscenarios. Further out-of-domain testing and experiments in two general domaindatasets showcase its generalizability and broad applicability.\rMist: Towards Improved Adversarial Examples for Diffusion Models\nChumeng Liang Xiaoyu Wu\nabstract\rabstract: Diffusion Models (DMs) have empowered great success inartificial-intelligence-generated content, especially in artwork creation, yetraising new concerns in intellectual properties and copyright. For example,infringers can make profits by imitating non-authorized human-created paintingswith DMs. Recent researches suggest that various adversarial examples fordiffusion models can be effective tools against these copyright infringements.However, current adversarial examples show weakness in transferability overdifferent painting-imitating methods and robustness under straightforwardadversarial defense, for example, noise purification. We surprisingly find thatthe transferability of adversarial examples can be significantly enhanced byexploiting a fused and modified adversarial loss term under consistentparameters. In this work, we comprehensively evaluate the cross-methodtransferability of adversarial examples. The experimental observation showsthat our method generates more transferable adversarial examples with evenstronger robustness against the simple adversarial defense.\r2023-05-21\nWatermarking Diffusion Model\nYugeng Liu Zheng Li Michael Backes Yun Shen Yang Zhang\nabstract\rabstract: The availability and accessibility of diffusion models (DMs) havesignificantly increased in recent years, making them a popular tool foranalyzing and predicting the spread of information, behaviors, or phenomenathrough a population. Particularly, text-to-image diffusion models (e.g., DALLE2 and Latent Diffusion Models (LDMs) have gained significant attention inrecent years for their ability to generate high-quality images and performvarious image synthesis tasks. Despite their widespread adoption in manyfields, DMs are often susceptible to various intellectual property violations.These can include not only copyright infringement but also more subtle forms ofmisappropriation, such as unauthorized use or modification of the model.Therefore, DM owners must be aware of these potential risks and takeappropriate steps to protect their models. In this work, we are the first toprotect the intellectual property of DMs. We propose a simple but effectivewatermarking scheme that injects the watermark into the DMs and can be verifiedby the pre-defined prompts. In particular, we propose two differentwatermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injectsthe watermark into the LDMs and activates it using a prompt containing thewatermark. On the other hand, the FIXEDWM is considered more advanced andstealthy compared to the NAIVEWM, as it can only activate the watermark whenusing a prompt containing a trigger in a fixed position. We conducted arigorous evaluation of both approaches, demonstrating their effectiveness inwatermark injection and verification with minimal impact on the LDM\u0026rsquo;sfunctionality.\rCommunication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter\nYi Liu Xiaohan Bi Lei Li Sishuo Chen Wenkai Yang Xu Sun\nabstract\rabstract: Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as apromising paradigm for institutions with limited language resources. Thisapproach allows multiple institutions to act as clients and train a unifiedmodel through model synchronization, rather than collecting sensitive data forcentralized training. This significantly reduces the cost of corpus collectionand preserves data privacy. However, as pre-trained language models (PLMs)continue to increase in size, the communication cost for transmittingparameters during synchronization has become a training speed bottleneck. Inthis paper, we propose a communication-efficient Fed-MNMT framework thataddresses this issue by keeping PLMs frozen and only transferring lightweightadapter modules between clients. Since different language pairs exhibitsubstantial discrepancies in data distributions, adapter parameters of clientsmay conflict with each other. To tackle this, we explore various clusteringstrategies to group parameters for integration and mitigate the negativeeffects of conflicting parameters. Experimental results demonstrate that ourframework reduces communication cost by over 98% while achieving similar oreven better performance compared to competitive baselines. Further analysisreveals that clustering strategies effectively solve the problem of linguisticdiscrepancy and pruning adapter modules further improves communicationefficiency.\r2023-05-20\nCan Public Large Language Models Help Private Cross-device Federated Learning?\nBoxin Wang Yibo Jacky Zhang Yuan Cao Bo Li H. Brendan McMahan Sewoong Oh Zheng Xu Manzil Zaheer\nabstract\rabstract: We study (differentially) private federated learning (FL) of language models.The language models in cross-device FL are relatively small, which can betrained with meaningful formal user-level differential privacy (DP) guaranteeswhen massive parallelism in training is enabled by the participation of amoderate size of users. Recently, public data has been used to improveprivacy-utility trade-offs for both large and small language models. In thiswork, we provide a systematic study of using large-scale public data and LLMsto help differentially private training of on-device FL models, and furtherimprove the privacy-utility tradeoff by techniques of distillation. Moreover,we propose a novel distribution matching algorithm with theoretical groundingto sample public data close to private data distribution, which significantlyimproves the sample efficiency of (pre-)training on public data. The proposedmethod is efficient and effective for training private model by takingadvantage of public data, especially for customized on-device architecturesthat do not have ready-to-use pre-trained models.\r2023-05-19\nControlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning\nMustafa Safa Ozdayi Charith Peris Jack FitzGerald Christophe Dupuy Jimit Majmudar Haidar Khan Rahil Parikh Rahul Gupta\nabstract\rabstract: Large Language Models (LLMs) are known to memorize significant portions oftheir training data. Parts of this memorized content have been shown to beextractable by simply querying the model, which poses a privacy risk. Wepresent a novel approach which uses prompt-tuning to control the extractionrates of memorized content in LLMs. We present two prompt training strategiesto increase and decrease extraction rates, which correspond to an attack and adefense, respectively. We demonstrate the effectiveness of our techniques byusing models from the GPT-Neo family on a public benchmark. For the 1.3Bparameter GPT-Neo model, our attack yields a 9.3 percentage point increase inextraction rate compared to our baseline. Our defense can be tuned to achievedifferent privacy-utility trade-offs by a user-specified hyperparameter. Weachieve an extraction rate reduction of up to 97.7% relative to our baseline,with a perplexity increase of 16.9%.\rTowards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models\nJiayi Fu Haoying Han Xing Su Chao Fan\nabstract\rabstract: Pre-trained large language models (PLMs) have the potential to support urbanscience research through content creation, information extraction, assistedprogramming, text classification, and other technical advances. In thisresearch, we explored the opportunities, challenges, and prospects of PLMs inurban science research. Specifically, we discussed potential applications ofPLMs to urban institution, urban space, urban information, and citizenbehaviors research through seven examples using ChatGPT. We also examined thechallenges of PLMs in urban science research from both technical and socialperspectives. The prospects of the application of PLMs in urban scienceresearch were then proposed. We found that PLMs can effectively aid inunderstanding complex concepts in urban science, facilitate urban spatial formidentification, assist in disaster monitoring, and sense public sentiment. Atthe same time, however, the applications of PLMs in urban science research faceevident threats, such as technical limitations, security, privacy, and socialbias. The development of fundamental models based on domain knowledge andhuman-AI collaboration may help improve PLMs to support urban science researchin future.\rChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery\nAnaelia Ovalle Mehrab Beikzadeh Parshan Teimouri Kai-Wei Chang Majid Sarrafzadeh\nabstract\rabstract: Large language models have been useful in expanding mental health caredelivery. ChatGPT, in particular, has gained popularity for its ability togenerate human-like dialogue. However, data-sensitive domains \u0026ndash; including butnot limited to healthcare \u0026ndash; face challenges in using ChatGPT due to privacyand data-ownership concerns. To enable its utilization, we propose a textambiguation framework that preserves user privacy. We ground this in the taskof addressing stress prompted by user-provided texts to demonstrate theviability and helpfulness of privacy-preserved generations. Our results suggestthat chatGPT recommendations are still able to be moderately helpful andrelevant, even when the original user text is not provided.\r2023-05-18\nComparing Biases and the Impact of Multilingual Training across Multiple Languages\nSharon Levy Neha Anna John Ling Liu Yogarshi Vyas Jie Ma Yoshinari Fujinuma Miguel Ballesteros Vittorio Castelli Dan Roth\nabstract\rabstract: Studies in bias and fairness in natural language processing have primarilyexamined social biases within a single language and/or across few attributes(e.g. gender, race). However, biases can manifest differently across variouslanguages for individual attributes. As a result, it is critical to examinebiases within each language and attribute. Of equal importance is to study howthese biases compare across languages and how the biases are affected whentraining a model on multilingual data versus monolingual data. We present abias analysis across Italian, Chinese, English, Hebrew, and Spanish on thedownstream sentiment analysis task to observe whether specific demographics areviewed more positively. We study bias similarities and differences across theselanguages and investigate the impact of multilingual vs. monolingual trainingdata. We adapt existing sentiment bias templates in English to Italian,Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality,and gender. Our results reveal similarities in bias expression such asfavoritism of groups that are dominant in each language\u0026rsquo;s culture (e.g.majority religions and nationalities). Additionally, we find an increasedvariation in predictions across protected groups, indicating biasamplification, after multilingual finetuning in comparison to multilingualpretraining.\rVaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2\nAryo Pradipta Gema Michał Kobiela Achille Fraisse Ajitha Rajan Diego A. Oyarzún Javier Antonio Alfaro\nabstract\rabstract: The SARS-CoV-2 pandemic has emphasised the importance of developing auniversal vaccine that can protect against current and future variants of thevirus. The present study proposes a novel conditional protein Language Modelarchitecture, called Vaxformer, which is designed to produce natural-lookingantigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generatedprotein sequences of the Vaxformer model using DDGun protein stability measure,netMHCpan antigenicity score, and a structure fidelity score with AlphaFold togauge its viability for vaccine development. Our results show that Vaxformeroutperforms the existing state-of-the-art Conditional Variational Autoencodermodel to generate antigenicity-controlled SARS-CoV-2 spike proteins. Thesefindings suggest promising opportunities for conditional Transformer models toexpand our understanding of vaccine design and their role in mitigating globalhealth challenges. The code used in this study is available athttps://github.com/aryopg/vaxformer .\rHow Deep Learning Sees the World: A Survey on Adversarial Attacks \u0026amp; Defenses\nJoana C. Costa Tiago Roxo Hugo Proença Pedro R. M. Inácio\nabstract\rabstract: Deep Learning is currently used to perform multiple tasks, such as objectrecognition, face recognition, and natural language processing. However, DeepNeural Networks (DNNs) are vulnerable to perturbations that alter the networkprediction (adversarial examples), raising concerns regarding its usage incritical areas, such as self-driving vehicles, malware detection, andhealthcare. This paper compiles the most recent adversarial attacks, grouped bythe attacker capacity, and modern defenses clustered by protection strategies.We also present the new advances regarding Vision Transformers, summarize thedatasets and metrics used in the context of adversarial settings, and comparethe state-of-the-art results under different attacks, finishing with theidentification of open issues.\rAugmented Large Language Models with Parametric Knowledge Guiding\nZiyang Luo Can Xu Pu Zhao Xiubo Geng Chongyang Tao Jing Ma Qingwei Lin Daxin Jiang\nabstract\rabstract: Large Language Models (LLMs) have significantly advanced natural languageprocessing (NLP) with their impressive language understanding and generationcapabilities. However, their performance may be suboptimal for domain-specifictasks that require specialized knowledge due to limited exposure to the relateddata. Additionally, the lack of transparency of most state-of-the-art (SOTA)LLMs, which can only be accessed via APIs, impedes further fine-tuning withdomain custom data. Moreover, providing private data to the LLMs\u0026rsquo; owner leadsto data privacy problems. To address these challenges, we propose the novelParametric Knowledge Guiding (PKG) framework, which equips LLMs with aknowledge-guiding module to access relevant knowledge without altering theLLMs\u0026rsquo; parameters. Our PKG is based on open-source \u0026ldquo;white-box\u0026rdquo; language models,allowing offline memory of any knowledge that LLMs require. We demonstrate thatour PKG framework can enhance the performance of \u0026ldquo;black-box\u0026rdquo; LLMs on a range ofdomain knowledge-intensive tasks that require factual (+7.9%), tabular(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.\rEthical ChatGPT: Concerns, Challenges, and Commandments\nJianlong Zhou Heimo Müller Andreas Holzinger Fang Chen\nabstract\rabstract: Large language models, e.g. ChatGPT are currently contributing enormously tomake artificial intelligence even more popular, especially among the generalpopulation. However, such chatbot models were developed as tools to supportnatural language communication between humans. Problematically, it is very mucha ``statistical correlation machine\u0026quot; (correlation instead of causality) andthere are indeed ethical concerns associated with the use of AI language modelssuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlightsspecific ethical concerns on ChatGPT and articulates key challenges whenChatGPT is used in various applications. Practical commandments for differentstakeholders of ChatGPT are also proposed that can serve as checklistguidelines for those applying ChatGPT in their applications. These commandmentexamples are expected to motivate the ethical use of ChatGPT.\r2023-05-17\nLife of PII \u0026ndash; A PII Obfuscation Transformer\nAjinkya Deshmukh Saumya Banthia Anantha Sharma\nabstract\rabstract: Protecting sensitive information is crucial in today\u0026rsquo;s world of LargeLanguage Models (LLMs) and data-driven services. One common method used topreserve privacy is by using data perturbation techniques to reduceoverreaching utility of (sensitive) Personal Identifiable Information (PII)data while maintaining its statistical and semantic properties. Dataperturbation methods often result in significant information loss, making themimpractical for use. In this paper, we propose \u0026lsquo;Life of PII\u0026rsquo;, a novelObfuscation Transformer framework for transforming PII into faux-PII whilepreserving the original information, intent, and context as much as possible.Our approach includes an API to interface with the given document, aconfiguration-based obfuscator, and a model based on the Transformerarchitecture, which has shown high context preservation and performance innatural language processing tasks and LLMs. Our Transformer-based approach learns mapping between the original PII andits transformed faux-PII representation, which we call \u0026ldquo;obfuscated\u0026rdquo; data. Ourexperiments demonstrate that our method, called Life of PII, outperformstraditional data perturbation techniques in terms of both utility preservationand privacy protection. We show that our approach can effectively reduceutility loss while preserving the original information, offering greaterflexibility in the trade-off between privacy protection and data utility. Ourwork provides a solution for protecting PII in various real-world applications.\rNetGPT: Generative Pretrained Transformer for Network Traffic\nXuying Meng Chungang Lin Yequan Wang Yujun Zhang\nabstract\rabstract: All data on the Internet are transferred by network traffic, thus accuratelymodeling network traffic can help improve network services quality and protectdata privacy. Pretrained models for network traffic can utilize large-scale rawdata to learn the essential characteristics of network traffic, and generatedistinguishable results for input traffic without considering specificdownstream tasks. Effective pretrained models can significantly optimize thetraining efficiency and effectiveness of downstream tasks, such as applicationclassification, attack detection and traffic generation. Despite the greatsuccess of pretraining in natural language processing, there is no work in thenetwork field. Considering the diverse demands and characteristics of networktraffic and network tasks, it is non-trivial to build a pretrained model fornetwork traffic and we face various challenges, especially the heterogeneousheaders and payloads in the multi-pattern network traffic and the differentdependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt toprovide a generative pretrained model NetGPT for both traffic understanding andgeneration tasks. We propose the multi-pattern network traffic modeling toconstruct unified text inputs and support both traffic understanding andgeneration tasks. We further optimize the adaptation effect of the pretrainedmodel to diversified tasks by shuffling header fields, segmenting packets inflows, and incorporating diverse task labels with prompts. With diverse trafficdatasets from encrypted software, DNS, private industrial protocols andcryptocurrency mining, expensive experiments demonstrate the effectiveness ofour NetGPT in a range of traffic understanding and generation tasks on trafficdatasets, and outperform state-of-the-art baselines by a wide margin.\rA Novel Procrustes Analysis Method to Quantify Multi-Joint Coordination of the Upper Extremity after Stroke\nKhadija F. Zaidi Michelle Harris-Love\nabstract\rabstract: Upper extremity motor impairment affects about 80% of persons after strokes.For stroke rehabilitation, upper limb kinematic assessments have increasinglybeen used as primary or secondary outcome measures. Studying the upperextremity provides a valuable tool for assessing limb coordination,mal-adaptations, and recovery. There is currently no universal standardizedscale for categorizing multi-joint upper extremity movement. We propose amodified Procrustes statistical shape method as a quantitative analysis thatcan recognize segments of movement where multiple limb segments arecoordinating movement. Generalized Procrustes methods allow data points to becompared across an array simultaneously rather than comparing them in pairs.Rather than rely solely on discrete kinematic values to contrast movement, thismethod allows evaluation of how movement progresses. The Procrustes analysis ofable-bodied movement showed that the hand and forearm segments moved in a morecoordinated manner during initiation. The shoulder and elbow become morecoordinated during movement completion. In impaired movement, this coordinationbetween the hand and forearm is disrupted. Potentially mal-adaptivecompensation occurs between the upper arm and forearm after movement enters thedeceleration phase. The utilization of Procrustes analysis may be a steptowards developing a comprehensive and universal quantitative tool that doesnot require changes to existing treatments or increase patient burden. Copyright 2023 IEEE. Personal use of this material is permitted. Permissionfrom IEEE must be obtained for all other uses, in any current or future media,including reprinting/republishing this material for advertising or promotionalpurposes, creating new collective works, for resale or redistribution toservers or lists, or reuse of any copyrighted component of this work in otherworks.\r2023-05-16\nQuantum Computation by Spin Parity Measurements with Encoded Spin Qubits\nMatthew Brooks Charles Tahan\nabstract\rabstract: Joint measurements of two-Pauli observables are a powerful tool for both thecontrol and protection of quantum information. By following a simple recipe formeasurement choices, single- and two- qubit rotations using two-Pauli parityand single qubit measurements are guaranteed to be unitary whilst requiringonly a single ancilla qubit. This language for measurement based quantumcomputing is shown to be directly applicable to encoded double quantum dotsinglet-triplet spin qubits, by measuring spin-parity between dots fromneighboring qubits. Along with exchange interaction, a complete, leakage free,measurement based gate set can be shown, up to a known Pauli correction. Boththeoretically exact spin-parity measurements and experimentally demonstratedasymmetric spin-parity measurements are shown to be viable for achieving theproposed measurement based scheme, provided some extra leakage mitigatingmeasurement steps. This new method of spin qubit control offers a leakagesuppressed, low resource overhead implementation of a measurement-based controlthat is viable on current spin qubit processor devices.\rBot or Human? Detecting ChatGPT Imposters with A Single Question\nHong Wang Xuan Luo Weizhi Wang Xifeng Yan\nabstract\rabstract: Large language models like ChatGPT have recently demonstrated impressivecapabilities in natural language understanding and generation, enabling variousapplications including translation, essay writing, and chit-chatting. However,there is a concern that they can be misused for malicious purposes, such asfraud or denial-of-service attacks. Therefore, it is crucial to develop methodsfor detecting whether the party involved in a conversation is a bot or a human.In this paper, we propose a framework named FLAIR, Finding Large language modelAuthenticity via a single Inquiry and Response, to detect conversational botsin an online manner. Specifically, we target a single question scenario thatcan effectively differentiate human users from bots. The questions are dividedinto two categories: those that are easy for humans but difficult for bots(e.g., counting, substitution, positioning, noise filtering, and ASCII art),and those that are easy for bots but difficult for humans (e.g., memorizationand computation). Our approach shows different strengths of these questions intheir effectiveness, providing a new way for online service providers toprotect themselves against nefarious activities and ensure that they areserving real users. We open-sourced our dataset onhttps://github.com/hongwang600/FLAIR and welcome contributions from thecommunity to enrich such detection datasets.\r2023-05-15\nA Reproducible Extraction of Training Images from Diffusion Models\nRyan Webster\nabstract\rabstract: Recently, Carlini et al. demonstrated the widely used model Stable Diffusioncan regurgitate real training samples, which is troublesome from a copyrightperspective. In this work, we provide an efficient extraction attack on parwith the recent attack, with several order of magnitudes less networkevaluations. In the process, we expose a new phenomena, which we dub templateverbatims, wherein a diffusion model will regurgitate a training sample largelyin tact. Template verbatims are harder to detect as they require retrieval andmasking to correctly label. Furthermore, they are still generated by newersystems, even those which de-duplicate their training set, and we give insightinto why they still appear during generation. We extract training images fromseveral state of the art systems, including Stable Diffusion 2.0, Deep ImageFloyd, and finally Midjourney v4. We release code to verify our extractionattack, perform the attack, as well as all extracted prompts at\\url{https://github.com/ryanwebster90/onestep-extraction}.\rSB-VQA: A Stack-Based Video Quality Assessment Framework for Video Enhancement\nDing-Jiun Huang Yu-Ting Kao Tieh-Hung Chuang Ya-Chun Tsai Jing-Kai Lou Shuen-Huei Guan\nabstract\rabstract: In recent years, several video quality assessment (VQA) methods have beendeveloped, achieving high performance. However, these methods were notspecifically trained for enhanced videos, which limits their ability to predictvideo quality accurately based on human subjective perception. To address thisissue, we propose a stack-based framework for VQA that outperforms existingstate-of-the-art methods on VDPVE, a dataset consisting of enhanced videos. Inaddition to proposing the VQA framework for enhanced videos, we alsoinvestigate its application on professionally generated content (PGC). Toaddress copyright issues with premium content, we create the PGCVQ dataset,which consists of videos from YouTube. We evaluate our proposed approach andstate-of-the-art methods on PGCVQ, and provide new insights on the results. Ourexperiments demonstrate that existing VQA algorithms can be applied to PGCvideos, and we find that VQA performance for PGC videos can be improved byconsidering the plot of a play, which highlights the importance of videosemantic understanding.\r2023-05-13\nBeyond the Safeguards: Exploring the Security Risks of ChatGPT\nErik Derner Kristina Batistič\nabstract\rabstract: The increasing popularity of large language models (LLMs) such as ChatGPT hasled to growing concerns about their safety, security risks, and ethicalimplications. This paper aims to provide an overview of the different types ofsecurity risks associated with ChatGPT, including malicious text and codegeneration, private data disclosure, fraudulent services, informationgathering, and producing unethical content. We present an empirical studyexamining the effectiveness of ChatGPT\u0026rsquo;s content filters and explore potentialways to bypass these safeguards, demonstrating the ethical implications andsecurity risks that persist in LLMs even when protections are in place. Basedon a qualitative analysis of the security implications, we discuss potentialstrategies to mitigate these risks and inform researchers, policymakers, andindustry professionals about the complex security challenges posed by LLMs likeChatGPT. This study contributes to the ongoing discussion on the ethical andsecurity implications of LLMs, underscoring the need for continued research inthis area.\r2023-05-12\nPLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English\nJianfeng Chi Wasi Uddin Ahmad Yuan Tian Kai-Wei Chang\nabstract\rabstract: Privacy policies provide individuals with information about their rights andhow their personal information is handled. Natural language understanding (NLU)technologies can support individuals and practitioners to understand betterprivacy practices described in lengthy and complex documents. However, existingefforts that use NLU technologies are limited by processing the language in away exclusive to a single task focusing on certain privacy practices. To thisend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)benchmark, a multi-task benchmark for evaluating the privacy policy languageunderstanding across various tasks. We also collect a large corpus of privacypolicies to enable privacy policy domain-specific language model pre-training.We evaluate several generic pre-trained language models and continuepre-training them on the collected corpus. We demonstrate that domain-specificcontinual pre-training offers performance improvements across all tasks.\r2023-05-11\nReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network\nNatan Semyonov Rami Puzis Asaf Shabtai Gilad Katz\nabstract\rabstract: Watermarking is one of the most important copyright protection tools fordigital media. The most challenging type of watermarking is the imperceptibleone, which embeds identifying information in the data while retaining thelatter\u0026rsquo;s original quality. To fulfill its purpose, watermarks need to withstandvarious distortions whose goal is to damage their integrity. In this study, weinvestigate a novel deep learning-based architecture for embeddingimperceptible watermarks. The key insight guiding our architecture design isthe need to correlate the dimensions of our watermarks with the sizes ofreceptive fields (RF) of modules of our architecture. This adaptation makes ourwatermarks more robust, while also enabling us to generate them in a way thatbetter maintains image quality. Extensive evaluations on a wide variety ofdistortions show that the proposed method is robust against most commondistortions on watermarks including collusive distortion.\rCryptSan: Leveraging ARM Pointer Authentication for Memory Safety in C/C++\nKonrad Hohentanner Philipp Zieris Julian Horsch\nabstract\rabstract: Memory safety bugs remain in the top ranks of security vulnerabilities, evenafter decades of research on their detection and prevention. Variousmitigations have been proposed for C/C++, ranging from language dialects toinstrumentation. Among these, compiler-based instrumentation is particularlypromising, not requiring manual code modifications and being able to achieveprecise memory safety. Unfortunately, existing compiler-based solutionscompromise in many areas, including performance but also usability and memorysafety guarantees. New developments in hardware can help improve performanceand security of compiler-based memory safety. ARM Pointer Authentication, addedin the ARMv8.3 architecture, is intended to enable hardware-assisted ControlFlow Integrity (CFI). But since its operations are generic, it also enablesother, more comprehensive hardware-supported runtime integrity approaches. Assuch, we propose CryptSan, a memory safety approach based on ARM PointerAuthentication. CryptSan uses pointer signatures to retrofit memory safety toC/C++ programs, protecting heap, stack, and globals against temporal andspatial vulnerabilities. We present a full LLVM-based prototype implementation,running on an M1 MacBook Pro, i.e., on actual ARMv8.3 hardware. Our prototypeevaluation shows that the system outperforms similar approaches underreal-world conditions. This, together with its interoperability withuninstrumented libraries and cryptographic protection against attacks onmetadata, makes CryptSan a viable solution for retrofitting memory safety toC/C++ programs.\r2023-05-10\nPrivacy-Preserving Prompt Tuning for Large Language Model Services\nYansong Li Zhixing Tan Yang Liu\nabstract\rabstract: Prompt tuning provides an efficient way for users to customize Large LanguageModels (LLMs) with their private data in the emerging LLM service scenario.However, the sensitive nature of private data brings the need for privacypreservation in LLM service customization. Based on prompt tuning, we proposePrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacyguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,allowing users to privatize their data locally with local differential privacy.As prompt tuning performs poorly when directly trained on privatized data, weintroduce a novel privatized token reconstruction task that is trained jointlywith the downstream task, allowing LLMs to learn better task-dependentrepresentations. Despite the simplicity of our framework, experiments show thatRAPT achieves competitive performance across tasks while providing privacyguarantees against adversaries.\r2023-05-09\nMeasuring Forgetting of Memorized Training Examples\nMatthew Jagielski Om Thakkar Florian Tramèr Daphne Ippolito Katherine Lee Nicholas Carlini Eric Wallace Shuang Song Abhradeep Thakurta Nicolas Papernot Chiyuan Zhang\nabstract\rabstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models \u0026ldquo;forget\u0026rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.\r2023-05-08\nLess is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness\nLiangliang Cao Bowen Zhang Chen Chen Yinfei Yang Xianzhi Du Wencong Zhang Zhiyun Lu Yantao Zheng\nabstract\rabstract: The CLIP (Contrastive Language-Image Pre-training) model and its variants arebecoming the de facto backbone in many applications. However, training a CLIPmodel from hundreds of millions of image-text pairs can be prohibitivelyexpensive. Furthermore, the conventional CLIP model doesn\u0026rsquo;t differentiatebetween the visual semantics and meaning of text regions embedded in images.This can lead to non-robustness when the text in the embedded region doesn\u0026rsquo;tmatch the image\u0026rsquo;s visual appearance. In this paper, we discuss two effectiveapproaches to improve the efficiency and robustness of CLIP training: (1)augmenting the training dataset while maintaining the same number ofoptimization steps, and (2) filtering out samples that contain text regions inthe image. By doing so, we significantly improve the classification andretrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering outimages with text regions also protects the model from typographic attacks. Toverify this, we build a new dataset named ImageNet with Adversarial TextRegions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1accuracy of 68.78%, outperforming previous models whose accuracy was all below50%.\rDifferentially Private Attention Computation\nYeqi Gao Zhao Song Xin Yang\nabstract\rabstract: Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].\rAugmented Datasheets for Speech Datasets and Ethical Decision-Making\nOrestis Papakyriakopoulos Anna Seo Gyeong Choi Jerone Andrews Rebecca Bourke William Thong Dora Zhao Alice Xiang Allison Koenecke\nabstract\rabstract: Speech datasets are crucial for training Speech Language Technologies (SLT);however, the lack of diversity of the underlying training data can lead toserious limitations in building equitable and robust SLT products, especiallyalong dimensions of language, accent, dialect, variety, and speech impairment -and the intersectionality of speech features with socioeconomic and demographicfeatures. Furthermore, there is often a lack of oversight on the underlyingtraining data - commonly built on massive web-crawling and/or publiclyavailable speech - with regard to the ethics of such data collection. Toencourage standardized documentation of such speech data components, weintroduce an augmented datasheet for speech datasets, which can be used inaddition to \u0026ldquo;Datasheets for Datasets\u0026rdquo;. We then exemplify the importance of eachquestion in our augmented datasheet based on in-depth literature reviews ofspeech data used in domains such as machine learning, linguistics, and health.Finally, we encourage practitioners - ranging from dataset creators toresearchers - to use our augmented datasheet to better define the scope,properties, and limits of speech datasets, while also encouraging considerationof data-subject protection and user community empowerment. Ethical datasetcreation is not a one-size-fits-all process, but dataset creators can use ouraugmented datasheet to reflexively consider the social context of related SLTapplications and data sources in order to foster more inclusive SLT productsdownstream.\r2023-05-05\nNot what you\u0026rsquo;ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\nKai Greshake Sahar Abdelnabi Shailesh Mishra Christoph Endres Thorsten Holz Mario Fritz\nabstract\rabstract: Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks\u0026rsquo; practical viability againstboth real-world systems, such as Bing\u0026rsquo;s GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication\u0026rsquo;s functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.\rStreamlining personal data access requests: From obstructive procedures to automated web workflows\nNicola Leschke Florian Kirsten Frank Pallas Elias Grünewald\nabstract\rabstract: Transparency and data portability are two core principles of modern privacylegislations such as the GDPR. From the regulatory perspective, providingindividuals (data subjects) with access to their data is a main building blockfor implementing these. Different from other privacy principles and respectiveregulatory provisions, however, this right to data access has so far only seenmarginal technical reflection. Processes related to performing data subjectaccess requests (DSARs) are thus still to be executed manually, hindering theconcept of data access from unfolding its full potential. To tackle this problem, we present an automated approach to the execution ofDSARs, employing modern techniques of web automation. In particular, we proposea generic DSAR workflow model, a corresponding formal language for representingthe particular workflows of different service providers (controllers), apublicly accessible and extendable workflow repository, and a browser-basedexecution engine, altogether providing ``one-click\u0026rsquo;\u0026rsquo; DSARs. To validate ourapproach and technical concepts, we examine, formalize and make publiclyavailable the DSAR workflows of 15 widely used service providers and implementthe execution engine in a publicly available browser extension. Altogether, wethereby pave the way for automated data subject access requests and lay thegroundwork for a broad variety of subsequent technical means helping web usersto better understand their privacy-related exposure to different serviceproviders.\r2023-05-04\nChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI\nBill Tomlinson Andrew W. Torrance Rebecca W. Black\nabstract\rabstract: Recent advances in artificial intelligence (AI) have raised questions aboutwhether the use of AI is appropriate and legal in various professionalcontexts. Here, we present a perspective on how scholars may approach writingin conjunction with AI, and offer approaches to evaluating whether or not suchAI-writing violates copyright or falls within the safe harbor of fair use. Wepresent a set of best practices for standard of care with regard to plagiarism,copyright, and fair use. As AI is likely to grow more capable in the comingyears, it is appropriate to begin integrating AI into scholarly writingactivities. We offer a framework for establishing sound legal and scholarlyfoundations.\rTraining Is Everything: Artificial Intelligence, Copyright, and Fair Training\nAndrew W. Torrance Bill Tomlinson\nabstract\rabstract: To learn how to behave, the current revolutionary generation of AIs must betrained on vast quantities of published images, written works, and sounds, manyof which fall within the core subject matter of copyright law. To some, the useof copyrighted works as training sets for AI is merely a transitory andnon-consumptive use that does not materially interfere with owners\u0026rsquo; content orcopyrights protecting it. Companies that use such content to train their AIengine often believe such usage should be considered \u0026ldquo;fair use\u0026rdquo; under UnitedStates law (sometimes known as \u0026ldquo;fair dealing\u0026rdquo; in other countries). By contrast,many copyright owners, as well as their supporters, consider the incorporationof copyrighted works into training sets for AI to constitute misappropriationof owners\u0026rsquo; intellectual property, and, thus, decidedly not fair use under thelaw. This debate is vital to the future trajectory of AI and its applications. In this article, we analyze the arguments in favor of, and against, viewingthe use of copyrighted works in training sets for AI as fair use. We call thisform of fair use \u0026ldquo;fair training\u0026rdquo;. We identify both strong and spuriousarguments on both sides of this debate. In addition, we attempt to take abroader perspective, weighing the societal costs (e.g., replacement of certainforms of human employment) and benefits (e.g., the possibility of novelAI-based approaches to global issues such as environmental disruption) ofallowing AI to make easy use of copyrighted works as training sets tofacilitate the development, improvement, adoption, and diffusion of AI.Finally, we suggest that the debate over AI and copyrighted works may be atempest in a teapot when placed in the wider context of massive societalchallenges such as poverty, equality, climate change, and loss of biodiversity,to which AI may be part of the solution.\r2023-05-03\nTraining Natural Language Processing Models on Encrypted Text for Enhanced Privacy\nDavut Emre Tasar Ceren Ocal Tasar\nabstract\rabstract: With the increasing use of cloud-based services for training and deployingmachine learning models, data privacy has become a major concern. This isparticularly important for natural language processing (NLP) models, whichoften process sensitive information such as personal communications andconfidential documents. In this study, we propose a method for training NLPmodels on encrypted text data to mitigate data privacy concerns whilemaintaining similar performance to models trained on non-encrypted data. Wedemonstrate our method using two different architectures, namelyDoc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroupsdataset. Our results indicate that both encrypted and non-encrypted modelsachieve comparable performance, suggesting that our encryption method iseffective in preserving data privacy without sacrificing model accuracy. Inorder to replicate our experiments, we have provided a Colab notebook at thefollowing address: https://t.ly/lR-TP\r2023-05-02\nMitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy\nAly M. Kassem\nabstract\rabstract: Large Language models (LLMs) are trained on large amounts of data, which caninclude sensitive information that may compromise personal privacy. LLMs showedto memorize parts of the training data and emit those data verbatim when anadversary prompts appropriately. Previous research has primarily focused ondata preprocessing and differential privacy techniques to address memorizationor prevent verbatim memorization exclusively, which can give a false sense ofprivacy. However, these methods rely on explicit and implicit assumptions aboutthe structure of the data to be protected, which often results in an incompletesolution to the problem. To address this, we propose a novel framework thatutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigateapproximate memorization. Our approach utilizes a negative similarity score,such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilaritypolicy. Our results demonstrate that this framework effectively mitigatesapproximate memorization while maintaining high levels of coherence and fluencyin the generated samples. Furthermore, our framework is robust in mitigatingapproximate memorization across various circumstances, including longercontext, which is known to increase memorization in LLMs.\rDifferentially Private Learning with Per-Sample Adaptive Clipping\nTianyu Xia Shuheng Shen Su Yao Xinyi Fu Ke Xu Xiaolong Xu Xing Fu\nabstract\rabstract: Privacy in AI remains a topic that draws attention from researchers and thegeneral public in recent years. As one way to implement privacy-preserving AI,differentially private learning is a framework that enables AI models to usedifferential privacy (DP). To achieve DP in the learning process, existingalgorithms typically limit the magnitude of gradients with a constant clipping,which requires carefully tuned due to its significant impact on modelperformance. As a solution to this issue, latest works NSGD and Auto-Sinnovatively propose to use normalization instead of clipping to avoidhyperparameter tuning. However, normalization-based approaches like NSGD andAuto-S rely on a monotonic weight function, which imposes excessive weight onsmall gradient samples and introduces extra deviation to the update. In thispaper, we propose a Differentially Private Per-Sample Adaptive Clipping(DP-PSAC) algorithm based on a non-monotonic adaptive weight function, whichguarantees privacy without the typical hyperparameter tuning process of using aconstant clipping while significantly reducing the deviation between the updateand true batch-averaged gradient. We provide a rigorous theoretical convergenceanalysis and show that with convergence rate at the same order, the proposedalgorithm achieves a lower non-vanishing bound, which is maintained overtraining iterations, compared with NSGD/Auto-S. In addition, through extensiveexperimental evaluation, we show that DP-PSAC outperforms or matches thestate-of-the-art methods on multiple main-stream vision and language tasks.\r2023-05-01\nA Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\nCe Zhou Qian Li Chen Li Jun Yu Yixin Liu Guangjing Wang Kai Zhang Cheng Ji Qiben Yan Lifang He Hao Peng Jianxin Li Jia Wu Ziwei Liu Pengtao Xie Caiming Xiong Jian Pei Philip S. Yu Lichao Sun\nabstract\rabstract: Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence.\r2023-04-30\nReliable Gradient-free and Likelihood-free Prompt Tuning\nMaohao Shen Soumya Ghosh Prasanna Sattigeri Subhro Das Yuheng Bu Gregory Wornell\nabstract\rabstract: Due to privacy or commercial constraints, large pre-trained language models(PLMs) are often offered as black-box APIs. Fine-tuning such models todownstream tasks is challenging because one can neither access the model\u0026rsquo;sinternal representations nor propagate gradients through it. This paperaddresses these challenges by developing techniques for adapting PLMs with onlyAPI access. Building on recent work on soft prompt tuning, we develop methodsto tune the soft prompts without requiring gradient computation. Further, wedevelop extensions that in addition to not requiring gradients also do not needto access any internal representation of the PLM beyond the input embeddings.Moreover, instead of learning a single prompt, our methods learn a distributionover prompts allowing us to quantify predictive uncertainty. Ours is the firstwork to consider uncertainty in prompts when only having API access to the PLM.Finally, through extensive experiments, we carefully vet the proposed methodsand find them competitive with (and sometimes even improving on) gradient-basedapproaches with full access to the PLM.\r2023-04-27\nLongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation\nPetra Galuščáková Romain Deveaud Gabriela Gonzalez-Saez Philippe Mulhem Lorraine Goeuriot Florina Piroi Martin Popel\nabstract\rabstract: LongEval-Retrieval is a Web document retrieval benchmark that focuses oncontinuous retrieval evaluation. This test collection is intended to be used tostudy the temporal persistence of Information Retrieval systems and will beused as the test collection in the Longitudinal Evaluation of Model PerformanceTrack (LongEval) at CLEF 2023. This benchmark simulates an evolving informationsystem environment - such as the one a Web search engine operates in - wherethe document collection, the query distribution, and relevance all movecontinuously, while following the Cranfield paradigm for offline evaluation. Todo that, we introduce the concept of a dynamic test collection that is composedof successive sub-collections each representing the state of an informationsystem at a given time step. In LongEval-Retrieval, each sub-collectioncontains a set of queries, documents, and soft relevance assessments built fromclick models. The data comes from Qwant, a privacy-preserving Web search enginethat primarily focuses on the French market. LongEval-Retrieval also provides a\u0026rsquo;mirror\u0026rsquo; collection: it is initially constructed in the French language tobenefit from the majority of Qwant\u0026rsquo;s traffic, before being translated toEnglish. This paper presents the creation process of LongEval-Retrieval andprovides baseline runs and analysis.\r2023-04-25\nTABLET: Learning From Instructions For Tabular Data\nDylan Slack Sameer Singh\nabstract\rabstract: Acquiring high-quality data is often a significant challenge in trainingmachine learning (ML) models for tabular prediction, particularly inprivacy-sensitive and costly domains like medicine and finance. Providingnatural language instructions to large language models (LLMs) offers analternative solution. However, it is unclear how effectively instructionsleverage the knowledge in LLMs for solving tabular prediction problems. Toaddress this gap, we introduce TABLET, a benchmark of 20 diverse tabulardatasets annotated with instructions that vary in their phrasing, granularity,and technicality. Additionally, TABLET includes the instructions\u0026rsquo; logic andstructured modifications to the instructions. We find in-context instructionsincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% forChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabularprediction in our benchmark by evaluating instruction faithfulness. We findLLMs often ignore instructions and fail to predict specific instancescorrectly, even with examples. Our analysis on TABLET shows that, whileinstructions help LLM performance, learning from instructions for tabular datarequires new capabilities.\r2023-04-24\nPARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis\nShu Wei Nuo Xu\nabstract\rabstract: Document layout analysis has a wide range of requirements across variousdomains, languages, and business scenarios. However, most currentstate-of-the-art algorithms are language-dependent, with architectures thatrely on transformer encoders or language-specific text encoders, such as BERT,for feature extraction. These approaches are limited in their ability to handlevery long documents due to input sequence length constraints and are closelytied to language-specific tokenizers. Additionally, training a cross-languagetext encoder can be challenging due to the lack of labeled multilingualdocument datasets that consider privacy. Furthermore, some layout tasks requirea clean separation between different layout components without overlap, whichcan be difficult for image segmentation-based algorithms to achieve. In thispaper, we present Paragraph2Graph, a language-independent graph neural network(GNN)-based model that achieves competitive results on common document layoutdatasets while being adaptable to business scenarios with strict separation.With only 19.95 million parameters, our model is suitable for industrialapplications, particularly in multi-language scenarios.\r2023-04-23\nAnalyzing Leakage of Personally Identifiable Information in Language Models\nNils Lukas Ahmed Salem Robert Sim Shruti Tople Lukas Wutschitz Santiago Zanella-Béguelin\nabstract\rabstract: Language Models (LMs) have been shown to leak information about training datathrough sentence-level membership inference and reconstruction attacks.Understanding the risk of LMs leaking Personally Identifiable Information (PII)has received less attention, which can be attributed to the false assumptionthat dataset curation techniques such as scrubbing are sufficient to preventPII leakage. Scrubbing techniques reduce but do not prevent the risk of PIIleakage: in practice scrubbing is imperfect and must balance the trade-offbetween minimizing disclosure and preserving the utility of the dataset. On theother hand, it is unclear to which extent algorithmic defenses such asdifferential privacy, designed to guarantee sentence- or user-level privacy,prevent PII disclosure. In this work, we introduce rigorous game-baseddefinitions for three types of PII leakage via black-box extraction, inference,and reconstruction attacks with only API access to an LM. We empiricallyevaluate the attacks against GPT-2 models fine-tuned with and without defensesin three domains: case law, health care, and e-mails. Our main contributionsare (i) novel attacks that can extract up to 10$\\times$ more PII sequences thanexisting attacks, (ii) showing that sentence-level differential privacy reducesthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)a subtle connection between record-level membership inference and PIIreconstruction. Code to reproduce all experiments in the paper is available athttps://github.com/microsoft/analysing_pii_leakage.\r2023-04-22\nRetrieval Enhanced Data Augmentation for Question Answering on Privacy Policies\nMd Rizwan Parvez Jianfeng Chi Wasi Uddin Ahmad Yuan Tian Kai-Wei Chang\nabstract\rabstract: Prior studies in privacy policies frame the question answering (QA) task asidentifying the most relevant text segment or a list of sentences from a policydocument given a user query. Existing labeled datasets are heavily imbalanced(only a few relevant segments), limiting the QA performance in this domain. Inthis paper, we develop a data augmentation framework based on ensemblingretriever models that captures the relevant text segments from unlabeled policydocuments and expand the positive examples in the training set. In addition, toimprove the diversity and quality of the augmented data, we leverage multiplepre-trained language models (LMs) and cascade them with noise reduction filtermodels. Using our augmented data on the PrivacyQA benchmark, we elevate theexisting baseline by a large margin (10% F1) and achieve a newstate-of-the-art F1 score of 50%. Our ablation studies provide furtherinsights into the effectiveness of our approach.\r2023-04-21\nA Group-Specific Approach to NLP for Hate Speech Detection\nKarina Halevy\nabstract\rabstract: Automatic hate speech detection is an important yet complex task, requiringknowledge of common sense, stereotypes of protected groups, and histories ofdiscrimination, each of which may constantly evolve. In this paper, we proposea group-specific approach to NLP for online hate speech detection. The approachconsists of creating and infusing historical and linguistic knowledge about aparticular protected group into hate speech detection models, analyzinghistorical data about discrimination against a protected group to betterpredict spikes in hate speech against that group, and critically evaluatinghate speech detection models through lenses of intersectionality and ethics. Wedemonstrate this approach through a case study on NLP for detection ofantisemitic hate speech. The case study synthesizes the currentEnglish-language literature on NLP for antisemitism detection, introduces anovel knowledge graph of antisemitic history and language from the 20th centuryto the present, infuses information from the knowledge graph into a set oftweets over Logistic Regression and uncased DistilBERT baselines, and suggeststhat incorporating context from the knowledge graph can help models pick upsubtle stereotypes.\r2023-04-20\nOn the Independence of Association Bias and Empirical Fairness in Language Models\nLaura Cabello Anna Katrine Jørgensen Anders Søgaard\nabstract\rabstract: The societal impact of pre-trained language models has prompted researchersto probe them for strong associations between protected attributes andvalue-loaded terms, from slur to prestigious job titles. Such work is said toprobe models for bias or fairness-or such probes \u0026lsquo;into representational biases\u0026rsquo;are said to be \u0026lsquo;motivated by fairness\u0026rsquo;-suggesting an intimate connectionbetween bias and fairness. We provide conceptual clarity by distinguishingbetween association biases (Caliskan et al., 2022) and empirical fairness (Shenet al., 2022) and show the two can be independent. Our main contribution,however, is showing why this should not come as a surprise. To this end, wefirst provide a thought experiment, showing how association bias and empiricalfairness can be completely orthogonal. Next, we provide empirical evidence thatthere is no correlation between bias metrics and fairness metrics across themost widely used language models. Finally, we survey the sociological andpsychological literature and show how this literature provides ample supportfor expecting these metrics to be uncorrelated.\r2023-04-19\nCatch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers\nAishwarya Deep Shukla Laksh Agarwal Jie Mein Goh Guodong Gao Ritu Agarwal\nabstract\rabstract: The proliferation of fake reviews of doctors has potentially detrimentalconsequences for patient well-being and has prompted concern among consumerprotection groups and regulatory bodies. Yet despite significant advancementsin the fields of machine learning and natural language processing, thereremains limited comprehension of the characteristics differentiating fraudulentfrom authentic reviews. This study utilizes a novel pre-labeled dataset of38048 physician reviews to establish the effectiveness of large language modelsin classifying reviews. Specifically, we compare the performance of traditionalML models, such as logistic regression and support vector machines, togenerative pre-trained transformer models. Furthermore, we use GPT4, the newestmodel in the GPT family, to uncover the key dimensions along which fake andgenuine physician reviews differ. Our findings reveal significantly superiorperformance of GPT-3 over traditional ML models in this context. Additionally,our analysis suggests that GPT3 requires a smaller training sample thantraditional models, suggesting its appropriateness for tasks with scarcetraining data. Moreover, the superiority of GPT3 performance increases in thecold start context i.e., when there are no prior reviews of a doctor. Finally,we employ GPT4 to reveal the crucial dimensions that distinguish fake physicianreviews. In sharp contrast to previous findings in the literature that wereobtained using simulated data, our findings from a real-world dataset show thatfake reviews are generally more clinically detailed, more reserved insentiment, and have better structure and grammar than authentic ones.\rMaybenot: A Framework for Traffic Analysis Defenses\nTobias Pulls\nabstract\rabstract: End-to-end encryption is a powerful tool for protecting the privacy ofInternet users. Together with the increasing use of technologies such as Tor,VPNs, and encrypted messaging, it is becoming increasingly difficult fornetwork adversaries to monitor and censor Internet traffic. One remainingavenue for adversaries is traffic analysis: the analysis of patterns inencrypted traffic to infer information about the users and their activities.Recent improvements using deep learning have made traffic analysis attacks moreeffective than ever before. We present Maybenot, a framework for traffic analysis defenses. Maybenot isdesigned to be easy to use and integrate into existing end-to-end encryptedprotocols. It is implemented in the Rust programming language as a crate(library), together with a simulator to further the development of defenses.Defenses in Maybenot are expressed as probabilistic state machines thatschedule actions to inject padding or block outgoing traffic. Maybenot is anevolution from the Tor Circuit Padding Framework by Perry and Kadianakis,designed to support a wide range of protocols and use cases.\rRevitalizing Endangered Languages: AI-powered language learning as a catalyst for language appreciation\nDinesh Kumar Nanduri Elizabeth M. Bonsignore\nabstract\rabstract: According to UNESCO, there are nearly 7,000 languages spoken worldwide, ofwhich around 3,000 languages are in danger of disappearing before the end ofthe century. With roughly 230 languages having already become extinct betweenthe years 1950-2010, collectively this represents a significant loss oflinguistic and cultural diversity. This position paper aims to explore thepotential of AI-based language learning approaches that promote early exposureand appreciation of languages to ultimately contribute to the preservation ofendangered languages, thereby addressing the urgent need to protect linguisticand cultural diversity.\r2023-04-18\nA Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese\nHugo Sousa Arian Pasquali Alípio Jorge Catarina Sousa Santos Mário Amorim Lopes\nabstract\rabstract: Textual health records of cancer patients are usually protracted and highlyunstructured, making it very time-consuming for health professionals to get acomplete overview of the patient\u0026rsquo;s therapeutic course. As such limitations canlead to suboptimal and/or inefficient treatment procedures, healthcareproviders would greatly benefit from a system that effectively summarizes theinformation of those records. With the advent of deep neural models, thisobjective has been partially attained for English clinical texts, however, theresearch community still lacks an effective solution for languages with limitedresources. In this paper, we present the approach we developed to extractprocedures, drugs, and diseases from oncology health records written inEuropean Portuguese. This project was conducted in collaboration with thePortuguese Institute for Oncology which, besides holding over $10$ years ofduly protected medical records, also provided oncologist expertise throughoutthe development of the project. Since there is no annotated corpus forbiomedical entity extraction in Portuguese, we also present the strategy wefollowed in annotating the corpus for the development of the models. The finalmodels, which combined a neural architecture with entity linking, achieved$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extractionof procedures, drugs, and diseases, respectively.\rOrder-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models\nJiawei Liu Yangyang Kang Di Tang Kaisong Song Changlong Sun Xiaofeng Wang Wei Lu Xiaozhong Liu\nabstract\rabstract: Neural text ranking models have witnessed significant advancement and areincreasingly being deployed in practice. Unfortunately, they also inheritadversarial vulnerabilities of general neural models, which have been detectedbut remain underexplored by prior studies. Moreover, the inherit adversarialvulnerabilities might be leveraged by blackhat SEO to defeat better-protectedsearch engines. In this study, we propose an imitation adversarial attack onblack-box neural passage ranking models. We first show that the target passageranking model can be transparentized and imitated by enumerating criticalqueries/candidates and then train a ranking imitation model. Leveraging theranking imitation model, we can elaborately manipulate the ranking results andtransfer the manipulation attack to the target ranking model. For this purpose,we propose an innovative gradient-based attack method, empowered by thepairwise objective function, to generate adversarial triggers, which causespremeditated disorderliness with very few tokens. To equip the triggercamouflages, we add the next sentence prediction loss and the language modelfluency constraint to the objective function. Experimental results on passageranking demonstrate the effectiveness of the ranking imitation attack model andadversarial triggers against various SOTA neural ranking models. Furthermore,various mitigation analyses and human evaluation show the effectiveness ofcamouflages when facing potential mitigation approaches. To motivate otherscholars to further investigate this novel and important problem, we make theexperiment data and code publicly available.\rFedTP: Federated Learning by Transformer Personalization\nHongxia Li Zhongyi Cai Jingya Wang Jiangnan Tang Weiping Ding Chin-Teng Lin Ye Shi\nabstract\rabstract: Federated learning is an emerging learning paradigm where multiple clientscollaboratively train a machine learning model in a privacy-preserving manner.Personalized federated learning extends this paradigm to overcome heterogeneityacross clients by learning personalized models. Recently, there have been someinitial attempts to apply Transformers to federated learning. However, theimpacts of federated learning algorithms on self-attention have not yet beenstudied. This paper investigates this relationship and reveals that federatedaveraging algorithms actually have a negative impact on self-attention wherethere is data heterogeneity. These impacts limit the capabilities of theTransformer model in federated learning settings. Based on this, we proposeFedTP, a novel Transformer-based federated learning framework that learnspersonalized self-attention for each client while aggregating the otherparameters among the clients. Instead of using a vanilla personalizationmechanism that maintains personalized self-attention layers of each clientlocally, we develop a learn-to-personalize mechanism to further encourage thecooperation among clients and to increase the scablability and generalizationof FedTP. Specifically, the learn-to-personalize is realized by learning ahypernetwork on the server that outputs the personalized projection matrices ofself-attention layers to generate client-wise queries, keys and values.Furthermore, we present the generalization bound for FedTP with thelearn-to-personalize mechanism. Notably, FedTP offers a convenient environmentfor performing a range of image and language tasks using the same federatednetwork architecture - all of which benefit from Transformer personalization.Extensive experiments verify that FedTP with the learn-to-personalize mechanismyields state-of-the-art performance in non-IID scenarios. Our code is availableonline.\r2023-04-17\nTraining Automated Defense Strategies Using Graph-based Cyber Attack Simulations\nJakob Nyberg Pontus Johnson\nabstract\rabstract: We implemented and evaluated an automated cyber defense agent. The agenttakes security alerts as input and uses reinforcement learning to learn apolicy for executing predefined defensive measures. The defender policies weretrained in an environment intended to simulate a cyber attack. In thesimulation, an attacking agent attempts to capture targets in the environment,while the defender attempts to protect them by enabling defenses. Theenvironment was modeled using attack graphs based on the Meta Attack Languagelanguage. We assumed that defensive measures have downtime costs, meaning thatthe defender agent was penalized for using them. We also assumed that theenvironment was equipped with an imperfect intrusion detection system thatoccasionally produces erroneous alerts based on the environment state. Toevaluate the setup, we trained the defensive agent with different volumes ofintrusion detection system noise. We also trained agents with differentattacker strategies and graph sizes. In experiments, the defensive agent usingpolicies trained with reinforcement learning outperformed agents usingheuristic policies. Experiments also demonstrated that the policies couldgeneralize across different attacker strategies. However, the performance ofthe learned policies decreased as the attack graphs increased in size.\r2023-04-15\nDoes Prompt-Tuning Language Model Ensure Privacy?\nShangyu Xie Wei Dai Esha Ghosh Sambuddha Roy Dan Schwartz Kim Laine\nabstract\rabstract: Prompt-tuning has received attention as an efficient tuning method in thelanguage domain, i.e., tuning a prompt that is a few tokens long, while keepingthe large language model frozen, yet achieving comparable performance withconventional fine-tuning. Considering the emerging privacy concerns withlanguage models, we initiate the study of privacy leakage in the setting ofprompt-tuning. We first describe a real-world email service pipeline to providecustomized output for various users via prompt-tuning. Then we propose a novelprivacy attack framework to infer users\u0026rsquo; private information by exploiting theprompt module with user-specific signals. We conduct a comprehensive privacyevaluation on the target pipeline to demonstrate the potential leakage fromprompt-tuning. The results also demonstrate the effectiveness of the proposedattack.\rUnderstanding Developers Privacy Concerns Through Reddit Thread Analysis\nJonathan Parsons Michael Schrider Oyebanjo Ogunlela Sepideh Ghanavati\nabstract\rabstract: With the growing global emphasis on regulating the protection of personalinformation and increasing user expectation of the same, developing withprivacy in mind is becoming ever more important. In this paper, we study theconcerns, questions, and solutions developers discuss on Reddit forums toenhance our understanding of their perceptions and challenges while developingapplications in the current privacy-focused world. We perform various forms ofNatural Language Processing (NLP) on 437,317 threads from subreddits such asr/webdev, r/androiddev, and r/iOSProgramming to identify both common points ofdiscussion and how these points change over time as new regulations are passedaround the globe. Our results show that there are common trends in privacytopics among the different subreddits while the frequency of those topicsdiffers between web and mobile applications.\r2023-04-14\nClassification of social media Toxic comments using Machine learning models\nK. Poojitha A. Sai Charish M. Arun Kuamr Reddy S. Ayyasamy\nabstract\rabstract: The abstract outlines the problem of toxic comments on social mediaplatforms, where individuals use disrespectful, abusive, and unreasonablelanguage that can drive users away from discussions. This behavior is referredto as anti-social behavior, which occurs during online debates, comments, andfights. The comments containing explicit language can be classified intovarious categories, such as toxic, severe toxic, obscene, threat, insult, andidentity hate. This behavior leads to online harassment and cyberbullying,which forces individuals to stop expressing their opinions and ideas. Toprotect users from offensive language, companies have started flagging commentsand blocking users. The abstract proposes to create a classifier using anLstm-cnn model that can differentiate between toxic and non-toxic comments withhigh accuracy. The classifier can help organizations examine the toxicity ofthe comment section better.\r2023-04-13\nChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review\nSunder Ali Khowaja Parus Khuwaja Kapal Dev\nabstract\rabstract: ChatGPT is another large language model (LLM) inline but due to itsperformance and ability to converse effectively, it has gained a hugepopularity amongst research as well as industrial community. Recently, manystudies have been published to show the effectiveness, efficiency, integration,and sentiments of chatGPT and other LLMs. In contrast, this study focuses onthe important aspects that are mostly overlooked, i.e. sustainability, privacy,digital divide, and ethics and suggests that not only chatGPT but everysubsequent entry in the category of conversational bots should undergoSustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. Thispaper discusses in detail about the issues and concerns raised over chatGPT inline with aforementioned characteristics. We support our hypothesis by somepreliminary data collection and visualizations along with hypothesized facts.We also suggest mitigations and recommendations for each of the concerns.Furthermore, we also suggest some policies and recommendations for AI policyact, if designed by the governments.\r2023-04-12\nUnleashing ChatGPT on the Metaverse: Savior or Destroyer?\nPengyuan Zhou\nabstract\rabstract: The incorporation of artificial intelligence (AI) technology, and inparticular natural language processing (NLP), is becoming increasingly vitalfor the development of immersive and interactive metaverse experiences. Onesuch artificial intelligence tool that is gaining traction in the metaverse isChatGPT, a large language model trained by OpenAI. The article delves into thepros and cons of utilizing ChatGPT for metaverse-based education,entertainment, personalization, and support. Dynamic and personalizedexperiences are possible with this technology, but there are also legitimateprivacy, bias, and ethical issues to consider. This article aims to helpreaders understand the possible influence of ChatGPT on the metaverse and howit may be used to effectively create a more immersive and engaging virtualenvironment by evaluating these opportunities and obstacles.\rFALQU: Finding Answers to Legal Questions\nBehrooz Mansouri Ricardo Campos\nabstract\rabstract: This paper presents a new test collection for Legal IR, FALQU: FindingAnswers to Legal Questions, where questions and answers were obtained from LawStack Exchange (LawSE), a Q\u0026amp;A website for legal professionals, and others withexperience in law. Much in line with Stack overflow, Law Stack Exchange has avariety of questions on different topics such as copyright, intellectualproperty, and criminal laws, making it an interesting source for datasetconstruction. Questions are also not limited to one country. Often, users ofdifferent nationalities may ask questions about laws in different countries andexpertise. Therefore, questions in FALQU represent real-world users\u0026rsquo;information needs thus helping to avoid lab-generated questions. Answers on theother side are given by experts in the field. FALQU is the first testcollection, to the best of our knowledge, to use LawSE, considering morediverse questions than the questions from the standard legal bar and judicialexams. It contains 9880 questions and 34,145 answers to legal questions.Alongside our new test collection, we provide different baseline systems thatinclude traditional information retrieval models such as TF-IDF and BM25, anddeep neural network search models. The results obtained from the BM25 modelachieved the highest effectiveness.\r2023-04-10\nDeterministic constant-depth preparation of the AKLT state on a quantum processor using fusion measurements\nKevin C. Smith Eleanor Crane Nathan Wiebe S. M. Girvin\nabstract\rabstract: The ground state of the spin-1 Affleck, Kennedy, Lieb and Tasaki (AKLT) modelis a paradigmatic example of both a matrix product state and asymmetry-protected topological phase, and additionally holds promise as aresource state for measurement-based quantum computation. Having a nonzerocorrelation length, the AKLT state cannot be exactly prepared by aconstant-depth unitary circuit composed of local gates. In this work, wedemonstrate that this no-go limit can be evaded by augmenting a constant-depthcircuit with fusion measurements, such that the total preparation time isindependent of system size and entirely deterministic. We elucidate ourpreparation scheme using the language of tensor networks, and furthermore showthat the $\\mathbb{Z}_2\\times\\mathbb{Z}_2$ symmetry of the AKLT state directlyaffords this speed-up over previously known preparation methods. To demonstratethe practical advantage of measurement-assisted preparation on noisyintermediate-scale quantum (NISQ) devices, we carry out our protocol on an IBMQuantum processor. We measure both the string order and entanglement spectrumof prepared AKLT chains and, employing these as metrics, find improved resultsover the known (purely unitary) sequential preparation approach. We concludewith a demonstration of quantum teleportation using the AKLT state prepared byour measurement-assisted scheme. This work thus serves to provide an efficientstrategy to prepare a specific resource in the form of the AKLT state and, morebroadly, experimentally demonstrates the possibility for realizable improvementin state preparation afforded by measurement-based circuit depth reductionstrategies on NISQ-era devices.\rDoes Synthetic Data Generation of LLMs Help Clinical Text Mining?\nRuixiang Tang Xiaotian Han Xiaoqian Jiang Xia Hu\nabstract\rabstract: Recent advancements in large language models (LLMs) have led to thedevelopment of highly potent models like OpenAI\u0026rsquo;s ChatGPT. These models haveexhibited exceptional performance in a variety of tasks, such as questionanswering, essay composition, and code generation. However, their effectivenessin the healthcare sector remains uncertain. In this study, we seek toinvestigate the potential of ChatGPT to aid in clinical text mining byexamining its ability to extract structured information from unstructuredhealthcare texts, with a focus on biological named entity recognition andrelation extraction. However, our preliminary results indicate that employingChatGPT directly for these tasks resulted in poor performance and raisedprivacy concerns associated with uploading patients\u0026rsquo; information to the ChatGPTAPI. To overcome these limitations, we propose a new training paradigm thatinvolves generating a vast quantity of high-quality synthetic data with labelsutilizing ChatGPT and fine-tuning a local model for the downstream task. Ourmethod has resulted in significant improvements in the performance ofdownstream tasks, improving the F1-score from 23.37% to 63.99% for the namedentity recognition task and from 75.86% to 83.59% for the relation extractiontask. Furthermore, generating data using ChatGPT can significantly reduce thetime and effort required for data collection and labeling, as well as mitigatedata privacy concerns. In summary, the proposed framework presents a promisingsolution to enhance the applicability of LLM models to clinical text mining.\rAssessing the Socio-economic Impacts of Secure Texting and Anti-Jamming Technologies in Non-Cooperative Networks\nOsoro B Ogutu Edward J Oughton Kai Zeng Brian L. Mark\nabstract\rabstract: Operating securely over 5G (and legacy) infrastructure is a challenge. Innon-cooperative networks, malicious actors may try to decipher, block encryptedmessages, or specifically jam wireless radio systems. Such activities candisrupt operations, from causing minor inconvenience, through to fullyparalyzing the functionality of critical infrastructure. While technologicalmitigation measures do exist, there are very few methods capable of assessingthe socio-economic impacts from different mitigation strategies. This leads toa lack of robust evidence to inform cost-benefit analysis, and thus supportdecision makers in industry and government. Consequently, this paper presentstwo open-source simulation models for assessing the socio-economic impacts ofoperating in untrusted non-cooperative networks. The first focuses on usingmultiple non-cooperative networks to transmit a message. The second modelsimulates a case where a message is converted into alternative plain languageto avoid detection, separated into different portions and then transmitted overmultiple non-cooperative networks. A probabilistic simulation of the two modelsis performed for a 15 km by 15 km spatial grid with 5 untrusted non-cooperativenetworks and intercepting agents. The results are used to estimate economiclosses for private, commercial, government and military sectors. The highestprobabilistic total losses for military applications include US$300, US$150,and US$75, incurred for a 1, 3 and 5 site multi-transmission approach,respectively, for non-cooperative networks when considering 1,000 texts beingsent. These results form a framework for deterministic socio-economic impactanalysis of using non-cooperative networks and secure texting as protectionagainst radio network attacks. The simulation data and the open-source codebaseis provided for reproducibility.\r2023-04-07\nComplex QA and language models hybrid architectures, Survey\nXavier Daull Patrice Bellot Emmanuel Bruno Vincent Martin Elisabeth Murisasco\nabstract\rabstract: This paper reviews the state-of-the-art of language models architectures andstrategies for \u0026ldquo;complex\u0026rdquo; question-answering (QA, CQA, CPS) with a focus onhybridization. Large Language Models (LLM) are good at leveraging public dataon standard problems but once you want to tackle more specific complexquestions or problems (e.g. How does the concept of personal freedom varybetween different cultures ? What is the best mix of power generation methodsto reduce climate change ?) you may need specific architecture, knowledge,skills, methods, sensitive data protection, explainability, human approval andversatile feedback\u0026hellip; Recent projects like ChatGPT and GALACTICA have allowednon-specialists to grasp the great potential as well as the equally stronglimitations of LLM in complex QA. In this paper, we start by reviewing requiredskills and evaluation techniques. We integrate findings from the robustcommunity edited research papers BIG, BLOOM and HELM which open source,benchmark and analyze limits and challenges of LLM in terms of tasks complexityand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, \u0026hellip;) asa baseline. We discuss some challenges associated with complex QA, includingdomain adaptation, decomposition and efficient multi-step QA, long form andnon-factoid QA, safety and multi-sensitivity data protection, multimodalsearch, hallucinations, explainability and truthfulness, temporal reasoning. Weanalyze current solutions and promising research trends, using elements suchas: hybrid LLM architectural patterns, training and prompting strategies,active human reinforcement learning supervised with AI, neuro-symbolic andstructured knowledge grounding, program synthesis, iterated decomposition andothers.\r2023-04-06\nWhose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics\nMadiha Zahrah Choksi David Goedicke\nabstract\rabstract: Intelligent or generative writing tools rely on large language models thatrecognize, summarize, translate, and predict content. This position paperprobes the copyright interests of open data sets used to train large languagemodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumventthe copyright interests of the used data? We start by defining softwarecopyright and tracing its history. We rely on GitHub Copilot as a modern casestudy challenging software copyright. Our conclusion outlines obstacles thatgenerative writing assistants create for copyright, and offers a practical roadmap for copyright analysis for developers, software law experts, and generalusers to consider in the context of intelligent LLM-powered writing tools.\r2023-04-05\nThe Saudi Privacy Policy Dataset\nHend Al-Khalifa Malak Mashaabi Ghadi Al-Yahya Raghad Alnashwan\nabstract\rabstract: This paper introduces the Saudi Privacy Policy Dataset, a diverse compilationof Arabic privacy policies from various sectors in Saudi Arabia, annotatedaccording to the 10 principles of the Personal Data Protection Law (PDPL); thePDPL was established to be compatible with General Data Protection Regulation(GDPR); one of the most comprehensive data regulations worldwide. Data werecollected from multiple sources, including the Saudi Central Bank, the SaudiArabia National United Platform, the Council of Health Insurance, and generalwebsites using Google and Wikipedia. The final dataset includes 1,000 websitesbelonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus sizeof 8,353 KB. The annotated dataset offers significant reuse potential forassessing privacy policy compliance, benchmarking privacy practices acrossindustries, and developing automated tools for monitoring adherence to dataprotection regulations. By providing a comprehensive and annotated dataset ofprivacy policies, this paper aims to facilitate further research anddevelopment in the areas of privacy policy analysis, natural languageprocessing, and machine learning applications related to privacy and dataprotection, while also serving as an essential resource for researchers,policymakers, and industry professionals interested in understanding andpromoting compliance with privacy regulations in Saudi Arabia.\rBengali Fake Review Detection using Semi-supervised Generative Adversarial Networks\nMd. Tanvir Rouf Shawon G. M. Shahariar Faisal Muhammad Shah Mohammad Shafiul Alam Md. Shahriar Mahbub\nabstract\rabstract: This paper investigates the potential of semi-supervised GenerativeAdversarial Networks (GANs) to fine-tune pretrained language models in order toclassify Bengali fake reviews from real reviews with a few annotated data. Withthe rise of social media and e-commerce, the ability to detect fake ordeceptive reviews is becoming increasingly important in order to protectconsumers from being misled by false information. Any machine learning modelwill have trouble identifying a fake review, especially for a low resourcelanguage like Bengali. We have demonstrated that the proposed semi-supervisedGAN-LM architecture (generative adversarial network on top of a pretrainedlanguage model) is a viable solution in classifying Bengali fake reviews as theexperimental results suggest that even with only 1024 annotated samples,BanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% anda f1-score of 84.89% outperforming other pretrained language models -BanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and10% respectively in terms of accuracy. The experiments were conducted on amanually labeled food review dataset consisting of total 6014 real and fakereviews collected from various social media groups. Researchers that areexperiencing difficulty recognizing not just fake reviews but otherclassification issues owing to a lack of labeled data may find a solution inour proposed methodology.\r2023-04-03\nROPfuscator: Robust Obfuscation with ROP\nGiulio De Pasquale Fukutomo Nakanishi Daniele Ferla Lorenzo Cavallaro\nabstract\rabstract: Software obfuscation plays a crucial role in protecting intellectual propertyin software from reverse engineering attempts. While some obfuscationtechniques originate from the obfuscation-reverse engineering arms race, othersstem from different research areas, such as binary software exploitation.Return-oriented programming (ROP) gained popularity as one of the mosteffective exploitation techniques for memory error vulnerabilities. ROPinterferes with our natural perception of a process control flow, inspiring usto repurpose ROP as a robust and effective form of software obfuscation.Although previous work already explores ROP\u0026rsquo;s effectiveness as an obfuscationtechnique, evolving reverse engineering research raises the need for principledreasoning to understand the strengths and limitations of ROP-based mechanismsagainst man-at-the-end (MATE) attacks. To this end, we present ROPfuscator, acompiler-driven obfuscation pass based on ROP for any programming languagesupported by LLVM. We incorporate opaque predicates and constants and a novelinstruction hiding technique to withstand sophisticated MATE attacks. Moreimportantly, we introduce a realistic and unified threat model to thoroughlyevaluate ROPfuscator and provide principled reasoning on ROP-based obfuscationtechniques that answer to code coverage, incurred overhead, correctness,robustness, and practicality challenges.\r2023-04-01\nWhen Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus\nWon Ik Cho Yoon Kyung Lee Seoyeon Bae Jihwan Kim Sangah Park Moosung Kim Sowon Hahn Nam Soo Kim\nabstract\rabstract: Building a natural language dataset requires caution since word semantics isvulnerable to subtle text change or the definition of the annotated concept.Such a tendency can be seen in generative tasks like question-answering anddialogue generation and also in tasks that create a categorization-basedcorpus, like topic classification or sentiment analysis. Open-domainconversations involve two or more crowdworkers freely conversing about anytopic, and collecting such data is particularly difficult for two reasons: 1)the dataset should be crafted\u0026quot; rather than obtained\u0026quot; due to privacyconcerns, and 2) paid creation of such dialogues may differ from howcrowdworkers behave in real-world settings. In this study, we tackle theseissues when creating a large-scale open-domain persona dialogue corpus, wherepersona implies that the conversation is performed by several actors with afixed persona and user-side workers from an unspecified crowd.\r2023-03-30\nA CI-based Auditing Framework for Data Collection Practices\nAthina Markopoulou Rahmadi Trimananda Hao Cui\nabstract\rabstract: Apps and devices (mobile devices, web browsers, IoT, VR, voice assistants,etc.) routinely collect user data, and send them to first- and third-partyservers through the network. Recently, there is a lot of interest in (1)auditing the actual data collection practices of those systems; and also in (2)checking the consistency of those practices against the statements made in thecorresponding privacy policies. In this paper, we argue that the contextualintegrity (CI) tuple can be the basic building block for defining andimplementing such an auditing framework. We elaborate on the special case wherethe tuple is partially extracted from the network traffic generated by theend-device of interest, and partially from the corresponding privacy policiesusing natural language processing (NLP) techniques. Along the way, we discussrelated bodies of work and representative examples that fit into thatframework. More generally, we believe that CI can be the building block notonly for auditing at the edge, but also for specifying privacy policies andsystem APIs. We also discuss limitations and directions for future work.\rConStruct-VL: Data-Free Continual Structured VL Concepts Learning\nJames Seale Smith Paola Cascante-Bonilla Assaf Arbelle Donghyun Kim Rameswar Panda David Cox Diyi Yang Zsolt Kira Rogerio Feris Leonid Karlinsky\nabstract\rabstract: Recently, large-scale pre-trained Vision-and-Language (VL) foundation modelshave demonstrated remarkable capabilities in many zero-shot downstream tasks,achieving competitive results for recognizing objects defined by as little asshort text prompts. However, it has also been shown that VL models are stillbrittle in Structured VL Concept (SVLC) reasoning, such as the ability torecognize object attributes, states, and inter-object relations. This leads toreasoning mistakes, which need to be corrected as they occur by teaching VLmodels the missing SVLC skills; often this must be done using private datawhere the issue was found, which naturally leads to a data-free continual (notask-id) VL learning setting. In this work, we introduce the first ContinualData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show itis challenging for many existing data-free CL strategies. We, therefore,propose a data-free method comprised of a new approach of AdversarialPseudo-Replay (APR) which generates adversarial reminders of past tasks frompast task models. To use this method efficiently, we also propose a continualparameter-efficient Layered-LoRA (LaLo) neural architecture allowingno-memory-cost access to all past models at train time. We show this approachoutperforms all data-free methods by as much as ~7% while even matching somelevels of experience-replay (prohibitive for applications where data-privacymust be preserved). Our code is publicly available athttps://github.com/jamessealesmith/ConStruct-VL\rForget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models\nEric Zhang Kai Wang Xingqian Xu Zhangyang Wang Humphrey Shi\nabstract\rabstract: The unlearning problem of deep learning models, once primarily an academicconcern, has become a prevalent issue in the industry. The significant advancesin text-to-image generation techniques have prompted global discussions onprivacy, copyright, and safety, as numerous unauthorized personal IDs, content,artistic creations, and potentially harmful materials have been learned bythese models and later utilized to generate and distribute uncontrolledcontent. To address this challenge, we propose \\textbf{Forget-Me-Not}, anefficient and low-cost solution designed to safely remove specified IDs,objects, or styles from a well-configured text-to-image model in as little as30 seconds, without impairing its ability to generate other content. Alongsideour method, we introduce the \\textbf{Memorization Score (M-Score)} and\\textbf{ConceptBench} to measure the models\u0026rsquo; capacity to generate generalconcepts, grouped into three primary categories: ID, object, and style. UsingM-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectivelyeliminate targeted concepts while maintaining the model\u0026rsquo;s performance on otherconcepts. Furthermore, Forget-Me-Not offers two practical extensions: a)removal of potentially harmful or NSFW content, and b) enhancement of modelaccuracy, inclusion and diversity through \\textbf{concept correction anddisentanglement}. It can also be adapted as a lightweight model patch forStable Diffusion, allowing for concept manipulation and convenientdistribution. To encourage future research in this critical area and promotethe development of safe and inclusive generative models, we will open-sourceour code and ConceptBench at\\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.\r2023-03-28\nSynthetically generated text for supervised text analysis\nAndrew Halterman\nabstract\rabstract: Supervised text models are a valuable tool for political scientists butpresent several obstacles to their use, including the expense of hand-labelingdocuments, the difficulty of retrieving rare relevant documents for annotation,and copyright and privacy concerns involved in sharing annotated documents.This article proposes a partial solution to these three issues, in the form ofcontrolled generation of synthetic text with large language models. I provide aconceptual overview of text generation, guidance on when researchers shouldprefer different techniques for generating synthetic text, a discussion ofethics, and a simple technique for improving the quality of synthetic text. Idemonstrate the usefulness of synthetic text with three applications:generating synthetic tweets describing the fighting in Ukraine, synthetic newsarticles describing specified political events for training an event detectionsystem, and a multilingual corpus of populist manifesto statements for traininga sentence-level populism classifier.\rFoundation Models and Fair Use\nPeter Henderson Xuechen Li Dan Jurafsky Tatsunori Hashimoto Mark A. Lemley Percy Liang\nabstract\rabstract: Existing foundation models are trained on copyrighted material. Deployingthese models can pose both legal and ethical risks when data creators fail toreceive appropriate attribution or compensation. In the United States andseveral other countries, copyrighted content may be used to build foundationmodels without incurring liability due to the fair use doctrine. However, thereis a caveat: If the model produces output that is similar to copyrighted data,particularly in scenarios that affect the market of that data, fair use may nolonger apply to the output of the model. In this work, we emphasize that fairuse is not guaranteed, and additional work may be necessary to keep modeldevelopment and deployment squarely in the realm of fair use. First, we surveythe potential risks of developing and deploying foundation models based oncopyrighted content. We review relevant U.S. case law, drawing parallels toexisting and potential applications for generating text, source code, andvisual art. Experiments confirm that popular foundation models can generatecontent considerably similar to copyrighted material. Second, we discusstechnical mitigations that can help foundation models stay in line with fairuse. We argue that more research is needed to align mitigation strategies withthe current state of the law. Lastly, we suggest that the law and technicalmitigations should co-evolve. For example, coupled with other policymechanisms, the law could more explicitly consider safe harbors when strongtechnical tools are used to mitigate infringement harms. This co-evolution mayhelp strike a balance between intellectual property and innovation, whichspeaks to the original goal of fair use. But we emphasize that the strategieswe describe here are not a panacea and more work is needed to develop policiesthat address the potential harms of foundation models.\r2023-03-27\nUnimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation\nRongyu Zhang Xiaowei Chi Guiliang Liu Wenyi Zhang Yuan Du Fangxin Wang\nabstract\rabstract: Multimodal learning has seen great success mining data features from multiplemodalities with remarkable model performance improvement. Meanwhile, federatedlearning (FL) addresses the data sharing problem, enabling privacy-preservedcollaborative training to provide sufficient precious data. Great potential,therefore, arises with the confluence of them, known as multimodal federatedlearning. However, limitation lies in the predominant approaches as they oftenassume that each local dataset records samples from all modalities. In thispaper, we aim to bridge this gap by proposing an Unimodal Training - MultimodalPrediction (UTMP) framework under the context of multimodal federated learning.We design HA-Fedformer, a novel transformer-based model that empowers unimodaltraining with only a unimodal dataset at the client and multimodal testing byaggregating multiple clients\u0026rsquo; knowledge for better accuracy. The key advantagesare twofold. Firstly, to alleviate the impact of data non-IID, we develop anuncertainty-aware aggregation method for the local encoders with layer-wiseMarkov Chain Monte Carlo sampling. Secondly, to overcome the challenge ofunaligned language sequence, we implement a cross-modal decoder aggregation tocapture the hidden signal correlation between decoders trained by data fromdifferent modalities. Our experiments on popular sentiment analysis benchmarks,CMU-MOSI and CMU-MOSEI, demonstrate that HA-Fedformer significantly outperformsstate-of-the-art multimodal models under the UTMP federated learningframeworks, with 15%-20% improvement on most attributes.\r2023-03-23\nPrimer: Fast Private Transformer Inference on Encrypted Data\nMengxin Zheng Qian Lou Lei Jiang\nabstract\rabstract: It is increasingly important to enable privacy-preserving inference for cloudservices based on Transformers. Post-quantum cryptographic techniques, e.g.,fully homomorphic encryption (FHE), and multi-party computation (MPC), arepopular methods to support private Transformer inference. However, existingworks still suffer from prohibitively computational and communicationaloverhead. In this work, we present, Primer, to enable a fast and accurateTransformer over encrypted data for natural language processing tasks. Inparticular, Primer is constructed by a hybrid cryptographic protocol optimizedfor attention-based Transformer models, as well as techniques includingcomputation merge and tokens-first ciphertext packing. Comprehensiveexperiments on encrypted language modeling show that Primer achievesstate-of-the-art accuracy and reduces the inference latency by 90.6% ~ 97.5%over previous methods.\rDevelopment and validation of a natural language processing algorithm to pseudonymize documents in the context of a clinical data warehouse\nXavier Tannier Perceval Wajsbürt Alice Calliger Basile Dura Alexandre Mouchet Martin Hilka Romain Bey\nabstract\rabstract: The objective of this study is to address the critical issue ofde-identification of clinical reports in order to allow access to data forresearch purposes, while ensuring patient privacy. The study highlights thedifficulties faced in sharing tools and resources in this domain and presentsthe experience of the Greater Paris University Hospitals (AP-HP) inimplementing a systematic pseudonymization of text documents from its ClinicalData Warehouse. We annotated a corpus of clinical documents according to 12types of identifying entities, and built a hybrid system, merging the resultsof a deep learning model as well as manual rules. Our results show an overallperformance of 0.99 of F1-score. We discuss implementation choices and presentexperiments to better understand the effort involved in such a task, includingdataset size, document types, language models, or rule addition. We shareguidelines and code under a 3-Clause BSD license.\r2023-03-22\nMan vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models\nConstantinos Patsakis Nikolaos Lykousas\nabstract\rabstract: The collection and use of personal data are becoming more common in today\u0026rsquo;sdata-driven culture. While there are many advantages to this, including betterdecision-making and service delivery, it also poses significant ethical issuesaround confidentiality and privacy. Text anonymisation tries to prune and/ormask identifiable information from a text while keeping the remaining contentintact to alleviate privacy concerns. Text anonymisation is especiallyimportant in industries like healthcare, law, as well as research, wheresensitive and personal information is collected, processed, and exchanged underhigh legal and ethical standards. Although text anonymization is widely adopted in practice, it continues toface considerable challenges. The most significant challenge is striking abalance between removing information to protect individuals\u0026rsquo; privacy whilemaintaining the text\u0026rsquo;s usability for future purposes. The question is whetherthese anonymisation methods sufficiently reduce the risk of re-identification,in which an individual can be identified based on the remaining information inthe text. In this work, we challenge the effectiveness of these methods and how weperceive identifiers. We assess the efficacy of these methods against theelephant in the room, the use of AI over big data. While most of the researchis focused on identifying and removing personal information, there is limiteddiscussion on whether the remaining information is sufficient to deanonymiseindividuals and, more precisely, who can do it. To this end, we conduct anexperiment using GPT over anonymised texts of famous people to determinewhether such trained networks can deanonymise them. The latter allows us torevise these methods and introduce a novel methodology that employs LargeLanguage Models to improve the anonymity of texts.\r2023-03-20\nGenerative AI and the Digital Commons\nSaffron Huang Divya Siddarth\nabstract\rabstract: Many generative foundation models (or GFMs) are trained on publicly availabledata and use public infrastructure, but 1) may degrade the \u0026ldquo;digital commons\u0026quot;that they depend on, and 2) do not have processes in place to return valuecaptured to data producers and stakeholders. Existing conceptions of datarights and protection (focusing largely on individually-owned data andassociated privacy concerns) and copyright or licensing-based models offer someinstructive priors, but are ill-suited for the issues that may arise frommodels trained on commons-based data. We outline the risks posed by GFMs andwhy they are relevant to the digital commons, and propose numerousgovernance-based solutions that include investments in standardizeddataset/model disclosure and other kinds of transparency when it comes togenerative models\u0026rsquo; training and capabilities, consortia-based funding formonitoring/standards/auditing organizations, requirements or norms for GFMcompanies to contribute high quality data to the commons, and structures forshared ownership based on individual or community provision of fine-tuningdata.\rPrivately Fine-Tuning Large Language Models with Differential Privacy\nRouzbeh Behnia Mohamamdreza Ebrahimi Jason Pacheco Balaji Padmanabhan\nabstract\rabstract: Pre-trained Large Language Models (LLMs) are an integral part of modern AIthat have led to breakthrough performances in complex AI tasks. Major AIcompanies with expensive infrastructures are able to develop and train theselarge models with billions and millions of parameters from scratch. Thirdparties, researchers, and practitioners are increasingly adopting thesepre-trained models and fine-tuning them on their private data to accomplishtheir downstream AI tasks. However, it has been shown that an adversary canextract/reconstruct the exact training samples from these LLMs, which can leadto revealing personally identifiable information. The issue has raised deepconcerns about the privacy of LLMs. Differential privacy (DP) provides arigorous framework that allows adding noise in the process of training orfine-tuning LLMs such that extracting the training data becomes infeasible(i.e., with a cryptographically small success probability). While thetheoretical privacy guarantees offered in most extant studies assume learningmodels from scratch through many training iterations in an asymptotic setting,this assumption does not hold in fine-tuning scenarios in which the number oftraining iterations is significantly smaller. To address the gap, we present\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant withfinite-sample privacy guarantees. Our results across four well-establishednatural language understanding (NLU) tasks show that while \\ewtune~adds privacyguarantees to LLM fine-tuning process, it directly contributes to decreasingthe induced noise to up to 5.6% and improves the state-of-the-art LLMsperformance by up to 1.1% across all NLU tasks. We have open-sourced ourimplementations for wide adoption and public testing purposes.\rBillionCOV: An Enriched Billion-scale Collection of COVID-19 tweets for Efficient Hydration\nRabindra Lamsal Maria Rodriguez Read Shanika Karunasekera\nabstract\rabstract: The COVID-19 pandemic introduced new norms such as social distancing, facemasks, quarantine, lockdowns, travel restrictions, work/study from home, andbusiness closures, to name a few. The pandemic\u0026rsquo;s seriousness made people vocalon social media, especially on microblogs such as Twitter. Researchers havebeen collecting and sharing large-scale datasets of COVID-19 tweets since theearly days of the outbreak. Sharing raw Twitter data with third parties isrestricted; users need to hydrate tweet identifiers in a public dataset tore-create the dataset locally. Large-scale datasets that include originaltweets, retweets, quotes, and replies have tweets in billions which takesmonths to hydrate. The existing datasets carry issues related to proportion andredundancy. We report that more than 500 million tweet identifiers point todeleted or protected tweets. In order to address these issues, this paperintroduces an enriched global billion-scale English-language COVID-19 tweetsdataset, BillionCOV, that contains 1.4 billion tweets originating from 240countries and territories between October 2019 and April 2022. Importantly,BillionCOV facilitates researchers to filter tweet identifiers for efficienthydration. This paper discusses associated methods to fetch raw Twitter datafor a set of tweet identifiers, presents multiple tweets\u0026rsquo; distributions toprovide an overview of BillionCOV, and finally, reviews the dataset\u0026rsquo;s potentialuse cases.\r2023-03-18\nDeAR: Debiasing Vision-Language Models with Additive Residuals\nAshish Seth Mayur Hemani Chirag Agarwal\nabstract\rabstract: Large pre-trained vision-language models (VLMs) reduce the time fordeveloping predictive models for various vision-grounded language downstreamtasks by providing rich, adaptable image and text representations. However,these models suffer from societal biases owing to the skewed distribution ofvarious identity groups in the training data. These biases manifest as theskewed similarity between the representations for specific text concepts andimages of people of different identity groups and, therefore, limit theusefulness of such models in real-world high-stakes applications. In this work,we present DeAR (Debiasing with Additive Residuals), a novel debiasing methodthat learns additive residual image representations to offset the originalrepresentations, ensuring fair output representations. In doing so, it reducesthe ability of the representations to distinguish between the differentidentity groups. Further, we observe that the current fairness tests areperformed on limited face image datasets that fail to indicate why a specifictext concept should/should not apply to them. To bridge this gap and betterevaluate DeAR, we introduce the Protected Attribute Tag Association (PATA)dataset - a new context-based bias benchmarking dataset for evaluating thefairness of large pre-trained VLMs. Additionally, PATA provides visual contextfor a diverse human population in different scenarios with both positive andnegative connotations. Experimental results for fairness and zero-shotperformance preservation using multiple datasets demonstrate the efficacy ofour framework.\rFedRight: An Effective Model Copyright Protection for Federated Learning\nJinyin Chen Mingjun Li Mingjun Li Haibin Zheng\nabstract\rabstract: Federated learning (FL), an effective distributed machine learning framework,implements model training and meanwhile protects local data privacy. It hasbeen applied to a broad variety of practice areas due to its great performanceand appreciable profits. Who owns the model, and how to protect the copyrighthas become a real problem. Intuitively, the existing property rights protectionmethods in centralized scenarios (e.g., watermark embedding and modelfingerprints) are possible solutions for FL. But they are still challenged bythe distributed nature of FL in aspects of the no data sharing, parameteraggregation, and federated training settings. For the first time, we formalizethe problem of copyright protection for FL, and propose FedRight to protectmodel copyright based on model fingerprints, i.e., extracting model features bygenerating adversarial examples as model fingerprints. FedRight outperformsprevious works in four key aspects: (i) Validity: it extracts model features togenerate transferable fingerprints to train a detector to verify the copyrightof the model. (ii) Fidelity: it is with imperceptible impact on the federatedtraining, thus promising good main task performance. (iii) Robustness: it isempirically robust against malicious attacks on copyright protection, i.e.,fine-tuning, model pruning, and adaptive attacks. (iv) Black-box: it is validin the black-box forensic scenario where only application programming interfacecalls to the model are available. Extensive evaluations across 3 datasets and 9model structures demonstrate FedRight\u0026rsquo;s superior fidelity, validity, androbustness.\r2023-03-17\nOn the De-duplication of LAION-2B\nRyan Webster Julien Rabin Loic Simon Frederic Jurie\nabstract\rabstract: Generative models, such as DALL-E, Midjourney, and Stable Diffusion, havesocietal implications that extend beyond the field of computer science. Thesemodels require large image databases like LAION-2B, which contain two billionimages. At this scale, manual inspection is difficult and automated analysis ischallenging. In addition, recent studies show that duplicated images posecopyright problems for models trained on LAION2B, which hinders its usability.This paper proposes an algorithmic chain that runs with modest compute, thatcompresses CLIP features to enable efficient duplicate detection, even for vastimage volumes. Our approach demonstrates that roughly 700 million images, orabout 30%, of LAION-2B\u0026rsquo;s images are likely duplicated. Our method alsoprovides the histograms of duplication on this dataset, which we use to revealmore examples of verbatim copies by Stable Diffusion and further justify theapproach. The current version of the de-duplicated set will be distributedonline.\rRethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation\nYifan Yan Xudong Pan Mi Zhang Min Yang\nabstract\rabstract: Copyright protection for deep neural networks (DNNs) is an urgent need for AIcorporations. To trace illegally distributed model copies, DNN watermarking isan emerging technique for embedding and verifying secret identity messages inthe prediction behaviors or the model internals. Sacrificing less functionalityand involving more knowledge about the target DNN, the latter branch called\\textit{white-box DNN watermarking} is believed to be accurate, credible andsecure against most known watermark removal attacks, with emerging researchefforts in both the academy and the industry. In this paper, we present the first systematic study on how the mainstreamwhite-box DNN watermarks are commonly vulnerable to neural structuralobfuscation with \\textit{dummy neurons}, a group of neurons which can be addedto a target model but leave the model behavior invariant. Devising acomprehensive framework to automatically generate and inject dummy neurons withhigh stealthiness, our novel attack intensively modifies the architecture ofthe target model to inhibit the success of watermark verification. Withextensive evaluation, our work for the first time shows that nine publishedwatermarking schemes require amendments to their verification procedures.\r2023-03-16\nA Short Survey of Viewing Large Language Models in Legal Aspect\nZhongxiang Sun\nabstract\rabstract: Large language models (LLMs) have transformed many fields, including naturallanguage processing, computer vision, and reinforcement learning. These modelshave also made a significant impact in the field of law, where they are beingincreasingly utilized to automate various legal tasks, such as legal judgementprediction, legal document analysis, and legal document writing. However, theintegration of LLMs into the legal field has also raised several legalproblems, including privacy concerns, bias, and explainability. In this survey,we explore the integration of LLMs into the field of law. We discuss thevarious applications of LLMs in legal tasks, examine the legal challenges thatarise from their use, and explore the data resources that can be used tospecialize LLMs in the legal domain. Finally, we discuss several promisingdirections and conclude this paper. By doing so, we hope to provide an overviewof the current state of LLMs in law and highlight the potential benefits andchallenges of their integration.\r2023-03-15\nCopyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution\nHaonan Zhong Jiamin Chang Ziyue Yang Tingmin Wu Pathum Chamikara Mahawaga Arachchige Chehara Pathmabandu Minhui Xue\nabstract\rabstract: Generative AI (e.g., Generative Adversarial Networks - GANs) has becomeincreasingly popular in recent years. However, Generative AI introducessignificant concerns regarding the protection of Intellectual Property Rights(IPR) (resp. model accountability) pertaining to images (resp. toxic images)and models (resp. poisoned models) generated. In this paper, we propose anevaluation framework to provide a comprehensive overview of the current stateof the copyright protection measures for GANs, evaluate their performanceacross a diverse range of GAN architectures, and identify the factors thataffect their performance and future research directions. Our findings indicatethat the current IPR protection methods for input images, model watermarking,and attribution networks are largely satisfactory for a wide range of GANs. Wehighlight that further attention must be directed towards protecting trainingsets, as the current approaches fail to provide robust IPR protection andprovenance tracing on training sets.\r2023-03-14\nNavigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning\nYunchao Zhang Zonglin Di Kaiwen Zhou Cihang Xie Xin Eric Wang\nabstract\rabstract: Federated embodied agent learning protects the data privacy of individualvisual environments by keeping data locally at each client (the individualenvironment) during training. However, since the local data is inaccessible tothe server under federated learning, attackers may easily poison the trainingdata of the local client to build a backdoor in the agent without notice.Deploying such an agent raises the risk of potential harm to humans, as theattackers may easily navigate and control the agent as they wish via thebackdoor. Towards Byzantine-robust federated embodied agent learning, in thispaper, we study the attack and defense for the task of vision-and-languagenavigation (VLN), where the agent is required to follow natural languageinstructions to navigate indoor environments. First, we introduce a simple buteffective attack strategy, Navigation as Wish (NAW), in which the maliciousclient manipulates local trajectory data to implant a backdoor into the globalmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easilynavigate the deployed VLN agent regardless of the language instruction, withoutaffecting its performance on normal test sets. Then, we propose a newPrompt-Based Aggregation (PBA) to defend against the NAW attack in federatedVLN, which provides the server with a \u0026lsquo;\u0026lsquo;prompt\u0026rsquo;\u0026rsquo; of the vision-and-languagealignment variance between the benign and malicious clients so that they can bedistinguished during training. We validate the effectiveness of the PBA methodon protecting the global model from the NAW attack, which outperforms otherstate-of-the-art defense methods by a large margin in the defense metrics onR2R and RxR.\r2023-03-11\nContext-based Ontology Modelling for Database: Enabling ChatGPT for Semantic Database Management\nWenjun Lin Paul Babyn Yan yan Wenjun Zhang\nabstract\rabstract: This research paper explores the use of ChatGPT in database management.ChatGPT, an AI-powered chatbot, has limitations in performing tasks related todatabase management due to the lack of standardized vocabulary and grammar forrepresenting database semantics. To address this limitation, the paper proposesa solution that involves developing a set of syntaxes that can representdatabase semantics in natural language. The syntax is used to convert databaseschemas into natural language formats, providing a new application of ChatGPTin database management. The proposed solution is demonstrated through a casestudy where ChatGPT is used to perform two tasks, semantic integration, andtables joining. Results demonstrate that the use of semantic databaserepresentations produces more precise outcomes and avoids common mistakescompared to cases with no semantic representation. The proposed method has thepotential to speed up the database management process, reduce the level ofunderstanding required for database domain knowledge, and enable automaticdatabase operations without accessing the actual data, thus illuminatingprivacy protection concerns when using AI. This paper provides a promising newdirection for research in the field of AI-based database management.\r2023-03-06\nQuantifying Memorization Across Neural Language Models\nNicholas Carlini Daphne Ippolito Matthew Jagielski Katherine Lee Florian Tramer Chiyuan Zhang\nabstract\rabstract: Large language models (LMs) have been shown to memorize parts of theirtraining data, and when prompted appropriately, they will emit the memorizedtraining data verbatim. This is undesirable because memorization violatesprivacy (exposing user data), degrades utility (repeated easy-to-memorize textis often low quality), and hurts fairness (some texts are memorized overothers). We describe three log-linear relationships that quantify the degree to whichLMs emit memorized training data. Memorization significantly grows as weincrease (1) the capacity of a model, (2) the number of times an example hasbeen duplicated, and (3) the number of tokens of context used to prompt themodel. Surprisingly, we find the situation becomes more complicated whengeneralizing these results across model families. On the whole, we find thatmemorization in LMs is more prevalent than previously believed and will likelyget worse as models continues to scale, at least without active mitigations.\rBlockchain-Empowered Lifecycle Management for AI-Generated Content (AIGC) Products in Edge Networks\nYinqiu Liu Hongyang Du Dusit Niyato Jiawen Kang Zehui Xiong Chunyan Miao Xuemin Shen Abbas Jamalipour\nabstract\rabstract: The rapid development of Artificial IntelligenceGenerated Content (AIGC) hasbrought daunting challenges regarding service latency, security, andtrustworthiness. Recently, researchers presented the edge AIGC paradigm,effectively optimize the service latency by distributing AIGC services to edgedevices. However, AIGC products are still unprotected and vulnerable totampering and plagiarization. Moreover, as a kind of online non-fungibledigital property, the free circulation of AIGC products is hindered by the lackof trustworthiness in open networks. In this article, for the first time, wepresent a blockchain-empowered framework to manage the lifecycle of edge AIGCproducts. Specifically, leveraging fraud proof, we first propose a protocol toprotect the ownership and copyright of AIGC, called Proof-of-AIGC. Then, wedesign an incentive mechanism to guarantee the legitimate and timely executionsof the funds-AIGC ownership exchanges among anonymous users. Furthermore, webuild a multi-weight subjective logic-based reputation scheme, with which AIGCproducers can determine which edge service provider is trustworthy and reliableto handle their services. Through numerical results, the superiority of theproposed approach is demonstrated. Last but not least, we discuss importantopen directions for further research.\r2023-03-04\nTopological Quantum Gates in Homotopy Type Theory\nDavid Jaz Myers Hisham Sati Urs Schreiber\nabstract\rabstract: Despite the evident necessity of topological protection for realizingscalable quantum computers, the conceptual underpinnings of topological quantumlogic gates had arguably remained shaky, both regarding their physicalrealization as well as their information-theoretic nature. Building on recent results on defect branes in string/M-theory and on theirholographically dual anyonic defects in condensed matter theory, here weexplain how the specification of realistic topological quantum gates, operatingby anyon defect braiding in topologically ordered quantum materials, has asurprisingly slick formulation in parameterized point-set topology, which is sofundamental that it lends itself to certification in modern homotopically typedprogramming languages, such as cubical Agda. We propose that this remarkable confluence of concepts may jointly kickstartthe development of topological quantum programming proper as well as ofreal-world application of homotopy type theory, both of which have arguablybeen falling behind their high expectations; in any case, it provides apowerful paradigm for simulating and verifying topological quantum computingarchitectures with high-level certification languages aware of the actualphysical principles of realistic topological quantum hardware. In a companion article, we will explain how further passage to \u0026ldquo;dependentlinear\u0026rdquo; homotopy data types naturally extends this scheme to a full-blownquantum programming/certification language in which our topological quantumgates may be compiled to verified quantum circuits, complete with quantummeasurement gates and classical control.\r2023-03-01\nOn Pre-trained Language Models for Antibody\nDanqing Wang Fei Ye Hao Zhou\nabstract\rabstract: Antibodies are vital proteins offering robust protection for the human bodyfrom pathogens. The development of general protein and antibody-specificpre-trained language models both facilitate antibody prediction tasks. However,there have been limited studies that comprehensively explore the representationcapability of distinct pre-trained language models on different antibody tasks.To investigate the problem, we aim to answer several key questions in thispaper, such as how pre-trained language models perform in antibody tasks withdifferent specificity and how introducing specific biological mechanisms to thepre-training process can benefit the model. Additionally, we evaluate if thelearned antibody pre-trained representations can be applied to real-worldantibody problems, like drug discovery and immune process understanding.Previously, no benchmark available largely hindered the study to answer thesequestions. To aid in our investigation, we provide an AnTibody UnderstandingEvaluation (ATUE) benchmark. We comprehensively evaluate the performance ofprotein pre-trained language models by empirical study along with conclusionsand new insights. Our ATUE and code are released athttps://github.com/dqwang122/EATLM.\rCANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning\nSamuel Maddock Alexandre Sablayrolles Pierre Stock\nabstract\rabstract: Federated Learning (FL) is a setting for training machine learning models indistributed environments where the clients do not share their raw data butinstead send model updates to a server. However, model updates can be subjectto attacks and leak private information. Differential Privacy (DP) is a leadingmitigation strategy which involves adding noise to clipped model updates,trading off performance for strong theoretical privacy guarantees. Previouswork has shown that the threat model of DP is conservative and that theobtained guarantees may be vacuous or may overestimate information leakage inpractice. In this paper, we aim to achieve a tighter measurement of the modelexposure by considering a realistic threat model. We propose a novel method,CANIFE, that uses canaries - carefully crafted samples by a strong adversary toevaluate the empirical privacy of a training round. We apply this attack tovision models trained on CIFAR-10 and CelebA and to language models trained onSent140 and Shakespeare. In particular, in realistic FL scenarios, wedemonstrate that the empirical per-round epsilon obtained with CANIFE is 4-5xlower than the theoretical bound.\rContextual Linear Types for Differential Privacy\nMatías Toro David Darais Chike Abuah Joe Near Damián Árquez Federico Olmedo Éric Tanter\nabstract\rabstract: Language support for differentially-private programming is both crucial anddelicate. While elaborate program logics can be very expressive, type-systembased approaches using linear types tend to be more lightweight and amenable toautomatic checking and inference, and in particular in the presence ofhigher-order programming. Since the seminal design of Fuzz, which is restrictedto $\\epsilon$-differential privacy in its original design, significant progresshas been made to support more advancedvariants of differential privacy,like($\\epsilon$,$\\delta$)-differential privacy. However, supporting theseadvanced privacy variants while also supporting higher-order programming infull has proven to be challenging. We present Jazz, a language and type systemwhich uses linear types and latent contextual effects to support both advancedvariants of differential privacy and higher-order programming. Latentcontextual effects allow delaying the payment of effects for connectives suchas products, sums and functions, yielding advantages in terms of precision ofthe analysis and annotation burden upon elimination, as well as modularity. Weformalize the core of Jazz, prove it sound for privacy via a logical relationfor metric preservation, and illustrate its expressive power through a numberof case studies drawn from the recent differential privacy literature.\rDTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction\nRaviteja Anantha Kriti Bhasin Daniela de la Parra Aguilar Prabal Vashisht Becci Williamson Srinivas Chappidi\nabstract\rabstract: Personal Digital Assistants (PDAs) - such as Siri, Alexa and GoogleAssistant, to name a few - play an increasingly important role to accessinformation and complete tasks spanning multiple domains, and by diverse groupsof users. A text-to-speech (TTS) module allows PDAs to interact in a natural,human-like manner, and play a vital role when the interaction involves peoplewith visual impairments or other disabilities. To cater to the needs of adiverse set of users, inclusive TTS is important to recognize and pronouncecorrectly text in different languages and dialects. Despite great progress inspeech synthesis, the pronunciation accuracy of named entities in amulti-lingual setting still has a large room for improvement. Existingapproaches to correct named entity (NE) mispronunciations, like retrainingGrapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciationdictionary, require expensive annotation of the ground truth pronunciation,which is also time consuming. In this work, we present a highly-precise,PDA-compatible pronunciation learning framework for the task of TTSmispronunciation detection and correction. In addition, we also propose a novelmispronunciation detection model called DTW-SiameseNet, which employs metriclearning with a Siamese architecture for Dynamic Time Warping (DTW) withtriplet loss. We demonstrate that a locale-agnostic, privacy-preservingsolution to the problem of TTS mispronunciation detection is feasible. Weevaluate our approach on a real-world dataset, and a corpus of NEpronunciations of an anonymized audio dataset of person names recorded byparticipants from 10 different locales. Human evaluation shows our proposedapproach improves pronunciation accuracy on average by ~6% compared to strongphoneme-based and audio-based baselines.\r2023-02-28\nThe (ab)use of Open Source Code to Train Large Language Models\nAli Al-Kaswan Maliheh Izadi\nabstract\rabstract: In recent years, Large Language Models (LLMs) have gained significantpopularity due to their ability to generate human-like text and their potentialapplications in various fields, such as Software Engineering. LLMs for Code arecommonly trained on large unsanitized corpora of source code scraped from theInternet. The content of these datasets is memorized and emitted by the models,often in a verbatim manner. In this work, we will discuss the security,privacy, and licensing implications of memorization. We argue why the use ofcopyleft code to train LLMs is a legal and ethical dilemma. Finally, we providefour actionable recommendations to address this issue.\r2023-02-27\nDo as You Say: Consistency Detection of Data Practice in Program Code and Privacy Policy in Mini-App\nYin Wang Ming Fan Junfeng Liu Junjie Tao Wuxia Jin Qi Xiong Yuhao Liu Qinghua Zheng Ting Liu\nabstract\rabstract: Mini-app is an emerging form of mobile application that combines webtechnology with native capabilities. Its features, e.g., no need to downloadand no installation, have made it popular rapidly. However, privacy issues thatviolate the laws or regulations are breeding in the swiftly expanding mini-appecosystem. The consistency between what the mini-app does about the data in theprogram code and what it declares in its privacy policy description isimportant. But no work has systematically investigated the privacy problem ofthe mini-app before. In this paper, to our best knowledge, we are the first toconduct the compliance detection of data practice and policy description inmini-apps. In this paper, we first customize a taint analysis method based ondata entity dependency network to adapt to the characteristics of theJavaScript language in the mini-apps. Then, we transform data types and dataoperations to data practices in program codes and privacy policies, so as tofinish a fine-grained consistency matching model.We crawl 100,000 mini-apps onWeChat client in the wild and extract 2,998 with a privacy policy. Among them,only 318 meet the consistency requirements, 2,680 are inconsistent, and theproportion of inconsistencies is as high as 89.4%. The inconsistency in themini-app is very serious. Based on 6 real-world cases analyzed, in order toreduce this potential data leakage risk, we suggest that the developer shouldreduce the collection of irrelevant information and the straightforward use oftemplates, and the platform should provide data flow detection tools andprivacy policy writing support.\rPQLM \u0026ndash; Multilingual Decentralized Portable Quantum Language Model for Privacy Protection\nShuyue Stella Li Xiangyu Zhang Shu Zhou Hongchao Shu Ruixing Liang Hexin Liu Leibny Paola Garcia\nabstract\rabstract: With careful manipulation, malicious agents can reverse engineer privateinformation encoded in pre-trained language models. Security concerns motivatethe development of quantum pre-training. In this work, we propose a highlyPortable Quantum Language Model (PQLM) that can easily transmit information todownstream tasks on classical machines. The framework consists of a cloud PQLMbuilt with random Variational Quantum Classifiers (VQC) and local models fordownstream applications. We demonstrate the ad hoc portability of the quantummodel by extracting only the word embeddings and effectively applying them todownstream tasks on classical machines. Our PQLM exhibits comparableperformance to its classical counterpart on both intrinsic evaluation (loss,perplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)metrics. We also perform ablation studies on the factors affecting PQLMperformance to analyze model stability. Our work establishes a theoreticalfoundation for a portable quantum pre-trained language model that could betrained on private data and made available for public use with privacyprotection guarantees.\r2023-02-25\nOn pitfalls (and advantages) of sophisticated large language models\nAnna Strasser\nabstract\rabstract: Natural language processing based on large language models (LLMs) is abooming field of AI research. After neural networks have proven to outperformhumans in games and practical domains based on pattern recognition, we mightstand now at a road junction where artificial entities might eventually enterthe realm of human communication. However, this comes with serious risks. Dueto the inherent limitations regarding the reliability of neural networks,overreliance on LLMs can have disruptive consequences. Since it will beincreasingly difficult to distinguish between human-written andmachine-generated text, one is confronted with new ethical challenges. Thisbegins with the no longer undoubtedly verifiable human authorship and continueswith various types of fraud, such as a new form of plagiarism. This alsoconcerns the violation of privacy rights, the possibility of circulatingcounterfeits of humans, and, last but not least, it makes a massive spread ofmisinformation possible.\r2023-02-23\nEfficiency 360: Efficient Vision Transformers\nBadri N. Patro Vijay Srinivas Agneeswaran\nabstract\rabstract: Transformers are widely used for solving tasks in natural languageprocessing, computer vision, speech, and music domains. In this paper, we talkabout the efficiency of transformers in terms of memory (the number ofparameters), computation cost (number of floating points operations), andperformance of models, including accuracy, the robustness of the model, andfair \u0026amp; bias-free features. We mainly discuss the vision transformer for theimage classification task. Our contribution is to introduce an efficient 360framework, which includes various aspects of the vision transformer, to make itmore efficient for industrial applications. By considering those applications,we categorize them into multiple dimensions such as privacy, robustness,transparency, fairness, inclusiveness, continual learning, probabilisticmodels, approximation, computational complexity, and spectral complexity. Wecompare various vision transformer models based on their performance, thenumber of parameters, and the number of floating point operations (FLOPs) onmultiple datasets.\rPrivately Customizing Prefinetuning to Better Match User Data in Federated Learning\nCharlie Hou Hongyuan Zhan Akshat Shrivastava Sid Wang Aleksandr Livshits Giulia Fanti Daniel Lazar\nabstract\rabstract: In Federated Learning (FL), accessing private client data incurscommunication and privacy costs. As a result, FL deployments commonlyprefinetune pretrained foundation models on a (large, possibly public) datasetthat is held by the central server; they then FL-finetune the model on aprivate, federated dataset held by clients. Evaluating prefinetuning datasetquality reliably and privately is therefore of high importance. To this end, wepropose FreD (Federated Private Fr'echet Distance) \u0026ndash; a privately computeddistance between a prefinetuning dataset and federated datasets. Intuitively,it privately computes and compares a Fr'echet distance between embeddingsgenerated by a large language model on both the central (public) dataset andthe federated private client data. To make this computation privacy-preserving,we use distributed, differentially-private mean and covariance estimators. Weshow empirically that FreD accurately predicts the best prefinetuning datasetat minimal privacy cost. Altogether, using FreD we demonstrate aproof-of-concept for a new approach in private FL training: (1) customize aprefinetuning dataset to better match user data (2) prefinetune (3) performFL-finetuning.\r2023-02-22\nPreventing Catastrophic Forgetting in Continual Learning of New Natural Language Tasks\nSudipta Kar Giuseppe Castellucci Simone Filice Shervin Malmasi Oleg Rokhlenko\nabstract\rabstract: Multi-Task Learning (MTL) is widely-accepted in Natural Language Processingas a standard technique for learning multiple related tasks in one model.Training an MTL model requires having the training data for all tasks availableat the same time. As systems usually evolve over time, (e.g., to support newfunctionalities), adding a new task to an existing MTL model usually requiresretraining the model from scratch on all the tasks and this can betime-consuming and computationally expensive. Moreover, in some scenarios, thedata used to train the original training may be no longer available, forexample, due to storage or privacy concerns. In this paper, we approach theproblem of incrementally expanding MTL models\u0026rsquo; capability to solve new tasksover time by distilling the knowledge of an already trained model on n tasksinto a new one for solving n+1 tasks. To avoid catastrophic forgetting, wepropose to exploit unlabeled data from the same distributions of the old tasks.Our experiments on publicly available benchmarks show that such a techniquedramatically benefits the distillation by preserving the already acquiredknowledge (i.e., preventing up to 20% performance drops on old tasks) whileobtaining good performance on the incrementally added tasks. Further, we alsoshow that our approach is beneficial in practical settings by using data from aleading voice assistant.\r2023-02-20\nProgrammable System Call Security with eBPF\nJinghao Jia YiFei Zhu Dan Williams Andrea Arcangeli Claudio Canella Hubertus Franke Tobin Feldman-Fitzthum Dimitrios Skarlatos Daniel Gruss Tianyin Xu\nabstract\rabstract: System call filtering is a widely used security mechanism for protecting ashared OS kernel against untrusted user applications. However, existing systemcall filtering techniques either are too expensive due to the context switchoverhead imposed by userspace agents, or lack sufficient programmability toexpress advanced policies. Seccomp, Linux\u0026rsquo;s system call filtering module, iswidely used by modern container technologies, mobile apps, and systemmanagement services. Despite the adoption of the classic BPF language (cBPF),security policies in Seccomp are mostly limited to static allow lists,primarily because cBPF does not support stateful policies. Consequently, manyessential security features cannot be expressed precisely and/or require kernelmodifications. In this paper, we present a programmable system call filtering mechanism,which enables more advanced security policies to be expressed by leveraging theextended BPF language (eBPF). More specifically, we create a new Seccomp eBPFprogram type, exposing, modifying or creating new eBPF helper functions tosafely manage filter state, access kernel and user state, and utilizesynchronization primitives. Importantly, our system integrates with existingkernel privilege and capability mechanisms, enabling unprivileged users toinstall advanced filters safely. Our evaluation shows that our eBPF-basedfiltering can enhance existing policies (e.g., reducing the attack surface ofearly execution phase by up to 55.4% for temporal specialization), mitigatereal-world vulnerabilities, and accelerate filters.\rFederated Learning for ASR based on Wav2vec 2.0\nTuan Nguyen Salima Mdhaffar Natalia Tomashenko Jean-François Bonastre Yannick Estève\nabstract\rabstract: This paper presents a study on the use of federated learning to train an ASRmodel based on a wav2vec 2.0 model pre-trained by self supervision. Carried outon the well-known TED-LIUM 3 dataset, our experiments show that such a modelcan obtain, with no use of a language model, a word error rate of 10.92% on theofficial TED-LIUM 3 test set, without sharing any data from the differentusers. We also analyse the ASR performance for speakers depending to theirparticipation to the federated learning. Since federated learning was firstintroduced for privacy purposes, we also measure its ability to protect speakeridentity. To do that, we exploit an approach to analyze information containedin exchanged models based on a neural network footprint on an indicatordataset. This analysis is made layer-wise and shows which layers in anexchanged wav2vec 2.0 based model bring the speaker identity information.\rOLYMPIA: A Simulation Framework for Evaluating the Concrete Scalability of Secure Aggregation Protocols\nIvoline C. Ngong Nicholas Gibson Joseph P. Near\nabstract\rabstract: Recent secure aggregation protocols enable privacy-preserving federatedlearning for high-dimensional models among thousands or even millions ofparticipants. Due to the scale of these use cases, however, end-to-endempirical evaluation of these protocols is impossible. We present OLYMPIA, aframework for empirical evaluation of secure protocols via simulation. OLYMPIAprovides an embedded domain-specific language for defining protocols, and asimulation framework for evaluating their performance. We implement severalrecent secure aggregation protocols using OLYMPIA, and perform the firstempirical comparison of their end-to-end running times. We release OLYMPIA asopen source.\rBlack Boxes, White Noise: Similarity Detection for Neural Functions\nFarima Farmahinifarahani Cristina V. Lopes\nabstract\rabstract: Similarity, or clone, detection has important applications in copyrightviolation, software theft, code search, and the detection of maliciouscomponents. There is now a good number of open source and proprietary clonedetectors for programs written in traditional programming languages. However,the increasing adoption of deep learning models in software poses a challengeto these tools: these models implement functions that are inscrutable blackboxes. As more software includes these DNN functions, new techniques are neededin order to assess the similarity between deep learning components of software.Previous work has unveiled techniques for comparing the representations learnedat various layers of deep neural network models by feeding canonical inputs tothe models. Our goal is to be able to compare DNN functions when canonicalinputs are not available \u0026ndash; because they may not be in many applicationscenarios. The challenge, then, is to generate appropriate inputs and toidentify a metric that, for those inputs, is capable of representing the degreeof functional similarity between two comparable DNN functions. Our approach uses random input with values between -1 and 1, in a shape thatis compatible with what the DNN models expect. We then compare the outputs byperforming correlation analysis. Our study shows how it is possible to performsimilarity analysis even in the absence of meaningful canonical inputs. Theresponse to random inputs of two comparable DNN functions exposes thosefunctions\u0026rsquo; similarity, or lack thereof. Of all the metrics tried, we find thatSpearman\u0026rsquo;s rank correlation coefficient is the most powerful and versatile,although in special cases other methods and metrics are more expressive. Wepresent a systematic empirical study comparing the effectiveness of severalsimilarity metrics using a dataset of 56,355 classifiers collected from GitHub.This is accompanied by a sensitivity analysis that reveals how certain models\u0026rsquo;training related properties affect the effectiveness of the similarity metrics. To the best of our knowledge, this is the first work that shows howsimilarity of DNN functions can be detected by using random inputs. Our studyof correlation metrics, and the identification of Spearman correlationcoefficient as the most powerful among them for this purpose, establishes acomplete and practical method for DNN clone detection that can be used in thedesign of new tools. It may also serve as inspiration for other programanalysis tasks whose approaches break in the presence of DNN components.\r2023-02-19\nMultilingual Content Moderation: A Case Study on Reddit\nMeng Ye Karan Sikka Katherine Atwell Sabit Hassan Ajay Divakaran Malihe Alikhani\nabstract\rabstract: Content moderation is the process of flagging content based on pre-definedplatform rules. There has been a growing need for AI moderators to safeguardusers as well as protect the mental health of human moderators from traumaticcontent. While prior works have focused on identifying hateful/offensivelanguage, they are not adequate for meeting the challenges of contentmoderation since 1) moderation decisions are based on violation of rules, whichsubsumes detection of offensive speech, and 2) such rules often differ acrosscommunities which entails an adaptive solution. We propose to study thechallenges of content moderation by introducing a multilingual dataset of 1.8Million Reddit comments spanning 56 subreddits in English, German, Spanish andFrench. We perform extensive experimental analysis to highlight the underlyingchallenges and suggest related research problems such as cross-lingualtransfer, learning under label noise (human biases), transfer of moderationmodels, and predicting the violated rule. Our dataset and analysis can helpbetter prepare for the challenges and opportunities of auto moderation.\rWhy Is Public Pretraining Necessary for Private Model Training?\nArun Ganesh Mahdi Haghifam Milad Nasr Sewoong Oh Thomas Steinke Om Thakkar Abhradeep Thakurta Lun Wang\nabstract\rabstract: In the privacy-utility tradeoff of a model trained on benchmark language andvision tasks, remarkable improvements have been widely reported with the use ofpretraining on publicly available data. This is in part due to the benefits oftransfer learning, which is the standard motivation for pretraining innon-private settings. However, the stark contrast in the improvement achievedthrough pretraining under privacy compared to non-private settings suggeststhat there may be a deeper, distinct cause driving these gains. To explain thisphenomenon, we hypothesize that the non-convex loss landscape of a modeltraining necessitates an optimization algorithm to go through two phases. Inthe first, the algorithm needs to select a good \u0026ldquo;basin\u0026rdquo; in the loss landscape.In the second, the algorithm solves an easy optimization within that basin. Theformer is a harder problem to solve with private data, while the latter isharder to solve with public data due to a distribution shift or data scarcity.Guided by this intuition, we provide theoretical constructions that provablydemonstrate the separation between private training with and without publicpretraining. Further, systematic experiments on CIFAR10 and LibriSpeech providesupporting evidence for our hypothesis.\r2023-02-18\nRobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks\nMarwan Omar\nabstract\rabstract: As machine learning (ML) systems are being increasingly employed in the realworld to handle sensitive tasks and make decisions in various fields, thesecurity and privacy of those models have also become increasingly critical. Inparticular, Deep Neural Networks (DNN) have been shown to be vulnerable tobackdoor attacks whereby adversaries have access to the training data and theopportunity to manipulate such data by inserting carefully developed samplesinto the training dataset. Although the NLP community has produced severalstudies on generating backdoor attacks proving the vulnerable state of languagemodes, to the best of our knowledge, there does not exist any work to combatsuch attacks. To bridge this gap, we present RobustEncoder: a novelclustering-based technique for detecting and removing backdoor attacks in thetext domain. Extensive empirical results demonstrate the effectiveness of ourtechnique in detecting and removing backdoor triggers. Our code is available athttps://github.com/marwanomar1/Backdoor-Learning-for-NLP\r2023-02-17\nPLACES: Prompting Language Models for Social Conversation Synthesis\nMaximillian Chen Alexandros Papangelis Chenyang Tao Seokhwan Kim Andy Rosenbaum Yang Liu Zhou Yu Dilek Hakkani-Tur\nabstract\rabstract: Collecting high quality conversational data can be very expensive for mostapplications and infeasible for others due to privacy, ethical, or similarconcerns. A promising direction to tackle this problem is to generate syntheticdialogues by prompting large language models. In this work, we use a small setof expert-written conversations as in-context examples to synthesize a socialconversation dataset using prompting. We perform several thorough evaluationsof our synthetic conversations compared to human-collected conversations. Thisincludes various dimensions of conversation quality with human evaluationdirectly on the synthesized conversations, and interactive human evaluation ofchatbots fine-tuned on the synthetically generated dataset. We additionallydemonstrate that this prompting approach is generalizable to multi-partyconversations, providing potential to create new synthetic data for multi-partytasks. Our synthetic multi-party conversations were rated more favorably acrossall measured dimensions compared to conversation excerpts sampled from ahuman-collected multi-party dataset.\rUncertainty-aware Self-training for Low-resource Neural Sequence Labeling\nJianing Wang Chengyu Wang Jun Huang Ming Gao Aoying Zhou\nabstract\rabstract: Neural sequence labeling (NSL) aims at assigning labels for input languagetokens, which covers a broad range of applications, such as named entityrecognition (NER) and slot filling, etc. However, the satisfying resultsachieved by traditional supervised-based approaches heavily depend on the largeamounts of human annotation data, which may not be feasible in real-worldscenarios due to data privacy and computation efficiency issues. This paperpresents SeqUST, a novel uncertain-aware self-training framework for NSL toaddress the labeled data scarcity issue and to effectively utilize unlabeleddata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neuralnetwork (BNN) to perform uncertainty estimation at the token level and thenselect reliable language tokens from unlabeled data based on the modelconfidence and certainty. A well-designed masked sequence labeling task with anoise-robust loss supports robust training, which aims to suppress the problemof noisy pseudo labels. In addition, we develop a Gaussian-based consistencyregularization technique to further improve the model robustness onGaussian-distributed perturbed representations. This effectively alleviates theover-fitting dilemma originating from pseudo-labeled augmented data. Extensiveexperiments over six benchmarks demonstrate that our SeqUST frameworkeffectively improves the performance of self-training, and consistentlyoutperforms strong baselines by a large margin in low-resource scenarios\r2023-02-14\nA Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3\nHa-Thanh Nguyen\nabstract\rabstract: LawGPT 1.0 is a virtual legal assistant built on the state-of-the-artlanguage model GPT-3, fine-tuned for the legal domain. The system is designedto provide legal assistance to users in a conversational manner, helping themwith tasks such as answering legal questions, generating legal documents, andproviding legal advice. In this paper, we provide a brief overview of LawGPT1.0, its architecture, and its performance on a set of legal benchmark tasks.Please note that the detailed information about the model is protected by anon-disclosure agreement (NDA) and cannot be disclosed in this report.\r2023-02-13\nThe 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health\nKevin Lybarger Meliha Yetisgen Özlem Uzuner\nabstract\rabstract: Objective: The n2c2/UW SDOH Challenge explores the extraction of socialdeterminant of health (SDOH) information from clinical notes. The objectivesinclude the advancement of natural language processing (NLP) informationextraction techniques for SDOH and clinical information more broadly. Thispaper presents the shared task, data, participating teams, performance results,and considerations for future work. Materials and Methods: The task used the Social History Annotated Corpus(SHAC), which consists of clinical text with detailed event-based annotationsfor SDOH events such as alcohol, drug, tobacco, employment, and livingsituation. Each SDOH event is characterized through attributes related tostatus, extent, and temporality. The task includes three subtasks related toinformation extraction (Subtask A), generalizability (Subtask B), and learningtransfer (Subtask C). In addressing this task, participants utilized a range oftechniques, including rules, knowledge bases, n-grams, word embeddings, andpretrained language models (LM). Results: A total of 15 teams participated, and the top teams utilizedpretrained deep learning LM. The top team across all subtasks used asequence-to-sequence approach achieving 0.901 F1 for Subtask A, 0.774 F1Subtask B, and 0.889 F1 for Subtask C. Conclusions: Similar to many NLP tasks and domains, pretrained LM yielded thebest performance, including generalizability and learning transfer. An erroranalysis indicates extraction performance varies by SDOH, with lowerperformance achieved for conditions, like substance use and homelessness, thatincrease health risks (risk factors) and higher performance achieved forconditions, like substance abstinence and living with family, that reducehealth risks (protective factors).\rTargeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge\nAli Al-Kaswan Maliheh Izadi Arie van Deursen\nabstract\rabstract: Previous work has shown that Large Language Models are susceptible toso-called data extraction attacks. This allows an attacker to extract a samplethat was contained in the training data, which has massive privacyimplications. The construction of data extraction attacks is challenging,current attacks are quite inefficient, and there exists a significant gap inthe extraction capabilities of untargeted attacks and memorization. Thus,targeted attacks are proposed, which identify if a given sample from thetraining data, is extractable from a model. In this work, we apply a targeteddata extraction attack to the SATML2023 Language Model Training Data ExtractionChallenge. We apply a two-step approach. In the first step, we maximise therecall of the model and are able to extract the suffix for 69% of the samples.In the second step, we use a classifier-based Membership Inference Attack onthe generations. Our AutoSklearn classifier achieves a precision of 0.841. Thefull approach reaches a score of 0.405 recall at a 10% false positive rate,which is an improvement of 34% over the baseline of 0.301.\rDataset of Natural Language Queries for E-Commerce\nAndrea Papenmeier Dagmar Kern Daniel Hienert Alfred Sliwa Ahmet Aker Norbert Fuhr\nabstract\rabstract: Shopping online is more and more frequent in our everyday life. Fore-commerce search systems, understanding natural language coming through voiceassistants, chatbots or from conversational search is an essential ability tounderstand what the user really wants. However, evaluation datasets withnatural and detailed information needs of product-seekers which could be usedfor research do not exist. Due to privacy issues and competitive consequences,only few datasets with real user search queries from logs are openly available.In this paper, we present a dataset of 3,540 natural language queries in twodomains that describe what users want when searching for a laptop or a jacketof their choice. The dataset contains annotations of vague terms and key factsof 1,754 laptop queries. This dataset opens up a range of researchopportunities in the fields of natural language processing and (interactive)information retrieval for product search.\r2023-02-12\nChat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models\nPaula Maddigan Teo Susnjak\nabstract\rabstract: The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies.\r2023-02-11\nSynthesizing Human Gaze Feedback for Improved NLP Performance\nVarun Khurana Yaman Kumar Singla Nora Hollenstein Rajesh Kumar Balaji Krishnamurthy\nabstract\rabstract: Integrating human feedback in models can improve the performance of naturallanguage processing (NLP) models. Feedback can be either explicit (e.g. rankingused in training language models) or implicit (e.g. using human cognitivesignals in the form of eyetracking). Prior eye tracking and NLP research revealthat cognitive processes, such as human scanpaths, gleaned from human gazepatterns aid in the understanding and performance of NLP models. However, thecollection of real eyetracking data for NLP tasks is challenging due to therequirement of expensive and precise equipment coupled with privacy invasionissues. To address this challenge, we propose ScanTextGAN, a novel model forgenerating human scanpaths over text. We show that ScanTextGAN-generatedscanpaths can approximate meaningful cognitive signals in human gaze patterns.We include synthetically generated scanpaths in four popular NLP tasks spanningsix different datasets as proof of concept and show that the models augmentedwith generated scanpaths improve the performance of all downstream NLP tasks.\rFormalizing Stack Safety as a Security Property\nSean Noble Anderson Roberto Blanco Leonidas Lampropoulos Benjamin C. Pierce Andrew Tolmach\nabstract\rabstract: The term stack safety is used for a variety of compiler, run-time, andhardware mechanisms for protecting stack memory. Unlike \u0026ldquo;the heap,\u0026rdquo; theISA-level stack does not correspond to a single high-level language concept:different compilers use it in different ways to support procedural andfunctional abstraction mechanisms from a wide range of languages. This proteannature makes it difficult to nail down what it means to correctly enforce stacksafety. We propose a formal characterization of stack safety using concepts fromlanguage-based security. Rather than packaging all aspects of stack safety intoa monolithic property, we decompose it into an integrity property and aconfidentiality property for each of the caller and the callee, plus acontrol-flow property \u0026ndash; five properties in all. This formulation is motivated by a particular class of enforcementmechanisms, the ``lazy\u0026rsquo;\u0026rsquo; stack safety micro-policies studied by Roessler andDeHon~\\cite{DBLP:conf/sp/RoesslerD18}, which permit functions to write into oneanother\u0026rsquo;s frames, but which taint the changed locations so that the frame\u0026rsquo;sowner cannot access it. No existing characterization of stack safety capturesthis style of safety. We capture it here by stating our properties in terms ofthe observable behavior of the system. Our properties go further than previous formal definitions of stack safety,supporting caller- and callee-saved registers, arguments passed on the stack,and tail-call elimination. We validate our properties by using them to distinguish between correct andincorrect implementations of Roessler and DeHon\u0026rsquo;s micro-policies usingproperty-based random testing. Our test harness successfully identifies severalbroken variants, including Roessler and DeHon\u0026rsquo;s lazy policy; a repaired versionof their policy does pass our tests.\r2023-02-10\nBuilding cross-language corpora for human understanding of privacy policies\nFrancesco Ciclosi Silvia Vidor Fabio Massacci\nabstract\rabstract: Making sure that users understand privacy policies that impact them is a keychallenge for a real GDPR deployment. Research studies are mostly carried inEnglish, but in Europe and elsewhere, users speak a language that is notEnglish. Replicating studies in different languages requires the availabilityof comparable cross-language privacy policies corpora. This work provides amethodology for building comparable cross-language in a national language and areference study language. We provide an application example of our methodologycomparing English and Italian extending the corpus of one of the first studiesabout users understanding of technical terms in privacy policies. We alsoinvestigate other open issues that can make replication harder.\rWatermarking Pre-trained Language Models with Backdooring\nChenxi Gu Chengsong Huang Xiaoqing Zheng Kai-Wei Chang Cho-Jui Hsieh\nabstract\rabstract: Large pre-trained language models (PLMs) have proven to be a crucialcomponent of modern natural language processing systems. PLMs typically need tobe fine-tuned on task-specific downstream datasets, which makes it hard toclaim the ownership of PLMs and protect the developer\u0026rsquo;s intellectual propertydue to the catastrophic forgetting phenomenon. We show that PLMs can bewatermarked with a multi-task learning framework by embedding backdoorstriggered by specific inputs defined by the owners, and those watermarks arehard to remove even though the watermarked PLMs are fine-tuned on multipledownstream tasks. In addition to using some rare words as triggers, we alsoshow that the combination of common words can be used as backdoor triggers toavoid them being easily detected. Extensive experiments on multiple datasetsdemonstrate that the embedded watermarks can be robustly extracted with a highsuccess rate and less influenced by the follow-up fine-tuning.\r2023-02-09\nOffsite-Tuning: Transfer Learning without Full Model\nGuangxuan Xiao Ji Lin Song Han\nabstract\rabstract: Transfer learning is important for foundation models to adapt to downstreamtasks. However, many foundation models are proprietary, so users must sharetheir data with model owners to fine-tune the models, which is costly and raiseprivacy concerns. Moreover, fine-tuning large foundation models iscomputation-intensive and impractical for most downstream users. In this paper,we propose Offsite-Tuning, a privacy-preserving and efficient transfer learningframework that can adapt billion-parameter foundation models to downstream datawithout access to the full model. In offsite-tuning, the model owner sends alight-weight adapter and a lossy compressed emulator to the data owner, whothen fine-tunes the adapter on the downstream data with the emulator\u0026rsquo;sassistance. The fine-tuned adapter is then returned to the model owner, whoplugs it into the full model to create an adapted foundation model.Offsite-tuning preserves both parties\u0026rsquo; privacy and is computationally moreefficient than the existing fine-tuning methods that require access to the fullmodel weights. We demonstrate the effectiveness of offsite-tuning on variouslarge language and vision foundation models. Offsite-tuning can achievecomparable accuracy as full model fine-tuning while being privacy-preservingand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code isavailable at https://github.com/mit-han-lab/offsite-tuning.\r2023-02-01\nDeveloping Hands-on Labs for Source Code Vulnerability Detection with AI\nMaryam Taeb\nabstract\rabstract: As the role of information and communication technologies gradually increasesin our lives, source code security becomes a significant issue to protectagainst malicious attempts Furthermore with the advent of data-driventechniques, there is now a growing interest in leveraging machine learning andnatural language processing as a source code assurance method to buildtrustworthy systems Therefore training our future software developers to writesecure source code is in high demand In this thesis we propose a frameworkincluding learning modules and hands on labs to guide future IT professionalstowards developing secure programming habits and mitigating source codevulnerabilities at the early stages of the software development lifecycle Inthis thesis our goal is to design learning modules with a set of hands on labsthat will introduce students to secure programming practices using source codeand log file analysis tools to predict and identify vulnerabilities In a SecureCoding Education framework we will improve students skills and awareness onsource code vulnerabilities detection tools and mitigation techniques integrateconcepts of source code vulnerabilities from Function API and library level tobad programming habits and practices leverage deep learning NLP and staticanalysis tools for log file analysis to introduce the root cause of source codevulnerabilities\rBunched Fuzz: Sensitivity for Vector Metrics\njune wunder Arthur Azevedo de Amorim Patrick Baillot Marco Gaboardi\nabstract\rabstract: Program sensitivity measures the distance between the outputs of a programwhen run on two related inputs. This notion, which plays a key role in areassuch as data privacy and optimization, has been the focus of several programanalysis techniques introduced in recent years. Among the most successful ones,we can highlight type systems inspired by linear logic, as pioneered by Reedand Pierce in the Fuzz programming language. In Fuzz, each type is equippedwith its own distance, and sensitivity analysis boils down to type checking. Inparticular, Fuzz features two product types, corresponding to two differentnotions of distance: the tensor product combines the distances of eachcomponent by adding them, while the with product takes their maximum. In this work, we show that these products can be generalized to arbitrary$L^p$ distances, metrics that are often used in privacy and optimization. Theoriginal Fuzz products, tensor and with, correspond to the special cases $L^1$and $L^\\infty$. To ease the handling of such products, we extend the Fuzz typesystem with bunches \u0026ndash; as in the logic of bunched implications \u0026ndash; where thedistances of different groups of variables can be combined using different$L^p$ distances. We show that our extension can be used to reason aboutquantitative properties of probabilistic programs.\r2023-01-31\nSTI: Turbocharge NLP Inference at the Edge via Elastic Pipelining\nLiwei Guo Wonkyo Choe Felix Xiaozhu Lin\nabstract\rabstract: Natural Language Processing (NLP) inference is seeing increasing adoption bymobile applications, where on-device inference is desirable for cruciallypreserving user data privacy and avoiding network roundtrips. Yet, theunprecedented size of an NLP model stresses both latency and memory, creating atension between the two key resources of a mobile device. To meet a targetlatency, holding the whole model in memory launches execution as soon aspossible but increases one app\u0026rsquo;s memory footprints by several times, limitingits benefits to only a few inferences before being recycled by mobile memorymanagement. On the other hand, loading the model from storage on demand incursIO as long as a few seconds, far exceeding the delay range satisfying to auser; pipelining layerwise model loading and execution does not hide IO either,due to the high skewness between IO and computation delays. To this end, we propose Speedy Transformer Inference (STI). Built on the keyidea of maximizing IO/compute resource utilization on the most important partsof a model, STI reconciles the latency v.s. memory tension via two noveltechniques. First, model sharding. STI manages model parameters asindependently tunable shards, and profiles their importance to accuracy.Second, elastic pipeline planning with a preload buffer. STI instantiates anIO/compute pipeline and uses a small buffer for preload shards to bootstrapexecution without stalling at early stages; it judiciously selects, tunes, andassembles shards per their importance for resource-elastic execution,maximizing inference accuracy. Atop two commodity SoCs, we build STI and evaluate it against a wide range ofNLP tasks, under a practical range of target latencies, and on both CPU andGPU. We demonstrate that STI delivers high accuracies with 1-2 orders ofmagnitude lower memory, outperforming competitive baselines.\r2023-01-30\nAutomating the Generation of Cyber Range Virtual Scenarios with VSDL\nGabriele Costa Enrico Russo Alessandro Armando\nabstract\rabstract: A cyber range is an environment used for training security experts andtesting attack and defence tools and procedures. Usually, a cyber rangesimulates one or more critical infrastructures that attacking (red) anddefending (blue) teams must compromise and protect, respectively. Theinfrastructure can be physically assembled, but much more convenient is to relyon the Infrastructure as a Service (IaaS) paradigm. Although some moderntechnologies support the IaaS, the design and deployment of scenarios ofinterest is mostly a manual operation. As a consequence, it is a commonpractice to have a cyber range hosting few (sometimes only one), consolidatedscenarios. However, reusing the same scenario may significantly reduce theeffectiveness of the training and testing sessions. In this paper, we propose aframework for automating the definition and deployment of arbitrarily complexcyber range scenarios. The framework relies on the virtual scenario descriptionlanguage (VSDL), i.e., a domain-specific language for defining high-levelfeatures of the desired infrastructure while hiding low-level details. Thesemantics of VSDL is given in terms of constraints that must be satisfied bythe virtual infrastructure. These constraints are then submitted to an SMTsolver for checking the satisfiability of the specification. If satisfiable,the specification gives rise to a model that is automatically converted to aset of deployment scripts to be submitted to the IaaS provider.\r2023-01-28\nContext-Aware Differential Privacy for Language Modeling\nMy H. Dinh Ferdinando Fioretto\nabstract\rabstract: The remarkable ability of language models (LMs) has also brought challengesat the interface of AI and security. A critical challenge pertains to how muchinformation these models retain and leak about the training data. This isparticularly urgent as the typical development of LMs relies on huge, oftenhighly sensitive data, such as emails and chat logs. To contrast thisshortcoming, this paper introduces Context-Aware Differentially PrivateLanguage Model (CADP-LM) , a privacy-preserving LM framework that relies on twokey insights: First, it utilizes the notion of \\emph{context} to define andaudit the potentially sensitive information. Second, it adopts the notion ofDifferential Privacy to protect sensitive information and characterize theprivacy leakage. A unique characteristic of CADP-LM is its ability to targetthe protection of sensitive sentences and contexts only, providing a highlyaccurate private model. Experiments on a variety of datasets and settingsdemonstrate these strengths of CADP-LM.\r2023-01-27\nDown the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech\nJarod Govers Philip Feldman Aaron Dant Panos Patros\nabstract\rabstract: Social media is a modern person\u0026rsquo;s digital voice to project and engage withnew ideas and mobilise communities $\\unicode{x2013}$ a power shared withextremists. Given the societal risks of unvetted content-moderating algorithmsfor Extremism, Radicalisation, and Hate speech (ERH) detection, responsiblesoftware engineering must understand the who, what, when, where, and why suchmodels are necessary to protect user safety and free expression. Hence, wepropose and examine the unique research field of ERH context mining to unifydisjoint studies. Specifically, we evaluate the start-to-finish design processfrom socio-technical definition-building and dataset collection strategies totechnical algorithm design and performance. Our 2015-2021 51-study SystematicLiterature Review (SLR) provides the first cross-examination of textual,network, and visual approaches to detecting extremist affiliation, hatefulcontent, and radicalisation towards groups and movements. We identifyconsensus-driven ERH definitions and propose solutions to existing ideologicaland geographic biases, particularly due to the lack of research inOceania/Australasia. Our hybridised investigation on Natural LanguageProcessing, Community Detection, and visual-text models demonstrates thedominating performance of textual transformer-based algorithms. We concludewith vital recommendations for ERH context mining researchers and propose anuptake roadmap with guidelines for researchers, industries, and governments toenable a safer cyberspace.\r2023-01-26\nCommunication-Efficient Learning of Deep Networks from Decentralized Data\nH. Brendan McMahan Eider Moore Daniel Ramage Seth Hampson Blaise Agüera y Arcas\nabstract\rabstract: Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksbased on iterative model averaging, and conduct an extensive empiricalevaluation, considering five different model architectures and four datasets.These experiments demonstrate the approach is robust to the unbalanced andnon-IID data distributions that are a defining characteristic of this setting.Communication costs are the principal constraint, and we show a reduction inrequired communication rounds by 10-100x as compared to synchronized stochasticgradient descent.\rJoint data rate and EMF exposure analysis in Manhattan environments: stochastic geometry and ray tracing approaches\nCharles Wiame Simon Demey Luc Vandendorpe Philippe De Doncker Claude Oestges\nabstract\rabstract: The objective of this study is to jointly analyze the data rate andelectromagnetic field (EMF) exposure in urban environments. Capitalizing onstochastic geometry (SG), a network level analysis is performed by modellingthese environments via Manhattan Poisson line processes (MPLP). Using thisframework, a number of performance metrics are derived: first moments, marginaldistributions and joint distributions of the data rate and exposure. Inaddition, the original Manhattan model is generalized to include advancedfeatures: corner diffraction, presence of potential blockages in streets, andusers positioned at crossroads. As a second approach, deterministic ray tracing(RT) is utilized to compute the same metrics. The two methods are shown toprovide close results on the condition that the model parameters are coherentlyselected. Furthermore, the numerical results enable to gain insight intoseveral aspects: the role of the propagation mechanisms in the performancemetrics, existing trade-offs between the rate and exposure requirements, aswell as the impact of the user location (at a crossroad or in a single street). (This work has been submitted to the IEEE for possible publication. Copyrightmay be transferred without notice, after which this version may no longer beaccessible)\r2023-01-24\nFedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning\nHaodong Zhao Wei Du Fangqi Li Peixuan Li Gongshen Liu\nabstract\rabstract: Federated learning (FL) has enabled global model training on decentralizeddata in a privacy-preserving way by aggregating model updates. However, formany natural language processing (NLP) tasks that utilize pre-trained languagemodels (PLMs) with large numbers of parameters, there are considerablecommunication costs associated with FL. Recently, prompt tuning, which tunessome soft prompts without modifying PLMs, has achieved excellent performance asa new learning paradigm. Therefore we want to combine the two methods andexplore the effect of prompt tuning under FL. In this paper, we propose\u0026quot;FedPrompt\u0026quot; to study prompt tuning in a model split aggregation way using FL,and prove that split aggregation greatly reduces the communication cost, only0.01% of the PLMs\u0026rsquo; parameters, with little decrease on accuracy both on IID andNon-IID data distribution. This improves the efficiency of FL method while alsoprotecting the data privacy in prompt tuning. In addition, like PLMs, promptsare uploaded and downloaded between public platforms and personal users, so wetry to figure out whether there is still a backdoor threat using only softprompts in FL scenarios. We further conduct backdoor attacks by data poisoningon FedPrompt. Our experiments show that normal backdoor attack can not achievea high attack success rate, proving the robustness of FedPrompt. We hope thiswork can promote the application of prompt in FL and raise the awareness of thepossible security threats.\r2023-01-21\nBlacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models\nAnoop Kadan Deepak P. Sahely Bhadra Manjary P. Gangan Lajish V. L\nabstract\rabstract: Groundbreaking inventions and highly significant performance improvements indeep learning based Natural Language Processing are witnessed through thedevelopment of transformer based large Pre-trained Language Models (PLMs). Thewide availability of unlabeled data within human generated data deluge alongwith self-supervised learning strategy helps to accelerate the success of largePLMs in language generation, language understanding, etc. But at the same time,latent historical bias/unfairness in human minds towards a particular gender,race, etc., encoded unintentionally/intentionally into the corpora harms andquestions the utility and efficacy of large PLMs in many real-worldapplications, particularly for the protected groups. In this paper, we presentan extensive investigation towards understanding the existence of \u0026ldquo;AffectiveBias\u0026rdquo; in large PLMs to unveil any biased association of emotions such as anger,fear, joy, etc., towards a particular gender, race or religion with respect tothe downstream task of textual emotion detection. We conduct our exploration ofaffective bias from the very initial stage of corpus level affective biasanalysis by searching for imbalanced distribution of affective words within adomain, in large scale corpora that are used to pre-train and fine-tune PLMs.Later, to quantify affective bias in model predictions, we perform an extensiveset of class-based and intensity-based evaluations using various biasevaluation corpora. Our results show the existence of statistically significantaffective bias in the PLM based emotion detection systems, indicating biasedassociation of certain emotions towards a particular gender, race, andreligion.\rMetaSys: A Practical Open-Source Metadata Management System to Implement and Evaluate Cross-Layer Optimizations\nNandita Vijaykumar Ataberk Olgun Konstantinos Kanellopoulos Nisa Bostancı Hasan Hassan Mehrshad Lotfi Phillip B. Gibbons Onur Mutlu\nabstract\rabstract: This paper introduces the first open-source FPGA-based infrastructure,MetaSys, with a prototype in a RISC-V core, to enable the rapid implementationand evaluation of a wide range of cross-layer techniques in real hardware.Hardware-software cooperative techniques are powerful approaches to improve theperformance, quality of service, and security of general-purpose processors.They are however typically challenging to rapidly implement and evaluate inreal hardware as they require full-stack changes to the hardware, OS, systemsoftware, and instruction-set architecture (ISA). MetaSys implements a rich hardware-software interface and lightweightmetadata support that can be used as a common basis to rapidly implement andevaluate new cross-layer techniques. We demonstrate MetaSys\u0026rsquo;s versatility andease-of-use by implementing and evaluating three cross-layer techniques for:(i) prefetching for graph analytics; (ii) bounds checking in memory unsafelanguages, and (iii) return address protection in stack frames; each techniqueonly requiring ~100 lines of Chisel code over MetaSys. Using MetaSys, we perform the first detailed experimental study to quantifythe performance overheads of using a single metadata management system toenable multiple cross-layer optimizations in CPUs. We identify the key sourcesof bottlenecks and system inefficiency of a general metadata management system.We design MetaSys to minimize these inefficiencies and provide increasedversatility compared to previously-proposed metadata systems. Using three usecases and a detailed characterization, we demonstrate that a common metadatamanagement system can be used to efficiently support diverse cross-layertechniques in CPUs.\r2023-01-19\nA Privacy Glossary for Cloud Computing\nTian Wang Masooda Bashir\nabstract\rabstract: Cloud computing is an evolving paradigm that is frequently changing the wayhumans share, store, and access their information in digital format. Whilecloud computing offers tremendous benefits (e.g., efficiency, flexibility, andreduced costs), it also brings both security and privacy challenges. Althoughcloud security has been extensively defined and developed, privacy protectionsin cloud environments are often described in abstract or vague language, whichmakes it difficult to interpret and implement. In this study, we propose aninitial approach of developing a privacy glossary for cloud computing thatprovides a consistent and comprehensive set of terminologies for cloud privacy.We believe that this systematic and structured privacy glossary could serve asa first step towards implementing requirements for privacy protections in cloudcomputing, as well as providing more effective and consistent language in cloudprivacy to researchers and professionals in the future.\r2023-01-17\nCommand Line Interface Risk Modeling\nDr Anthony L. Faulds\nabstract\rabstract: Protecting sensitive data is an essential part of security in cloudcomputing. However, only specific privileged individuals have access to view orinteract with this data; therefore, it is unscalable to depend on theseindividuals also to maintain the software. A solution to this is to allownon-privileged individuals access to maintain these systems but mask sensitiveinformation from egressing. To this end, we have created a machine-learningmodel to predict and redact fields with sensitive data. This work concentrateson Azure PowerShell, showing how it applies to other command-line interfacesand APIs. Using the F5-score as a weighted metric, we demonstrate differenttransformation techniques to map this problem from an unknown field to thewell-researched area of natural language processing.\r2023-01-13\nMuch Ado About Gender: Current Practices and Future Recommendations for Appropriate Gender-Aware Information Access\nChristine Pinney Amifa Raj Alex Hanna Michael D. Ekstrand\nabstract\rabstract: Information access research (and development) sometimes makes use of gender,whether to report on the demographics of participants in a user study, asinputs to personalized results or recommendations, or to make systemsgender-fair, amongst other purposes. This work makes a variety of assumptionsabout gender, however, that are not necessarily aligned with currentunderstandings of what gender is, how it should be encoded, and how a gendervariable should be ethically used. In this work, we present a systematic reviewof papers on information retrieval and recommender systems that mention genderin order to document how gender is currently being used in this field. We findthat most papers mentioning gender do not use an explicit gender variable, butmost of those that do either focus on contextualizing results of modelperformance, personalizing a system based on assumptions of user gender, orauditing a model\u0026rsquo;s behavior for fairness or other privacy-related issues.Moreover, most of the papers we review rely on a binary notion of gender, evenif they acknowledge that gender cannot be split into two categories. We connectthese findings with scholarship on gender theory and recent work on gender inhuman-computer interaction and natural language processing. We conclude bymaking recommendations for ethical and well-grounded use of gender in buildingand researching information access systems.\r2023-01-10\nUser-Centered Security in Natural Language Processing\nChris Emmery\nabstract\rabstract: This dissertation proposes a framework of user-centered security in NaturalLanguage Processing (NLP), and demonstrates how it can improve theaccessibility of related research. Accordingly, it focuses on two securitydomains within NLP with great public interest. First, that of author profiling,which can be employed to compromise online privacy through invasive inferences.Without access and detailed insight into these models\u0026rsquo; predictions, there is noreasonable heuristic by which Internet users might defend themselves from suchinferences. Secondly, that of cyberbullying detection, which by defaultpresupposes a centralized implementation; i.e., content moderation acrosssocial platforms. As access to appropriate data is restricted, and the natureof the task rapidly evolves (both through lexical variation, and culturalshifts), the effectiveness of its classifiers is greatly diminished and therebyoften misrepresented. Under the proposed framework, we predominantly investigate the use ofadversarial attacks on language; i.e., changing a given input (generatingadversarial samples) such that a given model does not function as intended.These attacks form a common thread between our user-centered security problems;they are highly relevant for privacy-preserving obfuscation methods againstauthor profiling, and adversarial samples might also prove useful to assess theinfluence of lexical variation and augmentation on cyberbullying detection.\rGPU-based high-precision orbital propagation of large sets of initial conditions through Picard-Chebyshev augmentation\nAlessandro Masat Camilla Colombo Arnaud Boutonnet\nabstract\rabstract: The orbital propagation of large sets of initial conditions under highaccuracy requirements is currently a bottleneck in the development of spacemissions, e.g. for planetary protection compliance analyses. The proposedapproach can include any force source in the dynamical model through efficientPicard-Chebyshev (PC) numerical simulations. A two-level augmentation of theintegration scheme is proposed, to run an arbitrary number of simulationswithin the same algorithm call, fully exploiting high performance and GPU(Graphics Processing Units) computing facilities. The performances obtainedwith implementation in C and NVIDIA CUDA programming languages are shown, on atest case taken from the optimization of a Solar Orbiter-like first resonantphase with Venus.\r2023-01-05\nWhat is in a Text-to-Image Prompt: The Potential of Stable Diffusion in Visual Arts Education\nNassim Dehouche Kullathida Dehouche\nabstract\rabstract: Text-to-Image artificial intelligence (AI) recently saw a major breakthroughwith the release of Dall-E and its open-source counterpart, Stable Diffusion.These programs allow anyone to create original visual art pieces by simplyproviding descriptions in natural language (prompts). Using a sample of 72,980Stable Diffusion prompts, we propose a formalization of this new medium of artcreation and assess its potential for teaching the history of art, aesthetics,and technique. Our findings indicate that text-to-Image AI has the potential torevolutionize the way art is taught, offering new, cost-effective possibilitiesfor experimentation and expression. However, it also raises important questionsabout the ownership of artistic works. As more and more art is created usingthese programs, it will be crucial to establish new legal and economic modelsto protect the rights of artists.\r2022-12-29\n$π$QLB: A Privacy-preserving with Integrity-assuring Query Language for Blockchain\nNasrin Sohrabi Norrathep Rattanavipanon Zahir Tari\nabstract\rabstract: The increase in the adoption of blockchain technology in differentapplication domains e.g., healthcare systems, supplychain management, hasraised the demand for a data query mechanism on blockchain. Since currentblockchain systems lack the support for querying data with embedded securityand privacy guarantees, there exists inherent security and privacy concerns onthose systems. In particular, existing systems require users to submit queriesto blockchain operators (e.g., a node validator) in plaintext. This directlyjeopardizes users\u0026rsquo; privacy as the submitted queries may contain sensitiveinformation, e.g., location or gender preferences, that the users may not becomfortable sharing. On the other hand, currently, the only way for users toensure integrity of the query result is to maintain the entire blockchaindatabase and perform the queries locally. Doing so incurs high storage andcomputational costs on the users, precluding this approach to be practicallydeployable on common light-weight devices (e.g., smartphones). To this end,this paper proposes $\\pi$QLB, a query language for blockchain systems thatensures both confidentiality of query inputs and integrity of query results.Additionally, $\\pi$QLB enables SQL-like queries over the blockchain data byintroducing relational data semantics into the existing blockchain database.$\\pi$QLB has applied the recent cryptography primitive, i.e., function secretsharing (FSS), to achieve confidentiality. To support integrity, we extend thetraditional FSS setting in such a way that integrity of FSS results can beefficiently verified. Successful verification indicates absence of maliciousbehaviors on the servers, allowing the user to establish trust from the result.To the best of our knowledge, $\\pi$QLB is the first query model designed forblockchain databases with support for confidentiality, integrity, and SQL-likequeries.\r2022-12-22\nAttribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings\nTiantian Feng Hanieh Hashemi Rajat Hebbar Murali Annavaram Shrikanth S. Narayanan\nabstract\rabstract: Speech emotion recognition (SER) processes speech signals to detect andcharacterize expressed perceived emotions. Many SER application systems oftenacquire and transmit speech data collected at the client-side to remote cloudplatforms for inference and decision making. However, speech data carry richinformation not only about emotions conveyed in vocal expressions, but alsoother sensitive demographic traits such as gender, age and language background.Consequently, it is desirable for SER systems to have the ability to classifyemotion constructs while preventing unintended/improper inferences of sensitiveand demographic information. Federated learning (FL) is a distributed machinelearning paradigm that coordinates clients to train a model collaborativelywithout sharing their local data. This training approach appears secure and canimprove privacy for SER. However, recent works have demonstrated that FLapproaches are still vulnerable to various privacy attacks like reconstructionattacks and membership inference attacks. Although most of these have focusedon computer vision applications, such information leakages exist in the SERsystems trained using the FL technique. To assess the information leakage ofSER systems trained using FL, we propose an attribute inference attackframework that infers sensitive attribute information of the clients fromshared gradients or model parameters, corresponding to the FedSGD and theFedAvg training algorithms, respectively. As a use case, we empiricallyevaluate our approach for predicting the client\u0026rsquo;s gender information usingthree SER benchmark datasets: IEMOCAP, CREMA-D, and MSP-Improv. We show thatthe attribute inference attack is achievable for SER systems trained using FL.We further identify that most information leakage possibly comes from the firstlayer in the SER model.\rInvBERT: Reconstructing Text from Contextualized Word Embeddings by inverting the BERT pipeline\nKai Kugler Simon Münker Johannes Höhmann Achim Rettinger\nabstract\rabstract: Digital Humanities and Computational Literary Studies apply text miningmethods to investigate literature. Such automated approaches enablequantitative studies on large corpora which would not be feasible by manualinspection alone. However, due to copyright restrictions, the availability ofrelevant digitized literary works is limited. Derived Text Formats (DTFs) havebeen proposed as a solution. Here, textual materials are transformed in such away that copyright-critical features are removed, but that the use of certainanalytical methods remains possible. Contextualized word embeddings produced bytransformer-encoders (like BERT) are promising candidates for DTFs because theyallow for state-of-the-art performance on various analytical tasks and, atfirst sight, do not disclose the original text. However, in this paper wedemonstrate that under certain conditions the reconstruction of the originalcopyrighted text becomes feasible and its publication in the form ofcontextualized token representations is not safe. Our attempts to invert BERTsuggest, that publishing the encoder as a black box together with thecontextualized embeddings is critical, since it allows to generate data totrain a decoder with a reconstruction accuracy sufficient to violate copyrightlaws.\r2022-12-21\nIs Your Model Sensitive? SPeDaC: A New Benchmark for Detecting and Classifying Sensitive Personal Data\nGaia Gambarelli Aldo Gangemi Rocco Tripodi\nabstract\rabstract: In recent years, there has been an exponential growth of applications,including dialogue systems, that handle sensitive personal information. Thishas brought to light the extremely important issue of personal data protectionin virtual environments. Sensitive Information Detection (SID) approachesdifferent domains and languages in literature. However, if we refer to thepersonal data domain, a shared benchmark or the absence of an available labeledresource makes comparison with the state-of-the-art difficult. We introduce andrelease SPeDaC , a new annotated resource for the identification of sensitivepersonal data categories in the English language. SPeDaC enables the evaluationof computational models for three different SID subtasks with increasing levelsof complexity. SPeDaC 1 regards binary classification, a model has to detect ifa sentence contains sensitive information or not; whereas, in SPeDaC 2 wecollected labeled sentences using 5 categories that relate to macro-domains ofpersonal information; in SPeDaC 3, the labeling is fine-grained (61 personaldata categories). We conduct an extensive evaluation of the resource usingdifferent state-of-the-art-classifiers. The results show that SPeDaC ischallenging, particularly with regard to fine-grained classification. Thetransformer models achieve the best results (acc. RoBERTa on SPeDaC 1 = 98.20%,DeBERTa on SPeDaC 2 = 95.81% and SPeDaC 3 = 77.63%).\r2022-12-20\nDeduplicating Training Data Mitigates Privacy Risks in Language Models\nNikhil Kandpal Eric Wallace Colin Raffel\nabstract\rabstract: Past work has shown that large language models are susceptible to privacyattacks, where adversaries generate sequences from a trained model and detectwhich sequences are memorized from the training set. In this work, we show thatthe success of these attacks is largely due to duplication in commonly usedweb-scraped training sets. We first show that the rate at which language modelsregenerate training sequences is superlinearly related to a sequence\u0026rsquo;s count inthe training set. For instance, a sequence that is present 10 times in thetraining data is on average generated ~1000 times more often than a sequencethat is present only once. We next show that existing methods for detectingmemorized sequences have near-chance accuracy on non-duplicated trainingsequences. Finally, we find that after applying methods to deduplicate trainingdata, language models are considerably more secure against these types ofprivacy attacks. Taken together, our results motivate an increased focus ondeduplication in privacy-sensitive applications and a reevaluation of thepracticality of existing privacy attacks.\r2022-12-19\nPrivacy Adhering Machine Un-learning in NLP\nVinayshekhar Bannihatti Kumar Rashmi Gangadharaiah Dan Roth\nabstract\rabstract: Regulations introduced by General Data Protection Regulation (GDPR) in the EUor California Consumer Privacy Act (CCPA) in the US have included provisions onthe \\textit{right to be forgotten} that mandates industry applications toremove data related to an individual from their systems. In several real worldindustry applications that use Machine Learning to build models on user data,such mandates require significant effort both in terms of data cleansing aswell as model retraining while ensuring the models do not deteriorate inprediction quality due to removal of data. As a result, continuous removal ofdata and model retraining steps do not scale if these applications receive suchrequests at a very high frequency. Recently, a few researchers proposed theidea of \\textit{Machine Unlearning} to tackle this challenge. Despite thesignificant importance of this task, the area of Machine Unlearning isunder-explored in Natural Language Processing (NLP) tasks. In this paper, weexplore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as,QQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC andSISA-A) to perform \\textit{guaranteed} Unlearning that provides significantreduction in terms of both memory (90-95%), time (100x) and space consumption(99%) in comparison to the baselines while keeping model performance constant.\rKnowledge Unlearning for Mitigating Privacy Risks in Language Models\nJoel Jang Dongkeun Yoon Sohee Yang Sungmin Cha Moontae Lee Lajanugen Logeswaran Minjoon Seo\nabstract\rabstract: Pretrained Language Models (LMs) memorize a vast amount of knowledge duringinitial pretraining, including information that may violate the privacy ofpersonal lives and identities. Previous work addressing privacy issues forlanguage models has mostly focused on data preprocessing and differentialprivacy methods, both requiring re-training the underlying LM. We proposeknowledge unlearning as an alternative method to reduce privacy risks for LMspost hoc. We show that simply performing gradient ascent on target tokensequences is effective at forgetting them with little to no degradation ofgeneral language modeling performances for larger LMs; it sometimes evensubstantially improves the underlying LM with just a few iterations. We alsofind that sequential unlearning is better than trying to unlearn all the dataat once and that unlearning is highly dependent on which kind of data (domain)is forgotten. By showing comparisons with a previous data preprocessing methodand a decoding method known to mitigate privacy risks for LMs, we show thatunlearning can give a stronger empirical privacy guarantee in scenarios wherethe data vulnerable to extraction attacks are known a priori while being muchmore efficient and robust. We release the code and dataset needed to replicateour results at https://github.com/joeljang/knowledge-unlearning.\rReview of security techniques for memristor computing systems\nMinhui Zou Nan Du Shahar Kvatinsky\nabstract\rabstract: Neural network (NN) algorithms have become the dominant tool in visual objectrecognition, natural language processing, and robotics. To enhance thecomputational efficiency of these algorithms, in comparison to the traditionalvon Neuman computing architectures, researchers have been focusing on memristorcomputing systems. A major drawback when using memristor computing systemstoday is that, in the artificial intelligence (AI) era, well-trained NN modelsare intellectual property and, when loaded in the memristor computing systems,face theft threats, especially when running in edge devices. An adversary maysteal the well-trained NN models through advanced attacks such as learningattacks and side-channel analysis. In this paper, we review different securitytechniques for protecting memristor computing systems. Two threat models aredescribed based on their assumptions regarding the adversary\u0026rsquo;s capabilities: ablack-box (BB) model and a white-box (WB) model. We categorize the existingsecurity techniques into five classes in the context of these threat models:thwarting learning attacks (BB), thwarting side-channel attacks (BB), NN modelencryption (WB), NN weight transformation (WB), and fingerprint embedding (WB).We also present a cross-comparison of the limitations of the securitytechniques. This paper could serve as an aid when designing secure memristorcomputing systems.\r2022-12-16\nPlanting and Mitigating Memorized Content in Predictive-Text Language Models\nC. M. Downey Wei Dai Huseyin A. Inan Kim Laine Saurabh Naik Tomasz Religa\nabstract\rabstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both \u0026ldquo;heuristic\u0026rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define\u0026quot;sensitive\u0026quot; or \u0026ldquo;private\u0026rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.\rDense Feature Memory Augmented Transformers for COVID-19 Vaccination Search Classification\nJai Gupta Yi Tay Chaitanya Kamath Vinh Q. Tran Donald Metzler Shailesh Bavadekar Mimi Sun Evgeniy Gabrilovich\nabstract\rabstract: With the devastating outbreak of COVID-19, vaccines are one of the cruciallines of defense against mass infection in this global pandemic. Given theprotection they provide, vaccines are becoming mandatory in certain social andprofessional settings. This paper presents a classification model for detectingCOVID-19 vaccination related search queries, a machine learning model that isused to generate search insights for COVID-19 vaccinations. The proposed methodcombines and leverages advancements from modern state-of-the-art (SOTA) naturallanguage understanding (NLU) techniques such as pretrained Transformers withtraditional dense features. We propose a novel approach of considering densefeatures as memory tokens that the model can attend to. We show that this newmodeling approach enables a significant improvement to the Vaccine SearchInsights (VSI) task, improving a strong well-established gradient-boostingbaseline by relative +15% improvement in F1 score and +14% in precision.\rFewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks\nWeilong Dong Xinwei Wu Junzhuo Li Shuangzhi Wu Chao Bian Deyi Xiong\nabstract\rabstract: Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods.\r2022-12-13\nExploring Consequences of Privacy Policies with Narrative Generation via Answer Set Programming\nChinmaya Dabral Emma Tosch Chris Martens\nabstract\rabstract: Informed consent has become increasingly salient for data privacy and itsregulation. Entities from governments to for-profit companies have addressedconcerns about data privacy with policies that enumerate the conditions forpersonal data storage and transfer. However, increased enumeration of andtransparency in data privacy policies has not improved end-users\u0026rsquo; comprehensionof how their data might be used: not only are privacy policies written in legallanguage that users may struggle to understand, but elements of these policiesmay compose in such a way that the consequences of the policy are notimmediately apparent. We present a framework that uses Answer Set Programming (ASP) \u0026ndash; a type oflogic programming \u0026ndash; to formalize privacy policies. Privacy policies thusbecome constraints on a narrative planning space, allowing end-users toforward-simulate possible consequences of the policy in terms of actors havingroles and taking actions in a domain. We demonstrate through the example of theHealth Insurance Portability and Accountability Act (HIPAA) how to use thesystem in various ways, including asking questions about possibilities andidentifying which clauses of the law are broken by a given sequence of events.\rFNDaaS: Content-agnostic Detection of Fake News sites\nPanagiotis Papadopoulos Dimitris Spithouris Evangelos P. Markatos Nicolas Kourtellis\nabstract\rabstract: Automatic fake news detection is a challenging problem in misinformationspreading, and it has tremendous real-world political and social impacts. Paststudies have proposed machine learning-based methods for detecting such fakenews, focusing on different properties of the published news articles, such aslinguistic characteristics of the actual content, which however havelimitations due to the apparent language barriers. Departing from such efforts,we propose FNDaaS, the first automatic, content-agnostic fake news detectionmethod, that considers new and unstudied features such as network andstructural characteristics per news website. This method can be enforcedas-a-Service, either at the ISP-side for easier scalability and maintenance, oruser-side for better end-user privacy. We demonstrate the efficacy of ourmethod using data crawled from existing lists of 637 fake and 1183 real newswebsites, and by building and testing a proof of concept system thatmaterializes our proposal. Our analysis of data collected from these websitesshows that the vast majority of fake news domains are very young and appear tohave lower time periods of an IP associated with their domain than real newsones. By conducting various experiments with machine learning classifiers, wedemonstrate that FNDaaS can achieve an AUC score of up to 0.967 on past sites,and up to 77-92% accuracy on newly-flagged ones.\r2022-12-12\nEnabling All In-Edge Deep Learning: A Literature Review\nPraveen Joshi Mohammed Hasanuzzaman Chandra Thapa Haithem Afli Ted Scully\nabstract\rabstract: In recent years, deep learning (DL) models have demonstrated remarkableachievements on non-trivial tasks such as speech recognition and naturallanguage understanding. One of the significant contributors to its success isthe proliferation of end devices that acted as a catalyst to provide data fordata-hungry DL models. However, computing DL training and inference is the mainchallenge. Usually, central cloud servers are used for the computation, but itopens up other significant challenges, such as high latency, increasedcommunication costs, and privacy concerns. To mitigate these drawbacks,considerable efforts have been made to push the processing of DL models to edgeservers. Moreover, the confluence point of DL and edge has given rise to edgeintelligence (EI). This survey paper focuses primarily on the fifth level ofEI, called all in-edge level, where DL training and inference (deployment) areperformed solely by edge servers. All in-edge is suitable when the end deviceshave low computing resources, e.g., Internet-of-Things, and other requirementssuch as latency and communication cost are important in mission-criticalapplications, e.g., health care. Firstly, this paper presents all in-edgecomputing architectures, including centralized, decentralized, and distributed.Secondly, this paper presents enabling technologies, such as model parallelismand split learning, which facilitate DL training and deployment at edgeservers. Thirdly, model adaptation techniques based on model compression andconditional computation are described because the standard cloud-based DLdeployment cannot be directly applied to all in-edge due to its limitedcomputational resources. Fourthly, this paper discusses eleven key performancemetrics to evaluate the performance of DL at all in-edge efficiently. Finally,several open research challenges in the area of all in-edge are presented.\rCollaborating Heterogeneous Natural Language Processing Tasks via Federated Learning\nChenhe Dong Yuexiang Xie Bolin Ding Ying Shen Yaliang Li\nabstract\rabstract: The increasing privacy concerns on personal private text data promote thedevelopment of federated learning (FL) in recent years. However, the existingstudies on applying FL in NLP are not suitable to coordinate participants withheterogeneous or private learning objectives. In this study, we further broadenthe application scope of FL in NLP by proposing an Assign-Then-Contrast(denoted as ATC) framework, which enables clients with heterogeneous NLP tasksto construct an FL course and learn useful knowledge from each other.Specifically, the clients are suggested to first perform local training withthe unified tasks assigned by the server rather than using their own learningobjectives, which is called the Assign training stage. After that, in theContrast training stage, clients train with different local learning objectivesand exchange knowledge with other clients who contribute consistent and usefulmodel updates. We conduct extensive experiments on six widely-used datasetscovering both Natural Language Understanding (NLU) and Natural LanguageGeneration (NLG) tasks, and the proposed ATC framework achieves significantimprovements compared with various baseline methods. The source code isavailable at\\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.\r2022-12-05\nExtending Expressive Access Policies with Privacy Features\nStefan More Sebastian Ramacher Lukas Alber Marco Herzl\nabstract\rabstract: Authentication, authorization, and trust verification are central parts of anaccess control system. The conditions for granting access in such a system arecollected in access policies. Since access conditions are often complex,dedicated languages \u0026ndash; policy languages \u0026ndash; for defining policies are in use. However, current policy languages are unable to express such conditionshaving privacy of users in mind. With privacy-preserving technologies, usersare enabled to prove information to the access system without revealing it. In this work, we present a generic design for supporting privacy-preservingtechnologies in policy languages. Our design prevents unnecessary disclosure ofsensitive information while still allowing the formulation of expressive rulesfor access control. For that we make use of zero-knowledge proofs (NIZKs). Wedemonstrate our design by applying it to the TPL policy language, while usingSNARKs. Also, we evaluate the resulting ZK-TPL language and its associatedtoolchain. Our evaluation shows that for regular-sized credentialscommunication and verification overhead is negligible.\r2022-12-04\nA Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification\nKaifa Zhao Le Yu Shiyao Zhou Jing Li Xiapu Luo Yat Fei Aemon Chiu Yutong Liu\nabstract\rabstract: Privacy protection raises great attention on both legal levels and userawareness. To protect user privacy, countries enact laws and regulationsrequiring software privacy policies to regulate their behavior. However,privacy policies are written in natural languages with many legal terms andsoftware jargon that prevent users from understanding and even reading them. Itis desirable to use NLP techniques to analyze privacy policies for helpingusers understand them. Furthermore, existing datasets ignore law requirementsand are limited to English. In this paper, we construct the first Chineseprivacy policy dataset, namely CA4P-483, to facilitate the sequence labelingtasks and regulation compliance identification between privacy policies andsoftware. Our dataset includes 483 Chinese Android application privacypolicies, over 11K sentences, and 52K fine-grained annotations. We evaluatefamilies of robust and representative baseline models on our dataset. Based onbaseline performance, we provide findings and potential research directions onour dataset. Finally, we investigate the potential applications of CA4P-483combing regulation requirements and program analysis.\r2022-12-02\nSemantics-Preserved Distortion for Personal Privacy Protection in Information Management\nJiajia Li Letian Peng Ping Wang Zuchao Li Xueyi Li Hai Zhao\nabstract\rabstract: Although machine learning and especially deep learning methods have played animportant role in the field of information management, privacy protection is animportant and concerning topic in current machine learning models. Ininformation management field, a large number of texts containing personalinformation are produced by users every day. As the model training oninformation from users is likely to invade personal privacy, many methods havebeen proposed to block the learning and memorizing of the sensitive data in rawtexts. In this paper, we try to do this more linguistically via distorting thetext while preserving the semantics. In practice, we leverage a recently ourproposed metric, Neighboring Distribution Divergence, to evaluate the semanticpreservation during the distortion. Based on the metric, we propose twoframeworks for semantics-preserved distortion, a generative one and asubstitutive one. We conduct experiments on named entity recognition,constituency parsing, and machine reading comprehension tasks. Results from ourexperiments show the plausibility and efficiency of our distortion as a methodfor personal privacy protection. Moreover, we also evaluate the attributeattack on three privacy-related tasks in the current natural languageprocessing field, and the results show the simplicity and effectiveness of ourdata-based improvement approach compared to the structural improvementapproach. Further, we also investigate the effects of privacy protection inspecific medical information management in this work and show that the medicalinformation pre-training model using our approach can effectively reduce thememory of patients and symptoms, which fully demonstrates the practicality ofour approach.\r2022-11-30\nA Case for Business Process-Specific Foundation Models\nYara Rizk Praveen Venkateswaran Vatche Isahagian Vinod Muthusamy\nabstract\rabstract: The inception of large language models has helped advance state-of-the-artperformance on numerous natural language tasks. This has also opened the doorfor the development of foundation models for other domains and data modalitiessuch as images, code, and music. In this paper, we argue that business processdata representations have unique characteristics that warrant the developmentof a new class of foundation models to handle tasks like process mining,optimization, and decision making. These models should also tackle the uniquechallenges of applying AI to business processes which include data scarcity,multi-modal representations, domain specific terminology, and privacy concerns.\r2022-11-28\nAttack on Unfair ToS Clause Detection: A Case Study using Universal Adversarial Triggers\nShanshan Xu Irina Broda Rashid Haddad Marco Negrini Matthias Grabmair\nabstract\rabstract: Recent work has demonstrated that natural language processing techniques cansupport consumer protection by automatically detecting unfair clauses in theTerms of Service (ToS) Agreement. This work demonstrates that transformer-basedToS analysis systems are vulnerable to adversarial attacks. We conductexperiments attacking an unfair-clause detector with universal adversarialtriggers. Experiments show that a minor perturbation of the text canconsiderably reduce the detection performance. Moreover, to measure thedetectability of the triggers, we conduct a detailed human evaluation study bycollecting both answer accuracy and response time from the participants. Theresults show that the naturalness of the triggers remains key to trickingreaders.\r2022-11-25\nDeep Learning on a Healthy Data Diet: Finding Important Examples for Fairness\nAbdelrahman Zayed Prasanna Parthasarathi Goncalo Mordido Hamid Palangi Samira Shabanian Sarath Chandar\nabstract\rabstract: Data-driven predictive solutions predominant in commercial applications tendto suffer from biases and stereotypes, which raises equity concerns. Predictionmodels may discover, use, or amplify spurious correlations based on gender orother protected personal characteristics, thus discriminating againstmarginalized groups. Mitigating gender bias has become an important researchfocus in natural language processing (NLP) and is an area where annotatedcorpora are available. Data augmentation reduces gender bias by addingcounterfactual examples to the training dataset. In this work, we show thatsome of the examples in the augmented dataset can be not important or evenharmful for fairness. We hence propose a general method for pruning both thefactual and counterfactual examples to maximize the model\u0026rsquo;s fairness asmeasured by the demographic parity, equality of opportunity, and equality ofodds. The fairness achieved by our method surpasses that of data augmentationon three text classification datasets, using no more than half of the examplesin the augmented dataset. Our experiments are conducted using models of varyingsizes and pre-training settings.\r2022-11-23\nEmerging Biometric Modalities and their Use: Loopholes in the Terminology of the GDPR and Resulting Privacy Risks\nTamas Bisztray Nils Gruschka Thirimachos Bourlai Lothar Fritsch\nabstract\rabstract: Technological advancements allow biometric applications to be moreomnipresent than in any other time before. This paper argues that in thecurrent EU data protection regulation, classification applications usingbiometric data receive less protection compared to biometric recognition. Weanalyse preconditions in the regulatory language and explore how this has thepotential to be the source of unique privacy risks for processing operationsclassifying individuals based on soft traits like emotions. This can have highimpact on personal freedoms and human rights and therefore, should be subjectto data protection impact assessment.\rAgent-Specific Deontic Modality Detection in Legal Language\nAbhilasha Sancheti Aparna Garimella Balaji Vasan Srinivasan Rachel Rudinger\nabstract\rabstract: Legal documents are typically long and written in legalese, which makes itparticularly difficult for laypeople to understand their rights and duties.While natural language understanding technologies can be valuable in supportingsuch understanding in the legal domain, the limited availability of datasetsannotated for deontic modalities in the legal domain, due to the cost of hiringexperts and privacy issues, is a bottleneck. To this end, we introduce,LEXDEMOD, a corpus of English contracts annotated with deontic modalityexpressed with respect to a contracting party or agent along with the modaltriggers. We benchmark this dataset on two tasks: (i) agent-specificmulti-label deontic modality classification, and (ii) agent-specific deonticmodality and trigger span detection using Transformer-based (Vaswani et al.,2017) language models. Transfer learning experiments show that the linguisticdiversity of modal expressions in LEXDEMOD generalizes reasonably from lease toemployment and rental agreements. A small case study indicates that a modeltrained on LEXDEMOD can detect red flags with high recall. We believe our workoffers a new research direction for deontic modality detection in the legaldomain.\r2022-11-22\nGDPR Compliant Collection of Therapist-Patient-Dialogues\nTobias Mayer Neha Warikoo Oliver Grimm Andreas Reif Iryna Gurevych\nabstract\rabstract: According to the Global Burden of Disease list provided by the World HealthOrganization (WHO), mental disorders are among the most debilitatingdisorders.To improve the diagnosis and the therapy effectiveness in recentyears, researchers have tried to identify individual biomarkers. Gatheringneurobiological data however, is costly and time-consuming. Another potentialsource of information, which is already part of the clinical routine, aretherapist-patient dialogues. While there are some pioneering worksinvestigating the role of language as predictors for various therapeuticparameters, for example patient-therapist alliance, there are no large-scalestudies. A major obstacle to conduct these studies is the availability ofsizeable datasets, which are needed to train machine learning models. Whilethese conversations are part of the daily routine of clinicians, gathering themis usually hindered by various ethical (purpose of data usage), legal (dataprivacy) and technical (data formatting) limitations. Some of these limitationsare particular to the domain of therapy dialogues, like the increaseddifficulty in anonymisation, or the transcription of the recordings. In thispaper, we elaborate on the challenges we faced in starting our collection oftherapist-patient dialogues in a psychiatry clinic under the General DataPrivacy Regulation of the European Union with the goal to use the data forNatural Language Processing (NLP) research. We give an overview of each step inour procedure and point out the potential pitfalls to motivate further researchin this field.\r2022-11-18\nDeepHider: A Covert NLP Watermarking Framework Based on Multi-task Learning\nLong Dai Jiarong Mao Xuefeng Fan Xiaoyi Zhou\nabstract\rabstract: Natural language processing (NLP) technology has shown great commercial valuein applications such as sentiment analysis. But NLP models are vulnerable tothe threat of pirated redistribution, damaging the economic interests of modelowners. Digital watermarking technology is an effective means to protect theintellectual property rights of NLP model. The existing NLP model protectionmainly designs watermarking schemes by improving both security and robustnesspurposes, however, the security and robustness of these schemes have thefollowing problems, respectively: (1) Watermarks are difficult to defendagainst fraudulent declaration by adversary and are easily detected and blockedfrom verification by human or anomaly detector during the verification process.(2) The watermarking model cannot meet multiple robustness requirements at thesame time. To solve the above problems, this paper proposes a novelwatermarking framework for NLP model based on the over-parameterization ofdepth model and the multi-task learning theory. Specifically, a covert triggerset is established to realize the perception-free verification of thewatermarking model, and a novel auxiliary network is designed to improve therobustness and security of the watermarking model. The proposed framework wasevaluated on two benchmark datasets and three mainstream NLP models, and theresults show that the framework can successfully validate model ownership with100% validation accuracy and advanced robustness and security withoutcompromising the host model performance.\r2022-11-17\nIncorporating Pre-training Paradigm for Antibody Sequence-Structure Co-design\nKaiyuan Gao Lijun Wu Jinhua Zhu Tianbo Peng Yingce Xia Liang He Shufang Xie Tao Qin Haiguang Liu Kun He Tie-Yan Liu\nabstract\rabstract: Antibodies are versatile proteins that can bind to pathogens and provideeffective protection for human body. Recently, deep learning-basedcomputational antibody design has attracted popular attention since itautomatically mines the antibody patterns from data that could be complementaryto human experiences. However, the computational methods heavily rely onhigh-quality antibody structure data, which is quite limited. Besides, thecomplementarity-determining region (CDR), which is the key component of anantibody that determines the specificity and binding affinity, is highlyvariable and hard to predict. Therefore, the data limitation issue furtherraises the difficulty of CDR generation for antibodies. Fortunately, thereexists a large amount of sequence data of antibodies that can help model theCDR and alleviate the reliance on structure data. By witnessing the success ofpre-training models for protein modeling, in this paper, we develop theantibody pre-training language model and incorporate it into the(antigen-specific) antibody design model in a systemic way. Specifically, wefirst pre-train an antibody language model based on the sequence data, thenpropose a one-shot way for sequence and structure generation of CDR to avoidthe heavy cost and error propagation from an autoregressive manner, and finallyleverage the pre-trained antibody model for the antigen-specific antibodygeneration model with some carefully designed modules. Through variousexperiments, we show that our method achieves superior performances overprevious baselines on different tasks, such as sequence and structuregeneration and antigen-binding CDR-H3 design.\r2022-11-16\n#maskUp: Selective Attribute Encryption for Sensitive Vocalization for English language on Social Media Platforms\nSupriti Vijay Aman Priyanshu\nabstract\rabstract: Social media has become a platform for people to stand up and raise theirvoices against social and criminal acts. Vocalization of such information hasallowed the investigation and identification of criminals. However, revealingsuch sensitive information may jeopardize the victim\u0026rsquo;s safety. We propose#maskUp, a safe method for information communication in a secure fashion to therelevant authorities, discouraging potential bullying of the victim. This wouldensure security by conserving their privacy through natural language processingsupplemented with selective encryption for sensitive attribute masking. To ourknowledge, this is the first work that aims to protect the privacy of thevictims by masking their private details as well as emboldening them to comeforward to report crimes. The use of masking technology allows only bindingauthorities to view/un-mask this data. We construct and evaluate the proposedmethodology on continual learning tasks, allowing practical implementation ofthe same in a real-world scenario. #maskUp successfully demonstrates thisintegration on sample datasets validating the presented objective.\r2022-11-15\nPrivacy Guarantees for De-identifying Text Transformations\nDavid Ifeoluwa Adelani Ali Davody Thomas Kleinbauer Dietrich Klakow\nabstract\rabstract: Machine Learning approaches to Natural Language Processing tasks benefit froma comprehensive collection of real-life user data. At the same time, there is aclear need for protecting the privacy of the users whose data is collected andprocessed. For text collections, such as, e.g., transcripts of voiceinteractions or patient records, replacing sensitive parts with benignalternatives can provide de-identification. However, how much privacy isactually guaranteed by such text transformations, and are the resulting textsstill useful for machine learning? In this paper, we derive formal privacyguarantees for general text transformation-based de-identification methods onthe basis of Differential Privacy. We also measure the effect that differentways of masking private information in dialog transcripts have on a subsequentmachine learning task. To this end, we formulate different masking strategiesand compare their privacy-utility trade-offs. In particular, we compare asimple redact approach with more sophisticated word-by-word replacement usingdeep learning models on multiple natural language understanding tasks likenamed entity recognition, intent detection, and dialog act classification. Wefind that only word-by-word replacement is robust against performance drops invarious tasks.\rA Closer Look at the Calibration of Differentially Private Learners\nHanlin Zhang Xuechen Li Prithviraj Sen Salim Roukos Tatsunori Hashimoto\nabstract\rabstract: We systematically study the calibration of classifiers trained withdifferentially private stochastic gradient descent (DP-SGD) and observemiscalibration across a wide range of vision and language tasks. Our analysisidentifies per-example gradient clipping in DP-SGD as a major cause ofmiscalibration, and we show that existing approaches for improving calibrationwith differential privacy only provide marginal improvements in calibrationerror while occasionally causing large degradations in accuracy. As a solution,we show that differentially private variants of post-processing calibrationmethods such as temperature scaling and Platt scaling are surprisinglyeffective and have negligible utility cost to the overall model. Across 7tasks, temperature scaling and Platt scaling with DP-SGD result in an average3.1-fold reduction in the in-domain expected calibration error and only incurat most a minor percent drop in accuracy.\r2022-11-13\nWatermarking Graph Neural Networks based on Backdoor Attacks\nJing Xu Stefanos Koffas Oguzhan Ersoy Stjepan Picek\nabstract\rabstract: Graph Neural Networks (GNNs) have achieved promising performance in variousreal-world applications. Building a powerful GNN model is not a trivial task,as it requires a large amount of training data, powerful computing resources,and human expertise in fine-tuning the model. Moreover, with the development ofadversarial attacks, e.g., model stealing attacks, GNNs raise challenges tomodel authentication. To avoid copyright infringement on GNNs, verifying theownership of the GNN models is necessary. This paper presents a watermarking framework for GNNs for both graph and nodeclassification tasks. We 1) design two strategies to generate watermarked datafor the graph classification task and one for the node classification task, 2)embed the watermark into the host model through training to obtain thewatermarked GNN model, and 3) verify the ownership of the suspicious model in ablack-box setting. The experiments show that our framework can verify theownership of GNN models with a very high probability (up to $99%$) for bothtasks. Finally, we experimentally show that our watermarking approach is robustagainst a state-of-the-art model extraction technique and four state-of-the-artdefenses against backdoor attacks.\r2022-11-11\nA Federated Approach to Predicting Emojis in Hindi Tweets\nDeep Gandhi Jash Mehta Nirali Parekh Karan Waghela Lynette D\u0026rsquo;Mello Zeerak Talat\nabstract\rabstract: The use of emojis affords a visual modality to, often private, textualcommunication. The task of predicting emojis however provides a challenge formachine learning as emoji use tends to cluster into the frequently used and therarely used emojis. Much of the machine learning research on emoji use hasfocused on high resource languages and has conceptualised the task ofpredicting emojis around traditional server-side machine learning approaches.However, traditional machine learning approaches for private communication canintroduce privacy concerns, as these approaches require all data to betransmitted to a central storage. In this paper, we seek to address the dualconcerns of emphasising high resource languages for emoji prediction andrisking the privacy of people\u0026rsquo;s data. We introduce a new dataset of $118$ktweets (augmented from $25$k unique tweets) for emoji prediction in Hindi, andpropose a modification to the federated learning algorithm, CausalFedGSD, whichaims to strike a balance between model performance and user privacy. We showthat our approach obtains comparative scores with more complex centralisedmodels while reducing the amount of data required to optimise the models andminimising risks to user privacy.\rAnonymization of Whole Slide Images in Histopathology for Research and Education\nTom Bisson Michael Franz Isil Dogan O Daniel Romberg Christoph Jansen Peter Hufnagl Norman Zerbe\nabstract\rabstract: Objective: The exchange of health-related data is subject to regional lawsand regulations, such as the General Data Protection Regulation (GDPR) in theEU or the Health Insurance Portability and Accountability Act (HIPAA) in theUnited States, resulting in non-trivial challenges for researchers andeducators when working with these data. In pathology, the digitization ofdiagnostic tissue samples inevitably generates identifying data that canconsist of sensitive but also acquisition-related information stored invendor-specific file formats. Distribution and off-clinical use of these WholeSlide Images (WSI) is usually done in these formats, as an industry-widestandardization such as DICOM is yet only tentatively adopted and slide scannervendors currently do not provide anonymization functionality. Methods: We developed a guideline for the proper handling ofhistopathological image data particularly for research and education withregard to the GDPR. In this context, we evaluated existing anonymizationmethods and examined proprietary format specifications to identify allsensitive information for the most common WSI formats. This work results in asoftware library that enables GDPR-compliant anonymization of WSIs whilepreserving the native formats. Results: Based on the analysis of proprietary formats, all occurrences ofsensitive information were identified for file formats frequently used inclinical routine, and finally, an open-source programming library with anexecutable CLI-tool and wrappers for different programming languages wasdeveloped. Conclusions: Our analysis showed that there is no straightforward softwaresolution to anonymize WSIs in a GDPR-compliant way while maintaining the dataformat. We closed this gap with our extensible open-source library that worksinstantaneously and offline.\r2022-11-10\nLarge Language Models Can Be Strong Differentially Private Learners\nXuechen Li Florian Tramèr Percy Liang Tatsunori Hashimoto\nabstract\rabstract: Differentially Private (DP) learning has seen limited success for buildinglarge deep learning models of text, and straightforward attempts at applyingDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks haveresulted in large performance drops and high computational overhead. We showthat this performance drop can be mitigated with (1) the use of largepretrained language models; (2) non-standard hyperparameters that suit DPoptimization; and (3) fine-tuning objectives which are aligned with thepretraining procedure. With the above, we obtain NLP models that outperformstate-of-the-art DP-trained models under the same privacy budget and strongnon-private baselines \u0026ndash; by directly fine-tuning pretrained models with DPoptimization on moderately-sized corpora. To address the computationalchallenge of running DP-SGD with large Transformers, we propose a memory savingtechnique that allows clipping in DP-SGD to run without instantiatingper-example gradients for any linear layer in the model. The technique enablesprivately training Transformers with almost the same memory cost as non-privatetraining at a modest run-time overhead. Contrary to conventional wisdom that DPoptimization fails at learning high-dimensional models (due to noise thatscales with dimension) empirical results reveal that private learning withpretrained language models doesn\u0026rsquo;t tend to suffer from dimension-dependentperformance degradation. Code to reproduce results can be found athttps://github.com/lxuechen/private-transformers.\r2022-11-09\nUser-Entity Differential Privacy in Learning Natural Language Models\nPhung Lai NhatHai Phan Tong Sun Rajiv Jain Franck Dernoncourt Jiuxiang Gu Nikolaos Barmpalios\nabstract\rabstract: In this paper, we introduce a novel concept of user-entity differentialprivacy (UeDP) to provide formal privacy protection simultaneously to bothsensitive entities in textual data and data owners in learning natural languagemodels (NLMs). To preserve UeDP, we developed a novel algorithm, calledUeDP-Alg, optimizing the trade-off between privacy loss and model utility witha tight sensitivity bound derived from seamlessly combining user and sensitiveentity sampling processes. An extensive theoretical analysis and evaluationshow that our UeDP-Alg outperforms baseline approaches in model utility underthe same privacy budget consumption on several NLM tasks, using benchmarkdatasets.\r2022-11-07\nInvestigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach\nJiayao Zhang Hongming Zhang Zhun Deng Dan Roth\nabstract\rabstract: Double-blind peer review mechanism has become the skeleton of academicresearch across multiple disciplines including computer science, yet severalstudies have questioned the quality of peer reviews and raised concerns onpotential biases in the process. In this paper, we conduct a thorough andrigorous study on fairness disparities in peer review with the help of largelanguage models (LMs). We collect, assemble, and maintain a comprehensiverelational database for the International Conference on LearningRepresentations (ICLR) conference from 2017 to date by aggregating data fromOpenReview, Google Scholar, arXiv, and CSRanking, and extracting high-levelfeatures using language models. We postulate and study fairness disparities onmultiple protective attributes of interest, including author gender, geography,author, and institutional prestige. We observe that the level of disparitydiffers and textual features are essential in reducing biases in the predictivemodeling. We distill several insights from our analysis on study the peerreview process with the help of large LMs. Our database also provides avenuesfor studying new natural language processing (NLP) methods that facilitate theunderstanding of the peer review mechanism. We study a concrete example towardsautomatic machine review systems and provide baseline models for the reviewgeneration and scoring tasks such that the database can be used as a benchmark.\r2022-11-05\nPrivacy-Preserving Models for Legal Natural Language Processing\nYing Yin Ivan Habernal\nabstract\rabstract: Pre-training large transformer models with in-domain data improves domainadaptation and helps gain performance on the domain-specific downstream tasks.However, sharing models pre-trained on potentially sensitive data is prone toadversarial privacy attacks. In this paper, we asked to which extent we canguarantee privacy of pre-training data and, at the same time, achieve betterdownstream performance on legal tasks without the need of additional labeleddata. We extensively experiment with scalable self-supervised learning oftransformer models under the formal paradigm of differential privacy and showthat under specific training configurations we can improve downstreamperformance without sacrifying privacy protection for the in-domain data. Ourmain contribution is utilizing differential privacy for large-scalepre-training of transformer language models in the legal NLP domain, which, tothe best of our knowledge, has not been addressed before.\rTextual Manifold-based Defense Against Natural Language Adversarial Examples\nDang Minh Nguyen Luu Anh Tuan\nabstract\rabstract: Recent studies on adversarial images have shown that they tend to leave theunderlying low-dimensional data manifold, making them significantly morechallenging for current models to make correct predictions. This so-calledoff-manifold conjecture has inspired a novel line of defenses againstadversarial attacks on images. In this study, we find a similar phenomenonoccurs in the contextualized embedding space induced by pretrained languagemodels, in which adversarial texts tend to have their embeddings diverge fromthe manifold of natural ones. Based on this finding, we propose TextualManifold-based Defense (TMD), a defense mechanism that projects text embeddingsonto an approximated embedding manifold before classification. It reduces thecomplexity of potential adversarial examples, which ultimately enhances therobustness of the protected model. Through extensive experiments, our methodconsistently and significantly outperforms previous defenses under variousattack settings without trading off clean accuracy. To the best of ourknowledge, this is the first NLP defense that leverages the manifold structureagainst adversarial attacks. Our code is available at\\url{https://github.com/dangne/tmd}.\r2022-11-04\nMemorization in NLP Fine-tuning Methods\nFatemehsadat Mireshghallah Archit Uniyal Tianhao Wang David Evans Taylor Berg-Kirkpatrick\nabstract\rabstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the\u0026quot;pre-train and fine-tune\u0026quot; paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.\rQuantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks\nFatemehsadat Mireshghallah Kartik Goyal Archit Uniyal Taylor Berg-Kirkpatrick Reza Shokri\nabstract\rabstract: The wide adoption and application of Masked language models~(MLMs) onsensitive data (from legal to medical) necessitates a thorough quantitativeinvestigation into their privacy vulnerabilities \u0026ndash; to what extent do MLMs leakinformation about their training data? Prior attempts at measuring leakage ofMLMs via membership inference attacks have been inconclusive, implying thepotential robustness of MLMs to privacy attacks. In this work, we posit thatprior attempts were inconclusive because they based their attack solely on theMLM\u0026rsquo;s model score. We devise a stronger membership inference attack based onlikelihood ratio hypothesis testing that involves an additional reference MLMto more accurately quantify the privacy risks of memorization in MLMs. We showthat masked language models are extremely susceptible to likelihood ratiomembership inference attacks: Our empirical results, on models trained onmedical notes, show that our attack improves the AUC of prior membershipinference attacks from 0.66 to an alarmingly high 0.90 level, with asignificant improvement in the low-error region: at 1% false positive rate, ourattack is 51X more powerful than prior work.\r2022-11-03\nVerifying RISC-V Physical Memory Protection\nKevin Cheang Cameron Rasmussen Dayeol Lee David W. Kohlbrenner Krste Asanović Sanjit A. Seshia\nabstract\rabstract: We formally verify an open-source hardware implementation of physical memoryprotection (PMP) in RISC-V, which is a standard feature used for memoryisolation in security critical systems such as the Keystone trusted executionenvironment. PMP provides per-hardware-thread machine-mode control registersthat specify the access privileges for physical memory regions. We firstformalize the functional property of the PMP rules based on the RISC-V ISAmanual. Then, we use the LIME tool to translate an open-source implementationof the PMP hardware module written in Chisel to the UCLID5 formal verificationlanguage. We encode the formal specification in UCLID5 and verify thefunctional correctness of the hardware. This is an initial effort towardsverifying the Keystone framework, where the trusted computing base (TCB) relieson PMP to provide security guarantees such as integrity and confidentiality.\r2022-11-02\nAn Easy-to-use and Robust Approach for the Differentially Private De-Identification of Clinical Textual Documents\nYakini Tchouka Jean-François Couchot David Laiymani\nabstract\rabstract: Unstructured textual data is at the heart of healthcare systems. For obviousprivacy reasons, these documents are not accessible to researchers as long asthey contain personally identifiable information. One way to share this datawhile respecting the legislative framework (notably GDPR or HIPAA) is, withinthe medical structures, to de-identify it, i.e. to detect the personalinformation of a person through a Named Entity Recognition (NER) system andthen replacing it to make it very difficult to associate the document with theperson. The challenge is having reliable NER and substitution tools withoutcompromising confidentiality and consistency in the document. Most of theconducted research focuses on English medical documents with coarsesubstitutions by not benefiting from advances in privacy. This paper shows howan efficient and differentially private de-identification approach can beachieved by strengthening the less robust de-identification method and byadapting state-of-the-art differentially private mechanisms for substitutionpurposes. The result is an approach for de-identifying clinical documents inFrench language, but also generalizable to other languages and whose robustnessis mathematically proven.\r2022-11-01\nShould I disclose my dataset? Caveats between reproducibility and individual data rights\nRaysa M. Benatti Camila M. L. Villarroel Sandra Avila Esther L. Colombini Fabiana C. Severi\nabstract\rabstract: Natural language processing techniques have helped domain experts solve legalproblems. Digital availability of court documents increases possibilities forresearchers, who can access them as a source for building datasets \u0026ndash; whosedisclosure is aligned with good reproducibility practices in computationalresearch. Large and digitized court systems, such as the Brazilian one, areprone to be explored in that sense. However, personal data protection lawsimpose restrictions on data exposure and state principles about whichresearchers should be mindful. Special caution must be taken in cases withhuman rights violations, such as gender discrimination, over which we elaborateas an example of interest. We present legal and ethical considerations on theissue, as well as guidelines for researchers dealing with this kind of data anddeciding whether to disclose it.\r2022-10-31\nFully Adaptive Composition for Gaussian Differential Privacy\nAdam Smith Abhradeep Thakurta\nabstract\rabstract: We show that Gaussian Differential Privacy, a variant of differential privacytailored to the analysis of Gaussian noise addition, composes gracefully evenin the presence of a fully adaptive analyst. Such an analyst selects mechanisms(to be run on a sensitive data set) and their privacy budgets adaptively, thatis, based on the answers from other mechanisms run previously on the same dataset. In the language of Rogers, Roth, Ullman and Vadhan, this gives a filterfor GDP with the same parameters as for nonadaptive composition.\rImproving Cause-of-Death Classification from Verbal Autopsy Reports\nThokozile Manaka Terence van Zyl Deepak Kar\nabstract\rabstract: In many lower-and-middle income countries including South Africa, data accessin health facilities is restricted due to patient privacy and confidentialitypolicies. Further, since clinical data is unique to individual institutions andlaboratories, there are insufficient data annotation standards and conventions.As a result of the scarcity of textual data, natural language processing (NLP)techniques have fared poorly in the health sector. A cause of death (COD) isoften determined by a verbal autopsy (VA) report in places without reliabledeath registration systems. A non-clinician field worker does a VA report usinga set of standardized questions as a guide to uncover symptoms of a COD. Thisanalysis focuses on the textual part of the VA report as a case study toaddress the challenge of adapting NLP techniques in the health domain. Wepresent a system that relies on two transfer learning paradigms of monolinguallearning and multi-source domain adaptation to improve VA narratives for thetarget task of the COD classification. We use the Bidirectional EncoderRepresentations from Transformers (BERT) and Embeddings from Language Models(ELMo) models pre-trained on the general English and health domains to extractfeatures from the VA narratives. Our findings suggest that this transferlearning system improves the COD classification tasks and that the narrativetext contains valuable information for figuring out a COD. Our results furthershow that combining binary VA features and narrative text features learned viathis framework boosts the classification task of COD.\rExtracted BERT Model Leaks More Information than You Think!\nXuanli He Chen Chen Lingjuan Lyu Qiongkai Xu\nabstract\rabstract: The collection and availability of big data, combined with advances inpre-trained models (e.g. BERT), have revolutionized the predictive performanceof natural language processing tasks. This allows corporations to providemachine learning as a service (MLaaS) by encapsulating fine-tuned BERT-basedmodels as APIs. Due to significant commercial interest, there has been a surgeof attempts to steal re mote services via model extraction. Although previousworks have made progress in defending against model extraction attacks, therehas been little discussion on their performance in preventing privacy leakage.This work bridges this gap by launching an attribute inference attack againstthe extracted BERT model. Our extensive experiments reveal that modelextraction can cause severe privacy leakage even when victim models arefacilitated with advanced defensive strategies.\r2022-10-27\nJust Fine-tune Twice: Selective Differential Privacy for Large Language Models\nWeiyan Shi Ryan Shea Si Chen Chiyuan Zhang Ruoxi Jia Zhou Yu\nabstract\rabstract: Protecting large language models from privacy leakage is becomingincreasingly crucial with their wide adoption in real-world products. Yetapplying differential privacy (DP), a canonical notion with provable privacyguarantees for machine learning models, to those models remains challenging dueto the trade-off between model utility and privacy loss. Utilizing the factthat sensitive information in language data tends to be sparse, Shi et al.(2021) formalized a DP notion extension called Selective Differential Privacy(SDP) to protect only the sensitive tokens defined by a policy function.However, their algorithm only works for RNN-based models. In this paper, wedevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP forstate-of-the-art large transformer-based models. Our method is easy toimplement: it first fine-tunes the model with redacted in-domain data, and thenfine-tunes it again with the original in-domain data using a private trainingmechanism. Furthermore, we study the scenario of imperfect implementation ofpolicy functions that misses sensitive tokens and develop systematic methods tohandle it. Experiments show that our method achieves strong utility compared toprevious baselines. We also analyze the SDP privacy guarantee empirically withthe canary insertion attack.\rLearning Location from Shared Elevation Profiles in Fitness Apps: A Privacy Perspective\nUlku Meteriz-Yildiran Necip Fazil Yildiran Joongheon Kim David Mohaisen\nabstract\rabstract: The extensive use of smartphones and wearable devices has facilitated manyuseful applications. For example, with Global Positioning System (GPS)-equippedsmart and wearable devices, many applications can gather, process, and sharerich metadata, such as geolocation, trajectories, elevation, and time. Forexample, fitness applications, such as Runkeeper and Strava, utilize theinformation for activity tracking and have recently witnessed a boom inpopularity. Those fitness tracker applications have their own web platforms andallow users to share activities on such platforms or even with other socialnetwork platforms. To preserve the privacy of users while allowing sharing,several of those platforms may allow users to disclose partial information,such as the elevation profile for an activity, which supposedly would not leakthe location of the users. In this work, and as a cautionary tale, we create aproof of concept where we examine the extent to which elevation profiles can beused to predict the location of users. To tackle this problem, we devise threeplausible threat settings under which the city or borough of the targets can bepredicted. Those threat settings define the amount of information available tothe adversary to launch the prediction attacks. Establishing that simplefeatures of elevation profiles, e.g., spectral features, are insufficient, wedevise both natural language processing (NLP)-inspired text-like representationand computer vision-inspired image-like representation of elevation profiles,and we convert the problem at hand into text and image classification problem.We use both traditional machine learning- and deep learning-based techniquesand achieve a prediction success rate ranging from 59.59% to 99.80%. Thefindings are alarming, highlighting that sharing elevation information may havesignificant location privacy risks.\r2022-10-26\nDifferentially Private Language Models for Secure Data Sharing\nJustus Mattern Zhijing Jin Benjamin Weggenmann Bernhard Schoelkopf Mrinmaya Sachan\nabstract\rabstract: To protect the privacy of individuals whose data is being shared, it is ofhigh importance to develop methods allowing researchers and companies torelease textual data while providing formal privacy guarantees to itsoriginators. In the field of NLP, substantial efforts have been directed atbuilding mechanisms following the framework of local differential privacy,thereby anonymizing individual text samples before releasing them. In practice,these approaches are often dissatisfying in terms of the quality of theiroutput language due to the strong noise required for local differentialprivacy. In this paper, we approach the problem at hand using globaldifferential privacy, particularly by training a generative language model in adifferentially private manner and consequently sampling data from it. Usingnatural language prompts and a new prompt-mismatch loss, we are able to createhighly accurate and fluent textual datasets taking on specific desiredattributes such as sentiment or topic and resembling statistical properties ofthe training data. We perform thorough experiments indicating that oursynthetic datasets do not leak information from our original data and are ofhigh language quality and highly suitable for training models for furtheranalysis on real-world data. Notably, we also demonstrate that trainingclassifiers on private synthetic data outperforms directly training classifierson real data with DP-SGD.\rWhen Does Differentially Private Learning Not Suffer in High Dimensions?\nXuechen Li Daogao Liu Tatsunori Hashimoto Huseyin A. Inan Janardhan Kulkarni Yin Tat Lee Abhradeep Guha Thakurta\nabstract\rabstract: Large pretrained models can be privately fine-tuned to achieve performanceapproaching that of non-private models. A common theme in these results is thesurprising observation that high-dimensional models can achieve favorableprivacy-utility trade-offs. This seemingly contradicts known results on themodel-size dependence of differentially private convex learning and raises thefollowing research question: When does the performance of differentiallyprivate learning not degrade with increasing model size? We identify that themagnitudes of gradients projected onto subspaces is a key factor thatdetermines performance. To precisely characterize this for private convexlearning, we introduce a condition on the objective that we term\\emph{restricted Lipschitz continuity} and derive improved bounds for theexcess empirical and population risks that are dimension-independent underadditional conditions. We empirically show that in private fine-tuning of largelanguage models, gradients obtained during fine-tuning are mostly controlled bya few principal components. This behavior is similar to conditions under whichwe obtain dimension-independent bounds in convex settings. Our theoretical andempirical results together provide a possible explanation for recent successesin large-scale private fine-tuning. Code to reproduce our results can be foundat\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.\r2022-10-25\nOn Robust Incremental Learning over Many Multilingual Steps\nKaran Praharaj Irina Matveeva\nabstract\rabstract: Recent work in incremental learning has introduced diverse approaches totackle catastrophic forgetting from data augmentation to optimized trainingregimes. However, most of them focus on very few training steps. We propose amethod for robust incremental learning over dozens of fine-tuning steps usingdata from a variety of languages. We show that a combination ofdata-augmentation and an optimized training regime allows us to continueimproving the model even for as many as fifty training steps. Crucially, ouraugmentation strategy does not require retaining access to previous trainingdata and is suitable in scenarios with privacy constraints.\rLeveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios\nZhuohao Chen Nikolaos Flemotomos Zac E. Imel David C. Atkins Shrikanth Narayanan\nabstract\rabstract: In psychotherapy interactions, the quality of a session is assessed bycodifying the communicative behaviors of participants during the conversationthrough manual observation and annotation. Developing computational approachesfor automated behavioral coding can reduce the burden on human coders andfacilitate the objective evaluation of the intervention. In the real world,however, implementing such algorithms is associated with data sparsitychallenges since privacy concerns lead to limited available in-domain data. Inthis paper, we leverage a publicly available conversation-based dataset andtransfer knowledge to the low-resource behavioral coding task by performing anintermediate language model training via meta-learning. We introduce a taskaugmentation method to produce a large number of \u0026ldquo;analogy tasks\u0026rdquo; - taskssimilar to the target one - and demonstrate that the proposed frameworkpredicts target behaviors more accurately than all the other baseline models.\r2022-10-23\nIdentifying Crisis Response Communities in Online Social Networks for Compound Disasters: The Case of Hurricane Laura and Covid-19\nKhondhaker Al Momin H M Imran Kays Arif Mohaimin Sadri\nabstract\rabstract: Online social networks allow different agencies and the public to interactand share the underlying risks and protective actions during major disasters.This study revealed such crisis communication patterns during hurricane Lauracompounded by the COVID-19 pandemic. Laura was one of the strongest (Category4) hurricanes on record to make landfall in Cameron, Louisiana. Using theApplication Programming Interface (API), this study utilizes large-scale socialmedia data obtained from Twitter through the recently released academic trackthat provides complete and unbiased observations. The data captured publiclyavailable tweets shared by active Twitter users from the vulnerable areasthreatened by Laura. Online social networks were based on user influencefeature ( mentions or tags) that allows notifying other users while posting atweet. Using network science theories and advanced community detectionalgorithms, the study split these networks into twenty-one components ofvarious sizes, the largest of which contained eight well-defined communities.Several natural language processing techniques (i.e., word clouds, bigrams,topic modeling) were applied to the tweets shared by the users in thesecommunities to observe their risk-taking or risk-averse behavior during a majorcompounding crisis. Social media accounts of local news media, radio,universities, and popular sports pages were among those who involved heavilyand interacted closely with local residents. In contrast, emergency managementand planning units in the area engaged less with the public. The findings ofthis study provide novel insights into the design of efficient social mediacommunication guidelines to respond better in future disasters.\rA Semantic Account of Metric Preservation\nArthur Azevedo de Amorim Marco Gaboardi Justin Hsu Shin-ya Katsumata Ikram Cherigui\nabstract\rabstract: Program sensitivity measures how robust a program is to small changes in itsinput, and is a fundamental notion in domains ranging from differential privacyto cyber-physical systems. A natural way to formalize program sensitivity is interms of metrics on the input and output spaces, requiring that an$r$-sensitive function map inputs that are at distance $d$ to outputs that areat distance at most $r \\cdot d$. Program sensitivity is thus an analogue ofLipschitz continuity for programs. Reed and Pierce introduced Fuzz, a functional language with a linear typesystem that can express program sensitivity. They show soundness operationally,in the form of a metric preservation property. Inspired by their work, we studyprogram sensitivity and metric preservation from a denotational point of view.In particular, we introduce metric CPOs, a novel semantic structure forreasoning about computation on metric spaces, by endowing CPOs with acompatible notion of distance. This structure is useful for reasoning aboutmetric properties of programs, and specifically about program sensitivity. Wedemonstrate metric CPOs by giving a model for the deterministic fragment ofFuzz.\r2022-10-20\nAre Large Pre-Trained Language Models Leaking Your Personal Information?\nJie Huang Hanyin Shao Kevin Chen-Chuan Chang\nabstract\rabstract: Are Large Pre-Trained Language Models Leaking Your Personal Information? Inthis paper, we analyze whether Pre-Trained Language Models (PLMs) are prone toleaking personal information. Specifically, we query PLMs for email addresseswith contexts of the email address or prompts containing the owner\u0026rsquo;s name. Wefind that PLMs do leak personal information due to memorization. However, sincethe models are weak at association, the risk of specific personal informationbeing extracted by attackers is low. We hope this work could help the communityto better understand the privacy risk of PLMs and bring new insights to makePLMs safe.\r2022-10-19\nLAMP: Extracting Text from Gradients with Language Model Priors\nMislav Balunović Dimitar I. Dimitrov Nikola Jovanović Martin Vechev\nabstract\rabstract: Recent work shows that sensitive user data can be reconstructed from gradientupdates, breaking the key privacy promise of federated learning. While successwas demonstrated primarily on image data, these methods do not directlytransfer to other domains such as text. In this work, we propose LAMP, a novelattack tailored to textual data, that successfully reconstructs original textfrom gradients. Our attack is based on two key insights: (i) modeling priortext probability with an auxiliary language model, guiding the search towardsmore natural text, and (ii) alternating continuous and discrete optimization,which minimizes reconstruction loss on embeddings, while avoiding local minimaby applying discrete text transformations. Our experiments demonstrate thatLAMP is significantly more effective than prior work: it reconstructs 5x morebigrams and 23% longer subsequences on average. Moreover, we are the first torecover inputs from batch sizes larger than 1 for textual models. Thesefindings indicate that gradient updates of models operating on textual dataleak more information than previously thought.\r2022-10-18\nRecovering Private Text in Federated Learning of Language Models\nSamyak Gupta Yangsibo Huang Zexuan Zhong Tianyu Gao Kai Li Danqi Chen\nabstract\rabstract: Federated learning allows distributed users to collaboratively train a modelwhile keeping each user\u0026rsquo;s data private. Recently, a growing body of work hasdemonstrated that an eavesdropping attacker can effectively recover image datafrom gradients transmitted during federated learning. However, little progresshas been made in recovering text data. In this paper, we present a novel attackmethod FILM for federated learning of language models (LMs). For the firsttime, we show the feasibility of recovering text from large batch sizes of upto 128 sentences. Unlike image-recovery methods that are optimized to matchgradients, we take a distinct approach that first identifies a set of wordsfrom gradients and then directly reconstructs sentences based on beam searchand a prior-based reordering strategy. We conduct the FILM attack on severallarge-scale datasets and show that it can successfully reconstruct singlesentences with high fidelity for large batch sizes and even multiple sentencesif applied iteratively. We evaluate three defense methods: gradient pruning,DPSGD, and a simple approach to freeze word embeddings that we propose. We showthat both gradient pruning and DPSGD lead to a significant drop in utility.However, if we fine-tune a public pre-trained LM on private text withoutupdating word embeddings, it can effectively defend the attack with minimaldata utility loss. Together, we hope that our results can encourage thecommunity to rethink the privacy concerns of LM training and its standardpractices in the future.\r2022-10-17\nUMLsec4Edge: Extending UMLsec to model data-protection-compliant edge computing systems\nSven Smolka Jan Laufer Zoltán Ádám Mann Klaus Pohl\nabstract\rabstract: Edge computing enables the processing of data - frequently personal data - atthe edge of the network. For personal data, legislation such as the EuropeanGeneral Data Protection Regulation requires data protection by design. Hence,data protection has to be accounted for in the design of edge computing systemswhenever personal data is involved. This leads to specific requirements formodeling the architecture of edge computing systems, e.g., representation ofdata and network properties. To the best of our knowledge, no existing modeling language fulfils all theserequirements. In our previous work we showed that the commonly used UML profileUMLsec fulfils some of these requirements, and can thus serve as a startingpoint. The aim of this paper is to create a modeling language which meets allrequirements concerning the design of the architecture of edge computingsystems accounting for data protection. Thus, we extend UMLsec to satisfy allrequirements. We call the resulting UML profile UMLsec4Edge. We follow asystematic approach to develop UMLsec4Edge. We apply UMLsec4Edge to real-worlduse cases from different domains, and create appropriate deployment diagramsand class diagrams. These diagrams show UMLsec4Edge is capable of meeting therequirements.\r2022-10-13\nMitigating Unintended Memorization in Language Models via Alternating Teaching\nZhe Liu Xuedong Zhang Fuchun Peng\nabstract\rabstract: Recent research has shown that language models have a tendency to memorizerare or unique sequences in the training corpora which can thus leak sensitiveattributes of user data. We employ a teacher-student framework and propose anovel approach called alternating teaching to mitigate unintended memorizationin sequential modeling. In our method, multiple teachers are trained ondisjoint training sets whose privacy one wishes to protect, and teachers\u0026rsquo;predictions supervise the training of a student model in an alternating mannerat each time step. Experiments on LibriSpeech datasets show that the proposedmethod achieves superior privacy-preserving results than other counterparts. Incomparison with no prevention for unintended memorization, the overall utilityloss is small when training records are sufficient.\r2022-10-11\nPromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning\nZifeng Wang Jimeng Sun\nabstract\rabstract: Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) ischallenging due to privacy concerns, which hinders the use of ML for healthcareapplications. Synthetic EHRs generation bypasses the need to share sensitivereal patient records. However, existing methods generate single-modal EHRs byunconditional generation or by longitudinal inference, which falls short of lowflexibility and makes unrealistic EHRs. In this work, we propose to formulateEHRs generation as a text-to-text translation task by language models (LMs),which suffices to highly flexible event imputation during generation. We alsodesign prompt learning to control the generation conditioned by numerical andcategorical demographic features. We evaluate synthetic EHRs quality by twoperplexity measures accounting for their longitudinal pattern (longitudinalimputation perplexity, lpl) and the connections cross modalities(cross-modality imputation perplexity, mpl). Moreover, we utilize twoadversaries: membership and attribute inference attacks for privacy-preservingevaluation. Experiments on MIMIC-III data demonstrate the superiority of ourmethods on realistic EHRs generation (53.1% decrease of lpl and 45.3%decrease of mpl on average compared to the best baselines) with low privacyrisks. Software is available at https://github.com/RyanWangZf/PromptEHR.\rAbstract interpretation of Michelson smart-contracts\nGuillaume Bau Antoine Miné Vincent Botbol Mehdi Bouaziz\nabstract\rabstract: Static analysis of smart-contracts is becoming more widespread on blockchainplatforms. Analyzers rely on techniques like symbolic execution or modelchecking, but few of them can provide strong soundness properties and guaranteethe analysis termination at the same time. As smart-contracts often manipulateeconomic assets, proving numerical properties beyond the absence of runtimeerrors is also desirable. Smart-contract execution models differ considerablyfrom mainstream programming languages and vary from one blockchain to another,making state-of-the-art analyses hard to adapt. For instance, smart-contractcalls may modify a persistent storage impacting subsequent calls. This makes itdifficult for tools to infer invariants required to formally ensure the absenceof exploitable vulnerabilities. The Michelson smart-contract language, used inthe Tezos blockchain, is strongly typed, stack-based, and has a strictexecution model leaving few opportunities for implicit runtime errors. Wepresent a work in progress static analyzer for Michelson based on AbstractInterpretation and implemented within MOPSA, a modular static analyzer. Ourtool supports the Michelson semantic features, including inner calls toexternal contracts. It can prove the absence of runtime errors and inferinvariants on the persistent storage over an unbounded number of calls. It isalso being extended to prove high-level numerical and security properties. CCSConcepts: $\\bullet$ Security and privacy $\\rightarrow$ Logic and verification;$\\bullet$ Software and its engineering $\\rightarrow$ Automated static analysis.\r2022-10-09\nQuantifying Social Biases Using Templates is Unreliable\nPreethi Seshadri Pouya Pezeshkpour Sameer Singh\nabstract\rabstract: Recently, there has been an increase in efforts to understand how largelanguage models (LLMs) propagate and amplify social biases. Several works haveutilized templates for fairness evaluation, which allow researchers to quantifysocial biases in the absence of test sets with protected attribute labels.While template evaluation can be a convenient and helpful diagnostic tool tounderstand model deficiencies, it often uses a simplistic and limited set oftemplates. In this paper, we study whether bias measurements are sensitive tothe choice of templates used for benchmarking. Specifically, we investigate theinstability of bias measurements by manually modifying templates proposed inprevious works in a semantically-preserving manner and measuring bias acrossthese modifications. We find that bias values and resulting conclusions varyconsiderably across template modifications on four tasks, ranging from an 81%reduction (NLI) to a 162% increase (MLM) in (task-specific) bias measurements.Our results indicate that quantifying fairness in LLMs, as done in currentpractice, can be brittle and needs to be approached with more care and caution.\r2022-10-08\nProviding Insights for Open-Response Surveys via End-to-End Context-Aware Clustering\nSoheil Esmaeilzadeh Brian Williams Davood Shamsi Onar Vikingstad\nabstract\rabstract: Teachers often conduct surveys in order to collect data from a predefinedgroup of students to gain insights into topics of interest. When analyzingsurveys with open-ended textual responses, it is extremely time-consuming,labor-intensive, and difficult to manually process all the responses into aninsightful and comprehensive report. In the analysis step, traditionally, theteacher has to read each of the responses and decide on how to group them inorder to extract insightful information. Even though it is possible to groupthe responses only using certain keywords, such an approach would be limitedsince it not only fails to account for embedded contexts but also cannot detectpolysemous words or phrases and semantics that are not expressible in singlewords. In this work, we present a novel end-to-end context-aware framework thatextracts, aggregates, and abbreviates embedded semantic patterns inopen-response survey data. Our framework relies on a pre-trained naturallanguage model in order to encode the textual data into semantic vectors. Theencoded vectors then get clustered either into an optimally tuned number ofgroups or into a set of groups with pre-specified titles. In the former case,the clusters are then further analyzed to extract a representative set ofkeywords or summary sentences that serve as the labels of the clusters. In ourframework, for the designated clusters, we finally provide context-awarewordclouds that demonstrate the semantically prominent keywords within eachgroup. Honoring user privacy, we have successfully built the on-deviceimplementation of our framework suitable for real-time analysis on mobiledevices and have tested it on a synthetic dataset. Our framework reduces thecosts at-scale by automating the process of extracting the most insightfulinformation pieces from survey data.\r2022-10-04\nThe black hole interior from non-isometric codes and complexity\nChris Akers Netta Engelhardt Daniel Harlow Geoff Penington Shreya Vardhan\nabstract\rabstract: Quantum error correction has given us a natural language for the emergence ofspacetime, but the black hole interior poses a challenge for this framework: atlate times the apparent number of interior degrees of freedom in effectivefield theory can vastly exceed the true number of fundamental degrees offreedom, so there can be no isometric (i.e. inner-product preserving) encodingof the former into the latter. In this paper we explain how quantum errorcorrection nonetheless can be used to explain the emergence of the black holeinterior, via the idea of \u0026ldquo;non-isometric codes protected by computationalcomplexity\u0026rdquo;. We show that many previous ideas, such as the existence of a largenumber of \u0026ldquo;null states\u0026rdquo;, a breakdown of effective field theory for operationsof exponential complexity, the quantum extremal surface calculation of the Pagecurve, post-selection, \u0026ldquo;state-dependent/state-specific\u0026rdquo; operatorreconstruction, and the \u0026ldquo;simple entropy\u0026rdquo; approach to complexitycoarse-graining, all fit naturally into this framework, and we illustrate allof these phenomena simultaneously in a soluble model.\rDifferentially Private Bias-Term only Fine-tuning of Foundation Models\nZhiqi Bu Yu-Xiang Wang Sheng Zha George Karypis\nabstract\rabstract: We study the problem of differentially private (DP) fine-tuning of largepre-trained models \u0026ndash; a recent privacy-preserving approach suitable for solvingdownstream tasks with sensitive data. Existing work has demonstrated that highaccuracy is possible under strong privacy constraint, yet requires significantcomputational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), whichmatches the state-of-the-art accuracy for DP algorithms and the efficiency ofthe standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the networkarchitecture), parameter efficient (only training about $0.1%$ of theparameters), and computation efficient (almost removing the overhead caused byDP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiTis $2\\sim 30\\times$ faster and uses $2\\sim 8\\times$ less memory than DP fullfine-tuning, even faster than the standard full fine-tuning. This amazingefficiency enables us to conduct DP fine-tuning on language and vision taskswith long-sequence texts and high-resolution images, which were computationallydifficult using existing methods.\rAn Embarrassingly Simple Approach for Intellectual Property Rights Protection on Recurrent Neural Networks\nZhi Qin Tan Hao Shan Wong Chee Seng Chan\nabstract\rabstract: Capitalise on deep learning models, offering Natural Language Processing(NLP) solutions as a part of the Machine Learning as a Service (MLaaS) hasgenerated handsome revenues. At the same time, it is known that the creation ofthese lucrative deep models is non-trivial. Therefore, protecting theseinventions intellectual property rights (IPR) from being abused, stolen andplagiarized is vital. This paper proposes a practical approach for the IPRprotection on recurrent neural networks (RNN) without all the bells andwhistles of existing IPR solutions. Particularly, we introduce the Gatekeeperconcept that resembles the recurrent nature in RNN architecture to embed keys.Also, we design the model training scheme in a way such that the protected RNNmodel will retain its original performance iff a genuine key is presented.Extensive experiments showed that our protection scheme is robust and effectiveagainst ambiguity and removal attacks in both white-box and black-boxprotection schemes on different RNN variants. Code is available athttps://github.com/zhiqin1998/RecurrentIPR\r2022-09-28\nDoing data science with platforms crumbs: an investigation into fakes views on YouTube\nMaria Castaldo Paolo Frasca Tommaso Venturini Floriana Gargiulo\nabstract\rabstract: This paper contributes to the ongoing discussions on the scholarly access tosocial media data, discussing a case where this access is barred despite itsvalue for understanding and countering online disinformation and despite theabsence of privacy or copyright issues. Our study concerns YouTube\u0026rsquo;s engagementmetrics and, more specifically, the way in which the platform removes \u0026ldquo;fakeviews\u0026rdquo; (i.e., views considered as artificial or illegitimate by the platform).Working with one and a half year of data extracted from a thousand FrenchYouTube channels, we show the massive extent of this phenomenon, which concernsthe large majority of the channels and more than half the videos in our corpus.Our analysis indicates that most fakes news are corrected relatively late inthe life of the videos and that the final view counts of the videos are notindependent from the fake views they received. We discuss the potential harmthat delays in corrections could produce in content diffusion: by inflatingviews counts, illegitimate views could make a video appear more popular than itis and unwarrantedly encourage its human and algorithmic recommendation.Unfortunately, we cannot offer a definitive assessment of this phenomenon,because YouTube provides no information on fake views in its API or interface.This paper is, therefore, also a call for greater transparency by YouTube andother online platforms about information that can have crucial implications forthe quality of online public debate.\r2022-09-23\nFedVLN: Privacy-preserving Federated Vision-and-Language Navigation\nKaiwen Zhou Xin Eric Wang\nabstract\rabstract: Data privacy is a central problem for embodied agents that can perceive theenvironment, communicate with humans, and act in the real world. While helpinghumans complete tasks, the agent may observe and process sensitive informationof users, such as house environments, human activities, etc. In this work, weintroduce privacy-preserving embodied agent learning for the task ofVision-and-Language Navigation (VLN), where an embodied agent navigates houseenvironments by following natural language instructions. We view each houseenvironment as a local client, which shares nothing other than local updateswith the cloud server and other clients, and propose a novel federatedvision-and-language navigation (FedVLN) framework to protect data privacyduring both training and pre-exploration. Particularly, we propose adecentralized training strategy to limit the data of each client to its localmodel training and a federated pre-exploration method to do partial modelaggregation to improve model generalizability to unseen environments. Extensiveresults on R2R and RxR datasets show that under our FedVLN framework,decentralized VLN models achieve comparable results with centralized trainingwhile protecting seen environment privacy, and federated pre-explorationsignificantly outperforms centralized pre-exploration while preserving unseenenvironment privacy.\r2022-09-17\nTopological Quantum Programming in TED-K\nHisham Sati Urs Schreiber\nabstract\rabstract: While the realization of scalable quantum computation will arguably requiretopological stabilization and, with it, topological-hardware-aware quantumprogramming and topological-quantum circuit verification, the propercombination of these strategies into dedicated topological quantum programminglanguages has not yet received attention. Here we describe a fundamental andnatural scheme that we are developing, for typed functional (hence verifiable)topological quantum programming which is topological-hardware aware \u0026ndash; in thatit natively reflects the universal fine technical detail of topological q-bits,namely of symmetry-protected (or enhanced) topologically ordered Laughlin-typeanyon ground states in topological phases of quantum materials. What makes this work is: (1) our recent result that wavefunctions ofrealistic and technologically viable anyon species \u0026ndash; namely of su(2)-anyonssuch as the popular Majorana/Ising anyons but also of computationally universalFibonacci anyons \u0026ndash; are reflected in the twisted equivariant differential (TED)K-cohomology of configuration spaces of codimension=2 nodal defects in the hostmaterial\u0026rsquo;s crystallographic orbifold; (2) combined with our earlier observationthat such TED generalized cohomology theories on orbifolds interpretintuitionistically-dependent linear data types in cohesive homotopy type theory(HoTT), supporting a powerful modern form of modal quantum logic. In this short note we give an exposition of the basic ideas, a quick reviewof the underlying results and a brief indication of the basic languageconstructs for anyon braiding via TED-K in cohesive HoTT. The language systemis under development at the \u0026ldquo;Center for Quantum and Topological Systems\u0026rdquo; at theResearch Institute of NYU, Abu Dhabi.\r2022-09-16\nJaco: An Offline Running Privacy-aware Voice Assistant\nDaniel Bermuth Alexander Poeppel Wolfgang Reif\nabstract\rabstract: With the recent advance in speech technology, smart voice assistants havebeen improved and are now used by many people. But often these assistants arerunning online as a cloud service and are not always known for a goodprotection of users\u0026rsquo; privacy. This paper presents the architecture of a novelvoice assistant, called Jaco, with the following features: (a) It can runcompletely offline, even on low resource devices like a RaspberryPi. (b)Through a skill concept it can be easily extended. (c) The architectural focusis on protecting users\u0026rsquo; privacy, but without restricting capabilities fordevelopers. (d) It supports multiple languages. (e) It is competitive withother voice assistant solutions. In this respect the assistant combines andextends the advantages of other approaches.\r2022-09-15\nContent-Context Factorized Representations for Automated Speech Recognition\nDavid M. Chan Shalini Ghosh\nabstract\rabstract: Deep neural networks have largely demonstrated their ability to performautomated speech recognition (ASR) by extracting meaningful features from inputaudio frames. Such features, however, may consist not only of information aboutthe spoken language content, but also may contain information about unnecessarycontexts such as background noise and sounds or speaker identity, accent, orprotected attributes. Such information can directly harm generalizationperformance, by introducing spurious correlations between the spoken words andthe context in which such words were spoken. In this work, we introduce anunsupervised, encoder-agnostic method for factoring speech-encoderrepresentations into explicit content-encoding representations and spuriouscontext-encoding representations. By doing so, we demonstrate improvedperformance on standard ASR benchmarks, as well as improved performance in bothreal-world and artificially noisy ASR scenarios.\r2022-09-12\nSpanish Facebook Posts as an Indicator of COVID-19 Vaccine Hesitancy in Texas\nAna Aleksandric Henry Isaac Anderson Sarah Melcher Shirin Nilizadeh Gabriela Mustata Wilson\nabstract\rabstract: Vaccination represents a major public health intervention intended to protectagainst COVID-19 infections and hospitalizations. However, vaccine hesitancydue to misinformation/disinformation, especially among ethnic minority groups,negatively impacts the effectiveness of such an intervention. The aim of thestudy is to provide an understanding of how information gleaned from socialmedia can be used to improve attitudes towards vaccination and decrease vaccinehesitancy. This work focused on Spanish-language posts and will highlight therelationship between vaccination rates across different Texas counties and thesentiment and emotional content of Facebook data, the most popular platformamong the Hispanic population. The analysis of this valuable dataset indicatesthat vaccination rates among this minority group are negatively correlated withnegative sentiment and fear, meaning that the higher prevalence of negative andfearful posts reveals lower vaccination rates in these counties. This firststudy investigating vaccine hesitancy in the Hispanic population suggests thatsocial media listening can be a valuable tool for measuring attitudes towardpublic health interventions.\r2022-09-09\nSafety and Performance, Why not Both? Bi-Objective Optimized Model Compression toward AI Software Deployment\nJie Zhu Leye Wang Xiao Han\nabstract\rabstract: The size of deep learning models in artificial intelligence (AI) software isincreasing rapidly, which hinders the large-scale deployment onresource-restricted devices (e.g., smartphones). To mitigate this issue, AIsoftware compression plays a crucial role, which aims to compress model sizewhile keeping high performance. However, the intrinsic defects in the big modelmay be inherited by the compressed one. Such defects may be easily leveraged byattackers, since the compressed models are usually deployed in a large numberof devices without adequate protection. In this paper, we try to address thesafe model compression problem from a safety-performance co-optimizationperspective. Specifically, inspired by the test-driven development (TDD)paradigm in software engineering, we propose a test-driven sparse trainingframework called SafeCompress. By simulating the attack mechanism as the safetytest, SafeCompress can automatically compress a big model to a small onefollowing the dynamic sparse training paradigm. Further, considering arepresentative attack, i.e., membership inference attack (MIA), we develop aconcrete safe model compression mechanism, called MIA-SafeCompress. Extensiveexperiments are conducted to evaluate MIA-SafeCompress on five datasets forboth computer vision and natural language processing tasks. The results verifythe effectiveness and generalization of our method. We also discuss how toadapt SafeCompress to other attacks besides MIA, demonstrating the flexibilityof SafeCompress.\r2022-09-08\nDifferentially Private Decoding in Large Language Models\nJimit Majmudar Christophe Dupuy Charith Peris Sami Smaili Rahul Gupta Richard Zemel\nabstract\rabstract: Recent large-scale natural language processing (NLP) systems use apre-trained Large Language Model (LLM) on massive and diverse corpora as aheadstart. In practice, the pre-trained model is adapted to a wide array oftasks via fine-tuning on task-specific datasets. LLMs, while effective, havebeen shown to memorize instances of training data thereby potentially revealingprivate information processed during pre-training. The potential leakage mightfurther propagate to the downstream tasks for which LLMs are fine-tuned. On theother hand, privacy-preserving algorithms usually involve retraining fromscratch, which is prohibitively expensive for LLMs. In this work, we propose asimple, easy to interpret, and computationally lightweight perturbationmechanism to be applied to an already trained model at the decoding stage. Ourperturbation mechanism is model-agnostic and can be used in conjunction withany LLM. We provide theoretical analysis showing that the proposed mechanism isdifferentially private, and experimental results showing a privacy-utilitytrade-off.\r2022-09-07\nSGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning\nEugenio Lomurno Alberto Archetti Lorenzo Cazzella Stefano Samele Leonardo Di Perna Matteo Matteucci\nabstract\rabstract: Privacy regulation laws, such as GDPR, impose transparency and security asdesign pillars for data processing algorithms. In this context, federatedlearning is one of the most influential frameworks for privacy-preservingdistributed machine learning, achieving astounding results in many naturallanguage processing and computer vision tasks. Several federated learningframeworks employ differential privacy to prevent private data leakage tounauthorized parties and malicious attackers. Many studies, however, highlightthe vulnerabilities of standard federated learning to poisoning and inference,thus raising concerns about potential risks for sensitive data. To address thisissue, we present SGDE, a generative data exchange protocol that improves usersecurity and machine learning performance in a cross-silo federation. The coreof SGDE is to share data generators with strong differential privacy guaranteestrained on private data instead of communicating explicit gradient information.These generators synthesize an arbitrarily large amount of data that retain thedistinctive features of private samples but differ substantially. In this work,SGDE is tested in a cross-silo federated network on images and tabulardatasets, exploiting beta-variational autoencoders as data generators. From theresults, the inclusion of SGDE turns out to improve task accuracy and fairness,as well as resilience to the most influential attacks on federated learning.\r2022-09-02\nDomain Adaptation from Scratch\nEyal Ben-David Yftah Ziser Roi Reichart\nabstract\rabstract: Natural language processing (NLP) algorithms are rapidly improving but oftenstruggle when applied to out-of-distribution examples. A prominent approach tomitigate the domain gap is domain adaptation, where a model trained on a sourcedomain is adapted to a new target domain. We present a new learning setup,``domain adaptation from scratch\u0026rsquo;\u0026rsquo;, which we believe to be crucial forextending the reach of NLP to sensitive domains in a privacy-preserving manner.In this setup, we aim to efficiently annotate data from a set of source domainssuch that the trained model performs well on a sensitive target domain fromwhich data is unavailable for annotation. Our study compares several approachesfor this challenging setup, ranging from data selection and domain adaptationalgorithms to active learning paradigms, on two NLP tasks: sentiment analysisand Named Entity Recognition. Our results suggest that using the abovementionedapproaches eases the domain gap, and combining them further improves theresults.\r2022-08-30\nMINERVAS: Massive INterior EnviRonments VirtuAl Synthesis\nHaocheng Ren Hao Zhang Jia Zheng Jiaxiang Zheng Rui Tang Yuchi Huo Hujun Bao Rui Wang\nabstract\rabstract: With the rapid development of data-driven techniques, data has played anessential role in various computer vision tasks. Many realistic and syntheticdatasets have been proposed to address different problems. However, there arelots of unresolved challenges: (1) the creation of dataset is usually a tediousprocess with manual annotations, (2) most datasets are only designed for asingle specific task, (3) the modification or randomization of the 3D scene isdifficult, and (4) the release of commercial 3D data may encounter copyrightissue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAlSynthesis system, to facilitate the 3D scene modification and the 2D imagesynthesis for various vision tasks. In particular, we design a programmablepipeline with Domain-Specific Language, allowing users to (1) select scenesfrom the commercial indoor scene database, (2) synthesize scenes for differenttasks with customized rules, and (3) render various imagery data, such asvisual color, geometric structures, semantic label. Our system eases thedifficulty of customizing massive numbers of scenes for different tasks andrelieves users from manipulating fine-grained scene configurations by providinguser-controllable randomness using multi-level samplers. Most importantly, itempowers users to access commercial scene databases with millions of indoorscenes and protects the copyright of core data assets, e.g., 3D CAD models. Wedemonstrate the validity and flexibility of our system by using our synthesizeddata to improve the performance on different kinds of computer vision tasks.\r2022-08-29\nNL2GDPR: Automatically Develop GDPR Compliant Android Application Features from Natural Language\nFaysal Hossain Shezan Yingjie Lao Minlong Peng Xin Wang Mingming Sun Ping Li\nabstract\rabstract: The recent privacy leakage incidences and the more strict policy regulationsdemand a much higher standard of compliance for companies and mobile apps.However, such obligations also impose significant challenges on app developersfor complying with these regulations that contain various perspectives,activities, and roles, especially for small companies and developers who areless experienced in this matter or with limited resources. To address thesehurdles, we develop an automatic tool, NL2GDPR, which can generate policiesfrom natural language descriptions from the developer while also ensuring theapp\u0026rsquo;s functionalities are compliant with General Data Protection Regulation(GDPR). NL2GDPR is developed by leveraging an information extraction tool, OIA(Open Information Annotation), developed by Baidu Cognitive Computing Lab. At the core, NL2GDPR is a privacy-centric information extraction model,appended with a GDPR policy finder and a policy generator. We perform acomprehensive study to grasp the challenges in extracting privacy-centricinformation and generating privacy policies, while exploiting optimizations forthis specific task. With NL2GDPR, we can achieve 92.9%, 95.2%, and 98.4%accuracy in correctly identifying GDPR policies related to personal datastorage, process, and share types, respectively. To the best of our knowledge,NL2GDPR is the first tool that allows a developer to automatically generateGDPR compliant policies, with only the need of entering the natural languagefor describing the app features. Note that other non-GDPR-related featuresmight be integrated with the generated features to build a complex app.\r2022-08-24\nFedIPR: Ownership Verification for Federated Deep Neural Network Models\nBowen Li Lixin Fan Hanlin Gu Jie Li Qiang Yang\nabstract\rabstract: Federated learning models are collaboratively developed upon valuabletraining data owned by multiple parties. During the development and deploymentof federated models, they are exposed to risks including illegal copying,re-distribution, misuse and/or free-riding. To address these risks, theownership verification of federated learning models is a prerequisite thatprotects federated learning model intellectual property rights (IPR) i.e.,FedIPR. We propose a novel federated deep neural network (FedDNN) ownershipverification scheme that allows private watermarks to be embedded and verifiedto claim legitimate IPR of FedDNN models. In the proposed scheme, each clientindependently verifies the existence of the model watermarks and claimsrespective ownership of the federated model without disclosing neither privatetraining data nor private watermark information. The effectiveness of embeddedwatermarks is theoretically justified by the rigorous analysis of conditionsunder which watermarks can be privately embedded and detected by multipleclients. Moreover, extensive experimental results on computer vision andnatural language processing tasks demonstrate that varying bit-lengthwatermarks can be embedded and reliably detected without compromising originalmodel performances. Our watermarking scheme is also resilient to variousfederated training settings and robust against removal attacks.\r2022-08-21\nAn Incentive-Compatible Mechanism for Decentralized Storage Network\nIman Vakilinia Weihong Wang Jiajun Xin\nabstract\rabstract: The dominance of a few big companies in the storage market arising variousconcerns including single point of failure, privacy violation, and oligopoly.To eliminate the dependency on such a centralized storage architecture, severalDecentralized Storage Network (DSN) schemes such as Filecoin, Sia, and Storjhave been introduced. DSNs leverage blockchain technology to create a storageplatform such that the micro storage providers can also participate in thestorage market. To verify the accurate data storage by the storage providersduring a storage contract, DSNs apply a Proof of Storage (PoS) scheme tocontinuously inspect the storage service. However, continuous verification ofthe storage provider imposes an extra cost to the network and thereforeend-users. Moreover, DSN\u0026rsquo;s PoS verification is vulnerable to a service denyingattack in which the storage provider submits valid PoS to the network whiledenying the service to the client. Considering the benefits and existing challenges of DSNs, this paperintroduces a novel incentive-compatible DSN scheme. In this scheme, the PoS isconducted only if the client submits a challenge request. We model the storageservice as a repeated dynamic game and set the players\u0026rsquo; payoffs such that thestorage provider\u0026rsquo;s dominant strategy is to honestly follow the storagecontract. Our proposed mechanism leverages the smart-contract and oraclenetwork to govern the storage agreement between the client and storage providerefficiently. Furthermore, our scheme is independent of a specific blockchainplatform but can be plugged into any blockchain platform with smart-contractexecution capability. As a proof of concept, we have implemented our schemeusing solidity language and chainlink oracle network. The performance analysisdemonstrates the applicability of our scheme.\rMockingBERT: A Method for Retroactively Adding Resilience to NLP Models\nJan Jezabek Akash Singh\nabstract\rabstract: Protecting NLP models against misspellings whether accidental or adversarialhas been the object of research interest for the past few years. Existingremediations have typically either compromised accuracy or required full modelre-training with each new class of attacks. We propose a novel method ofretroactively adding resilience to misspellings to transformer-based NLPmodels. This robustness can be achieved without the need for re-training of theoriginal NLP model and with only a minimal loss of language understandingperformance on inputs without misspellings. Additionally we propose a newefficient approximate method of generating adversarial misspellings, whichsignificantly reduces the cost needed to evaluate a model\u0026rsquo;s resilience toadversarial attacks.\r2022-08-19\nTo show or not to show: Redacting sensitive text from videos of electronic displays\nAbhishek Mukhopadhyay Shubham Agarwal Patrick Dylan Zwick Pradipta Biswas\nabstract\rabstract: With the increasing prevalence of video recordings there is a growing needfor tools that can maintain the privacy of those recorded. In this paper, wedefine an approach for redacting personally identifiable text from videos usinga combination of optical character recognition (OCR) and natural languageprocessing (NLP) techniques. We examine the relative performance of thisapproach when used with different OCR models, specifically Tesseract and theOCR system from Google Cloud Vision (GCV). For the proposed approach theperformance of GCV, in both accuracy and speed, is significantly higher thanTesseract. Finally, we explore the advantages and disadvantages of both modelsin real-world applications.\r2022-08-17\nDifferential Privacy in Natural Language Processing: The Story So Far\nOleksandra Klymenko Stephen Meisenbacher Florian Matthes\nabstract\rabstract: As the tide of Big Data continues to influence the landscape of NaturalLanguage Processing (NLP), the utilization of modern NLP methods has groundeditself in this data, in order to tackle a variety of text-based tasks. Thesemethods without a doubt can include private or otherwise personallyidentifiable information. As such, the question of privacy in NLP has gainedfervor in recent years, coinciding with the development of newPrivacy-Enhancing Technologies (PETs). Among these PETs, Differential Privacyboasts several desirable qualities in the conversation surrounding dataprivacy. Naturally, the question becomes whether Differential Privacy isapplicable in the largely unstructured realm of NLP. This topic has sparkednovel research, which is unified in one basic goal: how can one adaptDifferential Privacy to NLP methods? This paper aims to summarize thevulnerabilities addressed by Differential Privacy, the current thinking, andabove all, the crucial next steps that must be considered.\r2022-08-14\nSimply Logical \u0026ndash; Intelligent Reasoning by Example (Fully Interactive Online Edition)\nPeter Flach Kacper Sokol\nabstract\rabstract: \u0026ldquo;Simply Logical \u0026ndash; Intelligent Reasoning by Example\u0026rdquo; by Peter Flach was firstpublished by John Wiley in 1994. It could be purchased as book-only or with a3.5 inch diskette containing the SWI-Prolog programmes printed in the book (forvarious operating systems). In 2007 the copyright reverted back to the authorat which point the book and programmes were made freely available online; theprint version is no longer distributed through John Wiley publishers. In 2015,as a pilot, we ported most of the original book into an online, interactivewebsite using SWI-Prolog\u0026rsquo;s SWISH platform. Since then, we launched the SimplyLogical open source organisation committed to maintaining a suite of freelyavailable interactive online educational resources about ArtificialIntelligence and Logic Programming with Prolog. With the advent of neweducational technologies we were inspired to rebuild the book from the groundup using the Jupyter Book platform enhanced with a collection of bespokeplugins that implement, among other things, interactive SWI-Prolog code blocksthat can be executed directly in a web browser. This new version is moremodular, easier to maintain, and can be split into custom teaching modules, inaddition to being modern-looking, visually appealing, and compatible with arange of (mobile) devices of varying screen sizes.\r2022-08-11\nSearching for chromate replacements using natural language processing and machine learning algorithms\nShujing Zhao Nick Birbilis\nabstract\rabstract: The past few years has seen the application of machine learning utilised inthe exploration of new materials. As in many fields of research - the vastmajority of knowledge is published as text, which poses challenges in either aconsolidated or statistical analysis across studies and reports. Suchchallenges include the inability to extract quantitative information, and inaccessing the breadth of non-numerical information. To address this issue, theapplication of natural language processing (NLP) has been explored in severalstudies to date. In NLP, assignment of high-dimensional vectors, known asembeddings, to passages of text preserves the syntactic and semanticrelationship between words. Embeddings rely on machine learning algorithms andin the present work, we have employed the Word2Vec model, previously exploredby others, and the BERT model - applying them towards a unique challenge inmaterials engineering. That challenge is the search for chromate replacementsin the field of corrosion protection. From a database of over 80 millionrecords, a down-selection of 5990 papers focused on the topic of corrosionprotection were examined using NLP. This study demonstrates it is possible toextract knowledge from the automated interpretation of the scientificliterature and achieve expert human level insights.\r2022-08-03\nA Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval\nAlex Falcon Giuseppe Serra Oswald Lanz\nabstract\rabstract: Every hour, huge amounts of visual contents are posted on social media anduser-generated content platforms. To find relevant videos by means of a naturallanguage query, text-video retrieval methods have received increased attentionover the past few years. Data augmentation techniques were introduced toincrease the performance on unseen test examples by creating new trainingsamples with the application of semantics-preserving techniques, such as colorspace or geometric transformations on images. Yet, these techniques are usuallyapplied on raw data, leading to more resource-demanding solutions and alsorequiring the shareability of the raw data, which may not always be true, e.g.copyright issues with clips from movies or TV series. To address thisshortcoming, we propose a multimodal data augmentation technique which works inthe feature space and creates new videos and captions by mixing semanticallysimilar samples. We experiment our solution on a large scale public dataset,EPIC-Kitchens-100, and achieve considerable improvements over a baselinemethod, improved state-of-the-art performance, while at the same timeperforming multiple ablation studies. We release code and pretrained models onGithub at https://github.com/aranciokov/FSMMDA_VideoRetrieval.\r2022-08-01\nFederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning\nZhen Wang Weirui Kuang Yuexiang Xie Liuyi Yao Yaliang Li Bolin Ding Jingren Zhou\nabstract\rabstract: The incredible development of federated learning (FL) has benefited varioustasks in the domains of computer vision and natural language processing, andthe existing frameworks such as TFF and FATE has made the deployment easy inreal-world applications. However, federated graph learning (FGL), even thoughgraph data are prevalent, has not been well supported due to its uniquecharacteristics and requirements. The lack of FGL-related framework increasesthe efforts for accomplishing reproducible research and deploying in real-worldapplications. Motivated by such strong demand, in this paper, we first discussthe challenges in creating an easy-to-use FGL package and accordingly presentour implemented package FederatedScope-GNN (FS-G), which provides (1) a unifiedview for modularizing and expressing FGL algorithms; (2) comprehensive DataZooand ModelZoo for out-of-the-box FGL capability; (3) an efficient modelauto-tuning component; and (4) off-the-shelf privacy attack and defenseabilities. We validate the effectiveness of FS-G by conducting extensiveexperiments, which simultaneously gains many valuable insights about FGL forthe community. Moreover, we employ FS-G to serve the FGL application inreal-world E-commerce scenarios, where the attained improvements indicate greatpotential business benefits. We publicly release FS-G, as submodules ofFederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL\u0026rsquo;sresearch and enable broad applications that would otherwise be infeasible dueto the lack of a dedicated package.\r2022-07-29\nBlade: A Blockchain-supported Architecture for Decentralized Services\nSebastian Göndör Hakan Yildiz Martin Westerkamp Axel Küpper\nabstract\rabstract: Decentralized services and applications provide a multitude of advantages fortheir users, such as improved privacy, control, and independence from thirdparties. Anyhow, decentralization comes at the cost of certain disadvantages,such as increased application complexity or communication overhead. Thisaggravates the development and deployment of decentralized services andapplications. In this paper we present Blade, a software platform that aims toease the effort of development, deployment, and administration of decentralizedservices by implementing reusable solutions for recurring challenges developersare facing when designing decentralized service architectures. This includesfunctionality for e.g. identity management, access control, request handling,verification of authenticity and integrity, discovery, or routing. Bladeimplements all this functionality in a Blade server instance, which can bedeployed on a lightweight device, such as a NAS, Raspberry Pi, or router athome. This allows users without expert knowledge to run a Blade instance withalready existing hardware with little overhead. Blade supports polyglot Blademodules that implement extended functionality, such as interfaces, frontends,and business logic of decentralized applications, e.g. a decentralized instantmessaging service or an online social network. Based on the Oracle GraalVM,Blade modules can be implemented in a variety of programming languages andutilize the functionality provided by the Blade server instance. Blade modulesare published in a Ethereum-based decentralized marketplace from where they canbe installed directly via the Blade instances\u0026hellip;\r2022-07-28\nExploiting and Defending Against the Approximate Linearity of Apple\u0026rsquo;s NeuralHash\nJagdeep Singh Bhatia Kevin Meng\nabstract\rabstract: Perceptual hashes map images with identical semantic content to the same$n$-bit hash value, while mapping semantically-different images to differenthashes. These algorithms carry important applications in cybersecurity such ascopyright infringement detection, content fingerprinting, and surveillance.Apple\u0026rsquo;s NeuralHash is one such system that aims to detect the presence ofillegal content on users\u0026rsquo; devices without compromising consumer privacy. Wemake the surprising discovery that NeuralHash is approximately linear, whichinspires the development of novel black-box attacks that can (i) evadedetection of \u0026ldquo;illegal\u0026rdquo; images, (ii) generate near-collisions, and (iii) leakinformation about hashed images, all without access to model parameters. Thesevulnerabilities pose serious threats to NeuralHash\u0026rsquo;s security goals; to addressthem, we propose a simple fix using classical cryptographic standards.\r2022-07-23\nCatch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter\nDilara Dogan Bahadir Altun Muhammed Said Zengin Mucahid Kutlu Tamer Elsayed\nabstract\rabstract: The recent advances in natural language processing have yielded many excitingdevelopments in text analysis and language understanding models; however, thesemodels can also be used to track people, bringing severe privacy concerns. Inthis work, we investigate what individuals can do to avoid being detected bythose models while using social media platforms. We ground our investigation intwo exposure-risky tasks, stance detection and geotagging. We explore a varietyof simple techniques for modifying text, such as inserting typos in salientwords, paraphrasing, and adding dummy social media posts. Our experiments showthat the performance of BERT-based models fined tuned for stance detectiondecreases significantly due to typos, but it is not affected by paraphrasing.Moreover, we find that typos have minimal impact on state-of-the-art geotaggingmodels due to their increased reliance on social networks; however, we showthat users can deceive those models by interacting with different users,reducing their performance by almost 50%.\r2022-07-20\nA Large-Scale Dataset of Twitter Chatter about Online Learning during the Current COVID-19 Omicron Wave\nNirmalya Thakur\nabstract\rabstract: The COVID-19 Omicron variant, reported to be the most immune evasive variantof COVID-19, is resulting in a surge of COVID-19 cases globally. This hascaused schools, colleges, and universities in different parts of the world totransition to online learning. As a result, social media platforms such asTwitter are seeing an increase in conversations related to online learning inthe form of tweets. Mining such tweets to develop a dataset can serve as a dataresource for different applications and use-cases related to the analysis ofinterest, views, opinions, perspectives, attitudes, and feedback towards onlinelearning during the current surge of COVID-19 cases caused by the Omicronvariant. Therefore, this work presents a large-scale open-access Twitterdataset of conversations about online learning from different parts of theworld since the first detected case of the COVID-19 Omicron variant in November2021. The dataset is compliant with the privacy policy, developer agreement,and guidelines for content redistribution of Twitter, as well as with the FAIRprinciples (Findability, Accessibility, Interoperability, and Reusability)principles for scientific data management. The paper also briefly outlines somepotential applications in the fields of Big Data, Data Mining, Natural LanguageProcessing, and their related disciplines, with a specific focus on onlinelearning during this Omicron wave that may be studied, explored, andinvestigated by using this dataset.\r2022-07-18\nTraining Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices\nMingbin Xu Congzheng Song Ye Tian Neha Agrawal Filip Granqvist Rogier van Dalen Xiao Zhang Arturo Argueta Shiyi Han Yaqiao Deng Leo Liu Anmol Walia Alex Jin\nabstract\rabstract: Federated Learning (FL) is a technique to train models using data distributedacross devices. Differential Privacy (DP) provides a formal privacy guaranteefor sensitive data. Our goal is to train a large neural network language model(NNLM) on compute-constrained devices while preserving privacy using FL and DP.However, the DP-noise introduced to the model increases as the model sizegrows, which often prevents convergence. We propose Partial Embedding Updates(PEU), a novel technique to decrease noise by decreasing payload size.Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise ContrastiveEstimation (NCE) to reduce the memory demands of large models oncompute-constrained devices. This combination of techniques makes it possibleto train large-vocabulary language models while preserving accuracy andprivacy.\r2022-07-17\nLearning with Recoverable Forgetting\nJingwen Ye Yifang Fu Jie Song Xingyi Yang Songhua Liu Xin Jin Mingli Song Xinchao Wang\nabstract\rabstract: Life-long learning aims at learning a sequence of tasks without forgettingthe previously acquired knowledge. However, the involved training data may notbe life-long legitimate due to privacy or copyright reasons. In practicalscenarios, for instance, the model owner may wish to enable or disable theknowledge of specific tasks or specific samples from time to time. Suchflexible control over knowledge transfer, unfortunately, has been largelyoverlooked in previous incremental or decremental learning methods, even at aproblem-setup level. In this paper, we explore a novel learning scheme, termedas Learning wIth Recoverable Forgetting (LIRF), that explicitly handles thetask- or sample-specific knowledge removal and recovery. Specifically, LIRFbrings in two innovative schemes, namely knowledge deposit and withdrawal,which allow for isolating user-designated knowledge from a pre-trained networkand injecting it back when necessary. During the knowledge deposit process, thespecified knowledge is extracted from the target network and stored in adeposit module, while the insensitive or general knowledge of the targetnetwork is preserved and further augmented. During knowledge withdrawal, thetaken-off knowledge is added back to the target network. The deposit andwithdraw processes only demand for a few epochs of finetuning on the removaldata, ensuring both data and time efficiency. We conduct experiments on severaldatasets, and demonstrate that the proposed LIRF strategy yields encouragingresults with gratifying generalization capability.\r2022-07-16\nSelective Differential Privacy for Language Modeling\nWeiyan Shi Aiqi Cui Evan Li Ruoxi Jia Zhou Yu\nabstract\rabstract: With the increasing applications of language models, it has become crucial toprotect these models from leaking private information. Previous work hasattempted to tackle this challenge by training RNN-based language models withdifferential privacy guarantees. However, applying classical differentialprivacy to language models leads to poor model performance as the underlyingprivacy notion is over-pessimistic and provides undifferentiated protection forall tokens in the data. Given that the private information in natural languageis sparse (for example, the bulk of an email might not carry personallyidentifiable information), we propose a new privacy notion, selectivedifferential privacy, to provide rigorous privacy guarantees on the sensitiveportion of the data to improve model utility. To realize such a new notion, wedevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-basedlanguage models. Besides language modeling, we also apply the method to a moreconcrete application\u0026ndash;dialog systems. Experiments on both language modeling anddialog system building show that the proposed privacy-preserving mechanismachieves better utilities while remaining safe under various privacy attackscompared to the baselines. The data and code are released athttps://github.com/wyshi/lm_privacy to facilitate future research .\r2022-07-14\nDifferentially Private Fine-tuning of Language Models\nDa Yu Saurabh Naik Arturs Backurs Sivakanth Gopi Huseyin A. Inan Gautam Kamath Janardhan Kulkarni Yin Tat Lee Andre Manoel Lukas Wutschitz Sergey Yekhanin Huishuai Zhang\nabstract\rabstract: We give simpler, sparser, and faster algorithms for differentially privatefine-tuning of large-scale pre-trained language models, which achieve thestate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.We propose a meta-framework for this problem, inspired by the recent success ofhighly parameter-efficient methods for fine-tuning. Our experiments show thatdifferentially private adaptations of these approaches outperform previousprivate algorithms in three important dimensions: utility, privacy, and thecomputational and memory cost of private training. On many commonly studieddatasets, the utility of private models approaches that of non-private models.For example, on the MNLI dataset we achieve an accuracy of $87.8%$ usingRoBERTa-Large and $83.5%$ using RoBERTa-Base with a privacy budget of$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Largeachieves an accuracy of $90.2%$. Our findings are similar for natural languagegeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8respectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas thenon-private baseline is $48.1$. All our experiments suggest that larger modelsare better suited for private fine-tuning: while they are well known to achievesuperior accuracy non-privately, we find that they also better maintain theiraccuracy when privacy is introduced.\r2022-07-13\nRepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping\nPatrik Joslin Kenfack Kamil Sabbagh Adín Ramírez Rivera Adil Khan\nabstract\rabstract: Fairness has become an essential problem in many domains of Machine Learning(ML), such as classification, natural language processing, and GenerativeAdversarial Networks (GANs). In this research effort, we study the unfairnessof GANs. We formally define a new fairness notion for generative models interms of the distribution of generated samples sharing the same protectedattributes (gender, race, etc.). The defined fairness notion (representationalfairness) requires the distribution of the sensitive attributes at the testtime to be uniform, and, in particular for GAN model, we show that thisfairness notion is violated even when the dataset contains equally representedgroups, i.e., the generator favors generating one group of samples over theothers at the test time. In this work, we shed light on the source of thisrepresentation bias in GANs along with a straightforward method to overcomethis problem. We first show on two widely used datasets (MNIST, SVHN) that whenthe norm of the gradient of one group is more important than the other duringthe discriminator\u0026rsquo;s training, the generator favours sampling data from onegroup more than the other at test time. We then show that controlling thegroups\u0026rsquo; gradient norm by performing group-wise gradient norm clipping in thediscriminator during the training leads to a more fair data generation in termsof representational fairness compared to existing models while preserving thequality of generated samples.\r2022-07-10\nDeveloping an NLP-based Recommender System for the Ethical, Legal, and Social Implications of Synthetic Biology\nDamien Dablain Lilian Huang Brandon Sepulvado\nabstract\rabstract: Synthetic biology is an emerging field that involves the engineering andre-design of organisms for purposes such as food security, health, andenvironmental protection. As such, it poses numerous ethical, legal, and socialimplications (ELSI) for researchers and policy makers. Various efforts toensure socially responsible synthetic biology are underway. Policy making isone regulatory avenue, and other initiatives have sought to embed socialscientists and ethicists on synthetic biology projects. However, given thenascency of synthetic biology, the number of heterogeneous domains it spans,and the open nature of many ethical questions, it has proven challenging toestablish widespread concrete policies, and including social scientists andethicists on synthetic biology teams has met with mixed success. This text proposes a different approach, asking instead is it possible todevelop a well-performing recommender model based upon natural languageprocessing (NLP) to connect synthetic biologists with information on the ELSIof their specific research? This recommender was developed as part of a largerproject building a Synthetic Biology Knowledge System (SBKS) to acceleratediscovery and exploration of the synthetic biology design space. Our approachaims to distill for synthetic biologists relevant ethical and social scientificinformation and embed it into synthetic biology research workflows.\r2022-07-06\nCommunication Analysis through Visual Analytics: Current Practices, Challenges, and New Frontiers\nMaximilian T. Fischer Frederik L. Dennig Daniel Seebacher Daniel A. Keim Mennatallah El-Assady\nabstract\rabstract: The automated analysis of digital human communication data often focuses onspecific aspects such as content or network structure in isolation. This canprovide limited perspectives while making cross-methodological analyses,occurring in domains like investigative journalism, difficult. Communicationresearch in psychology and the digital humanities instead stresses theimportance of a holistic approach to overcome these limiting factors. In thiswork, we conduct an extensive survey on the properties of over fortysemi-automated communication analysis systems and investigate how they coverconcepts described in theoretical communication research. From theseinvestigations, we derive a design space and contribute a conceptual frameworkbased on communication research, technical considerations, and the surveyedapproaches. The framework describes the systems\u0026rsquo; properties, capabilities, andcomposition through a wide range of criteria organized in the dimensions (1)Data, (2) Processing and Models, (3) Visual Interface, and (4) KnowledgeGeneration. These criteria enable a formalization of digital communicationanalysis through visual analytics, which, we argue, is uniquely suited for thistask by tackling automation complexity while leveraging domain knowledge. Withour framework, we identify shortcomings and research challenges, such as groupcommunication dynamics, trust and privacy considerations, and holisticapproaches. Simultaneously, our framework supports the evaluation of systemsand promotes the mutual exchange between researchers through a structuredcommon language, laying the foundations for future research on communicationanalysis.\r2022-07-05\nFederated Phish Bowl: LSTM-Based Decentralized Phishing Email Detection\nYuwei Sun Ng Chong Hideya Ochiai\nabstract\rabstract: With increasingly more sophisticated phishing campaigns in recent years,phishing emails lure people using more legitimate-looking personal contexts. Totackle this problem, instead of traditional heuristics-based algorithms, moreadaptive detection systems such as natural language processing (NLP)-poweredapproaches are essential to understanding phishing text representations.Nevertheless, concerns surrounding the collection of phishing data that mightcover confidential information hinder the effectiveness of model learning. Wepropose a decentralized phishing email detection framework called FederatedPhish Bowl (FedPB) which facilitates collaborative phishing detection withprivacy. In particular, we devise a knowledge-sharing mechanism with federatedlearning (FL). Using long short-term memory (LSTM) for phishing detection, theframework adapts by sharing a global word embedding matrix across the clients,with each client running its local model with Non-IID data. We collected themost recent phishing samples to study the effectiveness of the proposed methodusing different client numbers and data distributions. The results show thatFedPB can attain a competitive performance with a centralized phishingdetector, with generality to various cases of FL retaining a predictionaccuracy of 83%.\r2022-07-01\nThe Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization\nIldikó Pilán Pierre Lison Lilja Øvrelid Anthi Papadopoulou David Sánchez Montserrat Batet\nabstract\rabstract: We present a novel benchmark and associated evaluation metrics for assessingthe performance of text anonymization methods. Text anonymization, defined asthe task of editing a text document to prevent the disclosure of personalinformation, currently suffers from a shortage of privacy-oriented annotatedtext resources, making it difficult to properly evaluate the level of privacyprotection offered by various anonymization methods. This paper presents TAB(Text Anonymization Benchmark), a new, open-source annotated corpus developedto address this shortage. The corpus comprises 1,268 English-language courtcases from the European Court of Human Rights (ECHR) enriched withcomprehensive annotations about the personal information appearing in eachdocument, including their semantic category, identifier type, confidentialattributes, and co-reference relations. Compared to previous work, the TABcorpus is designed to go beyond traditional de-identification (which is limitedto the detection of predefined semantic categories), and explicitly marks whichtext spans ought to be masked in order to conceal the identity of the person tobe protected. Along with presenting the corpus and its annotation layers, wealso propose a set of evaluation metrics that are specifically tailored towardsmeasuring the performance of text anonymization, both in terms of privacyprotection and utility preservation. We illustrate the use of the benchmark andthe proposed metrics by assessing the empirical performance of several baselinetext anonymization models. The full corpus along with its privacy-orientedannotation guidelines, evaluation scripts and baseline models are available on:https://github.com/NorskRegnesentral/text-anonymisation-benchmark\r2022-06-28\nThe NLP Sandbox: an efficient model-to-data system to enable federated and unbiased evaluation of clinical NLP models\nYao Yan Thomas Yu Kathleen Muenzen Sijia Liu Connor Boyle George Koslowski Jiaxin Zheng Nicholas Dobbins Clement Essien Hongfang Liu Larsson Omberg Meliha Yestigen Bradley Taylor James A Eddy Justin Guinney Sean Mooney Thomas Schaffter\nabstract\rabstract: Objective The evaluation of natural language processing (NLP) models forclinical text de-identification relies on the availability of clinical notes,which is often restricted due to privacy concerns. The NLP Sandbox is anapproach for alleviating the lack of data and evaluation frameworks for NLPmodels by adopting a federated, model-to-data approach. This enables unbiasedfederated model evaluation without the need for sharing sensitive data frommultiple institutions. Materials and Methods We leveraged the Synapsecollaborative framework, containerization software, and OpenAPI generator tobuild the NLP Sandbox (nlpsandbox.io). We evaluated two state-of-the-art NLPde-identification focused annotation models, Philter and NeuroNER, using datafrom three institutions. We further validated model performance using data froman external validation site. Results We demonstrated the usefulness of the NLPSandbox through de-identification clinical model evaluation. The externaldeveloper was able to incorporate their model into the NLP Sandbox template andprovide user experience feedback. Discussion We demonstrated the feasibility ofusing the NLP Sandbox to conduct a multi-site evaluation of clinical textde-identification models without the sharing of data. Standardized model anddata schemas enable smooth model transfer and implementation. To generalize theNLP Sandbox, work is required on the part of data owners and model developersto develop suitable and standardized schemas and to adapt their data or modelto fit the schemas. Conclusions The NLP Sandbox lowers the barrier to utilizingclinical data for NLP model evaluation and facilitates federated, multi-site,unbiased evaluation of NLP models.\rDetecting Unintended Memorization in Language-Model-Fused ASR\nW. Ronny Huang Steve Chien Om Thakkar Rajiv Mathews\nabstract\rabstract: End-to-end (E2E) models are often being accompanied by language models (LMs)via shallow fusion for boosting their overall quality as well as recognition ofrare words. At the same time, several prior works show that LMs are susceptibleto unintentionally memorizing rare or unique sequences in the training data. Inthis work, we design a framework for detecting memorization of random textualsequences (which we call canaries) in the LM training data when one has onlyblack-box (query) access to LM-fused speech recognizer, as opposed to directaccess to the LM. On a production-grade Conformer RNN-T E2E model fused with aTransformer LM, we show that detecting memorization of singly-occurringcanaries from the LM training data of 300M examples is possible. Motivated toprotect privacy, we also show that such memorization gets significantly reducedby per-example gradient-clipped LM training without compromising overallquality.\rNegDL: Privacy-Preserving Deep Learning Based on Negative Database\nDongdong Zhao Pingchuan Zhang Jianwen Xiang Jing Tian\nabstract\rabstract: In the era of big data, deep learning has become an increasingly populartopic. It has outstanding achievements in the fields of image recognition,object detection, and natural language processing et al. The first priority ofdeep learning is exploiting valuable information from a large amount of data,which will inevitably induce privacy issues that are worthy of attention.Presently, several privacy-preserving deep learning methods have been proposed,but most of them suffer from a non-negligible degradation of either efficiencyor accuracy. Negative database (\\textit{NDB}) is a new type of datarepresentation which can protect data privacy by storing and utilizing thecomplementary form of original data. In this paper, we propose aprivacy-preserving deep learning method named NegDL based on \\textit{NDB}.Specifically, private data are first converted to \\textit{NDB} as the input ofdeep learning models by a generation algorithm called \\textit{QK}-hiddenalgorithm, and then the sketches of \\textit{NDB} are extracted for training andinference. We demonstrate that the computational complexity of NegDL is thesame as the original deep learning model without privacy protection.Experimental results on Breast Cancer, MNIST, and CIFAR-10 benchmark datasetsdemonstrate that the accuracy of NegDL could be comparable to the original deeplearning model in most cases, and it performs better than the method based ondifferential privacy.\r2022-06-25\nTEVR: Improving Speech Recognition by Token Entropy Variance Reduction\nHajo Nils Krabbenhöft Erhardt Barth\nabstract\rabstract: This paper presents TEVR, a speech recognition model designed to minimize thevariation in token entropy w.r.t. to the language model. This takes advantageof the fact that if the language model will reliably and accurately predict atoken anyway, then the acoustic model doesn\u0026rsquo;t need to be accurate inrecognizing it. We train German ASR models with 900 million parameters and showthat on CommonVoice German, TEVR scores a very competitive 3.64% word errorrate, which outperforms the best reported results by a relative 16.89%reduction in word error rate. We hope that releasing our fully trained speechrecognition pipeline to the community will lead to privacy-preserving offlinevirtual assistants in the future.\r2022-06-23\nProvably Confidential Language Modelling\nXuandong Zhao Lei Li Yu-Xiang Wang\nabstract\rabstract: Large language models are shown to memorize privacy information such associal security numbers in training data. Given the sheer scale of the trainingcorpus, it is challenging to screen and filter these privacy data, eithermanually or automatically. In this paper, we propose Confidentially RedactedTraining (CRT), a method to train language generation models while protectingthe confidential segments. We borrow ideas from differential privacy (whichsolves a related but distinct problem) and show that our method is able toprovably prevent unintended memorization by randomizing parts of the trainingprocess. Moreover, we show that redaction with an approximately correctscreening policy amplifies the confidentiality guarantee. We implement themethod for both LSTM and GPT language models. Our experimental results showthat the models trained by CRT obtain almost the same perplexity whilepreserving strong confidentiality.\rVeHIF: An Accessible Vegetation High-Impedance Fault Data Set Format\nDouglas P. S. Gomes Cagil Ozansoy\nabstract\rabstract: High-impedance faults are a challenging problem in power distributionsystems. They often do not trigger protection devices and can result in serioushazards such as igniting fires when in contact with vegetation. The currentresearch field dedicated to studying these faults is extensive but suffers froma constraining bottleneck of a lack of real experimental data. Many works setto detect and localize such faults rely on high-impedance fault low-fidelitymodels, and the lack of public data sets makes it impractical to have objectiveperformance benchmarks. This letter describes and proposes a format for a dataset of more than 900 vegetation high-impedance faults funded by the VictorianGovernment in Australia recorded in high-sampling resolution. The original dataset is public, but it was made available through an obscure format that limitsits accessibility. The presented format in this letter uses the standardhierarchical data format (HDF5), which makes it easily accessible in manylanguages such as MATLAB, Python, C++, and more. The data set compiler andvisualizer script are also provided in the work repository.\r2022-06-22\nEnhancing Networking Cipher Algorithms with Natural Language\nJohn E. Ortega\nabstract\rabstract: This work provides a survey of several networking cipher algorithms andproposes a method for integrating natural language processing (NLP) as aprotective agent for them. Two main proposals are covered for the use of NLP innetworking. First, NLP is considered as the weakest link in a networkingencryption model; and, second, as a hefty deterrent when combined as an extralayer over what could be considered a strong type of encryption \u0026ndash; the streamcipher. This paper summarizes how languages can be integrated into symmetricencryption as a way to assist in the encryption of vulnerable streams that maybe found under attack due to the natural frequency distribution of letters orwords in a local language stream.\r2022-06-18\nReplacing Labeled Real-image Datasets with Auto-generated Contours\nHirokatsu Kataoka Ryo Hayamizu Ryosuke Yamada Kodai Nakashima Sora Takashima Xinyu Zhang Edgar Josafat Martinez-Noriega Nakamasa Inoue Rio Yokota\nabstract\rabstract: In the present work, we show that the performance of formula-drivensupervised learning (FDSL) can match or even exceed that of ImageNet-21kwithout the use of real images, human-, and self-supervision during thepre-training of Vision Transformers (ViTs). For example, ViT-Base pre-trainedon ImageNet-21k shows 81.8% top-1 accuracy when fine-tuned on ImageNet-1k andFDSL shows 82.7% top-1 accuracy when pre-trained under the same conditions(number of images, hyperparameters, and number of epochs). Images generated byformulas avoid the privacy/copyright issues, labeling cost and errors, andbiases that real images suffer from, and thus have tremendous potential forpre-training general models. To understand the performance of the syntheticimages, we tested two hypotheses, namely (i) object contours are what matter inFDSL datasets and (ii) increased number of parameters to create labels affectsperformance improvement in FDSL pre-training. To test the former hypothesis, weconstructed a dataset that consisted of simple object contour combinations. Wefound that this dataset can match the performance of fractals. For the latterhypothesis, we found that increasing the difficulty of the pre-training taskgenerally leads to better fine-tuning accuracy.\r2022-06-16\nBenchmarking Differential Privacy and Federated Learning for BERT Models\nPriyam Basu Tiasa Singha Roy Rakshit Naidu Zumrut Muftuoglu Sahib Singh Fatemehsadat Mireshghallah\nabstract\rabstract: Natural Language Processing (NLP) techniques can be applied to help with thediagnosis of medical conditions such as depression, using a collection of aperson\u0026rsquo;s utterances. Depression is a serious medical illness that can haveadverse effects on how one feels, thinks, and acts, which can lead to emotionaland physical problems. Due to the sensitive nature of such data, privacymeasures need to be taken for handling and training models with such data. Inthis work, we study the effects that the application of Differential Privacy(DP) has, in both a centralized and a Federated Learning (FL) setup, ontraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).We offer insights on how to privately train NLP models and what architecturesand setups provide more desirable privacy utility trade-offs. We envisage thiswork to be used in future healthcare and mental health studies to keep medicalhistory private. Therefore, we provide an open-source implementation of thiswork.\rOn the Protected Spectrum of the Minimal Argyres-Douglas Theory\nChinmaya Bhargava Matthew Buican Hongliang Jiang\nabstract\rabstract: Despite the power of supersymmetry, finding exact closed-form expressions forthe protected operator spectra of interacting superconformal field theories(SCFTs) is difficult. In this paper, we take a step towards a solution for the\u0026quot;simplest\u0026quot; interacting 4D $\\mathcal{N}=2$ SCFT: the minimal Argyres-Douglas(MAD) theory. We present two results that go beyond the well-understood Coulombbranch and Schur sectors. First, we find the exact closed-form spectrum ofmultiplets containing operators that are chiral with respect to any$\\mathcal{N}=1\\subset\\mathcal{N}=2$ superconformal subalgebra. We argue thatthis \u0026ldquo;full\u0026rdquo; chiral sector (FCS) is as simple as allowed by unitarity for atheory with a Coulomb branch and that, up to a rescaling of $U(1)r$ quantumnumbers and the vanishing of a finite number of states, the MAD FCS isisospectral to the FCS of the free $\\mathcal{N}=2$ Abelian gauge theory. In thelanguage of superconformal representation theory, this leaves only the spectrumof the poorly understood $\\bar{\\mathcal{C}}{R,r(j,\\bar j)}$ multiplets to bedetermined. Our second result sheds light on these observables: we find anexact closed-form answer for the number of $\\bar{\\mathcal{C}}_{0,r(j,0)}$multiplets, for any $r$ and $j$, in the MAD theory. We argue that thissub-sector is also as simple as allowed by unitarity for a theory with aCoulomb branch and that there is a natural map to the corresponding sector ofthe free $\\mathcal{N}=2$ Abelian gauge theory. These results motivate aconjecture on the full local operator algebra of the MAD theory.\r2022-06-15\nPrivate Language Model Adaptation for Speech Recognition\nZhe Liu Ke Li Shreyan Bakshi Fuchun Peng\nabstract\rabstract: Speech model adaptation is crucial to handle the discrepancy betweenserver-side proxy training data and actual data received on local devices ofusers. With the use of federated learning (FL), we introduce an efficientapproach on continuously adapting neural network language models (NNLMs) onprivate devices with applications on automatic speech recognition (ASR). Toaddress the potential speech transcription errors in the on-device trainingcorpus, we perform empirical studies on comparing various strategies ofleveraging token-level confidence scores to improve the NNLM quality in the FLsettings. Experiments show that compared with no model adaptation, the proposedmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on twospeech evaluation datasets, respectively. We also provide analysis inevaluating privacy guarantees of our presented procedure.\r2022-06-14\nSymmetry-Protected Infinite-Temperature Quantum Memory from Subsystem Codes\nJulia Wildeboer Thomas Iadecola Dominic J. Williamson\nabstract\rabstract: We study a mechanism whereby quantum information present in the initial stateof a quantum many-body system can be protected for arbitrary times due to acombination of symmetry and spatial locality. Remarkably, the mechanism issufficiently generic that the dynamics can be fully ergodic upon resolving theprotecting symmetry and fixing the encoded quantum state, resulting in aninfinite-temperature quantum memory. After exemplifying the mechanism in astrongly nonintegrable two-dimensional (2D) spin model inspired by the surfacecode, we find it has a natural interpretation in the language of noiselesssubsystems and stabilizer subsystem codes. This interpretation yields a numberof further examples, including a nonintegrable Hamiltonian with quantum memorybased on the Bacon-Shor code. The lifetime of the encoded quantum informationin these models is infinite provided the dynamics respect the stabilizersymmetry of the underlying subsystem code. In the presence ofsymmetry-violating perturbations, we make contact with previous work leveragingthe concept of prethermalization to show that the encoded quantum informationretains a parametrically long lifetime under dynamics with an enlargedcontinuous symmetry group. We identify conditions on the underlying subsystemcode that enable such a prethermal enhancement of the memory lifetime.\rFreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models\nYinpeng Guo Liangyou Li Xin Jiang Qun Liu\nabstract\rabstract: Cross-lingual transfer (CLT) is of various applications. However, labeledcross-lingual corpus is expensive or even inaccessible, especially in thefields where labels are private, such as diagnostic results of symptoms inmedicine and user profiles in business. Nevertheless, there are off-the-shelfmodels in these sensitive fields. Instead of pursuing the original labels, aworkaround for CLT is to transfer knowledge from the off-the-shelf modelswithout labels. To this end, we define a novel CLT problem named FreeTransfer-Xthat aims to achieve knowledge transfer from the off-the-shelf models inrich-resource languages. To address the problem, we propose a 2-step knowledgedistillation (KD, Hinton et al., 2015) framework based on multilingualpre-trained language models (mPLM). The significant improvement over strongneural machine translation (NMT) baselines demonstrates the effectiveness ofthe proposed method. In addition to reducing annotation cost and protectingprivate labels, the proposed method is compatible with different networks andeasy to be deployed. Finally, a range of analyses indicate the great potentialof the proposed method.\r2022-06-13\nConsent verification monitoring\nMarco Robol Travis D. Breaux Elda Paja Paolo Giorgini\nabstract\rabstract: Advances in service personalization are driven by low-cost data collectionand processing, in addition to the wide variety of third-party frameworks forauthentication, storage, and marketing. New privacy regulations, such as theGeneral Data Protection Regulation (GDPR) and the California Consumer PrivacyAct (CCPA), increasingly require organizations to explicitly state their datapractices in privacy policies. When data practices change, a new version of thepolicy is released. This can occur a few times a year, when data collection orprocessing requirements are rapidly changing. Consent evolution raises specificchallenges to ensuring GDPR compliance. We propose a formal consent frameworkto support organizations, data users and data subjects in their understandingof policy evolution under a consent regime that supports both the retroactiveand non-retroactive granting and withdrawal of consent. The contributionsinclude: (i) a formal framework to reason about data collection and accessunder multiple consent granting and revocation scenarios; (ii) a scriptinglanguage that implements the consent framework for encoding and executingdifferent scenarios; (iii) five consent evolution use cases that illustrate howorganizations would evolve their policies using this framework; and (iv) ascalability evaluation of the reasoning framework. The framework models areused to verify when user consent prevents or detects unauthorized datacollection and access. The framework can be integrated into a runtimearchitecture to monitor policy violations as data practices evolve inreal-time. The framework was evaluated using the five use cases and asimulation to measure the framework scalability. The simulation results showthat the approach is computationally scalable for use in runtime consentmonitoring under a standard model of data collection and access, and practiceand policy evolution.\r2022-06-12\nEvolutionary Multi-Task Injection Testing on Web Application Firewalls\nKe Li Heng Yang Willem Visser\nabstract\rabstract: Web application firewall (WAF) plays an integral role nowadays to protect webapplications from various malicious injection attacks such as SQL injection,XML injection, and PHP injection, to name a few. However, given the evolvingsophistication of injection attacks and the increasing complexity of tuning aWAF, it is challenging to ensure that the WAF is free of injectionvulnerabilities such that it will block all malicious injection attacks withoutwrongly affecting the legitimate message. Automatically testing the WAF is,therefore, a timely and essential task. In this paper, we propose DaNuoYi, anautomatic injection testing tool that simultaneously generates test inputs formultiple types of injection attacks on a WAF. Our basic idea derives from thecross-lingual translation in the natural language processing domain. Inparticular, test inputs for different types of injection attacks aresyntactically different but may be semantically similar. Sharing semanticknowledge across multiple programming languages can thus stimulate thegeneration of more sophisticated test inputs and discovering injectionvulnerabilities of the WAF that are otherwise difficult to find. To this end,in DaNuoYi, we train several injection translation models by using multi-tasklearning that translates the test inputs between any pair of injection attacks.The model is then used by a novel multi-task evolutionary algorithm toco-evolve test inputs for different types of injection attacks facilitated by ashared mating pool and domain-specific mutation operators at each generation.We conduct experiments on three real-world open-source WAFs and six types ofinjection attacks, the results reveal that DaNuoYi generates up to 3.8x and5.78x more valid test inputs (i.e., bypassing the underlying WAF) than itsstate-of-the-art single-task counterparts and the context-free grammar-basedinjection construction.\r2022-06-09\nPrivacy Leakage in Text Classification: A Data Extraction Approach\nAdel Elmahdy Huseyin A. Inan Robert Sim\nabstract\rabstract: Recent work has demonstrated the successful extraction of training data fromgenerative language models. However, it is not evident whether such extractionis feasible in text classification models since the training objective is topredict the class label as opposed to next-word prediction. This poses aninteresting challenge and raises an important question regarding the privacy oftraining data in text classification settings. Therefore, we study thepotential privacy leakage in the text classification domain by investigatingthe problem of unintended memorization of training data that is not pertinentto the learning task. We propose an algorithm to extract missing tokens of apartial text by exploiting the likelihood of the class label provided by themodel. We test the effectiveness of our algorithm by inserting canaries intothe training set and attempting to extract tokens in these canariespost-training. In our experiments, we demonstrate that successful extraction ispossible to some extent. This can also be used as an auditing strategy toassess any potential unauthorized use of personal data without consent.\r2022-06-07\nMarvolo: Programmatic Data Augmentation for Practical ML-Driven Malware Detection\nMichael D. Wong Edward Raff James Holt Ravi Netravali\nabstract\rabstract: Data augmentation has been rare in the cyber security domain due to technicaldifficulties in altering data in a manner that is semantically consistent withthe original data. This shortfall is particularly onerous given the uniquedifficulty of acquiring benign and malicious training data that runs intocopyright restrictions, and that institutions like banks and governmentsreceive targeted malware that will never exist in large quantities. We presentMARVOLO, a binary mutator that programmatically grows malware (and benign)datasets in a manner that boosts the accuracy of ML-driven malware detectors.MARVOLO employs semantics-preserving code transformations that mimic thealterations that malware authors and defensive benign developers routinely makein practice , allowing us to generate meaningful augmented data. Crucially,semantics-preserving transformations also enable MARVOLO to safely propagatelabels from original to newly-generated data samples without mandatingexpensive reverse engineering of binaries. Further, MARVOLO embeds several keyoptimizations that keep costs low for practitioners by maximizing the densityof diverse data samples generated within a given time (or resource) budget.Experiments using wide-ranging commercial malware datasets and a recentML-driven malware detector show that MARVOLO boosts accuracies by up to 5%,while operating on only a small fraction (15%) of the potential input binaries.\r2022-06-06\nPretrained Models for Multilingual Federated Learning\nOrion Weller Marc Marone Vladimir Braverman Dawn Lawrie Benjamin Van Durme\nabstract\rabstract: Since the advent of Federated Learning (FL), research has applied thesemethods to natural language processing (NLP) tasks. Despite a plethora ofpapers in FL for NLP, no previous works have studied how multilingual textimpacts FL algorithms. Furthermore, multilingual text provides an interestingavenue to examine the impact of non-IID text (e.g. different languages) on FLin naturally occurring data. We explore three multilingual language tasks,language modeling, machine translation, and text classification using differingfederated and non-federated learning algorithms. Our results show that usingpretrained models reduces the negative effects of FL, helping them to performnear or better than centralized (no privacy) learning, even when using non-IIDpartitioning.\r2022-06-03\nDifferentially Private Model Compression\nFatemehsadat Mireshghallah Arturs Backurs Huseyin A Inan Lukas Wutschitz Janardhan Kulkarni\nabstract\rabstract: Recent papers have shown that large pre-trained language models (LLMs) suchas BERT, GPT-2 can be fine-tuned on private data to achieve performancecomparable to non-private models for many downstream Natural LanguageProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.The inference cost of these models \u0026ndash; which consist of hundreds of millions ofparameters \u0026ndash; however, can be prohibitively large. Hence, often in practice,LLMs are compressed before they are deployed in specific applications. In thispaper, we initiate the study of differentially private model compression andpropose frameworks for achieving 50% sparsity levels while maintaining nearlyfull performance. We demonstrate these ideas on standard GLUE benchmarks usingBERT models, setting benchmarks for future research on this topic.\rFederating and querying heterogeneous and distributed Web APIs and triple stores\nTarcisio Mendes de Farias Christophe Dessimoz Aaron Ayllon Benitez Chen Yang Jiao Long Ana-Claudia Sima\nabstract\rabstract: Today\u0026rsquo;s international corporations such as BASF, a leading company in thecrop protection industry, produce and consume more and more data that are oftenfragmented and accessible through Web APIs. In addition, part of theproprietary and public data of BASF\u0026rsquo;s interest are stored in triple stores andaccessible with the SPARQL query language. Homogenizing the data access modesand the underlying semantics of the data without modifying or replicating theoriginal data sources become important requirements to achieve data integrationand interoperability. In this work, we propose a federated data integrationarchitecture within an industrial setup, that relies on an ontology-based dataaccess method. Our performance evaluation in terms of query response timeshowed that most queries can be answered in under 1 second.\r2022-06-02\nSkillBot: Identifying Risky Content for Children in Alexa Skills\nTu Le Danny Yuxing Huang Noah Apthorpe Yuan Tian\nabstract\rabstract: Many households include children who use voice personal assistants (VPA) suchas Amazon Alexa. Children benefit from the rich functionalities of VPAs andthird-party apps but are also exposed to new risks in the VPA ecosystem. Inthis paper, we first investigate \u0026ldquo;risky\u0026rdquo; child-directed voice apps that containinappropriate content or ask for personal information through voiceinteractions. We build SkillBot - a natural language processing (NLP)-basedsystem to automatically interact with VPA apps and analyze the resultingconversations. We find 28 risky child-directed apps and maintain a growingdataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexaapps. Our findings suggest that although child-directed VPA apps are subject tostricter policy requirements and more intensive vetting, children remainvulnerable to inappropriate content and privacy violations. We then conduct auser study showing that parents are concerned about the identified risky apps.Many parents do not believe that these apps are available and designed forfamilies/kids, although these apps are actually published in Amazon\u0026rsquo;s \u0026ldquo;Kids\u0026quot;product category. We also find that parents often neglect basic precautionssuch as enabling parental controls on Alexa devices. Finally, we identify anovel risk in the VPA ecosystem: confounding utterances, or voice commandsshared by multiple apps that may cause a user to interact with a different appthan intended. We identify 4,487 confounding utterances, including 581 sharedby child-directed and non-child-directed apps. We find that 27% of theseconfounding utterances prioritize invoking a non-child-directed app over achild-directed app. This indicates that children are at real risk ofaccidentally invoking non-child-directed apps due to confounding utterances.\rTHE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption\nTianyu Chen Hangbo Bao Shaohan Huang Li Dong Binxing Jiao Daxin Jiang Haoyi Zhou Jianxin Li Furu Wei\nabstract\rabstract: As more and more pre-trained language models adopt on-cloud deployment, theprivacy issues grow quickly, mainly for the exposure of plain-text user data(e.g., search history, medical record, bank account). Privacy-preservinginference of transformer models is on the demand of cloud service users. Toprotect privacy, it is an attractive choice to compute only with ciphertext inhomomorphic encryption (HE). However, enabling pre-trained models inference onciphertext data is difficult due to the complex computations in transformerblocks, which are not supported by current HE tools yet. In this work, weintroduce $\\textit{THE-X}$, an approximation approach for transformers, whichenables privacy-preserving inference of pre-trained models developed by popularframeworks. $\\textit{THE-X}$ proposes a workflow to deal with complexcomputation in transformer networks, including all the non-polynomial functionslike GELU, softmax, and LayerNorm. Experiments reveal our proposed$\\textit{THE-X}$ can enable transformer inference on encrypted data fordifferent downstream tasks, all with negligible performance drop but enjoyingthe theory-guaranteed privacy-preserving advantage.\r2022-05-30\nSecuring AI-based Healthcare Systems using Blockchain Technology: A State-of-the-Art Systematic Literature Review and Future Research Directions\nRucha Shinde Shruti Patil Ketan Kotecha Vidyasagar Potdar Ganeshsree Selvachandran Ajith Abraham\nabstract\rabstract: Healthcare systems are increasingly incorporating Artificial Intelligenceinto their systems, but it is not a solution for all difficulties. AI\u0026rsquo;sextraordinary potential is being held back by challenges such as a lack ofmedical datasets for training AI models, adversarial attacks, and a lack oftrust due to its black box working style. We explored how blockchain technologycan improve the reliability and trustworthiness of AI-based healthcare. Thispaper has conducted a Systematic Literature Review to explore thestate-of-the-art research studies conducted in healthcare applicationsdeveloped with different AI techniques and Blockchain Technology. Thissystematic literature review proceeds with three different paths as naturallanguage processing-based healthcare systems, computer vision-based healthcaresystems and acoustic AI-based healthcare systems. We found that 1) Defencetechniques for adversarial attacks on AI are available for specific kind ofattacks and even adversarial training is AI based technique which in furtherprone to different attacks. 2) Blockchain can address security and privacyissues in healthcare fraternity. 3) Medical data verification and userprovenance can be enabled with Blockchain. 4) Blockchain can protectdistributed learning on heterogeneous medical data. 5) The issues like singlepoint of failure, non-transparency in healthcare systems can be resolved withBlockchain. Nevertheless, it has been identified that research is at theinitial stage. As a result, we have synthesized a conceptual framework usingBlockchain Technology for AI-based healthcare applications that considers theneeds of each NLP, Computer Vision, and Acoustic AI application. A globalsolution for all sort of adversarial attacks on AI based healthcare. However,this technique has significant limits and challenges that need to be addressedin future studies.\r2022-05-29\nCPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI\nYirong Chen Weiquan Fan Xiaofen Xing Jianxin Pang Minlie Huang Wenjing Han Qianfeng Tie Xiangmin Xu\nabstract\rabstract: Human language expression is based on the subjective construal of thesituation instead of the objective truth conditions, which means that speakers\u0026rsquo;personalities and emotions after cognitive processing have an importantinfluence on conversation. However, most existing datasets for conversationalAI ignore human personalities and emotions, or only consider part of them. It\u0026rsquo;sdifficult for dialogue systems to understand speakers\u0026rsquo; personalities andemotions although large-scale pre-training language models have been widelyused. In order to consider both personalities and emotions in the process ofconversation generation, we propose CPED, a large-scale Chinese personalizedand emotional dialogue dataset, which consists of multi-source knowledgerelated to empathy and personal characteristic. These knowledge covers gender,Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPEDcontains more than 12K dialogues of 392 speakers from 40 TV shows. We releasethe textual dataset with audio features and video features according to thecopyright claims, privacy issues, terms of service of video platforms. Weprovide detailed description of the CPED construction process and introducethree tasks for conversational AI, including personality recognition, emotionrecognition in conversations as well as personalized and emotional conversationgeneration. Finally, we provide baseline systems for these tasks and considerthe function of speakers\u0026rsquo; personalities and emotions on conversation. Ourmotivation is to propose a dataset to be widely adopted by the NLP community asa new open benchmark for conversational AI research. The full dataset isavailable at https://github.com/scutcyr/CPED.\r2022-05-26\nFederated Split BERT for Heterogeneous Text Classification\nZhengyang Li Shijing Si Jianzong Wang Jing Xiao\nabstract\rabstract: Pre-trained BERT models have achieved impressive performance in many naturallanguage processing (NLP) tasks. However, in many real-world situations,textual data are usually decentralized over many clients and unable to beuploaded to a central server due to privacy protection and regulations.Federated learning (FL) enables multiple clients collaboratively to train aglobal model while keeping the local data privacy. A few researches haveinvestigated BERT in federated learning setting, but the problem of performanceloss caused by heterogeneous (e.g., non-IID) data over clients remainunder-explored. To address this issue, we propose a framework, FedSplitBERT,which handles heterogeneous data and decreases the communication cost bysplitting the BERT encoder layers into local part and global part. The localpart parameters are trained by the local client only while the global partparameters are trained by aggregating gradients of multiple clients. Due to thesheer size of BERT, we explore a quantization method to further reduce thecommunication cost with minimal performance loss. Our framework is ready-to-useand compatible to many existing federated learning algorithms, includingFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of theproposed framework, which outperforms baseline methods by a significant margin,while FedSplitBERT with quantization can reduce the communication cost by$11.9\\times$.\rLeveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks\nDivyam Goel Raksha Sharma\nabstract\rabstract: The last few years have witnessed an exponential rise in the propagation ofoffensive text on social media. Identification of this text with high precisionis crucial for the well-being of society. Most of the existing approaches tendto give high toxicity scores to innocuous statements (e.g., \u0026ldquo;I am a gay man\u0026rdquo;).These false positives result from over-generalization on the training datawhere specific terms in the statement may have been used in a pejorative sense(e.g., \u0026ldquo;gay\u0026rdquo;). Emphasis on such words alone can lead to discrimination againstthe classes these systems are designed to protect. In this paper, we addressthe problem of offensive language detection on Twitter, while also detectingthe type and the target of the offence. We propose a novel approach calledSyLSTM, which integrates syntactic features in the form of the dependency parsetree of a sentence and semantic features in the form of word embeddings into adeep learning architecture using a Graph Convolutional Network. Results showthat the proposed approach significantly outperforms the state-of-the-art BERTmodel with orders of magnitude fewer number of parameters.\r2022-05-20\nHow to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing\nSamuel Sousa Roman Kern\nabstract\rabstract: Deep learning (DL) models for natural language processing (NLP) tasks oftenhandle private data, demanding protection against breaches and disclosures.Data protection laws, such as the European Union\u0026rsquo;s General Data ProtectionRegulation (GDPR), thereby enforce the need for privacy. Although manyprivacy-preserving NLP methods have been proposed in recent years, nocategories to organize them have been introduced yet, making it hard to followthe progress of the literature. To close this gap, this article systematicallyreviews over sixty DL methods for privacy-preserving NLP published between 2016and 2020, covering theoretical foundations, privacy-enhancing technologies, andanalysis of their suitability for real-world scenarios. First, we introduce anovel taxonomy for classifying the existing methods into three categories: datasafeguarding methods, trusted methods, and verification methods. Second, wepresent an extensive summary of privacy threats, datasets for applications, andmetrics for privacy evaluation. Third, throughout the review, we describeprivacy issues in the NLP pipeline in a holistic view. Further, we discuss openchallenges in privacy-preserving NLP regarding data traceability, computationoverhead, dataset size, the prevalence of human biases in embeddings, and theprivacy-utility tradeoff. Finally, this review presents future researchdirections to guide successive research and development of privacy-preservingNLP models.\r2022-05-19\nNebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\nYang Xiang Zhihua Wu Weibao Gong Siyu Ding Xianjie Mo Yuang Liu Shuohuan Wang Peng Liu Yongshuai Hou Long Li Bin Wang Shaohuai Shi Yaqian Han Yue Yu Ge Li Yu Sun Yanjun Ma Dianhai Yu\nabstract\rabstract: The ever-growing model size and scale of compute have attracted increasinginterests in training deep learning models over multiple nodes. However, whenit comes to training on cloud clusters, especially across remote clusters, hugechallenges are faced. In this work, we introduce a general framework, Nebula-I,for collaboratively training deep learning models over remote heterogeneousclusters, the connections between which are low-bandwidth wide area networks(WANs). We took natural language processing (NLP) as an example to show howNebula-I works in different training phases that include: a) pre-training amultilingual language model using two remote clusters; and b) fine-tuning amachine translation model using knowledge distilled from pre-trained models,which run through the most popular paradigm of recent deep learning. To balancethe accuracy and communication efficiency, in Nebula-I, parameter-efficienttraining strategies, hybrid parallel computing methods and adaptivecommunication acceleration techniques are jointly applied. Meanwhile, securitystrategies are employed to guarantee the safety, reliability and privacy inintra-cluster computation and inter-cluster communication. Nebula-I isimplemented with the PaddlePaddle deep learning framework, which can supportcollaborative training over heterogeneous hardware, e.g. GPU and NPU.Experiments demonstrate that the proposed framework could substantiallymaximize the training efficiency while preserving satisfactory NLP performance.By using Nebula-I, users can run large-scale training tasks over cloud clusterswith minimum developments, and the utility of existed large pre-trained modelscould be further promoted. We also introduced new state-of-the-art results oncross-lingual natural language inference tasks, which are generated based upona novel learning framework and Nebula-I.\rTwenty-two years since revealing cross-site scripting attacks: a systematic mapping and a comprehensive survey\nAbdelhakim Hannousse Salima Yahiouche Mohamed Cherif Nait-Hamoud\nabstract\rabstract: Cross-site scripting (XSS) is one of the major threats menacing the privacyof data and the navigation of trusted web applications. Since its reveal inlate 1999 by Microsoft security engineers, several techniques have beendeveloped in the aim to secure web navigation and protect web applicationsagainst XSS attacks. The problem became worse with the emergence of advancedweb technologies such as Web services and APIs and new programming styles suchas AJAX, CSS3 and HTML5. While new technologies enable complex interactions anddata exchanges between clients and servers in the network, new programmingstyles introduce new and complicate injection flaws to web applications. XSShas been and still in the TOP 10 list of web vulnerabilities reported by theOpen Web Applications Security Project (OWASP). Consequently, handling XSSattacks became one of the major concerns of several web security communities.In this paper, we contribute by conducting a systematic mapping and acomprehensive survey. We summarize and categorize existent endeavors that aimto protect against XSS attacks and develop XSS-free web applications. Thepresent review covers 147 high quality published studies since 1999 includingearly publications of 2022. A comprehensive taxonomy is drawn out describingthe different techniques used to prevent, detect, protect and defend againstXSS attacks. Although the diversity of XSS attack types and the scriptinglanguages that can be used to state them, the systematic mapping revealed aremarkable bias toward basic and JavaScript XSS attacks and a dearth ofvulnerability repair mechanisms. The survey highlighted the limitations,discussed the potentials of existing XSS attack defense mechanisms andidentified potential gaps.\r2022-05-18\nAddressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation\nKevin Yang Olivia Deng Charles Chen Richard Shin Subhro Roy Benjamin Van Durme\nabstract\rabstract: We introduce a novel setup for low-resource task-oriented semantic parsingwhich incorporates several constraints that may arise in real-world scenarios:(1) lack of similar datasets/models from a related domain, (2) inability tosample useful logical forms directly from a grammar, and (3) privacyrequirements for unlabeled natural utterances. Our goal is to improve alow-resource semantic parser using utterances collected through userinteractions. In this highly challenging but realistic setting, we investigatedata augmentation approaches involving generating a set of structured canonicalutterances corresponding to logical forms, before simulating correspondingnatural language and filtering the resulting pairs. We find that suchapproaches are effective despite our restrictive setup: in a low-resourcesetting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), weobserve 33% relative improvement over a non-data-augmented baseline in top-1match.\r2022-05-17\nTranslatotron 2: High-quality direct speech-to-speech translation with voice preservation\nYe Jia Michelle Tadmor Ramanovich Tal Remez Roi Pomerantz\nabstract\rabstract: We present Translatotron 2, a neural direct speech-to-speech translationmodel that can be trained end-to-end. Translatotron 2 consists of a speechencoder, a linguistic decoder, an acoustic synthesizer, and a single attentionmodule that connects them together. Experimental results on three datasetsconsistently show that Translatotron 2 outperforms the original Translatotronby a large margin on both translation quality (up to +15.5 BLEU) and speechgeneration quality, and approaches the same of cascade systems. In addition, wepropose a simple method for preserving speakers\u0026rsquo; voices from the source speechto the translation speech in a different language. Unlike existing approaches,the proposed method is able to preserve each speaker\u0026rsquo;s voice on speaker turnswithout requiring for speaker segmentation. Furthermore, compared to existingapproaches, it better preserves speaker\u0026rsquo;s privacy and mitigates potentialmisuse of voice cloning for creating spoofing audio artifacts.\rFederated learning for violence incident prediction in a simulated cross-institutional psychiatric setting\nThomas Borger Pablo Mosteiro Heysem Kaya Emil Rijcken Albert Ali Salah Floortje Scheepers Marco Spruit\nabstract\rabstract: Inpatient violence is a common and severe problem within psychiatry. Knowingwho might become violent can influence staffing levels and mitigate severity.Predictive machine learning models can assess each patient\u0026rsquo;s likelihood ofbecoming violent based on clinical notes. Yet, while machine learning modelsbenefit from having more data, data availability is limited as hospitalstypically do not share their data for privacy preservation. Federated Learning(FL) can overcome the problem of data limitation by training models in adecentralised manner, without disclosing data between collaborators. However,although several FL approaches exist, none of these train Natural LanguageProcessing models on clinical notes. In this work, we investigate theapplication of Federated Learning to clinical Natural Language Processing,applied to the task of Violence Risk Assessment by simulating across-institutional psychiatric setting. We train and compare four models: twolocal models, a federated model and a data-centralised model. Our resultsindicate that the federated model outperforms the local models and has similarperformance as the data-centralised model. These findings suggest thatFederated Learning can be used successfully in a cross-institutional settingand is a step towards new applications of Federated Learning based on clinicalnotes\r2022-05-14\nBalancing out Bias: Achieving Fairness Through Balanced Training\nXudong Han Timothy Baldwin Trevor Cohn\nabstract\rabstract: Group bias in natural language processing tasks manifests as disparities insystem error rates across texts authorized by different demographic groups,typically disadvantaging minority groups. Dataset balancing has been shown tobe effective at mitigating bias, however existing approaches do not directlyaccount for correlations between author demographics and linguistic variables,limiting their effectiveness. To achieve Equal Opportunity fairness, such asequal job opportunity without regard to demographics, this paper introduces asimple, but highly effective, objective for countering bias using balancedtraining. We extend the method in the form of a gated model, which incorporatesprotected attributes as input, and show that it is effective at reducing biasin predictions through demographic input perturbation, outperforming all otherbias mitigation techniques when combined with balanced training.\r2022-05-11\nProteção intelectual de obras produzidas por sistemas baseados em inteligência artificial: uma visão tecnicista sobre o tema\nFábio Manoel França Lobato\nabstract\rabstract: The pervasiveness of Artificial Intelligence (AI) is unquestionable in oursociety. Even in the arts, AI is present. A notorious case is the song \u0026ldquo;HeyYa!\u0026rdquo; of the OutKast group, successful in the 2000s. At this time, the musicindustry began to make decisions based on data to strategize based onpredictions of listeners\u0026rsquo; habits. This case is just one of the countlessexamples of AI applications in the arts. The advent of deep learning made itpossible to build systems capable of accurately recognizing artistic style inpaintings. Content generation is also possible; for example, Deepart customizesimages from two \\textit{inputs}: 1) an image to be customized; 2) a style ofpainting. The generation of songs according to specific styles from AI-basedsystems is also possible. Such possibilities raise questions about theintellectual property of such works. On this occasion, who owns the copyrightof a work produced from a system based on Artificial Intelligence? To thecreator of the AI? The company/corporation that subsidized the development ofthis system? Or AI itself as a creator? This essay aims to contribute with atechnicist view on the discussion of copyright applicability from worksproduced by AI.\r2022-05-10\nSentence-level Privacy for Document Embeddings\nCasey Meehan Khalil Mrini Kamalika Chaudhuri\nabstract\rabstract: User language data can contain highly sensitive personal content. As such, itis imperative to offer users a strong and interpretable privacy guarantee whenlearning from their data. In this work, we propose SentDP: pure localdifferential privacy at the sentence level for a single user document. Wepropose a novel technique, DeepCandidate, that combines concepts from robuststatistics and language modeling to produce high-dimensional, general-purpose$\\epsilon$-SentDP document embeddings. This guarantees that any single sentencein a document can be substituted with any other sentence while keeping theembedding $\\epsilon$-indistinguishable. Our experiments indicate that theseprivate document embeddings are useful for downstream tasks like sentimentanalysis and topic classification and even outperform baseline methods withweaker guarantees like word-level Metric DP.\r2022-05-08\nA Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges\nZhenghua Chen Min Wu Alvin Chan Xiaoli Li Yew-Soon Ong\nabstract\rabstract: Artificial Intelligence (AI) is a fast-growing research and development (R\u0026amp;D)discipline which is attracting increasing attention because of its promises tobring vast benefits for consumers and businesses, with considerable benefitspromised in productivity growth and innovation. To date it has reportedsignificant accomplishments in many areas that have been deemed as challengingfor machines, ranging from computer vision, natural language processing, audioanalysis to smart sensing and many others. The technical trend in realizing thesuccesses has been towards increasing complex and large size AI models so as tosolve more complex problems at superior performance and robustness. This rapidprogress, however, has taken place at the expense of substantial environmentalcosts and resources. Besides, debates on the societal impacts of AI, such asfairness, safety and privacy, have continued to grow in intensity. These issueshave presented major concerns pertaining to the sustainable development of AI.In this work, we review major trends in machine learning approaches that canaddress the sustainability problem of AI. Specifically, we examine emerging AImethodologies and algorithms for addressing the sustainability issue of AI intwo major aspects, i.e., environmental sustainability and social sustainabilityof AI. We will also highlight the major limitations of existing studies andpropose potential research challenges and directions for the development ofnext generation of sustainable AI techniques. We believe that this technicalreview can help to promote a sustainable development of AI R\u0026amp;D activities forthe research community.\r2022-05-06\nFedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks\nBill Yuchen Lin Chaoyang He Zihang Zeng Hulin Wang Yufen Huang Christophe Dupuy Rahul Gupta Mahdi Soltanolkotabi Xiang Ren Salman Avestimehr\nabstract\rabstract: Increasing concerns and regulations about data privacy and sparsitynecessitate the study of privacy-preserving, decentralized learning methods fornatural language processing (NLP) tasks. Federated learning (FL) providespromising approaches for a large number of clients (e.g., personal devices ororganizations) to collaboratively learn a shared global model to benefit allclients while allowing users to keep their data locally. Despite interest instudying FL methods for NLP tasks, a systematic comparison and analysis islacking in the literature. Herein, we present the FedNLP, a benchmarkingframework for evaluating federated learning methods on four different taskformulations: text classification, sequence tagging, question answering, andseq2seq. We propose a universal interface between Transformer-based languagemodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) undervarious non-IID partitioning strategies. Our extensive experiments with FedNLPprovide empirical comparisons between FL methods and helps us better understandthe inherent challenges of this direction. The comprehensive analysis points tointriguing and exciting future research aimed at developing FL methods for NLPtasks.\r2022-05-05\nEvolution of language driven by social dynamics\nMoirangthem Shubhakanta Singh R. K. Brojen Singh\nabstract\rabstract: The survival of endangered languages in complex language competition dependson socio-cultural status and honour endowed (by itself and by the other) amongthem. The restriction in the endorsement of this honour leads to languageextinction of one language, and rise of the other. Endorsing proper mutualhonour each other trigger the co-existence of language speakers and can saveboth languages from extinction. The lost of respect to each other drives thedeath of both languages. We found a minimal or critical mutual honour(a=0.9635) which protects the two languages from extinction. The increase inmutual honour from this minimal value allows increase in the populations of thetwo languages speakers. The state of co-existence of competing languagesabolishes the concept of minority and majority in language competition whichcan be obtained by mutual honour. Further, excess biased honour to a particularlanguage (minority or majority) force the language to extinct. In mean-fieldapproximation of language competition, magnetization parameter can be taken asan indicator of survival of a language.\r2022-05-02\nThe Limits of Word Level Differential Privacy\nJustus Mattern Benjamin Weggenmann Florian Kerschbaum\nabstract\rabstract: As the issues of privacy and trust are receiving increasing attention withinthe research community, various attempts have been made to anonymize textualdata. A significant subset of these approaches incorporate differentiallyprivate mechanisms to perturb word embeddings, thus replacing individual wordsin a sentence. While these methods represent very important contributions, havevarious advantages over other techniques and do show anonymizationcapabilities, they have several shortcomings. In this paper, we investigatethese weaknesses and demonstrate significant mathematical constraintsdiminishing the theoretical privacy guarantee as well as major practicalshortcomings with regard to the protection against deanonymization attacks, thepreservation of content of the original sentences as well as the quality of thelanguage output. Finally, we propose a new method for text anonymization basedon transformer based language models fine-tuned for paraphrasing thatcircumvents most of the identified weaknesses and also offers a formal privacyguarantee. We evaluate the performance of our method via thoroughexperimentation and demonstrate superior performance over the discussedmechanisms.\rPrivacy-Preserving Graph Convolutional Networks for Text Classification\nTimour Igamberdiev Ivan Habernal\nabstract\rabstract: Graph convolutional networks (GCNs) are a powerful architecture forrepresentation learning on documents that naturally occur as graphs, e.g.,citation or social networks. However, sensitive personal information, such asdocuments with people\u0026rsquo;s profiles or relationships as edges, are prone toprivacy leaks, as the trained model might reveal the original input. Althoughdifferential privacy (DP) offers a well-founded privacy-preserving framework,GCNs pose theoretical and practical challenges due to their training specifics.We address these challenges by adapting differentially-private gradient-basedtraining to GCNs and conduct experiments using two optimizers on five NLPdatasets in two languages. We propose a simple yet efficient method based onrandom graph splits that not only improves the baseline privacy bounds by afactor of 2.7 while retaining competitive F1 scores, but also provides strongprivacy guarantees of epsilon = 1.0. We show that, under certain modelingchoices, privacy-preserving GCNs perform up to 90% of their non-privatevariants, while formally guaranteeing strong privacy measures.\r2022-04-28\nEVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification\nGeorgios P. Spithourakis Ivan Vulić Michał Lis Iñigo Casanueva Paweł Budzianowski\nabstract\rabstract: Knowledge-based authentication is crucial for task-oriented spoken dialoguesystems that offer personalised and privacy-focused services. Such systemsshould be able to enrol (E), verify (V), and identify (I) new and recurringusers based on their personal information, e.g. postcode, name, and date ofbirth. In this work, we formalise the three authentication tasks and theirevaluation protocols, and we present EVI, a challenging spoken multilingualdataset with 5,506 dialogues in English, Polish, and French. Our proposedmodels set the first competitive benchmarks, explore the challenges ofmultilingual natural language processing of spoken dialogue, and set directionsfor future research.\rRobots: the Century Past and the Century Ahead\nFederico Pigozzi\nabstract\rabstract: Let us reflect on the state of robotics. This year marks the $101$-stanniversary of R.U.R., a play by the writer Karel \\v{C}apek, often creditedwith introducing the word \u0026ldquo;robot\u0026rdquo;. The word used to refer to feudal forcedlabourers in Slavic languages. Indeed, it points to one key characteristic ofrobotic systems: they are mere slaves, have no rights, and execute our willsinstruction by instruction, without asking anything in return. The relationshipwith us humans is commensalism; in biology, commensalism subsists between twosymbiotic species when one species benefits from it (robots boost productivityfor humans), while the other species neither benefits nor is harmed (can youreally argue that robots benefit from simply functioning?). We then distinguish robots from \u0026ldquo;living machines\u0026rdquo;, that is, machines infusedwith life. If living machines should ever become a reality, we would need toshift our relationship with them from commensalism to mutualism. Thedistinction is not subtle: we experience it every day with domesticatedanimals, that exchange serfdom for forage and protection. This is because lifehas evolved to resist any attempt at enslaving it; it is stubborn. In the path towards living machines, let us ask: what has been achieved byrobotics in the last $100$ years? What is left to accomplish in the next $100$years? For us, the answers boil down to three words: juice, need (or death),and embodiment, as we shall see in the following.\r2022-04-27\nLanguage-Independent Speaker Anonymization Approach using Self-Supervised Pre-Trained Models\nXiaoxiao Miao Xin Wang Erica Cooper Junichi Yamagishi Natalia Tomashenko\nabstract\rabstract: Speaker anonymization aims to protect the privacy of speakers whilepreserving spoken linguistic information from speech. Current mainstream neuralnetwork speaker anonymization systems are complicated, containing an F0extractor, speaker encoder, automatic speech recognition acoustic model (ASRAM), speech synthesis acoustic model and speech waveform generation model.Moreover, as an ASR AM is language-dependent, trained on English data, it ishard to adapt it into another language. In this paper, we propose a simplerself-supervised learning (SSL)-based method for language-independent speakeranonymization without any explicit language-dependent model, which can beeasily used for other languages. Extensive experiments were conducted on theVoicePrivacy Challenge 2020 datasets in English and AISHELL-3 datasets inMandarin to demonstrate the effectiveness of our proposed SSL-basedlanguage-independent speaker anonymization method.\r2022-04-26\nYou Don\u0026rsquo;t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers\u0026rsquo; Private Personas\nHaoran Li Yangqiu Song Lixin Fan\nabstract\rabstract: Social chatbots, also known as chit-chat chatbots, evolve rapidly with largepretrained language models. Despite the huge progress, privacy concerns havearisen recently: training data of large language models can be extracted viamodel inversion attacks. On the other hand, the datasets used for trainingchatbots contain many private conversations between two individuals. In thiswork, we further investigate the privacy leakage of the hidden states ofchatbots trained by language modeling which has not been well studied yet. Weshow that speakers\u0026rsquo; personas can be inferred through a simple neural networkwith high accuracy. To this end, we propose effective defense objectives toprotect persona leakage from hidden states. We conduct extensive experiments todemonstrate that our proposed defense objectives can greatly reduce the attackaccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preservelanguage models\u0026rsquo; powerful generation ability.\r2022-04-25\nReconstructing Training Data with Informed Adversaries\nBorja Balle Giovanni Cherubin Jamie Hayes\nabstract\rabstract: Given access to a machine learning model, can an adversary reconstruct themodel\u0026rsquo;s training data? This work studies this question from the lens of apowerful informed adversary who knows all the training data points except one.By instantiating concrete attacks, we show it is feasible to reconstruct theremaining data point in this stringent threat model. For convex models (e.g.logistic regression), reconstruction attacks are simple and can be derived inclosed-form. For more general models (e.g. neural networks), we propose anattack strategy based on training a reconstructor network that receives asinput the weights of the model under attack and produces as output the targetdata point. We demonstrate the effectiveness of our attack on image classifierstrained on MNIST and CIFAR-10, and systematically investigate which factors ofstandard machine learning pipelines affect reconstruction success. Finally, wetheoretically investigate what amount of differential privacy suffices tomitigate reconstruction attacks by informed adversaries. Our work provides aneffective reconstruction attack that model developers can use to assessmemorization of individual points in general settings beyond those consideredin previous works (e.g. generative language models or access to traininggradients); it shows that standard models have the capacity to store enoughinformation to enable high-fidelity reconstruction of training data points; andit demonstrates that differential privacy can successfully mitigate suchattacks in a parameter regime where utility degradation is minimal.\r2022-04-24\nAutomated speech tools for helping communities process restricted-access corpora for language revival efforts\nNay San Martijn Bartelds Tolúlopé Ògúnrèmí Alison Mount Ruben Thompson Michael Higgins Roy Barker Jane Simpson Dan Jurafsky\nabstract\rabstract: Many archival recordings of speech from endangered languages remainunannotated and inaccessible to community members and language learningprograms. One bottleneck is the time-intensive nature of annotation. An evennarrower bottleneck occurs for recordings with access constraints, such aslanguage that must be vetted or filtered by authorised community members beforeannotation can begin. We propose a privacy-preserving workflow to widen bothbottlenecks for recordings where speech in the endangered language isintermixed with a more widely-used language such as English for meta-linguisticcommentary and questions (e.g. What is the word for \u0026rsquo;tree\u0026rsquo;?). We integratevoice activity detection (VAD), spoken language identification (SLI), andautomatic speech recognition (ASR) to transcribe the metalinguistic content,which an authorised person can quickly scan to triage recordings that can beannotated by people with lower levels of access. We report work-in-progressprocessing 136 hours archival audio containing a mix of English and Muruwari.Our collaborative work with the Muruwari custodian of the archival materialsshow that this workflow reduces metalanguage transcription time by 20% evengiven only minimal amounts of annotated training data: 10 utterances perlanguage for SLI and for ASR at most 39 minutes, and possibly as little as 39seconds.\r2022-04-20\nYou Are What You Write: Preserving Privacy in the Era of Large Language Models\nRichard Plant Valerio Giuffrida Dimitra Gkatzia\nabstract\rabstract: Large scale adoption of large language models has introduced a new era ofconvenient knowledge transfer for a slew of natural language processing tasks.However, these models also run the risk of undermining user trust by exposingunwanted information about the data subjects, which may be extracted by amalicious party, e.g. through adversarial attacks. We present an empiricalinvestigation into the extent of the personal information encoded intopre-trained representations by a range of popular models, and we show apositive correlation between the complexity of a model, the amount of data usedin pre-training, and data leakage. In this paper, we present the first widecoverage evaluation and comparison of some of the most popularprivacy-preserving algorithms, on a large, multi-lingual dataset on sentimentanalysis annotated with demographic information (location, age and gender). Theresults show since larger and more complex models are more prone to leakingprivate information, use of privacy-preserving methods is highly desirable. Wealso find that highly privacy-preserving technologies like differential privacy(DP) can have serious model utility effects, which can be ameliorated usinghybrid or metric-DP techniques.\r2022-04-17\nWhyGen: Explaining ML-powered Code Generation by Referring to Training Examples\nWeixiang Yan Yuanchun Li\nabstract\rabstract: Deep learning has demonstrated great abilities in various code generationtasks. However, despite the great convenience for some developers, many areconcerned that the code generators may recite or closely mimic copyrightedtraining data without user awareness, leading to legal and ethical concerns. Toease this problem, we introduce a tool, named WhyGen, to explain the generatedcode by referring to training examples. Specifically, we first introduce a datastructure, named inference fingerprint, to represent the decision process ofthe model when generating a prediction. The fingerprints of all trainingexamples are collected offline and saved to a database. When the model is usedat runtime for code generation, the most relevant training examples can beretrieved by querying the fingerprint database. Our experiments have shown thatWhyGen is able to precisely notify the users about possible recitations andhighly similar imitations with a top-10 accuracy of 81.21%. The demo video canbe found at https://youtu.be/EtoQP6850To.\r2022-04-11\nCommonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data\nKyungjune Baek Hyunjung Shim\nabstract\rabstract: Transfer learning for GANs successfully improves generation performance underlow-shot regimes. However, existing studies show that the pretrained modelusing a single benchmark dataset is not generalized to various target datasets.More importantly, the pretrained model can be vulnerable to copyright orprivacy risks as membership inference attack advances. To resolve both issues,we propose an effective and unbiased data synthesizer, namely Primitives-PS,inspired by the generic characteristics of natural images. Specifically, weutilize 1) the generic statistics on the frequency magnitude spectrum, 2) theelementary shape (i.e., image composition via elementary shapes) forrepresenting the structure information, and 3) the existence of saliency asprior. Since our synthesizer only considers the generic properties of naturalimages, the single model pretrained on our dataset can be consistentlytransferred to various target datasets, and even outperforms the previousmethods pretrained with the natural images in terms of Fr\u0026rsquo;echet inceptiondistance. Extensive analysis, ablation study, and evaluations demonstrate thateach component of our data synthesizer is effective, and provide insights onthe desirable nature of the pretrained model for the transferability of GANs.\r2022-04-10\nFew-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts\nSaadullah Amin Noon Pokaratsiri Goldstein Morgan Kelly Wixted Alejandro García-Rudolph Catalina Martínez-Costa Günter Neumann\nabstract\rabstract: Despite the advances in digital healthcare systems offering curatedstructured knowledge, much of the critical information still lies in largevolumes of unlabeled and unstructured clinical texts. These texts, which oftencontain protected health information (PHI), are exposed to informationextraction tools for downstream applications, risking patient identification.Existing works in de-identification rely on using large-scale annotated corporain English, which often are not suitable in real-world multilingual settings.Pre-trained language models (LM) have shown great potential for cross-lingualtransfer in low-resource settings. In this work, we empirically show thefew-shot cross-lingual transfer property of LMs for named entity recognition(NER) and apply it to solve a low-resource and real-world challenge ofcode-mixed (Spanish-Catalan) clinical notes de-identification in the strokedomain. We annotate a gold evaluation dataset to assess few-shot settingperformance where we only use a few hundred labeled examples for training. Ourmodel improves the zero-shot F1-score from 73.7% to 91.2% on the goldevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingualtarget corpus. When generalized to an out-of-sample test set, the best modelachieves a human-evaluation F1-score of 97.2%.\r2022-04-06\nDifferentially Private Set Union\nSivakanth Gopi Pankaj Gulhane Janardhan Kulkarni Judy Hanwen Shen Milad Shokouhi Sergey Yekhanin\nabstract\rabstract: We study the basic operation of set union in the global model of differentialprivacy. In this problem, we are given a universe $U$ of items, possibly ofinfinite size, and a database $D$ of users. Each user $i$ contributes a subset$W_i \\subseteq U$ of items. We want an ($\\epsilon$,$\\delta$)-differentiallyprivate algorithm which outputs a subset $S \\subset \\cup_i W_i$ such that thesize of $S$ is as large as possible. The problem arises in countless real worldapplications; it is particularly ubiquitous in natural language processing(NLP) applications as vocabulary extraction. For example, discovering words,sentences, $n$-grams etc., from private text data belonging to users is aninstance of the set union problem. Known algorithms for this problem proceed by collecting a subset of itemsfrom each user, taking the union of such subsets, and disclosing the itemswhose noisy counts fall above a certain threshold. Crucially, in the aboveprocess, the contribution of each individual user is always independent of theitems held by other users, resulting in a wasteful aggregation process, wheresome item counts happen to be way above the threshold. We deviate from theabove paradigm by allowing users to contribute their items in a$\\textit{dependent fashion}$, guided by a $\\textit{policy}$. In this newsetting ensuring privacy is significantly delicate. We prove that any policywhich has certain $\\textit{contractive}$ properties would result in adifferentially private algorithm. We design two new algorithms, one usingLaplace noise and other Gaussian noise, as specific instances of policiessatisfying the contractive properties. Our experiments show that the newalgorithms significantly outperform previously known mechanisms for theproblem.\r2022-04-04\nPrivacy in Open Search: A Review of Challenges and Solutions\nSamuel Sousa Christian Guetl Roman Kern\nabstract\rabstract: Privacy is of worldwide concern regarding activities and processes thatinclude sensitive data. For this reason, many countries and territories havebeen recently approving regulations controlling the extent to whichorganizations may exploit data provided by people. Artificial intelligenceareas, such as machine learning and natural language processing, have alreadysuccessfully employed privacy-preserving mechanisms in order to safeguard dataprivacy in a vast number of applications. Information retrieval (IR) islikewise prone to privacy threats, such as attacks and unintended disclosuresof documents and search history, which may cripple the security of users and bepenalized by data protection laws. This work aims at highlighting anddiscussing open challenges for privacy in the recent literature of IR, focusingon tasks featuring user-generated text data. Our contribution is threefold:firstly, we present an overview of privacy threats to IR tasks; secondly, wediscuss applicable privacy-preserving mechanisms which may be employed insolutions to restrain privacy hazards; finally, we bring insights on thetradeoffs between privacy preservation and utility performance for IR tasks.\rClues in Tweets: Twitter-Guided Discovery and Analysis of SMS Spam\nSiyuan Tang Xianghang Mi Ying Li XiaoFeng Wang Kai Chen\nabstract\rabstract: With its critical role in business and service delivery through mobiledevices, SMS (Short Message Service) has long been abused for spamming, whichis still on the rise today possibly due to the emergence of A2P bulk messaging.The effort to control SMS spam has been hampered by the lack of up-to-dateinformation about illicit activities. In our research, we proposed a novelsolution to collect recent SMS spam data, at a large scale, from Twitter, whereusers voluntarily report the spam messages they receive. For this purpose, wedesigned and implemented SpamHunter, an automated pipeline to discover SMS spamreporting tweets and extract message content from the attached screenshots.Leveraging SpamHunter, we collected from Twitter a dataset of 21,918 SMS spammessages in 75 languages, spanning over four years. To our best knowledge, thisis the largest SMS spam dataset ever made public. More importantly, SpamHunterenables us to continuously monitor emerging SMS spam messages, whichfacilitates the ongoing effort to mitigate SMS spamming. We also performed anin-depth measurement study that sheds light on the new trends in the spammer\u0026rsquo;sstrategies, infrastructure and spam campaigns. We also utilized our spam SMSdata to evaluate the robustness of the spam countermeasures put in place by theSMS ecosystem, including anti-spam services, bulk SMS services, and textmessaging apps. Our evaluation shows that such protection cannot effectivelyhandle those spam samples: either introducing significant false positives ormissing a large number of newly reported spam messages.\r2022-03-31\nPreliminary Steps Towards Federated Sentiment Classification\nXin-Chun Li Lan Li De-Chuan Zhan Yunfeng Shao Bingshuai Li Shaoming Song\nabstract\rabstract: Automatically mining sentiment tendency contained in natural language is afundamental research to some artificial intelligent applications, wheresolutions alternate with challenges. Transfer learning and multi-task learningtechniques have been leveraged to mitigate the supervision sparsity andcollaborate multiple heterogeneous domains correspondingly. Recent years, thesensitive nature of users\u0026rsquo; private data raises another challenge for sentimentclassification, i.e., data privacy protection. In this paper, we resort tofederated learning for multiple domain sentiment classification under theconstraint that the corpora must be stored on decentralized devices. In view ofthe heterogeneous semantics across multiple parties and the peculiarities ofword embedding, we pertinently provide corresponding solutions. First, wepropose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework forbetter model aggregation and personalization in federated sentimentclassification. Second, we propose KTEPS$^\\star$ with the consideration of therich semantic and huge embedding size properties of word vectors, utilizingProjection-based Dimension Reduction (PDR) methods for privacy protection andefficient transmission simultaneously. We propose two federated sentimentclassification scenes based on public benchmarks, and verify the superioritiesof our proposed methods with abundant experimental investigations.\rImproving speaker de-identification with functional data analysis of f0 trajectories\nLauri Tavi Tomi Kinnunen Rosa González Hautamäki\nabstract\rabstract: Due to a constantly increasing amount of speech data that is stored indifferent types of databases, voice privacy has become a major concern. Torespond to such concern, speech researchers have developed various methods forspeaker de-identification. The state-of-the-art solutions utilize deep learningsolutions which can be effective but might be unavailable or impractical toapply for, for example, under-resourced languages. Formant modification is asimpler, yet effective method for speaker de-identification which requires notraining data. Still, remaining intonational patterns in formant-anonymizedspeech may contain speaker-dependent cues. This study introduces a novelspeaker de-identification method, which, in addition to simple formant shifts,manipulates f0 trajectories based on functional data analysis. The proposedspeaker de-identification method will conceal plausibly identifying pitchcharacteristics in a phonetically controllable manner and improve formant-basedspeaker de-identification up to 25%.\r2022-03-28\nMixed Differential Privacy in Computer Vision\nAditya Golatkar Alessandro Achille Yu-Xiang Wang Aaron Roth Michael Kearns Stefano Soatto\nabstract\rabstract: We introduce AdaMix, an adaptive differentially private algorithm fortraining deep neural network classifiers using both private and public imagedata. While pre-training language models on large public datasets has enabledstrong differential privacy (DP) guarantees with minor loss of accuracy, asimilar practice yields punishing trade-offs in vision tasks. A few-shot oreven zero-shot learning baseline that ignores private data can outperformfine-tuning on a large private dataset. AdaMix incorporates few-shot training,or cross-modal zero-shot learning, on public data prior to private fine-tuning,to improve the trade-off. AdaMix reduces the error increase from thenon-private upper bound from the 167-311% of the baseline, on average across 6datasets, to 68-92% depending on the desired privacy level selected by theuser. AdaMix tackles the trade-off arising in visual classification, wherebythe most privacy sensitive data, corresponding to isolated points inrepresentation space, are also critical for high classification accuracy. Inaddition, AdaMix comes with strong theoretical privacy guarantees andconvergence analysis.\rThe MIT Voice Name System\nBrian Subirana Harry Levinson Ferran Hueto Prithvi Rajasekaran Alexander Gaidis Esteve Tarragó Peter Oliveira-Soens\nabstract\rabstract: This RFC white Paper summarizes our progress on the MIT Voice Name System(VNS) and Huey. The VNS, similar in name and function to the DNS, is a systemto reserve and use \u0026ldquo;wake words\u0026rdquo; to activate Artificial Intelligence (AI)devices. Just like you can say \u0026ldquo;Hey Siri\u0026rdquo; to activate Apple\u0026rsquo;s personalassistant, we propose using the VNS in smart speakers and other devices toroute wake requests based on commands such as \u0026ldquo;turn off\u0026rdquo;, \u0026ldquo;open groceryshopping list\u0026rdquo; or \u0026ldquo;271, start flash card review of my computer vision class\u0026rdquo;.We also introduce Huey, an unambiguous Natural Language to interact with AIdevices. We aim to standardize voice interactions to a universal reach similarto that of other systems such as phone numbering, with an agreed world-wideapproach to assign and use numbers, or the Internet\u0026rsquo;s DNS, with a standardnaming system, that has helped flourish popular services including theWorld-Wide-Web, FTP, and email. Just like these standards are \u0026ldquo;neutral\u0026rdquo;, wealso aim to endow the VNS with \u0026ldquo;wake neutrality\u0026rdquo; so that each participant candevelop its own digital voice. We focus on voice as a starting point to talk toany IoT object and explain briefly how the VNS may be expanded to other AItechnologies enabling person-to-machine conversations (reallymachine-to-machine), including computer vision or neural interfaces. We alsodescribe briefly considerations for a broader set of standards, MIT Open AI(MOA), including a reference architecture to serve as a starting point for thedevelopment of a general conversational commerce infrastructure that hasstandard \u0026ldquo;Wake Words\u0026rdquo;, NLP commands such as \u0026ldquo;Shopping Lists\u0026rdquo; or \u0026ldquo;Flash CardReviews\u0026rdquo;, and personalities such as Pi or 271. Privacy and security are keyelements considered because of speech-to-text errors and the amount of personalinformation contained in a voice sample.\r2022-03-24\nVerifiable Access Control for Augmented Reality Localization and Mapping\nShaowei Zhu Hyo Jin Kim Maurizio Monge G. Edward Suh Armin Alaghi Brandon Reagen Vincent Lee\nabstract\rabstract: Localization and mapping is a key technology for bridging the virtual andphysical worlds in augmented reality (AR). Localization and mapping works bycreating and querying maps made of anchor points that enable the overlay ofthese two worlds. As a result, information about the physical world is capturedin the map and naturally gives rise to concerns around who can map physicalspaces as well as who can access or modify the virtual ones. This paperdiscusses how we can provide access controls over virtual maps as a basicbuilding block to enhance security and privacy of AR systems. In particular, wepropose VACMaps: an access control system for localization and mapping usingformal methods. VACMaps defines a domain-specific language that enables usersto specify access control policies for virtual spaces. Access requests tovirtual spaces are then evaluated against relevant policies in a way thatpreserves confidentiality and integrity of virtual spaces owned by the users.The precise semantics of the policies are defined by SMT formulas, which allowVACMaps to reason about properties of access policies automatically. Anevaluation of VACMaps is provided using an AR testbed of a single-family home.We show that VACMaps is scalable in that it can run at practical speeds andthat it can also reason about access control policies automatically to detectpotential policy misconfigurations.\rClassifying Cyber-Risky Clinical Notes by Employing Natural Language Processing\nSuzanna Schmeelk Martins Samuel Dogo Yifan Peng Braja Gopal Patra\nabstract\rabstract: Clinical notes, which can be embedded into electronic medical records,document patient care delivery and summarize interactions between healthcareproviders and patients. These clinical notes directly inform patient care andcan also indirectly inform research and quality/safety metrics, among otherindirect metrics. Recently, some states within the United States of Americarequire patients to have open access to their clinical notes to improve theexchange of patient information for patient care. Thus, developing methods toassess the cyber risks of clinical notes before sharing and exchanging data iscritical. While existing natural language processing techniques are geared tode-identify clinical notes, to the best of our knowledge, few have focused onclassifying sensitive-information risk, which is a fundamental step towarddeveloping effective, widespread protection of patient health information. Tobridge this gap, this research investigates methods for identifyingsecurity/privacy risks within clinical notes. The classification either can beused upstream to identify areas within notes that likely contain sensitiveinformation or downstream to improve the identification of clinical notes thathave not been entirely de-identified. We develop several models using unigramand word2vec features with different classifiers to categorize sentence risk.Experiments on i2b2 de-identification dataset show that the SVM classifierusing word2vec features obtained a maximum F1-score of 0.792. Future researchinvolves articulation and differentiation of risk in terms of different globalregulatory requirements.\r2022-03-22\nA Girl Has A Name, And It\u0026rsquo;s \u0026hellip; Adversarial Authorship Attribution for Deobfuscation\nWanyue Zhai Jonathan Rusert Zubair Shafiq Padmini Srinivasan\nabstract\rabstract: Recent advances in natural language processing have enabled powerfulprivacy-invasive authorship attribution. To counter authorship attribution,researchers have proposed a variety of rule-based and learning-based textobfuscation approaches. However, existing authorship obfuscation approaches donot consider the adversarial threat model. Specifically, they are not evaluatedagainst adversarially trained authorship attributors that are aware ofpotential obfuscation. To fill this gap, we investigate the problem ofadversarial authorship attribution for deobfuscation. We show thatadversarially trained authorship attributors are able to degrade theeffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluatethe effectiveness of adversarial training when the attributor makes incorrectassumptions about whether and which obfuscator was used. While there is a aclear degradation in attribution accuracy, it is noteworthy that thisdegradation is still at or above the attribution accuracy of the attributorthat is not adversarially trained at all. Our results underline the need forstronger obfuscation approaches that are resistant to deobfuscation\r2022-03-20\nMitigating Gender Bias in Machine Translation through Adversarial Learning\nEve Fleisig Christiane Fellbaum\nabstract\rabstract: Machine translation and other NLP systems often contain significant biasesregarding sensitive attributes, such as gender or race, that worsen systemperformance and perpetuate harmful stereotypes. Recent preliminary researchsuggests that adversarial learning can be used as part of a model-agnostic biasmitigation method that requires no data modifications. However, adapting thisstrategy for machine translation and other modern NLP domains requires (1)restructuring training objectives in the context of fine-tuning pretrainedlarge language models and (2) developing measures for gender or other protectedvariables for tasks in which these attributes must be deduced from the dataitself. We present an adversarial learning framework that addresses these challengesto mitigate gender bias in seq2seq machine translation. Our framework improvesthe disparity in translation quality for sentences with male vs. femaleentities by 86% for English-German translation and 91% for English-Frenchtranslation, with minimal effect on translation quality. The results suggestthat adversarial learning is a promising technique for mitigating gender biasin machine translation.\rInterpretability of Fine-grained Classification of Sadness and Depression\nTiasa Singha Roy Priyam Basu Aman Priyanshu Rakshit Naidu\nabstract\rabstract: While sadness is a human emotion that people experience at certain timesthroughout their lives, inflicting them with emotional disappointment and pain,depression is a longer term mental illness which impairs social, occupational,and other vital regions of functioning making it a much more serious issue andneeds to be catered to at the earliest. NLP techniques can be utilized for thedetection and subsequent diagnosis of these emotions. Most of the open sourceddata on the web deal with sadness as a part of depression, as an emotion eventhough the difference in severity of both is huge. Thus, we create our ownnovel dataset illustrating the difference between the two. In this paper, weaim to highlight the difference between the two and highlight how interpretableour models are to distinctly label sadness and depression. Due to the sensitivenature of such information, privacy measures need to be taken for handling andtraining of such data. Hence, we also explore the effect of Federated Learning(FL) on contextualised language models.\r2022-03-19\nAnomaly Detection in Emails using Machine Learning and Header Information\nCraig Beaman Haruna Isah\nabstract\rabstract: Anomalies in emails such as phishing and spam present major security riskssuch as the loss of privacy, money, and brand reputation to both individualsand organizations. Previous studies on email anomaly detection relied on asingle type of anomaly and the analysis of the email body and subject content.A drawback of this approach is that it takes into account the written languageof the email content. To overcome this deficit, this study conducted featureextraction and selection on email header datasets and leveraged both multi andone-class anomaly detection approaches. Experimental analysis results obtaineddemonstrate that email header information only is enough to reliably detectspam and phishing emails. Supervised learning algorithms such as Random Forest,SVM, MLP, KNN, and their stacked ensembles were found to be very successful,achieving high accuracy scores of 97% for phishing and 99% for spam emails.One-class classification with One-Class SVM achieved accuracy scores of 87% and89% with spam and phishing emails, respectively. Real-world email filteringapplications will benefit from the use of only the header information in termsof resources utilization and efficiency.\rThe Online Behaviour of the Algerian Abusers in Social Media Networks\nKheireddine Abainia\nabstract\rabstract: Connecting to social media networks becomes a daily task for the majority ofpeople around the world, and the amount of shared information is growingexponentially. Thus, controlling the way in which people communicate isnecessary, in order to protect them from disorientation, conflicts,aggressions, etc. In this paper, we conduct a statistical study on thecyber-bullying and the abusive content in social media (i.e. Facebook), wherewe try to spot the online behaviour of the abusers in the Algerian community.More specifically, we have involved 200 Facebook users from different regionsamong 600 to carry out this study. The aim of this investigation is to aidautomatic systems of abuse detection to take decision by incorporating theonline activity. Abuse detection systems require a large amount of data toperform better on such kind of texts (i.e. unstructured and informal texts),and this is due to the lack of standard orthography, where there are variousAlgerian dialects and languages spoken.\r2022-03-17\nMultilingual Detection of Personal Employment Status on Twitter\nManuel Tonneau Dhaval Adjodah João Palotti Nir Grinberg Samuel Fraiberger\nabstract\rabstract: Detecting disclosures of individuals\u0026rsquo; employment status on social media canprovide valuable information to match job seekers with suitable vacancies,offer social protection, or measure labor market flows. However, identifyingsuch personal disclosures is a challenging task due to their rarity in a sea ofsocial media content and the variety of linguistic forms used to describe them.Here, we examine three Active Learning (AL) strategies in real-world settingsof extreme class imbalance, and identify five types of disclosures aboutindividuals\u0026rsquo; employment status (e.g. job loss) in three languages usingBERT-based classification models. Our findings show that, even under extremeimbalance settings, a small number of AL iterations is sufficient to obtainlarge and significant gains in precision, recall, and diversity of resultscompared to a supervised baseline with the same number of labels. We also findthat no AL strategy consistently outperforms the rest. Qualitative analysissuggests that AL helps focus the attention mechanism of BERT on core terms andadjust the boundaries of semantic expansion, highlighting the importance ofinterpretable models to provide greater control and visibility into thisdynamic learning process.\rKART: Parameterization of Privacy Leakage Scenarios from Pre-trained Language Models\nYuta Nakamura Shouhei Hanaoka Yukihiro Nomura Naoto Hayashi Osamu Abe Shuntaro Yada Shoko Wakamiya Eiji Aramaki\nabstract\rabstract: For the safe sharing pre-trained language models, no guidelines exist atpresent owing to the difficulty in estimating the upper bound of the risk ofprivacy leakage. One problem is that previous studies have assessed the riskfor different real-world privacy leakage scenarios and attack methods, whichreduces the portability of the findings. To tackle this problem, we representcomplex real-world privacy leakage scenarios under a universalparameterization, \\textit{Knowledge, Anonymization, Resource, and Target}(KART). KART parameterization has two merits: (i) it clarifies the definitionof privacy leakage in each experiment and (ii) it improves the comparability ofthe findings of risk assessments. We show that previous studies can be simplyreviewed by parameterizing the scenarios with KART. We also demonstrate privacyrisk assessments in different scenarios under the same attack method, whichsuggests that KART helps approximate the upper bound of risk under a specificattack or scenario. We believe that KART helps integrate past and futurefindings on privacy risk and will contribute to a standard for sharing languagemodels.\r2022-03-16\nCapsNet for Medical Image Segmentation\nMinh Tran Viet-Khoa Vo-Ho Kyle Quinn Hien Nguyen Khoa Luu Ngan Le\nabstract\rabstract: Convolutional Neural Networks (CNNs) have been successful in solving tasks incomputer vision including medical image segmentation due to their ability toautomatically extract features from unstructured data. However, CNNs aresensitive to rotation and affine transformation and their success relies onhuge-scale labeled datasets capturing various input variations. This networkparadigm has posed challenges at scale because acquiring annotated data formedical segmentation is expensive, and strict privacy regulations. Furthermore,visual representation learning with CNNs has its own flaws, e.g., it isarguable that the pooling layer in traditional CNNs tends to discard positionalinformation and CNNs tend to fail on input images that differ in orientationsand sizes. Capsule network (CapsNet) is a recent new architecture that hasachieved better robustness in representation learning by replacing poolinglayers with dynamic routing and convolutional strides, which has shownpotential results on popular tasks such as classification, recognition,segmentation, and natural language processing. Different from CNNs, whichresult in scalar outputs, CapsNet returns vector outputs, which aim to preservethe part-whole relationships. In this work, we first introduce the limitationsof CNNs and fundamentals of CapsNet. We then provide recent developments ofCapsNet for the task of medical image segmentation. We finally discuss variouseffective network architectures to implement a CapsNet for both 2D images and3D volumetric medical image segmentation.\rMeasuring Fairness of Text Classifiers via Prediction Sensitivity\nSatyapriya Krishna Rahul Gupta Apurv Verma Jwala Dhamala Yada Pruksachatkun Kai-Wei Chang\nabstract\rabstract: With the rapid growth in language processing applications, fairness hasemerged as an important consideration in data-driven solutions. Althoughvarious fairness definitions have been explored in the recent literature, thereis lack of consensus on which metrics most accurately reflect the fairness of asystem. In this work, we propose a new formulation : ACCUMULATED PREDICTIONSENSITIVITY, which measures fairness in machine learning models based on themodel\u0026rsquo;s prediction sensitivity to perturbations in input features. The metricattempts to quantify the extent to which a single prediction depends on aprotected attribute, where the protected attribute encodes the membershipstatus of an individual in a protected group. We show that the metric can betheoretically linked with a specific notion of group fairness (statisticalparity) and individual fairness. It also correlates well with humans\u0026rsquo;perception of fairness. We conduct experiments on two text classificationdatasets : JIGSAW TOXICITY, and BIAS IN BIOS, and evaluate the correlationsbetween metrics and manual annotations on whether the model produced a fairoutcome. We observe that the proposed fairness metric based on predictionsensitivity is statistically significantly more correlated with humanannotation than the existing counterfactual fairness metric.\r2022-03-15\nTraining a Tokenizer for Free with Private Federated Learning\nEugene Bagdasaryan Congzheng Song Rogier van Dalen Matt Seigel Áine Cahill\nabstract\rabstract: Federated learning with differential privacy, i.e. private federated learning(PFL), makes it possible to train models on private data distributed acrossusers\u0026rsquo; devices without harming privacy. PFL is efficient for models, such asneural networks, that have a fixed number of parameters, and thus afixed-dimensional gradient vector. Such models include neural-net languagemodels, but not tokenizers, the topic of this work. Training a tokenizerrequires frequencies of words from an unlimited vocabulary, and existingmethods for finding an unlimited vocabulary need a separate privacy budget. A workaround is to train the tokenizer on publicly available data. However,in this paper we first show that a tokenizer trained on mismatched data resultsin worse model performance compared to a privacy-violating \u0026ldquo;oracle\u0026rdquo; tokenizerthat accesses user data, with perplexity increasing by 20%. We also show thatsub-word tokenizers are better suited to the federated context than word-levelones, since they can encode new words, though with more tokens per word. Second, we propose a novel method to obtain a tokenizer without using anyadditional privacy budget. During private federated learning of the languagemodel, we sample from the model, train a new tokenizer on the sampledsequences, and update the model embeddings. We then continue private federatedlearning, and obtain performance within 1% of the \u0026ldquo;oracle\u0026rdquo; tokenizer. Sincethis process trains the tokenizer only indirectly on private data, we can usethe \u0026ldquo;postprocessing guarantee\u0026rdquo; of differential privacy and thus use noadditional privacy budget.\r2022-03-14\nThe Big-O Problem\nDmitry Chistikov Stefan Kiefer Andrzej S. Murawski David Purser\nabstract\rabstract: Given two weighted automata, we consider the problem of whether one is big-Oof the other, i.e., if the weight of every finite word in the first is notgreater than some constant multiple of the weight in the second. We show that the problem is undecidable, even for the instantiation ofweighted automata as labelled Markov chains. Moreover, even when it is knownthat one weighted automaton is big-O of another, the problem of finding orapproximating the associated constant is also undecidable. Our positive results show that the big-O problem is polynomial-time solvablefor unambiguous automata, coNP-complete for unlabelled weighted automata (i.e.,when the alphabet is a single character) and decidable, subject to Schanuel\u0026rsquo;sconjecture, when the language is bounded (i.e., a subset of $w_1^\\dots w_m^$for some finite words $w_1,\\dots,w_m$) or when the automaton has finiteambiguity. On labelled Markov chains, the problem can be restated as a ratio totalvariation distance, which, instead of finding the maximum difference betweenthe probabilities of any two events, finds the maximum ratio between theprobabilities of any two events. The problem is related to$\\varepsilon$-differential privacy, for which the optimal constant of the big-Onotation is exactly $\\exp(\\varepsilon)$.\rIn Search of Lost Utility: Private Location Data\nSzilvia Lestyán Gergely Ács Gergely Biczók\nabstract\rabstract: The unavailability of training data is a permanent source of much frustrationin research, especially when it is due to privacy concerns. This isparticularly true for location data since previous techniques all suffer fromthe inherent sparseness and high dimensionality of location trajectories whichrender most techniques impractical, resulting in unrealistic traces andunscalable methods. Moreover, time information of location visits is usuallydropped, or its resolution is drastically reduced. In this paper we present anovel technique for privately releasing a composite generative model and wholehigh-dimensional location datasets with detailed time information. To generatehigh-fidelity synthetic data, we leverage several peculiarities of vehicularmobility such as its language-like characteristics (\u0026ldquo;you should know a locationby the company it keeps\u0026rdquo;) or how humans plan their trips from one point to theother. We model the generator distribution of the dataset by first constructinga variational autoencoder to generate the source and destination locations, andthe corresponding timing of trajectories. Next, we compute transitionprobabilities between locations with a feed forward network, and build atransition graph from the output of this model, which approximates thedistribution of all paths between the source and destination (at a given time).Finally, a path is sampled from this distribution with a Markov Chain MonteCarlo method. The generated synthetic dataset is highly realistic, scalable,provides good utility and, nonetheless, provably private. We evaluate our modelagainst two state-of-the-art methods and three real-life datasets demonstratingthe benefits of our approach.\r2022-03-12\nOn Information Hiding in Natural Language Systems\nGeetanjali Bihani Julia Taylor Rayz\nabstract\rabstract: With data privacy becoming more of a necessity than a luxury in today\u0026rsquo;sdigital world, research on more robust models of privacy preservation andinformation security is on the rise. In this paper, we take a look at NaturalLanguage Steganography (NLS) methods, which perform information hiding innatural language systems, as a means to achieve data security as well asconfidentiality. We summarize primary challenges regarding the secrecy andimperceptibility requirements of these systems and propose potential directionsof improvement, specifically targeting steganographic text quality. We believethat this study will act as an appropriate framework to build more resilientmodels of Natural Language Steganography, working towards instilling securitywithin natural language-based neural models.\r2022-03-02\nA Blockchain-Based Consent Mechanism for Access to Fitness Data in the Healthcare Context\nMay Alhajri Carsten Rudolph Ahmad Salehi Shahraki\nabstract\rabstract: Wearable fitness devices are widely used to track an individual\u0026rsquo;s health andphysical activities to improve the quality of health services. These devicessense a considerable amount of sensitive data processed by a centralized thirdparty. While many researchers have thoroughly evaluated privacy issuessurrounding wearable fitness trackers, no study has addressed privacy issues intrackers by giving control of the data to the user. Blockchain is an emergingtechnology with outstanding advantages in resolving consent management privacyconcerns. As there are no fully transparent, legally compliant solutions forsharing personal fitness data, this study introduces an architecture for ahuman-centric, legally compliant, decentralized and dynamic consent systembased on blockchain and smart contracts. Algorithms and sequence diagrams ofthe proposed system\u0026rsquo;s activities show consent-related data flow among variousagents, which are used later to prove the system\u0026rsquo;s trustworthiness byformalizing the security requirements. The security properties of the proposedsystem were evaluated using the formal security modeling framework SeMF, whichdemonstrates the feasibility of the solution at an abstract level based onformal language theory. As a result, we have empirically proven that blockchaintechnology is suitable for mitigating the privacy issues of fitness providersby recording individuals\u0026rsquo; consent using blockchain and smart contracts.\rAn Efficient DP-SGD Mechanism for Large Scale NLP Models\nChristophe Dupuy Radhika Arava Rahul Gupta Anna Rumshisky\nabstract\rabstract: Recent advances in deep learning have drastically improved performance onmany Natural Language Understanding (NLU) tasks. However, the data used totrain NLU models may contain private information such as addresses or phonenumbers, particularly when drawn from human subjects. It is desirable thatunderlying models do not expose private information contained in the trainingdata. Differentially Private Stochastic Gradient Descent (DP-SGD) has beenproposed as a mechanism to build privacy-preserving models. However, DP-SGD canbe prohibitively slow to train. In this work, we propose a more efficientDP-SGD for training using a GPU infrastructure and apply it to fine-tuningmodels based on LSTM and transformer architectures. We report faster trainingtimes, alongside accuracy, theoretical privacy guarantees and success ofMembership inference attacks for our models and observe that fine-tuning withproposed variant of DP-SGD can yield competitive models without significantdegradation in training time and improvement in privacy protection. We alsomake observations such as looser theoretical $\\epsilon, \\delta$ can translateinto significant practical privacy gains.\r2022-03-01\nCompliance Checking with NLI: Privacy Policies vs. Regulations\nAmin Rabinia Zane Nygaard\nabstract\rabstract: A privacy policy is a document that states how a company intends to handleand manage their customers\u0026rsquo; personal data. One of the problems that arises withthese privacy policies is that their content might violate data privacyregulations. Because of the enormous number of privacy policies that exist, theonly realistic way to check for legal inconsistencies in all of them is throughan automated method. In this work, we use Natural Language Inference (NLI)techniques to compare privacy regulations against sections of privacy policiesfrom a selection of large companies. Our NLI model uses pre-trained embeddings,along with BiLSTM in its attention mechanism. We tried two versions of ourmodel: one that was trained on the Stanford Natural Language Inference (SNLI)and the second on the Multi-Genre Natural Language Inference (MNLI) dataset. Wefound that our test accuracy was higher on our model trained on the SNLI, butwhen actually doing NLI tasks on real world privacy policies, the model trainedon MNLI generalized and performed much better.\r2022-02-26\nA Robust Document Image Watermarking Scheme using Deep Neural Network\nSulong Ge Zhihua Xia Jianwei Fei Xingming Sun Jian Weng\nabstract\rabstract: Watermarking is an important copyright protection technology which generallyembeds the identity information into the carrier imperceptibly. Then theidentity can be extracted to prove the copyright from the watermarked carriereven after suffering various attacks. Most of the existing watermarkingtechnologies take the nature images as carriers. Different from the naturalimages, document images are not so rich in color and texture, and thus haveless redundant information to carry watermarks. This paper proposes anend-to-end document image watermarking scheme using the deep neural network.Specifically, an encoder and a decoder are designed to embed and extract thewatermark. A noise layer is added to simulate the various attacks that could beencountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussiannoise, Resize, and JPEG Compression. A text-sensitive loss function is designedto limit the embedding modification on characters. An embedding strengthadjustment strategy is proposed to improve the quality of watermarked imagewith little loss of extraction accuracy. Experimental results show that theproposed document image watermarking technology outperforms threestate-of-the-arts in terms of the robustness and image quality.\r2022-02-23\nAbsolute Zero-Shot Learning\nRui Gao Fan Wan Daniel Organisciak Jiyao Pu Junyan Wang Haoran Duan Peng Zhang Xingsong Hou Yang Long\nabstract\rabstract: Considering the increasing concerns about data copyright and privacy issues,we present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., traininga classifier with zero real data. The key innovation is to involve a teachermodel as the data safeguard to guide the AZSL model training without dataleaking. The AZSL model consists of a generator and student network, which canachieve date-free knowledge transfer while maintaining the performance of theteacher network. We investigate black-box' and white-box\u0026rsquo; scenarios in AZSLtask as different levels of model security. Besides, we also provide discussionof teacher model in both inductive and transductive settings. Despiteembarrassingly simple implementations and data-missing disadvantages, our AZSLframework can retain state-of-the-art ZSL and GZSL performance under thewhite-box' scenario. Extensive qualitative and quantitative analysis alsodemonstrates promising results when deploying the model under black-box\u0026rsquo;scenario.\r2022-02-19\nProgrammable Interface for Statistical \u0026amp; Simulation Models (PRISM): Towards Greater Accessibility of Clinical and Healthcare Decision Models\nAmin Adibi Stephanie Harvard Mohsen Sadatsafavi\nabstract\rabstract: Background: Increasingly, decision-making in healthcare relies on computermodels, be it clinical prediction models at point of care or decision-analyticmodels at the policymaking level. Given the important role models play in bothcontexts, their structure and implementation be rigorously scrutinized. Theability to interrogate input/output associations without facing barriers canimprove quality assurance mechanisms while satisfying privacy/confidentialityconcerns and facilitating the integration of models into decision-making. Thispaper reports on the development of Programmable Interface for Statistical \u0026amp;Simulation Models (PRISM), a cloud-based platform for model accessibility.Methods: PRISM emphasizes two main principles: 1) minimal specifications on theside of model developer to make the model fit for cloud hosting, and 2) makingclient access completely independent of the resource requirement and softwaredependencies of the model. The server architecture integrates a RESTfulApplication Programming Interface (API) infrastructure, JSON for data transfer,a routing layer for access management, container technology for management ofcomputer resources and package dependencies, and the capacity for synchronousor asynchronous model calls. Results: We discuss the architecture, the minimalAPI standards that enable a universal language for access to such models, theunderlying server infrastructure, and the standards used for data transfer. Aninstance of PRISM is available as a service via the Peer Models Networkhttp://peermodelsnetwork.com. Through a series of case studies, we demonstratehow interrogating models becomes possible in standardized fashion, in a waythat is irrespective of the specifics of any model. Conclusions: We havedeveloped a publicly accessible platform and minimalist standards thatfacilitate model accessibility for both clinical and policy models.\r2022-02-17\nWhen BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing\nChao-Han Huck Yang Jun Qi Samuel Yen-Chi Chen Yu Tsao Pin-Yu Chen\nabstract\rabstract: The rapid development of quantum computing has demonstrated many uniquecharacteristics of quantum advantages, such as richer feature representationand more secured protection on model parameters. This work proposes a verticalfederated learning architecture based on variational quantum circuits todemonstrate the competitive performance of a quantum-enhanced pre-trained BERTmodel for text classification. In particular, our proposed hybridclassical-quantum model consists of a novel random quantum temporal convolution(QTC) learning framework replacing some layers in the BERT-based decoder. Ourexperiments on intent classification show that our proposed BERT-QTC modelattains competitive experimental results in the Snips and ATIS spoken languagedatasets. Particularly, the BERT-QTC boosts the performance of the existingquantum circuit-based language model in two text classification datasets by1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasiblydeployed on both existing commercial-accessible quantum computation hardwareand CPU-based interface for ensuring data isolation.\r2022-02-16\nRegional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal\nFelipe González-Pizarro Andrea Figueroa Claudia López Cecilia Aragon\nabstract\rabstract: While there is increasing global attention to data privacy, most of theircurrent theoretical understanding is based on research conducted in a fewcountries. Prior work argues that people\u0026rsquo;s cultural backgrounds might shapetheir privacy concerns; thus, we could expect people from different worldregions to conceptualize them in diverse ways. We collected and analyzed alarge-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanishand English to start exploring this hypothesis. We employed word embeddings andqualitative analysis to identify which information privacy concerns are presentand characterize language and regional differences in emphasis on theseconcerns. Our results suggest that related concepts, such as regulations, canbe added to current information privacy frameworks. We also observe a greateremphasis on data collection in English than in Spanish. Additionally, data fromNorth America exhibits a narrower focus on awareness compared to other regionsunder study. Our results call for more diverse sources of data and nuancedanalysis of data privacy concerns around the globe.\r2022-02-15\nDefending against Reconstruction Attacks with Rényi Differential Privacy\nPierre Stock Igor Shilov Ilya Mironov Alexandre Sablayrolles\nabstract\rabstract: Reconstruction attacks allow an adversary to regenerate data samples of thetraining set using access to only a trained model. It has been recently shownthat simple heuristics can reconstruct data samples from language models,making this threat scenario an important aspect of model release. Differentialprivacy is a known solution to such attacks, but is often used with arelatively large privacy budget (epsilon \u0026gt; 8) which does not translate tomeaningful guarantees. In this paper we show that, for a same mechanism, we canderive privacy guarantees for reconstruction attacks that are better than thetraditional ones from the literature. In particular, we show that largerprivacy budgets do not protect against membership inference, but can stillprotect extraction of rare secrets. We show experimentally that our guaranteeshold against various language models, including GPT-2 finetuned onWikitext-103.\r2022-02-14\nThreats to Pre-trained Language Models: Survey and Taxonomy\nShangwei Guo Chunlong Xie Jiwei Li Lingjuan Lyu Tianwei Zhang\nabstract\rabstract: Pre-trained language models (PTLMs) have achieved great success andremarkable performance over a wide range of natural language processing (NLP)tasks. However, there are also growing concerns regarding the potentialsecurity issues in the adoption of PTLMs. In this survey, we comprehensivelysystematize recently discovered threats to PTLM systems and applications. Weperform our attack characterization from three interesting perspectives. (1) Weshow threats can occur at different stages of the PTLM pipeline raised bydifferent malicious entities. (2) We identify two types of modeltransferability (landscape, portrait) that facilitate attacks. (3) Based on theattack goals, we summarize four categories of attacks (backdoor, evasion, dataprivacy and model privacy). We also discuss some open problems and researchdirections. We believe our survey and taxonomy will inspire future studiestowards secure and privacy-preserving PTLMs.\rWhat Does it Mean for a Language Model to Preserve Privacy?\nHannah Brown Katherine Lee Fatemehsadat Mireshghallah Reza Shokri Florian Tramèr\nabstract\rabstract: Natural language reflects our private lives and identities, making itsprivacy concerns as broad as those of real life. Language models lack theability to understand the context and sensitivity of text, and tend to memorizephrases present in their training sets. An adversary can exploit this tendencyto extract training data. Depending on the nature of the content and thecontext in which this data was collected, this could violate expectations ofprivacy. Thus there is a growing interest in techniques for training languagemodels that preserve privacy. In this paper, we discuss the mismatch betweenthe narrow assumptions made by popular data protection techniques (datasanitization and differential privacy), and the broadness of natural languageand of privacy as a social norm. We argue that existing protection methodscannot guarantee a generic and meaningful notion of privacy for languagemodels. We conclude that language models should be trained on text data whichwas explicitly produced for public use.\r2022-02-12\nWav2Vec2.0 on the Edge: Performance Evaluation\nSantosh Gondi\nabstract\rabstract: Wav2Vec2.0 is a state-of-the-art model which learns speech representationsthrough unlabeled speech data, aka, self supervised learning. The pretrainedmodel is then fine tuned on small amounts of labeled data to use it forspeech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformativesolution for low resource languages as it is mainly developed using unlabeledaudio data. Getting large amounts of labeled data is resource intensive andespecially challenging to do for low resource languages such as Swahilli,Tatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpassesthe very recent supervised learning algorithms while using 100x less labeleddata. Given its importance and enormous potential in enabling speech basedtasks on world\u0026rsquo;s 7000 languages, it is key to evaluate the accuracy, latencyand efficiency of this model on low resource and low power edge devices andinvestigate the feasibility of using it in such devices for private, secure andreliable speech based tasks. On-device speech tasks preclude sending audio datato the server hence inherently providing privacy, reduced latency and enhancedreliability. In this paper, Wav2Vec2.0 model\u0026rsquo;s accuracy and latency has beenevaluated on Raspberry Pi along with the KenLM language model for speechrecognition tasks. How to tune certain parameters to achieve desired level ofWER rate and latency while meeting the CPU, memory and energy budgets of theproduct has been discussed.\r2022-02-10\nNÜWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN\nMinheng Ni Chenfei Wu Haoyang Huang Daxin Jiang Wangmeng Zuo Nan Duan\nabstract\rabstract: Language guided image inpainting aims to fill in the defective regions of animage under the guidance of text while keeping non-defective regions unchanged.However, the encoding process of existing models suffers from either receptivespreading of defective regions or information loss of non-defective regions,giving rise to visually unappealing inpainting results. To address the aboveissues, this paper proposes N\u0026quot;UWA-LIP by incorporating defect-free VQGAN(DF-VQGAN) with multi-perspective sequence to sequence (MP-S2S). In particular,DF-VQGAN introduces relative estimation to control receptive spreading andadopts symmetrical connections to protect information. MP-S2S further enhancesvisual information from complementary perspectives, including both low-levelpixels and high-level tokens. Experiments show that DF-VQGAN performs morerobustness than VQGAN. To evaluate the inpainting performance of our model, webuilt up 3 open-domain benchmarks, where N\u0026quot;UWA-LIP is also superior to recentstrong baselines.\r2022-02-09\nFedQAS: Privacy-aware machine reading comprehension with federated learning\nAddi Ait-Mlouk Sadi Alawadi Salman Toor Andreas Hellander\nabstract\rabstract: Machine reading comprehension (MRC) of text data is one important task inNatural Language Understanding. It is a complex NLP problem with a lot ofongoing research fueled by the release of the Stanford Question AnsweringDataset (SQuAD) and Conversational Question Answering (CoQA). It is consideredto be an effort to teach computers how to \u0026ldquo;understand\u0026rdquo; a text, and then to beable to answer questions about it using deep learning. However, until nowlarge-scale training on private text data and knowledge sharing has beenmissing for this NLP task. Hence, we present FedQAS, a privacy-preservingmachine reading system capable of leveraging large-scale private data withoutthe need to pool those datasets in a central location. The proposed approachcombines transformer models and federated learning technologies. The system isdeveloped using the FEDn framework and deployed as a proof-of-concept allianceinitiative. FedQAS is flexible, language-agnostic, and allows intuitiveparticipation and execution of local model training. In addition, we presentthe architecture and implementation of the system, as well as provide areference evaluation based on the SQUAD dataset, to showcase how it overcomesdata privacy issues and enables knowledge sharing between alliance members in aFederated learning setting.\rFairness-aware Summarization for Justified Decision-Making\nMoniba Keymanesh Tanya Berger-Wolf Micha Elsner Srinivasan Parthasarathy\nabstract\rabstract: In consequential domains such as recidivism prediction, facility inspection,and benefit assignment, it\u0026rsquo;s important for individuals to know thedecision-relevant information for the model\u0026rsquo;s prediction. In addition,predictions should be fair both in terms of the outcome and the justificationof the outcome. In other words, decision-relevant features should providesufficient information for the predicted outcome and should be independent ofthe membership of individuals in protected groups such as race and gender. Inthis work, we focus on the problem of (un)fairness in the justification of thetext-based neural models. We tie the explanatory power of the model to fairnessin the outcome and propose a fairness-aware summarization mechanism to detectand counteract the bias in such models. Given a potentially biased naturallanguage explanation for a decision, we use a multi-task neural model and anattribution mechanism based on integrated gradients to extract high-utility andlow-bias justifications in form of a summary. The extracted summary is thenused for training a model to make decisions for individuals. Results on severalreal world datasets suggest that our method drastically limits the demographicleakage in the input (fairness in justification) while moderately enhancing thefairness in the outcome. Our model is also effective in detecting andcounteracting several types of data poisoning attacks that synthesizerace-coded reasoning or irrelevant justifications.\r2022-02-07\nDeletion Inference, Reconstruction, and Compliance in Machine (Un)Learning\nJi Gao Sanjam Garg Mohammad Mahmoody Prashant Nalini Vasudevan\nabstract\rabstract: Privacy attacks on machine learning models aim to identify the data that isused to train such models. Such attacks, traditionally, are studied on staticmodels that are trained once and are accessible by the adversary. Motivated tomeet new legal requirements, many machine learning methods are recentlyextended to support machine unlearning, i.e., updating models as if certainexamples are removed from their training sets, and meet new legal requirements.However, privacy attacks could potentially become more devastating in this newsetting, since an attacker could now access both the original model beforedeletion and the new model after the deletion. In fact, the very act ofdeletion might make the deleted record more vulnerable to privacy attacks. Inspired by cryptographic definitions and the differential privacy framework,we formally study privacy implications of machine unlearning. We formalize(various forms of) deletion inference and deletion reconstruction attacks, inwhich the adversary aims to either identify which record is deleted or toreconstruct (perhaps part of) the deleted records. We then present successfuldeletion inference and reconstruction attacks for a variety of machine learningmodels and tasks such as classification, regression, and language models.Finally, we show that our attacks would provably be precluded if the schemessatisfy (variants of) Deletion Compliance (Garg, Goldwasser, and Vasudevan,Eurocrypt\u0026rsquo; 20).\r2022-02-05\nReGVD: Revisiting Graph Neural Networks for Vulnerability Detection\nVan-Anh Nguyen Dai Quoc Nguyen Van Nguyen Trung Le Quan Hung Tran Dinh Phung\nabstract\rabstract: Identifying vulnerabilities in the source code is essential to protect thesoftware systems from cyber security attacks. It, however, is also achallenging step that requires specialized expertise in security and coderepresentation. To this end, we aim to develop a general, practical, andprogramming language-independent model capable of running on various sourcecodes and libraries without difficulty. Therefore, we consider vulnerabilitydetection as an inductive text classification problem and propose ReGVD, asimple yet effective graph neural network-based model for the problem. Inparticular, ReGVD views each raw source code as a flat sequence of tokens tobuild a graph, wherein node features are initialized by only the tokenembedding layer of a pre-trained programming language (PL) model. ReGVD thenleverages residual connection among GNN layers and examines a mixture ofgraph-level sum and max poolings to return a graph embedding for the sourcecode. ReGVD outperforms the existing state-of-the-art models and obtains thehighest accuracy on the real-world benchmark dataset from CodeXGLUE forvulnerability detection. Our code is available at:\\url{https://github.com/daiquocnguyen/GNN-ReGVD}.\r2022-02-03\nEggCounts: a Bayesian hierarchical toolkit to model faecal egg count reductions\nCraig Wang Reinhard Furrer\nabstract\rabstract: This is a vignette for the R package eggCounts version 2.0. The packageimplements a suite of Bayesian hierarchical models dealing with faecal eggcount reductions. The models are designed for a variety of practicalsituations, including individual treatment efficacy, zero inflation, smallsample size (less than 10) and potential outliers. The functions are intuitiveto use and their output are easy to interpret, such that users are protectedfrom being exposed to complex Bayesian hierarchical modelling tasks. Inaddition, the package includes plotting functions to display data and resultsin a visually appealing manner. The models are implemented in Stan modellinglanguage, which provides efficient sampling technique to obtain posteriorsamples. This vignette briefly introduces different models, and provides ashort walk-through analysis with example data.\r2022-02-02\nDetecting Privacy Requirements from User Stories with NLP Transfer Learning Models\nFrancesco Casillo Vincenzo Deufemia Carmine Gravino\nabstract\rabstract: To provide privacy-aware software systems, it is crucial to consider privacyfrom the very beginning of the development. However, developers do not have theexpertise and the knowledge required to embed the legal and social requirementsfor data protection into software systems. Objective: We present an approach todecrease privacy risks during agile software development by automaticallydetecting privacy-related information in the context of user storyrequirements, a prominent notation in agile Requirement Engineering (RE).Methods: The proposed approach combines Natural Language Processing (NLP) andlinguistic resources with deep learning algorithms to identify privacy aspectsinto User Stories. NLP technologies are used to extract information regardingthe semantic and syntactic structure of the text. This information is thenprocessed by a pre-trained convolutional neural network, which paved the wayfor the implementation of a Transfer Learning technique. We evaluate theproposed approach by performing an empirical study with a dataset of 1680 userstories. Results: The experimental results show that deep learning algorithmsallow to obtain better predictions than those achieved with conventional(shallow) machine learning methods. Moreover, the application of TransferLearning allows to considerably improve the accuracy of the predictions, ca.10%. Conclusions: Our study contributes to encourage software engineeringresearchers in considering the opportunities to automate privacy detection inthe early phase of design, by also exploiting transfer learning models.\r2022-01-30\nFormalism-Driven Development of Decentralized Systems\nYepeng Ding Hiroyuki Sato\nabstract\rabstract: Decentralized systems have been widely developed and applied to addresssecurity and privacy issues in centralized systems, especially since theadvancement of distributed ledger technology. However, it is challenging toensure their correct functioning with respect to their designs and minimize thetechnical risk before the delivery. Although formal methods have madesignificant progress over the past decades, a feasible solution based on formalmethods from a development process perspective has not been well developed. Inthis paper, we formulate an iterative and incremental development process,named formalism-driven development (FDD), for developing provably correctdecentralized systems under the guidance of formal methods. We also present aframework named Seniz, to practicalize FDD with a new modeling language andscaffolds. Furthermore, we conduct case studies to demonstrate theeffectiveness of FDD in practice with the support of Seniz.\r2022-01-26\nTwitter-Demographer: A Flow-based Tool to Enrich Twitter Data\nFederico Bianchi Vincenzo Cutrona Dirk Hovy\nabstract\rabstract: Twitter data have become essential to Natural Language Processing (NLP) andsocial science research, driving various scientific discoveries in recentyears. However, the textual data alone are often not enough to conduct studies:especially social scientists need more variables to perform their analysis andcontrol for various factors. How we augment this information, such as users\u0026rsquo;location, age, or tweet sentiment, has ramifications for anonymity andreproducibility, and requires dedicated effort. This paper describesTwitter-Demographer, a simple, flow-based tool to enrich Twitter data withadditional information about tweets and users. Twitter-Demographer is aimed atNLP practitioners and (computational) social scientists who want to enrichtheir datasets with aggregated information, facilitating reproducibility, andproviding algorithmic privacy-by-design measures for pseudo-anonymity. Wediscuss our design choices, inspired by the flow-based programming paradigm, touse black-box components that can easily be chained together and extended. Wealso analyze the ethical issues related to the use of this tool, and thebuilt-in measures to facilitate pseudo-anonymity.\r2022-01-24\nPolytope: Practical Memory Access Control for C++ Applications\nIoannis Agadakos Manuel Egele William Robertson\nabstract\rabstract: Designing and implementing secure software is inarguably more important thanever. However, despite years of research into privilege separating programs, itremains difficult to actually do so and such efforts can take years oflabor-intensive engineering to reach fruition. At the same time, newintra-process isolation primitives make strong data isolation and privilegeseparation more attractive from a performance perspective. Yet, substitutingintra-process security boundaries for time-tested process boundaries opens thedoor to subtle but devastating privilege leaks. In this work, we presentPolytope, a language extension to C++ that aims to make efficient privilegeseparation accessible to a wider audience of developers. Polytope defines apolicy language encoded as C++11 attributes that separate code and data intodistinct program partitions. A modified Clang front-end embeds source-levelpolicy as metadata nodes in the LLVM IR. An LLVM pass interprets embeddedpolicy and instruments an IR with code to enforce the source-level policy usingIntel MPK. A run-time support library manages partitions, protection keys,dynamic memory operations, and indirect call target privileges. An evaluationdemonstrates that Polytope provides equivalent protection to prior systems witha low annotation burden and comparable performance overhead. Polytope alsorenders privilege leaks that contradict intended policy impossible to express.\r2022-01-22\nhybrid-Falcon: Hybrid Pattern Malware Detection and Categorization with Network Traffic and Program Code\nPeng Xu Claudia Eckert Apostolis Zarras\nabstract\rabstract: Nowadays, Android is the most dominant operating system in the mobileecosystem, with billions of people using its apps daily. As expected, thistrend did not go unnoticed by miscreants, and Android became the favoriteplatform for discovering new victims through malicious apps. Moreover, theseapps have become so sophisticated that they can bypass anti-malware measures toprotect the users. Therefore, it is safe to admit that traditional anti-malwaretechniques have become cumbersome, sparking the urge to develop an efficientway to detect Android malware. This paper presents hybrid-Flacon, a hybrid pattern Android malware detectionand categorization framework. It combines dynamic and static features ofAndroid malware, which are from network traffic and code graph structure. Inhybrid-Flacon, we treat network traffic as a dynamic feature and process it asa 2D image sequence. Meanwhile, hybrid-Flacon handles each network flow in thepacket as a 2D image and uses a bidirectional LSTM network to process those2D-image sequences to obtain vectors representing network packets. We use theprogram code graph for a static feature and introduce natural languageprocessing (NLP) inspired techniques on function call graph (FCG). We design agraph neural network-based approach to convert the whole graph structure ofAndroid apps to vectors. Finally, We utilize those converted vectors, bothnetwork and program code features, and concatenate them to detect andcategorize the malware. Our results reveal that hybrid-Flacon yields betterresults as we get 97.16% accuracy on average for malware detection and 88.32%accuracy for malware categorization. Additionally, we release a datasetAndroNetMnist, which converts the network traffic to a 2D-image sequence andhelps to accomplish malware detection on a 2D-image sequence.\r2022-01-21\nPrivacy Policies Across the Ages: Content and Readability of Privacy Policies 1996\u0026ndash;2021\nIsabel Wagner\nabstract\rabstract: It is well-known that most users do not read privacy policies, but almost allusers tick the box to agree with them. In this paper, we analyze the 25-yearhistory of privacy policies using methods from transparency research, machinelearning, and natural language processing. Specifically, we collect alarge-scale longitudinal corpus of privacy policies from 1996 to 2021 andanalyze the length and readability of privacy policies as well as their contentin terms of the data practices they describe, the rights they grant to users,and the rights they reserve for their organizations. We pay particularattention to changes in response to recent privacy regulations such as the GDPRand CCPA. Our results show that policies are getting longer and harder to read,especially after new regulations take effect, and we find a range of concerningdata practices. Our results allow us to speculate why privacy policies arerarely read and propose changes that would make privacy policies serve theirreaders instead of their writers.\r2022-01-17\nDynamic Differential-Privacy Preserving SGD\nJian Du Song Li Xiangyi Chen Siheng Chen Mingyi Hong\nabstract\rabstract: The vanilla Differentially-Private Stochastic Gradient Descent (DP-SGD),including DP-Adam and other variants, ensures the privacy of training data byuniformly distributing privacy costs across training steps. The equivalentprivacy costs controlled by maintaining the same gradient clipping thresholdsand noise powers in each step result in unstable updates and a lower modelaccuracy when compared to the non-DP counterpart. In this paper, we propose thedynamic DP-SGD (along with dynamic DP-Adam, and others) to reduce theperformance loss gap while maintaining privacy by dynamically adjustingclipping thresholds and noise powers while adhering to a total privacy budgetconstraint. Extensive experiments on a variety of deep learning tasks,including image classification, natural language processing, and federatedlearning, demonstrate that the proposed dynamic DP-SGD algorithm stabilizesupdates and, as a result, significantly improves model accuracy in the strongprivacy protection region when compared to the vanilla DP-SGD. We also conducttheoretical analysis to better understand the privacy-utility trade-off withdynamic DP-SGD, as well as to learn why Dynamic DP-SGD can outperform vanillaDP-SGD.\rRobust Aggregation for Federated Learning\nKrishna Pillutla Sham M. Kakade Zaid Harchaoui\nabstract\rabstract: Federated learning is the centralized training of statistical models fromdecentralized data on mobile devices while preserving the privacy of eachdevice. We present a robust aggregation approach to make federated learningrobust to settings when a fraction of the devices may be sending corruptedupdates to the server. The approach relies on a robust aggregation oracle basedon the geometric median, which returns a robust aggregate using a constantnumber of iterations of a regular non-robust averaging oracle. The robustaggregation oracle is privacy-preserving, similar to the non-robust secureaverage oracle it builds upon. We establish its convergence for least squaresestimation of additive models. We provide experimental results with linearmodels and deep networks for three tasks in computer vision and naturallanguage processing. The robust aggregation approach is agnostic to the levelof corruption; it outperforms the classical aggregation approach in terms ofrobustness when the level of corruption is high, while being competitive in theregime of low corruption. Two variants, a faster one with one-step robustaggregation and another one with on-device personalization, round off thepaper.\r2022-01-15\nHow are Diverse End-user Human-centric Issues Discussed on GitHub?\nHourieh Khalajzadeh Mojtaba Shahin Humphrey O. Obie John Grundy\nabstract\rabstract: Many software systems fail to meet the needs of the diverse end-users insociety and are prone to pose problems, such as accessibility and usabilityissues. Some of these problems (partially) stem from the failure to considerthe characteristics, limitations, and abilities of diverse end-users duringsoftware development. We refer to this class of problems as human-centricissues. Despite their importance, there is a limited understanding of the typesof human-centric issues encountered by developers. In-depth knowledge of thesehuman-centric issues is needed to design software systems that better meettheir diverse end-users\u0026rsquo; needs. This paper aims to provide insights for thesoftware development and research communities on which human-centric issues area topic of discussion for developers on GitHub. We conducted an empirical studyby extracting and manually analysing 1,691 issue comments from 12 diverseprojects, ranging from small to large-scale projects, including projectsdesigned for challenged end-users, e.g., visually impaired and dyslexic users.Our analysis shows that eight categories of human-centric issues are discussedby developers. These include Inclusiveness, Privacy \u0026amp; Security, Compatibility,Location \u0026amp; Language, Preference, Satisfaction, Emotional Aspects, andAccessibility. Guided by our findings, we highlight some implications andpossible future paths to further understand and incorporate human-centricissues in software development to be able to design software that meets theneeds of diverse end users in society.\rAddressing the Challenges of Cross-Lingual Hate Speech Detection\nIrina Bigoulaeva Viktor Hangya Iryna Gurevych Alexander Fraser\nabstract\rabstract: The goal of hate speech detection is to filter negative online content aimingat certain groups of people. Due to the easy accessibility of social mediaplatforms it is crucial to protect everyone which requires building hate speechdetection systems for a wide range of languages. However, the available labeledhate speech datasets are limited making it problematic to build systems formany languages. In this paper we focus on cross-lingual transfer learning tosupport hate speech detection in low-resource languages. We leveragecross-lingual word embeddings to train our neural network systems on the sourcelanguage and apply it to the target language, which lacks labeled examples, andshow that good performance can be achieved. We then incorporate unlabeledtarget language data for further model improvements by bootstrapping labelsusing an ensemble of different model architectures. Furthermore, we investigatethe issue of label imbalance of hate speech datasets, since the high ratio ofnon-hate examples compared to hate examples often leads to low modelperformance. We test simple data undersampling and oversampling techniques andshow their effectiveness.\r2022-01-14\nSkillVet: Automated Traceability Analysis of Amazon Alexa Skills\nJide S Edu Xavier Ferrer-Aran Jose M Such Guillermo Suarez-Tangil\nabstract\rabstract: Third-party software, or skills, are essential components in Smart PersonalAssistants (SPA). The number of skills has grown rapidly, dominated by achanging environment that has no clear business model. Skills can accesspersonal information and this may pose a risk to users. However, there islittle information about how this ecosystem works, let alone the tools that canfacilitate its study. In this paper, we present the largest systematicmeasurement of the Amazon Alexa skill ecosystem to date. We study developers\u0026rsquo;practices in this ecosystem, including how they collect and justify the needfor sensitive information, by designing a methodology to identifyover-privileged skills with broken privacy policies. We collect 199,295 Alexaskills and uncover that around 43% of the skills (and 50% of the developers)that request these permissions follow bad privacy practices, including(partially) broken data permissions traceability. In order to perform this kindof analysis at scale, we present SkillVet that leverages machine learning andnatural language processing techniques, and generates high-accuracy predictionsets. We report a number of concerning practices including how developers canbypass Alexa\u0026rsquo;s permission system through account linking and conversationalskills, and offer recommendations on how to improve transparency, privacy andsecurity. Resulting from the responsible disclosure we have conducted, 13% ofthe reported issues no longer pose a threat at submission time.\r2022-01-05\nWikipedia Reader Navigation: When Synthetic Data Is Enough\nAkhil Arora Martin Gerlach Tiziano Piccardi Alberto García-Durán Robert West\nabstract\rabstract: Every day millions of people read Wikipedia. When navigating the vast spaceof available topics using hyperlinks, readers describe trajectories on thearticle network. Understanding these navigation patterns is crucial to betterserve readers\u0026rsquo; needs and address structural biases and knowledge gaps. However,systematic studies of navigation on Wikipedia are hindered by a lack ofpublicly available data due to the commitment to protect readers\u0026rsquo; privacy bynot storing or sharing potentially sensitive data. In this paper, we ask: Howwell can Wikipedia readers\u0026rsquo; navigation be approximated by using publiclyavailable resources, most notably the Wikipedia clickstream data? Wesystematically quantify the differences between real navigation sequences andsynthetic sequences generated from the clickstream data, in 6 analyses across 8Wikipedia language versions. Overall, we find that the differences between realand synthetic sequences are statistically significant, but with small effectsizes, often well below 10%. This constitutes quantitative evidence for theutility of the Wikipedia clickstream data as a public resource: clickstreamdata can closely capture reader navigation on Wikipedia and provides asufficient approximation for most practical downstream applications relying onreader data. More broadly, this study provides an example for howclickstream-like data can generally enable research on user navigation ononline platforms while protecting users\u0026rsquo; privacy.\r2022-01-04\nSubmix: Practical Private Prediction for Large-Scale Language Models\nAntonio Ginart Laurens van der Maaten James Zou Chuan Guo\nabstract\rabstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model\u0026rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.\r2022-01-03\nAI \u0026amp; Racial Equity: Understanding Sentiment Analysis Artificial Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems\nAlia Abbas\nabstract\rabstract: Various forms of implications of artificial intelligence that eitherexacerbate or decrease racial systemic injustice have been explored in thisapplied research endeavor. Taking each thematic area of identifying, analyzing,and debating an systemic issue have been leveraged in investigating merits anddrawbacks of using algorithms to automate human decision making in raciallysensitive environments. It has been asserted through the analysis of historicalsystemic patterns, implicit biases, existing algorithmic risks, and legalimplications that natural language processing based AI, such as risk assessmenttools, have racially disparate outcomes. It is concluded that more litigativepolicies are needed to regulate and restrict how internal governmentinstitutions and corporations utilize algorithms, privacy and security risks,and auditing requirements in order to diverge from racially injustice outcomesand practices of the past.\r2021-12-31\nInverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer\nChin-Tung Lin Mu Yang\nabstract\rabstract: Many social media users prefer consuming content in the form of videos ratherthan text. However, in order for content creators to produce videos with a highclick-through rate, much editing is needed to match the footage to the music.This posts additional challenges for more amateur video makers. Therefore, wepropose a novel attention-based model VMT (Video-Music Transformer) thatautomatically generates piano scores from video frames. Using music generatedfrom models also prevent potential copyright infringements that often come withusing existing music. To the best of our knowledge, there is no work besidesthe proposed VMT that aims to compose music for video. Additionally, therelacks a dataset with aligned video and symbolic music. We release a new datasetcomposed of over 7 hours of piano scores with fine alignment between pop musicvideos and MIDI files. We conduct experiments with human evaluation on VMT,SeqSeq model (our baseline), and the original piano version soundtrack. VMTachieves consistent improvements over the baseline on music smoothness andvideo relevance. In particular, with the relevance scores and our case study,our model has shown the capability of multimodality on frame-level actors\u0026rsquo;movement for music generation. Our VMT model, along with the new dataset,presents a promising research direction toward composing the matchingsoundtrack for videos. We have released our code athttps://github.com/linchintung/VMT\rData-Free Knowledge Transfer: A Survey\nYuang Liu Wei Zhang Jun Wang Jianyong Wang\nabstract\rabstract: In the last decade, many deep learning models have been well trained and madea great success in various fields of machine intelligence, especially forcomputer vision and natural language processing. To better leverage thepotential of these well-trained models in intra-domain or cross-domain transferlearning situations, knowledge distillation (KD) and domain adaptation (DA) areproposed and become research highlights. They both aim to transfer usefulinformation from a well-trained model with original training data. However, theoriginal data is not always available in many cases due to privacy, copyrightor confidentiality. Recently, the data-free knowledge transfer paradigm hasattracted appealing attention as it deals with distilling valuable knowledgefrom well-trained models without requiring to access to the training data. Inparticular, it mainly consists of the data-free knowledge distillation (DFKD)and source data-free domain adaptation (SFDA). On the one hand, DFKD aims totransfer the intra-domain knowledge of original data from a cumbersome teachernetwork to a compact student network for model compression and efficientinference. On the other hand, the goal of SFDA is to reuse the cross-domainknowledge stored in a well-trained source model and adapt it to a targetdomain. In this paper, we provide a comprehensive survey on data-free knowledgetransfer from the perspectives of knowledge distillation and unsuperviseddomain adaptation, to help readers have a better understanding of the currentresearch status and ideas. Applications and challenges of the two areas arebriefly reviewed, respectively. Furthermore, we provide some insights to thesubject of future research.\r"}]