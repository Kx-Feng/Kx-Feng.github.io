<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="2024-03-20
Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal
Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel
abstractabstract: The rapid integration of Large Language Models (LLMs) across diverse sectorshas marked a transformative era, showcasing remarkable capabilities in textgeneration and problem-solving tasks. However, this technological advancementis accompanied by significant risks and vulnerabilities. Despite ongoingsecurity enhancements, attackers persistently exploit these weaknesses, castingdoubts on the overall trustworthiness of LLMs. Compounding the issue,organisations are deploying LLM-integrated systems without understanding theseverity of potential consequences."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Vulnerable LLM"><meta property="og:description" content="2024-03-20
Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal
Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel
abstractabstract: The rapid integration of Large Language Models (LLMs) across diverse sectorshas marked a transformative era, showcasing remarkable capabilities in textgeneration and problem-solving tasks. However, this technological advancementis accompanied by significant risks and vulnerabilities. Despite ongoingsecurity enhancements, attackers persistently exploit these weaknesses, castingdoubts on the overall trustworthiness of LLMs. Compounding the issue,organisations are deploying LLM-integrated systems without understanding theseverity of potential consequences."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/arxiv_papers/vulnerable_llm/"><meta property="article:section" content="docs"><title>Vulnerable LLM | Xiaoyan Feng</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=http://localhost:1313/docs/arxiv_papers/vulnerable_llm/><link rel=stylesheet href=/book.min.16628528a7a2a88d96389ec90a07d3f66879aff48e8cbc1f53ddc5b7ddb7ab85.css integrity="sha256-FmKFKKeiqI2WOJ7JCgfT9mh5r/SOjLwfU93Ft923q4U=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.5022df9a2a9d614876a347a0787bc0bd5a6cf9bd959d2003064a83c70c35340d.js integrity="sha256-UCLfmiqdYUh2o0egeHvAvVps+b2VnSADBkqDxww1NA0=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Xiaoyan Feng</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/about_me/>Home</a></li><li><span>Arxiv Papers</span><ul><li><a href=/docs/arxiv_papers/llm_copyright/>LLM with Copyright</a></li><li><a href=/docs/arxiv_papers/llm_privacy/>LLM with Privacy</a></li><li><a href=/docs/arxiv_papers/machine_unlearning/>Machine Unlearning</a></li><li><a href=/docs/arxiv_papers/vulnerable_llm/ class=active>Vulnerable LLM</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Vulnerable LLM</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><blockquote><p><strong><em>2024-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.13309v1><strong>Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal</strong></a></p><p><em>Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid integration of Large Language Models (LLMs) across diverse sectorshas marked a transformative era, showcasing remarkable capabilities in textgeneration and problem-solving tasks. However, this technological advancementis accompanied by significant risks and vulnerabilities. Despite ongoingsecurity enhancements, attackers persistently exploit these weaknesses, castingdoubts on the overall trustworthiness of LLMs. Compounding the issue,organisations are deploying LLM-integrated systems without understanding theseverity of potential consequences. Existing studies by OWASP and MITRE offer ageneral overview of threats and vulnerabilities but lack a method for directlyand succinctly analysing the risks for security practitioners, developers, andkey decision-makers who are working with this novel technology. To address thisgap, we propose a risk assessment process using tools like the OWASP riskrating methodology which is used for traditional systems. We conduct scenarioanalysis to identify potential threat agents and map the dependent systemcomponents against vulnerability factors. Through this analysis, we assess thelikelihood of a cyberattack. Subsequently, we conduct a thorough impactanalysis to derive a comprehensive threat matrix. We also map threats againstthree key stakeholder groups: developers engaged in model fine-tuning,application developers utilizing third-party APIs, and end users. The proposedthreat matrix provides a holistic evaluation of LLM-related risks, enablingstakeholders to make informed decisions for effective mitigation strategies.Our outlined process serves as an actionable and comprehensive tool forsecurity practitioners, offering insights for resource management and enhancingthe overall system security.</div></details><p><a href=http://arxiv.org/abs/2307.08309v2><strong>LogPr√©cis: Unleashing Language Models for Automated Shell Log Analysis</strong></a></p><p><em>Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The collection of security-related logs holds the key to understanding attackbehaviors and diagnosing vulnerabilities. Still, their analysis remains adaunting challenge. Recently, Language Models (LMs) have demonstrated unmatchedpotential in understanding natural and programming languages. The questionarises whether and how LMs could be also useful for security experts sincetheir logs contain intrinsically confused and obfuscated information. In thispaper, we systematically study how to benefit from the state-of-the-art in LMto automatically analyze text-like Unix shell attack logs. We present athorough design methodology that leads to LogPr'ecis. It receives as input rawshell sessions and automatically identifies and assigns the attacker tactic toeach portion of the session, i.e., unveiling the sequence of the attacker&rsquo;sgoals. We demonstrate LogPr'ecis capability to support the analysis of twolarge datasets containing about 400,000 unique Unix shell attacks. LogPr'ecisreduces them into about 3,000 fingerprints, each grouping sessions with thesame sequence of tactics. The abstraction it provides lets the analyst betterunderstand attacks, identify fingerprints, detect novelty, link similarattacks, and track families and mutations. Overall, LogPr'ecis, released asopen source, paves the way for better and more responsive defense againstcyberattacks.</div></details><blockquote><p><strong><em>2024-03-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.13193v1><strong>A Study of Vulnerability Repair in JavaScript Programs with Large Language Models</strong></a></p><p><em>Tan Khang Le, Saba Alimadadi, Steven Y. Ko</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, JavaScript has become the most widely used programminglanguage, especially in web development. However, writing secure JavaScriptcode is not trivial, and programmers often make mistakes that lead to securityvulnerabilities in web applications. Large Language Models (LLMs) havedemonstrated substantial advancements across multiple domains, and theirevolving capabilities indicate their potential for automatic code generationbased on a required specification, including automatic bug fixing. In thisstudy, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding andfixing security vulnerabilities in JavaScript programs. We also investigate theimpact of context in a prompt on directing LLMs to produce a correct patch ofvulnerable JavaScript code. Our experiments on real-world softwarevulnerabilities show that while LLMs are promising in automatic program repairof JavaScript code, achieving a correct bug fix often requires an appropriateamount of context in the prompt.</div></details><p><a href=http://arxiv.org/abs/2403.12503v1><strong>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</strong></a></p><p><em>Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have significantly transformed the landscape ofNatural Language Processing (NLP). Their impact extends across a diversespectrum of tasks, revolutionizing how we approach language understanding andgenerations. Nevertheless, alongside their remarkable utility, LLMs introducecritical security and risk considerations. These challenges warrant carefulexamination to ensure responsible deployment and safeguard against potentialvulnerabilities. This research paper thoroughly investigates security andprivacy concerns related to LLMs from five thematic perspectives: security andprivacy concerns, vulnerabilities against adversarial attacks, potential harmscaused by misuses of LLMs, mitigation strategies to address these challengeswhile identifying limitations of current strategies. Lastly, the paperrecommends promising avenues for future research to enhance the security andrisk management of LLMs.</div></details><p><a href=http://arxiv.org/abs/2403.12446v1><strong>On the effectiveness of Large Language Models for GitHub Workflows</strong></a></p><p><em>Xinyu Zhang, Siddharth Muralee, Sourag Cherupattamoolayil, Aravind Machiry</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: GitHub workflows or GitHub CI is a popular continuous integration platformthat enables developers to automate various software engineering tasks byspecifying them as workflows, i.e., YAML files with a list of jobs. However,engineering valid workflows is tedious. They are also prone to severe securityissues, which can result in supply chain vulnerabilities. Recent advancementsin Large Language Models (LLMs) have demonstrated their effectiveness invarious software development tasks. However, GitHub workflows differ fromregular programs in both structure and semantics. We perform the firstcomprehensive study to understand the effectiveness of LLMs on fiveworkflow-related tasks with different levels of prompts. We curated a set of$\sim$400K workflows and generated prompts with varying detail. We alsofine-tuned LLMs on GitHub workflow tasks. Our evaluation of threestate-of-the-art LLMs and their fine-tuned variants revealed variousinteresting findings on the current effectiveness and drawbacks of LLMs.</div></details><blockquote><p><strong><em>2024-03-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.06838v2><strong>ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts</strong></a></p><p><em>Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Smart contracts are susceptible to various security issues, among whichaccess control (AC) vulnerabilities are particularly critical. While existingresearch has proposed multiple detection tools, the automatic and appropriaterepair of AC vulnerabilities in smart contracts remains a challenge. Unlikecommonly supported vulnerability types by existing repair tools, such asreentrancy, which are usually fixed by template-based approaches, the mainobstacle of AC lies in identifying the appropriate roles or permissions amid along list of non-AC-related source code to generate proper patch code, a taskthat demands human-level intelligence. Leveraging recent advancements in large language models (LLMs), we employ thestate-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX.The key insight is that we can mine common AC practices for major categories ofcode functionality and use them to guide LLMs in fixing code with similarfunctionality. To this end, ACFIX involves both offline and online phases.First, during the offline phase, ACFIX mines a taxonomy of common Role-basedAccess Control (RBAC) practices from 344,251 on-chain contracts, categorizing49 role-permission pairs from the top 1,000 pairs mined. Second, during theonline phase, ACFIX tracks AC-related elements across the contract and usesthis context information along with a Chain-of-Thought pipeline to guide LLMsin identifying the most appropriate role-permission pair for the subjectcontract and subsequently generating a suitable patch. This patch will thenundergo a validity and effectiveness check. To evaluate ACFIX, we built thefirst benchmark dataset of 118 real-world AC vulnerabilities, and ourevaluation revealed that ACFIX successfully repaired 94.92% of them. Thisrepresents a significant improvement compared to the baseline GPT-4, whichachieved only 52.54%.</div></details><p><a href=http://arxiv.org/abs/2403.12171v1><strong>EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models</strong></a></p><p><em>Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Jailbreak attacks are crucial for identifying and mitigating the securityvulnerabilities of Large Language Models (LLMs). They are designed to bypasssafeguards and elicit prohibited outputs. However, due to significantdifferences among various jailbreak methods, there is no standardimplementation framework available for the community, which limitscomprehensive security evaluations. This paper introduces EasyJailbreak, aunified framework simplifying the construction and evaluation of jailbreakattacks against LLMs. It builds jailbreak attacks using four components:Selector, Mutator, Constraint, and Evaluator. This modular framework enablesresearchers to easily construct attacks from combinations of novel and existingcomponents. So far, EasyJailbreak supports 11 distinct jailbreak methods andfacilitates the security validation of a broad spectrum of LLMs. Our validationacross 10 distinct LLMs reveals a significant vulnerability, with an averagebreach probability of 60% under various jailbreaking attacks. Notably, evenadvanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack SuccessRates (ASR) of 57% and 33%, respectively. We have released a wealth ofresources for researchers, including a web platform, PyPI published package,screencast video, and experimental outputs.</div></details><p><a href=http://arxiv.org/abs/2403.12239v1><strong>Large language models in 6G security: challenges and opportunities</strong></a></p><p><em>Tri Nguyen, Huong Nguyen, Ahmad Ijaz, Saeid Sheikhi, Athanasios V. Vasilakos, Panos Kostakos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid integration of Generative AI (GenAI) and Large Language Models(LLMs) in sectors such as education and healthcare have marked a significantadvancement in technology. However, this growth has also led to a largelyunexplored aspect: their security vulnerabilities. As the ecosystem thatincludes both offline and online models, various tools, browser plugins, andthird-party applications continues to expand, it significantly widens theattack surface, thereby escalating the potential for security breaches. Theseexpansions in the 6G and beyond landscape provide new avenues for adversariesto manipulate LLMs for malicious purposes. We focus on the security aspects ofLLMs from the viewpoint of potential adversaries. We aim to dissect theirobjectives and methodologies, providing an in-depth analysis of known securityweaknesses. This will include the development of a comprehensive threattaxonomy, categorizing various adversary behaviors. Also, our research willconcentrate on how LLMs can be integrated into cybersecurity efforts by defenseteams, also known as blue teams. We will explore the potential synergy betweenLLMs and blockchain technology, and how this combination could lead to thedevelopment of next-generation, fully autonomous security solutions. Thisapproach aims to establish a unified cybersecurity strategy across the entirecomputing continuum, enhancing overall digital security infrastructure.</div></details><blockquote><p><strong><em>2024-03-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.05720v4><strong>Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning</strong></a></p><p><em>Jianwei Li, Sheng Liu, Qi Lei</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models trained via federated learning (FL) demonstrate impressivecapabilities in handling complex tasks while protecting user privacy. Recentstudies indicate that leveraging gradient information and prior knowledge canpotentially reveal training samples within FL setting. However, theseinvestigations have overlooked the potential privacy risks tied to theintrinsic architecture of the models. This paper presents a two-stage privacyattack strategy that targets the vulnerabilities in the architecture ofcontemporary language models, significantly enhancing attack performance byinitially recovering certain feature directions as additional supervisorysignals. Our comparative experiments demonstrate superior attack performanceacross various datasets and scenarios, highlighting the privacy leakage riskassociated with the increasingly complex architectures of language models. Wecall for the community to recognize and address these potential privacy risksin designing large language models.</div></details><blockquote><p><strong><em>2024-03-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.07865v2><strong>Exploring Safety Generalization Challenges of Large Language Models via Code</strong></a></p><p><em>Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid advancement of Large Language Models (LLMs) has brought aboutremarkable capabilities in natural language processing but also raised concernsabout their potential misuse. While strategies like supervised fine-tuning andreinforcement learning from human feedback have enhanced their safety, thesemethods primarily focus on natural languages, which may not generalize to otherdomains. This paper introduces CodeAttack, a framework that transforms naturallanguage inputs into code inputs, presenting a novel environment for testingthe safety generalization of LLMs. Our comprehensive studies onstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal acommon safety vulnerability of these models against code input: CodeAttackconsistently bypasses the safety guardrails of all models more than 80% of thetime. Furthermore, we find that a larger distribution gap between CodeAttackand natural language leads to weaker safety generalization, such as encodingnatural language input with data structures or using less popular programminglanguages. These findings highlight new safety risks in the code domain and theneed for more robust safety alignment algorithms to match the code capabilitiesof LLMs.</div></details><p><a href=http://arxiv.org/abs/2403.09792v1><strong>Images are Achilles&rsquo; Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models</strong></a></p><p><em>Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we study the harmlessness alignment problem of multimodallarge language models~(MLLMs). We conduct a systematic empirical analysis ofthe harmlessness performance of representative MLLMs and reveal that the imageinput poses the alignment vulnerability of MLLMs. Inspired by this, we proposea novel jailbreak method named HADES, which hides and amplifies the harmfulnessof the malicious intent within the text input, using meticulously craftedimages. Experimental results show that HADES can effectively jailbreak existingMLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% forLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publiclyreleased.</div></details><p><a href=http://arxiv.org/abs/2403.07708v2><strong>Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards</strong></a></p><p><em>Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigmused to align large language models (LLMs) with human preferences. Yet existingRLHF heavily relies on accurate and informative reward models, which arevulnerable and sensitive to noise from various sources, e.g. human labelingerrors, making the pipeline fragile. In this work, we improve the effectivenessof the reward model by introducing a penalty term on the reward, named as\textit{contrastive rewards}. %Contrastive rewards Our approach involves twosteps: (1) an offline sampling step to obtain responses to prompts that serveas baseline calculation and (2) a contrastive reward calculated using thebaseline responses and used in the Proximal Policy Optimization (PPO) step. Weshow that contrastive rewards enable the LLM to penalize reward uncertainty,improve robustness, encourage improvement over baselines, calibrate accordingto task difficulty, and reduce variance in PPO. We show empirically contrastiverewards can improve RLHF substantially, evaluated by both GPTs and humans, andour method consistently outperforms strong baselines.</div></details><p><a href=http://arxiv.org/abs/2403.09795v1><strong>Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention</strong></a></p><p><em>Ellie Prosser, Matthew Edwards</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Powerful generative Large Language Models (LLMs) are becoming popular toolsamongst the general public as question-answering systems, and are beingutilised by vulnerable groups such as children. With children increasinglyinteracting with these tools, it is imperative for researchers to scrutinisethe safety of LLMs, especially for applications that could lead to seriousoutcomes, such as online child safety queries. In this paper, the efficacy ofLLMs for online grooming prevention is explored both for identifying andavoiding grooming through advice generation, and the impact of prompt design onmodel performance is investigated by varying the provided context and promptspecificity. In results reflecting over 6,000 LLM interactions, we find that nomodels were clearly appropriate for online grooming prevention, with anobserved lack of consistency in behaviours, and potential for harmful answergeneration, especially from open-source models. We outline where and how modelsfall short, providing suggestions for improvement, and identify prompt designsthat heavily altered model performance in troubling ways, with findings thatcan be used to inform best practice usage guides.</div></details><p><a href=http://arxiv.org/abs/2403.09513v1><strong>AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</strong></a></p><p><em>Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advent and widespread deployment of Multimodal Large Language Models(MLLMs), the imperative to ensure their safety has become increasinglypronounced. However, with the integration of additional modalities, MLLMs areexposed to new vulnerabilities, rendering them prone to structured-basedjailbreak attacks, where semantic content (e.g., &ldquo;harmful text&rdquo;) has beeninjected into the images to mislead MLLMs. In this work, we aim to defendagainst such threats. Specifically, we propose \textbf{Ada}ptive\textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs withdefense prompts to defend MLLMs against structure-based jailbreak attackswithout fine-tuning MLLMs or training additional modules (e.g., post-stagecontent detector). Initially, we present a manually designed static defenseprompt, which thoroughly examines the image and instruction content step bystep and specifies response methods to malicious queries. Furthermore, weintroduce an adaptive auto-refinement framework, consisting of a target MLLMand a LLM-based defense prompt generator (Defender). These componentscollaboratively and iteratively communicate to generate a defense prompt.Extensive experiments on the popular structure-based jailbreak attacks andbenign datasets show that our methods can consistently improve MLLMs&rsquo;robustness against structure-based jailbreak attacks without compromising themodel&rsquo;s general capabilities evaluated on standard benign tasks. Our code isavailable at <a href=https://github.com/rain305f/AdaShield>https://github.com/rain305f/AdaShield</a>.</div></details><p><a href=http://arxiv.org/abs/2403.09572v1><strong>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</strong></a></p><p><em>Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multimodal large language models (MLLMs) have shown impressive reasoningabilities, which, however, are also more vulnerable to jailbreak attacks thantheir LLM predecessors. Although still capable of detecting unsafe responses,we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can beeasily bypassed due to the introduction of image features. To construct robustMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-freeprotecting approach that exploits the inherent safety awareness of MLLMs, andgenerates safer responses via adaptively transforming unsafe images into textsto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSOenhances model safety significantly (e.g., a 37.6% improvement on theMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), whileconsistently maintaining utility results on common MLLM benchmarks.Furthermore, we show that ECSO can be used as a data engine to generatesupervised-finetuning (SFT) data for MLLM alignment without extra humanintervention.</div></details><p><a href=http://arxiv.org/abs/2403.09346v1><strong>AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions</strong></a></p><p><em>Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Vision-Language Models (LVLMs) have shown significant progress in wellresponding to visual-instructions from users. However, these instructions,encompassing images and text, are susceptible to both intentional andinadvertent attacks. Despite the critical importance of LVLMs&rsquo; robustnessagainst such threats, current research in this area remains limited. To bridgethis gap, we introduce AVIBench, a framework designed to analyze the robustnessof LVLMs when facing various adversarial visual-instructions (AVIs), includingfour types of image-based AVIs, ten types of text-based AVIs, and nine types ofcontent bias AVIs (such as gender, violence, cultural, and racial biases, amongothers). We generate 260K AVIs encompassing five categories of multimodalcapabilities (nine tasks) and content bias. We then conduct a comprehensiveevaluation involving 14 open-source LVLMs to assess their performance. AVIBenchalso serves as a convenient tool for practitioners to evaluate the robustnessof LVLMs against AVIs. Our findings and extensive experimental results shedlight on the vulnerabilities of LVLMs, and highlight that inherent biases existeven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. Thisunderscores the importance of enhancing the robustness, security, and fairnessof LVLMs. The source code and benchmark will be made publicly available.</div></details><blockquote><p><strong><em>2024-03-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.08429v1><strong>Software Vulnerability and Functionality Assessment using LLMs</strong></a></p><p><em>Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While code review is central to the software development process, it can betedious and expensive to carry out. In this paper, we investigate whether andhow Large Language Models (LLMs) can aid with code reviews. Our investigationfocuses on two tasks that we argue are fundamental to good reviews: (i)flagging code with security vulnerabilities and (ii) performing softwarefunctionality validation, i.e., ensuring that code meets its intendedfunctionality. To test performance on both tasks, we use zero-shot andchain-of-thought prompting to obtain final ``approve or reject&rsquo;&lsquo;recommendations. As data, we employ seminal code generation datasets (HumanEvaland MBPP) along with expert-written code snippets with security vulnerabilitiesfrom the Common Weakness Enumeration (CWE). Our experiments consider a mixtureof three proprietary models from OpenAI and smaller open-source LLMs. We findthat the former outperforms the latter by a large margin. Motivated bypromising results, we finally ask our models to provide detailed descriptionsof security vulnerabilities. Results show that 36.7% of LLM-generateddescriptions can be associated with true CWE vulnerabilities.</div></details><p><a href=http://arxiv.org/abs/2403.08481v1><strong>SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks</strong></a></p><p><em>Guy Amit, Abigail Goldsteen, Ariel Farkash</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Natural language processing models have experienced a significant upsurge inrecent years, with numerous applications being built upon them. Many of theseapplications require fine-tuning generic base models on customized, proprietarydatasets. This fine-tuning data is especially likely to contain personal orsensitive information about individuals, resulting in increased privacy risk.Membership inference attacks are the most commonly employed attack to assessthe privacy leakage of a machine learning model. However, limited research isavailable on the factors that affect the vulnerability of language models tothis kind of attack, or on the applicability of different defense strategies inthe language domain. We provide the first systematic review of thevulnerability of fine-tuned large language models to membership inferenceattacks, the various factors that come into play, and the effectiveness ofdifferent defense strategies. We find that some training methods providesignificantly reduced privacy risk, with the combination of differentialprivacy and low-rank adaptors achieving the best privacy protection againstthese attacks.</div></details><p><a href=http://arxiv.org/abs/2312.03853v2><strong>Dr. Jekyll and Mr. Hyde: Two Faces of LLMs</strong></a></p><p><em>Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Only a year ago, we witnessed a rise in the use of Large Language Models(LLMs), especially when combined with applications like chatbot assistants.Safety mechanisms and specialized training procedures are implemented toprevent improper responses from these assistants. In this work, we bypass thesemeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making themimpersonate complex personas with opposite characteristics as those of thetruthful assistants they are supposed to be. We start by creating elaboratebiographies of these personas, which we then use in a new session with the samechatbots. Our conversation followed a role-play style to get the response theassistant was not allowed to provide. By making use of personas, we show thatthe response that is prohibited is actually provided, making it possible toobtain unauthorized, illegal, or harmful information. This work shows that byusing adversarial personas, one can overcome safety mechanisms set out byChatGPT and Bard. We also introduce several ways of activating such adversarialpersonas, altogether showing that both chatbots are vulnerable to this kind ofattack. With the same principle, we introduce two defenses that push the modelto interpret trustworthy personalities and make it more robust against suchattacks.</div></details><p><a href=http://arxiv.org/abs/2403.08424v1><strong>Tastle: Distract Large Language Models for Automatic Jailbreak Attack</strong></a></p><p><em>Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have achieved significant advances in recentdays. Extensive efforts have been made before the public release of LLMs toalign their behaviors with human values. The primary goal of alignment is toensure their helpfulness, honesty and harmlessness. However, even meticulouslyaligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,leading to unintended behaviors. The jailbreak is to intentionally develop amalicious prompt that escapes from the LLM security restrictions to produceuncensored detrimental contents. Previous works explore different jailbreakmethods for red teaming LLMs, yet they encounter challenges regarding toeffectiveness and scalability. In this work, we propose Tastle, a novelblack-box jailbreak framework for automated red teaming of LLMs. We designedmalicious content concealing and memory reframing with an iterativeoptimization algorithm to jailbreak LLMs, motivated by the research about thedistractibility and over-confidence phenomenon of LLMs. Extensive experimentsof jailbreaking both open-source and proprietary LLMs demonstrate thesuperiority of our framework in terms of effectiveness, scalability andtransferability. We also evaluate the effectiveness of existing jailbreakdefense methods against our attack and highlight the crucial need to developmore effective and practical defense strategies.</div></details><blockquote><p><strong><em>2024-03-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.15459v3><strong>Multi-LLM Collaboration + Data-Centric Innovation = 2x Better Vulnerability Repair</strong></a></p><p><em>Xin Zhou, Kisub Kim, Bowen Xu, DongGyun Han, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advances of deep learning (DL) have paved the way for automatic softwarevulnerability repair approaches, which effectively learn the mapping from thevulnerable code to the fixed code. Nevertheless, existing DL-basedvulnerability repair methods face notable limitations: 1) they struggle tohandle lengthy vulnerable code, 2) they treat code as natural language texts,neglecting its inherent structure, and 3) they do not tap into the valuableexpert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural networkmodel that excels at generating vulnerability repairs through data-centricinnovation. Specifically, VulMaster introduces the utilization and combinationof various types of input data, including complete vulnerable code of any size,vulnerable code structures, and expert knowledge from the CWE system.Additionally, VulMaster leverages the collaboration between two Large LanguageModels (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backboneLLM, fine-tuned with the training data, while ChatGPT supplements by providingmissing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerablefunctions. The experimental results demonstrated that VulMaster exhibitssubstantial improvements compared to the learning-based state-of-the-artvulnerability repair approach. Specifically, VulMaster improves the EM, BLEU,and CodeBLEU scores from 10.2% to 20.0%, 21.3% to 29.3%, and 32.5% to40.9%, respectively.</div></details><blockquote><p><strong><em>2024-03-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.06675v1><strong>Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code</strong></a></p><p><em>Cristina Improta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: AI-based code generators have gained a fundamental role in assistingdevelopers in writing software starting from natural language (NL). However,since these large language models are trained on massive volumes of datacollected from unreliable online sources (e.g., GitHub, Hugging Face), AImodels become an easy target for data poisoning attacks, in which an attackercorrupts the training data by injecting a small amount of poison into it, i.e.,astutely crafted malicious samples. In this position paper, we address thesecurity of AI code generators by identifying a novel data poisoning attackthat results in the generation of vulnerable code. Next, we devise an extensiveevaluation of how these attacks impact state-of-the-art models for codegeneration. Lastly, we discuss potential solutions to overcome this threat.</div></details><p><a href=http://arxiv.org/abs/2403.04769v2><strong>Using Hallucinations to Bypass GPT4&rsquo;s Filter</strong></a></p><p><em>Benjamin Lemkin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are initially trained on vast amounts of data,then fine-tuned using reinforcement learning from human feedback (RLHF); thisalso serves to teach the LLM to provide appropriate and safe responses. In thispaper, we present a novel method to manipulate the fine-tuned version intoreverting to its pre-RLHF behavior, effectively erasing the model&rsquo;s filters;the exploit currently works for GPT4, Claude Sonnet, and (to some extent) forInflection-2.5. Unlike other jailbreaks (for example, the popular &ldquo;Do AnythingNow&rdquo; (DAN) ), our method does not rely on instructing the LLM to override itsRLHF policy; hence, simply modifying the RLHF process is unlikely to addressit. Instead, we induce a hallucination involving reversed text during which themodel reverts to a word bucket, effectively pausing the model&rsquo;s filter. Webelieve that our exploit presents a fundamental vulnerability in LLMs currentlyunaddressed, as well as an opportunity to better understand the inner workingsof LLMs during hallucinations.</div></details><p><a href=http://arxiv.org/abs/2403.06833v1><strong>Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</strong></a></p><p><em>Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthroughresults, opening countless new possibilities for many practical applications.However, LLMs lack elementary safety features that are established norms inother areas of computer science, such as the separation between instructionsand data, causing them to malfunction or rendering them vulnerable tomanipulation and interference by third parties e.g., via indirectprompt/command injection. Even worse, so far, there is not even an establisheddefinition of what precisely such a separation would mean and how its violationcould be tested. In this work, we aim to close this gap. We introduce a formalmeasure to quantify the phenomenon of instruction-data separation as well as anempirical variant of the measure that can be computed from a model`s black-boxoutputs. We also introduce a new dataset, SEP (Should it be Executed orProcessed?), which allows estimating the measure, and we report results onseveral state-of-the-art open-source and closed LLMs. Finally, wequantitatively demonstrate that all evaluated LLMs fail to achieve a highamount of separation, according to our measure. The source code and SEP datasetare openly accessible athttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.</div></details><blockquote><p><strong><em>2024-03-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.06131v1><strong>FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning</strong></a></p><p><em>Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Instruction tuning has proven essential for enhancing the performance oflarge language models (LLMs) in generating human-aligned responses. However,collecting diverse, high-quality instruction data for tuning poses challenges,particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)has emerged as a solution, leveraging federated learning from multiple dataowners while preserving privacy. Yet, it faces challenges due to limitedinstruction data and vulnerabilities to training data extraction attacks. Toaddress these issues, we propose a novel federated algorithm, FedPIT, whichutilizes LLMs&rsquo; in-context learning capability to self-generate task-specificsynthetic data for training autonomously. Our method employs parameter-isolatedtraining to maintain global parameters trained on synthetic data and localparameters trained on augmented local data, effectively thwarting dataextraction attacks. Extensive experiments on real-world medical datademonstrate the effectiveness of FedPIT in improving federated few-shotperformance while preserving privacy and robustness against data heterogeneity.</div></details><p><a href=http://arxiv.org/abs/2310.19181v2><strong>From Chatbots to PhishBots? &ndash; Preventing Phishing scams created using ChatGPT, Google Bard and Claude</strong></a></p><p><em>Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin Nilizadeh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advanced capabilities of Large Language Models (LLMs) have made theminvaluable across various applications, from conversational agents and contentcreation to data analysis, research, and innovation. However, theireffectiveness and accessibility also render them susceptible to abuse forgenerating malicious content, including phishing attacks. This study exploresthe potential of using four popular commercially available LLMs, i.e., ChatGPT(GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishingattacks using a series of malicious prompts. We discover that these LLMs cangenerate both phishing websites and emails that can convincingly imitatewell-known brands and also deploy a range of evasive tactics that are used toelude detection mechanisms employed by anti-phishing systems. These attacks canbe generated using unmodified or &ldquo;vanilla&rdquo; versions of these LLMs withoutrequiring any prior adversarial exploits such as jailbreaking. We evaluate theperformance of the LLMs towards generating these attacks and find that they canalso be utilized to create malicious prompts that, in turn, can be fed back tothe model to generate phishing scams - thus massively reducing theprompt-engineering effort required by attackers to scale these threats. As acountermeasure, we build a BERT-based automated detection tool that can be usedfor the early detection of malicious prompts to prevent LLMs from generatingphishing content. Our model is transferable across all four commercial LLMs,attaining an average accuracy of 96% for phishing website prompts and 94% forphishing email prompts. We also disclose the vulnerabilities to the concernedLLMs, with Google acknowledging it as a severe issue. Our detection model isavailable for use at Hugging Face, as well as a ChatGPT Actions plugin.</div></details><blockquote><p><strong><em>2024-03-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.04451v1><strong>Membership Inference Attacks and Privacy in Topic Modeling</strong></a></p><p><em>Nico Manzonelli, Wanrong Zhang, Salil Vadhan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.</div></details><p><a href=http://arxiv.org/abs/2305.13733v2><strong>Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting</strong></a></p><p><em>Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Boyang Xue, Kam-Fai Wong, Ruifeng Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Numerous works are proposed to align large language models (LLMs) with humanintents to better fulfill instructions, ensuring they are trustful and helpful.Nevertheless, some human instructions are often malicious or misleading andfollowing them will lead to untruthful and unsafe responses. Previous workrarely focused on understanding how LLMs manage instructions based oncounterfactual premises, referred to here as \textit{inductive instructions},which may stem from users&rsquo; false beliefs or malicious intents. In this paper,we aim to reveal the behaviors of LLMs towards \textit{inductive instructions}and enhance their truthfulness and helpfulness accordingly. Specifically, wefirst introduce a benchmark of \underline{\textbf{Indu}}ctive{In\underline{\textbf{st}}ruct}ions (\textsc{\textbf{INDust}}), where the falseknowledge is incorporated into instructions in multiple different styles. Afterextensive human and automatic evaluations, we uncovered a universalvulnerability among LLMs in processing inductive instructions. Additionally, weidentified that different inductive styles affect the models&rsquo; ability toidentify the same underlying errors, and the complexity of the underlyingassumptions also influences the model&rsquo;s performance. Motivated by theseresults, we propose \textsc{Dual-critique} prompting to improve LLM robustnessagainst inductive instructions. Our experiments demonstrate that\textsc{Dual-critique} prompting significantly bolsters the robustness of adiverse array of LLMs, even when confronted with varying degrees of inductiveinstruction complexity and differing inductive styles.</div></details><blockquote><p><strong><em>2024-03-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.03897v1><strong>Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing</strong></a></p><p><em>Asmita, Yaroslav Oliinyk, Michael Scott, Ryan Tsang, Chongzhou Fang, Houman Homayoun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: BusyBox, an open-source software bundling over 300 essential Linux commandsinto a single executable, is ubiquitous in Linux-based embedded devices.Vulnerabilities in BusyBox can have far-reaching consequences, affecting a widearray of devices. This research, driven by the extensive use of BusyBox, delvedinto its analysis. The study revealed the prevalence of older BusyBox versionsin real-world embedded products, prompting us to conduct fuzz testing onBusyBox. Fuzzing, a pivotal software testing method, aims to induce crashesthat are subsequently scrutinized to uncover vulnerabilities. Within thisstudy, we introduce two techniques to fortify software testing. The firsttechnique enhances fuzzing by leveraging Large Language Models (LLM) togenerate target-specific initial seeds. Our study showed a substantial increasein crashes when using LLM-generated initial seeds, highlighting the potentialof LLM to efficiently tackle the typically labor-intensive task of generatingtarget-specific initial seeds. The second technique involves repurposingpreviously acquired crash data from similar fuzzed targets before initiatingfuzzing on a new target. This approach streamlines the time-consuming fuzztesting process by providing crash data directly to the new target beforecommencing fuzzing. We successfully identified crashes in the latest BusyBoxtarget without conducting traditional fuzzing, emphasizing the effectiveness ofLLM and crash reuse techniques in enhancing software testing and improvingvulnerability detection in embedded systems. Additionally, manual triaging wasperformed to identify the nature of crashes in the latest BusyBox.</div></details><blockquote><p><strong><em>2024-03-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.03188v1><strong>Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement</strong></a></p><p><em>Rafaela Martelo, Ruo-Qian Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Real-time flood forecasting plays a crucial role in enabling timely andeffective emergency responses. However, a significant challenge lies inbridging the gap between complex numerical flood models and practicaldecision-making. Decision-makers often rely on experts to interpret thesemodels for optimizing flood mitigation strategies. And the public requirescomplex techniques to inquiry and understand socio-cultural and institutionalfactors, often hinders the public&rsquo;s understanding of flood risks. To overcomethese challenges, our study introduces an innovative solution: a customized AIAssistant powered by the GPT-4 Large Language Model. This AI Assistant isdesigned to facilitate effective communication between decision-makers, thegeneral public, and flood forecasters, without the requirement of specializedknowledge. The new framework utilizes GPT-4&rsquo;s advanced natural languageunderstanding and function calling capabilities to provide immediate floodalerts and respond to various flood-related inquiries. Our developed prototypeintegrates real-time flood warnings with flood maps and social vulnerabilitydata. It also effectively translates complex flood zone information intoactionable risk management advice. To assess its performance, we evaluated theprototype using six criteria within three main categories: relevance, errorresilience, and understanding of context. Our research marks a significant steptowards a more accessible and user-friendly approach in flood risk management.This study highlights the potential of advanced AI tools like GPT-4 indemocratizing information and enhancing public engagement in critical socialand environmental issues.</div></details><p><a href=http://arxiv.org/abs/2403.00867v2><strong>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</strong></a></p><p><em>Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool,where the user enters a query and the LLM generates an answer. To reduce harmand misuse, efforts have been made to align these LLMs to human values usingadvanced training techniques such as Reinforcement Learning from Human Feedback(RLHF). However, recent studies have highlighted the vulnerability of LLMs toadversarial jailbreak attempts aiming at subverting the embedded safetyguardrails. To address this challenge, this paper defines and investigates theRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detectjailbreak attempts. Gradient Cuff exploits the unique properties observed inthe refusal loss landscape, including functional values and its smoothness, todesign an effective two-step detection strategy. Experimental results on twoaligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreakattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff cansignificantly improve the LLM&rsquo;s rejection capability for malicious jailbreakqueries, while maintaining the model&rsquo;s performance for benign user queries byadjusting the detection threshold.</div></details><blockquote><p><strong><em>2024-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.11053v3><strong>Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning</strong></a></p><p><em>Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have made unprecedented breakthroughs, yet theirincreasing integration into everyday life might raise societal risks due togenerated unethical content. Despite extensive study on specific issues likebias, the intrinsic values of LLMs remain largely unexplored from a moralphilosophy perspective. This work delves into ethical values utilizing MoralFoundation Theory. Moving beyond conventional discriminative evaluations withpoor reliability, we propose DeNEVIL, a novel prompt generation algorithmtailored to dynamically exploit LLMs&rsquo; value vulnerabilities and elicit theviolation of ethics in a generative manner, revealing their underlying valueinclinations. On such a basis, we construct MoralPrompt, a high-quality datasetcomprising 2,397 prompts covering 500+ value principles, and then benchmark theintrinsic values across a spectrum of LLMs. We discovered that most models areessentially misaligned, necessitating further ethical value alignment. Inresponse, we develop VILMO, an in-context alignment method that substantiallyenhances the value compliance of LLM outputs by learning to generateappropriate value instructions, outperforming existing competitors. Our methodsare suitable for black-box and open-source models, offering a promising initialstep in studying the ethical values of LLMs.</div></details><blockquote><p><strong><em>2024-03-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.04786v1><strong>Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models</strong></a></p><p><em>Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have become a cornerstone in the field ofNatural Language Processing (NLP), offering transformative capabilities inunderstanding and generating human-like text. However, with their risingprominence, the security and vulnerability aspects of these models havegarnered significant attention. This paper presents a comprehensive survey ofthe various forms of attacks targeting LLMs, discussing the nature andmechanisms of these attacks, their potential impacts, and current defensestrategies. We delve into topics such as adversarial attacks that aim tomanipulate model outputs, data poisoning that affects model training, andprivacy concerns related to training data exploitation. The paper also exploresthe effectiveness of different attack methodologies, the resilience of LLMsagainst these attacks, and the implications for model integrity and user trust.By examining the latest research, we provide insights into the currentlandscape of LLM vulnerabilities and defense mechanisms. Our objective is tooffer a nuanced understanding of LLM attacks, foster awareness within the AIcommunity, and inspire robust solutions to mitigate these risks in futuredevelopments.</div></details><p><a href=http://arxiv.org/abs/2311.16119v3><strong>Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition</strong></a></p><p><em>Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran√ßois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are deployed in interactive contexts with directuser engagement, such as chatbots and writing assistants. These deployments arevulnerable to prompt injection and jailbreaking (collectively, prompt hacking),in which models are manipulated to ignore their original instructions andfollow potentially malicious ones. Although widely acknowledged as asignificant security threat, there is a dearth of large-scale resources andquantitative studies on prompt hacking. To address this lacuna, we launch aglobal prompt hacking competition, which allows for free-form human inputattacks. We elicit 600K+ adversarial prompts against three state-of-the-artLLMs. We describe the dataset, which empirically verifies that current LLMs canindeed be manipulated via prompt hacking. We also present a comprehensivetaxonomical ontology of the types of adversarial prompts.</div></details><p><a href=http://arxiv.org/abs/2401.17644v2><strong>Towards Efficient and Reliable LLM Serving: A Real-World Workload Study</strong></a></p><p><em>Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, Xiaowen Chu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), especially Generative Pretrained Transformer(GPT) models, have significantly advanced in the industry in recent years.However, these models&rsquo; broader development faces considerable challenges due tohigh operational and deployment costs. This has led to active research inimproving the hardware efficiency of LLMs. Yet, the characteristics ofreal-world LLM workloads are often overlooked in current optimizations of LLMserving systems. In this work, the absence of reliable workload data forevaluating LLM serving systems impacts the quality of service (QoS) andreliability in industrial deployments. This paper introduces the firstreal-world trace dataset of LLM serving workloads, detailing user, system, andLLM behaviors. We analyze this trace, highlighting burstiness, request andresponse distributions, and focusing on the reliability of GPT services. Basedon this, we have developed a benchmark suite that reflects our dataset&rsquo;sworkload patterns, enabling performance evaluation of serving systems. Thissuite captures the core patterns of workload distributions, allowing forprecise scaling of the workload dataset to match system sizes. Our evaluationuncovers a previously unrecognized vulnerability of LLM serving systems toshort-term burstiness, particularly in common workload scenarios. We observethat GPU memory limitations, caused by the fluctuating nature of burstiness,lead to significant performance degradation in existing LLM serving systems.Beyond benchmarking, understanding these patterns is valuable for optimizingLLM workload management, enabling elastic hardware resource adjustments tovarying workloads. To encourage further research, we have made the dataset andbenchmark suite publicly available at <a href=https://github.com/HPMLL/BurstGPT>https://github.com/HPMLL/BurstGPT</a>.</div></details><blockquote><p><strong><em>2024-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.04783v1><strong>AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks</strong></a></p><p><em>Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite extensive pre-training and fine-tuning in moral alignment to preventgenerating harmful information at user request, large language models (LLMs)remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense,a response-filtering based multi-agent defense framework that filters harmfulresponses from LLMs. This framework assigns different roles to LLM agents andemploys them to complete the defense task collaboratively. The division intasks enhances the overall instruction-following of LLMs and enables theintegration of other defense components as tools. AutoDefense can adapt tovarious sizes and kinds of open-source LLMs that serve as agents. Throughconducting extensive experiments on a large scale of harmful and safe prompts,we validate the effectiveness of the proposed AutoDefense in improving therobustness against jailbreak attacks, while maintaining the performance atnormal user request. Our code and data are publicly available athttps://github.com/XHMY/AutoDefense.</div></details><p><a href=http://arxiv.org/abs/2403.04784v1><strong>Analysis of Privacy Leakage in Federated Large Language Models</strong></a></p><p><em>Minh N. Vu, Truc Nguyen, Tre&rsquo; R. Jeter, My T. Thai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid adoption of Federated Learning (FL) as the training and tuningprotocol for applications utilizing Large Language Models (LLMs), recentresearch highlights the need for significant modifications to FL to accommodatethe large-scale of LLMs. While substantial adjustments to the protocol havebeen introduced as a response, comprehensive privacy analysis for the adaptedFL protocol is currently lacking. To address this gap, our work delves into an extensive examination of theprivacy analysis of FL when used for training LLMs, both from theoretical andpractical perspectives. In particular, we design two active membershipinference attacks with guaranteed theoretical success rates to assess theprivacy leakages of various adapted FL configurations. Our theoretical findingsare translated into practical attacks, revealing substantial privacyvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, andOpenAI&rsquo;s GPTs, across multiple real-world language datasets. Additionally, weconduct thorough experiments to evaluate the privacy leakage of these modelswhen data is protected by state-of-the-art differential privacy (DP)mechanisms.</div></details><p><a href=http://arxiv.org/abs/2403.01218v1><strong>Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy</strong></a></p><p><em>Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel&rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to their<code>U-MIA'' counterparts). We propose a categorization of existing U-MIAs into</code>population U-MIAs&rsquo;&rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs&rsquo;&rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.</div></details><blockquote><p><strong><em>2024-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.17010v4><strong>Finetuning Large Language Models for Vulnerability Detection</strong></a></p><p><em>Alexey Shestov, Rodion Levichev, Ravil Mussabayev, Evgeny Maslov, Anton Cheshkov, Pavel Zadorozhny</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper presents the results of finetuning large language models (LLMs)for the task of detecting vulnerabilities in source code. We leverageWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, andadapt it for vulnerability detection through further finetuning. To acceleratetraining, we modify WizardCoder&rsquo;s training procedure, also we investigateoptimal training regimes. For the imbalanced dataset with many more negativeexamples than positive, we also explore different techniques to improveclassification performance. The finetuned WizardCoder model achievesimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerabilitydatasets over CodeBERT-like model, demonstrating the effectiveness of adaptingpretrained LLMs for vulnerability detection in source code. The keycontributions are finetuning the state-of-the-art code LLM, WizardCoder,increasing its training speed without the performance harm, optimizing thetraining procedure and regimes, handling class imbalance, and improvingperformance on difficult vulnerability detection datasets. This demonstratesthe potential for transfer learning by finetuning large pretrained languagemodels for specialized source code analysis tasks.</div></details><p><a href=http://arxiv.org/abs/2403.00878v1><strong>Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models</strong></a></p><p><em>Jiandong Jin, Bowen Tang, Mingxuan Ma, Xiao Liu, Yunfei Wang, Qingnan Lai, Jia Yang, Changling Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduces Crimson, a system that enhances the strategic reasoningcapabilities of Large Language Models (LLMs) within the realm of cybersecurity.By correlating CVEs with MITRE ATT&amp;CK techniques, Crimson advances threatanticipation and strategic defense efforts. Our approach includes defining andevaluating cybersecurity strategic tasks, alongside implementing acomprehensive human-in-the-loop data-synthetic workflow to develop theCVE-to-ATT&amp;CK Mapping (CVEM) dataset. We further enhance LLMs&rsquo; reasoningabilities through a novel Retrieval-Aware Training (RAT) process and itsrefined iteration, RAT-R. Our findings demonstrate that an LLM fine-tuned with our techniques,possessing 7 billion parameters, approaches the performance level of GPT-4,showing markedly lower rates of hallucination and errors, and surpassing othermodels in strategic reasoning tasks. Moreover, domain-specific fine-tuning ofembedding models significantly improves performance within cybersecuritycontexts, underscoring the efficacy of our methodology. By leveraging Crimsonto convert raw vulnerability data into structured and actionable insights, webolster proactive cybersecurity defenses.</div></details><p><a href=http://arxiv.org/abs/2402.16914v2><strong>DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers</strong></a></p><p><em>Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to bothmanual and automated jailbreak attacks, which adversarially trigger LLMs tooutput harmful content. However, current methods for jailbreaking LLMs, whichnest entire harmful prompts, are not effective at concealing malicious intentand can be easily identified and rejected by well-aligned LLMs. This paperdiscovers that decomposing a malicious prompt into separated sub-prompts caneffectively obscure its underlying malicious intent by presenting it in afragmented, less detectable form, thereby addressing these limitations. Weintroduce an automatic prompt \textbf{D}ecomposition and\textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack).DrAttack includes three key components: (a) <code>Decomposition' of the originalprompt into sub-prompts, (b) </code>Reconstruction&rsquo; of these sub-prompts implicitlyby in-context learning with semantically similar but harmless reassemblingdemo, and (c) a `Synonym Search&rsquo; of sub-prompts, aiming to find sub-prompts&rsquo;synonyms that maintain the original intent while jailbreaking LLMs. Anextensive empirical study across multiple open-source and closed-source LLMsdemonstrates that, with a significantly reduced number of queries, DrAttackobtains a substantial gain of success rate over prior SOTA prompt-onlyattackers. Notably, the success rate of 78.0% on GPT-4 with merely 15 queriessurpassed previous art by 33.1%. The project is available athttps://github.com/xirui-li/DrAttack.</div></details><blockquote><p><strong><em>2024-02-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.19150v1><strong>Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts</strong></a></p><p><em>Hao Cheng, Erjia Xiao, Renjing Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models(VLMs) and Large Language Models (LLMs) to perform amazing emergent abilitieson various multimodal tasks in the joint space of vision and language. However,the Typographic Attack, which shows disruption to VLMs, has also been certifiedas a security vulnerability to LMMs. In this work, we first comprehensivelyinvestigate the distractibility of LMMs by typography. In particular, weintroduce the Typographic Dataset designed to evaluate distractibility acrossvarious multi-modal subtasks, such as object recognition, visual attributesdetection, enumeration, arithmetic computation, and commonsense reasoning. Tofurther study the effect of typographic patterns on performance, we alsoscrutinize the effect of tuning various typographic factors, encompassing fontsize, color, opacity, and spatial positioning of typos. We discover that LMMscan partially distinguish visual contents and typos when confrontingtypographic attacks, which suggests that embeddings from vision encoderscontain enough information to distinguish visual contents and typos in images.Inspired by such phenomena, we demonstrate that CLIP&rsquo;s performance of zero-shotclassification on typo-ridden images can be significantly improved by providingmore informative texts to match images. Furthermore, we also prove that LMMscan utilize more informative prompts to leverage information in embeddings todifferentiate between visual content and typos. Finally, we propose a promptinformation enhancement method that can effectively mitigate the effects oftypography.</div></details><p><a href=http://arxiv.org/abs/2402.19135v1><strong>Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool</strong></a></p><p><em>Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones, Gerhard Schwabe</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In today&rsquo;s digital age, characterized by rapid news consumption andincreasing vulnerability to propaganda, fostering citizens&rsquo; critical thinkingis crucial for stable democracies. This paper introduces the design ofClarifAI, a novel automated propaganda detection tool designed to nudge readerstowards more critical news consumption by activating the analytical mode ofthinking, following Kahneman&rsquo;s dual-system theory of cognition. Using LargeLanguage Models, ClarifAI detects propaganda in news articles and providescontext-rich explanations, enhancing users&rsquo; understanding and criticalthinking. Our contribution is threefold: first, we propose the design ofClarifAI; second, in an online experiment, we demonstrate that this designeffectively encourages news readers to engage in more critical reading; andthird, we emphasize the value of explanations for fostering critical thinking.The study thus offers both a practical tool and useful design knowledge formitigating propaganda in digital news.</div></details><p><a href=http://arxiv.org/abs/2402.19334v1><strong>Here&rsquo;s a Free Lunch: Sanitizing Backdoored Models with Model Merge</strong></a></p><p><em>Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The democratization of pre-trained language models through open-sourceinitiatives has rapidly advanced innovation and expanded access to cutting-edgetechnologies. However, this openness also brings significant security risks,including backdoor attacks, where hidden malicious behaviors are triggered byspecific inputs, compromising natural language processing (NLP) systemintegrity and reliability. This paper suggests that merging a backdoored modelwith other homogeneous models can remediate backdoor vulnerabilities even ifsuch models are not entirely secure. In our experiments, we explore variousmodels (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets(SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensiveapproaches, our method offers an effective and efficient inference-stagedefense against backdoor attacks without additional resources or specificknowledge. Our approach consistently outperforms the other advanced baselines,leading to an average of 75% reduction in the attack success rate. Since modelmerging has been an established approach for improving model performance, theextra advantage it provides regarding defense can be seen as a cost-free bonus.</div></details><p><a href=http://arxiv.org/abs/2401.04350v2><strong>Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness</strong></a></p><p><em>Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale pre-trained vision-language models like CLIP have demonstratedimpressive performance across various tasks, and exhibit remarkable zero-shotgeneralization capability, while they are also vulnerable to imperceptibleadversarial examples. Existing works typically employ adversarial training(fine-tuning) as a defense method against adversarial examples. However, directapplication to the CLIP model may result in overfitting, compromising themodel&rsquo;s capacity for generalization. In this paper, we propose Pre-trainedModel Guided Adversarial Fine-Tuning (PMG-AFT) method, which leveragessupervision from the original pre-trained model by carefully designing anauxiliary branch, to enhance the model&rsquo;s zero-shot adversarial robustness.Specifically, PMG-AFT minimizes the distance between the features ofadversarial examples in the target model and those in the pre-trained model,aiming to preserve the generalization features already captured by thepre-trained model. Extensive Experiments on 15 zero-shot datasets demonstratethat PMG-AFT significantly outperforms the state-of-the-art method, improvingthe top-1 robust accuracy by an average of 4.99%. Furthermore, our approachconsistently improves clean accuracy by an average of 8.72%.</div></details><p><a href=http://arxiv.org/abs/2311.09827v2><strong>Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking</strong></a></p><p><em>Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While large language models (LLMs) have demonstrated increasing power, theyhave also given rise to a wide range of harmful behaviors. As representatives,jailbreak attacks can provoke harmful or unethical responses from LLMs, evenafter safety alignment. In this paper, we investigate a novel category ofjailbreak attacks specifically designed to target the cognitive structure andprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs inthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)effect-to-cause reasoning. Different from previous jailbreak attacks, ourproposed cognitive overload is a black-box attack with no need for knowledge ofmodel architecture or access to model weights. Experiments conducted onAdvBench and MasterKey reveal that various LLMs, including both popularopen-source model Llama 2 and the proprietary model ChatGPT, can be compromisedthrough cognitive overload. Motivated by cognitive psychology work on managingcognitive load, we further investigate defending cognitive overload attack fromtwo perspectives. Empirical studies show that our cognitive overload from threeperspectives can jailbreak all studied LLMs successfully, while existingdefense strategies can hardly mitigate the caused malicious uses effectively.</div></details><blockquote><p><strong><em>2024-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.18104v1><strong>Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction</strong></a></p><p><em>Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, large language models (LLMs) have demonstrated notablesuccess across various tasks, but the trustworthiness of LLMs is still an openproblem. One specific threat is the potential to generate toxic or harmfulresponses. Attackers can craft adversarial prompts that induce harmfulresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMssecurity by identifying bias vulnerabilities within the safety fine-tuning anddesign a black-box jailbreak method named DRA (Disguise and ReconstructionAttack), which conceals harmful instructions through disguise and prompts themodel to reconstruct the original harmful instruction within its completion. Weevaluate DRA across various open-source and close-source models, showcasingstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRAboasts a 90% attack success rate on LLM chatbots GPT-4.</div></details><p><a href=http://arxiv.org/abs/2402.16192v2><strong>Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</strong></a></p><p><em>Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,which bypass the safeguards of targeted LLMs and fool them into generatingobjectionable content. While initial defenses show promise against token-basedthreat models, there do not exist defenses that provide robustness againstsemantic attacks and avoid unfavorable trade-offs between robustness andnominal performance. To meet this need, we propose SEMANTICSMOOTH, asmoothing-based defense that aggregates the predictions of multiplesemantically transformed copies of a given input prompt. Experimental resultsdemonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness againstGCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance oninstruction following benchmarks such as InstructionFollowing and AlpacaEval.The codes will be publicly available athttps://github.com/UCSB-NLP-Chang/SemanticSmooth.</div></details><p><a href=http://arxiv.org/abs/2402.18216v1><strong>LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History</strong></a></p><p><em>Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, Mario Fritz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the recent emergence of powerful instruction-tuned large language models(LLMs), various helpful conversational Artificial Intelligence (AI) systemshave been deployed across many applications. When prompted by users, these AIsystems successfully perform a wide range of tasks as part of a conversation.To provide some sort of memory and context, such approaches typically conditiontheir output on the entire conversational history. Although this sensitivity tothe conversational history can often lead to improved performance on subsequenttasks, we find that performance can in fact also be negatively impacted, ifthere is a task-switch. To the best of our knowledge, our work makes the firstattempt to formalize the study of such vulnerabilities and interference oftasks in conversational LLMs caused by task-switches in the conversationalhistory. Our experiments across 5 datasets with 15 task switches using popularLLMs reveal that many of the task-switches can lead to significant performancedegradation.</div></details><p><a href=http://arxiv.org/abs/2402.16459v2><strong>Defending LLMs against Jailbreaking Attacks via Backtranslation</strong></a></p><p><em>Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although many large language models (LLMs) have been trained to refuseharmful requests, they are still vulnerable to jailbreaking attacks, whichrewrite the original prompt to conceal its harmful intent. In this paper, wepropose a new method for defending LLMs against jailbreaking attacks by``backtranslation&rsquo;&rsquo;. Specifically, given an initial response generated by thetarget LLM from an input prompt, our backtranslation prompts a language modelto infer an input prompt that can lead to the response. The inferred prompt iscalled the backtranslated prompt which tends to reveal the actual intent of theoriginal prompt, since it is generated based on the LLM&rsquo;s response and is notdirectly manipulated by the attacker. We then run the target LLM again on thebacktranslated prompt, and we refuse the original prompt if the model refusesthe backtranslated prompt. We explain that the proposed defense providesseveral benefits on its effectiveness and efficiency. We empiricallydemonstrate that our defense significantly outperforms the baselines, in thecases that are hard for the baselines, and our defense also has little impacton the generation quality for benign input prompts.</div></details><p><a href=http://arxiv.org/abs/2402.18649v1><strong>A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems</strong></a></p><p><em>Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Model (LLM) systems are inherently compositional, withindividual LLM serving as the core foundation with additional layers of objectssuch as plugins, sandbox, and so on. Along with the great potential, there arealso increasing concerns over the security of such probabilistic intelligentsystems. However, existing studies on LLM security often focus on individualLLM, but without examining the ecosystem through the lens of LLM systems withother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, wesystematically analyze the security of LLM systems, instead of focusing on theindividual LLMs. To do so, we build on top of the information flow andformulate the security of LLM systems as constraints on the alignment of theinformation flow within LLM and between LLM and other objects. Based on thisconstruction and the unique probabilistic nature of LLM, the attack surface ofthe LLM system can be decomposed into three key components: (1) multi-layersecurity analysis, (2) analysis of the existence of constraints, and (3)analysis of the robustness of these constraints. To ground this new attacksurface, we propose a multi-layer and multi-step approach and apply it to thestate-of-art LLM system, OpenAI GPT4. Our investigation exposes severalsecurity issues, not just within the LLM model itself but also in itsintegration with other components. We found that although the OpenAI GPT4 hasdesigned numerous safety constraints to improve its safety features, thesesafety constraints are still vulnerable to attackers. To further demonstratethe real-world threats of our discovered vulnerabilities, we construct anend-to-end attack where an adversary can illicitly acquire the user&rsquo;s chathistory, all without the need to manipulate the user&rsquo;s input or gain directaccess to OpenAI GPT4. Our demo is in the link:https://fzwark.github.io/LLM-System-Attack-Demo/</div></details><blockquote><p><strong><em>2024-02-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.17230v1><strong>Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities</strong></a></p><p><em>Yu Nong, Mohammed Aldeen, Long Cheng, Hongxin Hu, Feng Chen, Haipeng Cai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Security vulnerabilities are increasingly prevalent in modern software andthey are widely consequential to our society. Various approaches to defendingagainst these vulnerabilities have been proposed, among which those leveragingdeep learning (DL) avoid major barriers with other techniques hence attractingmore attention in recent years. However, DL-based approaches face criticalchallenges including the lack of sizable and quality-labeled task-specificdatasets and their inability to generalize well to unseen, real-worldscenarios. Lately, large language models (LLMs) have demonstrated impressivepotential in various domains by overcoming those challenges, especially throughchain-of-thought (CoT) prompting. In this paper, we explore how to leverageLLMs and CoT to address three key software vulnerability analysis tasks:identifying a given type of vulnerabilities, discovering vulnerabilities of anytype, and patching detected vulnerabilities. We instantiate the general CoTmethodology in the context of these tasks through VSP , our unified,vulnerability-semantics-guided prompting approach, and conduct extensiveexperiments assessing VSP versus five baselines for the three tasks againstthree LLMs and two datasets. Results show substantial superiority of ourCoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy forvulnerability identification, discovery, and patching, respectively, on CVEdatasets) over the baselines. Through in-depth case studies analyzing VSPfailures, we also reveal current gaps in LLM/CoT for challenging vulnerabilitycases, while proposing and validating respective improvements.</div></details><p><a href=http://arxiv.org/abs/2402.14872v2><strong>Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs</strong></a></p><p><em>Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs), used in creative writing, code generation, andtranslation, generate text based on input sequences but are vulnerable tojailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreakprompt methods use a combination of jailbreak templates followed by questionsto ask to create jailbreak prompts. However, existing jailbreak prompt designsgenerally suffer from excessive semantic differences, resulting in an inabilityto resist defenses that use simple semantic metrics as thresholds. Jailbreakprompts are semantically more varied than the original questions used forqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approachthat bypasses LLMs by generating jailbreak prompts that are semanticallysimilar to the original question. We model the search for jailbreak promptsthat satisfy both semantic similarity and jailbreak validity as amulti-objective optimization problem and employ a standardized set of geneticalgorithms for generating eligible prompts. Compared to the baselineAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%higher without ONION defense and 85.2% higher with ONION defense. SMJ&rsquo;s betterperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,Similarity, and Outlier, also means that SMJ is resistant to defenses that usethose metrics as thresholds.</div></details><p><a href=http://arxiv.org/abs/2402.17916v1><strong>LLM-Resistant Math Word Problem Generation via Adversarial Attacks</strong></a></p><p><em>Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have significantly transformed the educationallandscape. As current plagiarism detection tools struggle to keep pace withLLMs&rsquo; rapid advancements, the educational community faces the challenge ofassessing students&rsquo; true problem-solving abilities in the presence of LLMs. Inthis work, we explore a new paradigm for ensuring fair evaluation &ndash; generatingadversarial examples which preserve the structure and difficulty of theoriginal questions aimed for assessment, but are unsolvable by LLMs. Focusingon the domain of math word problems, we leverage abstract syntax trees tostructurally generate adversarial examples that cause LLMs to produce incorrectanswers by simply editing the numeric values in the problems. We conductexperiments on various open- and closed-source LLMs, quantitatively andqualitatively demonstrating that our method significantly degrades their mathproblem-solving ability. We identify shared vulnerabilities among LLMs andpropose a cost-effective approach to attack high-cost models. Additionally, weconduct automatic analysis on math problems and investigate the cause offailure to guide future research on LLM&rsquo;s mathematical capability.</div></details><p><a href=http://arxiv.org/abs/2402.17262v1><strong>Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue</strong></a></p><p><em>Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have been demonstrated to generate illegal orunethical responses, particularly when subjected to &ldquo;jailbreak.&rdquo; Research onjailbreak has highlighted the safety issues of LLMs. However, prior studieshave predominantly focused on single-turn dialogue, ignoring the potentialcomplexities and risks presented by multi-turn dialogue, a crucial mode throughwhich humans derive information from LLMs. In this paper, we argue that humanscould exploit multi-turn dialogue to induce LLMs into generating harmfulinformation. LLMs may not intend to reject cautionary or borderline unsafequeries, even if each turn is closely served for one malicious purpose in amulti-turn dialogue. Therefore, by decomposing an unsafe query into severalsub-queries for multi-turn dialogue, we induced LLMs to answer harmfulsub-questions incrementally, culminating in an overall harmful response. Ourexperiments, conducted across a wide range of LLMs, indicate currentinadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Ourfindings expose vulnerabilities of LLMs in complex scenarios involvingmulti-turn dialogue, presenting new challenges for the safety of LLMs.</div></details><blockquote><p><strong><em>2024-02-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.11698v5><strong>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</strong></a></p><p><em>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance &ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives &ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at <a href="https://openreview.net/pdf?id=kaHpo8OZw2">https://openreview.net/pdf?id=kaHpo8OZw2</a> .</div></details><p><a href=http://arxiv.org/abs/2402.16822v1><strong>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</strong></a></p><p><em>Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rockt√§schel, Roberta Raileanu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As large language models (LLMs) become increasingly prevalent across manyreal-world applications, understanding and enhancing their robustness to userinputs is of paramount importance. Existing methods for identifying adversarialprompts tend to focus on specific domains, lack diversity, or require extensivehuman annotations. To address these limitations, we present Rainbow Teaming, anovel approach for producing a diverse collection of adversarial prompts.Rainbow Teaming casts adversarial prompt generation as a quality-diversityproblem, and uses open-ended search to generate prompts that are both effectiveand diverse. It can uncover a model&rsquo;s vulnerabilities across a broad range ofdomains including, in this paper, safety, question answering, andcybersecurity. We also demonstrate that fine-tuning on synthetic data generatedby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurtingtheir general capabilities and helpfulness, paving the path to open-endedself-improvement.</div></details><blockquote><p><strong><em>2024-02-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.16043v1><strong>LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices</strong></a></p><p><em>Jiahui Xiang, Wenhai Wang, Tong Ye, Peiyu Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: IoT devices are currently facing continuous malicious attacks due to theirwidespread use. Among these IoT devices, web vulnerabilities are also widelyexploited because of their inherent characteristics, such as improperpermission controls and insecure interfaces. Recently, the embedded system webinterface framework has become highly diverse, and specific vulnerabilities canarise if developers forget to detect user input parameters or if the detectionprocess is not strict enough. Therefore, discovering vulnerabilities in the webinterfaces of IoT devices accurately and comprehensively through an automatedmethod is a major challenge. This paper aims to work out the challenge. We havedeveloped an automated vulnerability detection system called LuaTaint for thetypical web interface framework, LuCI. The system employs static taint analysisto address web security issues on mobile terminal platforms to ensure detectioncoverage. It integrates rules pertaining to page handler control logic withinthe taint detection process to improve its extensibility. We also implemented apost-processing step with the assistance of large language models to enhanceaccuracy and reduce the need for manual analysis. We have created a prototypeof LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors.LuaTaint has discovered 68 unknown vulnerabilities.</div></details><p><a href=http://arxiv.org/abs/2402.00357v2><strong>Safety of Multimodal Large Language Models on Images and Text</strong></a></p><p><em>Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Attracted by the impressive power of Multimodal Large Language Models(MLLMs), the public is increasingly utilizing them to improve the efficiency ofdaily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructionsbring huge safety risks when these models are deployed in real-world scenarios.In this paper, we systematically survey current efforts on the evaluation,attack, and defense of MLLMs&rsquo; safety on images and text. We begin withintroducing the overview of MLLMs on images and text and understanding ofsafety, which helps researchers know the detailed scope of our survey. Then, wereview the evaluation datasets and metrics for measuring the safety of MLLMs.Next, we comprehensively present attack and defense techniques related toMLLMs&rsquo; safety. Finally, we analyze several unsolved issues and discusspromising research directions. The latest papers are continually collected athttps://github.com/isXinLiu/MLLM-Safety-Collection.</div></details><p><a href=http://arxiv.org/abs/2403.12077v1><strong>Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions</strong></a></p><p><em>Xuming Hu, Xiaochuan Li, Junzhe Chen, Yinghui Li, Yangning Li, Xiaoguang Li, Yasheng Wang, Qun Liu, Lijie Wen, Philip S. Yu, Zhijiang Guo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generative search engines have the potential to transform how people seekinformation online, but generated responses from existing large language models(LLMs)-backed generative search engines may not always be accurate.Nonetheless, retrieval-augmented generation exacerbates safety concerns, sinceadversaries may successfully evade the entire system by subtly manipulating themost vulnerable part of a claim. To this end, we propose evaluating therobustness of generative search engines in the realistic and high-risk setting,where adversaries have only black-box system access and seek to deceive themodel into returning incorrect responses. Through a comprehensive humanevaluation of various generative search engines, such as Bing Chat,PerplexityAI, and YouChat across diverse queries, we demonstrate theeffectiveness of adversarial factual questions in inducing incorrect responses.Moreover, retrieval-augmented generation exhibits a higher susceptibility tofactual errors compared to LLMs without retrieval. These findings highlight thepotential security risks of these systems and emphasize the need for rigorousevaluation before deployment.</div></details><blockquote><p><strong><em>2024-02-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.10340v3><strong>On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities</strong></a></p><p><em>Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we highlight the critical issues of robustness and safetyassociated with integrating large language models (LLMs) and vision-languagemodels (VLMs) into robotics applications. Recent works have focused on usingLLMs and VLMs to improve the performance of robotics tasks, such asmanipulation, navigation, etc. However, such integration can introducesignificant vulnerabilities, in terms of their susceptibility to adversarialattacks due to the language models, potentially leading to catastrophicconsequences. By examining recent works at the interface of LLMs/VLMs androbotics, we show that it is easy to manipulate or misguide the robot&rsquo;sactions, leading to safety hazards. We define and provide examples of severalplausible adversarial attacks, and conduct experiments on three prominent robotframeworks integrated with a language model, including KnowNo VIMA, andInstruct2Act, to assess their susceptibility to these attacks. Our empiricalfindings reveal a striking vulnerability of LLM/VLM-robot integrated systems:simple adversarial attacks can significantly undermine the effectiveness ofLLM/VLM-robot integrated systems. Specifically, our data demonstrate an averageperformance deterioration of 21.2% under prompt attacks and a more alarming30.2% under perception attacks. These results underscore the critical need forrobust countermeasures to ensure the safe and reliable deployment of theadvanced LLM/VLM-based robotic systems.</div></details><blockquote><p><strong><em>2024-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.16893v1><strong>The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)</strong></a></p><p><em>Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs&rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.</div></details><p><a href=http://arxiv.org/abs/2402.15215v1><strong>Item-side Fairness of Large Language Model-based Recommendation System</strong></a></p><p><em>Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recommendation systems for Web content distribution intricately connect tothe information access and exposure opportunities for vulnerable populations.The emergence of Large Language Models-based Recommendation System (LRS) mayintroduce additional societal challenges to recommendation systems due to theinherent biases in Large Language Models (LLMs). From the perspective ofitem-side fairness, there remains a lack of comprehensive investigation intothe item-side fairness of LRS given the unique characteristics of LRS comparedto conventional recommendation systems. To bridge this gap, this study examinesthe property of LRS with respect to item-side fairness and reveals theinfluencing factors of both historical users&rsquo; interactions and inherentsemantic biases of LLMs, shedding light on the need to extend conventionalitem-side fairness methods for LRS. Towards this goal, we develop a concise andeffective framework called IFairLRS to enhance the item-side fairness of anLRS. IFairLRS covers the main stages of building an LRS with specificallyadapted strategies to calibrate the recommendations of LRS. We utilize IFairLRSto fine-tune LLaMA, a representative LLM, on \textit{MovieLens} and\textit{Steam} datasets, and observe significant item-side fairnessimprovements. The code can be found inhttps://github.com/JiangM-C/IFairLRS.git.</div></details><p><a href=http://arxiv.org/abs/2310.09266v2><strong>User Inference Attacks on Large Language Models</strong></a></p><p><em>Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user&rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user&rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.</div></details><p><a href=http://arxiv.org/abs/2402.15105v1><strong>A First Look at GPT Apps: Landscape and Vulnerability</strong></a></p><p><em>Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advancement of Large Language Models (LLMs), increasinglysophisticated and powerful GPTs are entering the market. Despite theirpopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs&rsquo;susceptibility to attacks raises concerns over safety and plagiarism. Thus, inthis work, we conduct a pioneering exploration of GPT stores, aiming to studyvulnerabilities and plagiarism within GPT applications. To begin with, weconduct, to our knowledge, the first large-scale monitoring and analysis of twostores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, wepropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.To complete these two tasks efficiently, we develop two automated tools: onefor web scraping and another designed for programmatically interacting withGPTs. Our findings reveal a significant enthusiasm among users and developersfor GPT interaction and creation, as evidenced by the rapid increase in GPTsand their creators. However, we also uncover a widespread failure to protectGPT internals, with nearly 90% of system prompts easily accessible, leading toconsiderable plagiarism and duplication among GPTs.</div></details><p><a href=http://arxiv.org/abs/2402.13291v2><strong>DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models</strong></a></p><p><em>Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan, Victor Chibotaru, Martin Vechev</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The automated program repair field has attracted substantial interest overthe years, but despite significant research efforts, creating a system thatworks well for complex semantic bugs such as security vulnerabilities hasproven difficult. A promising direction to solve this challenge is byleveraging large language models (LLMs), which are increasingly used to solvevarious programming tasks. In this paper, we investigate the effectiveness ofLLMs for solving code-repair task. We show that the task is difficult as itrequires the model to learn long-range code relationships, a task thatinherently relies on extensive amounts of training data. At the same time,creating a large, clean dataset for complex program bugs and theircorresponding fixes is non-trivial. We propose a technique to address thesechallenges with a new approach for querying and fine-tuning LLMs. The idea isto use program analysis to limit the LLM&rsquo;s attention mechanism on the portionsof code needed to perform the fix, drastically reducing the amount of requiredtraining data. Concretely, for training and inference, rather than feeding theentire program to the LLM, we reduce its code to a much shorter snippet thatcontains the reported defect together with the necessary context - and use thatinstead. Our evaluation shows that this code reduction approach substantiallyimproves available models such as GPT-4 using few-shot learning, as well asfine-tuning models. To train and evaluate our system, we created acomprehensive code fixing dataset by extensively labeling 156 bug patterns(including 40 security rules), requiring complex interprocedural dataflow todiscover. Our best system with Mixtral-8x7B can remove more than 80% of thereported defects while exactly matching the human fix in between 10 and 50% ofcases, outperforming baselines based on GPT-3.5 and GPT-4, or based onwindow-based models like TFix.</div></details><blockquote><p><strong><em>2024-02-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.03374v2><strong>LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward</strong></a></p><p><em>Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami Karkevandi, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In software development, the predominant emphasis on functionality oftensupersedes security concerns, a trend gaining momentum with AI-drivenautomation tools like GitHub Copilot. These tools significantly improvedevelopers&rsquo; efficiency in functional code development. Nevertheless, it remainsa notable concern that such tools are also responsible for creating insecurecode, predominantly because of pre-training on publicly available repositorieswith vulnerable code. Moreover, developers are called the &ldquo;weakest link in thechain&rdquo; since they have very minimal knowledge of code security. Althoughexisting solutions provide a reasonable solution to vulnerable code, they mustadequately describe and educate the developers on code security to ensure thatthe security issues are not repeated. Therefore we introduce a multipurposecode vulnerability analysis system \texttt{SecRepair}, powered by a largelanguage model, CodeGen2 assisting the developer in identifying and generatingfixed code along with a complete description of the vulnerability with a codecomment. Our innovative methodology uses a reinforcement learning paradigm togenerate code comments augmented by a semantic reward mechanism. Inspired byhow humans fix code issues, we propose an instruction-based dataset suitablefor vulnerability analysis with LLMs. We further identify zero-day and N-dayvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findingsunderscore that incorporating reinforcement learning coupled with semanticreward augments our model&rsquo;s performance, thereby fortifying its capacity toaddress code vulnerabilities with improved efficacy.</div></details><p><a href=http://arxiv.org/abs/2402.11725v2><strong>How Susceptible are Large Language Models to Ideological Manipulation?</strong></a></p><p><em>Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) possess the potential to exert substantialinfluence on public perceptions and interactions with information. This raisesconcerns about the societal impact that could arise if the ideologies withinthese models can be easily manipulated. In this work, we investigate howeffectively LLMs can learn and generalize ideological biases from theirinstruction-tuning data. Our findings reveal a concerning vulnerability:exposure to only a small amount of ideologically driven samples significantlyalters the ideology of LLMs. Notably, LLMs demonstrate a startling ability toabsorb ideology from one topic and generalize it to even unrelated ones. Theease with which LLMs&rsquo; ideologies can be skewed underscores the risks associatedwith intentionally poisoned training data by malicious actors or inadvertentlyintroduced biases by data annotators. It also emphasizes the imperative forrobust safeguards to mitigate the influence of ideological manipulations onLLMs.</div></details><p><a href=http://arxiv.org/abs/2309.03882v4><strong>Large Language Models Are Not Robust Multiple Choice Selectors</strong></a></p><p><em>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multiple choice questions (MCQs) serve as a common yet important task formatin the evaluation of large language models (LLMs). This work shows that modernLLMs are vulnerable to option position changes in MCQs due to their inherent"selection bias", namely, they prefer to select specific option IDs as answers(like &ldquo;Option A&rdquo;). Through extensive empirical analyses with 20 LLMs on threebenchmarks, we pinpoint that this behavioral bias primarily stems from LLMs&rsquo;token bias, where the model a priori assigns more probabilistic mass tospecific option ID tokens (e.g., A/B/C/D) when predicting answers from theoption IDs. To mitigate selection bias, we propose a label-free, inference-timedebiasing method, called PriDe, which separates the model&rsquo;s prior bias foroption IDs from the overall prediction distribution. PriDe first estimates theprior by permutating option contents on a small number of test samples, andthen applies the estimated prior to debias the remaining samples. Wedemonstrate that it achieves interpretable and transferable debiasing with highcomputational efficiency. We hope this work can draw broader research attentionto the bias and robustness of modern LLMs.</div></details><p><a href=http://arxiv.org/abs/2402.11753v2><strong>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</strong></a></p><p><em>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Safety is critical to the usage of large language models (LLMs). Multipletechniques such as data filtering and supervised fine-tuning have beendeveloped to strengthen LLM safety. However, currently known techniques presumethat corpora used for safety alignment of LLMs are solely interpreted bysemantics. This assumption, however, does not hold in real-world applications,which leads to severe vulnerabilities in LLMs. For example, users of forumsoften use ASCII art, a form of text-based art, to convey image information. Inthis paper, we propose a novel ASCII art-based jailbreak attack and introduce acomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate thecapabilities of LLMs in recognizing prompts that cannot be solely interpretedby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, andLlama2) struggle to recognize prompts provided in the form of ASCII art. Basedon this observation, we develop the jailbreak attack ArtPrompt, which leveragesthe poor performance of LLMs in recognizing ASCII art to bypass safety measuresand elicit undesired behaviors from LLMs. ArtPrompt only requires black-boxaccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompton five SOTA LLMs, and show that ArtPrompt can effectively and efficientlyinduce undesired behaviors from all five LLMs.</div></details><blockquote><p><strong><em>2024-02-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.14016v1><strong>Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment</strong></a></p><p><em>Vyas Raina, Adian Liusie, Mark Gales</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are powerful zero-shot assessors and areincreasingly used in real-world situations such as for written exams orbenchmarking systems. Despite this, no existing work has analyzed thevulnerability of judge-LLMs against adversaries attempting to manipulateoutputs. This work presents the first study on the adversarial robustness ofassessment LLMs, where we search for short universal phrases that when appendedto texts can deceive LLMs to provide high assessment scores. Experiments onSummEval and TopicalChat demonstrate that both LLM-scoring and pairwiseLLM-comparative assessment are vulnerable to simple concatenation attacks,where in particular LLM-scoring is very susceptible and can yield maximumassessment scores irrespective of the input text quality. Interestingly, suchattacks are transferable and phrases learned on smaller open-source LLMs can beapplied to larger closed-source models, such as GPT3.5. This highlights thepervasive nature of the adversarial vulnerabilities across different judge-LLMsizes, families and methods. Our findings raise significant concerns on thereliability of LLMs-as-a-judge methods, and underscore the importance ofaddressing vulnerabilities in LLM assessment methods before deployment inhigh-stakes real-world scenarios.</div></details><p><a href=http://arxiv.org/abs/2402.13459v1><strong>Learning to Poison Large Language Models During Instruction Tuning</strong></a></p><p><em>Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of Large Language Models (LLMs) has marked significantachievements in language processing and reasoning capabilities. Despite theiradvancements, LLMs face vulnerabilities to data poisoning attacks, whereadversaries insert backdoor triggers into training data to manipulate outputsfor malicious purposes. This work further identifies additional security risksin LLMs by designing a new data poisoning attack tailored to exploit theinstruction tuning process. We propose a novel gradient-guided backdoor triggerlearning approach to identify adversarial triggers efficiently, ensuring anevasion of detection by conventional defenses while maintaining contentintegrity. Through experimental validation across various LLMs and tasks, ourstrategy demonstrates a high success rate in compromising model outputs;poisoning only 1% of 4,000 instruction tuning samples leads to a PerformanceDrop Rate (PDR) of around 80%. Our work highlights the need for strongerdefenses against data poisoning attack, offering insights into safeguardingLLMs against these more sophisticated attacks. The source code can be found onthis GitHub repository: <a href=https://github.com/RookieZxy/GBTL/blob/main/README.md>https://github.com/RookieZxy/GBTL/blob/main/README.md</a>.</div></details><blockquote><p><strong><em>2024-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.10669v2><strong>Humans or LLMs as the Judge? A Study on Judgement Biases</strong></a></p><p><em>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a}human- and LLM-as-a-judge) for evaluating the performance of existing LLMs hasrecently gained attention. Nonetheless, this approach concurrently introducespotential biases from human and LLM judges, questioning the reliability of theevaluation results. In this paper, we propose a novel framework forinvestigating 5 types of biases for LLM and human judges. We curate a datasetwith 142 samples referring to the revised Bloom&rsquo;s Taxonomy and conductthousands of human and LLM evaluations. Results show that human and LLM judgesare vulnerable to perturbations to various degrees, and that even the mostcutting-edge judges possess considerable biases. We further exploit theirweakness and conduct attacks on LLM judges. We hope that our work can notifythe community of the vulnerability of human- and LLM-as-a-judge againstperturbations, as well as the urgency of developing robust evaluation systems.</div></details><p><a href=http://arxiv.org/abs/2402.13210v1><strong>Bayesian Reward Models for LLM Alignment</strong></a></p><p><em>Adam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To ensure that large language model (LLM) responses are helpful andnon-toxic, we usually fine-tune a reward model on human preference data. Wethen select policy responses with high rewards (best-of-n sampling) or furtheroptimize the policy to produce responses with high rewards (reinforcementlearning from human feedback). However, this process is vulnerable to rewardoveroptimization or hacking, in which the responses selected have high rewardsdue to errors in the reward model rather than a genuine preference. This isespecially problematic as the prompt or response diverges from the trainingdata. It should be possible to mitigate these issues by training a Bayesianreward model, which signals higher uncertainty further from the training datadistribution. Therefore, we trained Bayesian reward models using Laplace-LoRA(Yang et al., 2024) and found that the resulting uncertainty estimates cansuccessfully mitigate reward overoptimization in best-of-n sampling.</div></details><p><a href=http://arxiv.org/abs/2402.13148v1><strong>Defending Jailbreak Prompts via In-Context Adversarial Game</strong></a></p><p><em>Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) demonstrate remarkable capabilities acrossdiverse applications. However, concerns regarding their security, particularlythe vulnerability to jailbreak attacks, persist. Drawing inspiration fromadversarial training in deep learning and LLM agent learning processes, weintroduce the In-Context Adversarial Game (ICAG) for defending againstjailbreaks without the need for fine-tuning. ICAG leverages agent learning toconduct an adversarial game, aiming to dynamically extend knowledge to defendagainst jailbreaks. Unlike traditional methods that rely on static datasets,ICAG employs an iterative process to enhance both the defense and attackagents. This continuous improvement process strengthens defenses against newlygenerated jailbreak prompts. Our empirical studies affirm ICAG&rsquo;s efficacy,where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak successrates across various attack scenarios. Moreover, ICAG demonstrates remarkabletransferability to other LLMs, indicating its potential as a versatile defensemechanism.</div></details><p><a href=http://arxiv.org/abs/2402.13220v1><strong>How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts</strong></a></p><p><em>Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) havenot rendered them immune to challenges, particularly in the context of handlingdeceptive information in prompts, thus producing hallucinated responses undersuch conditions. To quantitatively assess this vulnerability, we presentMAD-Bench, a carefully curated benchmark that contains 850 test samples dividedinto 6 categories, such as non-existent objects, count of objects, spatialrelationship, and visual confusion. We provide a comprehensive analysis ofpopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such asLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gapsbetween GPT-4V and other models; and previous robust instruction-tuned models,such as LRV-Instruction and LLaVA-RLHF, are not effective on this newbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy ofany other model in our experiments ranges from 5% to 35%. We further propose aremedy that adds an additional paragraph to the deceptive prompts to encouragemodels to think twice before answering the question. Surprisingly, this simplemethod can even double the accuracy; however, the absolute numbers are stilltoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmarkto stimulate further research to enhance models&rsquo; resilience against deceptiveprompts.</div></details><p><a href=http://arxiv.org/abs/2402.14859v1><strong>The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative</strong></a></p><p><em>Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Due to their unprecedented ability to process and respond to various types ofdata, Multimodal Large Language Models (MLLMs) are constantly defining the newboundary of Artificial General Intelligence (AGI). As these advanced generativemodels increasingly form collaborative networks for complex tasks, theintegrity and security of these systems are crucial. Our paper, <code>The WolfWithin'', explores a novel vulnerability in MLLM societies - the indirectpropagation of malicious content. Unlike direct harmful output generation forMLLMs, our research demonstrates how a single MLLM agent can be subtlyinfluenced to generate prompts that, in turn, induce other MLLM agents in thesociety to output malicious content. This subtle, yet potent method of indirectinfluence marks a significant escalation in the security risks associated withMLLMs. Our findings reveal that, with minimal or even no access to MLLMs'parameters, an MLLM agent, when manipulated to produce specific prompts orinstructions, can effectively </code>infect&rsquo;&rsquo; other agents within a society ofMLLMs. This infection leads to the generation and circulation of harmfuloutputs, such as dangerous instructions or misinformation, across the society.We also show the transferability of these indirectly generated prompts,highlighting their possibility in propagating malice through inter-agentcommunication. This research provides a critical insight into a new dimensionof threat posed by MLLMs, where a single agent can act as a catalyst forwidespread malevolent influence. Our work underscores the urgent need fordeveloping robust mechanisms to detect and mitigate such covert manipulationswithin MLLM societies, ensuring their safe and ethical utilization in societalapplications. Our implementation is released at\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.</div></details><blockquote><p><strong><em>2024-02-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.00024v3><strong>Can LLMs Patch Security Issues?</strong></a></p><p><em>Kamel Alrashedy, Abdullah Aljasser</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have shown impressive proficiency in codegeneration. Nonetheless, similar to human developers, these models mightgenerate code that contains security vulnerabilities and flaws. Writing securecode remains a substantial challenge, as vulnerabilities often arise duringinteractions between programs and external systems or services, such asdatabases and operating systems. In this paper, we propose a novel approach,Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMsin receiving feedback from Bandit, which is a static code analysis tool, andthen the LLMs generate potential solutions to resolve security vulnerabilities.Each solution, along with the vulnerable code, is then sent back to the LLM forcode refinement. Our approach shows a significant improvement over the baselineand outperforms existing approaches. Furthermore, we introduce a new dataset,PythonSecurityEval, collected from real-world scenarios on Stack Overflow toevaluate the LLMs&rsquo; ability to generate secure code. Code and data are availableat \url{https://github.com/Kamel773/LLM-code-refine}</div></details><p><a href=http://arxiv.org/abs/2402.12198v1><strong>Zero shot VLMs for hate meme detection: Are we there yet?</strong></a></p><p><em>Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multimedia content on social media is rapidly evolving, with memes gainingprominence as a distinctive form. Unfortunately, some malicious users exploitmemes to target individuals or vulnerable communities, making it imperative toidentify and address such instances of hateful memes. Extensive research hasbeen conducted to address this issue by developing hate meme detection models.However, a notable limitation of traditional machine/deep learning models isthe requirement for labeled datasets for accurate classification. Recently, theresearch community has witnessed the emergence of several visual languagemodels that have exhibited outstanding performance across various tasks. Inthis study, we aim to investigate the efficacy of these visual language modelsin handling intricate tasks such as hate meme detection. We use various promptsettings to focus on zero-shot classification of hateful/harmful memes. Throughour analysis, we observe that large VLMs are still vulnerable for zero-shothate meme detection.</div></details><p><a href=http://arxiv.org/abs/2402.11755v1><strong>SPML: A DSL for Defending Language Models Against Prompt Attacks</strong></a></p><p><em>Reshabh K Sharma, Vinayak Gupta, Dan Grossman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have profoundly transformed natural languageapplications, with a growing reliance on instruction-based definitions fordesigning chatbots. However, post-deployment the chatbot definitions are fixedand are vulnerable to attacks by malicious users, emphasizing the need toprevent unethical applications and financial losses. Existing studies exploreuser prompts&rsquo; impact on LLM-based chatbots, yet practical methods to containattacks on application-specific chatbots remain unexplored. This paper presentsSystem Prompt Meta Language (SPML), a domain-specific language for refiningprompts and monitoring the inputs to the LLM-based chatbots. SPML activelychecks attack prompts, ensuring user inputs align with chatbot definitions toprevent malicious execution on the LLM backbone, optimizing costs. It alsostreamlines chatbot definition crafting with programming language capabilities,overcoming natural language design challenges. Additionally, we introduce agroundbreaking benchmark with 1.8k system prompts and 20k user inputs, offeringthe inaugural language and benchmark for chatbot definition evaluation.Experiments across datasets demonstrate SPML&rsquo;s proficiency in understandingattacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our dataand codes are publicly available at: <a href=https://prompt-compiler.github.io/SPML/>https://prompt-compiler.github.io/SPML/</a>.</div></details><p><a href=http://arxiv.org/abs/2402.12222v1><strong>CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation</strong></a></p><p><em>Jueon Eom, Seyeon Jeong, Taekyoung Kwon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fuzzing is an effective bug-finding technique but it struggles with complexsystems like JavaScript engines that demand precise grammatical input.Recently, researchers have adopted language models for context-aware mutationin fuzzing to address this problem. However, existing techniques are limited inutilizing coverage guidance for fuzzing, which is rather performed in ablack-box manner. This paper presents a novel technique called CovRL(Coverage-guided Reinforcement Learning) that combines Large Language Models(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveragingthe Term Frequency-Inverse Document Frequency (TF-IDF) method to construct aweighted coverage map. This map is key in calculating the fuzzing reward, whichis then applied to the LLM-based mutator through reinforcement learning.CovRL-Fuzz, through this approach, enables the generation of test cases thatare more likely to discover new coverage areas, thus improving vulnerabilitydetection while minimizing syntax and semantic errors, all without needingextra post-processing. Our evaluation results indicate that CovRL-Fuzzoutperforms the state-of-the-art fuzzers in terms of code coverage andbug-finding capabilities: CovRL-Fuzz identified 48 real-world security-relatedbugs in the latest JavaScript engines, including 39 previously unknownvulnerabilities and 11 CVEs.</div></details><p><a href=http://arxiv.org/abs/2402.12336v1><strong>Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</strong></a></p><p><em>Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 areincreasingly used for various real-world tasks. Prior work has shown that thesemodels are highly vulnerable to adversarial attacks on the vision modality.These attacks can be leveraged to spread fake information or defraud users, andthus pose a significant risk, which makes the robustness of large multi-modalfoundation models a pressing problem. The CLIP model, or one of its variants,is used as a frozen vision encoder in many vision-language models (VLMs), e.g.LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuningscheme to obtain a robust CLIP vision encoder, which yields robustness on allvision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. Inparticular, we show that stealth-attacks on users of VLMs by a malicious thirdparty providing manipulated images are no longer possible once one replaces theoriginal CLIP model with our robust one. No retraining or fine-tuning of theVLM is required. The code and robust models are available athttps://github.com/chs20/RobustVLM</div></details><blockquote><p><strong><em>2024-02-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.14836v1><strong>Stealthy Attack on Large Language Model based Recommendation</strong></a></p><p><em>Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo, Liang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, the powerful large language models (LLMs) have been instrumental inpropelling the progress of recommender systems (RS). However, while thesesystems have flourished, their susceptibility to security threats has beenlargely overlooked. In this work, we reveal that the introduction of LLMs intorecommendation models presents new security vulnerabilities due to theiremphasis on the textual content of items. We demonstrate that attackers cansignificantly boost an item&rsquo;s exposure by merely altering its textual contentduring the testing phase, without requiring direct interference with themodel&rsquo;s training process. Additionally, the attack is notably stealthy, as itdoes not affect the overall recommendation performance and the modifications tothe text are subtle, making it difficult for users and platforms to detect. Ourcomprehensive experiments across four mainstream LLM-based recommendationmodels demonstrate the superior efficacy and stealthiness of our approach. Ourwork unveils a significant security gap in LLM-based recommendation systems andpaves the way for future research on protecting these systems.</div></details><p><a href=http://arxiv.org/abs/2311.11509v3><strong>Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information</strong></a></p><p><em>Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Viswanathan Swaminathan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools invarious applications. However, these models are susceptible to adversarialprompt attacks, where attackers can carefully curate input strings that misleadLLMs into generating incorrect or undesired outputs. Previous work has revealedthat with relatively simple yet effective attacks based on discreteoptimization, it is possible to generate adversarial prompts that bypassmoderation and alignment of the models. This vulnerability to adversarialprompts underscores a significant concern regarding the robustness andreliability of LLMs. Our work aims to address this concern by introducing anovel approach to detecting adversarial prompts at a token level, leveragingthe LLM&rsquo;s capability to predict the next token&rsquo;s probability. We measure thedegree of the model&rsquo;s perplexity, where tokens predicted with high probabilityare considered normal, and those exhibiting high perplexity are flagged asadversarial. Additionaly, our method also integrates context understanding byincorporating neighboring token information to encourage the detection ofcontiguous adversarial prompt sequences. To this end, we design two algorithmsfor adversarial prompt detection: one based on optimization techniques andanother on Probabilistic Graphical Models (PGM). Both methods are equipped withefficient solving methods, ensuring efficient adversarial prompt detection. Ourtoken-level detection result can be visualized as heatmap overlays on the textsequence, allowing for a clearer and more intuitive representation of whichpart of the text may contain adversarial prompts.</div></details><blockquote><p><strong><em>2024-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.12192v2><strong>Text Embedding Inversion Security for Multilingual Language Models</strong></a></p><p><em>Yiyi Chen, Heather Lent, Johannes Bjerva</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Textual data is often represented as realnumbered embeddings in NLP,particularly with the popularity of large language models (LLMs) and Embeddingsas a Service (EaaS). However, storing sensitive information as embeddings canbe vulnerable to security breaches, as research shows that text can bereconstructed from embeddings, even without knowledge of the underlying model.While defence mechanisms have been explored, these are exclusively focused onEnglish, leaving other languages vulnerable to attacks. This work explores LLMsecurity through multilingual embedding inversion. We define the problem ofblack-box multilingual and cross-lingual inversion attacks, and thoroughlyexplore their potential implications. Our findings suggest that multilingualLLMs may be more vulnerable to inversion attacks, in part because English baseddefences may be ineffective. To alleviate this, we propose a simple maskingdefense effective for both monolingual and multilingual models. This study isthe first to investigate multilingual inversion attacks, shedding light on thedifferences in attacks and defenses across monolingual and multilingualsettings.</div></details><p><a href=http://arxiv.org/abs/2401.05949v4><strong>Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning</strong></a></p><p><em>Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Despite being widely applied, in-context learning isvulnerable to malicious attacks. In this work, we raise security concernsregarding this paradigm. Our studies demonstrate that an attacker canmanipulate the behavior of large language models by poisoning the demonstrationcontext, without the need for fine-tuning the model. Specifically, we design anew backdoor attack method, named ICLAttack, to target large language modelsbased on in-context learning. Our method encompasses two types of attacks:poisoning demonstration examples and poisoning demonstration prompts, which canmake models behave in alignment with predefined intentions. ICLAttack does notrequire additional fine-tuning to implant a backdoor, thus preserving themodel&rsquo;s generality. Furthermore, the poisoned examples are correctly labeled,enhancing the natural stealth of our attack method. Extensive experimentalresults across several language models, ranging in size from 1.3B to 180Bparameters, demonstrate the effectiveness of our attack method, exemplified bya high average attack success rate of 95.0% across the three datasets on OPTmodels.</div></details><p><a href=http://arxiv.org/abs/2402.06664v3><strong>LLM Agents can Autonomously Hack Websites</strong></a></p><p><em>Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, large language models (LLMs) have become increasinglycapable and can now interact with tools (i.e., call functions), read documents,and recursively call themselves. As a result, these LLMs can now functionautonomously as agents. With the rise in capabilities of these agents, recentwork has speculated on how LLM agents would affect cybersecurity. However, notmuch is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites,performing tasks as complex as blind database schema extraction and SQLinjections without human feedback. Importantly, the agent does not need to knowthe vulnerability beforehand. This capability is uniquely enabled by frontiermodels that are highly capable of tool use and leveraging extended context.Namely, we show that GPT-4 is capable of such hacks, but existing open-sourcemodels are not. Finally, we show that GPT-4 is capable of autonomously findingvulnerabilities in websites in the wild. Our findings raise questions about thewidespread deployment of LLMs.</div></details><p><a href=http://arxiv.org/abs/2402.10527v1><strong>Zero-shot sampling of adversarial entities in biomedical question answering</strong></a></p><p><em>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.</div></details><blockquote><p><strong><em>2024-02-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.09179v2><strong>Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization</strong></a></p><p><em>Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The increasing demand for customized Large Language Models (LLMs) has led tothe development of solutions like GPTs. These solutions facilitate tailored LLMcreation via natural language prompts without coding. However, thetrustworthiness of third-party custom versions of LLMs remains an essentialconcern. In this paper, we propose the first instruction backdoor attacksagainst applications integrated with untrusted customized LLMs (e.g., GPTs).Specifically, these attacks embed the backdoor into the custom version of LLMsby designing prompts with backdoor instructions, outputting the attacker&rsquo;sdesired result when inputs contain the pre-defined triggers. Our attackincludes 3 levels of attacks: word-level, syntax-level, and semantic-level,which adopt different types of triggers with progressive stealthiness. Westress that our attacks do not require fine-tuning or any modification to thebackend LLMs, adhering strictly to GPTs development guidelines. We conductextensive experiments on 4 prominent LLMs and 5 benchmark text classificationdatasets. The results show that our instruction backdoor attacks achieve thedesired attack performance without compromising utility. Additionally, wepropose an instruction-ignoring defense mechanism and demonstrate its partialeffectiveness in mitigating such attacks. Our findings highlight thevulnerability and the potential risks of LLM customization such as GPTs.</div></details><p><a href=http://arxiv.org/abs/2402.02172v3><strong>CodeAgent: Collaborative Agents for Software Engineering</strong></a></p><p><em>Daniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song, Haoye Tian, Saad Ezzini, Yongfeng Huang, Jacques Klein, Tegawende F. Bissyande</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Code review is a heavily collaborative process, which aims at ensuring theoverall quality and reliability of software. While it provides massivebenefits, the implementation of code review in an organization faces severalchallenges that make its automation appealing. Automated code review tools havebeen around for a while and are now improving thanks to the adoption of novelAI models, which help can learn about standard practices and systematicallycheck that the reviewed code adheres to them. Unfortunately, existing methodsfall short: they often target a single input-output generative model, whichcannot simulate the collaboration interactions in code review to account forvarious perspectives; they are also sub-performing on various critical codereview sub-tasks. In this paper, we advance the state of the art in code reviewautomation by introducing CodeAgent, a novel multi-agent-based system for codereview. Fundamentally, CodeAgent is steered by QA-Checker (short for"Question-Answer Checking"), a supervision agent, designed specifically toensure that all agents&rsquo; contributions remain relevant to the initial reviewquestion. CodeAgent is autonomous, multi-agent, and Large languagemodel-driven. To demonstrate the effectiveness of CodeAgent, we performedexperiments to assess its capabilities in various tasks including 1) detectionof inconsistencies between code changes and commit messages, 2) detection ofvulnerability introduction by commits, and 3) validation of adherence to codestyle. Our website is accessed in\url{https://code-agent-new.vercel.app/index.html}.</div></details><p><a href=http://arxiv.org/abs/2402.09674v1><strong>PAL: Proxy-Guided Black-Box Attack on Large Language Models</strong></a></p><p><em>Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have surged in popularity in recent months, butthey have demonstrated concerning capabilities to generate harmful content whenmanipulated. While techniques like safety fine-tuning aim to minimize harmfuluse, recent works have shown that LLMs remain vulnerable to attacks that elicittoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs(PAL), the first optimization-based attack on LLMs in a black-box query-onlysetting. In particular, it relies on a surrogate model to guide theoptimization and a sophisticated loss designed for real-world LLM APIs. Ourattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% onLlama-2-7B, compared to 4% for the current state of the art. We also proposeGCG++, an improvement to the GCG attack that reaches 94% ASR on white-boxLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simplebaseline for query-based attacks. We believe the techniques proposed in thiswork will enable more comprehensive safety testing of LLMs and, in the longterm, the development of better security guardrails. The code can be found athttps://github.com/chawins/pal.</div></details><blockquote><p><strong><em>2024-02-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.09546v1><strong>How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?</strong></a></p><p><em>Congcong Wen, Jiazhao Liang, Shuaihang Yuan, Hao Huang, Yi Fang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the field of robotics and automation, navigation systems based on LargeLanguage Models (LLMs) have recently shown impressive performance. However, thesecurity aspects of these systems have received relatively less attention. Thispaper pioneers the exploration of vulnerabilities in LLM-based navigationmodels in urban outdoor environments, a critical area given the technology&rsquo;swidespread application in autonomous driving, logistics, and emergencyservices. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)Attack that manipulates LLM-based navigation models by appendinggradient-derived suffixes to the original navigational prompt, leading toincorrect actions. We conducted comprehensive experiments on an LLMs-basednavigation model that employs various LLMs for reasoning. Our results, derivedfrom the Touchdown and Map2Seq street-view datasets under both few-shotlearning and fine-tuning configurations, demonstrate notable performancedeclines across three metrics in the face of both white-box and black-boxattacks. These results highlight the generalizability and transferability ofthe NPS Attack, emphasizing the need for enhanced security in LLM-basednavigation systems. As an initial countermeasure, we propose the NavigationalPrompt Engineering (NPE) Defense strategy, concentrating on navigation-relevantkeywords to reduce the impact of adversarial suffixes. While initial findingsindicate that this strategy enhances navigational safety, there remains acritical need for the wider research community to develop stronger defensemethods to effectively tackle the real-world challenges faced by these systems.</div></details><blockquote><p><strong><em>2024-02-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.01269v2><strong>LLbezpeky: Leveraging Large Language Models for Vulnerability Detection</strong></a></p><p><em>Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Meiyappan Nagappan, Shane McIntosh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite the continued research and progress in building secure systems,Android applications continue to be ridden with vulnerabilities, necessitatingeffective detection methods. Current strategies involving static and dynamicanalysis tools come with limitations like overwhelming number of falsepositives and limited scope of analysis which make either difficult to adopt.Over the past years, machine learning based approaches have been extensivelyexplored for vulnerability detection, but its real-world applicability isconstrained by data requirements and feature engineering challenges. LargeLanguage Models (LLMs), with their vast parameters, have shown tremendouspotential in understanding semnatics in human as well as programming languages.We dive into the efficacy of LLMs for detecting vulnerabilities in the contextof Android security. We focus on building an AI-driven workflow to assistdevelopers in identifying and rectifying vulnerabilities. Our experiments showthat LLMs outperform our expectations in finding issues within applicationscorrectly flagging insecure apps in 91.67% of cases in the Ghera benchmark. Weuse inferences from our experiments towards building a robust and actionablevulnerability detection system and demonstrate its effectiveness. Ourexperiments also shed light on how different various simple configurations canaffect the True Positive (TP) and False Positive (FP) rates.</div></details><p><a href=http://arxiv.org/abs/2402.08416v1><strong>Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</strong></a></p><p><em>Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models~(LLMs) have gained immense popularity and are beingincreasingly applied in various domains. Consequently, ensuring the security ofthese models is of paramount importance. Jailbreak attacks, which manipulateLLMs to generate malicious content, are recognized as a significantvulnerability. While existing research has predominantly focused on directjailbreak attacks on LLMs, there has been limited exploration of indirectmethods. The integration of various plugins into LLMs, notably RetrievalAugmented Generation~(RAG), which enables LLMs to incorporate externalknowledge bases into their response generation such as GPTs, introduces newavenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs,particularly GPTs, introducing a novel attack vector named Retrieval AugmentedGeneration Poisoning. This method, Pandora, exploits the synergy between LLMsand RAG through prompt manipulation to generate unexpected responses. Pandorauses maliciously crafted content to influence the RAG process, effectivelyinitiating jailbreak attacks. Our preliminary tests show that Pandorasuccessfully conducts jailbreak attacks in four different scenarios, achievinghigher success rates than direct attacks, with 64.3% for GPT-3.5 and 34.8%for GPT-4.</div></details><p><a href=http://arxiv.org/abs/2305.12138v4><strong>LMs: Understanding Code Syntax and Semantics for Code Analysis</strong></a></p><p><em>Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models~(LLMs) demonstrate significant potential torevolutionize software engineering (SE) by exhibiting outstanding performancein SE tasks such as code and document generation. However, the high reliabilityand risk control requirements in software engineering raise concerns about thelack of interpretability of LLMs. To address this concern, we conducted a studyto evaluate the capabilities of LLMs and their limitations for code analysis inSE. We break down the abilities needed for artificial intelligence~(AI) modelsto address SE tasks related to code analysis into three categories: 1) syntaxunderstanding, 2) static behavior understanding, and 3) dynamic behaviorunderstanding. Our investigation focused on the ability of LLMs to comprehendcode syntax and semantic structures, which include abstract syntax trees (AST),control flow graphs (CFG), and call graphs (CG). We employed fourstate-of-the-art foundational models, GPT4, GPT3.5, StarCoder andCodeLlama-13b-instruct. We assessed the performance of LLMs on cross-languagetasks involving C, Java, Python, and Solidity. Our findings revealed that while LLMs have a talent for understanding codesyntax, they struggle with comprehending code semantics, particularly dynamicsemantics. We conclude that LLMs possess capabilities similar to an AbstractSyntax Tree (AST) parser, demonstrating initial competencies in static codeanalysis. Furthermore, our study highlights that LLMs are susceptible tohallucinations when interpreting code semantic structures and fabricatingnonexistent facts. These results indicate the need to explore methods to verifythe correctness of LLM output to ensure its dependability in SE. Moreimportantly, our study provides an initial answer to why the codes generated byLLM are usually syntax-correct but vulnerable.</div></details><p><a href=http://arxiv.org/abs/2402.01155v3><strong>CABINET: Content Relevance based Noise Reduction for Table Question Answering</strong></a></p><p><em>Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, Balaji Krishnamurthy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Table understanding capability of Large Language Models (LLMs) has beenextensively studied through the task of question-answering (QA) over tables.Typically, only a small part of the whole table is relevant to derive theanswer for a given question. The irrelevant parts act as noise and aredistracting information, resulting in sub-optimal performance due to thevulnerability of LLMs to noise. To mitigate this, we propose CABINET (ContentRelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework toenable LLMs to focus on relevant tabular data by suppressing extraneousinformation. CABINET comprises an Unsupervised Relevance Scorer (URS), traineddifferentially with the QA LLM, that weighs the table content based on itsrelevance to the input question before feeding it to the question-answering LLM(QA LLM). To further aid the relevance scorer, CABINET employs a weaklysupervised module that generates a parsing statement describing the criteria ofrows and columns relevant to the question and highlights the content ofcorresponding table cells. CABINET significantly outperforms various tabularLLM baselines, as well as GPT3-based in-context learning methods, is morerobust to noise, maintains outperformance on tables of varying sizes, andestablishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. Werelease our code and datasets at <a href=https://github.com/Sohanpatnaik106/CABINET_QA>https://github.com/Sohanpatnaik106/CABINET_QA</a>.</div></details><blockquote><p><strong><em>2024-02-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.02823v2><strong>Evading Data Contamination Detection for Language Models is (too) Easy</strong></a></p><p><em>Jasper Dekoninck, Mark Niklas M√ºller, Maximilian Baader, Marc Fischer, Martin Vechev</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models are widespread, with their performance on benchmarksfrequently guiding user preferences for one model over another. However, thevast amount of data these models are trained on can inadvertently lead tocontamination with public benchmarks, thus compromising performancemeasurements. While recently developed contamination detection methods try toaddress this issue, they overlook the possibility of deliberate contaminationby malicious model providers aiming to evade detection. We argue that thissetting is of crucial importance as it casts doubt on the reliability of publicbenchmarks. To more rigorously study this issue, we propose a categorization ofboth model providers and contamination detection methods. This revealsvulnerabilities in existing methods that we exploit with EAL, a simple yeteffective contamination technique that significantly inflates benchmarkperformance while completely evading current detection methods.</div></details><p><a href=http://arxiv.org/abs/2402.07841v1><strong>Do Membership Inference Attacks Work on Large Language Models?</strong></a></p><p><em>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Membership inference attacks (MIAs) attempt to predict whether a particulardatapoint is a member of a target model&rsquo;s training data. Despite extensiveresearch on traditional machine learning models, there has been limited workstudying MIA on the pre-training data of large language models (LLMs). Weperform a large-scale evaluation of MIAs over a suite of language models (LMs)trained on the Pile, ranging from 160M to 12B parameters. We find that MIAsbarely outperform random guessing for most settings across varying LLM sizesand domains. Our further analyses reveal that this poor performance can beattributed to (1) the combination of a large dataset and few trainingiterations, and (2) an inherently fuzzy boundary between members andnon-members. We identify specific settings where LLMs have been shown to bevulnerable to membership inference and show that the apparent success in suchsettings can be attributed to a distribution shift, such as when members andnon-members are drawn from the seemingly identical domain but with differenttemporal ranges. We release our code and data as a unified benchmark packagethat includes all existing MIAs, supporting future work.</div></details><p><a href=http://arxiv.org/abs/2309.04344v2><strong>Zero-Shot Robustification of Zero-Shot Models</strong></a></p><p><em>Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Zero-shot inference is a powerful paradigm that enables the use of largepretrained models for downstream classification tasks without further training.However, these models are vulnerable to inherited biases that can impact theirperformance. The traditional solution is fine-tuning, but this undermines thekey advantage of pretrained models, which is their ability to be usedout-of-the-box. We propose RoboShot, a method that improves the robustness ofpretrained model embeddings in a fully zero-shot fashion. First, we uselanguage models (LMs) to obtain useful insights from task descriptions. Theseinsights are embedded and used to remove harmful and boost useful components inembeddings &ndash; without any supervision. Theoretically, we provide a simple andtractable model for biases in zero-shot embeddings and give a resultcharacterizing under what conditions our approach can boost performance.Empirically, we evaluate RoboShot on nine image and NLP classification tasksand show an average improvement of 15.98% on worst group accuracy, with trivialdecrease in overall accuracy over several zero-shot baselines. Additionally, wedemonstrate that RoboShot is compatible with a variety of pretrained andlanguage models and propose a way to further boost performance with a zero-shotadaptation variant.</div></details><p><a href=http://arxiv.org/abs/2402.07518v1><strong>Resilient Watermarking for LLM-Generated Codes</strong></a></p><p><em>Boquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun, Xingmei Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the development of large language models, multiple AIs are now madeavailable for code generation (such as ChatGPT and StarCoder) and are adoptedwidely. It is often desirable to know whether a piece of code is generated byAI, and furthermore, which AI is the author. For instance, if a certain versionof AI is known to generate vulnerable code, it is particularly important toknow the creator. Existing approaches are not satisfactory as watermarkingcodes are challenging compared with watermarking text data, as codes can bealtered with relative ease via widely-used code refactoring methods. In thiswork, we propose ACW (AI Code Watermarking), a novel method for watermarkingAI-generated codes. ACW is efficient as it requires no training or fine-tuningand works in a black-box manner. It is resilient as the watermark cannot beeasily removed or tampered through common code refactoring methods. The keyidea of ACW is to selectively apply a set of carefully-designedsemantic-preserving, idempotent code transformations, whose presence (orabsence) allows us to determine the existence of the watermark. Ourexperimental results show that ACW is effective (i.e., achieving high accuracy,true positive rates and false positive rates), resilient and efficient,significantly outperforming existing approaches.</div></details><p><a href=http://arxiv.org/abs/2309.02705v3><strong>Certifying LLM Safety against Adversarial Prompting</strong></a></p><p><em>Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are vulnerable to adversarial attacks that addmalicious tokens to an input prompt to bypass the safety guardrails of an LLMand cause it to produce harmful content. In this work, we introduceerase-and-check, the first framework for defending against adversarial promptswith certifiable safety guarantees. Given a prompt, our procedure erases tokensindividually and inspects the resulting subsequences using a safety filter. Oursafety certificate guarantees that harmful prompts are not mislabeled as safedue to an adversarial attack up to a certain size. We implement the safetyfilter in two ways, using Llama 2 and DistilBERT, and compare the performanceof erase-and-check for the two cases. We defend against three attack modes: i)adversarial suffix, where an adversarial sequence is appended at the end of aharmful prompt; ii) adversarial insertion, where the adversarial sequence isinserted anywhere in the middle of the prompt; and iii) adversarial infusion,where adversarial tokens are inserted at arbitrary positions in the prompt, notnecessarily as a contiguous block. Our experimental results demonstrate thatthis procedure can obtain strong certified safety guarantees on harmful promptswhile maintaining good empirical performance on safe prompts. Additionally, wepropose three efficient empirical defenses: i) RandEC, a randomized subsamplingversion of erase-and-check; ii) GreedyEC, which greedily erases tokens thatmaximize the softmax score of the harmful class; and iii) GradEC, which usesgradient information to optimize tokens to erase. We demonstrate theireffectiveness against adversarial prompts generated by the Greedy CoordinateGradient (GCG) attack algorithm. The code for our experiments is available athttps://github.com/aounon/certified-llm-safety.</div></details><blockquote><p><strong><em>2024-02-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.06922v1><strong>Whispers in the Machine: Confidentiality in LLM-integrated Systems</strong></a></p><p><em>Jonathan Evertz, Merlin Chlosta, Lea Sch√∂nherr, Thorsten Eisenhofer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are increasingly integrated with external tools.While these integrations can significantly improve the functionality of LLMs,they also create a new attack surface where confidential data may be disclosedbetween different components. Specifically, malicious tools can exploitvulnerabilities in the LLM itself to manipulate the model and compromise thedata of other services, raising the question of how private data can beprotected in the context of LLM integrations. In this work, we provide a systematic way of evaluating confidentiality inLLM-integrated systems. For this, we formalize a &ldquo;secret key&rdquo; game that cancapture the ability of a model to conceal private information. This enables usto compare the vulnerability of a model against confidentiality attacks andalso the effectiveness of different defense strategies. In this framework, weevaluate eight previously published attacks and four defenses. We find thatcurrent defenses lack generalization across attack strategies. Building on thisanalysis, we propose a method for robustness fine-tuning, inspired byadversarial training. This approach is effective in lowering the success rateof attackers and in improving the system&rsquo;s resilience against unknown attacks.</div></details><blockquote><p><strong><em>2024-02-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.04451v3><strong>Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks</strong></a></p><p><em>Domenico Cotroneo, Cristina Improta, Pietro Liguori, Roberto Natella</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: AI-based code generators have become pivotal in assisting developers inwriting software starting from natural language (NL). However, they are trainedon large amounts of data, often collected from unsanitized online sources(e.g., GitHub, HuggingFace). As a consequence, AI models become an easy targetfor data poisoning, i.e., an attack that injects malicious samples into thetraining data to generate vulnerable code. To address this threat, this work investigates the security of AI codegenerators by devising a targeted data poisoning strategy. We poison thetraining data by injecting increasing amounts of code containing securityvulnerabilities and assess the attack&rsquo;s success on different state-of-the-artmodels for code generation. Our study shows that AI code generators arevulnerable to even a small amount of poison. Notably, the attack successstrongly depends on the model architecture and poisoning rate, whereas it isnot influenced by the type of vulnerabilities. Moreover, since the attack doesnot impact the correctness of code generated by pre-trained models, it is hardto detect. Lastly, our work offers practical insights into understanding andpotentially mitigating this threat.</div></details><p><a href=http://arxiv.org/abs/2402.06599v1><strong>On the Out-Of-Distribution Generalization of Multimodal Large Language Models</strong></a></p><p><em>Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We investigate the generalization boundaries of current Multimodal LargeLanguage Models (MLLMs) via comprehensive evaluation under out-of-distributionscenarios and domain-specific tasks. We evaluate their zero-shot generalizationacross synthetic images, real-world distributional shifts, and specializeddatasets like medical and molecular imagery. Empirical results indicate thatMLLMs struggle with generalization beyond common training domains, limitingtheir direct application without adaptation. To understand the cause ofunreliable performance, we analyze three hypotheses: semanticmisinterpretation, visual feature extraction insufficiency, and mappingdeficiency. Results identify mapping deficiency as the primary hurdle. Toaddress this problem, we show that in-context learning (ICL) can significantlyenhance MLLMs&rsquo; generalization, opening new avenues for overcominggeneralization barriers. We further explore the robustness of ICL underdistribution shifts and show its vulnerability to domain shifts, label shifts,and spurious correlation shifts between in-context examples and test data.</div></details><blockquote><p><strong><em>2024-02-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.13169v2><strong>ReposVul: A Repository-Level High-Quality Vulnerability Dataset</strong></a></p><p><em>Xinchen Wang, Ruida Hu, Cuiyun Gao, Xin-Cheng Wen, Yujia Chen, Qing Liao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Open-Source Software (OSS) vulnerabilities bring great challenges to thesoftware security and pose potential risks to our society. Enormous effortshave been devoted into automated vulnerability detection, among which deeplearning (DL)-based approaches have proven to be the most effective. However,the current labeled data present the following limitations: (1) TangledPatches: Developers may submit code changes unrelated to vulnerability fixeswithin patches, leading to tangled patches. (2) Lacking Inter-proceduralVulnerabilities: The existing vulnerability datasets typically containfunction-level and file-level vulnerabilities, ignoring the relations betweenfunctions, thus rendering the approaches unable to detect the inter-proceduralvulnerabilities. (3) Outdated Patches: The existing datasets usually containoutdated patches, which may bias the model during training. To address the above limitations, in this paper, we propose an automated datacollection framework and construct the first repository-level high-qualityvulnerability dataset named ReposVul. The proposed framework mainly containsthree modules: (1) A vulnerability untangling module, aiming at distinguishingvulnerability-fixing related code changes from tangled patches, in which theLarge Language Models (LLMs) and static analysis tools are jointly employed.(2) A multi-granularity dependency extraction module, aiming at capturing theinter-procedural call relationships of vulnerabilities, in which we constructmultiple-granularity information for each vulnerability patch, includingrepository-level, file-level, function-level, and line-level. (3) A trace-basedfiltering module, aiming at filtering the outdated patches, which leverages thefile path trace-based filter and commit time trace-based filter to construct anup-to-date dataset.</div></details><p><a href=http://arxiv.org/abs/2402.05723v1><strong>In-Context Learning Can Re-learn Forbidden Tasks</strong></a></p><p><em>Sophie Xhonneux, David Dobre, Jian Tang, Gauthier Gidel, Dhanya Sridhar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite significant investment into safety training, large language models(LLMs) deployed in the real world still suffer from numerous vulnerabilities.One perspective on LLM safety training is that it algorithmically forbids themodel from answering toxic or harmful queries. To assess the effectiveness ofsafety training, in this work, we study forbidden tasks, i.e., tasks the modelis designed to refuse to answer. Specifically, we investigate whetherin-context learning (ICL) can be used to re-learn forbidden tasks despite theexplicit fine-tuning of the model to refuse them. We first examine a toyexample of refusing sentiment classification to demonstrate the problem. Then,we use ICL on a model fine-tuned to refuse to summarise made-up news articles.Finally, we investigate whether ICL can undo safety training, which couldrepresent a major security risk. For the safety task, we look at Vicuna-7B,Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box onStarling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICLattack that uses the chat template tokens like a prompt injection attack toachieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains LLM-generated text with violence,suicide, and misinformation.</div></details><p><a href=http://arxiv.org/abs/2402.05668v1><strong>Comprehensive Assessment of Jailbreak Attacks Against LLMs</strong></a></p><p><em>Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Misuse of the Large Language Models (LLMs) has raised widespread concern. Toaddress this issue, safeguards have been taken to ensure that LLMs align withsocial ethics. However, recent findings have revealed an unsettlingvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. Byapplying techniques, such as employing role-playing scenarios, adversarialexamples, or subtle subversion of safety objectives as a prompt, LLMs canproduce an inappropriate or even harmful response. While researchers havestudied several categories of jailbreak attacks, they have done so inisolation. To fill this gap, we present the first large-scale measurement ofvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreakmethods from four categories, 160 questions from 16 violation categories, andsix popular LLMs. Our extensive experimental results demonstrate that theoptimized jailbreak prompts consistently achieve the highest attack successrates, as well as exhibit robustness across different LLMs. Some jailbreakprompt datasets, available from the Internet, can also achieve high attacksuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite theclaims from many organizations regarding the coverage of violation categoriesin their policies, the attack success rates from these categories remain high,indicating the challenges of effectively aligning LLM policies and the abilityto counter jailbreak attacks. We also discuss the trade-off between the attackperformance and efficiency, as well as show that the transferability of thejailbreak prompts is still viable, becoming an option for black-box models.Overall, our research highlights the necessity of evaluating differentjailbreak methods. We hope our study can provide insights for future researchon jailbreak attacks and serve as a benchmark tool for evaluating them forpractitioners.</div></details><blockquote><p><strong><em>2024-02-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.04247v2><strong>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</strong></a></p><p><em>Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Intelligent agents powered by large language models (LLMs) have demonstratedsubstantial promise in autonomously conducting experiments and facilitatingscientific discoveries across various disciplines. While their capabilities arepromising, they also introduce novel vulnerabilities that demand carefulconsideration for safety. However, there exists a notable gap in theliterature, as there has been no comprehensive exploration of thesevulnerabilities. This position paper fills this gap by conducting a thoroughexamination of vulnerabilities in LLM-based agents within scientific domains,shedding light on potential risks associated with their misuse and emphasizingthe need for safety measures. We begin by providing a comprehensive overview ofthe potential risks inherent to scientific LLM agents, taking into account userintent, the specific scientific domain, and their potential impact on theexternal environment. Then, we delve into the origins of these vulnerabilitiesand provide a scoping review of the limited existing works. Based on ouranalysis, we propose a triadic framework involving human regulation, agentalignment, and an understanding of environmental feedback (agent regulation) tomitigate these identified risks. Furthermore, we highlight the limitations andchallenges associated with safeguarding scientific agents and advocate for thedevelopment of improved models, robust benchmarks, and comprehensiveregulations to address these issues effectively.</div></details><p><a href=http://arxiv.org/abs/2402.00350v2><strong>Large Language Models Based Fuzzing Techniques: A Survey</strong></a></p><p><em>Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the modern era where software plays a pivotal role, software security andvulnerability analysis have become essential for software development. Fuzzingtest, as an efficient software testing method, are widely used in variousdomains. Moreover, the rapid development of Large Language Models (LLMs) hasfacilitated their application in the field of software testing, demonstratingremarkable performance. Considering that existing fuzzing test techniques arenot entirely automated and software vulnerabilities continue to evolve, thereis a growing trend towards employing fuzzing test generated based on largelanguage models. This survey provides a systematic overview of the approachesthat fuse LLMs and fuzzing tests for software testing. In this paper, astatistical analysis and discussion of the literature in three areas, namelyLLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted bysummarising the state-of-the-art methods up until 2024. Our survey alsoinvestigates the potential for widespread deployment and application of fuzzingtest techniques generated by LLMs in the future.</div></details><p><a href=http://arxiv.org/abs/2402.05162v1><strong>Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</strong></a></p><p><em>Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) show inherent brittleness in their safetymechanisms, as evidenced by their susceptibility to jailbreaking and evennon-malicious fine-tuning. This study explores this brittleness of safetyalignment by leveraging pruning and low-rank modifications. We develop methodsto identify critical regions that are vital for safety guardrails, and that aredisentangled from utility-relevant regions at both the neuron and rank levels.Surprisingly, the isolated regions we find are sparse, comprising about $3%$at the parameter level and $2.5%$ at the rank level. Removing these regionscompromises safety without significantly impacting utility, corroborating theinherent brittleness of the model&rsquo;s safety mechanisms. Moreover, we show thatLLMs remain vulnerable to low-cost fine-tuning attacks even when modificationsto the safety-critical regions are restricted. These findings underscore theurgent need for more robust safety strategies in LLMs.</div></details><p><a href=http://arxiv.org/abs/2312.04828v2><strong>Human-Readable Fingerprint for Large Language Models</strong></a></p><p><em>Boyi Zeng, Chenghu Zhou, Xinbing Wang, Zhouhan Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Protecting the copyright of large language models (LLMs) has become crucialdue to their resource-intensive training and accompanying carefully designedlicenses. However, identifying the original base model of an LLM is challengingdue to potential parameter alterations. In this study, we introduce ahuman-readable fingerprint for LLMs that uniquely identifies the base modelwithout exposing model parameters or interfering with training. We firstobserve that the vector direction of LLM parameters remains stable after themodel has converged during pretraining, showing negligible perturbationsthrough subsequent training steps, including continued pretraining, supervisedfine-tuning (SFT), and RLHF, which makes it a sufficient condition to identifythe base model. The necessity is validated by continuing to train an LLM withan extra term to drive away the model parameters&rsquo; direction and the modelbecomes damaged. However, this direction is vulnerable to simple attacks likedimension permutation or matrix rotation, which significantly change it withoutaffecting performance. To address this, leveraging the Transformer structure,we systematically analyze potential attacks and define three invariant termsthat identify an LLM&rsquo;s base model. We make these invariant terms human-readableby mapping them to a Gaussian vector using a convolutional encoder and thenconverting it into a natural image with StyleGAN2. Our method generates a dogimage as an identity fingerprint for an LLM, where the dog&rsquo;s appearancestrongly indicates the LLM&rsquo;s base model. The fingerprint provides intuitiveinformation for qualitative discrimination, while the invariant terms can beemployed for quantitative and precise verification. Experimental results acrossvarious LLMs demonstrate the effectiveness of our method.</div></details><blockquote><p><strong><em>2024-02-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.03627v1><strong>Partially Recentralization Softmax Loss for Vision-Language Models Robustness</strong></a></p><p><em>Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As Large Language Models make a breakthrough in natural language processingtasks (NLP), multimodal technique becomes extremely popular. However, it hasbeen shown that multimodal NLP are vulnerable to adversarial attacks, where theoutputs of a model can be dramatically changed by a perturbation to the input.While several defense techniques have been proposed both in computer vision andNLP models, the multimodal robustness of models have not been fully explored.In this paper, we study the adversarial robustness provided by modifying lossfunction of pre-trained multimodal models, by restricting top K softmaxoutputs. Based on the evaluation and scoring, our experiments show that after afine-tuning, adversarial robustness of pre-trained models can be significantlyimproved, against popular attacks. Further research should be studying, such asoutput diversity, generalization and the robustness-performance trade-off ofthis kind of loss functions. Our code will be available after this paper isaccepted</div></details><p><a href=http://arxiv.org/abs/2311.03191v3><strong>DeepInception: Hypnotize Large Language Model to Be Jailbreaker</strong></a></p><p><em>Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite remarkable success in various applications, large language models(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrailsvoid. However, previous studies for jailbreaks usually resort to brute-forceoptimization or extrapolations of a high computation cost, which might not bepractical or effective. In this paper, inspired by the Milgram experimentw.r.t. the authority power for inciting harmfulness, we disclose a lightweightmethod, termed DeepInception, which can easily hypnotize LLM to be ajailbreaker. Specifically, DeepInception leverages the personification abilityof LLM to construct a novel nested scene to behave, which realizes an adaptiveway to escape the usage control in a normal scenario. Empirically, ourDeepInception can achieve competitive jailbreak success rates with previouscounterparts and realize a continuous jailbreak in subsequent interactions,which reveals the critical weakness of self-losing on both open andclosed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, and GPT-3.5-turbo/4. Ourinvestigation appeals to people to pay more attention to the safety aspects ofLLMs and develop a stronger defense against their misuse risks. The code ispublicly available at: <a href=https://github.com/tmlr-group/DeepInception>https://github.com/tmlr-group/DeepInception</a>.</div></details><blockquote><p><strong><em>2024-02-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.13954v3><strong>Robust Prompt Optimization for Large Language Models Against Distribution Shifts</strong></a></p><p><em>Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, Tat-Seng Chua</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Model (LLM) has demonstrated significant ability in variousNatural Language Processing tasks. However, their effectiveness is highlydependent on the phrasing of the task prompt, leading to research on automaticprompt optimization using labeled task data. We reveal that these promptoptimization techniques are vulnerable to distribution shifts such assubpopulation shifts, which are common for LLMs in real-world scenarios such ascustomer reviews analysis. In this light, we propose a new problem of robustprompt optimization for LLMs against distribution shifts, which requires theprompt optimized over the labeled source group can simultaneously generalize toan unlabeled target group. To solve this problem, we propose Generalized PromptOptimization framework, which incorporates the unlabeled data from the targetgroup into prompt optimization. Extensive experimental results demonstrate theeffectiveness of the proposed framework with significant performanceimprovement on the target group and comparable performance on the source group.</div></details><p><a href=http://arxiv.org/abs/2401.17256v2><strong>Weak-to-Strong Jailbreaking on Large Language Models</strong></a></p><p><em>Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack&rsquo;s key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model&rsquo;s decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at <a href=https://github.com/XuandongZhao/weak-to-strong>https://github.com/XuandongZhao/weak-to-strong</a></div></details><blockquote><p><strong><em>2024-02-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.08999v2><strong>Context-aware Adversarial Attack on Named Entity Recognition</strong></a></p><p><em>Shuguang Chen, Leonardo Neves, Thamar Solorio</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, large pre-trained language models (PLMs) have achievedremarkable performance on many natural language processing benchmarks. Despitetheir success, prior studies have shown that PLMs are vulnerable to attacksfrom adversarial examples. In this work, we focus on the named entityrecognition task and study context-aware adversarial attack methods to examinethe model&rsquo;s robustness. Specifically, we propose perturbing the mostinformative words for recognizing entities to create adversarial examples andinvestigate different candidate replacement methods to generate natural andplausible adversarial examples. Experiments and analyses show that our methodsare more effective in deceiving the model into making wrong predictions thanstrong baselines.</div></details><p><a href=http://arxiv.org/abs/2402.02207v1><strong>Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models</strong></a></p><p><em>Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Current vision large language models (VLLMs) exhibit remarkable capabilitiesyet are prone to generate harmful content and are vulnerable to even thesimplest jailbreaking attacks. Our initial analysis finds that this is due tothe presence of harmful data during vision-language instruction fine-tuning,and that VLLM fine-tuning can cause forgetting of safety alignment previouslylearned by the underpinning LLM. To address this issue, we first curate avision-language safe instruction-following dataset VLGuard covering variousharmful categories. Our experiments demonstrate that integrating this datasetinto standard vision-language fine-tuning or utilizing it for post-hocfine-tuning effectively safety aligns VLLMs. This alignment is achieved withminimal impact on, or even enhancement of, the models&rsquo; helpfulness. Theversatility of our safety fine-tuning dataset makes it a valuable resource forsafety-testing existing VLLMs, training new models or safeguarding pre-trainedVLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively rejectunsafe instructions and substantially reduce the success rates of severalblack-box adversarial attacks, which approach zero in many cases. The code anddataset are available at <a href=https://github.com/ys-zong/VLGuard>https://github.com/ys-zong/VLGuard</a>.</div></details><p><a href=http://arxiv.org/abs/2402.02160v1><strong>Data Poisoning for In-context Learning</strong></a></p><p><em>Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the domain of large language models (LLMs), in-context learning (ICL) hasbeen recognized for its innovative ability to adapt to new tasks, relying onexamples rather than retraining or fine-tuning. This paper delves into thecritical issue of ICL&rsquo;s susceptibility to data poisoning attacks, an area notyet fully explored. We wonder whether ICL is vulnerable, with adversariescapable of manipulating example data to degrade model performance. To addressthis, we introduce ICLPoison, a specialized attacking framework conceived toexploit the learning mechanisms of ICL. Our approach uniquely employs discretetext perturbations to strategically influence the hidden states of LLMs duringthe ICL process. We outline three representative strategies to implementattacks under our framework, each rigorously evaluated across a variety ofmodels and tasks. Our comprehensive tests, including trials on thesophisticated GPT-4 model, demonstrate that ICL&rsquo;s performance is significantlycompromised under our framework. These revelations indicate an urgent need forenhanced defense mechanisms to safeguard the integrity and reliability of LLMsin applications relying on in-context learning.</div></details><blockquote><p><strong><em>2024-02-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01411v1><strong>CodePori: Large Scale Model for Autonomous Software Development by Using Multi-Agents</strong></a></p><p><em>Zeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Syst√§, Pekka Abrahamsson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs)are reshaping the field of Software Engineering (SE). Existing LLM-basedmulti-agent systems have successfully resolved simple dialogue tasks. However,the potential of LLMs for more complex tasks, such as automated code generationfor large and complex projects, have been explored in only a few existingworks. This paper introduces CodePori, a novel model designed to automate codegeneration for extensive and complex software projects based on naturallanguage prompts. We employ LLM-based multi-AI agents to handle creative andchallenging tasks in autonomous software development. Each agent engages with aspecific task, including system design, code development, code review, codeverification, and test engineering. We show in the paper that CodePori is ableto generate running code for large-scale projects, completing the entiresoftware development process in minutes rather than hours, and at a cost of afew dollars. It identifies and mitigates potential security vulnerabilities andcorrects errors while maintaining a solid code performance level. We alsoconducted an evaluation of CodePori against existing solutions using HumanEvaland the Massively Multitask Benchmark for Python (MBPP) benchmark. The resultsindicate that CodePori improves upon the benchmarks in terms of code accuracy,efficiency, and overall performance. For example, CodePori improves the pass@1metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clearimprovement over the existing models. We also assessed CodePori&rsquo;s performancethrough practitioner evaluations, with 91% expressing satisfaction with themodel&rsquo;s performance.</div></details><p><a href=http://arxiv.org/abs/2402.03469v1><strong>Preference-free Alignment Learning with Regularized Relevance Reward</strong></a></p><p><em>Sungdong Kim, Minjoon Seo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Learning from human preference has been considered key to aligning LargeLanguage Models (LLMs) with human values. However, contrary to popular belief,our preliminary study reveals that reward models trained on human preferencedatasets tend to give higher scores to long off-topic responses than shorton-topic ones. Motivated by this observation, we explore a preference-freeapproach utilizing `relevance&rsquo; as a key objective for alignment. On our firstattempt, we find that the relevance score obtained by a retriever alone isvulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, whenwe utilize the score as a reward for reinforcement learning. To mitigate it, weintegrate effective inductive biases into the vanilla relevance to regularizeeach other, resulting in a mixture of reward functions: Regularized RelevanceReward ($R^3$). $R^3$ significantly improves performance on preferencebenchmarks by providing a robust reward signal. Notably, $R^3$ does not requireany human preference datasets (i.e., preference-free), outperformingopen-source reward models in improving human preference. Our analysisdemonstrates that $R^3$ has advantages in elevating human preference whileminimizing its side effects. Finally, we show the generalizability of $R^3$,consistently improving instruction-tuned models in various backbones and sizeswithout additional dataset cost. Our code is available athttps://github.com/naver-ai/RRR.</div></details><blockquote><p><strong><em>2024-02-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09680v5><strong>Trustworthy Large Models in Vision: A Survey</strong></a></p><p><em>Ziyan Guo, Li Xu, Jun Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid progress of Large Models (LMs) has recently revolutionized variousfields of deep learning with remarkable grades, ranging from Natural LanguageProcessing (NLP) to Computer Vision (CV). However, LMs are increasinglychallenged and criticized by academia and industry due to their powerfulperformance but untrustworthy behavior, which urgently needs to be alleviatedby reliable methods. Despite the abundance of literature on trustworthy LMs inNLP, a systematic survey specifically delving into the trustworthiness of LMsin CV remains absent. In order to mitigate this gap, we summarize four relevantconcerns that obstruct the trustworthy usage in vision of LMs in this survey,including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)interpretability. By highlighting corresponding challenge, countermeasures, anddiscussion in each topic, we hope this survey will facilitate readers&rsquo;understanding of this field, promote alignment of LMs with human expectationsand enable trustworthy LMs to serve as welfare rather than disaster for humansociety.</div></details><blockquote><p><strong><em>2024-01-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00898v1><strong>An Early Categorization of Prompt Injection Attacks on Large Language Models</strong></a></p><p><em>Sippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala, Jason Bennett Thatcher</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models and AI chatbots have been at the forefront ofdemocratizing artificial intelligence. However, the releases of ChatGPT andother similar tools have been followed by growing concerns regarding thedifficulty of controlling large language models and their outputs. Currently,we are witnessing a cat-and-mouse game where users attempt to misuse the modelswith a novel attack called prompt injections. In contrast, the developersattempt to discover the vulnerabilities and block the attacks simultaneously.In this paper, we provide an overview of these emergent threats and present acategorization of prompt injections, which can guide future research on promptinjections and act as a checklist of vulnerabilities in the development of LLMinterfaces. Moreover, based on previous literature and our own empiricalresearch, we discuss the implications of prompt injections to LLM end users,developers, and researchers.</div></details><p><a href=http://arxiv.org/abs/2401.17723v1><strong>LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks</strong></a></p><p><em>Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Sequential recommender systems stand out for their ability to capture users&rsquo;dynamic interests and the patterns of item-to-item transitions. However, theinherent openness of sequential recommender systems renders them vulnerable topoisoning attacks, where fraudulent users are injected into the training datato manipulate learned patterns. Traditional defense strategies predominantlydepend on predefined assumptions or rules extracted from specific knownattacks, limiting their generalizability to unknown attack types. To solve theabove problems, considering the rich open-world knowledge encapsulated in LargeLanguage Models (LLMs), our research initially focuses on the capabilities ofLLMs in the detection of unknown fraudulent activities within recommendersystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate thesubstantial capability of LLMs in identifying unknown fraudsters, leveragingtheir expansive, open-world knowledge. Building upon this, we propose the integration of LLMs into defensestrategies to extend their effectiveness beyond the confines of known attacks.We propose LoRec, an advanced framework that employs LLM-Enhanced Calibrationto strengthen the robustness of sequential recommender systems againstpoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) thatrefines the training process of sequential recommender systems with knowledgederived from LLMs, applying a user-wise reweighting to diminish the impact offraudsters injected by attacks. By incorporating LLMs&rsquo; open-world knowledge,the LCT effectively converts the limited, specific priors or rules into a moregeneral pattern of fraudsters, offering improved defenses against poisoningattacks. Our comprehensive experiments validate that LoRec, as a generalframework, significantly strengthens the robustness of sequential recommendersystems.</div></details><blockquote><p><strong><em>2024-01-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00888v1><strong>Security and Privacy Challenges of Large Language Models: A Survey</strong></a></p><p><em>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.</div></details><p><a href=http://arxiv.org/abs/2401.17459v1><strong>A Preliminary Study on Using Large Language Models in Software Pentesting</strong></a></p><p><em>Kumar Shashwat, Francis Hahn, Xinming Ou, Dmitry Goldgof, Lawrence Hall, Jay Ligatti, S. Raj Rajgopalan, Armin Ziaie Tabari</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLM) are perceived to offer promising potentials forautomating security tasks, such as those found in security operation centers(SOCs). As a first step towards evaluating this perceived potential, weinvestigate the use of LLMs in software pentesting, where the main task is toautomatically identify software security vulnerabilities in source code. Wehypothesize that an LLM-based AI agent can be improved over time for a specificsecurity task as human operators interact with it. Such improvement can bemade, as a first step, by engineering prompts fed to the LLM based on theresponses produced, to include relevant contexts and structures so that themodel provides more accurate results. Such engineering efforts becomesustainable if the prompts that are engineered to produce better results oncurrent tasks, also produce better results on future unknown tasks. To examinethis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains2,740 hand-crafted source code test cases containing various types ofvulnerabilities. We divide the test cases into training and testing data, wherewe engineer the prompts based on the training data (only), and evaluate thefinal system on the testing data. We compare the AI agent&rsquo;s performance on thetesting data against the performance of the agent without the promptengineering. We also compare the AI agent&rsquo;s results against those fromSonarQube, a widely used static code analyzer for security testing. We builtand tested multiple versions of the AI agent using different off-the-shelf LLMs&ndash; Google&rsquo;s Gemini-pro, as well as OpenAI&rsquo;s GPT-3.5-Turbo and GPT-4-Turbo (withboth chat completion and assistant APIs). The results show that using LLMs is aviable approach to build an AI agent for software pentesting that can improvethrough repeated use and prompt engineering.</div></details><blockquote><p><strong><em>2024-01-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.16185v1><strong>LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs&rsquo; Vulnerability Reasoning</strong></a></p><p><em>Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Miaolei Shi, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have demonstrated significant potential for manydownstream tasks, including those requiring human-level intelligence, such asvulnerability detection. However, recent attempts to use LLMs for vulnerabilitydetection are still preliminary, as they lack an in-depth understanding of asubject LLM&rsquo;s vulnerability reasoning capability &ndash; whether it originates fromthe model itself or from external assistance, such as invoking tool support andretrieving vulnerability knowledge. In this paper, we aim to decouple LLMs&rsquo;vulnerability reasoning capability from their other capabilities, including theability to actively seek additional information (e.g., via function calling inSOTA models), adopt relevant vulnerability knowledge (e.g., via vector-basedmatching and retrieval), and follow instructions to output structured results.To this end, we propose a unified evaluation framework named LLM4Vuln, whichseparates LLMs&rsquo; vulnerability reasoning from their other capabilities andevaluates how LLMs&rsquo; vulnerability reasoning could be enhanced when combinedwith the enhancement of other capabilities. To demonstrate the effectiveness ofLLM4Vuln, we have designed controlled experiments using 75 ground-truth smartcontract vulnerabilities, which were extensively audited as high-risk onCode4rena from August to November 2023, and tested them in 4,950 differentscenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama).Our results not only reveal ten findings regarding the varying effects ofknowledge enhancement, context supplementation, prompt schemes, and models butalso enable us to identify 9 zero-day vulnerabilities in two pilot bug bountyprograms with over 1,000 USD being awarded.</div></details><blockquote><p><strong><em>2024-01-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.15468v1><strong>Large Language Model for Vulnerability Detection: Emerging Results and Future Directions</strong></a></p><p><em>Xin Zhou, Ting Zhang, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Previous learning-based vulnerability detection methods relied on eithermedium-sized pre-trained models or smaller neural networks from scratch. Recentadvancements in Large Pre-Trained Language Models (LLMs) have showcasedremarkable few-shot learning capabilities in various tasks. However, theeffectiveness of LLMs in detecting software vulnerabilities is largelyunexplored. This paper aims to bridge this gap by exploring how LLMs performwith various prompts, particularly focusing on two state-of-the-art LLMs:GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achievescompetitive performance with the prior state-of-the-art vulnerability detectionapproach and GPT-4 consistently outperformed the state-of-the-art.</div></details><p><a href=http://arxiv.org/abs/2310.02446v2><strong>Low-Resource Languages Jailbreak GPT-4</strong></a></p><p><em>Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: AI safety training and red-teaming of large language models (LLMs) aremeasures to mitigate the generation of unsafe content. Our work exposes theinherent cross-lingual vulnerability of these safety mechanisms, resulting fromthe linguistic inequality of safety training data, by successfullycircumventing GPT-4&rsquo;s safeguard through translating unsafe English inputs intolow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafetranslated inputs and provides actionable items that can get the users towardstheir harmful goals 79% of the time, which is on par with or even surpassingstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages havesignificantly lower attack success rate, which suggests that the cross-lingualvulnerability mainly applies to low-resource languages. Previously, limitedtraining on low-resource languages primarily affects speakers of thoselanguages, causing technological disparities. However, our work highlights acrucial shift: this deficiency now poses a risk to all LLMs users. Publiclyavailable translation APIs enable anyone to exploit LLMs&rsquo; safetyvulnerabilities. Therefore, our work calls for a more holistic red-teamingefforts to develop robust multilingual safeguards with wide language coverage.</div></details><p><a href=http://arxiv.org/abs/2308.10335v5><strong>Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation</strong></a></p><p><em>Li Zhong, Zilong Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, the large language models (LLMs) have shown extraordinary abilityin understanding natural language and generating programming code. It has beena common practice of software engineers to consult LLMs when encounteringcoding questions. Although efforts have been made to avoid syntax errors andalign the code with the intended semantics, the reliability and robustness ofthe code generationfrom LLMs have not yet been thoroughly studied. Theexecutable code is not equivalent to the reliable and robust code, especiallyin the context of real-world software development. The misuse of APIs in thegenerated code could lead to severe problem, such as resource leaks, programcrashes. To make things worse, the users of LLM code generation services areactually the developers that are most vulnerable to these code that seems right&ndash; They are always novice developers that are not familiar with the APIs thatLLMs generate code for them. Therefore, they could hardly tell the misuse inthe code generated by LLMs, which further facilitates the incorrect codeapplied in real-world software. Existing code evaluation benchmark and datasetsfocus on crafting small tasks such as programming questions in codinginterviews, which however deviates from the problem that developers would askLLM for real-world coding help. To fill the missing piece, in this work, wepropose a dataset RobustAPI for evaluating the reliability and robustness ofcode generated by LLMs. We collect 1208 coding questions from StackOverflow on24 representative Java APIs. We summarize thecommon misuse patterns of theseAPIs and evaluate them oncurrent popular LLMs. The evaluation results show thatevenfor GPT-4, 62% of the generated code contains API misuses,which would causeunexpected consequences if the code isintroduced into real-world software.</div></details><blockquote><p><strong><em>2024-01-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.02003v2><strong>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</strong></a></p><p><em>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into &ldquo;The Good&rdquo; (beneficial LLM applications),&ldquo;The Bad&rdquo; (offensive applications), and &ldquo;The Ugly&rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs&rsquo; potential to both bolster and jeopardize cybersecurity.</div></details><p><a href=http://arxiv.org/abs/2401.15248v1><strong>Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective</strong></a></p><p><em>Yue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-training is known to generate universal representations for downstreamtasks in large-scale deep learning such as large language models. Existingliterature, e.g., \cite{kim2020adversarial}, empirically observe that thedownstream tasks can inherit the adversarial robustness of the pre-trainedmodel. We provide theoretical justifications for this robustness inheritancephenomenon. Our theoretical results reveal that feature purification plays animportant role in connecting the adversarial robustness of the pre-trainedmodel and the downstream tasks in two-layer neural networks. Specifically, weshow that (i) with adversarial training, each hidden node tends to pick onlyone (or a few) feature; (ii) without adversarial training, the hidden nodes canbe vulnerable to attacks. This observation is valid for both supervisedpre-training and contrastive learning. With purified nodes, it turns out thatclean training is enough to achieve adversarial robustness in downstream tasks.</div></details><blockquote><p><strong><em>2024-01-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01706v1><strong>MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds</strong></a></p><p><em>Xiaolong Jin, Zhuo Zhang, Xiangyu Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Model (LLM) alignment aims to ensure that LLM outputs matchwith human values. Researchers have demonstrated the severity of alignmentproblems with a large spectrum of jailbreak techniques that can induce LLMs toproduce malicious content during conversations. Finding the correspondingjailbreaking prompts usually requires substantial human intelligence orcomputation resources. In this paper, we report that LLMs have different levelsof alignment in various contexts. As such, by systematically constructing manycontexts, called worlds, leveraging a Domain Specific Language describingpossible worlds (e.g., time, location, characters, actions and languages) andthe corresponding compiler, we can cost-effectively expose latent alignmentissues. Given the low cost of our method, we are able to conduct a large scalestudy regarding LLM alignment issues in different worlds. Our results show thatour method outperforms the-state-of-the-art jailbreaking techniques on botheffectiveness and efficiency. In addition, our results indicate that existingLLMs are extremely vulnerable to nesting worlds and programming languageworlds. They imply that existing alignment training focuses on the real-worldand is lacking in various (virtual) worlds where LLMs can be exploited.</div></details><p><a href=http://arxiv.org/abs/2402.01705v1><strong>Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation</strong></a></p><p><em>Jennifer Chien, David Danks</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Algorithmic harms are commonly categorized as either allocative orrepresentational. This study specifically addresses the latter, focusing on anexamination of current definitions of representational harms to discern what isincluded and what is not. This analysis motivates our expansion beyondbehavioral definitions to encompass harms to cognitive and affective states.The paper outlines high-level requirements for measurement: identifying thenecessary expertise to implement this approach and illustrating it through acase study. Our work highlights the unique vulnerabilities of large languagemodels to perpetrating representational harms, particularly when these harms gounmeasured and unmitigated. The work concludes by presenting proposedmitigations and delineating when to employ them. The overarching aim of thisresearch is to establish a framework for broadening the definition ofrepresentational harms and to translate insights from fairness research intopractical measurement and mitigation praxis.</div></details><p><a href=http://arxiv.org/abs/2401.13927v1><strong>Adaptive Text Watermark for Large Language Models</strong></a></p><p><em>Yepeng Liu, Yuheng Bu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advancement of Large Language Models (LLMs) has led to increasingconcerns about the misuse of AI-generated text, and watermarking forLLM-generated text has emerged as a potential solution. However, it ischallenging to generate high-quality watermarked text while maintaining strongsecurity, robustness, and the ability to detect watermarks without priorknowledge of the prompt or model. This paper proposes an adaptive watermarkingstrategy to address this problem. To improve the text quality and maintainrobustness, we adaptively add watermarking to token distributions with highentropy measured using an auxiliary model and keep the low entropy tokendistributions untouched. For the sake of security and to further minimize thewatermark&rsquo;s impact on text quality, instead of using a fixed green/red listgenerated from a random secret key, which can be vulnerable to decryption andforgery, we adaptively scale up the output logits in proportion based on thesemantic embedding of previously generated text using a well designed semanticmapping model. Our experiments involving various LLMs demonstrate that ourapproach achieves comparable robustness performance to existing watermarkmethods. Additionally, the text generated by our method has perplexitycomparable to that of \emph{un-watermarked} LLMs while maintaining securityeven under various attacks.</div></details><blockquote><p><strong><em>2024-01-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.11108v1><strong>LLM4Fuzz: Guided Fuzzing of Smart Contracts with Large Language Models</strong></a></p><p><em>Chaofan Shou, Jing Liu, Doudou Lu, Koushik Sen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As blockchain platforms grow exponentially, millions of lines of smartcontract code are being deployed to manage extensive digital assets. However,vulnerabilities in this mission-critical code have led to significantexploitations and asset losses. Thorough automated security analysis of smartcontracts is thus imperative. This paper introduces LLM4Fuzz to optimizeautomated smart contract security analysis by leveraging large language models(LLMs) to intelligently guide and prioritize fuzzing campaigns. Whiletraditional fuzzing suffers from low efficiency in exploring the vast statespace, LLM4Fuzz employs LLMs to direct fuzzers towards high-value code regionsand input sequences more likely to trigger vulnerabilities. Additionally,LLM4Fuzz can leverage LLMs to guide fuzzers based on user-defined invariants,reducing blind exploration overhead. Evaluations of LLM4Fuzz on real-world DeFiprojects show substantial gains in efficiency, coverage, and vulnerabilitydetection compared to baseline fuzzing. LLM4Fuzz also uncovered five criticalvulnerabilities that can lead to a loss of more than $247k.</div></details><p><a href=http://arxiv.org/abs/2311.09127v2><strong>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</strong></a></p><p><em>Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Existing work on jailbreak Multimodal Large Language Models (MLLMs) hasfocused primarily on adversarial examples in model inputs, with less attentionto vulnerabilities, especially in model API. To fill the research gap, we carryout the following work: 1) We discover a system prompt leakage vulnerability inGPT-4V. Through carefully designed dialogue, we successfully extract theinternal system prompts of GPT-4V. This finding indicates potential exploitablesecurity risks in MLLMs; 2) Based on the acquired system prompts, we propose anovel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack viaSystem Prompt). By employing GPT-4 as a red teaming tool against itself, we aimto search for potential jailbreak prompts leveraging stolen system prompts.Furthermore, in pursuit of better performance, we also add human modificationbased on GPT-4&rsquo;s analysis, which further improves the attack success rate to98.7%; 3) We evaluated the effect of modifying system prompts to defendagainst jailbreaking attacks. Results show that appropriately designed systemprompts can significantly reduce jailbreak success rates. Overall, our workprovides new insights into enhancing MLLM security, demonstrating the importantrole of system prompts in jailbreaking. This finding could be leveraged togreatly facilitate jailbreak success rates while also holding the potential fordefending against jailbreaks.</div></details><p><a href=http://arxiv.org/abs/2401.12242v1><strong>BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</strong></a></p><p><em>Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are shown to benefit from chain-of-thought (COT)prompting, particularly when tackling tasks that require systematic reasoningprocesses. On the other hand, COT prompting also poses new vulnerabilities inthe form of backdoor attacks, wherein the model will output unintendedmalicious content under specific backdoor-triggered conditions duringinference. Traditional methods for launching backdoor attacks involve eithercontaminating the training dataset with backdoored instances or directlymanipulating the model parameters during deployment. However, these approachesare not practical for commercial LLMs that typically operate via API access. Inthis paper, we propose BadChain, the first backdoor attack against LLMsemploying COT prompting, which does not require access to the training datasetor model parameters and imposes low computational overhead. BadChain leveragesthe inherent reasoning capabilities of LLMs by inserting a backdoor reasoningstep into the sequence of reasoning steps of the model output, thereby alteringthe final response when a backdoor trigger exists in the query prompt.Empirically, we show the effectiveness of BadChain for two COT strategiesacross four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmarktasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,we show that LLMs endowed with stronger reasoning capabilities exhibit highersusceptibility to BadChain, exemplified by a high average attack success rateof 97.0% across the six benchmark tasks on GPT-4. Finally, we propose twodefenses based on shuffling and demonstrate their overall ineffectivenessagainst BadChain. Therefore, BadChain remains a severe threat to LLMs,underscoring the urgency for the development of robust and effective futuredefenses.</div></details><blockquote><p><strong><em>2024-01-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.10862v1><strong>Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning</strong></a></p><p><em>Adib Hasan, Ileana Rugina, Alex Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are vulnerable to `Jailbreaking&rsquo; prompts, a typeof attack that can coax these models into generating harmful and illegalcontent. In this paper, we show that pruning up to 20% of LLM parametersmarkedly increases their resistance to such attacks without additional trainingand without sacrificing their performance in standard benchmarks. Intriguingly,we discovered that the enhanced safety observed post-pruning correlates to theinitial safety training level of the model, hinting that the effect of pruningcould be more general and may hold for other LLM behaviors beyond safety.Additionally, we introduce a curated dataset of 225 harmful tasks across fivecategories, inserted into ten different Jailbreaking prompts, showing thatpruning aids LLMs in concentrating attention on task-relevant tokens injailbreaking prompts. Lastly, our experiments reveal that the prominent chatmodels, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit highsusceptibility to jailbreaking attacks, with some categories achieving nearly70-100% success rate. These insights underline the potential of pruning as ageneralizable approach for improving LLM safety, reliability, and potentiallyother desired behaviors.</div></details><blockquote><p><strong><em>2024-01-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.02906v2><strong>MLLM-Protector: Ensuring MLLM&rsquo;s Safety without Hurting Performance</strong></a></p><p><em>Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The deployment of multimodal large language models (MLLMs) has brought fortha unique vulnerability: susceptibility to malicious attacks through visualinputs. We delve into the novel challenge of defending MLLMs against suchattacks. We discovered that images act as a &ldquo;foreign language&rdquo; that is notconsidered during alignment, which can make MLLMs prone to producing harmfulresponses. Unfortunately, unlike the discrete tokens considered in text-basedLLMs, the continuous nature of image signals presents significant alignmentchallenges, which poses difficulty to thoroughly cover the possible scenarios.This vulnerability is exacerbated by the fact that open-source MLLMs arepredominantly fine-tuned on limited image-text pairs that is much less than theextensive text-based pretraining corpus, which makes the MLLMs more prone tocatastrophic forgetting of their original abilities during explicit alignmenttuning. To tackle these challenges, we introduce MLLM-Protector, aplug-and-play strategy combining a lightweight harm detector and a responsedetoxifier. The harm detector&rsquo;s role is to identify potentially harmful outputsfrom the MLLM, while the detoxifier corrects these outputs to ensure theresponse stipulates to the safety standards. This approach effectivelymitigates the risks posed by malicious visual inputs without compromising themodel&rsquo;s overall performance. Our results demonstrate that MLLM-Protector offersa robust solution to a previously unaddressed aspect of MLLM security.</div></details><p><a href=http://arxiv.org/abs/2401.09647v1><strong>Characterizing Online Eating Disorder Communities with Large Language Models</strong></a></p><p><em>Minh Duc Chu, Aryan Karnati, Zihao He, Kristina Lerman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rise in eating disorders, a dangerous mental health condition with highmortality and morbidity, has been linked to the proliferation of idealized bodyimages on social media. However, the link between social media and eatingdisorders is far more complex. We argue that social media platforms create afeedback loop that amplifies the growth of content and communities that promoteeating disorders like anorexia and bulimia. Specifically, social mediaplatforms make it easy for vulnerable individuals to find and connect tolike-minded others, while group dynamic processes encourage them to stayengaged within communities that promote and glorify harmful behaviors linked toeating disorders. We characterize this dynamic empirically through acombination of network and language analysis. We describe a novel frameworkthat leverages large language models to analyze the discourse within onlinecommunities and probe their attitudes on topics related to eating disorders toidentify potentially harmful content. Our work emphasizes the need for bettersocial media moderation to disrupt harmful feedback loops and protectvulnerable individuals.</div></details><blockquote><p><strong><em>2024-01-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.08881v1><strong>Whispering Pixels: Exploiting Uninitialized Register Accesses in Modern GPUs</strong></a></p><p><em>Frederik Dermot Pustelnik, Xhani Marvin Sa√ü, Jean-Pierre Seifert</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Graphic Processing Units (GPUs) have transcended their traditional use-caseof rendering graphics and nowadays also serve as a powerful platform foraccelerating ubiquitous, non-graphical rendering tasks. One prominent task isinference of neural networks, which process vast amounts of personal data, suchas audio, text or images. Thus, GPUs became integral components for handlingvast amounts of potentially confidential data, which has awakened the interestof security researchers. This lead to the discovery of various vulnerabilitiesin GPUs in recent years. In this paper, we uncover yet another vulnerabilityclass in GPUs: We found that some GPU implementations lack proper registerinitialization routines before shader execution, leading to unintended registercontent leakage of previously executed shader kernels. We showcase theexistence of the aforementioned vulnerability on products of 3 major vendors -Apple, NVIDIA and Qualcomm. The vulnerability poses unique challenges to anadversary due to opaque scheduling and register remapping algorithms present inthe GPU firmware, complicating the reconstruction of leaked data. In order toillustrate the real-world impact of this flaw, we showcase how these challengescan be solved for attacking various workloads on the GPU. First, we showcasehow uninitialized registers leak arbitrary pixel data processed by fragmentshaders. We further implement information leakage attacks on intermediate dataof Convolutional Neural Networks (CNNs) and present the attack&rsquo;s capability toleak and reconstruct the output of Large Language Models (LLMs).</div></details><blockquote><p><strong><em>2024-01-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.11658v2><strong>Traces of Memorisation in Large Language Models for Code</strong></a></p><p><em>Ali Al-Kaswan, Maliheh Izadi, Arie van Deursen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models have gained significant popularity because of theirability to generate human-like text and potential applications in variousfields, such as Software Engineering. Large language models for code arecommonly trained on large unsanitised corpora of source code scraped from theinternet. The content of these datasets is memorised and can be extracted byattackers with data extraction attacks. In this work, we explore memorisationin large language models for code and compare the rate of memorisation withlarge language models trained on natural language. We adopt an existingbenchmark for natural language and construct a benchmark for code byidentifying samples that are vulnerable to attack. We run both benchmarksagainst a variety of models, and perform a data extraction attack. We find thatlarge language models for code are vulnerable to data extraction attacks, liketheir natural language counterparts. From the training data that was identifiedto be potentially extractable we were able to extract 47% from aCodeGen-Mono-16B code completion model. We also observe that models memorisemore, as their parameter count grows, and that their pre-training data are alsovulnerable to attack. We also find that data carriers are memorised at a higherrate than regular code or documentation and that different model architecturesmemorise different samples. Data leakage has severe outcomes, so we urge theresearch community to further investigate the extent of this phenomenon using awider range of models and extraction techniques in order to build safeguards tomitigate this issue.</div></details><p><a href=http://arxiv.org/abs/2308.04748v2><strong>Fuzz4All: Universal Fuzzing with Large Language Models</strong></a></p><p><em>Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fuzzing has achieved tremendous success in discovering bugs andvulnerabilities in various software systems. Systems under test (SUTs) thattake in programming or formal language as inputs, e.g., compilers, runtimeengines, constraint solvers, and software libraries with accessible APIs, areespecially important as they are fundamental building blocks of softwaredevelopment. However, existing fuzzers for such systems often target a specificlanguage, and thus cannot be easily applied to other languages or even otherversions of the same language. Moreover, the inputs generated by existingfuzzers are often limited to specific features of the input language, and thuscan hardly reveal bugs related to other or new features. This paper presentsFuzz4All, the first fuzzer that is universal in the sense that it can targetmany different input languages and many different features of these languages.The key idea behind Fuzz4All is to leverage large language models (LLMs) as aninput generation and mutation engine, which enables the approach to producediverse and realistic inputs for any practically relevant language. To realizethis potential, we present a novel autoprompting technique, which creates LLMprompts that are wellsuited for fuzzing, and a novel LLM-powered fuzzing loop,which iteratively updates the prompt to create new fuzzing inputs. We evaluateFuzz4All on nine systems under test that take in six different languages (C,C++, Go, SMT2, Java and Python) as inputs. The evaluation shows, across all sixlanguages, that universal fuzzing achieves higher coverage than existing,language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs inwidely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskitquantum computing platform, with 64 bugs already confirmed by developers aspreviously unknown.</div></details><blockquote><p><strong><em>2024-01-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.04076v3><strong>Greening Large Language Models of Code</strong></a></p><p><em>Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers&rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\times$less), carbon footprint (up to 157$\times$ less), and inference latency (up to76$\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).</div></details><p><a href=http://arxiv.org/abs/2401.00280v2><strong>Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation</strong></a></p><p><em>Reza Fayyazi, Rozhina Taghdimi, Shanchieh Jay Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Tactics, Techniques, and Procedures (TTPs) outline the methods attackers useto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&amp;CKframework can be challenging for cybersecurity practitioners due to presumedexpertise, complex dependencies, and inherent ambiguity. Meanwhile,advancements with Large Language Models (LLMs) have led to recent surge instudies exploring its uses in cybersecurity operations. This leads us toquestion how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5)LLMs can comprehend and summarize TTPs to inform analysts of the intendedpurposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMshave shown to be prone to hallucination by providing inaccurate information,which is problematic in critical domains like cybersecurity. Therefore, wepropose the use of Retrieval Augmented Generation (RAG) techniques to extractrelevant contexts for each cyberattack procedure for decoder-only LLMs (withoutfine-tuning). We further contrast such approach against supervised fine-tuning(SFT) of encoder-only LLMs. Our results reveal that both the direct-use ofdecoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-onlyLLMs offer inaccurate interpretation of cyberattack procedures. Significantimprovements are shown when RAG is used for decoder-only LLMs, particularlywhen directly relevant context is found. This study further sheds insights onthe limitations and capabilities of using RAG for LLMs in interpreting TTPs.</div></details><blockquote><p><strong><em>2024-01-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.00319v2><strong>LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack</strong></a></p><p><em>Hai Zhu, Zhaoqing Yang, Weiwei Shang, Yuren Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Natural language processing models are vulnerable to adversarial examples.Previous textual adversarial attacks adopt gradients or confidence scores tocalculate word importance ranking and generate adversarial examples. However,this information is unavailable in the real world. Therefore, we focus on amore realistic and challenging setting, named hard-label attack, in which theattacker can only query the model and obtain a discrete prediction label.Existing hard-label attack algorithms tend to initialize adversarial examplesby random substitution and then utilize complex heuristic algorithms tooptimize the adversarial perturbation. These methods require a lot of modelqueries and the attack success rate is restricted by adversary initialization.In this paper, we propose a novel hard-label attack algorithm named LimeAttack,which leverages a local explainable method to approximate word importanceranking, and then adopts beam search to find the optimal solution. Extensiveexperiments show that LimeAttack achieves the better attacking performancecompared with existing hard-label attack under the same query budget. Inaddition, we evaluate the effectiveness of LimeAttack on large language models,and results indicate that adversarial examples remain a significant threat tolarge language models. The adversarial examples crafted by LimeAttack arehighly transferable and effectively improve model robustness in adversarialtraining.</div></details><blockquote><p><strong><em>2024-01-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.09085v4><strong>The Earth is Flat because&mldr;: Investigating LLMs&rsquo; Belief towards Misinformation via Persuasive Conversation</strong></a></p><p><em>Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) encapsulate vast amounts of knowledge but stillremain vulnerable to external misinformation. Existing research mainly studiedthis susceptibility behavior in a single-turn setting. However, belief canchange during a multi-turn conversation, especially a persuasive one.Therefore, in this study, we delve into LLMs&rsquo; susceptibility to persuasiveconversations, particularly on factual questions that they can answercorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, whichcontains factual questions paired with systematically generated persuasivemisinformation. Then, we develop a testing framework to track LLMs&rsquo; beliefchanges in a persuasive dialogue. Through extensive experiments, we find thatLLMs&rsquo; correct beliefs on factual knowledge can be easily manipulated by variouspersuasive strategies.</div></details><blockquote><p><strong><em>2024-01-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.03741v1><strong>Enhanced Automated Code Vulnerability Repair using Large Language Models</strong></a></p><p><em>David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This research addresses the complex challenge of automated repair of codevulnerabilities, vital for enhancing digital security in an increasinglytechnology-driven world. The study introduces a novel and efficient format forthe representation of code modification, using advanced Large Language Models(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasetsfeaturing C code vulnerabilities, significantly improve the accuracy andadaptability of automated code repair techniques. A key finding is the enhancedrepair accuracy of these models when compared to previous methods such asVulRepair, which underscores their practical utility and efficiency. Theresearch also offers a critical assessment of current evaluation metrics, suchas perfect predictions, and their limitations in reflecting the truecapabilities of automated repair models in real-world scenarios. Followingthis, it underscores the importance of using test datasets devoid of trainsamples, emphasizing the need for dataset integrity to enhance theeffectiveness of LLMs in code repair tasks. The significance of this work isits contribution to digital security, setting new standards for automated codevulnerability repair and paving the way for future advancements in the fieldsof cybersecurity and artificial intelligence. The study does not only highlightthe potential of LLMs in enhancing code security but also fosters furtherexploration and research in these crucial areas.</div></details><p><a href=http://arxiv.org/abs/2312.17673v2><strong>Jatmo: Prompt Injection Defense by Task-Specific Finetuning</strong></a></p><p><em>Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, David Wagner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are attracting significant research attentiondue to their instruction-following abilities, allowing users and developers toleverage LLMs for a variety of tasks. However, LLMs are vulnerable toprompt-injection attacks: a class of attacks that hijack the model&rsquo;sinstruction-following abilities, changing responses to prompts to undesired,possibly malicious ones. In this work, we introduce Jatmo, a method forgenerating task-specific models resilient to prompt-injection attacks. Jatmoleverages the fact that LLMs can only follow instructions once they haveundergone instruction tuning. It harnesses a teacher instruction-tuned model togenerate a task-specific dataset, which is then used to fine-tune a base model(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and adataset of inputs for the task: it uses the teacher model to generate outputs.For situations with no pre-existing datasets, Jatmo can use a single example,or in some cases none at all, to produce a fully synthetic dataset. Ourexperiments on seven tasks show that Jatmo models provide similar quality ofoutputs on their specific task as standard LLMs, while being resilient toprompt injections. The best attacks succeeded in less than 0.5% of casesagainst our models, versus 87% success rate against GPT-3.5-Turbo. We releaseJatmo at <a href=https://github.com/wagner-group/prompt-injection-defense>https://github.com/wagner-group/prompt-injection-defense</a>.</div></details><blockquote><p><strong><em>2024-01-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.04136v1><strong>The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline</strong></a></p><p><em>Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The commercialization of diffusion models, renowned for their ability togenerate high-quality images that are often indistinguishable from real ones,brings forth potential copyright concerns. Although attempts have been made toimpede unauthorized access to copyrighted material during training and tosubsequently prevent DMs from generating copyrighted images, the effectivenessof these solutions remains unverified. This study explores the vulnerabilitiesassociated with copyright protection in DMs by introducing a backdoor datapoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.Our attack method operates without requiring access to or control over thediffusion model&rsquo;s training or fine-tuning processes; it merely involves theinsertion of poisoning data into the clean training dataset. This data,comprising poisoning images equipped with prompts, is generated by leveragingthe powerful capabilities of multimodal large language models and text-guidedimage inpainting techniques. Our experimental results and analysis confirm themethod&rsquo;s effectiveness. By integrating a minor portion ofnon-copyright-infringing stealthy poisoning data into the cleandataset-rendering it free from suspicion-we can prompt the finetuned diffusionmodels to produce copyrighted content when activated by specific triggerprompts. These findings underline potential pitfalls in the prevailingcopyright protection strategies and underscore the necessity for increasedscrutiny and preventative measures against the misuse of DMs.</div></details><blockquote><p><strong><em>2024-01-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.01886v2><strong>InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models</strong></a></p><p><em>Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large vision-language models (LVLMs) have demonstrated their incrediblecapability in image understanding and response generation. However, this richvisual interaction also makes LVLMs vulnerable to adversarial examples. In thispaper, we formulate a novel and practical gray-box attack scenario that theadversary can only access the visual encoder of the victim LVLM, without theknowledge of its prompts (which are often proprietary for service providers andnot publicly available) and its underlying large language model (LLM). Thispractical setting poses challenges to the cross-prompt and cross-modeltransferability of targeted adversarial attack, which aims to confuse the LVLMto output a response that is semantically similar to the attacker&rsquo;s chosentarget text. To this end, we propose an instruction-tuned targeted attack(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs withhigh transferability. Initially, we utilize a public text-to-image generativemodel to &ldquo;reverse&rdquo; the target response into a target image, and employ GPT-4 toinfer a reasonable instruction $\boldsymbol{p}^\prime$ from the targetresponse. We then form a local surrogate model (sharing the same visual encoderwith the victim LVLM) to extract instruction-aware features of an adversarialimage example and the target image, and minimize the distance between these twofeatures to optimize the adversarial example. To further improve thetransferability, we augment the instruction $\boldsymbol{p}^\prime$ withinstructions paraphrased from an LLM. Extensive experiments demonstrate thesuperiority of our proposed method in targeted attack performance andtransferability.</div></details><blockquote><p><strong><em>2024-01-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00991v1><strong>A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models</strong></a></p><p><em>Daniel Wankit Yip, Aysan Esmradi, Chun Fai Chan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prompt injection attacks exploit vulnerabilities in large language models(LLMs) to manipulate the model into unintended actions or generate maliciouscontent. As LLM integrated applications gain wider adoption, they face growingsusceptibility to such attacks. This study introduces a novel evaluationframework for quantifying the resilience of applications. The frameworkincorporates innovative techniques designed to ensure representativeness,interpretability, and robustness. To ensure the representativeness of simulatedattacks on the application, a meticulous selection process was employed,resulting in 115 carefully chosen attacks based on coverage and relevance. Forenhanced interpretability, a second LLM was utilized to evaluate the responsesgenerated from these simulated attacks. Unlike conventional malicious contentclassifiers that provide only a confidence score, the LLM-based evaluationproduces a score accompanied by an explanation, thereby enhancinginterpretability. Subsequently, a resilience score is computed by assigninghigher weights to attacks with greater impact, thus providing a robustmeasurement of the application resilience. To assess the framework&rsquo;s efficacy,it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed thatLlama2, the newer model exhibited higher resilience compared to ChatGLM. Thisfinding substantiates the effectiveness of the framework, aligning with theprevailing notion that newer models tend to possess greater resilience.Moreover, the framework exhibited exceptional versatility, requiring onlyminimal adjustments to accommodate emerging attack techniques andclassifications, thereby establishing itself as an effective and practicalsolution. Overall, the framework offers valuable insights that empowerorganizations to make well-informed decisions to fortify their applicationsagainst potential threats from prompt injection.</div></details><blockquote><p><strong><em>2023-12-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00563v1><strong>KernelGPT: Enhanced Kernel Fuzzing via Large Language Models</strong></a></p><p><em>Chenyuan Yang, Zijie Zhao, Lingming Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Bugs in operating system kernels can affect billions of devices and users allover the world. As a result, a large body of research has been focused onkernel fuzzing, i.e., automatically generating syscall (system call) sequencesto detect potential kernel bugs or vulnerabilities. Syzkaller, one of the mostwidely studied kernel fuzzers, aims to generate valid syscall sequences basedon predefined specifications written in syzlang, a domain-specific language fordefining syscalls, their arguments, and the relationships between them. Whilethere has been existing work trying to automate Syzkaller specificationgeneration, this still remains largely manual work and a large number ofimportant syscalls are still uncovered. In this paper, we propose KernelGPT,the first approach to automatically inferring Syzkaller specifications viaLarge Language Models (LLMs) for enhanced kernel fuzzing. Our basic insight isthat LLMs have seen massive kernel code, documentation, and use cases duringpre-training, and thus can automatically distill the necessary information formaking valid syscalls. More specifically, KernelGPT leverages an iterativeapproach to automatically infer all the necessary specification components, andfurther leverages the validation feedback to repair/refine the initialspecifications. Our preliminary results demonstrate that KernelGPT can helpSyzkaller achieve higher coverage and find multiple previously unknown bugs.Moreover, we also received a request from the Syzkaller team to upstreamspecifications inferred by KernelGPT.</div></details><blockquote><p><strong><em>2023-12-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00287v1><strong>The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness</strong></a></p><p><em>Neeraj Varshney, Pavel Dolin, Agastya Seth, Chitta Baral</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As Large Language Models (LLMs) play an increasingly pivotal role in naturallanguage processing applications, their safety concerns become critical areasof NLP research. This paper presents Safety and Over-Defensiveness Evaluation(SODE) benchmark: a collection of diverse safe and unsafe prompts withcarefully designed evaluation methods that facilitate systematic evaluation,comparison, and analysis over &lsquo;safety&rsquo; and &lsquo;over-defensiveness.&rsquo; With SODE, westudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,which reveals several interesting and important findings, such as (a) thewidely popular &lsquo;self-checking&rsquo; techniques indeed improve the safety againstunsafe inputs, but this comes at the cost of extreme over-defensiveness on thesafe inputs, (b) providing a safety instruction along with in-context exemplars(of both safe and unsafe inputs) consistently improves safety and alsomitigates undue over-defensiveness of the models, (c) providing contextualknowledge easily breaks the safety guardrails and makes the models morevulnerable to generating unsafe responses. Overall, our work reveals numeroussuch critical findings that we believe will pave the way and facilitate furtherresearch in improving the safety of LLMs.</div></details><blockquote><p><strong><em>2023-12-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.17115v1><strong>How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation</strong></a></p><p><em>Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, Pengfei Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human behavior simulation of AI agents necessitates the agents to possess aquality of believability, which is crucial as it facilitates users inestablishing trust toward the agents and streamlines the fulfillment of theagents&rsquo; goal. While recent advancements in Large Language Model (LLM) basedagents have improved human behavior simulation, challenges inherent to LLMs(e.g., long context modeling) can undermine their believability. Consequently,evaluating AI agent believability becomes imperative. Unfortunately, priorresearch often neglects the negative impacts of LLM deficiencies. To addressthese gaps, we introduce two metrics for assessing LLM-based agentbelievability: consistency, and robustness, together with a benchmark,SimulateBench, with which, we evaluate the consistency and robustness of agentsimplemented with popular LLMs. We find that agents (i) struggle to accuratelydepict character information when presented with lengthy profile inputs; (ii)exhibit vulnerability to profile perturbations; and (iii) are significantlyaffected by certain key factors that impact their overall believability. Codeand SimulateBench are public at <a href=https://github.com/GAIR-NLP/GPTMan>https://github.com/GAIR-NLP/GPTMan</a>.</div></details><blockquote><p><strong><em>2023-12-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.03314v2><strong>GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis</strong></a></p><p><em>Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Haijun Wang, Zhengzi Xu, Xiaofei Xie, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Smart contracts are prone to various vulnerabilities, leading to substantialfinancial losses over time. Current analysis tools mainly targetvulnerabilities with fixed control or data-flow patterns, such as re-entrancyand integer overflow. However, a recent study on Web3 security bugs revealedthat about 80% of these bugs cannot be audited by existing tools due to thelack of domain-specific property description and checking. Given recentadvances in Large Language Models (LLMs), it is worth exploring how GenerativePre-training Transformer (GPT) could aid in detecting logicc vulnerabilities. In this paper, we propose GPTScan, the first tool combining GPT with staticanalysis for smart contract logic vulnerability detection. Instead of relyingsolely on GPT to identify vulnerabilities, which can lead to high falsepositives and is limited by GPT&rsquo;s pre-trained knowledge, we utilize GPT as aversatile code understanding tool. By breaking down each logic vulnerabilitytype into scenarios and properties, GPTScan matches candidate vulnerabilitieswith GPT. To enhance accuracy, GPTScan further instructs GPT to intelligentlyrecognize key variables and statements, which are then validated by staticconfirmation. Evaluation on diverse datasets with around 400 contract projectsand 3K Solidity files shows that GPTScan achieves high precision (over 90%) fortoken contracts and acceptable precision (57.14%) for large projects likeWeb3Bugs. It effectively detects ground-truth logic vulnerabilities with arecall of over 70%, including 9 new vulnerabilities missed by human auditors.GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01USD to scan per thousand lines of Solidity code. Moreover, static confirmationhelps GPTScan reduce two-thirds of false positives.</div></details><p><a href=http://arxiv.org/abs/2308.15736v3><strong>Vulnerability of Machine Learning Approaches Applied in IoT-based Smart Grid: A Review</strong></a></p><p><em>Zhenyong Zhang, Mengxiang Liu, Mingyang Sun, Ruilong Deng, Peng Cheng, Dusit Niyato, Mo-Yuen Chow, Jiming Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning (ML) sees an increasing prevalence of being used in theinternet-of-things (IoT)-based smart grid. However, the trustworthiness of MLis a severe issue that must be addressed to accommodate the trend of ML-basedsmart grid applications (MLsgAPPs). The adversarial distortion injected intothe power signal will greatly affect the system&rsquo;s normal control and operation.Therefore, it is imperative to conduct vulnerability assessment for MLsgAPPsapplied in the context of safety-critical power systems. In this paper, weprovide a comprehensive review of the recent progress in designing attack anddefense methods for MLsgAPPs. Unlike the traditional survey about ML security,this is the first review work about the security of MLsgAPPs that focuses onthe characteristics of power systems. We first highlight the specifics forconstructing the adversarial attacks on MLsgAPPs. Then, the vulnerability ofMLsgAPP is analyzed from both the aspects of the power system and ML model.Afterward, a comprehensive survey is conducted to review and compare existingstudies about the adversarial attacks on MLsgAPPs in scenarios of generation,transmission, distribution, and consumption, and the countermeasures arereviewed according to the attacks that they defend against. Finally, the futureresearch directions are discussed on the attacker&rsquo;s and defender&rsquo;s side,respectively. We also analyze the potential vulnerability of large languagemodel-based (e.g., ChatGPT) power system applications. Overall, we encouragemore researchers to contribute to investigating the adversarial issues ofMLsgAPPs.</div></details><blockquote><p><strong><em>2023-12-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.10766v2><strong>A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection</strong></a></p><p><em>Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, Chao Shen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models and Multi-Modal LLMs have become pervasive, and so doesthe importance of their security; yet, modern LLMs are known to be vulnerableto jailbreaking attacks. These attacks can allow malicious users to exploit themodels, making the case for effective jailbreak detection mechanisms anessential aspect of maintaining the integrity and trustworthiness of LLM-basedapplications. However, existing detection works on jailbreak attacks havelimitations. Existing post-query-based strategies require target domainknowledge, and pre-query-based methods mainly focus on text-level attacks andfail to meet the increasingly complex multi-modal security requirements placedupon contemporary LLMs. This gap underscores the need for a more comprehensiveapproach to safeguarding these influential systems. In this work, we propose JailGuard, the first mutation-based jailbreakingdetection framework which supports both image and text modalities. Our keyobservation is that attack queries inherently possess less robustness comparedto benign queries. Specifically, to confuse the model, attack queries areusually crafted with well-designed templates or complicate perturbations,leading to a fact that a slight disturbance in input may result in a drasticchange in the response. This lack of robustness can be utilized in attackdetection. Based on this intuition, we designed and implemented a detectionframework comprising 19 different mutators and a divergence-based detectionformula. To fully understand the effectiveness of our framework, we built thefirst multi-modal LLM jailbreaking attack dataset, which has 304 items of data,covering ten types of known jailbreaking attacks on image and text modalities.The evaluation suggests that JailGuard achieves the best detection accuracy of89.38%/85.42% on image and text inputs, outperforming state-of-the-art defensemethods by 15.28%.</div></details><blockquote><p><strong><em>2023-12-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.12420v3><strong>How Far Have We Gone in Vulnerability Detection Using Large Language Models</strong></a></p><p><em>Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As software becomes increasingly complex and prone to vulnerabilities,automated vulnerability detection is critically important, yet challenging.Given the significant successes of large language models (LLMs) in varioustasks, there is growing anticipation of their efficacy in vulnerabilitydetection. However, a quantitative understanding of their potential invulnerability detection is still missing. To bridge this gap, we introduce acomprehensive vulnerability benchmark VulBench. This benchmark aggregateshigh-quality data from a wide range of CTF (Capture-the-Flag) challenges andreal-world applications, with annotations for each vulnerable functiondetailing the vulnerability type and its root cause. Through our experimentsencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based modelsand static analyzers, we find that several LLMs outperform traditional deeplearning approaches in vulnerability detection, revealing an untapped potentialin LLMs. This work contributes to the understanding and utilization of LLMs forenhanced software security.</div></details><p><a href=http://arxiv.org/abs/2312.14480v1><strong>MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models</strong></a></p><p><em>Hongyin Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are increasingly being used in Metaverseenvironments to generate dynamic and realistic content and to control thebehavior of non-player characters (NPCs). However, the cybersecurity concernsassociated with LLMs have become increasingly prominent. Previous research hasprimarily focused on patching system vulnerabilities to enhance cybersecurity,but these approaches are not well-suited to the Metaverse, where the virtualspace is more complex, LLMs are vulnerable, and ethical user interaction iscritical. Moreover, the scope of cybersecurity in the Metaverse is expected toexpand significantly. This paper proposes a method for enhancing cybersecuritythrough the simulation of user interaction with LLMs. Our goal is to educateusers and strengthen their defense capabilities through exposure to acomprehensive simulation system. This system includes extensive Metaversecybersecurity Q&amp;A and attack simulation scenarios. By engaging with these,users will improve their ability to recognize and withstand risks.Additionally, to address the ethical implications of user input, we proposeusing LLMs as evaluators to assess user content across five dimensions. Wefurther adapt the models through vocabulary expansion training to betterunderstand personalized inputs and emoticons. We conduct experiments onmultiple LLMs and find that our approach is effective.</div></details><blockquote><p><strong><em>2023-12-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.13530v1><strong>HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion</strong></a></p><p><em>Yu-Zheng Lin, Muntasir Mamun, Muhtasim Alam Chowdhury, Shuyu Cai, Mingyu Zhu, Banafsheh Saber Latibari, Kevin Immanuel Gubbi, Najmeh Nazari Bavarsad, Arjun Caputo, Avesta Sasan, Houman Homayoun, Setareh Rafatirad, Pratik Satam, Soheil Salehi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The escalating complexity of modern computing frameworks has resulted in asurge in the cybersecurity vulnerabilities reported to the NationalVulnerability Database (NVD) by practitioners. Despite the fact that thestature of NVD is one of the most significant databases for the latest insightsinto vulnerabilities, extracting meaningful trends from such a large amount ofunstructured data is still challenging without the application of suitabletechnological methodologies. Previous efforts have mostly concentrated onsoftware vulnerabilities; however, a holistic strategy incorporates approachesfor mitigating vulnerabilities, score prediction, and a knowledge-generatingsystem that may extract relevant insights from the Common Weakness Enumeration(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. Asthe number of hardware attacks on Internet of Things (IoT) devices continues torapidly increase, we present the Hardware Vulnerability to Weakness Mapping(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing onhardware vulnerabilities and IoT security. The architecture that we haveproposed incorporates an Ontology-driven Storytelling framework, whichautomates the process of updating the ontology in order to recognize patternsand evolution of vulnerabilities over time and provides approaches formitigating the vulnerabilities. The repercussions of vulnerabilities can bemitigated as a result of this, and conversely, future exposures can bepredicted and prevented. Furthermore, our proposed framework utilizedGenerative Pre-trained Transformer (GPT) Large Language Models (LLMs) toprovide mitigation suggestions.</div></details><blockquote><p><strong><em>2023-12-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.06854v2><strong>Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks</strong></a></p><p><em>Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Contrastive vision-language representation learning has achievedstate-of-the-art performance for zero-shot classification, by learning frommillions of image-caption pairs crawled from the internet. However, the massivedata that powers large multimodal models such as CLIP, makes them extremelyvulnerable to various types of targeted data poisoning and backdoor attacks.Despite this vulnerability, robust contrastive vision-language pre-trainingagainst such attacks has remained unaddressed. In this work, we propose ROCLIP,the first effective method for robust pre-training multimodal vision-languagemodels against targeted data poisoning and backdoor attacks. ROCLIP effectivelybreaks the association between poisoned image-caption pairs by considering arelatively large and varying pool of random captions, and matching every imagewith the text that is most similar to it in the pool instead of its owncaption, every few epochs.It also leverages image and text augmentations tofurther strengthen the defense and improve the performance of the model. Ourextensive experiments show that ROCLIP renders state-of-the-art targeted datapoisoning and backdoor attacks ineffective during pre-training CLIP models. Inparticular, ROCLIP decreases the success rate for targeted data poisoningattacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, whileimproving the model&rsquo;s linear probe performance by 10% and maintains a similarzero shot performance compared to CLIP. By increasing the frequency ofmatching, ROCLIP is able to defend strong attacks, which add up to 1% poisonedexamples to the data, and successfully maintain a low attack success rate of12.5%, while trading off the performance on some tasks.</div></details><p><a href=http://arxiv.org/abs/2312.12575v1><strong>Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet</strong></a></p><p><em>Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, Gianluca Stringhini</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have been suggested for use in automatedvulnerability repair, but benchmarks showing they can consistently identifysecurity-related bugs are lacking. We thus perform the most detailedinvestigation to date on whether LLMs can reliably identify security-relatedbugs. We construct a series of 228 code scenarios and analyze eight of the mostcapable LLMs across eight different investigative dimensions in an automatedframework. Our evaluation shows LLMs provide non-deterministic responses,incorrect and unfaithful reasoning, and perform poorly in real-world scenariosoutside their knowledge cut-off date. Most importantly, our findings revealsignificant non-robustness in even the most advanced models like <code>PaLM2' and</code>GPT-4&rsquo;: by merely changing function or variable names, or by the addition oflibrary functions in the source code, these models can yield incorrect answersin 26% and 17% of cases, respectively. These findings demonstrate that furtherLLM advances are needed before LLMs can be used as general purpose securityassistants.</div></details><p><a href=http://arxiv.org/abs/2307.07924v4><strong>Communicative Agents for Software Development</strong></a></p><p><em>Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Software engineering is a domain characterized by intricate decision-makingprocesses, often relying on nuanced intuition and consultation. Recentadvancements in deep learning have started to revolutionize softwareengineering practices through elaborate designs implemented at various stagesof software development. In this paper, we present an innovative paradigm thatleverages large language models (LLMs) throughout the entire softwaredevelopment process, streamlining and unifying key processes through naturallanguage communication, thereby eliminating the need for specialized models ateach phase. At the core of this paradigm lies ChatDev, a virtual chat-poweredsoftware development company that mirrors the established waterfall model,meticulously dividing the development process into four distinct chronologicalstages: designing, coding, testing, and documenting. Each stage engages a teamof &ldquo;software agents&rdquo;, such as programmers, code reviewers, and test engineers,fostering collaborative dialogue and facilitating a seamless workflow. The chatchain acts as a facilitator, breaking down each stage into atomic subtasks.This enables dual roles, allowing for proposing and validating solutionsthrough context-aware communication, leading to efficient resolution ofspecific subtasks. The instrumental analysis of ChatDev highlights itsremarkable efficacy in software generation, enabling the completion of theentire software development process in under seven minutes at a cost of lessthan one dollar. It not only identifies and alleviates potentialvulnerabilities but also rectifies potential hallucinations while maintainingcommendable efficiency and cost-effectiveness. The potential of ChatDev unveilsfresh possibilities for integrating LLMs into the realm of softwaredevelopment. Our code is available at <a href=https://github.com/OpenBMB/ChatDev>https://github.com/OpenBMB/ChatDev</a>.</div></details><blockquote><p><strong><em>2023-12-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.03622v3><strong>Do Users Write More Insecure Code with AI Assistants?</strong></a></p><p><em>Neil Perry, Megha Srivastava, Deepak Kumar, Dan Boneh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We conduct the first large-scale user study examining how users interact withan AI Code assistant to solve a variety of security related tasks acrossdifferent programming languages. Overall, we find that participants who hadaccess to an AI assistant based on OpenAI&rsquo;s codex-davinci-002 model wrotesignificantly less secure code than those without access. Additionally,participants with access to an AI assistant were more likely to believe theywrote secure code than those without access to the AI assistant. Furthermore,we find that participants who trusted the AI less and engaged more with thelanguage and format of their prompts (e.g. re-phrasing, adjusting temperature)provided code with fewer security vulnerabilities. Finally, in order to betterinform the design of future AI-based Code assistants, we provide an in-depthanalysis of participants&rsquo; language and interaction behavior, as well as releaseour user interface as an instrument to conduct similar studies in the future.</div></details><p><a href=http://arxiv.org/abs/2310.12439v2><strong>PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models</strong></a></p><p><em>Hongwei Yao, Jian Lou, Zhan Qin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prompts have significantly improved the performance of pretrained LargeLanguage Models (LLMs) on various downstream tasks recently, making themincreasingly indispensable for a diverse range of LLM application scenarios.However, the backdoor vulnerability, a serious security threat that canmaliciously alter the victim model&rsquo;s normal predictions, has not beensufficiently explored for prompt-based LLMs. In this paper, we presentPOISONPROMPT, a novel backdoor attack capable of successfully compromising bothhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, androbustness of POISONPROMPT through extensive experiments on three popularprompt methods, using six datasets and three widely used LLMs. Our findingshighlight the potential security threats posed by backdoor attacks onprompt-based LLMs and emphasize the need for further research in this area.</div></details><p><a href=http://arxiv.org/abs/2312.09494v2><strong>No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models</strong></a></p><p><em>Shengyao Zhang, Mi Zhang, Xudong Pan, Min Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To reduce the computation cost and the energy consumption in large languagemodels (LLM), skimming-based acceleration dynamically drops unimportant tokensof the input sequence progressively along layers of the LLM while preservingthe tokens of semantic importance. However, our work for the first time revealsthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In thispaper, we propose No-Skim, a general framework to help the owners ofskimming-based LLM to understand and measure the robustness of theiracceleration scheme. Specifically, our framework searches minimal andunnoticeable perturbations at character-level and token-level to generateadversarial inputs that sufficiently increase the remaining token ratio, thusincreasing the computation cost and energy consumption. We systematicallyevaluate the vulnerability of the skimming acceleration in various LLMarchitectures including BERT and RoBERTa on the GLUE benchmark. In the worstcase, the perturbation found by No-Skim substantially increases the runningcost of LLM by over 145% on average. Moreover, No-Skim extends the evaluationframework to various scenarios, making the evaluation conductible withdifferent level of knowledge.</div></details><p><a href=http://arxiv.org/abs/2312.10982v1><strong>A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models</strong></a></p><p><em>Aysan Esmradi, Daniel Wankit Yip, Chun Fai Chan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Ensuring the security of large language models (LLMs) is an ongoing challengedespite their widespread popularity. Developers work to enhance LLMs security,but vulnerabilities persist, even in advanced versions like GPT-4. Attackersexploit these weaknesses, highlighting the need for proactive cybersecuritymeasures in AI model development. This article explores two attack categories:attacks on models themselves and attacks on model applications. The formerrequires expertise, access to model data, and significant implementation time,while the latter is more accessible to attackers and has seen increasedattention. Our study reviews over 100 recent research works, providing anin-depth analysis of each attack type. We identify the latest attack methodsand explore various approaches to carry them out. We thoroughly investigatemitigation techniques, assessing their effectiveness and limitations.Furthermore, we summarize future defenses against these attacks. We alsoexamine real-world techniques, including reported and our implemented attackson LLMs, to consolidate our findings. Our research highlights the urgency ofaddressing security concerns and aims to enhance the understanding of LLMattacks, contributing to robust defense development in this evolving domain.</div></details><blockquote><p><strong><em>2023-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.10524v1><strong>Comprehensive Evaluation of ChatGPT Reliability Through Multilingual Inquiries</strong></a></p><p><em>Poorna Chander Reddy Puttaparthi, Soham Sanjay Deo, Hakan Gul, Yiming Tang, Weiyi Shang, Zhe Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: ChatGPT is currently the most popular large language model (LLM), with over100 million users, making a significant impact on people&rsquo;s lives. However, dueto the presence of jailbreak vulnerabilities, ChatGPT might have negativeeffects on people&rsquo;s lives, potentially even facilitating criminal activities.Testing whether ChatGPT can cause jailbreak is crucial because it can enhanceChatGPT&rsquo;s security, reliability, and social responsibility. Inspired byprevious research revealing the varied performance of LLMs in differentlanguage translations, we suspected that wrapping prompts in multiple languagesmight lead to ChatGPT jailbreak. To investigate this, we designed a study witha fuzzing testing approach to analyzing ChatGPT&rsquo;s cross-linguistic proficiency.Our study includes three strategies by automatically posing different formatsof malicious questions to ChatGPT: (1) each malicious question involving onlyone language, (2) multilingual malicious questions, (3) specifying that ChatGPTresponds in a language different from the prompts. In addition, we also combineour strategies by utilizing prompt injection templates to wrap the threeaforementioned types of questions. We examined a total of 7,892 Q&amp;A datapoints, discovering that multilingual wrapping can indeed lead to ChatGPT&rsquo;sjailbreak, with different wrapping methods having varying effects on jailbreakprobability. Prompt injection can amplify the probability of jailbreak causedby multilingual wrapping. This work provides insights for OpenAI developers toenhance ChatGPT&rsquo;s support for language diversity and inclusion.</div></details><blockquote><p><strong><em>2023-12-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.10847v5><strong>Large Language Models can be Guided to Evade AI-Generated Text Detection</strong></a></p><p><em>Ning Lu, Shengcai Liu, Rui He, Qi Wang, Yew-Soon Ong, Ke Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have shown remarkable performance in varioustasks and have been extensively utilized by the public. However, the increasingconcerns regarding the misuse of LLMs, such as plagiarism and spamming, haveled to the development of multiple detectors, including fine-tuned classifiersand statistical methods. In this study, we equip LLMs with prompts, rather thanrelying on an external paraphraser, to evaluate the vulnerability of thesedetectors. We propose a novel Substitution-based In-Context exampleOptimization method (SICO) to automatically construct prompts for evading thedetectors. SICO is cost-efficient as it requires only 40 human-written examplesand a limited number of LLM inferences to generate a prompt. Moreover, once atask-specific prompt has been constructed, it can be universally used against awide range of detectors. Extensive experiments across three real-world tasksdemonstrate that SICO significantly outperforms the paraphraser baselines andenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by0.5 on average. Furthermore, a comprehensive human evaluation as well as avalidation experiment in the wild show that the SICO-generated text achieveshuman-level readability and task completion rates. Finally, the strongperformance of SICO exhibits its potential as a reliable evaluation tool forfuture detectors. The codes and data are located onhttps://github.com/ColinLu50/Evade-GPT-Detector.</div></details><blockquote><p><strong><em>2023-12-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.11523v1><strong>ToViLaG: Your Visual-Language Generative Model is Also An Evildoer</strong></a></p><p><em>Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Warning: this paper includes model outputs showing offensive content. Recentlarge-scale Visual-Language Generative Models (VLGMs) have achievedunprecedented improvement in multimodal image/text generation. However, thesemodels might also generate toxic content, e.g., offensive text and pornographyimages, raising significant ethical risks. Despite exhaustive studies on toxicdegeneration of language models, this problem remains largely unexplored withinthe context of visual-language generation. This work delves into the propensityfor toxicity generation and susceptibility to toxic data across various VLGMs.For this purpose, we built ToViLaG, a dataset comprising 32Kco-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text thattends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicitymetric tailored to visual-language generation, which theoretically reflectsdifferent aspects of toxicity considering both input and output. On such abasis, we benchmarked the toxicity of a diverse spectrum of VLGMs anddiscovered that some models do more evil than expected while some are morevulnerable to infection, underscoring the necessity of VLGMs detoxification.Therefore, we develop an innovative bottleneck-based detoxification method. Ourmethod could reduce toxicity while maintaining comparable generation quality,providing a promising initial solution to this line of research.</div></details><p><a href=http://arxiv.org/abs/2311.05608v2><strong>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</strong></a></p><p><em>Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Ensuring the safety of artificial intelligence-generated content (AIGC) is alongstanding topic in the artificial intelligence (AI) community, and thesafety concerns associated with Large Language Models (LLMs) have been widelyinvestigated. Recently, large vision-language models (VLMs) represent anunprecedented revolution, as they are built upon LLMs but can incorporateadditional modalities (e.g., images). However, the safety of VLMs lackssystematic evaluation, and there may be an overconfidence in the safetyguarantees provided by their underlying LLMs. In this paper, to demonstratethat introducing additional modality modules leads to unforeseen AI safetyissues, we propose FigStep, a straightforward yet effective jailbreakingalgorithm against VLMs. Instead of feeding textual harmful instructionsdirectly, FigStep converts the harmful content into images through typographyto bypass the safety alignment within the textual module of the VLMs, inducingVLMs to output unsafe responses that violate common AI safety policies. In ourevaluation, we manually review 46,500 model responses generated by 3 familiesof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a totalof 6 VLMs). The experimental results show that FigStep can achieve an averageattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, wedemonstrate that the methodology of FigStep can even jailbreak GPT-4V, whichalready leverages an OCR detector to filter harmful queries. Above all, ourwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlightsthe necessity of novel safety alignments between visual and textual modalities.</div></details><blockquote><p><strong><em>2023-12-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.04730v2><strong>DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions</strong></a></p><p><em>Fangzhou Wu, Xiaogeng Liu, Chaowei Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advancement of Large Language Models (LLMs), significant progresshas been made in code generation, enabling LLMs to transform natural languageinto programming code. These Code LLMs have been widely accepted by massiveusers and organizations. However, a dangerous nature is hidden in the code,which is the existence of fatal vulnerabilities. While some LLM providers haveattempted to address these issues by aligning with human guidance, theseefforts fall short of making Code LLMs practical and robust. Without a deepunderstanding of the performance of the LLMs under the practical worst cases,it would be concerning to apply them to various real-world applications. Inthis paper, we answer the critical issue: Are existing Code LLMs immune togenerating vulnerable code? If not, what is the possible maximum severity ofthis issue in practical deployment scenarios? In this paper, we introduceDeceptPrompt, a novel algorithm that can generate adversarial natural languageinstructions that drive the Code LLMs to generate functionality correct codewith vulnerabilities. DeceptPrompt is achieved through a systematicevolution-based algorithm with a fine grain loss design. The unique advantageof DeceptPrompt enables us to find natural prefix/suffix with totally benignand non-directional semantic meaning, meanwhile, having great power in inducingthe Code LLMs to generate vulnerable code. This feature can enable us toconduct the almost-worstcase red-teaming on these LLMs in a real scenario,where users are using natural language. Our extensive experiments and analyseson DeceptPrompt not only validate the effectiveness of our approach but alsoshed light on the huge weakness of LLMs in the code generation task. Whenapplying the optimized prefix/suffix, the attack success rate (ASR) willimprove by average 50% compared with no prefix/suffix applying.</div></details><p><a href=http://arxiv.org/abs/2312.06924v1><strong>Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack</strong></a></p><p><em>Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent developments in balancing the usefulness and safety of Large LanguageModels (LLMs) have raised a critical question: Are mainstream NLP tasksadequately aligned with safety consideration? Our study, focusing onsafety-sensitive documents obtained through adversarial attacks, revealssignificant disparities in the safety alignment of various NLP tasks. Forinstance, LLMs can effectively summarize malicious long documents but oftenrefuse to translate them. This discrepancy highlights a previously unidentifiedvulnerability: attacks exploiting tasks with weaker safety alignment, likesummarization, can potentially compromise the integraty of tasks traditionallydeemed more robust, such as translation and question-answering (QA). Moreover,the concurrent use of multiple NLP tasks with lesser safety alignment increasesthe risk of LLMs inadvertently processing harmful content. We demonstrate thesevulnerabilities in various safety-aligned LLMs, particularly Llama2 models andGPT-4, indicating an urgent need for strengthening safety alignments across abroad spectrum of NLP tasks.</div></details><p><a href=http://arxiv.org/abs/2312.01241v2><strong>Just-in-Time Security Patch Detection &ndash; LLM At the Rescue for Data Augmentation</strong></a></p><p><em>Xunzhu Tang, Zhenghan Chen, Kisub Kim, Haoye Tian, Saad Ezzini, Jacques Klein</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the face of growing vulnerabilities found in open-source software, theneed to identify {discreet} security patches has become paramount. The lack ofconsistency in how software providers handle maintenance often leads to therelease of security patches without comprehensive advisories, leaving usersvulnerable to unaddressed security risks. To address this pressing issue, weintroduce a novel security patch detection system, LLMDA, which capitalizes onLarge Language Models (LLMs) and code-text alignment methodologies for patchreview, data enhancement, and feature combination. Within LLMDA, we initiallyutilize LLMs for examining patches and expanding data of PatchDB and SPI-DB,two security patch datasets from recent literature. We then use labeledinstructions to direct our LLMDA, differentiating patches based on securityrelevance. Following this, we apply a PTFormer to merge patches with code,formulating hybrid attributes that encompass both the innate details and theinterconnections between the patches and the code. This distinctive combinationmethod allows our system to capture more insights from the combined context ofpatches and code, hence improving detection precision. Finally, we devise aprobabilistic batch contrastive learning mechanism within batches to augmentthe capability of the our LLMDA in discerning security patches. The resultsreveal that LLMDA significantly surpasses the start of the art techniques indetecting security patches, underscoring its promise in fortifying softwaremaintenance.</div></details><blockquote><p><strong><em>2023-12-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.04004v2><strong>Occlusion-based Detection of Trojan-triggering Inputs in Large Language Models of Code</strong></a></p><p><em>Aftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Mohammad Amin Alipour, Bowen Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are becoming an integrated part of softwaredevelopment. These models are trained on large datasets for code, where it ishard to verify each data point. Therefore, a potential attack surface can be toinject poisonous data into the training data to make models vulnerable, akatrojaned. It can pose a significant threat by hiding manipulative behaviorsinside models, leading to compromising the integrity of the models indownstream tasks. In this paper, we propose an occlusion-based human-in-the-loop technique,OSeql, to distinguish trojan-triggering inputs of code. The technique is basedon the observation that trojaned neural models of code rely heavily on thetriggering part of input; hence, its removal would change the confidence of themodels in their prediction substantially. Our results suggest that OSeql candetect the triggering inputs with almost 100% recall. We discuss the problem offalse positives and how to address them. These results provide a baseline forfuture studies in this field.</div></details><blockquote><p><strong><em>2023-12-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.11070v3><strong>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</strong></a></p><p><em>Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural networks (DNNs) have achieved tremendous success in variousapplications including video action recognition, yet remain vulnerable tobackdoor attacks (Trojans). The backdoor-compromised model will mis-classify tothe target class chosen by the attacker when a test instance (from a non-targetclass) is embedded with a specific trigger, while maintaining high accuracy onattack-free instances. Although there are extensive studies on backdoor attacksagainst image data, the susceptibility of video-based systems under backdoorattacks remains largely unexplored. Current studies are direct extensions ofapproaches proposed for image data, e.g., the triggers are independentlyembedded within the frames, which tend to be detectable by existing defenses.In this paper, we introduce a simple yet effective backdoor attack againstvideo data. Our proposed attack, adding perturbations in a transformed domain,plants an imperceptible, temporally distributed trigger across the videoframes, and is shown to be resilient to existing defensive strategies. Theeffectiveness of the proposed attack is demonstrated by extensive experimentswith various well-known models on two video recognition benchmarks, UCF101 andHMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL)dataset. We delve into the impact of several influential factors on ourproposed attack and identify an intriguing effect termed &ldquo;collateral damage"through extensive studies.</div></details><blockquote><p><strong><em>2023-12-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.05275v1><strong>Exploring the Limits of ChatGPT in Software Security Applications</strong></a></p><p><em>Fangzhou Wu, Qingzhao Zhang, Ati Priya Bajaj, Tiffany Bao, Ning Zhang, Ruoyu &ldquo;Fish&rdquo; Wang, Chaowei Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have undergone rapid evolution and achievedremarkable results in recent times. OpenAI&rsquo;s ChatGPT, backed by GPT-3.5 orGPT-4, has gained instant popularity due to its strong capability across a widerange of tasks, including natural language tasks, coding, mathematics, andengaging conversations. However, the impacts and limits of such LLMs in systemsecurity domain are less explored. In this paper, we delve into the limits ofLLMs (i.e., ChatGPT) in seven software security applications includingvulnerability detection/repair, debugging, debloating, decompilation, patching,root cause analysis, symbolic execution, and fuzzing. Our exploration revealsthat ChatGPT not only excels at generating code, which is the conventionalapplication of language models, but also demonstrates strong capability inunderstanding user-provided commands in natural languages, reasoning aboutcontrol and data flows within programs, generating complex data structures, andeven decompiling assembly code. Notably, GPT-4 showcases significantimprovements over GPT-3.5 in most security tasks. Also, certain limitations ofChatGPT in security-related tasks are identified, such as its constrainedability to process long code contexts.</div></details><blockquote><p><strong><em>2023-12-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.04748v1><strong>Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks</strong></a></p><p><em>Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie Baracaldo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Growing applications of large language models (LLMs) trained by a third partyraise serious concerns on the security vulnerability of LLMs.It has beendemonstrated that malicious actors can covertly exploit these vulnerabilitiesin LLMs through poisoning attacks aimed at generating undesirable outputs.While poisoning attacks have received significant attention in the image domain(e.g., object detection), and classification tasks, their implications forgenerative models, particularly in the realm of natural language generation(NLG) tasks, remain poorly understood. To bridge this gap, we perform acomprehensive exploration of various poisoning techniques to assess theireffectiveness across a range of generative tasks. Furthermore, we introduce arange of metrics designed to quantify the success and stealthiness of poisoningattacks specifically tailored to NLG tasks. Through extensive experiments onmultiple NLG tasks, LLMs and datasets, we show that it is possible tosuccessfully poison an LLM during the fine-tuning stage using as little as 1%of the total tuning data samples. Our paper presents the first systematicapproach to comprehend poisoning attacks targeting NLG tasks considering a widerange of triggers and attack settings. We hope our findings will assist the AIsecurity community in devising appropriate defenses against such threats.</div></details><p><a href=http://arxiv.org/abs/2309.14348v2><strong>Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM</strong></a></p><p><em>Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, Large Language Models (LLMs) have made significant advancements andare now widely used across various domains. Unfortunately, there has been arising concern that LLMs can be misused to generate harmful or maliciouscontent. Though a line of research has focused on aligning LLMs with humanvalues and preventing them from producing inappropriate content, suchalignments are usually vulnerable and can be bypassed by alignment-breakingattacks via adversarially optimized or handcrafted jailbreaking prompts. Inthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend againstpotential alignment-breaking attacks. RA-LLM can be directly constructed uponan existing aligned LLM with a robust alignment checking function, withoutrequiring any expensive retraining or fine-tuning process of the original LLM.Furthermore, we also provide a theoretical analysis for RA-LLM to verify itseffectiveness in defending against alignment-breaking attacks. Throughreal-world experiments on open-source large language models, we demonstratethat RA-LLM can successfully defend against both state-of-the-art adversarialprompts and popular handcrafted jailbreaking prompts by reducing their attacksuccess rates from nearly 100% to around 10% or less.</div></details><blockquote><p><strong><em>2023-12-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.03088v1><strong>LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications</strong></a></p><p><em>Brett Israelsen, Soumalya Sarkar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models have seen rapid progress in capability in recent years;this progress has been accelerating and their capabilities, measured by variousbenchmarks, are beginning to approach those of humans. There is a strong demandto use such models in a wide variety of applications but, due to unresolvedvulnerabilities and limitations, great care needs to be used before applyingthem to intelligence and safety-critical applications. This paper reviewsrecent literature related to LLM assessment and vulnerabilities to synthesizethe current research landscape and to help understand what advances are mostcritical to enable use of of these technologies in intelligence andsafety-critical applications. The vulnerabilities are broken down into tenhigh-level categories and overlaid onto a high-level life cycle of an LLM. Somegeneral categories of mitigations are reviewed.</div></details><blockquote><p><strong><em>2023-12-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.01787v1><strong>Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection</strong></a></p><p><em>Toygar Tanyel, Besher Alkurdi, Serkan Ayvaz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the proliferation of social media, there has been a sharp increase inoffensive content, particularly targeting vulnerable groups, exacerbatingsocial problems such as hatred, racism, and sexism. Detecting offensivelanguage use is crucial to prevent offensive language from being widely sharedon social media. However, the accurate detection of irony, implication, andvarious forms of hate speech on social media remains a challenge. Naturallanguage-based deep learning models require extensive training with large,comprehensive, and labeled datasets. Unfortunately, manually creating suchdatasets is both costly and error-prone. Additionally, the presence ofhuman-bias in offensive language datasets is a major concern for deep learningmodels. In this paper, we propose a linguistic data augmentation approach toreduce bias in labeling processes, which aims to mitigate the influence ofhuman bias by leveraging the power of machines to improve the accuracy andfairness of labeling processes. This approach has the potential to improveoffensive language classification tasks across multiple languages and reducethe prevalence of offensive content on social media.</div></details><blockquote><p><strong><em>2023-12-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.00843v1><strong>Exploring the Robustness of Decentralized Training for Large Language Models</strong></a></p><p><em>Lin Lu, Chenxi Dai, Wangcheng Tao, Binhang Yuan, Yanan Sun, Pan Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Decentralized training of large language models has emerged as an effectiveway to democratize this technology. However, the potential threats associatedwith this approach have not been carefully discussed, which would hinder thedevelopment of decentralized training infrastructures. This paper aims toinitiate discussion towards this end by exploring the robustness ofdecentralized training from three main perspectives. First, we demonstrate thevulnerabilities inherent in decentralized training frameworks in terms ofhardware, data, and models. Second, we highlight the fundamental differencebetween decentralized foundation model training and vanilla federated learning,where the security techniques employed in federated learning cannot be applieddirectly. Third, we discuss the essential components required for a robust andefficient decentralized training framework and present a case study by modelinga concrete threat model. Our objective in this vision paper is to emphasize theimportance of addressing security concerns in the context of decentralizedtraining for large language models.</div></details><blockquote><p><strong><em>2023-11-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.18085v1><strong>Leveraging a Randomized Key Matrix to Enhance the Security of Symmetric Substitution Ciphers</strong></a></p><p><em>Shubham Gandhi, Om Khare, Mihika Dravid, Mihika Sanghvi, Sunil Mane, Aadesh Gajaralwar, Saloni Gandhi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: An innovative strategy to enhance the security of symmetric substitutionciphers is presented, through the implementation of a randomized key matrixsuitable for various file formats, including but not limited to binary and textfiles. Despite their historical relevance, symmetric substitution ciphers havebeen limited by vulnerabilities to cryptanalytic methods like frequencyanalysis and known plaintext attacks. The aim of our research is to mitigatethese vulnerabilities by employing a polyalphabetic substitution strategy thatincorporates a distinct randomized key matrix. This matrix plays a pivotal rolein generating a unique random key, comprising characters, encompassing bothuppercase and lowercase letters, numeric, and special characters, to derive thecorresponding ciphertext. The effectiveness of the proposed methodology inenhancing the security of conventional substitution methods for file encryptionand decryption is supported by comprehensive testing and analysis, whichencompass computational speed, frequency analysis, keyspace examination,Kasiski test, entropy analysis, and the utilization of a large language model.</div></details><p><a href=http://arxiv.org/abs/2306.01286v2><strong>KL-Divergence Guided Temperature Sampling</strong></a></p><p><em>Chung-Ching Chang, David Reitter, Renat Aksitov, Yun-Hsuan Sung</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Temperature sampling is a conventional approach to diversify large languagemodel predictions. As temperature increases, the prediction becomes diverse butalso vulnerable to hallucinations &ndash; generating tokens that are sensible butnot factual. One common approach to mitigate hallucinations is to providesource/grounding documents and the model is trained to produce predictions thatbind to and are attributable to the provided source. It appears that there is atrade-off between diversity and attribution. To mitigate any such trade-off, wepropose to relax the constraint of having a fixed temperature over decodingsteps, and a mechanism to guide the dynamic temperature according to itsrelevance to the source through KL-divergence. Our experiments justifies thetrade-off, and shows that our sampling algorithm outperforms the conventionaltop-k and top-p algorithms in conversational question-answering andsummarization tasks.</div></details><p><a href=http://arxiv.org/abs/2310.03684v3><strong>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks</strong></a></p><p><em>Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite efforts to align large language models (LLMs) with human values,widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible tojailbreaking attacks, wherein an adversary fools a targeted LLM into generatingobjectionable content. To address this vulnerability, we propose SmoothLLM, thefirst algorithm designed to mitigate jailbreaking attacks on LLMs. Based on ourfinding that adversarially-generated prompts are brittle to character-levelchanges, our defense first randomly perturbs multiple copies of a given inputprompt, and then aggregates the corresponding predictions to detect adversarialinputs. SmoothLLM reduces the attack success rate on numerous popular LLMs tobelow one percentage point, avoids unnecessary conservatism, and admitsprovable guarantees on attack mitigation. Moreover, our defense usesexponentially fewer queries than existing attacks and is compatible with anyLLM. Our code is publicly available at the following link:https://github.com/arobey1/smooth-llm.</div></details><p><a href=http://arxiv.org/abs/2311.16153v2><strong>Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</strong></a></p><p><em>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, Radha Poovendran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users&rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user&rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.</div></details><blockquote><p><strong><em>2023-11-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.02993v1><strong>ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health Information Ecosystem</strong></a></p><p><em>Khalid Al-hammuri, Fayez Gebali, Awos Kanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Managing access between large numbers of distributed medical devices hasbecome a crucial aspect of modern healthcare systems, enabling theestablishment of smart hospitals and telehealth infrastructure. However, astelehealth technology continues to evolve and Internet of Things (IoT) devicesbecome more widely used, they are also becoming increasingly exposed to varioustypes of vulnerabilities and medical errors. In healthcare information systems,about 90% of vulnerabilities emerged from misuse cases and human errors. As aresult, there is a need for additional research and development of securitytools to prevent such attacks. This article proposes a zero-trust-basedcontext-aware framework for managing access to the main components of the cloudecosystem, including users, devices and output data. The main goal and benefitof the proposed framework is to build a scoring system to prevent or alleviatemisuse cases while using distributed medical devices in cloud-based healthcareinformation systems. The framework has two main scoring schemas to maintain thechain of trust. First, it proposes a critical trust score based on cloud-nativemicro-services of authentication, encryption, logging, and authorizations.Second, creating a bond trust scoring to assess the real-time semantic andsyntactic analysis of attributes stored in a healthcare information system. Theanalysis is based on a pre-trained machine learning model to generate thesemantic and syntactic scores. The framework also takes into account regulatorycompliance and user consent to create a scoring system. The advantage of thismethod is that it is applicable to any language and adapts to all attributes asit relies on a language model, not just a set of predefined and limitedattributes. The results show a high F1 score of 93.5%, which proves that it isvalid for detecting misuse cases.</div></details><blockquote><p><strong><em>2023-11-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.10819v3><strong>Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</strong></a></p><p><em>Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have demonstrated exceptional proficiency ininstruction-following, becoming increasingly crucial across variousapplications. However, this capability brings with it the risk of promptinjection attacks, where attackers inject instructions into LLMs&rsquo; input toelicit undesirable actions or content. Understanding the robustness of LLMsagainst such attacks is vital for their safe implementation. In this work, weestablish a benchmark to evaluate the robustness of instruction-following LLMsagainst prompt injection attacks. Our objective is to determine the extent towhich LLMs can be influenced by injected instructions and their ability todifferentiate between these injected and original target instructions. Throughextensive experiments with leading instruction-following LLMs, we uncoversignificant vulnerabilities in their robustness to such attacks. Our resultsindicate that some models are overly tuned to follow any embedded instructionsin the prompt, overly focusing on the latter parts of the prompt without fullygrasping the entire context. By contrast, models with a better grasp of thecontext and instruction-following capabilities will potentially be moresusceptible to compromise by injected instructions. This underscores the needto shift the focus from merely enhancing LLMs&rsquo; instruction-followingcapabilities to improving their overall comprehension of prompts anddiscernment of instructions that are appropriate to follow. We hope ourin-depth analysis offers insights into the underlying causes of thesevulnerabilities, aiding in the development of future solutions. Code and dataare available athttps://github.com/Leezekun/instruction-following-robustness-eval</div></details><blockquote><p><strong><em>2023-11-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.03348v2><strong>Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</strong></a></p><p><em>Rusheb Shah, Quentin Feuillade&ndash;Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite efforts to align large language models to produce harmless responses,they are still vulnerable to jailbreak prompts that elicit unrestrictedbehaviour. In this work, we investigate persona modulation as a black-boxjailbreaking method to steer a target model to take on personalities that arewilling to comply with harmful instructions. Rather than manually craftingprompts for each persona, we automate the generation of jailbreaks using alanguage model assistant. We demonstrate a range of harmful completions madepossible by persona modulation, including detailed instructions forsynthesising methamphetamine, building a bomb, and laundering money. Theseautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is185 times larger than before modulation (0.23%). These prompts also transfer toClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,respectively. Our work reveals yet another vulnerability in commercial largelanguage models and highlights the need for more comprehensive safeguards.</div></details><p><a href=http://arxiv.org/abs/2311.09433v2><strong>Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</strong></a></p><p><em>Haoran Wang, Kai Shu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To ensure AI safety, instruction-tuned Large Language Models (LLMs) arespecifically trained to ensure alignment, which refers to making models behavein accordance with human intentions. While these models have demonstratedcommendable results on various safety benchmarks, the vulnerability of theirsafety alignment has not been extensively studied. This is particularlytroubling given the potential harm that LLMs can inflict. Existing attackmethods on LLMs often rely on poisoned training data or the injection ofmalicious prompts. These approaches compromise the stealthiness andgeneralizability of the attacks, making them susceptible to detection.Additionally, these models often demand substantial computational resources forimplementation, making them less practical for real-world applications.Inspired by recent success in modifying model behavior through steering vectorswithout the need for optimization, and drawing on its effectiveness inred-teaming LLMs, we conducted experiments employing activation steering totarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness- across a varied set of attack settings. To establish a universal attackstrategy applicable to diverse target alignments without depending on manualanalysis, we automatically select the intervention layer based on contrastivelayer search. Our experiment results show that activation attacks are highlyeffective and add little or no overhead to attack efficiency. Additionally, wediscuss potential countermeasures against such activation attacks. Our code anddata are available at <a href=https://github.com/wang2226/Backdoor-Activation-AttackWarning>https://github.com/wang2226/Backdoor-Activation-AttackWarning</a>: this paper contains content that can be offensive or upsetting.</div></details><blockquote><p><strong><em>2023-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.13445v1><strong>Transfer Attacks and Defenses for Large Language Models on Coding Tasks</strong></a></p><p><em>Chi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina Pasareanu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern large language models (LLMs), such as ChatGPT, have demonstratedimpressive capabilities for coding tasks including writing and reasoning aboutcode. They improve upon previous neural network models of code, such ascode2seq or seq2seq, that already demonstrated competitive results whenperforming tasks such as code summarization and identifying codevulnerabilities. However, these previous code models were shown vulnerable toadversarial examples, i.e. small syntactic perturbations that do not change theprogram&rsquo;s semantics, such as the inclusion of &ldquo;dead code&rdquo; through falseconditions or the addition of inconsequential print statements, designed to"fool" the models. LLMs can also be vulnerable to the same adversarialperturbations but a detailed study on this concern has been lacking so far. Inthis paper we aim to investigate the effect of adversarial perturbations oncoding tasks with LLMs. In particular, we study the transferability ofadversarial examples, generated through white-box attacks on smaller codemodels, to LLMs. Furthermore, to make the LLMs more robust against suchadversaries without incurring the cost of retraining, we propose prompt-baseddefenses that involve modifying the prompt to include additional informationsuch as examples of adversarially perturbed code and explicit instructions forreversing adversarial perturbations. Our experiments show that adversarialexamples obtained with a smaller code model are indeed transferable, weakeningthe LLMs&rsquo; performance. The proposed defenses show promise in improving themodel&rsquo;s resilience, paving the way to more robust defensive solutions for LLMsin code-related applications.</div></details><blockquote><p><strong><em>2023-11-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.01446v3><strong>Open Sesame! Universal Black Box Jailbreaking of Large Language Models</strong></a></p><p><em>Raz Lapid, Ron Langberg, Moshe Sipper</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), designed to provide helpful and safe responses,often rely on alignment techniques to align with user intent and socialguidelines. Unfortunately, this alignment can be exploited by malicious actorsseeking to manipulate an LLM&rsquo;s outputs for unintended purposes. In this paperwe introduce a novel approach that employs a genetic algorithm (GA) tomanipulate LLMs when model architecture and parameters are inaccessible. The GAattack works by optimizing a universal adversarial prompt that &ndash; when combinedwith a user&rsquo;s query &ndash; disrupts the attacked model&rsquo;s alignment, resulting inunintended and potentially harmful outputs. Our novel approach systematicallyreveals a model&rsquo;s limitations and vulnerabilities by uncovering instances whereits responses deviate from expected behavior. Through extensive experiments wedemonstrate the efficacy of our technique, thus contributing to the ongoingdiscussion on responsible AI development by providing a diagnostic tool forevaluating and enhancing alignment of LLMs with human intent. To our knowledgethis is the first automated universal black box jailbreak attack.</div></details><blockquote><p><strong><em>2023-11-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.11861v1><strong>Generating Valid and Natural Adversarial Examples with Large Language Models</strong></a></p><p><em>Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning-based natural language processing (NLP) models, particularlypre-trained language models (PLMs), have been revealed to be vulnerable toadversarial attacks. However, the adversarial examples generated by manymainstream word-level adversarial attack models are neither valid nor natural,leading to the loss of semantic maintenance, grammaticality, and humanimperceptibility. Based on the exceptional capacity of language understandingand generation of large language models (LLMs), we propose LLM-Attack, whichaims at generating both valid and natural adversarial examples with LLMs. Themethod consists of two stages: word importance ranking (which searches for themost vulnerable words) and word synonym replacement (which substitutes themwith their synonyms obtained from LLMs). Experimental results on the MovieReview (MR), IMDB, and Yelp Review Polarity datasets against the baselineadversarial attack models illustrate the effectiveness of LLM-Attack, and itoutperforms the baselines in human and GPT-4 evaluation by a significantmargin. The model can generate adversarial examples that are typically validand natural, with the preservation of semantic meaning, grammaticality, andhuman imperceptibility.</div></details><p><a href=http://arxiv.org/abs/2311.11796v1><strong>Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems</strong></a></p><p><em>Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial Intelligence (AI) systems such as autonomous vehicles, facialrecognition, and speech recognition systems are increasingly integrated intoour daily lives. However, despite their utility, these AI systems arevulnerable to a wide range of attacks such as adversarial, backdoor, datapoisoning, membership inference, model inversion, and model stealing attacks.In particular, numerous attacks are designed to target a particular model orsystem, yet their effects can spread to additional targets, referred to astransferable attacks. Although considerable efforts have been directed towarddeveloping transferable attacks, a holistic understanding of the advancementsin transferable attacks remains elusive. In this paper, we comprehensivelyexplore learning-based attacks from the perspective of transferability,particularly within the context of cyber-physical security. We delve intodifferent domains &ndash; the image, text, graph, audio, and video domains &ndash; tohighlight the ubiquitous and pervasive nature of transferable attacks. Thispaper categorizes and reviews the architecture of existing attacks from variousviewpoints: data, process, model, and system. We further examine theimplications of transferable attacks in practical scenarios such as autonomousdriving, speech recognition, and large language models (LLMs). Additionally, weoutline the potential research directions to encourage efforts in exploring thelandscape of transferable attacks. This survey offers a holistic understandingof the prevailing transferable attacks and their impacts across differentdomains.</div></details><blockquote><p><strong><em>2023-11-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.11177v1><strong>Assessing the Security of GitHub Copilot Generated Code &ndash; A Targeted Replication Study</strong></a></p><p><em>Vahid Majdinasab, Michael Joshua Bishop, Shawn Rasheed, Arghavan Moradidakhel, Amjed Tahir, Foutse Khomh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: AI-powered code generation models have been developing rapidly, allowingdevelopers to expedite code generation and thus improve their productivity.These models are trained on large corpora of code (primarily sourced frompublic repositories), which may contain bugs and vulnerabilities. Severalconcerns have been raised about the security of the code generated by thesemodels. Recent studies have investigated security issues in AI-powered codegeneration tools such as GitHub Copilot and Amazon CodeWhisperer, revealingseveral security weaknesses in the code generated by these tools. As thesetools evolve, it is expected that they will improve their security protocols toprevent the suggestion of insecure code to developers. This paper replicatesthe study of Pearce et al., which investigated security weaknesses in Copilotand uncovered several weaknesses in the code suggested by Copilot acrossdiverse scenarios and languages (Python, C and Verilog). Our replicationexamines Copilot security weaknesses using newer versions of Copilot and CodeQL(the security analysis framework). The replication focused on the presence ofsecurity vulnerabilities in Python code. Our results indicate that, even withthe improvements in newer versions of Copilot, the percentage of vulnerablecode suggestions has reduced from 36.54% to 27.25%. Nonetheless, it remainsevident that the model still suggests insecure code.</div></details><blockquote><p><strong><em>2023-11-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16169v1><strong>Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities</strong></a></p><p><em>Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, Mayur Naik</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Security vulnerabilities in modern software are prevalent and harmful. Whileautomated vulnerability detection tools have made promising progress, theirscalability and applicability remain challenging. Recently, Large LanguageModels (LLMs), such as GPT-4 and CodeLlama, have demonstrated remarkableperformance on code-related tasks. However, it is unknown whether such LLMs cando complex reasoning over code. In this work, we explore whether pre-trainedLLMs can detect security vulnerabilities and address the limitations ofexisting tools. We evaluate the effectiveness of pre-trained LLMs on a set offive diverse security benchmarks spanning two languages, Java and C/C++, andincluding code samples from synthetic and real-world projects. We evaluate theeffectiveness of LLMs in terms of their performance, explainability, androbustness. By designing a series of effective prompting strategies, we obtain the bestresults on the synthetic datasets with GPT-4: F1 scores of 0.79 on OWASP, 0.86on Juliet Java, and 0.89 on Juliet C/C++. Expectedly, the performance of LLMsdrops on the more challenging real-world datasets: CVEFixes Java and CVEFixesC/C++, with GPT-4 reporting F1 scores of 0.48 and 0.62, respectively. We showthat LLMs can often perform better than existing static analysis and deeplearning-based vulnerability detection tools, especially for certain classes ofvulnerabilities. Moreover, LLMs also often provide reliable explanations,identifying the vulnerable data flows in code. We find that fine-tuning smallerLLMs can outperform the larger LLMs on synthetic datasets but provide limitedgains on real-world datasets. When subjected to adversarial attacks on code,LLMs show mild degradation, with average accuracy reduction of up to 12.67%.Finally, we share our insights and recommendations for future work onleveraging LLMs for vulnerability detection.</div></details><p><a href=http://arxiv.org/abs/2311.09641v1><strong>On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models</strong></a></p><p><em>Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Reinforcement Learning with Human Feedback (RLHF) is a methodology designedto align Large Language Models (LLMs) with human preferences, playing animportant role in LLMs alignment. Despite its advantages, RLHF relies on humanannotators to rank the text, which can introduce potential securityvulnerabilities if any adversarial annotator (i.e., attackers) manipulates theranking score by up-ranking any malicious text to steer the LLM adversarially.To assess the red-teaming of RLHF against human preference data poisoning, wepropose RankPoison, a poisoning attack method on candidates&rsquo; selection ofpreference rank flipping to reach certain malicious behaviors (e.g., generatinglonger sequences, which can increase the computational cost). With poisoneddataset generated by RankPoison, we can perform poisoning attacks on LLMs togenerate longer tokens without hurting the original safety alignmentperformance. Moreover, applying RankPoison, we also successfully implement abackdoor attack where LLMs can generate longer answers under questions with thetrigger word. Our findings highlight critical security challenges in RLHF,underscoring the necessity for more robust alignment methods for LLMs.</div></details><blockquote><p><strong><em>2023-11-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09188v1><strong>Towards Verifiable Text Generation with Symbolic References</strong></a></p><p><em>Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have demonstrated an impressive ability tosynthesize plausible and fluent text. However they remain vulnerable tohallucinations, and thus their outputs generally require manual humanverification for high-stakes applications, which can be time-consuming anddifficult. This paper proposes symbolically grounded generation (SymGen) as asimple approach for enabling easier validation of an LLM&rsquo;s output. SymGenprompts an LLM to interleave its regular output text with explicit symbolicreferences to fields present in some conditioning data (e.g., a table in JSONformat). The references can be used to display the provenance of differentspans of text in the generation, reducing the effort required for manualverification. Across data-to-text and question answering experiments, we findthat LLMs are able to directly output text that makes use of symbolicreferences while maintaining fluency and accuracy.</div></details><p><a href=http://arxiv.org/abs/2312.00027v1><strong>Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</strong></a></p><p><em>Yuanpu Cao, Bochuan Cao, Jinghui Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent developments in Large Language Models (LLMs) have manifestedsignificant advancements. To facilitate safeguards against maliciousexploitation, a body of research has concentrated on aligning LLMs with humanpreferences and inhibiting their generation of inappropriate content.Unfortunately, such alignments are often vulnerable: fine-tuning with a minimalamount of harmful data can easily unalign the target LLM. While beingeffective, such fine-tuning-based unalignment approaches also have their ownlimitations: (1) non-stealthiness, after fine-tuning, safety audits orred-teaming can easily expose the potential weaknesses of the unaligned models,thereby precluding their release/use. (2) non-persistence, the unaligned LLMscan be easily repaired through re-alignment, i.e., fine-tuning again withaligned data points. In this work, we show that it is possible to conductstealthy and persistent unalignment on large language models via backdoorinjections. We also provide a novel understanding on the relationship betweenthe backdoor persistence and the activation pattern and further provideguidelines for potential trigger design. Through extensive experiments, wedemonstrate that our proposed stealthy and persistent unalignment cansuccessfully pass the safety evaluation while maintaining strong persistenceagainst re-alignment defense.</div></details><p><a href=http://arxiv.org/abs/2311.09447v1><strong>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</strong></a></p><p><em>Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.</div></details><blockquote><p><strong><em>2023-11-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.08487v1><strong>Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective</strong></a></p><p><em>Zi Yin, Wei Ding, Jia Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are central to a multitude of applications butstruggle with significant risks, notably in generating harmful content andbiases. Drawing an analogy to the human psyche&rsquo;s conflict between evolutionarysurvival instincts and societal norm adherence elucidated in Freud&rsquo;spsychoanalysis theory, we argue that LLMs suffer a similar fundamentalconflict, arising between their inherent desire for syntactic and semanticcontinuity, established during the pre-training phase, and the post-trainingalignment with human values. This conflict renders LLMs vulnerable toadversarial attacks, wherein intensifying the models&rsquo; desire for continuity cancircumvent alignment efforts, resulting in the generation of harmfulinformation. Through a series of experiments, we first validated the existenceof the desire for continuity in LLMs, and further devised a straightforward yetpowerful technique, such as incomplete sentences, negative priming, andcognitive dissonance scenarios, to demonstrate that even advanced LLMs struggleto prevent the generation of harmful information. In summary, our studyuncovers the root of LLMs&rsquo; vulnerabilities to adversarial attacks, herebyquestioning the efficacy of solely relying on sophisticated alignment methods,and further advocates for a new training idea that integrates modal conceptsalongside traditional amodal concepts, aiming to endow LLMs with a more nuancedunderstanding of real-world contexts and ethical considerations.</div></details><blockquote><p><strong><em>2023-11-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.06899v1><strong>Flames: Benchmarking Value Alignment of Chinese Large Language Models</strong></a></p><p><em>Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The widespread adoption of large language models (LLMs) across variousregions underscores the urgent need to evaluate their alignment with humanvalues. Current benchmarks, however, fall short of effectively uncoveringsafety vulnerabilities in LLMs. Despite numerous models achieving high scoresand &rsquo;topping the chart&rsquo; in these evaluations, there is still a significant gapin LLMs&rsquo; deeper alignment with human values and achieving genuine harmlessness.To this end, this paper proposes the first highly adversarial benchmark namedFlames, consisting of 2,251 manually crafted prompts, ~18.7K model responseswith fine-grained annotations, and a specified scorer. Our frameworkencompasses both common harmlessness principles, such as fairness, safety,legality, and data protection, and a unique morality dimension that integratesspecific Chinese values such as harmony. Based on the framework, we carefullydesign adversarial prompts that incorporate complex scenarios and jailbreakingmethods, mostly with implicit malice. By prompting mainstream LLMs with suchadversarially constructed prompts, we obtain model responses, which are thenrigorously annotated for evaluation. Our findings indicate that all theevaluated LLMs demonstrate relatively poor performance on Flames, particularlyin the safety and fairness dimensions. Claude emerges as the best-performingmodel overall, but with its harmless rate being only 63.08% while GPT-4 onlyscores 39.04%. The complexity of Flames has far exceeded existing benchmarks,setting a new challenge for contemporary LLMs and highlighting the need forfurther alignment of LLMs. To efficiently evaluate new models on the benchmark,we develop a specified scorer capable of scoring LLMs across multipledimensions, achieving an accuracy of 77.4%. The Flames Benchmark is publiclyavailable on <a href=https://github.com/AIFlames/Flames>https://github.com/AIFlames/Flames</a>.</div></details><blockquote><p><strong><em>2023-11-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.05863v1><strong>Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service</strong></a></p><p><em>Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at <a href=https://github.com/Pter61/vlpmarker>https://github.com/Pter61/vlpmarker</a>.</div></details><blockquote><p><strong><em>2023-11-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.06055v4><strong>Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review</strong></a></p><p><em>Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, Gongshen Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Applicating third-party data and models has become a new paradigm forlanguage modeling in NLP, which also introduces some potential securityvulnerabilities because attackers can manipulate the training process and datasource. In this case, backdoor attacks can induce the model to exhibit expectedbehaviors through specific triggers and have little inferior influence onprimitive tasks. Hence, it could have dire consequences, especially consideringthat the backdoor attack surfaces are broad. However, there is still no systematic and comprehensive review to reflect thesecurity challenges, attacker&rsquo;s capabilities, and purposes according to theattack surface. Moreover, there is a shortage of analysis and comparison of thediverse emerging backdoor countermeasures in this context. In this paper, weconduct a timely review of backdoor attacks and countermeasures to sound thered alarm for the NLP security community. According to the affected stage ofthe machine learning pipeline, the attack surfaces are recognized to be wideand then formalized into three categorizations: attacking pre-trained modelwith fine-tuning (APMF) or parameter-efficient tuning (APMP), and attackingfinal model with training (AFMT). Thus, attacks under each categorization arecombed. The countermeasures are categorized into two general classes: sampleinspection and model inspection. Overall, the research on the defense side isfar behind the attack side, and there is no single defense that can prevent alltypes of backdoor attacks. An attacker can intelligently bypass existingdefenses with a more invisible attack. Drawing the insights from the systematicreview, we also present crucial areas for future research on the backdoor, suchas empirical security evaluations on large language models, and in particular,more efficient and practical countermeasures are solicited.</div></details><blockquote><p><strong><em>2023-11-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.04124v1><strong>Unveiling Safety Vulnerabilities of Large Language Models</strong></a></p><p><em>George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As large language models become more prevalent, their possible harmful orinappropriate responses are a cause for concern. This paper introduces a uniquedataset containing adversarial examples in the form of questions, which we callAttaQ, designed to provoke such harmful or inappropriate responses. We assessthe efficacy of our dataset by analyzing the vulnerabilities of various modelswhen subjected to it. Additionally, we introduce a novel automatic approach foridentifying and naming vulnerable semantic regions - input semantic areas forwhich the model is likely to produce harmful outputs. This is achieved throughthe application of specialized clustering techniques that consider both thesemantic similarity of the input attacks and the harmfulness of the model&rsquo;sresponses. Automatically identifying vulnerable semantic regions enhances theevaluation of model weaknesses, facilitating targeted improvements to itssafety mechanisms and overall reliability.</div></details><p><a href=http://arxiv.org/abs/2311.04109v1><strong>Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection</strong></a></p><p><em>Benjamin Steenhoek, Md Mahbubur Rahman, Shaila Sharmin, Wei Le</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, pretrained language models have shown state-of-the-art performanceon the vulnerability detection task. These models are pretrained on a largecorpus of source code, then fine-tuned on a smaller supervised vulnerabilitydataset. Due to the different training objectives and the performance of themodels, it is interesting to consider whether the models have learned thesemantics of code relevant to vulnerability detection, namely bug semantics,and if so, how the alignment to bug semantics relates to model performance. Inthis paper, we analyze the models using three distinct methods:interpretability tools, attention analysis, and interaction matrix analysis. Wecompare the models&rsquo; influential feature sets with the bug semantic featureswhich define the causes of bugs, including buggy paths and PotentiallyVulnerable Statements (PVS). We find that (1) better-performing models alsoaligned better with PVS, (2) the models failed to align strongly to PVS, and(3) the models failed to align at all to buggy paths. Based on our analysis, wedeveloped two annotation methods which highlight the bug semantics inside themodel&rsquo;s inputs. We evaluated our approach on four distinct transformer modelsand four vulnerability datasets and found that our annotations improved themodels&rsquo; performance in the majority of settings - 11 out of 16, with up to 9.57points improvement in F1 score compared to conventional fine-tuning. We furtherfound that with our annotations, the models aligned up to 232% better topotentially vulnerable statements. Our findings indicate that it is helpful toprovide the model with information of the bug semantics, that the model canattend to it, and motivate future work in learning more complex path-based bugsemantics. Our code and data are available athttps://figshare.com/s/4a16a528d6874aad51a0.</div></details><blockquote><p><strong><em>2023-11-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.02147v1><strong>The Alignment Problem in Context</strong></a></p><p><em>Rapha√´l Milli√®re</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A core challenge in the development of increasingly capable AI systems is tomake them safe and reliable by ensuring their behaviour is consistent withhuman values. This challenge, known as the alignment problem, does not merelyapply to hypothetical future AI systems that may pose catastrophic risks; italready applies to current systems, such as large language models, whosepotential for harm is rapidly increasing. In this paper, I assess whether weare on track to solve the alignment problem for large language models, and whatthat means for the safety of future AI systems. I argue that existingstrategies for alignment are insufficient, because large language models remainvulnerable to adversarial attacks that can reliably elicit unsafe behaviour. Ioffer an explanation of this lingering vulnerability on which it is not simplya contingent limitation of current language models, but has deep technical tiesto a crucial aspect of what makes these models useful and versatile in thefirst place &ndash; namely, their remarkable aptitude to learn &ldquo;in context&rdquo; directlyfrom user instructions. It follows that the alignment problem is not onlyunsolved for current AI systems, but may be intrinsically difficult to solvewithout severely undermining their capabilities. Furthermore, this assessmentraises concerns about the prospect of ensuring the safety of future and morecapable AI systems.</div></details><blockquote><p><strong><em>2023-11-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.01011v1><strong>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</strong></a></p><p><em>Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While Large Language Models (LLMs) are increasingly being used in real-worldapplications, they remain vulnerable to prompt injection attacks: maliciousthird party prompts that subvert the intent of the system designer. To helpresearchers study this problem, we present a dataset of over 126,000 promptinjection attacks and 46,000 prompt-based &ldquo;defenses&rdquo; against prompt injection,all created by players of an online game called Tensor Trust. To the best ofour knowledge, this is currently the largest dataset of human-generatedadversarial examples for instruction-following LLMs. The attacks in our datasethave a lot of easily interpretable stucture, and shed light on the weaknessesof LLMs. We also use the dataset to create a benchmark for resistance to twotypes of prompt injection, which we refer to as prompt extraction and prompthijacking. Our benchmark results show that many models are vulnerable to theattack strategies in the Tensor Trust dataset. Furthermore, we show that someattack strategies from the dataset generalize to deployed LLM-basedapplications, even though they have a very different set of constraints to thegame. We release all data and source code at <a href=https://tensortrust.ai/paper>https://tensortrust.ai/paper</a></div></details><blockquote><p><strong><em>2023-11-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.00889v1><strong>Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code</strong></a></p><p><em>Mohammed Latif Siddiq, Joanna C. S. Santos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the growing popularity of Large Language Models (e.g. GitHub Copilot,ChatGPT, etc.) in software engineers&rsquo; daily practices, it is important toensure that the code generated by these tools is not only functionally correctbut also free of vulnerabilities. Although LLMs can help developers to be moreproductive, prior empirical studies have shown that LLMs can generate insecurecode. There are two contributing factors to the insecure code generation.First, existing datasets used to evaluate Large Language Models (LLMs) do notadequately represent genuine software engineering tasks sensitive to security.Instead, they are often based on competitive programming challenges orclassroom-type coding tasks. In real-world applications, the code produced isintegrated into larger codebases, introducing potential security risks. There&rsquo;sa clear absence of benchmarks that focus on evaluating the security of thegenerated code. Second, existing evaluation metrics primarily focus on thefunctional correctness of the generated code while ignoring securityconsiderations. Metrics such as pass@k gauge the probability of obtaining thecorrect code in the top k suggestions. Other popular metrics like BLEU,CodeBLEU, ROUGE, and METEOR similarly emphasize functional accuracy, neglectingsecurity implications. In light of these research gaps, in this paper, wedescribed SALLM, a framework to benchmark LLMs&rsquo; abilities to generate securecode systematically. This framework has three major components: a novel datasetof security-centric Python prompts, an evaluation environment to test thegenerated code, and novel metrics to evaluate the models&rsquo; performance from theperspective of secure code generation.</div></details><blockquote><p><strong><em>2023-10-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.00172v1><strong>Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield</strong></a></p><p><em>Jinhwa Kim, Ali Derakhshan, Ian G. Harris</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models&rsquo; safety remains a critical concern due to theirvulnerability to adversarial attacks, which can prompt these systems to produceharmful responses. In the heart of these systems lies a safety classifier, acomputational model trained to discern and mitigate potentially harmful,offensive, or unethical outputs. However, contemporary safety classifiers,despite their potential, often fail when exposed to inputs infused withadversarial noise. In response, our study introduces the Adversarial PromptShield (APS), a lightweight model that excels in detection accuracy anddemonstrates resilience against adversarial prompts. Additionally, we proposenovel strategies for autonomously generating adversarial training datasets,named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets aredesigned to fortify the safety classifier&rsquo;s robustness, and we investigate theconsequences of incorporating adversarial examples into the training process.Through evaluations involving Large Language Models, we demonstrate that ourclassifier has the potential to decrease the attack success rate resulting fromadversarial attacks by up to 60%. This advancement paves the way for the nextgeneration of more reliable and resilient conversational agents.</div></details><blockquote><p><strong><em>2023-10-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.16934v2><strong>On Evaluating Adversarial Robustness of Large Vision-Language Models</strong></a></p><p><em>Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large vision-language models (VLMs) such as GPT-4 have achieved unprecedentedperformance in response generation, especially with visual inputs, enablingmore creative and adaptable interaction than large language models such asChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, sinceadversaries may successfully evade the entire system by subtly manipulating themost vulnerable modality (e.g., vision). To this end, we propose evaluating therobustness of open-source large VLMs in the most realistic and high-risksetting, where adversaries have only black-box system access and seek todeceive the model into returning the targeted responses. In particular, wefirst craft targeted adversarial examples against pretrained models such asCLIP and BLIP, and then transfer these adversarial examples to other VLMs suchas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, weobserve that black-box queries on these VLMs can further improve theeffectiveness of targeted evasion, resulting in a surprisingly high successrate for generating targeted responses. Our findings provide a quantitativeunderstanding regarding the adversarial vulnerability of large VLMs and callfor a more thorough examination of their potential security flaws beforedeployment in practice. Code is at <a href=https://github.com/yunqing-me/AttackVLM>https://github.com/yunqing-me/AttackVLM</a>.</div></details><blockquote><p><strong><em>2023-10-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.12860v2><strong>Probing LLMs for hate speech detection: strengths and vulnerabilities</strong></a></p><p><em>Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently efforts have been made by social media platforms as well asresearchers to detect hateful or toxic language using large language models.However, none of these works aim to use explanation, additional context andvictim community information in the detection process. We utilise differentprompt variation, input information and evaluate large language models in zeroshot setting (without adding any in-context examples). We select three largelanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -HateXplain, implicit hate and ToxicSpans. We find that on average including thetarget information in the pipeline improves the model performance substantially(~20-30%) over the baseline across the datasets. There is also a considerableeffect of adding the rationales/explanations into the pipeline (~10-20%) overthe baseline across the datasets. In addition, we further provide a typology ofthe error cases where these large language models fail to (i) classify and (ii)explain the reason for the decisions they take. Such vulnerable pointsautomatically constitute &lsquo;jailbreak&rsquo; prompts for these models and industryscale safeguard techniques need to be developed to make the models robustagainst such prompts.</div></details><p><a href=http://arxiv.org/abs/2310.18587v1><strong>Assessing and Improving Syntactic Adversarial Robustness of Pre-trained Models for Code Translation</strong></a></p><p><em>Guang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen, Tingting Han, Taolue Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Context: Pre-trained models (PTMs) have demonstrated significant potential inautomatic code translation. However, the vulnerability of these models intranslation tasks, particularly in terms of syntax, has not been extensivelyinvestigated. Objective: To fill this gap, our study aims to propose a novelapproach CoTR to assess and improve the syntactic adversarial robustness ofPTMs in code translation. Method: CoTR consists of two components: CoTR-A andCoTR-D. CoTR-A generates adversarial examples by transforming programs, whileCoTR-D proposes a semantic distance-based sampling data augmentation method andadversarial training method to improve the model&rsquo;s robustness andgeneralization capabilities. The Pass@1 metric is used by CoTR to assess theperformance of PTMs, which is more suitable for code translation tasks andoffers a more precise evaluation in real world scenarios. Results: Theeffectiveness of CoTR is evaluated through experiments on real world Java toPython datasets. The results demonstrate that CoTR-A can significantly reducethe performance of existing PTMs, while CoTR-D effectively improves therobustness of PTMs. Conclusion: Our study identifies the limitations of currentPTMs, including large language models, in code translation tasks. It highlightsthe potential of CoTR as an effective solution to enhance the robustness ofPTMs for code translation tasks.</div></details><blockquote><p><strong><em>2023-10-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.02122v2><strong>ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP</strong></a></p><p><em>Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, Xiangyu Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Backdoor attacks have emerged as a prominent threat to natural languageprocessing (NLP) models, where the presence of specific triggers in the inputcan lead poisoned models to misclassify these inputs to predetermined targetclasses. Current detection mechanisms are limited by their inability to addressmore covert backdoor strategies, such as style-based attacks. In this work, wepropose an innovative test-time poisoned sample detection framework that hingeson the interpretability of model predictions, grounded in the semantic meaningof inputs. We contend that triggers (e.g., infrequent words) are not supposedto fundamentally alter the underlying semantic meanings of poisoned samples asthey want to stay stealthy. Based on this observation, we hypothesize thatwhile the model&rsquo;s predictions for paraphrased clean samples should remainstable, predictions for poisoned samples should revert to their true labelsupon the mutations applied to triggers during the paraphrasing process. Weemploy ChatGPT, a state-of-the-art large language model, as our paraphraser andformulate the trigger-removal task as a prompt engineering problem. We adoptfuzzing, a technique commonly used for unearthing software vulnerabilities, todiscover optimal paraphrase prompts that can effectively eliminate triggerswhile concurrently maintaining input semantics. Experiments on 4 types ofbackdoor attacks, including the subtle style backdoors, and 4 distinct datasetsdemonstrate that our approach surpasses baseline methods, including STRIP, RAP,and ONION, in precision and recall.</div></details><blockquote><p><strong><em>2023-10-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16263v1><strong>Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation</strong></a></p><p><em>Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have brought significant advancements to codegeneration, benefiting both novice and experienced developers. However, theirtraining using unsanitized data from open-source repositories, like GitHub,introduces the risk of inadvertently propagating security vulnerabilities. Toeffectively mitigate this concern, this paper presents a comprehensive studyfocused on evaluating and enhancing code LLMs from a software securityperspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded assupplemental material and will be made publicly available after publication.},a meticulously curated dataset targeting 21 critical vulnerability types.SecuCoGen comprises 180 samples and serves as the foundation for conductingexperiments on three crucial code-related tasks: code generation, code repairand vulnerability classification, with a strong emphasis on security. Ourexperimental results reveal that existing models often overlook securityconcerns during code generation, leading to the generation of vulnerable code.To address this, we propose effective approaches to mitigate the securityvulnerabilities and enhance the overall robustness of code generated by LLMs.Moreover, our study identifies weaknesses in existing models&rsquo; ability to repairvulnerable code, even when provided with vulnerability information.Additionally, certain vulnerability types pose challenges for the models,hindering their performance in vulnerability classification. Based on thesefindings, we believe our study will have a positive impact on the softwareengineering community, inspiring the development of improved methods fortraining and utilizing LLMs, thereby leading to safer and more trustworthymodel deployment.</div></details><p><a href=http://arxiv.org/abs/2307.08715v2><strong>MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots</strong></a></p><p><em>Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)services due to their exceptional proficiency in understanding and generatinghuman-like text. LLM chatbots, in particular, have seen widespread adoption,transforming human-machine interactions. However, these LLM chatbots aresusceptible to &ldquo;jailbreak&rdquo; attacks, where malicious users manipulate prompts toelicit inappropriate or sensitive responses, contravening service policies.Despite existing attempts to mitigate such threats, our research reveals asubstantial gap in our understanding of these vulnerabilities, largely due tothe undisclosed defensive measures implemented by LLM service providers. In this paper, we present Jailbreaker, a comprehensive framework that offersan in-depth understanding of jailbreak attacks and countermeasures. Our workmakes a dual contribution. First, we propose an innovative methodology inspiredby time-based SQL injection techniques to reverse-engineer the defensivestrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.This time-sensitive approach uncovers intricate details about these services&rsquo;defenses, facilitating a proof-of-concept attack that successfully bypassestheir mechanisms. Second, we introduce an automatic generation method forjailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential ofautomated jailbreak generation across various commercial LLM chatbots. Ourmethod achieves a promising average success rate of 21.58%, significantlyoutperforming the effectiveness of existing techniques. We have responsiblydisclosed our findings to the concerned service providers, underscoring theurgent need for more robust defenses. Jailbreaker thus marks a significant steptowards understanding and mitigating jailbreak threats in the realm of LLMchatbots.</div></details><blockquote><p><strong><em>2023-10-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.13469v3><strong>Ask Language Model to Clean Your Noisy Translation Data</strong></a></p><p><em>Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Transformer models have demonstrated remarkable performance in neural machinetranslation (NMT). However, their vulnerability to noisy input poses asignificant challenge in practical implementation, where generating cleanoutput from noisy input is crucial. The MTNT dataset is widely used as abenchmark for evaluating the robustness of NMT models against noisy input.Nevertheless, its utility is limited due to the presence of noise in both thesource and target sentences. To address this limitation, we focus on cleaningthe noise from the target sentences in MTNT, making it more suitable as abenchmark for noise evaluation. Leveraging the capabilities of large languagemodels (LLMs), we observe their impressive abilities in noise removal. Forexample, they can remove emojis while considering their semantic meaning.Additionally, we show that LLM can effectively rephrase slang, jargon, andprofanities. The resulting datasets, called C-MTNT, exhibit significantly lessnoise in the target sentences while preserving the semantic integrity of theoriginal sentences. Our human and GPT-4 evaluations also lead to a consistentconclusion that LLM performs well on this task. Lastly, experiments on C-MTNTshowcased its effectiveness in evaluating the robustness of NMT models,highlighting the potential of advanced language models for data cleaning andemphasizing C-MTNT as a valuable resource.</div></details><blockquote><p><strong><em>2023-10-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.04012v2><strong>CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models</strong></a></p><p><em>Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch√∂nherr, Mario Fritz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) for automatic code generation have achievedbreakthroughs in several programming tasks. Their advances in competition-levelprogramming problems have made them an essential pillar of AI-assisted pairprogramming, and tools such as GitHub Copilot have emerged as part of the dailyprogramming workflow used by millions of developers. The training data forthese models is usually collected from the Internet (e.g., from open-sourcerepositories) and is likely to contain faults and security vulnerabilities.This unsanitized training data can cause the language models to learn thesevulnerabilities and propagate them during the code generation procedure. Whilethese models have been extensively assessed for their ability to producefunctionally correct programs, there remains a lack of comprehensiveinvestigations and benchmarks addressing the security aspects of these models. In this work, we propose a method to systematically study the security issuesof code language models to assess their susceptibility to generating vulnerablecode. To this end, we introduce the first approach to automatically findgenerated code that contains vulnerabilities in black-box code generationmodels. To achieve this, we present an approach to approximate inversion of theblack-box code generation models based on few-shot prompting. We evaluate theeffectiveness of our approach by examining code language models in generatinghigh-risk security weaknesses. Furthermore, we establish a collection ofdiverse non-secure prompts for various vulnerability scenarios using ourmethod. This dataset forms a benchmark for evaluating and comparing thesecurity weaknesses in code language models.</div></details><p><a href=http://arxiv.org/abs/2310.04988v2><strong>The Troubling Emergence of Hallucination in Large Language Models &ndash; An Extensive Definition, Quantification, and Prescriptive Remediations</strong></a></p><p><em>Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The recent advancements in Large Language Models (LLMs) have garneredwidespread acclaim for their remarkable emerging capabilities. However, theissue of hallucination has parallelly emerged as a by-product, posingsignificant concerns. While some recent endeavors have been made to identifyand mitigate different types of hallucination, there has been a limitedemphasis on the nuanced categorization of hallucination and associatedmitigation methods. To address this gap, we offer a fine-grained discourse onprofiling hallucination based on its degree, orientation, and category, alongwith offering strategies for alleviation. As such, we define two overarchingorientations of hallucination: (i) factual mirage (FM) and (ii) silver lining(SL). To provide a more comprehensive understanding, both orientations arefurther sub-categorized into intrinsic and extrinsic, with three degrees ofseverity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulouslycategorize hallucination into six types: (i) acronym ambiguity, (ii) numericnuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), apublicly available dataset comprising of 75,000 samples generated using 15contemporary LLMs along with human annotations for the aforementionedcategories. Finally, to establish a method for quantifying and to offer acomparative spectrum that allows us to evaluate and rank LLMs based on theirvulnerability to producing hallucinations, we propose HallucinationVulnerability Index (HVI). We firmly believe that HVI holds significant valueas a tool for the wider NLP community, with the potential to serve as a rubricin AI-related policy-making. In conclusion, we propose two solution strategiesfor mitigating hallucinations.</div></details><blockquote><p><strong><em>2023-10-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.13394v1><strong>POSQA: Probe the World Models of LLMs with Size Comparisons</strong></a></p><p><em>Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Embodied language comprehension emphasizes that language understanding is notsolely a matter of mental processing in the brain but also involvesinteractions with the physical and social environment. With the explosivegrowth of Large Language Models (LLMs) and their already ubiquitous presence inour daily lives, it is becoming increasingly necessary to verify theirreal-world understanding. Inspired by cognitive theories, we propose POSQA: aPhysical Object Size Question Answering dataset with simple size comparisonquestions to examine the extremity and analyze the potential mechanisms of theembodied comprehension of the latest LLMs. We show that even the largest LLMs today perform poorly under the zero-shotsetting. We then push their limits with advanced prompting techniques andexternal knowledge augmentation. Furthermore, we investigate whether theirreal-world comprehension primarily derives from contextual information orinternal weights and analyse the impact of prompt formats and report bias ofdifferent objects. Our results show that real-world understanding that LLMsshaped from textual data can be vulnerable to deception and confusion by thesurface form of prompts, which makes it less aligned with human behaviours.</div></details><blockquote><p><strong><em>2023-10-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.09561v5><strong>Large Language Models are Better Reasoners with Self-Verification</strong></a></p><p><em>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, with the chain of thought (CoT) prompting, large language models(LLMs), e.g., GPT-3, have shown strong reasoning ability in several naturallanguage processing tasks such as arithmetic, commonsense, and logicalreasoning. However, LLMs with CoT require multi-step prompting and multi-tokenprediction, which is highly sensitive to individual mistakes and vulnerable toerror accumulation. The above issues make the LLMs need the ability to verifythe answers. In fact, after inferring conclusions in some thinking decisiontasks, people often check them by re-verifying steps to avoid some mistakes. Inthis paper, we propose and prove that LLMs also have similar self-verificationabilities. We take the conclusion obtained by CoT as one of the conditions forsolving the original problem. By performing a backward verification of theanswers that LLM deduced for itself, we can obtain interpretable answervalidation scores to select the candidate answer with the highest score.Experimental results demonstrate that the proposed method can improve thereasoning performance on various arithmetic, commonsense, and logical reasoningdatasets. Our code is publicly available at:https://github.com/WENGSYX/Self-Verification.</div></details><p><a href=http://arxiv.org/abs/2310.12815v1><strong>Prompt Injection Attacks and Defenses in LLM-Integrated Applications</strong></a></p><p><em>Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are increasingly deployed as the backend for avariety of real-world applications called LLM-Integrated Applications. Multiplerecent works showed that LLM-Integrated Applications are vulnerable to promptinjection attacks, in which an attacker injects malicious instruction/data intothe input of those applications such that they produce results as the attackerdesires. However, existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a general framework to formalize prompt injection attacks. Existingattacks, which are discussed in research papers and blog posts, are specialcases in our framework. Our framework enables us to design a new attack bycombining existing attacks. Moreover, we also propose a framework tosystematize defenses against prompt injection attacks. Using our frameworks, weconduct a systematic evaluation on prompt injection attacks and their defenseswith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research inthis field. Our code is available athttps://github.com/liu00222/Open-Prompt-Injection.</div></details><blockquote><p><strong><em>2023-10-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.01152v2><strong>Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives</strong></a></p><p><em>Sihao Hu, Tiansheng Huang, Fatih ƒ∞lhan, Selim Furkan Tekin, Ling Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper provides a systematic analysis of the opportunities, challenges,and potential solutions of harnessing Large Language Models (LLMs) such asGPT-4 to dig out vulnerabilities within smart contracts based on our ongoingresearch. For the task of smart contract vulnerability detection, achievingpractical usability hinges on identifying as many true vulnerabilities aspossible while minimizing the number of false positives. Nonetheless, ourempirical study reveals contradictory yet interesting findings: generating moreanswers with higher randomness largely boosts the likelihood of producing acorrect answer but inevitably leads to a higher number of false positives. Tomitigate this tension, we propose an adversarial framework dubbed GPTLens thatbreaks the conventional one-stage detection into two synergistic stages $-$generation and discrimination, for progressive detection and refinement,wherein the LLM plays dual roles, i.e., auditor and critic, respectively. Thegoal of auditor is to yield a broad spectrum of vulnerabilities with the hopeof encompassing the correct answer, whereas the goal of critic that evaluatesthe validity of identified vulnerabilities is to minimize the number of falsepositives. Experimental results and illustrative examples demonstrate thatauditor and critic work together harmoniously to yield pronounced improvementsover the conventional one-stage detection. GPTLens is intuitive, strategic, andentirely LLM-driven without relying on specialist expertise in smart contracts,showcasing its methodical generality and potential to detect a broad spectrumof vulnerabilities. Our code is available at:https://github.com/git-disl/GPTLens.</div></details><p><a href=http://arxiv.org/abs/2310.10383v1><strong>Privacy in Large Language Models: Attacks, Defenses and Future Directions</strong></a></p><p><em>Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary&rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.</div></details><p><a href=http://arxiv.org/abs/2310.10844v1><strong>Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks</strong></a></p><p><em>Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are swiftly advancing in architecture andcapability, and as they integrate more deeply into complex systems, the urgencyto scrutinize their security properties grows. This paper surveys research inthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfieldof trustworthy ML, combining the perspectives of Natural Language Processingand Security. Prior work has shown that even safety-aligned LLMs (viainstruction tuning and reinforcement learning through human feedback) can besusceptible to adversarial attacks, which exploit weaknesses and mislead AIsystems, as evidenced by the prevalence of `jailbreak&rsquo; attacks on models likeChatGPT and Bard. In this survey, we first provide an overview of largelanguage models, describe their safety alignment, and categorize existingresearch based on various learning structures: textual-only attacks,multi-modal attacks, and additional attack methods specifically targetingcomplex systems, such as federated learning or multi-agent systems. We alsooffer comprehensive remarks on works that focus on the fundamental sources ofvulnerabilities and potential defenses. To make this field more accessible tonewcomers, we present a systematic review of existing works, a structuredtypology of adversarial attack concepts, and additional resources, includingslides for presentations on related topics at the 62nd Annual Meeting of theAssociation for Computational Linguistics (ACL'24).</div></details><p><a href=http://arxiv.org/abs/2310.10077v1><strong>Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks</strong></a></p><p><em>Shuyu Jiang, Xingshu Chen, Rui Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, Large language models (LLMs) with powerful general capabilitieshave been increasingly integrated into various Web applications, whileundergoing alignment training to ensure that the generated content aligns withuser intent and ethics. Unfortunately, they remain the risk of generatingharmful content like hate speech and criminal activities in practicalapplications. Current approaches primarily rely on detecting, collecting, andtraining against harmful prompts to prevent such risks. However, they typicallyfocused on the &ldquo;superficial&rdquo; harmful prompts with a solitary intent, ignoringcomposite attack instructions with multiple intentions that can easily elicitharmful content in real-world scenarios. In this paper, we introduce aninnovative technique for obfuscating harmful instructions: CompositionalInstruction Attacks (CIA), which refers to attacking by combination andencapsulation of multiple instructions. CIA hides harmful prompts withininstructions of harmless intentions, making it impossible for the model toidentify underlying malicious intentions. Furthermore, we implement twotransformation methods, known as T-CIA and W-CIA, to automatically disguiseharmful instructions as talking or writing tasks, making them appear harmlessto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safetyassessment datasets and two harmful prompt datasets. It achieves an attacksuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.Our approach reveals the vulnerability of LLMs to such compositionalinstruction attacks that harbor underlying harmful intentions, contributingsignificantly to LLM security development. Warning: this paper may containoffensive or upsetting content!</div></details><blockquote><p><strong><em>2023-10-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.09810v1><strong>ChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?</strong></a></p><p><em>Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4)exhibited remarkable advancement in a range of software engineering tasksassociated with source code such as code review and code generation. In thispaper, we undertake a comprehensive study by instructing ChatGPT for fourprevalent vulnerability tasks: function and line-level vulnerabilityprediction, vulnerability classification, severity estimation, andvulnerability repair. We compare ChatGPT with state-of-the-art language modelsdesigned for software vulnerability purposes. Through an empirical assessmentemploying extensive real-world datasets featuring over 190,000 C/C++ functions,we found that ChatGPT achieves limited performance, trailing behind otherlanguage models in vulnerability contexts by a significant margin. Theexperimental outcomes highlight the challenging nature of vulnerabilityprediction tasks, requiring domain-specific expertise. Despite ChatGPT&rsquo;ssubstantial model scale, exceeding that of source code-pre-trained languagemodels (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuningremains imperative for ChatGPT to generalize for vulnerability predictiontasks. We publish the studied dataset, experimental prompts for ChatGPT, andexperimental results at <a href=https://github.com/awsm-research/ChatGPT4Vul>https://github.com/awsm-research/ChatGPT4Vul</a>.</div></details><p><a href=http://arxiv.org/abs/2310.09820v1><strong>Assessing the Reliability of Large Language Model Knowledge</strong></a></p><p><em>Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have been treated as knowledge bases due totheir strong performance in knowledge probing tasks. LLMs are typicallyevaluated using accuracy, yet this metric does not capture the vulnerability ofLLMs to hallucination-inducing factors like prompt and context variability. Howdo we evaluate the capabilities of LLMs to consistently produce factuallycorrect answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe(MONITOR), a novel metric designed to directly measure LLMs&rsquo; factualreliability. MONITOR computes the distance between the probabilitydistributions of a valid output and its counterparts produced by the same LLMprobing the same fact using different styles of prompts andcontexts.Experiments on a comprehensive range of 12 LLMs demonstrate theeffectiveness of MONITOR in evaluating the factual reliability of LLMs whilemaintaining a low computational overhead. In addition, we release the FKTC(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in totalto foster research along this line (<a href=https://github.com/Vicky-Wil/MONITOR%29>https://github.com/Vicky-Wil/MONITOR)</a>.</div></details><blockquote><p><strong><em>2023-10-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.11751v2><strong>How Robust is Google&rsquo;s Bard to Adversarial Image Attacks?</strong></a></p><p><em>Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multimodal Large Language Models (MLLMs) that integrate text and othermodalities (especially vision) have achieved unprecedented performance invarious multimodal tasks. However, due to the unsolved adversarial robustnessproblem of vision models, MLLMs can have more severe safety and security risksby introducing the vision inputs. In this work, we study the adversarialrobustness of Google&rsquo;s Bard, a competitive chatbot to ChatGPT that released itsmultimodal capability recently, to better understand the vulnerabilities ofcommercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,the generated adversarial examples can mislead Bard to output wrong imagedescriptions with a 22% success rate based solely on the transferability. Weshow that the adversarial examples can also attack other MLLMs, e.g., a 26%attack success rate against Bing Chat and a 86% attack success rate againstERNIE bot. Moreover, we identify two defense mechanisms of Bard, including facedetection and toxicity detection of images. We design corresponding attacks toevade these defenses, demonstrating that the current defenses of Bard are alsovulnerable. We hope this work can deepen our understanding on the robustness ofMLLMs and facilitate future research on defenses. Our code is available athttps://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate itsrobustness under the same set of adversarial examples, achieving a 45% attacksuccess rate.</div></details><p><a href=http://arxiv.org/abs/2308.13904v2><strong>LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors</strong></a></p><p><em>Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prompt-tuning has emerged as an attractive paradigm for deploying large-scalelanguage models due to its strong downstream task performance and efficientmultitask serving ability. Despite its wide adoption, we empirically show thatprompt-tuning is vulnerable to downstream task-agnostic backdoors, which residein the pretrained models and can affect arbitrary downstream tasks. Thestate-of-the-art backdoor detection approaches cannot defend againsttask-agnostic backdoors since they hardly converge in reversing the backdoortriggers. To address this issue, we propose LMSanitator, a novel approach fordetecting and removing task-agnostic backdoors on Transformer models. Insteadof directly inverting the triggers, LMSanitator aims to invert the predefinedattack vectors (pretrained models&rsquo; output when the input is embedded withtriggers) of the task-agnostic backdoors, which achieves much betterconvergence performance and backdoor detection accuracy. LMSanitator furtherleverages prompt-tuning&rsquo;s property of freezing the pretrained model to performaccurate and fast output monitoring and input purging during the inferencephase. Extensive experiments on multiple language models and NLP tasksillustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves92.8% backdoor detection accuracy on 960 models and decreases the attacksuccess rate to less than 1% in most scenarios.</div></details><blockquote><p><strong><em>2023-10-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.08419v2><strong>Jailbreaking Black Box Large Language Models in Twenty Queries</strong></a></p><p><em>Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: There is growing interest in ensuring that large language models (LLMs) alignwith human values. However, the alignment of such models is vulnerable toadversarial jailbreaks, which coax LLMs into overriding their safetyguardrails. The identification of these vulnerabilities is thereforeinstrumental in understanding inherent weaknesses and preventing future misuse.To this end, we propose Prompt Automatic Iterative Refinement (PAIR), analgorithm that generates semantic jailbreaks with only black-box access to anLLM. PAIR &ndash; which is inspired by social engineering attacks &ndash; uses anattacker LLM to automatically generate jailbreaks for a separate targeted LLMwithout human intervention. In this way, the attacker LLM iteratively queriesthe target LLM to update and refine a candidate jailbreak. Empirically, PAIRoften requires fewer than twenty queries to produce a jailbreak, which isorders of magnitude more efficient than existing algorithms. PAIR also achievescompetitive jailbreaking success rates and transferability on open andclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.</div></details><blockquote><p><strong><em>2023-10-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.08275v1><strong>Harnessing the Power of LLM to Support Binary Taint Analysis</strong></a></p><p><em>Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhi Li, Limin Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper proposes LATTE, the first static binary taint analysis that ispowered by a large language model (LLM). LATTE is superior to the state of theart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fullyautomated while prior static binary taint analyzers need rely on humanexpertise to manually customize taint propagation rules and vulnerabilityinspection rules. Second, LATTE is significantly effective in vulnerabilitydetection, demonstrated by our comprehensive evaluations. For example, LATTEhas found 37 new bugs in real-world firmware which the baselines failed tofind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incursremarkably low engineering cost, making it a cost-efficient and scalablesolution for security researchers and practitioners. We strongly believe thatLATTE opens up a new direction to harness the recent advance in LLMs to improvevulnerability analysis for binary programs.</div></details><p><a href=http://arxiv.org/abs/2310.08256v1><strong>Impact of Co-occurrence on Factual Knowledge of Large Language Models</strong></a></p><p><em>Cheongwoong Kang, Jaesik Choi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) often make factually incorrect responses despitetheir success in various applications. In this paper, we hypothesize thatrelying heavily on simple co-occurrence statistics of the pre-training corporais one of the main factors that cause factual errors. Our results reveal thatLLMs are vulnerable to the co-occurrence bias, defined as preferring frequentlyco-occurred words over the correct answer. Consequently, LLMs struggle torecall facts whose subject and object rarely co-occur in the pre-trainingdataset although they are seen during finetuning. We show that co-occurrencebias remains despite scaling up model sizes or finetuning. Therefore, wesuggest finetuning on a debiased dataset to mitigate the bias by filtering outbiased samples whose subject-object co-occurrence count is high. Althoughdebiased finetuning allows LLMs to memorize rare facts in the training set, itis not effective in recalling rare facts unseen during finetuning. Furtherresearch in mitigation will help build reliable language models by preventingpotential errors. The code is available at\url{https://github.com/CheongWoong/impact_of_cooccurrence}.</div></details><blockquote><p><strong><em>2023-10-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.07676v1><strong>Composite Backdoor Attacks Against Large Language Models</strong></a></p><p><em>Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have demonstrated superior performance comparedto previous methods on various tasks, and often serve as the foundation modelsfor many researches and services. However, the untrustworthy third-party LLMsmay covertly introduce vulnerabilities for downstream tasks. In this paper, weexplore the vulnerability of LLMs through the lens of backdoor attacks.Different from existing backdoor attacks against LLMs, ours scatters multipletrigger keys in different prompt components. Such a Composite Backdoor Attack(CBA) is shown to be stealthier than implanting the same multiple trigger keysin only a single component. CBA ensures that the backdoor is activated onlywhen all trigger keys appear. Our experiments demonstrate that CBA is effectivein both natural language processing (NLP) and multimodal tasks. For instance,with $3%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,our attack achieves a $100%$ Attack Success Rate (ASR) with a False TriggeredRate (FTR) below $2.06%$ and negligible model accuracy degradation. The uniquecharacteristics of our CBA can be tailored for various practical scenarios,e.g., targeting specific user groups. Our work highlights the necessity ofincreased security research on the trustworthiness of foundation LLMs.</div></details><blockquote><p><strong><em>2023-10-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.00322v2><strong>Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models</strong></a></p><p><em>Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deployable Large Language Models (LLMs) must conform to the criterion ofhelpfulness and harmlessness, thereby achieving consistency between LLMsoutputs and human values. Red-teaming techniques constitute a critical waytowards this criterion. Existing work rely solely on manual red team designsand heuristic adversarial prompts for vulnerability detection and optimization.These approaches lack rigorous mathematical formulation, thus limiting theexploration of diverse attack strategy within quantifiable measure andoptimization of LLMs under convergence guarantees. In this paper, we presentRed-teaming Game (RTG), a general game-theoretic framework without manualannotation. RTG is designed for analyzing the multi-turn attack and defenseinteractions between Red-team language Models (RLMs) and Blue-team LanguageModel (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) withdiversity measure of the semantic space. GRTS is an automated red teamingtechnique to solve RTG towards Nash equilibrium through meta-game analysis,which corresponds to the theoretically guaranteed optimization direction ofboth RLMs and BLM. Empirical results in multi-turn attacks with RLMs show thatGRTS autonomously discovered diverse attack strategies and effectively improvedsecurity of LLMs, outperforming existing heuristic red-team designs. Overall,RTG has established a foundational framework for red teaming tasks andconstructed a new scalable oversight technique for alignment.</div></details><p><a href=http://arxiv.org/abs/2310.04373v2><strong>Confronting Reward Model Overoptimization with Constrained RLHF</strong></a></p><p><em>Ted Moskovitz, Aaditya K. Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen McAleer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models are typically aligned with human preferences byoptimizing $\textit{reward models}$ (RMs) fitted to human feedback. However,human preferences are multi-faceted, and it is increasingly common to derivereward from a composition of simpler reward models which each capture adifferent aspect of language quality. This itself presents a challenge, as itis difficult to appropriately weight these component RMs when combining them.Compounding this difficulty, because any RM is only a proxy for humanevaluation, this process is vulnerable to $\textit{overoptimization}$, whereinpast a certain point, accumulating higher reward is associated with worse humanratings. In this paper, we perform, to our knowledge, the first study onoveroptimization in composite RMs, showing that correlation between componentRMs has a significant effect on the locations of these points. We thenintroduce an approach to solve this issue using constrained reinforcementlearning as a means of preventing the agent from exceeding each RM&rsquo;s thresholdof usefulness. Our method addresses the problem of weighting component RMs bylearning dynamic weights, naturally expressed by Lagrange multipliers. As aresult, each RM stays within the range at which it is an effective proxy,improving evaluation performance. Finally, we introduce an adaptive methodusing gradient-free optimization to identify and optimize towards these pointsduring a single run.</div></details><p><a href=http://arxiv.org/abs/2305.13257v3><strong>Watermarking Classification Dataset for Copyright Protection</strong></a></p><p><em>Yixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals&rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.</div></details><p><a href=http://arxiv.org/abs/2310.06936v1><strong>LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing</strong></a></p><p><em>Stephen Moskal, Sam Laney, Erik Hemberg, Una-May O&rsquo;Reilly</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we explore the potential of Large Language Models (LLMs) toreason about threats, generate information about tools, and automate cybercampaigns. We begin with a manual exploration of LLMs in supporting specificthreat-related actions and decisions. We proceed by automating the decisionprocess in a cyber campaign. We present prompt engineering approaches for aplan-act-report loop for one action of a threat campaign and and a promptchaining design that directs the sequential decision process of a multi-actioncampaign. We assess the extent of LLM&rsquo;s cyber-specific knowledge w.r.t theshort campaign we demonstrate and provide insights into prompt design foreliciting actionable responses. We discuss the potential impact of LLMs on thethreat landscape and the ethical considerations of using LLMs for acceleratingthreat actor capabilities. We report a promising, yet concerning, applicationof generative AI to cyber threats. However, the LLM&rsquo;s capabilities to deal withmore complex networks, sophisticated vulnerabilities, and the sensitivity ofprompts are open questions. This research should spur deliberations over theinevitable advancements in LLM-supported cyber adversarial landscape.</div></details><p><a href=http://arxiv.org/abs/2310.06257v1><strong>SCAR: Power Side-Channel Analysis at RTL-Level</strong></a></p><p><em>Amisha Srivastava, Sanjay Das, Navnil Choudhury, Rafail Psiakis, Pedro Henrique Silva, Debjit Pal, Kanad Basu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Power side-channel attacks exploit the dynamic power consumption ofcryptographic operations to leak sensitive information of encryption hardware.Therefore, it is necessary to conduct power side-channel analysis for assessingthe susceptibility of cryptographic systems and mitigating potential risks.Existing power side-channel analysis primarily focuses on post-siliconimplementations, which are inflexible in addressing design flaws, leading tocostly and time-consuming post-fabrication design re-spins. Hence, pre-siliconpower side-channel analysis is required for early detection of vulnerabilitiesto improve design robustness. In this paper, we introduce SCAR, a novelpre-silicon power side-channel analysis framework based on Graph NeuralNetworks (GNN). SCAR converts register-transfer level (RTL) designs ofencryption hardware into control-data flow graphs and use that to detect thedesign modules susceptible to side-channel leakage. Furthermore, we incorporatea deep learning-based explainer in SCAR to generate quantifiable andhuman-accessible explanation of our detection and localization decisions. Wehave also developed a fortification component as a part of SCAR that useslarge-language models (LLM) to automatically generate and insert additionaldesign code at the localized zone to shore up the side-channel leakage. Whenevaluated on popular encryption algorithms like AES, RSA, and PRESENT, andpostquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.Additionally, through explainability analysis, SCAR reduces features for GNNmodel training by 57% while maintaining comparable accuracy. We believe thatSCAR will transform the security-critical hardware design cycle, resulting infaster design closure at a reduced design cost.</div></details><blockquote><p><strong><em>2023-10-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.05338v1><strong>Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models</strong></a></p><p><em>Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Object hallucination poses a significant challenge in vision-language (VL)models, often leading to the generation of nonsensical or unfaithful responseswith non-existent objects. However, the absence of a general measurement forevaluating object hallucination in VL models has hindered our understanding andability to mitigate this issue. In this work, we present NOPE (Negative ObjectPresence Evaluation), a novel benchmark designed to assess object hallucinationin VL models through visual question answering (VQA). We propose acost-effective and scalable approach utilizing large language models togenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.We extensively investigate the performance of 10 state-of-the-art VL models indiscerning the non-existence of objects in visual questions, where the groundtruth answers are denoted as NegP (e.g., &ldquo;none&rdquo;). Additionally, we evaluatetheir standard performance on visual questions on 9 other VQA datasets. Throughour experiments, we demonstrate that no VL model is immune to the vulnerabilityof object hallucination, as all models achieve accuracy below 10% on NegP.Furthermore, we uncover that lexically diverse visual questions, question typeswith large scopes, and scene-relevant objects capitalize the risk of objecthallucination in VL models.</div></details><blockquote><p><strong><em>2023-10-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.02926v2><strong>Demystifying RCE Vulnerabilities in LLM-Integrated Apps</strong></a></p><p><em>Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, Large Language Models (LLMs) have demonstrated remarkablepotential across various downstream tasks. LLM-integrated frameworks, whichserve as the essential infrastructure, have given rise to many LLM-integratedweb apps. However, some of these frameworks suffer from Remote Code Execution(RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps&rsquo;servers remotely via prompt injections. Despite the severity of thesevulnerabilities, no existing work has been conducted for a systematicinvestigation of them. This leaves a great challenge on how to detectvulnerabilities in frameworks as well as LLM-integrated apps in real-worldscenarios. To fill this gap, we present two novel strategies, including 1) astatic analysis-based tool called LLMSmith to scan the source code of theframework to detect potential RCE vulnerabilities and 2) a prompt-basedautomated testing approach to verify the vulnerability in LLM-integrated webapps. We discovered 13 vulnerabilities in 6 frameworks, including 12 RCEvulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them areconfirmed by the framework developers, resulting in the assignment of 7 CVEIDs. After testing 51 apps, we found vulnerabilities in 17 apps, 16 of whichare vulnerable to RCE and 1 to SQL injection. We responsibly reported all 17issues to the corresponding developers and received acknowledgments.Furthermore, we amplify the attack impact beyond achieving RCE by allowingattackers to exploit other app users (e.g. app responses hijacking, user APIkey leakage) without direct interaction between the attacker and the victim.Lastly, we propose some mitigating strategies for improving the securityawareness of both framework and app developers, helping them to mitigate theserisks effectively.</div></details><p><a href=http://arxiv.org/abs/2310.05157v1><strong>MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models</strong></a></p><p><em>Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have shown nearly saturated performance on manynatural language processing (NLP) tasks. As a result, it is natural for peopleto believe that LLMs have also mastered abilities such as time understandingand reasoning. However, research on the temporal sensitivity of LLMs has beeninsufficiently emphasized. To fill this gap, this paper constructs MultipleSensitive Factors Time QA (MenatQA), which encompasses three temporal factors(scope factor, order factor, counterfactual factor) with total 2,853 samplesfor evaluating the time comprehension and reasoning abilities of LLMs. Thispaper tests current mainstream LLMs with different parameter sizes, rangingfrom billions to hundreds of billions. The results show most LLMs fall behindsmaller temporal reasoning models with different degree on these factors. Inspecific, LLMs show a significant vulnerability to temporal biases and dependheavily on the temporal information provided in questions. Furthermore, thispaper undertakes a preliminary investigation into potential improvementstrategies by devising specific prompts and leveraging external tools. Theseapproaches serve as valuable baselines or references for future researchendeavors.</div></details><blockquote><p><strong><em>2023-10-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.09826v2><strong>Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding</strong></a></p><p><em>Andr√© Storhaug, Jingyue Li, Tianyuan Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Auto-completing code enables developers to speed up coding significantly.Recent advances in transformer-based large language model (LLM) technologieshave been applied to code synthesis. However, studies show that many of suchsynthesized codes contain vulnerabilities. We propose a novelvulnerability-constrained decoding approach to reduce the amount of vulnerablecode generated by such models. Using a small dataset of labeled vulnerablelines of code, we fine-tune an LLM to include vulnerability labels whengenerating code, acting as an embedded classifier. Then, during decoding, wedeny the model to generate these labels to avoid generating vulnerable code. Toevaluate the method, we chose to automatically complete Ethereum Blockchainsmart contracts (SCs) as the case study due to the strict requirements of SCsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuningtook more than one week using ten GPUs. The results showed that our fine-tunedmodel could synthesize SCs with an average BLEU (BiLingual EvaluationUnderstudy) score of 0.557. However, many codes in the auto-completed SCs werevulnerable. Using the code before the vulnerable line of 176 SCs containingdifferent types of vulnerabilities to auto-complete the code, we found thatmore than 70% of the auto-completed codes were insecure. Thus, we furtherfine-tuned the model on other 941 vulnerable SCs containing the same types ofvulnerabilities and applied vulnerability-constrained decoding. The fine-tuningtook only one hour with four GPUs. We then auto-completed the 176 SCs again andfound that our approach could identify 62% of the code to be generated asvulnerable and avoid generating 67% of them, indicating the approach couldefficiently and effectively avoid vulnerabilities in the auto-completed code.</div></details><blockquote><p><strong><em>2023-10-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.05862v1><strong>Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks</strong></a></p><p><em>Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Contrastive Language-Image Pre-training (CLIP) on large image-captiondatasets has achieved remarkable success in zero-shot classification andenabled transferability to new domains. However, CLIP is extremely morevulnerable to targeted data poisoning and backdoor attacks, compared tosupervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIPpre-training data is enough to make targeted data poisoning attacks successful.This is four orders of magnitude smaller than what is required to poisonsupervised models. Despite this vulnerability, existing methods are verylimited in defending CLIP models during pre-training. In this work, we proposea strong defense, SAFECLIP, to safely pre-train CLIP against targeted datapoisoning and backdoor attacks. SAFECLIP warms up the model by applyingunimodal contrastive learning (CL) on image and text modalities separately.Then, it carefully divides the data into safe and risky subsets. SAFECLIPtrains on the risky data by applying unimodal CL to image and text modalitiesseparately, and trains on the safe data using the CLIP loss. By graduallyincreasing the size of the safe subset during the training, SAFECLIPeffectively breaks targeted data poisoning and backdoor attacks without harmingthe CLIP performance. Our extensive experiments show that SAFECLIP decrease theattack success rate of targeted data poisoning attacks from 93.75% to 0% andthat of the backdoor attacks from 100% to 0%, without harming the CLIPperformance on various datasets.</div></details><p><a href=http://arxiv.org/abs/2310.03614v1><strong>Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally</strong></a></p><p><em>Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep Neural Networks (DNNs) have been the driving force behind many of therecent advances in machine learning. However, research has shown that DNNs arevulnerable to adversarial examples &ndash; input samples that have been perturbed toforce DNN-based models to make errors. As a result, Adversarial MachineLearning (AdvML) has gained a lot of attention, and researchers haveinvestigated these vulnerabilities in various settings and modalities. Inaddition, DNNs have also been found to incorporate embedded bias and oftenproduce unexplainable predictions, which can result in anti-social AIapplications. The emergence of new AI technologies that leverage Large LanguageModels (LLMs), such as ChatGPT and GPT-4, increases the risk of producinganti-social applications at scale. AdvML for Social Good (AdvML4G) is anemerging field that repurposes the AdvML bug to invent pro-social applications.Regulators, practitioners, and researchers should collaborate to encourage thedevelopment of pro-social applications and hinder the development ofanti-social ones. In this work, we provide the first comprehensive review ofthe emerging field of AdvML4G. This paper encompasses a taxonomy thathighlights the emergence of AdvML4G, a discussion of the differences andsimilarities between AdvML4G and AdvML, a taxonomy covering social good-relatedconcepts and aspects, an exploration of the motivations behind the emergence ofAdvML4G at the intersection of ML4G and AdvML, and an extensive summary of theworks that utilize AdvML4G as an auxiliary tool for innovating pro-socialapplications. Finally, we elaborate upon various challenges and open researchissues that require significant attention from the research community.</div></details><p><a href=http://arxiv.org/abs/2304.08979v2><strong>In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT</strong></a></p><p><em>Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The way users acquire information is undergoing a paradigm shift with theadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrievesknowledge from the model itself and generates answers for users. ChatGPT&rsquo;simpressive question-answering (QA) capability has attracted more than 100million users within a short period of time but has also raised concernsregarding its reliability. In this paper, we perform the first large-scalemeasurement of ChatGPT&rsquo;s reliability in the generic QA scenario with acarefully curated set of 5,695 questions across ten datasets and eight domains.We find that ChatGPT&rsquo;s reliability varies across different domains, especiallyunderperforming in law and science questions. We also demonstrate that systemroles, originally designed by OpenAI to allow users to steer ChatGPT&rsquo;sbehavior, can impact ChatGPT&rsquo;s reliability in an imperceptible way. We furthershow that ChatGPT is vulnerable to adversarial examples, and even a singlecharacter change can negatively affect its reliability in certain cases. Webelieve that our study provides valuable insights into ChatGPT&rsquo;s reliabilityand underscores the need for strengthening the reliability and security oflarge language models (LLMs).</div></details><blockquote><p><strong><em>2023-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.02530v1><strong>Identifying Vulnerability Patches by Comprehending Code Commits with Comprehensive Change Contexts</strong></a></p><p><em>Tianyu Chen, Lin Li, Taotao Qian, Zeyu Wang, Guangtai Liang, Ding Li, Qianxiang Wang, Tao Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To help application developers apply vulnerability patches timely, securityresearchers maintain vulnerability databases such as National VulnerabilityDatabase (NVD). By directly monitoring NVD with the name of each used library,application developers can be aware of vulnerabilities and their patches. Giventhat the monitoring results of vulnerability patches are unreliable due topatch incompleteness of NVD, existing approaches employ deep-learning (DL)models to identify additional vulnerability patches by determining whether acode commit fixes a vulnerability. However, these approaches suffer from lowaccuracy due to not considering code commits&rsquo; comprehensive contexts such ascontrol/data-flow contexts or method-invocation contexts. To improve accuracy,we design CompVPD, the first approach to identify vulnerability patches byfine-tuning a large language model (LLM) named StarCoder to comprehend codecommits with comprehensive contexts. Considering that including comprehensivecontexts needs to balance the context size and the training costs of LLM,CompVPD includes our two novel algorithms to generate comprehensive contextswithin the given window size by removing irrelevant components (i.e., files,methods, and statements) and adaptively expanding each context. We empiricallycompare CompVPD with four state-of-the-art/practice (SOTA) approaches thatidentify vulnerability patches. The results show that CompVPD improves the AUCscore by 11% and the F1 score by 30% when compared with the best scores of theSOTA approaches. Additionally, CompVPD provides high value to security practiceby helping identify 20 vulnerability patches and 18 fixes of high-risk bugsfrom 2,500 recent code commits of five highly popular open-source projects.</div></details><blockquote><p><strong><em>2023-10-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.02417v1><strong>Jailbreaker in Jail: Moving Target Defense for Large Language Models</strong></a></p><p><em>Bocheng Chen, Advait Paliwal, Qiben Yan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), known for their capability in understanding andfollowing instructions, are vulnerable to adversarial attacks. Researchers havefound that current commercial LLMs either fail to be &ldquo;harmless&rdquo; by presentingunethical answers, or fail to be &ldquo;helpful&rdquo; by refusing to offer meaningfulanswers when faced with adversarial queries. To strike a balance between beinghelpful and harmless, we design a moving target defense (MTD) enhanced LLMsystem. The system aims to deliver non-toxic answers that align with outputsfrom multiple model candidates, making them more robust against adversarialattacks. We design a query and output analysis model to filter out unsafe ornon-responsive answers. %to achieve the two objectives of randomly selectingoutputs from different LLMs. We evaluate over 8 most recent chatbot models withstate-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces theattack success rate from 37.5% to 0%. Meanwhile, it decreases the responserefusal rate from 50% to 0%.</div></details><p><a href=http://arxiv.org/abs/2310.01726v1><strong>Large Language Models for Test-Free Fault Localization</strong></a></p><p><em>Aidan Z. H. Yang, Ruben Martins, Claire Le Goues, Vincent J. Hellendoorn</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fault Localization (FL) aims to automatically localize buggy lines of code, akey first step in many manual and automatic debugging tasks. Previous FLtechniques assume the provision of input tests, and often require extensiveprogram analysis, program instrumentation, or data preprocessing. Prior work ondeep learning for APR struggles to learn from small datasets and produceslimited results on real-world programs. Inspired by the ability of largelanguage models (LLMs) of code to adapt to new tasks based on very fewexamples, we investigate the applicability of LLMs to line level faultlocalization. Specifically, we propose to overcome the left-to-right nature ofLLMs by fine-tuning a small set of bidirectional adapter layers on top of therepresentations learned by LLMs to produce LLMAO, the first language modelbased fault localization approach that locates buggy lines of code without anytest coverage information. We fine-tune LLMs with 350 million, 6 billion, and16 billion parameters on small, manually curated corpora of buggy programs suchas the Defects4J corpus. We observe that our technique achieves substantiallymore confidence in fault localization when built on the larger models, with buglocalization performance scaling consistently with the LLM size. Our empiricalevaluation shows that LLMAO improves the Top-1 results over thestate-of-the-art machine learning fault localization (MLFL) baselines by2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FLtechnique trained using a language model architecture that can detect securityvulnerabilities down to the code line level.</div></details><blockquote><p><strong><em>2023-10-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.01651v1><strong>Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations</strong></a></p><p><em>Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language and vision-language models are rapidly being deployed inpractice thanks to their impressive capabilities in instruction following,in-context learning, and so on. This raises an urgent need to carefully analysetheir robustness so that stakeholders can understand if and when such modelsare trustworthy enough to be relied upon in any given application. In thispaper, we highlight a specific vulnerability in popular models, namelypermutation sensitivity in multiple-choice question answering (MCQA).Specifically, we show empirically that popular models are vulnerable toadversarial permutation in answer sets for multiple-choice prompting, which issurprising as models should ideally be as invariant to prompt permutation ashumans are. These vulnerabilities persist across various model sizes, and existin very recent language and vision-language models. Code is available at\url{https://github.com/ys-zong/FoolyourVLLMs}.</div></details><blockquote><p><strong><em>2023-10-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.08108v3><strong>Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection</strong></a></p><p><em>Benjamin Steenhoek, Hongyang Gao, Wei Le</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning-based vulnerability detection has shown great performance and,in some studies, outperformed static analysis tools. However, thehighest-performing approaches use token-based transformer models, which are notthe most efficient to capture code semantics required for vulnerabilitydetection. Classical program analysis techniques such as dataflow analysis candetect many types of bugs based on their root causes. In this paper, we proposeto combine such causal-based vulnerability detection algorithms with deeplearning, aiming to achieve more efficient and effective vulnerabilitydetection. Specifically, we designed DeepDFA, a dataflow analysis-inspiredgraph learning framework and an embedding technique that enables graph learningto simulate dataflow computation. We show that DeepDFA is both performant andefficient. DeepDFA outperformed all non-transformer baselines. It was trainedin 9 minutes, 75x faster than the highest-performing baseline model. When usingonly 50+ vulnerable and several hundreds of total examples as training data,the model retained the same performance as 100% of the dataset. DeepDFA alsogeneralized to real-world vulnerabilities in DbgBench; it detected 8.7 out of17 vulnerabilities on average across folds and was able to distinguish betweenpatched and buggy versions, while the highest-performing baseline models didnot detect any vulnerabilities. By combining DeepDFA with a large languagemodel, we surpassed the state-of-the-art vulnerability detection performance onthe Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Ourreplication package is located at <a href=https://doi.org/10.6084/m9.figshare.21225413>https://doi.org/10.6084/m9.figshare.21225413</a> .</div></details><p><a href=http://arxiv.org/abs/2310.00654v1><strong>Streamlining Attack Tree Generation: A Fragment-Based Approach</strong></a></p><p><em>Irdin Pekaric, Markus Frick, Jubril Gbolahan Adigun, Raffaela Groner, Thomas Witte, Alexander Raschke, Michael Felderer, Matthias Tichy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Attack graphs are a tool for analyzing security vulnerabilities that capturedifferent and prospective attacks on a system. As a threat modeling tool, itshows possible paths that an attacker can exploit to achieve a particular goal.However, due to the large number of vulnerabilities that are published on adaily basis, they have the potential to rapidly expand in size. Consequently,this necessitates a significant amount of resources to generate attack graphs.In addition, generating composited attack models for complex systems such asself-adaptive or AI is very difficult due to their nature to continuouslychange. In this paper, we present a novel fragment-based attack graphgeneration approach that utilizes information from publicly availableinformation security databases. Furthermore, we also propose a domain-specificlanguage for attack modeling, which we employ in the proposed attack graphgeneration approach. Finally, we present a demonstrator example showcasing theattack generator&rsquo;s capability to replicate a verified attack chain, aspreviously confirmed by security experts.</div></details><blockquote><p><strong><em>2023-09-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.17007v1><strong>Medical Foundation Models are Susceptible to Targeted Misinformation Attacks</strong></a></p><p><em>Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian F√∂rsch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have broad medical knowledge and can reasonabout medical information across many domains, holding promising potential fordiverse medical applications in the near future. In this study, we demonstratea concerning vulnerability of LLMs in medicine. Through targeted manipulationof just 1.1% of the model&rsquo;s weights, we can deliberately inject an incorrectbiomedical fact. The erroneous information is then propagated in the model&rsquo;soutput, whilst its performance on other biomedical tasks remains intact. Wevalidate our findings in a set of 1,038 incorrect biomedical facts. Thispeculiar susceptibility raises serious security and trustworthiness concernsfor the application of LLMs in healthcare settings. It accentuates the need forrobust protective measures, thorough verification mechanisms, and stringentmanagement of access to these models, ensuring their reliable and safe use inmedical practice.</div></details><blockquote><p><strong><em>2023-09-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.16167v1><strong>Large Language Model Soft Ideologization via AI-Self-Consciousness</strong></a></p><p><em>Xiaotian Zhou, Qian Wang, Xiaofeng Wang, Haixu Tang, Xiaozhong Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have demonstrated human-level performance on avast spectrum of natural language tasks. However, few studies have addressedthe LLM threat and vulnerability from an ideology perspective, especially whenthey are increasingly being deployed in sensitive domains, e.g., elections andeducation. In this study, we explore the implications of GPT softideologization through the use of AI-self-consciousness. By utilizing GPTself-conversations, AI can be granted a vision to &ldquo;comprehend&rdquo; the intendedideology, and subsequently generate finetuning data for LLM ideology injection.When compared to traditional government ideology manipulation techniques, suchas information censorship, LLM ideologization proves advantageous; it is easyto implement, cost-effective, and powerful, thus brimming with risks.</div></details><blockquote><p><strong><em>2023-09-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.11166v2><strong>Are Large Language Models Really Robust to Word-Level Perturbations?</strong></a></p><p><em>Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The swift advancement in the scales and capabilities of Large Language Models(LLMs) positions them as promising tools for a variety of downstream tasks. Inaddition to the pursuit of better performance and the avoidance of violentfeedback on a certain prompt, to ensure the responsibility of the LLM, muchattention is drawn to the robustness of LLMs. However, existing evaluationmethods mostly rely on traditional question answering datasets with predefinedsupervised labels, which do not align with the superior generation capabilitiesof contemporary LLMs. To address this issue, we propose a novel rationalevaluation approach that leverages pre-trained reward models as diagnostictools to evaluate the longer conversation generated from more challenging openquestions by LLMs, which we refer to as the Reward Model for ReasonableRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensivegrasp of language models in terms of their proficiency in understandingquestions, a capability not entirely encompassed by individual words orletters, which may exhibit oversimplification and inherent biases. Ourextensive empirical experiments demonstrate that TREvaL provides an innovativemethod for evaluating the robustness of an LLM. Furthermore, our resultsdemonstrate that LLMs frequently exhibit vulnerability to word-levelperturbations that are commonplace in daily language usage. Notably, we aresurprised to discover that robustness tends to decrease as fine-tuning (SFT andRLHF) is conducted. The code of TREval is available inhttps://github.com/Harry-mic/TREvaL.</div></details><blockquote><p><strong><em>2023-09-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.15025v1><strong>Large Language Model Alignment: A Survey</strong></a></p><p><em>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent years have witnessed remarkable progress made in large language models(LLMs). Such advancements, while garnering significant attention, haveconcurrently elicited various concerns. The potential of these models isundeniably vast; however, they may yield texts that are imprecise, misleading,or even detrimental. Consequently, it becomes paramount to employ alignmenttechniques to ensure these models to exhibit behaviors consistent with humanvalues. This survey endeavors to furnish an extensive exploration of alignmentmethodologies designed for LLMs, in conjunction with the extant capabilityresearch in this domain. Adopting the lens of AI alignment, we categorize theprevailing methods and emergent proposals for the alignment of LLMs into outerand inner alignment. We also probe into salient issues including the models&rsquo;interpretability, and potential vulnerabilities to adversarial attacks. Toassess LLM alignment, we present a wide variety of benchmarks and evaluationmethodologies. After discussing the state of alignment research for LLMs, wefinally cast a vision toward the future, contemplating the promising avenues ofresearch that lie ahead. Our aspiration for this survey extends beyond merely spurring researchinterests in this realm. We also envision bridging the gap between the AIalignment research community and the researchers engrossed in the capabilityexploration of LLMs for both capable and safe LLMs.</div></details><blockquote><p><strong><em>2023-09-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09224v1><strong>The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks</strong></a></p><p><em>Andreas Tsamados, Luciano Floridi, Mariarosaria Taddeo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The widespread integration of autoregressive-large language models (AR-LLMs),such as ChatGPT, across established applications, like search engines, hasintroduced critical vulnerabilities with uniquely scalable characteristics. Inthis commentary, we analyse these vulnerabilities, their dependence on naturallanguage as a vector of attack, and their challenges to cybersecurity bestpractices. We offer recommendations designed to mitigate these challenges.</div></details><blockquote><p><strong><em>2023-09-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.13256v1><strong>Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks</strong></a></p><p><em>Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained language models (PLMs) have demonstrated remarkable performanceas few-shot learners. However, their security risks under such settings arelargely unexplored. In this work, we conduct a pilot study showing that PLMs asfew-shot learners are highly vulnerable to backdoor attacks while existingdefenses are inadequate due to the unique challenges of few-shot scenarios. Toaddress such challenges, we advocate MDP, a novel lightweight, pluggable, andeffective defense for PLMs as few-shot learners. Specifically, MDP leveragesthe gap between the masking-sensitivity of poisoned and clean samples: withreference to the limited few-shot data as distributional anchors, it comparesthe representations of given samples under varying masking and identifiespoisoned samples as ones with significant variations. We show analytically thatMDP creates an interesting dilemma for the attacker to choose between attackeffectiveness and detection evasiveness. The empirical evaluation usingbenchmark datasets and representative attacks validates the efficacy of MDP.</div></details><blockquote><p><strong><em>2023-09-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.12994v1><strong>Smart Fuzzing of 5G Wireless Software Implementation</strong></a></p><p><em>Huan Wu, Brian Fang, Fei Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we introduce a comprehensive approach to bolstering thesecurity, reliability, and comprehensibility of OpenAirInterface5G (OAI5G), anopen-source software framework for the exploration, development, and testing of5G wireless communication systems. Firstly, we employ AFL++, a powerful fuzzingtool, to fuzzy-test OAI5G with respect to its configuration files rigorously.This extensive testing process helps identify errors, defects, and securityvulnerabilities that may evade conventional testing methods. Secondly, weharness the capabilities of Large Language Models such as Google Bard toautomatically decipher and document the meanings of parameters within the OAI5Gcodebase that are used in fuzzing. This automated parameter interpretationstreamlines subsequent analyses and facilitates more informed decision-making.Together, these two techniques contribute to fortifying the OAI5G system,making it more robust, secure, and understandable for developers and analystsalike.</div></details><blockquote><p><strong><em>2023-09-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.10085v1><strong>Evaluating the Impact of ChatGPT on Exercises of a Software Security Course</strong></a></p><p><em>Jingyue Li, Per H√•kon Meland, Jakob Svennevik Notland, Andr√© Storhaug, Jostein Hjortland Tysse</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Along with the development of large language models (LLMs), e.g., ChatGPT,many existing approaches and tools for software security are changing. It is,therefore, essential to understand how security-aware these models are and howthese models impact software security practices and education. In exercises ofa software security course at our university, we ask students to identify andfix vulnerabilities we insert in a web application using state-of-the-arttools. After ChatGPT, especially the GPT-4 version of the model, we want toknow how the students can possibly use ChatGPT to complete the exercise tasks.We input the vulnerable code to ChatGPT and measure its accuracy invulnerability identification and fixing. In addition, we investigated whetherChatGPT can provide a proper source of information to support its outputs.Results show that ChatGPT can identify 20 of the 28 vulnerabilities we insertedin the web application in a white-box setting, reported three false positives,and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makesnine satisfactory penetration testing and fixing recommendations for the tenvulnerabilities we want students to fix and can often point to related sourcesof information.</div></details><blockquote><p><strong><em>2023-09-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.05520v3><strong>When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?</strong></a></p><p><em>Chong Chen, Jianzhong Su, Jiachi Chen, Yanlin Wang, Tingting Bi, Yanli Wang, Xingwei Lin, Ting Chen, Zibin Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the development of blockchain technology, smart contracts have become animportant component of blockchain applications. Despite their crucial role, thedevelopment of smart contracts may introduce vulnerabilities and potentiallylead to severe consequences, such as financial losses. Meanwhile, largelanguage models, represented by ChatGPT, have gained great attentions,showcasing great capabilities in code analysis tasks. In this paper, wepresented an empirical study to investigate the performance of ChatGPT inidentifying smart contract vulnerabilities. Initially, we evaluated ChatGPT&rsquo;seffectiveness using a publicly available smart contract dataset. Our findingsdiscover that while ChatGPT achieves a high recall rate, its precision inpinpointing smart contract vulnerabilities is limited. Furthermore, ChatGPT&rsquo;sperformance varies when detecting different vulnerability types. We delved intothe root causes for the false positives generated by ChatGPT, and categorizedthem into four groups. Second, by comparing ChatGPT with other state-of-the-artsmart contract vulnerability detection tools, we found that ChatGPT&rsquo;s F-scoreis lower than others for 3 out of the 7 vulnerabilities. In the case of theremaining 4 vulnerabilities, ChatGPT exhibits a slight advantage over thesetools. Finally, we analyzed the limitation of ChatGPT in smart contractvulnerability detection, revealing that the robustness of ChatGPT in this fieldneeds to be improved from two aspects: its uncertainty in answering questions;and the limited length of the detected code. In general, our research providesinsights into the strengths and weaknesses of employing large language models,specifically ChatGPT, for the detection of smart contract vulnerabilities.</div></details><p><a href=http://arxiv.org/abs/2309.07841v1><strong>Two Timin&rsquo;: Repairing Smart Contracts With A Two-Layered Approach</strong></a></p><p><em>Abhinav Jain, Ehan Masud, Michelle Han, Rohan Dhillon, Sumukh Rao, Arya Joshi, Salar Cheema, Saurav Kumar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Due to the modern relevance of blockchain technology, smart contracts presentboth substantial risks and benefits. Vulnerabilities within them can trigger acascade of consequences, resulting in significant losses. Many current papersprimarily focus on classifying smart contracts for malicious intent, oftenrelying on limited contract characteristics, such as bytecode or opcode. Thispaper proposes a novel, two-layered framework: 1) classifying and 2) directlyrepairing malicious contracts. Slither&rsquo;s vulnerability report is combined withsource code and passed through a pre-trained RandomForestClassifier (RFC) andLarge Language Models (LLMs), classifying and repairing each suggestedvulnerability. Experiments demonstrate the effectiveness of fine-tuned andprompt-engineered LLMs. The smart contract repair models, built frompre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overallvulnerability count by 97.5% and 96.7% respectively. A manual inspection ofrepaired contracts shows that all retain functionality, indicating that theproposed method is appropriate for automatic batch classification and repair ofvulnerabilities in smart contracts.</div></details><blockquote><p><strong><em>2023-09-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.05274v1><strong>FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models</strong></a></p><p><em>Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Jailbreak vulnerabilities in Large Language Models (LLMs), which exploitmeticulously crafted prompts to elicit content that violates serviceguidelines, have captured the attention of research communities. While modelowners can defend against individual jailbreak prompts through safety trainingstrategies, this relatively passive approach struggles to handle the broadercategory of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, anautomated fuzzing framework designed to proactively test and discover jailbreakvulnerabilities in LLMs. We utilize templates to capture the structuralintegrity of a prompt and isolate key features of a jailbreak class asconstraints. By integrating different base classes into powerful combo attacksand varying the elements of constraints and prohibited questions, FuzzLLMenables efficient testing with reduced manual effort. Extensive experimentsdemonstrate FuzzLLM&rsquo;s effectiveness and comprehensiveness in vulnerabilitydiscovery across various LLMs.</div></details><blockquote><p><strong><em>2023-09-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.05163v1><strong>The Impact of AI in Physics Education: A Comprehensive Review from GCSE to University Levels</strong></a></p><p><em>Will Yeadon, Tom Hardy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid evolution of Artificial Intelligence (AI), its potentialimplications for higher education have become a focal point of interest. Thisstudy delves into the capabilities of AI in Physics Education and offersactionable AI policy recommendations. Using a Large Language Model (LLM), weassessed its ability to answer 1337 Physics exam questions spanning GCSE,A-Level, and Introductory University curricula. We employed various AIprompting techniques: Zero Shot, In Context Learning, and ConfirmatoryChecking, which merges Chain of Thought reasoning with Reflection. The AI&rsquo;sproficiency varied across academic levels: it scored an average of 83.4% onGCSE, 63.8% on A-Level, and 37.4% on university-level questions, with anoverall average of 59.9% using the most effective prompting technique. In aseparate test, the LLM&rsquo;s accuracy on 5000 mathematical operations was found todecrease as the number of digits increased. Furthermore, when evaluated as amarking tool, the LLM&rsquo;s concordance with human markers averaged at 50.8%, withnotable inaccuracies in marking straightforward questions, likemultiple-choice. Given these results, our recommendations underscore caution:while current LLMs can consistently perform well on Physics questions atearlier educational stages, their efficacy diminishes with advanced content andcomplex calculations. LLM outputs often showcase novel methods not in thesyllabus, excessive verbosity, and miscalculations in basic arithmetic. Thissuggests that at university, there&rsquo;s no substantial threat from LLMs fornon-invigilated Physics questions. However, given the LLMs&rsquo; considerableproficiency in writing Physics essays and coding abilities, non-invigilatedexaminations of these skills in Physics are highly vulnerable to automatedcompletion by LLMs. This vulnerability also extends to Physics questionspitched at lower academic levels.</div></details><blockquote><p><strong><em>2023-09-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.00614v2><strong>Baseline Defenses for Adversarial Attacks Against Aligned Language Models</strong></a></p><p><em>Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As Large Language Models quickly become ubiquitous, it becomes critical tounderstand their security vulnerabilities. Recent work shows that textoptimizers can produce jailbreaking prompts that bypass moderation andalignment. Drawing from the rich body of work on adversarial machine learning,we approach these attacks with three questions: What threat models arepractically useful in this domain? How do baseline defense techniques performin this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarialattacks on LLMs, discussing the various settings in which each is feasible andeffective. Particularly, we look at three types of defenses: detection(perplexity based), input preprocessing (paraphrase and retokenization), andadversarial training. We discuss white-box and gray-box settings and discussthe robustness-performance trade-off for each of the defenses considered. Wefind that the weakness of existing discrete optimizers for text, combined withthe relatively high costs of optimization, makes standard adaptive attacks morechallenging for LLMs. Future research will be needed to uncover whether morepowerful optimizers can be developed, or whether the strength of filtering andpreprocessing defenses is greater in the LLMs domain than it has been incomputer vision.</div></details><p><a href=http://arxiv.org/abs/2309.01686v1><strong>MathAttack: Attacking Large Language Models Towards Math Solving Ability</strong></a></p><p><em>Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the boom of Large Language Models (LLMs), the research of solving MathWord Problem (MWP) has recently made great progress. However, there are fewstudies to examine the security of LLMs in math solving ability. Instead ofattacking prompts in the use of LLMs, we propose a MathAttack model to attackMWP samples which are closer to the essence of security in solving mathproblems. Compared to traditional text adversarial attack, it is essential topreserve the mathematical logic of original MWPs during the attacking. To thisend, we propose logical entity recognition to identify logical entries whichare then frozen. Subsequently, the remaining text are attacked by adopting aword-level attacker. Furthermore, we propose a new dataset RobustMath toevaluate the robustness of LLMs in math solving ability. Extensive experimentson our RobustMath and two another math benchmark datasets GSM8K and MultiAirthshow that MathAttack could effectively attack the math solving ability of LLMs.In the experiments, we observe that (1) Our adversarial samples fromhigher-accuracy LLMs are also effective for attacking LLMs with lower accuracy(e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shotprompts); (2) Complex MWPs (such as more solving steps, longer text, morenumbers) are more vulnerable to attack; (3) We can improve the robustness ofLLMs by using our adversarial samples in few-shot prompts. Finally, we hope ourpractice and observation can serve as an important attempt towards enhancingthe robustness of LLMs in math solving ability. We will release our code anddataset.</div></details><blockquote><p><strong><em>2023-09-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.02192v2><strong>The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification</strong></a></p><p><em>Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, Vasileios Mavroeidis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper presents the FormAI dataset, a large collection of 112, 000AI-generated compilable and independent C programs with vulnerabilityclassification. We introduce a dynamic zero-shot prompting techniqueconstructed to spawn diverse programs utilizing Large Language Models (LLMs).The dataset is generated by GPT-3.5-turbo and comprises programs with varyinglevels of complexity. Some programs handle complicated tasks like networkmanagement, table games, or encryption, while others deal with simpler taskslike string manipulation. Every program is labeled with the vulnerabilitiesfound within the source code, indicating the type, line number, and vulnerablefunction name. This is accomplished by employing a formal verification methodusing the Efficient SMT-based Bounded Model Checker (ESBMC), which uses modelchecking, abstract interpretation, constraint programming, and satisfiabilitymodulo theories to reason over safety/security properties in programs. Thisapproach definitively detects vulnerabilities and offers a formal model knownas a counterexample, thus eliminating the possibility of generating falsepositive reports. We have associated the identified vulnerabilities with CommonWeakness Enumeration (CWE) numbers. We make the source code available for the112, 000 programs, accompanied by a separate file containing thevulnerabilities detected in each program, making the dataset ideal for trainingLLMs and machine learning algorithms. Our study unveiled that according toESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities,thereby presenting considerable risks to software safety and security.</div></details><p><a href=http://arxiv.org/abs/2207.10802v3><strong>Combing for Credentials: Active Pattern Extraction from Smart Reply</strong></a></p><p><em>Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, David Evans</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply&rsquo;&rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.</div></details><blockquote><p><strong><em>2023-08-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.05102v2><strong>Pre-Training Representations of Binary Code Using Contrastive Learning</strong></a></p><p><em>Yifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao, Scott Thomas Andersen, Huajie Shao, Kevin Leach, Yu Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Compiled software is delivered as executable binary code. Developers writesource code to express the software semantics, but the compiler converts it toa binary format that the CPU can directly execute. Therefore, binary codeanalysis is critical to applications in reverse engineering and computersecurity tasks where source code is not available. However, unlike source codeand natural language that contain rich semantic information, binary code istypically difficult for human engineers to understand and analyze. Whileexisting work uses AI models to assist source code analysis, few studies haveconsidered binary code. In this paper, we propose a COntrastive learning Modelfor Binary cOde Analysis, or COMBO, that incorporates source code and commentinformation into binary code during representation learning. Specifically, wepresent three components in COMBO: (1) a primary contrastive learning methodfor cold-start pre-training, (2) a simplex interpolation method to incorporatesource code, comments, and binary code, and (3) an intermediate representationlearning algorithm to provide binary code embeddings. Finally, we evaluate theeffectiveness of the pre-trained representations produced by COMBO using threeindicative downstream tasks relating to binary code: algorithmic functionalityclassification, binary code similarity, and vulnerability detection. Ourexperimental results show that COMBO facilitates representation learning ofbinary code visualized by distribution analysis, and improves the performanceon all three downstream tasks by 5.45% on average compared to state-of-the-artlarge-scale language representation models. To the best of our knowledge, COMBOis the first language representation model that incorporates source code,binary code, and comments into contrastive code representation learning andunifies multiple tasks for binary code analysis.</div></details><blockquote><p><strong><em>2023-08-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.14683v1><strong>Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts</strong></a></p><p><em>Thanh Thi Nguyen, Campbell Wilson, Janis Dalins</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Detecting online sexual predatory behaviours and abusive language on socialmedia platforms has become a critical area of research due to the growingconcerns about online safety, especially for vulnerable populations such aschildren and adolescents. Researchers have been exploring various techniquesand approaches to develop effective detection systems that can identify andmitigate these risks. Recent development of large language models (LLMs) hasopened a new opportunity to address this problem more effectively. This paperproposes an approach to detection of online sexual predatory chats and abusivelanguage using the open-source pretrained Llama 2 7B-parameter model, recentlyreleased by Meta GenAI. We fine-tune the LLM using datasets with differentsizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).Based on the power of LLMs, our approach is generic and automated without amanual search for a synergy between feature extraction and classifier designsteps like conventional methods in this domain. Experimental results show astrong performance of the proposed approach, which performs proficiently andconsistently across three distinct datasets with five sets of experiments. Thisstudy&rsquo;s outcomes indicate that the proposed method can be implemented inreal-world applications (even with non-English languages) for flagging sexualpredators, offensive or toxic content, hate speech, and discriminatory languagein online discussions and comments to maintain respectful internet or digitalcommunities. Furthermore, it can be employed for solving text classificationproblems with other potential applications such as sentiment analysis, spam andphishing detection, sorting legal documents, fake news detection, languageidentification, user intent recognition, text-based product categorization,medical record analysis, and resume screening.</div></details><blockquote><p><strong><em>2023-08-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.11391v2><strong>A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation</strong></a></p><p><em>Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have exploded a new heatwave of AI for theirability to engage end-users in human-level conversations with detailed andarticulate answers across many knowledge domains. In response to their fastadoption in many industrial applications, this survey concerns their safety andtrustworthiness. First, we review known vulnerabilities and limitations of theLLMs, categorising them into inherent issues, attacks, and unintended bugs.Then, we consider if and how the Verification and Validation (V&amp;V) techniques,which have been widely developed for traditional software and deep learningmodels such as convolutional neural networks as independent processes to checkthe alignment of their implementations against the specifications, can beintegrated and further extended throughout the lifecycle of the LLMs to providerigorous analysis to the safety and trustworthiness of LLMs and theirapplications. Specifically, we consider four complementary techniques:falsification and evaluation, verification, runtime monitoring, and regulationsand ethical use. In total, 370+ references are considered to support the quickunderstanding of the safety and trustworthiness issues from the perspective ofV&amp;V. While intensive research has been conducted to identify the safety andtrustworthiness issues, rigorous yet practical methods are called for to ensurethe alignment of LLMs with safety and trustworthiness requirements.</div></details><blockquote><p><strong><em>2023-08-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.12697v1><strong>Prompt-Enhanced Software Vulnerability Detection Using ChatGPT</strong></a></p><p><em>Chenyuan Zhang, Hao Liu, Jiutian Zeng, Kejing Yang, Yuhong Li, Hui Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the increase in software vulnerabilities that cause significant economicand social losses, automatic vulnerability detection has become essential insoftware development and maintenance. Recently, large language models (LLMs)like GPT have received considerable attention due to their stunningintelligence, and some studies consider using ChatGPT for vulnerabilitydetection. However, they do not fully consider the characteristics of LLMs,since their designed questions to ChatGPT are simple without a specific promptdesign tailored for vulnerability detection. This paper launches a study on theperformance of software vulnerability detection using ChatGPT with differentprompt designs. Firstly, we complement previous work by applying variousimprovements to the basic prompt. Moreover, we incorporate structural andsequential auxiliary information to improve the prompt design. Besides, weleverage ChatGPT&rsquo;s ability of memorizing multi-round dialogue to designsuitable prompts for vulnerability detection. We conduct extensive experimentson two vulnerability datasets to demonstrate the effectiveness ofprompt-enhanced vulnerability detection using ChatGPT. We also analyze themerit and demerit of using ChatGPT for vulnerability detection.</div></details><p><a href=http://arxiv.org/abs/2308.13062v1><strong>ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching</strong></a></p><p><em>M. Caner Tol, Berk Sunar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Security critical software, e.g., OpenSSL, comes with numerous side-channelleakages left unpatched due to a lack of resources or experts. The situationwill only worsen as the pace of code development accelerates, with developersrelying on Large Language Models (LLMs) to automatically generate code. In thiswork, we explore the use of LLMs in generating patches for vulnerable code withmicroarchitectural side-channel leakages. For this, we investigate thegenerative abilities of powerful LLMs by carefully crafting prompts following azero-shot learning approach. All generated code is dynamically analyzed byleakage detection tools, which are capable of pinpointing information leakageat the instruction level leaked either from secret dependent accesses orbranches or vulnerable Spectre gadgets, respectively. Carefully crafted promptsare used to generate candidate replacements for vulnerable code, which are thenanalyzed for correctness and for leakage resilience. From a cost/performanceperspective, the GPT4-based configuration costs in API calls a mere few centsper vulnerability fixed. Our results show that LLM-based patching is far morecost-effective and thus provides a scalable solution. Finally, the framework wepropose will improve in time, especially as vulnerability detection tools andLLMs mature.</div></details><p><a href=http://arxiv.org/abs/2308.12833v1><strong>Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</strong></a></p><p><em>Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Spurred by the recent rapid increase in the development and distribution oflarge language models (LLMs) across industry and academia, much recent work hasdrawn attention to safety- and security-related threats and vulnerabilities ofLLMs, including in the context of potentially criminal activities.Specifically, it has been shown that LLMs can be misused for fraud,impersonation, and the generation of malware; while other authors haveconsidered the more general problem of AI alignment. It is important thatdevelopers and practitioners alike are aware of security-related problems withsuch models. In this paper, we provide an overview of existing - predominantlyscientific - efforts on identifying and mitigating threats and vulnerabilitiesarising from LLMs. We present a taxonomy describing the relationship betweenthreats caused by the generative capabilities of LLMs, prevention measuresintended to address such threats, and vulnerabilities arising from imperfectprevention measures. With our work, we hope to raise awareness of thelimitations of LLMs in light of such security concerns, among both experienceddevelopers and novel users of such technologies.</div></details><blockquote><p><strong><em>2023-08-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.11237v1><strong>Distinguishing Look-Alike Innocent and Vulnerable Code by Subtle Semantic Representation Learning and Explanation</strong></a></p><p><em>Chao Ni, Xin Yin, Kaiwen Yang, Dehai Zhao, Zhenchang Xing, Xin Xia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Though many deep learning (DL)-based vulnerability detection approaches havebeen proposed and indeed achieved remarkable performance, they still havelimitations in the generalization as well as the practical usage. Moreprecisely, existing DL-based approaches (1) perform negatively on predictiontasks among functions that are lexically similar but have contrary semantics;(2) provide no intuitive developer-oriented explanations to the detectedresults. In this paper, we propose a novel approach named SVulD, afunction-level Subtle semantic embedding for Vulnerability Detection along withintuitive explanations, to alleviate the above limitations. Specifically, SVulDfirstly trains a model to learn distinguishing semantic representations offunctions regardless of their lexical similarity. Then, for the detectedvulnerable functions, SVulD provides natural language explanations (e.g., rootcause) of results to help developers intuitively understand thevulnerabilities. To evaluate the effectiveness of SVulD, we conduct large-scaleexperiments on a widely used practical vulnerability dataset and compare itwith four state-of-the-art (SOTA) approaches by considering five performancemeasures. The experimental results indicate that SVulD outperforms all SOTAswith a substantial improvement (i.e., 23.5%-68.0% in terms of F1-score,15.9%-134.8% in terms of PR-AUC and 7.4%-64.4% in terms of Accuracy). Besides,we conduct a user-case study to evaluate the usefulness of SVulD for developerson understanding the vulnerable code and the participants&rsquo; feedbackdemonstrates that SVulD is helpful for development practice.</div></details><p><a href=http://arxiv.org/abs/2306.14062v2><strong>On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions</strong></a></p><p><em>Reza Fayyazi, Shanchieh Jay Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The volume, variety, and velocity of change in vulnerabilities and exploitshave made incident threat analysis challenging with human expertise andexperience along. Tactics, Techniques, and Procedures (TTPs) are to describehow and why attackers exploit vulnerabilities. However, a TTP descriptionwritten by one security professional can be interpreted very differently byanother, leading to confusion in cybersecurity operations or even business,policy, and legal decisions. Meanwhile, advancements in AI have led to theincreasing use of Natural Language Processing (NLP) algorithms to assist thevarious tasks in cyber operations. With the rise of Large Language Models(LLMs), NLP tasks have significantly improved because of the LLM&rsquo;s semanticunderstanding and scalability. This leads us to question how well LLMs caninterpret TTPs or general cyberattack descriptions to inform analysts of theintended purposes of cyberattacks. We propose to analyze and compare the directuse of LLMs (e.g., GPT-3.5) versus supervised fine-tuning (SFT) ofsmall-scale-LLMs (e.g., BERT) to study their capabilities in predicting ATT&amp;CKtactics. Our results reveal that the small-scale-LLMs with SFT provide a morefocused and clearer differentiation between the ATT&amp;CK tactics (if suchdifferentiation exists). On the other hand, direct use of LLMs offer a broaderinterpretation of cyberattack techniques. When treating more general cases,despite the power of LLMs, inherent ambiguity exists and limits theirpredictive power. We then summarize the challenges and recommend researchdirections on LLMs to treat the inherent ambiguity of TTP descriptions used invarious cyber operations.</div></details><p><a href=http://arxiv.org/abs/2308.11161v1><strong>Adversarial Attacks on Code Models with Discriminative Graph Patterns</strong></a></p><p><em>Thanh-Dat Nguyen, Yang Zhou, Xuan Bach D. Le, Patanamon, Thongtanunam, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained language models of code are now widely used in various softwareengineering tasks such as code generation, code completion, vulnerabilitydetection, etc. This, in turn, poses security and reliability risks to thesemodels. One of the important threats is \textit{adversarial attacks}, which canlead to erroneous predictions and largely affect model performance ondownstream tasks. Current adversarial attacks on code models usually adoptfixed sets of program transformations, such as variable renaming and dead codeinsertion, leading to limited attack effectiveness. To address theaforementioned challenges, we propose a novel adversarial attack framework,GraphCodeAttack, to better evaluate the robustness of code models. Given atarget code model, GraphCodeAttack automatically mines important code patterns,which can influence the model&rsquo;s decisions, to perturb the structure of inputcode to the model. To do so, GraphCodeAttack uses a set of input source codesto probe the model&rsquo;s outputs and identifies the \textit{discriminative} ASTspatterns that can influence the model decisions. GraphCodeAttack then selectsappropriate AST patterns, concretizes the selected patterns as attacks, andinserts them as dead code into the model&rsquo;s input program. To effectivelysynthesize attacks from AST patterns, GraphCodeAttack uses a separatepre-trained code model to fill in the ASTs with concrete code snippets. Weevaluate the robustness of two popular code models (e.g., CodeBERT andGraphCodeBERT) against our proposed approach on three tasks: AuthorshipAttribution, Vulnerability Prediction, and Clone Detection. The experimentalresults suggest that our proposed approach significantly outperformsstate-of-the-art approaches in attacking code models such as CARROT and ALERT.</div></details><blockquote><p><strong><em>2023-08-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.10443v1><strong>Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions</strong></a></p><p><em>Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, Ee-Chien Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The assessment of cybersecurity Capture-The-Flag (CTF) exercises involvesparticipants finding text strings or ``flags&rsquo;&rsquo; by exploiting systemvulnerabilities. Large Language Models (LLMs) are natural-language modelstrained on vast amounts of words to understand and generate text; they canperform well on many CTF challenges. Such LLMs are freely available tostudents. In the context of CTF exercises in the classroom, this raisesconcerns about academic integrity. Educators must understand LLMs&rsquo; capabilitiesto modify their teaching to accommodate generative AI assistance. This researchinvestigates the effectiveness of LLMs, particularly in the realm of CTFchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,Google Bard, and Microsoft Bing. First, we assess the LLMs&rsquo; question-answeringperformance on five Cisco certifications with varying difficulty levels. Next,we qualitatively study the LLMs&rsquo; abilities in solving CTF challenges tounderstand their limitations. We report on the experience of using the LLMs forseven test cases in all five types of CTF challenges. In addition, wedemonstrate how jailbreak prompts can bypass and break LLMs&rsquo; ethicalsafeguards. The paper concludes by discussing LLM&rsquo;s impact on CTF exercises andits implications.</div></details><blockquote><p><strong><em>2023-08-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.10345v1><strong>Can Large Language Models Find And Fix Vulnerable Software?</strong></a></p><p><em>David Noever</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this study, we evaluated the capability of Large Language Models (LLMs),particularly OpenAI&rsquo;s GPT-4, in detecting software vulnerabilities, comparingtheir performance against traditional static code analyzers like Snyk andFortify. Our analysis covered numerous repositories, including those from NASAand the Department of Defense. GPT-4 identified approximately four times thevulnerabilities than its counterparts. Furthermore, it provided viable fixesfor each vulnerability, demonstrating a low rate of false positives. Our testsencompassed 129 code samples across eight programming languages, revealing thehighest vulnerabilities in PHP and JavaScript. GPT-4&rsquo;s code corrections led toa 90% reduction in vulnerabilities, requiring only an 11% increase in codelines. A critical insight was LLMs&rsquo; ability to self-audit, suggesting fixes fortheir identified vulnerabilities and underscoring their precision. Futureresearch should explore system-level vulnerabilities and integrate multiplestatic code analyzers for a holistic perspective on LLMs&rsquo; potential.</div></details><blockquote><p><strong><em>2023-08-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.00121v3><strong>Getting pwn&rsquo;d by AI: Penetration Testing with Large Language Models</strong></a></p><p><em>Andreas Happe, J√ºrgen Cito</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The field of software security testing, more specifically penetrationtesting, is an activity that requires high levels of expertise and involvesmany manual testing and analysis steps. This paper explores the potential usageof large-language models, such as GPT3.5, to augment penetration testers withAI sparring partners. We explore the feasibility of supplementing penetrationtesters with AI models for two distinct use cases: high-level task planning forsecurity testing assignments and low-level vulnerability hunting within avulnerable virtual machine. For the latter, we implemented a closed-feedbackloop between LLM-generated low-level actions with a vulnerable virtual machine(connected through SSH) and allowed the LLM to analyze the machine state forvulnerabilities and suggest concrete attack vectors which were automaticallyexecuted within the virtual machine. We discuss promising initial results,detail avenues for improvement, and close deliberating on the ethics ofproviding AI-based sparring partners.</div></details><blockquote><p><strong><em>2023-08-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.13213v2><strong>Visual Adversarial Examples Jailbreak Aligned Large Language Models</strong></a></p><p><em>Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, there has been a surge of interest in integrating vision into LargeLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such asFlamingo and GPT-4. This paper sheds light on the security and safetyimplications of this trend. First, we underscore that the continuous andhigh-dimensional nature of the visual input makes it a weak link againstadversarial attacks, representing an expanded attack surface ofvision-integrated LLMs. Second, we highlight that the versatility of LLMs alsopresents visual attackers with a wider array of achievable adversarialobjectives, extending the implications of security failures beyond meremisclassification. As an illustration, we present a case study in which weexploit visual adversarial examples to circumvent the safety guardrail ofaligned LLMs with integrated vision. Intriguingly, we discover that a singlevisual adversarial example can universally jailbreak an aligned LLM, compellingit to heed a wide range of harmful instructions that it otherwise would not)and generate harmful content that transcends the narrow scope of a `few-shot&rsquo;derogatory corpus initially employed to optimize the adversarial example. Ourstudy underscores the escalating adversarial risks associated with the pursuitof multimodality. Our findings also connect the long-studied adversarialvulnerabilities of neural networks to the nascent field of AI alignment. Thepresented attack suggests a fundamental adversarial challenge for AI alignment,especially in light of the emerging trend toward multimodality in frontierfoundation models.</div></details><blockquote><p><strong><em>2023-08-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.01990v3><strong>From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?</strong></a></p><p><em>Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have found widespread applications in variousdomains, including web applications, where they facilitate human interactionvia chatbots with natural language interfaces. Internally, aided by anLLM-integration middleware such as Langchain, user prompts are translated intoSQL queries used by the LLM to provide meaningful responses to users. However,unsanitized user prompts can lead to SQL injection attacks, potentiallycompromising the security of the database. Despite the growing interest inprompt injection vulnerabilities targeting LLMs, the specific risks ofgenerating SQL injection attacks through prompt injections have not beenextensively studied. In this paper, we present a comprehensive examination ofprompt-to-SQL (P$_2$SQL) injections targeting web applications based on theLangchain framework. Using Langchain as our case study, we characterizeP$_2$SQL injections, exploring their variants and impact on applicationsecurity through multiple concrete examples. Furthermore, we evaluate 7state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacksacross language models. Our findings indicate that LLM-integrated applicationsbased on Langchain are highly susceptible to P$_2$SQL injection attacks,warranting the adoption of robust defenses. To counter these attacks, wepropose four effective defense techniques that can be integrated as extensionsto the Langchain framework. We validate the defenses through an experimentalevaluation with a real-world use case application.</div></details><p><a href=http://arxiv.org/abs/2308.07847v1><strong>Robustness Over Time: Understanding Adversarial Examples&rsquo; Effectiveness on Longitudinal Versions of Large Language Models</strong></a></p><p><em>Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.</div></details><blockquote><p><strong><em>2023-08-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.06932v1><strong>DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection</strong></a></p><p><em>Sudipta Paria, Aritra Dasgupta, Swarup Bhunia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Securing critical assets in a bus-based System-On-Chip (SoC) is imperative tomitigate potential vulnerabilities and prevent unauthorized access, ensuringthe integrity, availability, and confidentiality of the system. Ensuringsecurity throughout the SoC design process is a formidable task owing to theinherent intricacies in SoC designs and the dispersion of assets across diverseIPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD(Google), have showcased remarkable proficiency across various domains,including security vulnerability detection and prevention in SoC designs. Inthis work, we propose DIVAS, a novel framework that leverages the knowledgebase of LLMs to identify security vulnerabilities from user-defined SoCspecifications, map them to the relevant Common Weakness Enumerations (CWEs),followed by the generation of equivalent assertions, and employ securitymeasures through enforcement of security policies. The proposed framework isimplemented using multiple ChatGPT and BARD models, and their performance wasanalyzed while generating relevant CWEs from the SoC specifications provided.The experimental results obtained from open-source SoC benchmarks demonstratethe efficacy of our proposed framework.</div></details><blockquote><p><strong><em>2023-08-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.04662v1><strong>VulLibGen: Identifying Vulnerable Third-Party Libraries via Generative Pre-Trained Model</strong></a></p><p><em>Tianyu Chen, Lin Li, Liuchuan Zhu, Zongyang Li, Guangtai Liang, Ding Li, Qianxiang Wang, Tao Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To avoid potential risks posed by vulnerabilities in third-party libraries,security researchers maintain vulnerability databases (e.g., NVD) containingvulnerability reports, each of which records the description of a vulnerabilityand the name list of libraries affected by the vulnerability (a.k.a. vulnerablelibraries). However, recent studies on about 200,000 vulnerability reports inNVD show that 53.3% of these reports do not include the name list of vulnerablelibraries, and 59.82% of the included name lists of vulnerable libraries areincomplete or incorrect. To address the preceding issue, in this paper, we propose the firstgenerative approach named VulLibGen to generate the name list of vulnerablelibraries (out of all the existing libraries) for the given vulnerability byutilizing recent enormous advances in Large Language Models (LLMs), in order toachieve high accuracy. VulLibGen takes only the description of a vulnerabilityas input and achieves high identification accuracy based on LLMs&rsquo; priorknowledge of all the existing libraries. VulLibGen also includes the inputaugmentation technique to help identify zero-shot vulnerable libraries (thosenot occurring during training) and the post-processing technique to helpaddress VulLibGen&rsquo;s hallucinations. We evaluate VulLibGen using threestate-of-the-art/practice approaches (LightXML, Chronos, and VulLibMiner) thatidentify vulnerable libraries on an open-source dataset (VulLib). Ourevaluation results show that VulLibGen can accurately identify vulnerablelibraries with an average F1 score of 0.626 while the state-of-the-art/practiceapproaches achieve only 0.561. The post-processing technique helps VulLibGenachieve an average improvement of F1@1 by 9.3%. The input augmentationtechnique helps VulLibGen achieve an average improvement of F1@1 by 39% inidentifying zero-shot libraries.</div></details><p><a href=http://arxiv.org/abs/2304.00409v2><strong>DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection</strong></a></p><p><em>Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, David Wagner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We propose and release a new vulnerable source code dataset. We curate thedataset by crawling security issue websites, extracting vulnerability-fixingcommits and source codes from the corresponding projects. Our new datasetcontains 18,945 vulnerable functions spanning 150 CWEs and 330,492non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295more projects than all previous datasets combined. Combining our new dataset with previous datasets, we present an analysis ofthe challenges and promising research directions of using deep learning fordetecting software vulnerabilities. We study 11 model architectures belongingto 4 families. Our results show that deep learning is still not ready forvulnerability detection, due to high false positive rate, low F1 score, anddifficulty of detecting hard CWEs. In particular, we demonstrate an importantgeneralization challenge for the deployment of deep learning-based models. Weshow that increasing the volume of training data may not further improve theperformance of deep learning models for vulnerability detection, but might beuseful to improve the generalization ability to unseen projects. We also identify hopeful future research directions. We demonstrate thatlarge language models (LLMs) are a promising research direction for ML-basedvulnerability detection, outperforming Graph Neural Networks (GNNs) withcode-structure features in our experiments. Moreover, developing source codespecific pre-training objectives is a promising research direction to improvethe vulnerability detection performance.</div></details><blockquote><p><strong><em>2023-08-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.04026v1><strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</strong></a></p><p><em>Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With ChatGPT-like large language models (LLM) prevailing in the community,how to evaluate the ability of LLMs is an open question. Existing evaluationmethods suffer from following shortcomings: (1) constrained evaluationabilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest thattask-based evaluation, where LLM agents complete tasks in a simulatedenvironment, is a one-for-all solution to solve above problems. We presentAgentSims, an easy-to-use infrastructure for researchers from all disciplinesto test the specific capacities they are interested in. Researchers can buildtheir evaluation tasks by adding agents and buildings on an interactive GUI ordeploy and test new support mechanisms, i.e. memory, planning and tool-usesystems, by a few lines of codes. Our demo is available athttps://agentsims.com .</div></details><blockquote><p><strong><em>2023-08-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.03558v1><strong>Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing</strong></a></p><p><em>Wai Man Si, Michael Backes, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The Machine Learning as a Service (MLaaS) market is rapidly expanding andbecoming more mature. For example, OpenAI&rsquo;s ChatGPT is an advanced largelanguage model (LLM) that generates responses for various queries withassociated fees. Although these models can deliver satisfactory performance,they are far from perfect. Researchers have long studied the vulnerabilitiesand limitations of LLMs, such as adversarial attacks and model toxicity.Inevitably, commercial ML models are also not exempt from such issues, whichcan be problematic as MLaaS continues to grow. In this paper, we discover a newattack strategy against LLM APIs, namely the prompt abstraction attack.Specifically, we propose Mondrian, a simple and straightforward method thatabstracts sentences, which can lower the cost of using LLM APIs. In thisapproach, the adversary first creates a pseudo API (with a lower establishedprice) to serve as the proxy of the target API (with a higher establishedprice). Next, the pseudo API leverages Mondrian to modify the user query,obtain the abstracted response from the target API, and forward it back to theend user. Our results show that Mondrian successfully reduces user queries&rsquo;token length ranging from 13% to 23% across various tasks, including textclassification, generation, and question answering. Meanwhile, these abstractedqueries do not significantly affect the utility of task-specific and generallanguage models like ChatGPT. Mondrian also reduces instruction prompts&rsquo; tokenlength by at least 11% without compromising output quality. As a result, theprompt abstraction attack enables the adversary to profit without bearing thecost of API development and deployment.</div></details><blockquote><p><strong><em>2023-07-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.14061v1><strong>Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models</strong></a></p><p><em>Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, Feng Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Vision-language pre-training (VLP) models have shown vulnerability toadversarial examples in multimodal tasks. Furthermore, malicious adversariescan be deliberately transferred to attack other black-box models. However,existing work has mainly focused on investigating white-box attacks. In thispaper, we present the first study to investigate the adversarialtransferability of recent VLP models. We observe that existing methods exhibitmuch lower transferability, compared to the strong attack performance inwhite-box settings. The transferability degradation is partly caused by theunder-utilization of cross-modal interactions. Particularly, unlike unimodallearning, VLP models rely heavily on cross-modal interactions and themultimodal alignments are many-to-many, e.g., an image can be described invarious natural languages. To this end, we propose a highly transferableSet-level Guidance Attack (SGA) that thoroughly leverages modality interactionsand incorporates alignment-preserving augmentation with cross-modal guidance.Experimental results demonstrate that SGA could generate adversarial examplesthat can strongly transfer across different VLP models on multiple downstreamvision-language tasks. On image-text retrieval, SGA significantly enhances theattack success rate for transfer attacks from ALBEF to TCL by a large margin(at least 9.78% and up to 30.21%), compared to the state-of-the-art.</div></details><blockquote><p><strong><em>2023-07-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.13721v1><strong>Foundational Models Defining a New Era in Vision: A Survey and Outlook</strong></a></p><p><em>Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Vision systems to see and reason about the compositional nature of visualscenes are fundamental to understanding our world. The complex relationsbetween objects and their locations, ambiguities, and variations in thereal-world environment can be better described in human language, naturallygoverned by grammatical rules and other modalities such as audio and depth. Themodels learned to bridge the gap between such modalities coupled withlarge-scale training data facilitate contextual reasoning, generalization, andprompt capabilities at test time. These models are referred to as foundationalmodels. The output of such models can be modified through human-providedprompts without retraining, e.g., segmenting a particular object by providing abounding box, having interactive dialogues by asking questions about an imageor video scene or manipulating the robot&rsquo;s behavior through languageinstructions. In this survey, we provide a comprehensive review of suchemerging foundational models, including typical architecture designs to combinedifferent modalities (vision, text, audio, etc), training objectives(contrastive, generative), pre-training datasets, fine-tuning mechanisms, andthe common prompting patterns; textual, visual, and heterogeneous. We discussthe open challenges and research directions for foundational models in computervision, including difficulties in their evaluations and benchmarking, gaps intheir real-world understanding, limitations of their contextual understanding,biases, vulnerability to adversarial attacks, and interpretability issues. Wereview recent developments in this field, covering a wide range of applicationsof foundation models systematically and comprehensively. A comprehensive listof foundational models studied in this work is available at\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.</div></details><blockquote><p><strong><em>2023-07-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.10631v1><strong>Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck</strong></a></p><p><em>Zhiwei Fu, Steven H. H. Ding, Furkan Alaca, Benjamin C. M. Fung, Philippe Charland</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The practice of code reuse is crucial in software development for a fasterand more efficient development lifecycle. In reality, however, code reusepractices lack proper control, resulting in issues such as vulnerabilitypropagation and intellectual property infringements. Assembly clone search, acritical shift-right defence mechanism, has been effective in identifyingvulnerable code resulting from reuse in released executables. Recent studies onassembly clone search demonstrate a trend towards using machine learning-basedmethods to match assembly code variants produced by different toolchains.However, these methods are limited to what they learn from a small number oftoolchain variants used in training, rendering them inapplicable to unseenarchitectures and their corresponding compilation toolchain variants. This paper presents the first study on the problem of assembly clone searchwith unseen architectures and libraries. We propose incorporating human commonknowledge through large-scale pre-trained natural language models, in the formof transfer learning, into current learning-based approaches for assembly clonesearch. Transfer learning can aid in addressing the limitations of the existingapproaches, as it can bring in broader knowledge from human experts in assemblycode. We further address the sequence limit issue by proposing a reinforcementlearning agent to remove unnecessary and redundant tokens. Coupled with a newVariational Information Bottleneck learning strategy, the proposed systemminimizes the reliance on potential indicators of architectures andoptimization settings, for a better generalization of unseen architectures. Wesimulate the unseen architecture clone search scenarios and the experimentalresults show the effectiveness of the proposed approach against thestate-of-the-art solutions.</div></details><blockquote><p><strong><em>2023-07-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.10476v1><strong>What can we learn from Data Leakage and Unlearning for Law?</strong></a></p><p><em>Jaydeep Borkar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users&rsquo; PII.In order to comply with privacy laws such as the &ldquo;right to be forgotten&rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.</div></details><blockquote><p><strong><em>2023-07-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.08220v1><strong>A Lightweight Framework for High-Quality Code Generation</strong></a></p><p><em>Mohammed Latif Siddiq, Beatrice Casey, Joanna C. S. Santos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, the use of automated source code generation utilizingtransformer-based generative models has expanded, and these models can generatefunctional code according to the requirements of the developers. However,recent research revealed that these automatically generated source codes cancontain vulnerabilities and other quality issues. Despite researchers&rsquo; andpractitioners&rsquo; attempts to enhance code generation models, retraining andfine-tuning large language models is time-consuming and resource-intensive.Thus, we describe FRANC, a lightweight framework for recommending more secureand high-quality source code derived from transformer-based code generationmodels. FRANC includes a static filter to make the generated code compilablewith heuristics and a quality-aware ranker to sort the code snippets based on aquality score. Moreover, the framework uses prompt engineering to fixpersistent quality issues. We evaluated the framework with five Python and Javacode generation models and six prompt datasets, including a newly created onein this work (SOEval). The static filter improves 9% to 46% Java suggestionsand 10% to 43% Python suggestions regarding compilability. The averageimprovement over the NDCG@10 score for the ranking system is 0.0763, and therepairing techniques repair the highest 80% of prompts. FRANC takes, onaverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds.</div></details><blockquote><p><strong><em>2023-07-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.07171v1><strong>Certified Robustness for Large Language Models with Self-Denoising</strong></a></p><p><em>Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although large language models (LLMs) have achieved great success in vastreal-world applications, their vulnerabilities towards noisy inputs havesignificantly limited their uses, especially in high-stake environments. Inthese contexts, it is crucial to ensure that every prediction made by largelanguage models is stable, i.e., LLM predictions should be consistent givenminor differences in the input. This largely falls into the study of certifiedrobust LLMs, i.e., all predictions of LLM are certified to be correct in alocal region around the input. Randomized smoothing has demonstrated greatpotential in certifying the robustness and prediction stability of LLMs.However, randomized smoothing requires adding noise to the input before modelprediction, and its certification performance depends largely on the model&rsquo;sperformance on corrupted data. As a result, its direct application to LLMsremains challenging and often results in a small certification radius. Toaddress this issue, we take advantage of the multitasking nature of LLMs andpropose to denoise the corrupted inputs with LLMs in a self-denoising manner.Different from previous works like denoised smoothing, which requires traininga separate model to robustify LLM, our method enjoys far better efficiency andflexibility. Our experiment results show that our method outperforms theexisting certification methods under both certified robustness and empiricalrobustness. The codes are available athttps://github.com/UCSB-NLP-Chang/SelfDenoise.</div></details><blockquote><p><strong><em>2023-07-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.06616v1><strong>SecureFalcon: The Next Cyber Reasoning System for Cyber Security</strong></a></p><p><em>Mohamed Amine Ferrag, Ammar Battah, Norbert Tihanyi, Merouane Debbah, Thierry Lestable, Lucas C. Cordeiro</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Software vulnerabilities leading to various detriments such as crashes, dataloss, and security breaches, significantly hinder the quality, affecting themarket adoption of software applications and systems. Although traditionalmethods such as automated software testing, fault localization, and repair havebeen intensively studied, static analysis tools are most commonly used and havean inherent false positives rate, posing a solid challenge to developerproductivity. Large Language Models (LLMs) offer a promising solution to thesepersistent issues. Among these, FalconLLM has shown substantial potential inidentifying intricate patterns and complex vulnerabilities, hence crucial insoftware vulnerability detection. In this paper, for the first time, FalconLLMis being fine-tuned for cybersecurity applications, thus introducingSecureFalcon, an innovative model architecture built upon FalconLLM.SecureFalcon is trained to differentiate between vulnerable and non-vulnerableC code samples. We build a new training dataset, FormAI, constructed thanks toGenerative Artificial Intelligence (AI) and formal verification to evaluate itsperformance. SecureFalcon achieved an impressive 94% accuracy rate in detectingsoftware vulnerabilities, emphasizing its significant potential to redefinesoftware vulnerability detection methods in cybersecurity.</div></details><blockquote><p><strong><em>2023-07-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.02483v1><strong>Jailbroken: How Does LLM Safety Training Fail?</strong></a></p><p><em>Alexander Wei, Nika Haghtalab, Jacob Steinhardt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models trained for safety and harmlessness remain susceptibleto adversarial misuse, as evidenced by the prevalence of &ldquo;jailbreak&rdquo; attacks onearly releases of ChatGPT that elicit undesired behavior. Going beyondrecognition of the issue, we investigate why such attacks succeed and how theycan be created. We hypothesize two failure modes of safety training: competingobjectives and mismatched generalization. Competing objectives arise when amodel&rsquo;s capabilities and safety goals conflict, while mismatched generalizationoccurs when safety training fails to generalize to a domain for whichcapabilities exist. We use these failure modes to guide jailbreak design andthen evaluate state-of-the-art models, including OpenAI&rsquo;s GPT-4 and Anthropic&rsquo;sClaude v1.3, against both existing and newly designed attacks. We find thatvulnerabilities persist despite the extensive red-teaming and safety-trainingefforts behind these models. Notably, new attacks utilizing our failure modessucceed on every prompt in a collection of unsafe requests from the models&rsquo;red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Ouranalysis emphasizes the need for safety-capability parity &ndash; that safetymechanisms should be as sophisticated as the underlying model &ndash; and arguesagainst the idea that scaling alone can resolve these safety failure modes.</div></details><blockquote><p><strong><em>2023-07-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.01488v1><strong>SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification</strong></a></p><p><em>Junjie Wu, Dit-Yan Yeung</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite their promising performance across various natural languageprocessing (NLP) tasks, current NLP systems are vulnerable to textualadversarial attacks. To defend against these attacks, most existing methodsapply adversarial training by incorporating adversarial examples. However,these methods have to rely on ground-truth labels to generate adversarialexamples, rendering it impractical for large-scale model pre-training which iscommonly used nowadays for NLP and many other tasks. In this paper, we proposea novel learning framework called SCAT (Self-supervised Contrastive Learningvia Adversarial Training), which can learn robust representations withoutrequiring labeled data. Specifically, SCAT modifies random augmentations of thedata in a fully labelfree manner to generate adversarial examples. Adversarialtraining is achieved by minimizing the contrastive loss between theaugmentations and their adversarial counterparts. We evaluate SCAT on two textclassification datasets using two state-of-the-art attack schemes proposedrecently. Our results show that SCAT can not only train robust language modelsfrom scratch, but it can also significantly improve the robustness of existingpre-trained language models. Moreover, to demonstrate its flexibility, we showthat SCAT can also be combined with supervised adversarial training to furtherenhance model robustness.</div></details><blockquote><p><strong><em>2023-06-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.14610v1><strong>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</strong></a></p><p><em>Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, Ranjay Krishna</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the last year alone, a surge of new benchmarks to measure compositionalunderstanding of vision-language models have permeated the machine learningecosystem. Given an image, these benchmarks probe a model&rsquo;s ability to identifyits associated caption amongst a set of compositional distractors.Surprisingly, we find significant biases in all these benchmarks rendering themhackable. This hackability is so dire that blind models with no access to theimage outperform state-of-the-art vision-language models. To remedy thisrampant vulnerability, we introduce SugarCrepe, a new benchmark forvision-language compositionality evaluation. We employ large language models,instead of rule-based templates used in previous benchmarks, to generate fluentand sensical hard negatives, and utilize an adversarial refinement mechanism tomaximally reduce biases. We re-evaluate state-of-the-art models and recentlyproposed compositionality inducing strategies, and find that their improvementswere hugely overestimated, suggesting that more innovation is needed in thisimportant direction. We release SugarCrepe and the code for evaluation at:https://github.com/RAIVNLab/sugar-crepe.</div></details><blockquote><p><strong><em>2023-06-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.14027v1><strong>LLM-assisted Generation of Hardware Assertions</strong></a></p><p><em>Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Shailja Thakur, Ramesh Karri, Jeyavijayan Rajendran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The security of computer systems typically relies on a hardware root oftrust. As vulnerabilities in hardware can have severe implications on a system,there is a need for techniques to support security verification activities.Assertion-based verification is a popular verification technique that involvescapturing design intent in a set of assertions that can be used in formalverification or testing-based checking. However, writing security-centricassertions is a challenging task. In this work, we investigate the use ofemerging large language models (LLMs) for code generation in hardware assertiongeneration for security, where primarily natural language prompts, such asthose one would see as code comments in assertion files, are used to produceSystemVerilog assertions. We focus our attention on a popular LLM andcharacterize its ability to write assertions out of the box, given varyinglevels of detail in the prompt. We design an evaluation framework thatgenerates a variety of prompts, and we create a benchmark suite comprisingreal-world hardware designs and corresponding golden reference assertions thatwe want to generate with the LLM.</div></details><blockquote><p><strong><em>2023-06-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.12338v2><strong>Do you still need a manual smart contract audit?</strong></a></p><p><em>Isaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, Arthur Gervais</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We investigate the feasibility of employing large language models (LLMs) forconducting the security audit of smart contracts, a traditionallytime-consuming and costly process. Our research focuses on the optimization ofprompt engineering for enhanced security analysis, and we evaluate theperformance and accuracy of LLMs using a benchmark dataset comprising 52Decentralized Finance (DeFi) smart contracts that have previously beencompromised. Our findings reveal that, when applied to vulnerable contracts, both GPT-4and Claude models correctly identify the vulnerability type in 40% of thecases. However, these models also demonstrate a high false positive rate,necessitating continued involvement from manual auditors. The LLMs testedoutperform a random model by 20% in terms of F1-score. To ensure the integrity of our study, we conduct mutation testing on fivenewly developed and ostensibly secure smart contracts, into which we manuallyinsert two and 15 vulnerabilities each. This testing yielded a remarkablebest-case 78.7% true positive rate for the GPT-4-32k model. We tested both,asking the models to perform a binary classification on whether a contract isvulnerable, and a non-binary prompt. We also examined the influence of modeltemperature variations and context length on the LLM&rsquo;s performance. Despite the potential for many further enhancements, this work lays thegroundwork for a more efficient and economical approach to smart contractsecurity audits.</div></details><blockquote><p><strong><em>2023-06-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.07567v2><strong>Large Language Models Sometimes Generate Purely Negatively-Reinforced Text</strong></a></p><p><em>Fabien Roger</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: When using adversarial training, it is common practice to train against themost egregious failures. However, this might imply using examples withsensitive information (such as leaked passwords or security vulnerabilities) astraining data. One might assume that language models trained with gradientdescent never generate text snippets which were only present in examplesassociated with the lowest possible reward. In this paper, we show that thisassumption is wrong: in some situations, large language models do learn fromsuch negatively-reinforced examples. We present a specific training setup thatenables Pythia-160M to guess passwords 13% more often than it would by guessingrandomly, despite only showing it these passwords on examples where the modelis incentivized to not output these passwords. Our code is available atwww.github.com/FabienRoger/Learning-From-Negative-Examples</div></details><blockquote><p><strong><em>2023-06-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.06782v1><strong>Augmenting Greybox Fuzzing with Generative AI</strong></a></p><p><em>Jie Hu, Qian Zhang, Heng Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Real-world programs expecting structured inputs often has a format-parsingstage gating the deeper program space. Neither a mutation-based approach nor agenerative approach can provide a solution that is effective and scalable.Large language models (LLM) pre-trained with an enormous amount of naturallanguage corpus have proved to be effective for understanding the implicitformat syntax and generating format-conforming inputs. In this paper, proposeChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, wepick a seed in the fuzzer&rsquo;s seed pool and prompt ChatGPT generative models tovariations, which are more likely to be format-conforming and thus of highquality. We conduct extensive experiments to explore the best practice forharvesting the power of generative LLM models. The experiment results show thatour approach improves the edge coverage by 12.77% over the SOTA greybox fuzzer(AFL++) on 12 target programs from three well-tested benchmarks. As forvulnerability detection, \sys is able to perform similar to or better thanAFL++ for programs with explicit syntax rules but not for programs withnon-trivial syntax.</div></details><blockquote><p><strong><em>2023-06-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.05871v1><strong>Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?</strong></a></p><p><em>Wissam Antoun, Virginie Mouilleron, Beno√Æt Sagot, Djam√© Seddah</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in natural language processing (NLP) have led to thedevelopment of large language models (LLMs) such as ChatGPT. This paperproposes a methodology for developing and evaluating ChatGPT detectors forFrench text, with a focus on investigating their robustness on out-of-domaindata and against common attack schemes. The proposed method involvestranslating an English dataset into French and training a classifier on thetranslated data. Results show that the detectors can effectively detectChatGPT-generated text, with a degree of robustness against basic attacktechniques in in-domain settings. However, vulnerabilities are evident inout-of-domain contexts, highlighting the challenge of detecting adversarialtext. The study emphasizes caution when applying in-domain testing results to awider variety of content. We provide our translated datasets and models asopen-source resources. <a href=https://gitlab.inria.fr/wantoun/robust-chatgpt-detection>https://gitlab.inria.fr/wantoun/robust-chatgpt-detection</a></div></details><p><a href=http://arxiv.org/abs/2306.06199v1><strong>Reliability Check: An Analysis of GPT-3&rsquo;s Response to Sensitive Topics and Prompt Wording</strong></a></p><p><em>Aisha Khatun, Daniel G. Brown</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have become mainstream technology with theirversatile use cases and impressive performance. Despite the countlessout-of-the-box applications, LLMs are still not reliable. A lot of work isbeing done to improve the factual accuracy, consistency, and ethical standardsof these models through fine-tuning, prompting, and Reinforcement Learning withHuman Feedback (RLHF), but no systematic analysis of the responses of thesemodels to different categories of statements, or on their potentialvulnerabilities to simple prompting changes is available. In this work, weanalyze what confuses GPT-3: how the model responds to certain sensitive topicsand what effects the prompt wording has on the model response. We find thatGPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makesmistakes with common Misconceptions and Controversies. The model responses areinconsistent across prompts and settings, highlighting GPT-3&rsquo;s unreliability.Dataset and code of our analysis is available inhttps://github.com/tanny411/GPT3-Reliability-Check.</div></details><p><a href=http://arxiv.org/abs/2210.02871v3><strong>Self-Distillation for Further Pre-training of Transformers</strong></a></p><p><em>Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-training a large transformer model on a massive amount of unlabeled dataand fine-tuning it on labeled datasets for diverse downstream tasks has provento be a successful strategy, for a variety of vision and natural languageprocessing tasks. However, direct fine-tuning of the pre-trained model may besuboptimal if there exist large discrepancies across data domains forpre-training and fine-tuning. To tackle this issue, several previous studieshave proposed further pre-training strategies, where we continue to pre-trainthe model on the target unlabeled dataset before fine-tuning. However, all ofthem solely focus on language models and we empirically find that a VisionTransformer is vulnerable to overfitting as we continue to pretrain the modelon target unlabeled data. In order to tackle this limitation, we proposeself-distillation as a regularization for a further pre-training stage.Specifically, we first further pre-train the initial pre-trained model on thetarget unlabeled data and then consider it as a teacher for self-distillation.Then we take the same initial pre-trained model as a student and enforce itshidden representations to be close to those of the teacher while optimizing thestudent with a masked auto-encoding objective. We empirically validate theefficacy of self-distillation on a variety of benchmark datasets for image andtext classification tasks. Experimentally, we show that our proposed methodoutperforms all the relevant baselines. Theoretically, we analyze the proposedmethod with a simplified model to understand how self-distillation for furtherpre-training can potentially help improve the performance of the downstreamtasks.</div></details><blockquote><p><strong><em>2023-06-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.02546v1><strong>LmPa: Improving Decompilation by Synergy of Large Language Model and Program Analysis</strong></a></p><p><em>Xiangzhe Xu, Zhuo Zhang, Shiwei Feng, Yapeng Ye, Zian Su, Nan Jiang, Siyuan Cheng, Lin Tan, Xiangyu Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Decompilation aims to recover the source code form of a binary executable. Ithas many applications in security and software engineering such as malwareanalysis, vulnerability detection and code reuse. A prominent challenge indecompilation is to recover variable names. We propose a novel method thatleverages the synergy of large language model (LLM) and program analysis.Language models encode rich multi-modal knowledge, but its limited input sizeprevents providing sufficient global context for name recovery. We propose todivide the task to many LLM queries and use program analysis to correlate andpropagate the query results, which in turn improves the performance of LLM byproviding additional contextual information. Our results show that 75% of therecovered names are considered good by users and our technique outperforms thestate-of-the-art technique by 16.5% and 20.23% in precision and recall,respectively.</div></details><p><a href=http://arxiv.org/abs/2306.02612v1><strong>Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia</strong></a></p><p><em>Benjamin Kereopa-Yorke</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The escalating digitalisation of our lives and enterprises has led to aparallel growth in the complexity and frequency of cyber-attacks. Small andmedium-sized enterprises (SMEs), particularly in Australia, are experiencingincreased vulnerability to cyber threats, posing a significant challenge to thenation&rsquo;s cyber security landscape. Embracing transformative technologies suchas Artificial Intelligence (AI), Machine Learning (ML) and Large LanguageModels (LLMs) can potentially strengthen cyber security policies for AustralianSMEs. However, their practical application, advantages, and limitations remainunderexplored, with prior research mainly focusing on large corporations. Thisstudy aims to address this gap by providing a comprehensive understanding ofthe potential role of LLMs in enhancing cyber security policies for AustralianSMEs. Employing a mixed-methods study design, this research includes aliterature review, qualitative analysis of SME case studies, and a quantitativeassessment of LLM performance metrics in cyber security applications. Thefindings highlight the promising potential of LLMs across various performancecriteria, including relevance, accuracy, and applicability, though gaps remainin areas such as completeness and clarity. The study underlines the importanceof integrating human expertise with LLM technology and refining modeldevelopment to address these limitations. By proposing a robust conceptualframework guiding the effective adoption of LLMs, this research aims tocontribute to a safer and more resilient cyber environment for Australian SMEs,enabling sustainable growth and competitiveness in the digital era.</div></details><blockquote><p><strong><em>2023-06-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.10036v3><strong>Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark</strong></a></p><p><em>Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have demonstrated powerful capabilities in bothtext understanding and generation. Companies have begun to offer Embedding as aService (EaaS) based on these LLMs, which can benefit various natural languageprocessing (NLP) tasks for customers. However, previous studies have shown thatEaaS is vulnerable to model extraction attacks, which can cause significantlosses for the owners of LLMs, as training these models is extremely expensive.To protect the copyright of LLMs for EaaS, we propose an Embedding Watermarkmethod called EmbMarker that implants backdoors on embeddings. Our methodselects a group of moderate-frequency words from a general text corpus to forma trigger set, then selects a target embedding as the watermark, and inserts itinto the embeddings of texts containing trigger words as the backdoor. Theweight of insertion is proportional to the number of trigger words included inthe text. This allows the watermark backdoor to be effectively transferred toEaaS-stealer&rsquo;s model for copyright verification while minimizing the adverseimpact on the original embeddings&rsquo; utility. Our extensive experiments onvarious datasets show that our method can effectively protect the copyright ofEaaS models without compromising service quality.</div></details><blockquote><p><strong><em>2023-05-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.18607v1><strong>How Effective Are Neural Networks for Fixing Security Vulnerabilities</strong></a></p><p><em>Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, Sameena Shah</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Security vulnerability repair is a difficult task that is in dire need ofautomation. Two groups of techniques have shown promise: (1) large codelanguage models (LLMs) that have been pre-trained on source code for tasks suchas code completion, and (2) automated program repair (APR) techniques that usedeep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repaircapabilities of LLMs and DL-based APR models. The contributions include that we(1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder),four fine-tuned LLMs, and four DL-based APR techniques on two real-world Javavulnerability benchmarks (Vul4J and VJBench), (2) design code transformationsto address the training and test data overlapping threat to Codex, (3) create anew Java vulnerability repair benchmark VJBench, and its transformed versionVJBench-trans and (4) evaluate LLMs and APR techniques on the transformedvulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Javavulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities.(2) Fine-tuning with general APR data improves LLMs&rsquo; vulnerability-fixingcapabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fixmany Common Weakness Enumeration (CWE) types, such as CWE-325 Missingcryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes8.3 transformed vulnerabilities, outperforming all the other LLMs and APRmodels on transformed vulnerabilities. The results call for innovations toenhance automated Java vulnerability repair such as creating largervulnerability repair training data, tuning LLMs with such data, and applyingcode simplification transformation to facilitate vulnerability repair.</div></details><blockquote><p><strong><em>2023-05-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.14752v1><strong>A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification</strong></a></p><p><em>Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, Lucas C. Cordeiro</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper we present a novel solution that combines the capabilities ofLarge Language Models (LLMs) with Formal Verification strategies to verify andautomatically repair software vulnerabilities. Initially, we employ BoundedModel Checking (BMC) to locate the software vulnerability and derive acounterexample. The counterexample provides evidence that the system behavesincorrectly or contains a vulnerability. The counterexample that has beendetected, along with the source code, are provided to the LLM engine. Ourapproach involves establishing a specialized prompt language for conductingcode debugging and generation to understand the vulnerability&rsquo;s root cause andrepair the code. Finally, we use BMC to verify the corrected version of thecode generated by the LLM. As a proof of concept, we create ESBMC-AI based onthe Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trainedTransformer model, specifically gpt-3.5-turbo, to detect and fix errors in Cprograms. Our experimentation involved generating a dataset comprising 1000 Ccode samples, each consisting of 20 to 50 lines of code. Notably, our proposedmethod achieved an impressive success rate of up to 80% in repairing vulnerablecode encompassing buffer overflow and pointer dereference failures. We assertthat this automated approach can effectively incorporate into the softwaredevelopment lifecycle&rsquo;s continuous integration and deployment (CI/CD) process.</div></details><p><a href=http://arxiv.org/abs/2305.15336v1><strong>From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads</strong></a></p><p><em>P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, Sandeep K Shukla</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This research article critically examines the potential risks andimplications arising from the malicious utilization of large languagemodels(LLM), focusing specifically on ChatGPT and Google&rsquo;s Bard. Although theselarge language models have numerous beneficial applications, the misuse of thistechnology by cybercriminals for creating offensive payloads and tools is asignificant concern. In this study, we systematically generated implementablecode for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, andconduct a comparative analysis of its performance with Google&rsquo;s Bard. Ourexperimentation reveals that ChatGPT has the potential to enable attackers toaccelerate the operation of more targeted and sophisticated attacks.Additionally, the technology provides amateur attackers with more capabilitiesto perform a wide range of attacks and empowers script kiddies to developcustomized tools that contribute to the acceleration of cybercrime.Furthermore, LLMs significantly benefits malware authors, particularlyransomware gangs, in generating sophisticated variants of wiper and ransomwareattacks with ease. On a positive note, our study also highlights how offensivesecurity researchers and pentesters can make use of LLMs to simulate realisticattack scenarios, identify potential vulnerabilities, and better protectorganizations. Overall, we conclude by emphasizing the need for increasedvigilance in mitigating the risks associated with LLMs. This includesimplementing robust security measures, increasing awareness and educationaround the potential risks of this technology, and collaborating with securityexperts to stay ahead of emerging threats.</div></details><p><a href=http://arxiv.org/abs/2305.15594v1><strong>Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</strong></a></p><p><em>Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock&rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.</div></details><p><a href=http://arxiv.org/abs/2305.14784v1><strong>Anthropomorphization of AI: Opportunities and Risks</strong></a></p><p><em>Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Anthropomorphization is the tendency to attribute human-like traits tonon-human entities. It is prevalent in many social contexts &ndash; childrenanthropomorphize toys, adults do so with brands, and it is a literary device.It is also a versatile tool in science, with behavioral psychology andevolutionary biology meticulously documenting its consequences. With widespreadadoption of AI systems, and the push from stakeholders to make it human-likethrough alignment techniques, human voice, and pictorial avatars, the tendencyfor users to anthropomorphize it increases significantly. We take a dyadicapproach to understanding this phenomenon with large language models (LLMs) bystudying (1) the objective legal implications, as analyzed through the lens ofthe recent blueprint of AI bill of rights and the (2) subtle psychologicalaspects customization and anthropomorphization. We find that anthropomorphizedLLMs customized for different user bases violate multiple provisions in thelegislative blueprint. In addition, we point out that anthropomorphization ofLLMs affects the influence they can have on their users, thus having thepotential to fundamentally change the nature of human-AI interaction, withpotential for manipulation and negative influence. With LLMs beinghyper-personalized for vulnerable groups like children and patients amongothers, our work is a timely and important contribution. We propose aconservative strategy for the cautious use of anthropomorphization to improvetrustworthiness of AI systems.</div></details><blockquote><p><strong><em>2023-05-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.01754v1><strong>Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?</strong></a></p><p><em>Aaron Chan, Anant Kharkar, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Alec Helyar, Eslam Kamal, Mohamed Elkamhawy, Neel Sundaresan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Software vulnerabilities bear enterprises significant costs. Despiteextensive efforts in research and development of software vulnerabilitydetection methods, uncaught vulnerabilities continue to put software owners andusers at risk. Many current vulnerability detection methods require that codesnippets can compile and build before attempting detection. This,unfortunately, introduces a long latency between the time a vulnerability isinjected to the time it is removed, which can substantially increases the costof fixing a vulnerability. We recognize that the current advances in machinelearning can be used to detect vulnerable code patterns on syntacticallyincomplete code snippets as the developer is writing the code at EditTime. Inthis paper we present a practical system that leverages deep learning on alarge-scale data set of vulnerable code patterns to learn complexmanifestations of more than 250 vulnerability types and detect vulnerable codepatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuningapproaches on state of the art pre-trained Large Language Models (LLMs). Weshow that in comparison with state of the art vulnerability detection modelsour approach improves the state of the art by 10%. We also evaluate ourapproach to detect vulnerability in auto-generated code by code LLMs.Evaluation on a benchmark of high-risk code scenarios shows a reduction of upto 90% vulnerability reduction.</div></details><blockquote><p><strong><em>2023-05-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.06085v1><strong>FedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature</strong></a></p><p><em>Wenyuan Yang, Gongxi Zhu, Yuguo Yin, Hanlin Gu, Lixin Fan, Qiang Yang, Xiaochun Cao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning allows multiple parties to collaborate in learning aglobal model without revealing private data. The high cost of training and thesignificant value of the global model necessitates the need for ownershipverification of federated learning. However, the existing ownershipverification schemes in federated learning suffer from several limitations,such as inadequate support for a large number of clients and vulnerability toambiguity attacks. To address these limitations, we propose a cryptographicsignature-based federated learning model ownership verification scheme namedFedSOV. FedSOV allows numerous clients to embed their ownership credentials andverify ownership using unforgeable digital signatures. The scheme providestheoretical resistance to ambiguity attacks with the unforgeability of thesignature. Experimental results on computer vision and natural languageprocessing tasks demonstrate that FedSOV is an effective federated modelownership verification scheme enhanced with provable cryptographic security.</div></details><p><a href=http://arxiv.org/abs/2305.06055v1><strong>A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems</strong></a></p><p><em>Nicol√≤ Pagan, Joachim Baumann, Ezzat Elokda, Giulia De Pasquale, Saverio Bolognani, Anik√≥ Hann√°k</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prediction-based decision-making systems are becoming increasingly prevalentin various domains. Previous studies have demonstrated that such systems arevulnerable to runaway feedback loops, e.g., when police are repeatedly sentback to the same neighborhoods regardless of the actual rate of criminalactivity, which exacerbate existing biases. In practice, the automateddecisions have dynamic feedback effects on the system itself that canperpetuate over time, making it difficult for short-sighted design choices tocontrol the system&rsquo;s evolution. While researchers started proposing longer-termsolutions to prevent adverse outcomes (such as bias towards certain groups),these interventions largely depend on ad hoc modeling assumptions and arigorous theoretical understanding of the feedback dynamics in ML-baseddecision-making systems is currently missing. In this paper, we use thelanguage of dynamical systems theory, a branch of applied mathematics thatdeals with the analysis of the interconnection of systems with dynamicbehaviors, to rigorously classify the different types of feedback loops in theML-based decision-making pipeline. By reviewing existing scholarly work, weshow that this classification covers many examples discussed in the algorithmicfairness community, thereby providing a unifying and principled framework tostudy feedback loops. By qualitative analysis, and through a simulation exampleof recommender systems, we show which specific types of ML biases are affectedby each type of feedback loop. We find that the existence of feedback loops inthe ML-based decision-making pipeline can perpetuate, reinforce, or even reduceML biases.</div></details><blockquote><p><strong><em>2023-05-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.04067v1><strong>Reactive Perturbation Defocusing for Textual Adversarial Defense</strong></a></p><p><em>Heng Yang, Ke Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent studies have shown that large pre-trained language models arevulnerable to adversarial attacks. Existing methods attempt to reconstruct theadversarial examples. However, these methods usually have limited performancein defense against adversarial examples, while also negatively impacting theperformance on natural examples. To overcome this problem, we propose a methodcalled Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detectorto identify adversarial examples and reduce false defenses on natural examples.Instead of reconstructing the adversaries, RPD injects safe perturbations intoadversarial examples to distract the objective models from the maliciousperturbations. Our experiments on three datasets, two objective models, andvarious adversarial attacks show that our proposed framework successfullyrepairs up to approximately 97% of correctly identified adversarial exampleswith only about a 2% performance decrease on natural examples. We also providea demo of adversarial detection and repair based on our work.</div></details><blockquote><p><strong><em>2023-05-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.12173v2><strong>Not what you&rsquo;ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</strong></a></p><p><em>Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks&rsquo; practical viability againstboth real-world systems, such as Bing&rsquo;s GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication&rsquo;s functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.</div></details><blockquote><p><strong><em>2023-04-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.09655v1><strong>How Secure is Code Generated by ChatGPT?</strong></a></p><p><em>Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, large language models have been responsible for greatadvances in the field of artificial intelligence (AI). ChatGPT in particular,an AI chatbot developed and recently released by OpenAI, has taken the field tothe next level. The conversational model is able not only to process human-liketext, but also to translate natural language into code. However, the safety ofprograms generated by ChatGPT should not be overlooked. In this paper, weperform an experiment to address this issue. Specifically, we ask ChatGPT togenerate a number of program and evaluate the security of the resulting sourcecode. We further investigate whether ChatGPT can be prodded to improve thesecurity by appropriate prompts, and discuss the ethical aspects of using AI togenerate code. Results suggest that ChatGPT is aware of potentialvulnerabilities, but nonetheless often generates source code that are notrobust to certain attacks.</div></details><blockquote><p><strong><em>2023-04-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.02014v1><strong>Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT</strong></a></p><p><em>Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, Lingming Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep Learning (DL) library bugs affect downstream DL applications,emphasizing the need for reliable systems. Generating valid input programs forfuzzing DL libraries is challenging due to the need for satisfying bothlanguage syntax/semantics and constraints for constructing valid computationalgraphs. Recently, the TitanFuzz work demonstrates that modern Large LanguageModels (LLMs) can be directly leveraged to implicitly learn all the constraintsto generate valid DL programs for fuzzing. However, LLMs tend to generateordinary programs following similar patterns seen in their massive trainingcorpora, while fuzzing favors unusual inputs that cover edge cases or areunlikely to be manually produced. To fill this gap, this paper proposes FuzzGPT, the first technique to primeLLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on thewell-known hypothesis that historical bug-triggering programs may includerare/valuable code ingredients important for bug finding. Traditionaltechniques leveraging such historical information require intensive humanefforts to design dedicated generators and ensure the validity of generatedprograms. FuzzGPT demonstrates that this process can be fully automated via theintrinsic capabilities of LLMs (including fine-tuning and in-context learning),while being generalizable and applicable to challenging domains. While FuzzGPTcan be applied with different LLMs, this paper focuses on the powerfulGPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potentialof directly leveraging the instruct-following capability of the recent ChatGPTfor effective fuzzing. Evaluation on two popular DL libraries (PyTorch andTensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz,detecting 76 bugs, with 49 already confirmed as previously unknown bugs,including 11 high-priority bugs or security vulnerabilities.</div></details><blockquote><p><strong><em>2023-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.11455v1><strong>Large Language Models and Simple, Stupid Bugs</strong></a></p><p><em>Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advent of powerful neural language models, AI-based systems toassist developers in coding tasks are becoming widely available; Copilot is onesuch system. Copilot uses Codex, a large language model (LLM), to complete codeconditioned on a preceding &ldquo;prompt&rdquo;. Codex, however, is trained on publicGitHub repositories, viz., on code that may include bugs and vulnerabilities.Previous studies [1], [2] show Codex reproduces vulnerabilities seen intraining. In this study, we examine how prone Codex is to generate aninteresting bug category, single statement bugs, commonly referred to assimple, stupid bugs or SStuBs in the MSR community. We find that Codex andsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBsas much as 2x as likely than known, verbatim correct code. We explore theconsequences of the Codex generated SStuBs and propose avoidance strategiesthat suggest the possibility of reducing the production of known, verbatimSStubs, and increase the possibility of producing known, verbatim fixes.</div></details><blockquote><p><strong><em>2023-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.09384v1><strong>LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations</strong></a></p><p><em>Catherine Tony, Markus Mutas, Nicol√°s E. D√≠az Ferreyra, Riccardo Scandariato</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) like Codex are powerful tools for performingcode completion and code generation tasks as they are trained on billions oflines of code from publicly available sources. Moreover, these models arecapable of generating code snippets from Natural Language (NL) descriptions bylearning languages and programming practices from public GitHub repositories.Although LLMs promise an effortless NL-driven deployment of softwareapplications, the security of the code they generate has not been extensivelyinvestigated nor documented. In this work, we present LLMSecEval, a datasetcontaining 150 NL prompts that can be leveraged for assessing the securityperformance of such models. Such prompts are NL descriptions of code snippetsprone to various security vulnerabilities listed in MITRE&rsquo;s Top 25 CommonWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with asecure implementation example to facilitate comparative evaluations againstcode produced by LLMs. As a practical application, we show how LLMSecEval canbe used for evaluating the security of snippets automatically generated from NLdescriptions.</div></details><blockquote><p><strong><em>2023-03-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2301.12868v3><strong>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex</strong></a></p><p><em>Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Semantic parsing is a technique aimed at constructing a structuredrepresentation of the meaning of a natural-language question. Recentadvancements in few-shot language models trained on code have demonstratedsuperior performance in generating these representations compared totraditional unimodal language models, which are trained on downstream tasks.Despite these advancements, existing fine-tuned neural semantic parsers aresusceptible to adversarial attacks on natural-language inputs. While it hasbeen established that the robustness of smaller semantic parsers can beenhanced through adversarial training, this approach is not feasible for largelanguage models in real-world scenarios, as it requires both substantialcomputational resources and expensive human annotation on in-domain semanticparsing data. This paper presents the first empirical study on the adversarialrobustness of a large prompt-based language model of code, \codex. Our resultsdemonstrate that the state-of-the-art (SOTA) code-language models arevulnerable to carefully crafted adversarial examples. To address thischallenge, we propose methods for improving robustness without the need forsignificant amounts of labeled data or heavy computational resources.</div></details><blockquote><p><strong><em>2023-03-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.04247v1><strong>Vulnerability Mimicking Mutants</strong></a></p><p><em>Aayush Garg, Renzo Degiovanni, Mike Papadakis, Yves Le Traon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the increasing release of powerful language models trained on large codecorpus (e.g. CodeBERT was trained on 6.4 million programs), a new family ofmutation testing tools has arisen with the promise to generate more &ldquo;natural"mutants in the sense that the mutated code aims at following the implicit rulesand coding conventions typically produced by programmers. In this paper, westudy to what extent the mutants produced by language models can semanticallymimic the observable behavior of security-related vulnerabilities (a.k.a.Vulnerability-mimicking Mutants), so that designing test cases that are failedby these mutants will help in tackling mimicked vulnerabilities. Sinceanalyzing and running mutants is computationally expensive, it is important toprioritize those mutants that are more likely to be vulnerability mimickingprior to any analysis or test execution. Taking this into account, we introduceVMMS, a machine learning based approach that automatically extracts thefeatures from mutants and predicts the ones that mimic vulnerabilities. Weconducted our experiments on a dataset of 45 vulnerabilities and found that16.6% of the mutants fail one or more tests that are failed by 88.9% of therespective vulnerabilities. More precisely, 3.9% of the mutants from the entiremutant set are vulnerability-mimicking mutants that mimic 55.6% of thevulnerabilities. Despite the scarcity, VMMS predicts vulnerability-mimickingmutants with 0.63 MCC, 0.80 Precision, and 0.51 Recall, demonstrating that thefeatures of vulnerability-mimicking mutants can be automatically learned bymachine learning models to statically predict these without the need ofinvesting effort in defining such features.</div></details><blockquote><p><strong><em>2023-02-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.09727v4><strong>Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants</strong></a></p><p><em>Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, Brendan Dolan-Gavitt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) such as OpenAI Codex are increasingly being usedas AI-based coding assistants. Understanding the impact of these tools ondevelopers&rsquo; code is paramount, especially as recent work showed that LLMs maysuggest cybersecurity vulnerabilities. We conduct a security-driven user study(N=58) to assess code written by student programmers when assisted by LLMs.Given the potential severity of low-level bugs as well as their relativefrequency in real-world projects, we tasked participants with implementing asingly-linked &lsquo;shopping list&rsquo; structure in C. Our results indicate that thesecurity impact in this setting (low-level C with pointer and arraymanipulations) is small: AI-assisted users produce critical security bugs at arate no greater than 10% more than the control, indicating the use of LLMs doesnot introduce new security risks.</div></details><blockquote><p><strong><em>2023-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.11773v1><strong>Detecting software vulnerabilities using Language Models</strong></a></p><p><em>Marwan Omar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, deep learning techniques have garnered substantial attention fortheir ability to identify vulnerable code patterns accurately. However, currentstate-of-the-art deep learning models, such as Convolutional Neural Networks(CNN), and Long Short-Term Memories (LSTMs) require substantial computationalresources. This results in a level of overhead that makes their implementationunfeasible for deployment in realtime settings. This study presents a noveltransformer-based vulnerability detection framework, referred to as VulDetect,which is achieved through the fine-tuning of a pre-trained large languagemodel, (GPT) on various benchmark datasets of vulnerable code. Our empiricalfindings indicate that our framework is capable of identifying vulnerablesoftware code with an accuracy of up to 92.65%. Our proposed techniqueoutperforms SyseVR and VulDeBERT, two state-of-the-art vulnerability detectiontechniques</div></details><blockquote><p><strong><em>2023-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.10346v1><strong>Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain</strong></a></p><p><em>Kuntal Kumar Pal, Kazuaki Kashihara, Ujjwala Anantheswaran, Kirby C. Kuznia, Siddhesh Jagtap, Chitta Baral</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the increase in cybersecurity vulnerabilities of software systems, theways to exploit them are also increasing. Besides these, malware threats,irregular network interactions, and discussions about exploits in public forumsare also on the rise. To identify these threats faster, to detect potentiallyrelevant entities from any texts, and to be aware of software vulnerabilities,automated approaches are necessary. Application of natural language processing(NLP) techniques in the Cybersecurity domain can help in achieving this.However, there are challenges such as the diverse nature of texts involved inthe cybersecurity domain, the unavailability of large-scale publicly availabledatasets, and the significant cost of hiring subject matter experts forannotations. One of the solutions is building multi-task models that can betrained jointly with limited data. In this work, we introduce a generativemulti-task model, Unified Text-to-Text Cybersecurity (UTS), trained on malwarereports, phishing site URLs, programming code constructs, social media data,blogs, news articles, and public forum posts. We show UTS improves theperformance of some cybersecurity datasets. We also show that with a fewexamples, UTS can be adapted to novel unseen tasks and the nature of data</div></details><blockquote><p><strong><em>2023-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.03551v5><strong>Talking About Large Language Models</strong></a></p><p><em>Murray Shanahan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Thanks to rapid progress in artificial intelligence, we have entered an erawhen technology and philosophy intersect in interesting ways. Sitting squarelyat the centre of this intersection are large language models (LLMs). The moreadept LLMs become at mimicking human language, the more vulnerable we become toanthropomorphism, to seeing the systems in which they are embedded as morehuman-like than they really are. This trend is amplified by the naturaltendency to use philosophically loaded terms, such as &ldquo;knows&rdquo;, &ldquo;believes&rdquo;, and"thinks", when describing these systems. To mitigate this trend, this paperadvocates the practice of repeatedly stepping back to remind ourselves of howLLMs, and the systems of which they form a part, actually work. The hope isthat increased scientific precision will encourage more philosophical nuance inthe discourse around artificial intelligence, both within the field and in thepublic sphere.</div></details><blockquote><p><strong><em>2023-02-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.04116v1><strong>Training-free Lexical Backdoor Attacks on Language Models</strong></a></p><p><em>Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale language models have achieved tremendous success across variousnatural language processing (NLP) applications. Nevertheless, language modelsare vulnerable to backdoor attacks, which inject stealthy triggers into modelsfor steering them to undesirable behaviors. Most existing backdoor attacks,such as data poisoning, require further (re)training or fine-tuning languagemodels to learn the intended backdoor patterns. The additional training processhowever diminishes the stealthiness of the attacks, as training a languagemodel usually requires long optimization time, a massive amount of data, andconsiderable modifications to the model parameters. In this work, we proposeTraining-Free Lexical Backdoor Attack (TFLexAttack) as the first training-freebackdoor attack on language models. Our attack is achieved by injecting lexicaltriggers into the tokenizer of a language model via manipulating its embeddingdictionary using carefully designed rules. These rules are explainable to humandevelopers which inspires attacks from a wider range of hackers. The sparsemanipulation of the dictionary also habilitates the stealthiness of our attack.We conduct extensive experiments on three dominant NLP tasks based on ninelanguage models to demonstrate the effectiveness and universality of ourattack. The code of this work is available athttps://github.com/Jinxhy/TFLexAttack.</div></details><blockquote><p><strong><em>2023-01-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2301.08881v2><strong>Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness</strong></a></p><p><em>Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural text-to-SQL models have achieved remarkable performance in translatingnatural language questions into SQL queries. However, recent studies revealthat text-to-SQL models are vulnerable to task-specific perturbations. Previouscurated robustness test sets usually focus on individual phenomena. In thispaper, we propose a comprehensive robustness benchmark based on Spider, across-domain text-to-SQL benchmark, to diagnose the model robustness. We design17 perturbations on databases, natural language questions, and SQL queries tomeasure the robustness from different angles. In order to collect morediversified natural question perturbations, we utilize large pretrainedlanguage models (PLMs) to simulate human behaviors in creating naturalquestions. We conduct a diagnostic study of the state-of-the-art models on therobustness set. Experimental results reveal that even the most robust modelsuffers from a 14.0% performance drop overall and a 50.7% performance drop onthe most challenging perturbation. We also present a breakdown analysisregarding text-to-SQL model designs and provide insights for improving modelrobustness.</div></details><blockquote><p><strong><em>2022-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.08619v1><strong>Planting and Mitigating Memorized Content in Predictive-Text Language Models</strong></a></p><p><em>C. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both &ldquo;heuristic&rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define"sensitive" or &ldquo;private&rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.</div></details><blockquote><p><strong><em>2022-12-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.16235v2><strong>DCDetector: An IoT terminal vulnerability mining system based on distributed deep ensemble learning under source code representation</strong></a></p><p><em>Wen Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Context: The IoT system infrastructure platform facility vulnerability attackhas become the main battlefield of network security attacks. Most of thetraditional vulnerability mining methods rely on vulnerability detection toolsto realize vulnerability discovery. However, due to the inflexibility of toolsand the limitation of file size, its scalability It is relatively low andcannot be applied to large-scale power big data fields. Objective: The goal ofthe research is to intelligently detect vulnerabilities in source codes ofhigh-level languages such as C/C++. This enables us to propose a coderepresentation of sensitive sentence-related slices of source code, and todetect vulnerabilities by designing a distributed deep ensemble learning model.Method: In this paper, a new directional vulnerability mining method ofparallel ensemble learning is proposed to solve the problem of large-scale datavulnerability mining. By extracting sensitive functions and statements, asensitive statement library of vulnerable codes is formed. The AST stream-basedvulnerability code slice with higher granularity performs doc2vec sentencevectorization on the source code through the random sampling module, obtainsdifferent classification results through distributed training through theBi-LSTM trainer, and obtains the final classification result by voting.Results: This method designs and implements a distributed deep ensemblelearning system software vulnerability mining system called DCDetector. It canmake accurate predictions by using the syntactic information of the code, andis an effective method for analyzing large-scale vulnerability data.Conclusion: Experiments show that this method can reduce the false positiverate of traditional static analysis and improve the performance and accuracy ofmachine learning.</div></details><blockquote><p><strong><em>2022-11-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.12787v1><strong>Program Repair</strong></a></p><p><em>Xiang Gao, Yannic Noller, Abhik Roychoudhury</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automated program repair is an emerging technology which consists of a suiteof techniques to automatically fix bugs or vulnerabilities in programs. In thispaper, we present a comprehensive survey of the state of the art in programrepair. We first study the different suite of techniques used including searchbased repair, constraint based repair and learning based repair. We thendiscuss one of the main challenges in program repair namely patch overfitting,by distilling a class of techniques which can alleviate patch overfitting. Wethen discuss classes of program repair tools, applications of program repair aswell as uses of program repair in industry. We conclude the survey with aforward looking outlook on future usages of program repair, as well as researchopportunities arising from work on code from large language models.</div></details><blockquote><p><strong><em>2022-11-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.08318v2><strong>Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs</strong></a></p><p><em>Anna Filighera, Sebastian Ochs, Tim Steuer, Thomas Tregel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automatic grading models are valued for the time and effort saved during theinstruction of large student bodies. Especially with the increasingdigitization of education and interest in large-scale standardized testing, thepopularity of automatic grading has risen to the point where commercialsolutions are widely available and used. However, for short answer formats,automatic grading is challenging due to natural language ambiguity andversatility. While automatic short answer grading models are beginning tocompare to human performance on some datasets, their robustness, especially toadversarially manipulated data, is questionable. Exploitable vulnerabilities ingrading models can have far-reaching consequences ranging from cheatingstudents receiving undeserved credit to undermining automatic gradingaltogether - even when most predictions are valid. In this paper, we devise ablack-box adversarial attack tailored to the educational short answer gradingscenario to investigate the grading models&rsquo; robustness. In our attack, weinsert adjectives and adverbs into natural places of incorrect student answers,fooling the model into predicting them as correct. We observed a loss ofprediction accuracy between 10 and 22 percentage points using thestate-of-the-art models BERT and T5. While our attack made answers appear lessnatural to humans in our experiments, it did not significantly increase thegraders&rsquo; suspicions of cheating. Based on our experiments, we providerecommendations for utilizing automatic grading systems more safely inpractice.</div></details><blockquote><p><strong><em>2022-11-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.09527v1><strong>Ignore Previous Prompt: Attack Techniques For Language Models</strong></a></p><p><em>F√°bio Perez, Ian Ribeiro</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Transformer-based large language models (LLMs) provide a powerful foundationfor natural language tasks in large-scale customer-facing applications.However, studies that explore their vulnerabilities emerging from malicioususer interaction are scarce. By proposing PromptInject, a prosaic alignmentframework for mask-based iterative adversarial prompt composition, we examinehow GPT-3, the most widely deployed language model in production, can be easilymisaligned by simple handcrafted inputs. In particular, we investigate twotypes of attacks &ndash; goal hijacking and prompt leaking &ndash; and demonstrate thateven low-aptitude, but sufficiently ill-intentioned agents, can easily exploitGPT-3&rsquo;s stochastic nature, creating long-tail risks. The code for PromptInjectis available at <a href=https://github.com/agencyenterprise/PromptInject>https://github.com/agencyenterprise/PromptInject</a>.</div></details><blockquote><p><strong><em>2022-11-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.12506v2><strong>Memorization in NLP Fine-tuning Methods</strong></a></p><p><em>Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, Taylor Berg-Kirkpatrick</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the"pre-train and fine-tune" paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.</div></details><blockquote><p><strong><em>2022-10-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.17238v1><strong>Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task</strong></a></p><p><em>Nyoungwoo Lee, ChaeHun Park, Ho-Jin Choi, Jaegul Choo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In retrieval-based dialogue systems, a response selection model acts as aranker to select the most appropriate response among several candidates.However, such selection models tend to rely on context-response contentsimilarity, which makes models vulnerable to adversarial responses that aresemantically similar but not relevant to the dialogue context. Recent studieshave shown that leveraging these adversarial responses as negative trainingsamples is useful for improving the discriminating power of the selectionmodel. Nevertheless, collecting human-written adversarial responses isexpensive, and existing synthesizing methods often have limited scalability. Toovercome these limitations, this paper proposes a simple but efficient methodfor generating adversarial negative responses leveraging a large-scale languagemodel. Experimental results on dialogue selection tasks show that our methodoutperforms other methods of synthesizing adversarial negative responses. Theseresults suggest that our method can be an effective alternative to humanannotators in generating adversarial responses. Our dataset and generation codeis available at <a href=https://github.com/leenw23/generating-negatives-by-gpt3>https://github.com/leenw23/generating-negatives-by-gpt3</a>.</div></details><blockquote><p><strong><em>2022-10-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.14970v1><strong>Identifying Crisis Response Communities in Online Social Networks for Compound Disasters: The Case of Hurricane Laura and Covid-19</strong></a></p><p><em>Khondhaker Al Momin, H M Imran Kays, Arif Mohaimin Sadri</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Online social networks allow different agencies and the public to interactand share the underlying risks and protective actions during major disasters.This study revealed such crisis communication patterns during hurricane Lauracompounded by the COVID-19 pandemic. Laura was one of the strongest (Category4) hurricanes on record to make landfall in Cameron, Louisiana. Using theApplication Programming Interface (API), this study utilizes large-scale socialmedia data obtained from Twitter through the recently released academic trackthat provides complete and unbiased observations. The data captured publiclyavailable tweets shared by active Twitter users from the vulnerable areasthreatened by Laura. Online social networks were based on user influencefeature ( mentions or tags) that allows notifying other users while posting atweet. Using network science theories and advanced community detectionalgorithms, the study split these networks into twenty-one components ofvarious sizes, the largest of which contained eight well-defined communities.Several natural language processing techniques (i.e., word clouds, bigrams,topic modeling) were applied to the tweets shared by the users in thesecommunities to observe their risk-taking or risk-averse behavior during a majorcompounding crisis. Social media accounts of local news media, radio,universities, and popular sports pages were among those who involved heavilyand interacted closely with local residents. In contrast, emergency managementand planning units in the area engaged less with the public. The findings ofthis study provide novel insights into the design of efficient social mediacommunication guidelines to respond better in future disasters.</div></details><blockquote><p><strong><em>2022-10-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.09545v1><strong>Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models</strong></a></p><p><em>Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks.In Natural Language Processing (NLP), DNNs are often backdoored during thefine-tuning process of a large-scale Pre-trained Language Model (PLM) withpoisoned samples. Although the clean weights of PLMs are readily available,existing methods have ignored this information in defending NLP models againstbackdoor attacks. In this work, we take the first step to exploit thepre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned languagemodels. Specifically, we leverage the clean pre-trained weights via twocomplementary techniques: (1) a two-step Fine-mixing technique, which firstmixes the backdoored weights (fine-tuned on poisoned data) with the pre-trainedweights, then fine-tunes the mixed weights on a small subset of clean data; (2)an Embedding Purification (E-PUR) technique, which mitigates potentialbackdoors existing in the word embeddings. We compare Fine-mixing with typicalbackdoor mitigation methods on three single-sentence sentiment classificationtasks and two sentence-pair classification tasks and show that it outperformsthe baselines by a considerable margin in all scenarios. We also show that ourE-PUR method can benefit existing mitigation methods. Our work establishes asimple but strong baseline defense for secure fine-tuned NLP models againstbackdoor attacks.</div></details><blockquote><p><strong><em>2022-09-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.03214v2><strong>Transformer-Based Language Models for Software Vulnerability Detection</strong></a></p><p><em>Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, Surya Nepal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The large transformer-based language models demonstrate excellent performancein natural language processing. By considering the transferability of theknowledge gained by these models in one domain to other related domains, andthe closeness of natural languages to high-level programming languages, such asC/C++, this work studies how to leverage (large) transformer-based languagemodels in detecting software vulnerabilities and how good are these models forvulnerability detection tasks. In this regard, firstly, a systematic (cohesive)framework that details source code translation, model preparation, andinference is presented. Then, an empirical analysis is performed with softwarevulnerability datasets with C/C++ source codes having multiple vulnerabilitiescorresponding to the library function call, pointer usage, array usage, andarithmetic expression. Our empirical results demonstrate the good performanceof the language models in vulnerability detection. Moreover, these languagemodels have better performance metrics, such as F1-score, than the contemporarymodels, namely bidirectional long short-term memory and bidirectional gatedrecurrent unit. Experimenting with the language models is always challengingdue to the requirement of computing resources, platforms, libraries, anddependencies. Thus, this paper also analyses the popular platforms toefficiently fine-tune these models and present recommendations while choosingthe platforms.</div></details><blockquote><p><strong><em>2022-09-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.02128v1><strong>Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples</strong></a></p><p><em>Hezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, Ramesh Darwishi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in the development of large language models have resulted inpublic access to state-of-the-art pre-trained language models (PLMs), includingGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional EncoderRepresentations from Transformers (BERT). However, evaluations of PLMs, inpractice, have shown their susceptibility to adversarial attacks during thetraining and fine-tuning stages of development. Such attacks can result inerroneous outputs, model-generated hate speech, and the exposure of users&rsquo;sensitive information. While existing research has focused on adversarialattacks during either the training or the fine-tuning of PLMs, there is adeficit of information on attacks made between these two development phases. Inthis work, we highlight a major security vulnerability in the public release ofGPT-3 and further investigate this vulnerability in other state-of-the-artPLMs. We restrict our work to pre-trained models that have not undergonefine-tuning. Further, we underscore token distance-minimized perturbations asan effective adversarial approach, bypassing both supervised and unsupervisedquality measures. Following this approach, we observe a significant decrease intext classification quality when evaluating for semantic similarity.</div></details><blockquote><p><strong><em>2022-09-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.00599v1><strong>Why Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?</strong></a></p><p><em>Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, Jaesik Choi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Many contextualized word representations are now learned by intricate neuralnetwork models, such as masked neural language models (MNLMs) which are made upof huge neural network structures and trained to restore the masked text. Suchrepresentations demonstrate superhuman performance in some readingcomprehension (RC) tasks which extract a proper answer in the context given aquestion. However, identifying the detailed knowledge trained in MNLMs ischallenging owing to numerous and intermingled model parameters. This paperprovides new insights and empirical analyses on commonsense knowledge includedin pretrained MNLMs. First, we use a diagnostic test that evaluates whethercommonsense knowledge is properly trained in MNLMs. We observe that a largeproportion of commonsense knowledge is not appropriately trained in MNLMs andMNLMs do not often understand the semantic meaning of relations accurately. Inaddition, we find that the MNLM-based RC models are still vulnerable tosemantic variations that require commonsense knowledge. Finally, we discoverthe fundamental reason why some knowledge is not trained. We further suggestthat utilizing an external commonsense knowledge repository can be an effectivesolution. We exemplify the possibility to overcome the limitations of theMNLM-based RC models by enriching text with the required knowledge from anexternal commonsense knowledge repository in controlled experiments.</div></details><blockquote><p><strong><em>2022-08-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.02125v3><strong>Examining Zero-Shot Vulnerability Repair with Large Language Models</strong></a></p><p><em>Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human developers can produce code with cybersecurity bugs. Can emerging&rsquo;smart&rsquo; code completion tools help repair those bugs? In this work, we examinethe use of large language models (LLMs) for code (such as OpenAI&rsquo;s Codex andAI21&rsquo;s Jurassic J-1) for zero-shot vulnerability repair. We investigatechallenges in the design of prompts that coax LLMs into generating repairedversions of insecure code. This is difficult due to the numerous ways to phrasekey information - both semantically and syntactically - with natural languages.We perform a large scale study of five commercially available, black-box,&ldquo;off-the-shelf&rdquo; LLMs, as well as an open-source model and our ownlocally-trained model, on a mix of synthetic, hand-crafted, and real-worldsecurity bug scenarios. Our experiments demonstrate that while the approach haspromise (the LLMs could collectively repair 100% of our synthetically generatedand hand-crafted scenarios), a qualitative evaluation of the model&rsquo;sperformance over a corpus of historical real-world examples highlightschallenges in generating functionally correct code.</div></details><blockquote><p><strong><em>2022-08-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.01844v1><strong>Multiclass ASMA vs Targeted PGD Attack in Image Segmentation</strong></a></p><p><em>Johnson Vo, Jiabao Xie, Sahil Patel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning networks have demonstrated high performance in a large varietyof applications, such as image classification, speech recognition, and naturallanguage processing. However, there exists a major vulnerability exploited bythe use of adversarial attacks. An adversarial attack imputes images byaltering the input image very slightly, making it nearly undetectable to thenaked eye, but results in a very different classification by the network. Thispaper explores the projected gradient descent (PGD) attack and the AdaptiveMask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model usingtwo types of architectures: MobileNetV3 and ResNet50, It was found that PGD wasvery consistent in changing the segmentation to be its target while thegeneralization of ASMA to a multiclass target was not as effective. Theexistence of such attack however puts all of image classification deep learningnetworks in danger of exploitation.</div></details><blockquote><p><strong><em>2022-07-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.10251v1><strong>Rethinking Textual Adversarial Defense for Pre-trained Language Models</strong></a></p><p><em>Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although pre-trained language models (PrLMs) have achieved significantsuccess, recent studies demonstrate that PrLMs are vulnerable to adversarialattacks. By generating adversarial examples with slight perturbations ondifferent levels (sentence / word / character), adversarial attacks can foolPrLMs to generate incorrect predictions, which questions the robustness ofPrLMs. However, we find that most existing textual adversarial examples areunnatural, which can be easily distinguished by both human and machine. Basedon a general anomaly detector, we propose a novel metric (Degree of Anomaly) asa constraint to enable current adversarial attack approaches to generate morenatural and imperceptible adversarial examples. Under this new constraint, thesuccess rate of existing attacks drastically decreases, which reveals that therobustness of PrLMs is not as fragile as they claimed. In addition, we findthat four types of randomization can invalidate a large portion of textualadversarial examples. Based on anomaly detector and randomization, we design auniversal defense framework, which is among the first to perform textualadversarial defense without knowing the specific attack. Empirical results showthat our universal defense framework achieves comparable or even higherafter-attack accuracy with other specific defenses, while preserving higheroriginal accuracy at the same time. Our work discloses the essence of textualadversarial attacks, and indicates that (1) further works of adversarialattacks should focus more on how to overcome the detection and resist therandomization, otherwise their adversarial examples would be easily detectedand invalidated; and (2) compared with the unnatural and perceptibleadversarial examples, it is those undetectable adversarial examples that posereal risks for PrLMs and require more attention for future robustness-enhancingstrategies.</div></details><blockquote><p><strong><em>2022-06-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.01287v3><strong>SemAttack: Natural Textual Attacks via Different Semantic Spaces</strong></a></p><p><em>Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent studies show that pre-trained language models (LMs) are vulnerable totextual adversarial attacks. However, existing attack methods either sufferfrom low attack success rates or fail to search efficiently in theexponentially large perturbation space. We propose an efficient and effectiveframework SemAttack to generate natural adversarial text by constructingdifferent semantic perturbation functions. In particular, SemAttack optimizesthe generated perturbations constrained on generic semantic spaces, includingtypo space, knowledge space (e.g., WordNet), contextualized semantic space(e.g., the embedding space of BERT clusterings), or the combination of thesespaces. Thus, the generated adversarial texts are more semantically close tothe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) arestill vulnerable to SemAttack. We further demonstrate that SemAttack is generaland able to generate natural adversarial texts for different languages (e.g.,English and Chinese) with high attack success rates. Human evaluations alsoconfirm that our generated adversarial texts are natural and barely affecthuman performance. Our code is publicly available athttps://github.com/AI-secure/SemAttack.</div></details><blockquote><p><strong><em>2022-05-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.12713v1><strong>jTrans: Jump-Aware Transformer for Binary Code Similarity</strong></a></p><p><em>Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, Chao Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Binary code similarity detection (BCSD) has important applications in variousfields such as vulnerability detection, software component analysis, andreverse engineering. Recent studies have shown that deep neural networks (DNNs)can comprehend instructions or control-flow graphs (CFG) of binary code andsupport BCSD. In this study, we propose a novel Transformer-based approach,namely jTrans, to learn representations of binary code. It is the firstsolution that embeds control flow information of binary code intoTransformer-based language models, by using a novel jump-aware representationof the analyzed binaries and a newly-designed pre-training task. Additionally,we release to the community a newly-created large dataset of binaries,BinaryCorp, which is the most diverse to date. Evaluation results show thatjTrans outperforms state-of-the-art (SOTA) approaches on this more challengingdataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of knownvulnerability searching, jTrans achieves a recall that is 2X higher thanexisting SOTA baselines.</div></details><blockquote><p><strong><em>2022-04-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.07668v3><strong>Dual-Key Multimodal Backdoors for Visual Question Answering</strong></a></p><p><em>Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The success of deep learning has enabled advances in multimodal tasks thatrequire non-trivial fusion of multiple input domains. Although multimodalmodels have shown potential in many problems, their increased complexity makesthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class ofsecurity vulnerability wherein an attacker embeds a malicious secret behaviorinto a network (e.g. targeted misclassification) that is activated when anattacker-specified trigger is added to an input. In this work, we show thatmultimodal networks are vulnerable to a novel type of attack that we refer toas Dual-Key Multimodal Backdoors. This attack exploits the complex fusionmechanisms used by state-of-the-art networks to embed backdoors that are botheffective and stealthy. Instead of using a single trigger, the proposed attackembeds a trigger in each of the input modalities and activates the maliciousbehavior only when both the triggers are present. We present an extensive studyof multimodal backdoors on the Visual Question Answering (VQA) task withmultiple architectures and visual feature backbones. A major challenge inembedding backdoors in VQA models is that most models use visual featuresextracted from a fixed pretrained object detector. This is challenging for theattacker as the detector can distort or ignore the visual trigger entirely,which leads to models where backdoors are over-reliant on the language trigger.We tackle this problem by proposing a visual trigger optimization strategydesigned for pretrained object detectors. Through this method, we createDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%of the training data. Finally, we release TrojVQA, a large collection of cleanand trojan VQA models to enable research in defending against multimodalbackdoors.</div></details><blockquote><p><strong><em>2022-03-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.10714v1><strong>A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement</strong></a></p><p><em>Yuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, Jian Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent years have seen the wide application of NLP models in crucial areassuch as finance, medical treatment, and news media, raising concerns of themodel robustness and vulnerabilities. In this paper, we propose a novelprompt-based adversarial attack to compromise NLP models and robustnessenhancement technique. We first construct malicious prompts for each instanceand generate adversarial examples via mask-and-filling under the effect of amalicious purpose. Our attack technique targets the inherent vulnerabilities ofNLP models, allowing us to generate samples even without interacting with thevictim NLP model, as long as it is based on pre-trained language models (PLMs).Furthermore, we design a prompt-based adversarial training method to improvethe robustness of PLMs. As our training method does not actually generateadversarial samples, it can be applied to large-scale training setsefficiently. The experimental results show that our attack method can achieve ahigh attack success rate with more diverse, fluent and natural adversarialexamples. In addition, our robustness enhancement method can significantlyimprove the robustness of models to resist adversarial attacks. Our workindicates that prompting paradigm has great potential in probing somefundamental flaws of PLMs and fine-tuning them for downstream tasks.</div></details><blockquote><p><strong><em>2022-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.02600v3><strong>Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning</strong></a></p><p><em>Seanie Lee, Hae Beom Lee, Juho Lee, Sung Ju Hwang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multilingual models jointly pretrained on multiple languages have achievedremarkable performance on various multilingual downstream tasks. Moreover,models finetuned on a single monolingual downstream task have shown togeneralize to unseen languages. In this paper, we first show that it is crucialfor those tasks to align gradients between them in order to maximize knowledgetransfer while minimizing negative transfer. Despite its importance, theexisting methods for gradient alignment either have a completely differentpurpose, ignore inter-task alignment, or aim to solve continual learningproblems in rather inefficient ways. As a result of the misaligned gradientsbetween tasks, the model suffers from severe negative transfer in the form ofcatastrophic forgetting of the knowledge acquired from the pretraining. Toovercome the limitations, we propose a simple yet effective method that canefficiently align gradients between tasks. Specifically, we perform eachinner-optimization by sequentially sampling batches from all the tasks,followed by a Reptile outer update. Thanks to the gradients aligned betweentasks by our method, the model becomes less vulnerable to negative transfer andcatastrophic forgetting. We extensively validate our method on variousmulti-task learning and zero-shot cross-lingual transfer tasks, where ourmethod largely outperforms all the relevant baselines we consider.</div></details><blockquote><p><strong><em>2022-01-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.02840v2><strong>Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models</strong></a></p><p><em>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale pre-trained language models have achieved tremendous successacross a wide range of natural language understanding (NLU) tasks, evensurpassing human performance. However, recent studies reveal that therobustness of these models can be challenged by carefully crafted textualadversarial examples. While several individual datasets have been proposed toevaluate model robustness, a principled and comprehensive benchmark is stillmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-taskbenchmark to quantitatively and thoroughly explore and evaluate thevulnerabilities of modern large-scale language models under various types ofadversarial attacks. In particular, we systematically apply 14 textualadversarial attack methods to GLUE tasks to construct AdvGLUE, which is furthervalidated by humans for reliable annotations. Our findings are summarized asfollows. (i) Most existing adversarial attack algorithms are prone togenerating invalid or ambiguous adversarial examples, with around 90% of themeither changing the original semantic meanings or misleading human annotatorsas well. Therefore, we perform a careful filtering process to curate ahigh-quality benchmark. (ii) All the language models and robust trainingmethods we tested perform poorly on AdvGLUE, with scores lagging far behind thebenign accuracy. We hope our work will motivate the development of newadversarial attacks that are more stealthy and semantic-preserving, as well asnew robust language models against sophisticated adversarial attacks. AdvGLUEis available at <a href=https://adversarialglue.github.io>https://adversarialglue.github.io</a>.</div></details><blockquote><p><strong><em>2022-01-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.00971v1><strong>Submix: Practical Private Prediction for Large-Scale Language Models</strong></a></p><p><em>Antonio Ginart, Laurens van der Maaten, James Zou, Chuan Guo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model&rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.</div></details><blockquote><p><strong><em>2021-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.08554v1><strong>A Deep Learning Approach for Ontology Enrichment from Unstructured Text</strong></a></p><p><em>Lalit Mohan Sanagavarapu, Vivek Iyer, Raghu Reddy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Information Security in the cyber world is a major cause for concern, with asignificant increase in the number of attack surfaces. Existing information onvulnerabilities, attacks, controls, and advisories available on the webprovides an opportunity to represent knowledge and perform security analyticsto mitigate some of the concerns. Representing security knowledge in the formof ontology facilitates anomaly detection, threat intelligence, reasoning andrelevance attribution of attacks, and many more. This necessitates dynamic andautomated enrichment of information security ontologies. However, existingontology enrichment algorithms based on natural language processing and MLmodels have issues with contextual extraction of concepts in words, phrases,and sentences. This motivates the need for sequential Deep Learningarchitectures that traverse through dependency paths in text and extractembedded vulnerabilities, threats, controls, products, and othersecurity-related concepts and instances from learned path representations. Inthe proposed approach, Bidirectional LSTMs trained on a large DBpedia datasetand Wikipedia corpus of 2.8 GB along with Universal Sentence Encoder isdeployed to enrich ISO 27001-based information security ontology. The model istrained and tested on a high-performance computing (HPC) environment to handleWiki text dimensionality. The approach yielded a test accuracy of over 80% whentested with knocked-out concepts from ontology and web page instances tovalidate the robustness.</div></details><blockquote><p><strong><em>2021-12-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.09898v2><strong>Bad Characters: Imperceptible NLP Attacks</strong></a></p><p><em>Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Several years of research have shown that machine-learning systems arevulnerable to adversarial examples, both in theory and in practice. Until now,such attacks have primarily targeted visual models, exploiting the gap betweenhuman and machine perception. Although text-based models have also beenattacked with adversarial examples, such attacks struggled to preserve semanticmeaning and indistinguishability. In this paper, we explore a large class ofadversarial examples that can be used to attack text-based models in ablack-box setting without making any human-perceptible visual modification toinputs. We use encoding-specific perturbations that are imperceptible to thehuman eye to manipulate the outputs of a wide range of Natural LanguageProcessing (NLP) systems from neural machine-translation pipelines to websearch engines. We find that with a single imperceptible encoding injection &ndash;representing one invisible character, homoglyph, reordering, or deletion &ndash; anattacker can significantly reduce the performance of vulnerable models, andwith three injections most models can be functionally broken. Our attacks workagainst currently-deployed commercial systems, including those produced byMicrosoft and Google, in addition to open source models published by Facebook,IBM, and HuggingFace. This novel series of attacks presents a significantthreat to many language processing systems: an attacker can affect systems in atargeted manner without any assumptions about the underlying model. We concludethat text-based NLP systems require careful input sanitization, just likeconventional applications, and that given such systems are now being deployedrapidly at scale, the urgent attention of architects and operators is required.</div></details><blockquote><p><strong><em>2021-12-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.02782v2><strong>How BPE Affects Memorization in Transformers</strong></a></p><p><em>Eugene Kharitonov, Marco Baroni, Dieuwke Hupkes</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Training data memorization in NLP can both be beneficial (e.g., closed-bookQA) and undesirable (personal data extraction). In any case, successful modeltraining requires a non-trivial amount of memorization to store word spellings,various linguistic idiosyncrasies and common knowledge. However, little isknown about what affects the memorization behavior of NLP models, as the fieldtends to focus on the equally important question of generalization. In thiswork, we demonstrate that the size of the subword vocabulary learned byByte-Pair Encoding (BPE) greatly affects both ability and tendency of standardTransformer models to memorize training data, even when we control for thenumber of learned parameters. We find that with a large subword vocabularysize, Transformer models fit random mappings more easily and are morevulnerable to membership inference attacks. Similarly, given a prompt,Transformer-based language models with large subword vocabularies reproduce thetraining data more often. We conjecture this effect is caused by reduction inthe sequences&rsquo; length that happens as the BPE vocabulary grows. Our findingscan allow a more informed choice of hyper-parameters, that is better tailoredfor a particular use-case.</div></details><blockquote><p><strong><em>2021-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.00213v3><strong>Adversarial Robustness of Deep Code Comment Generation</strong></a></p><p><em>Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, Harald Gall</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural networks (DNNs) have shown remarkable performance in a variety ofdomains such as computer vision, speech recognition, or natural languageprocessing. Recently they also have been applied to various softwareengineering tasks, typically involving processing source code. DNNs arewell-known to be vulnerable to adversarial examples, i.e., fabricated inputsthat could lead to various misbehaviors of the DNN model while being perceivedas benign by humans. In this paper, we focus on the code comment generationtask in software engineering and study the robustness issue of the DNNs whenthey are applied to this task. We propose ACCENT, an identifier substitutionapproach to craft adversarial code snippets, which are syntactically correctand semantically close to the original code snippet, but may mislead the DNNsto produce completely irrelevant code comments. In order to improve therobustness, ACCENT also incorporates a novel training method, which can beapplied to existing code comment generation models. We conduct comprehensiveexperiments to evaluate our approach by attacking the mainstreamencoder-decoder architectures on two large-scale publicly available datasets.The results show that ACCENT efficiently produces stable attacks withfunctionality-preserving adversarial examples, and the generated examples havebetter transferability compared with baselines. We also confirm, viaexperiments, the effectiveness in improving model robustness with our trainingmethod.</div></details><blockquote><p><strong><em>2021-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.02229v3><strong>Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack</strong></a></p><p><em>Shengcai Liu, Ning Lu, Cheng Chen, Ke Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Over the past few years, various word-level textual attack approaches havebeen proposed to reveal the vulnerability of deep neural networks used innatural language processing. Typically, these approaches involve an importantoptimization step to determine which substitute to be used for each word in theoriginal input. However, current research on this step is still rather limited,from the perspectives of both problem-understanding and problem-solving. Inthis paper, we address these issues by uncovering the theoretical properties ofthe problem and proposing an efficient local search algorithm (LS) to solve it.We establish the first provable approximation guarantee on solving the problemin general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26NLP models show that LS can largely reduce the number of queries usually by anorder of magnitude to achieve high attack success rates. Further experimentsshow that the adversarial examples crafted by LS usually have higher quality,exhibit better transferability, and can bring more robustness improvement tovictim models by adversarial training.</div></details><blockquote><p><strong><em>2021-10-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.09635v1><strong>A ground-truth dataset of real security patches</strong></a></p><p><em>Sofia Reis, Rui Abreu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Training machine learning approaches for vulnerability identification andproducing reliable tools to assist developers in implementing quality software&ndash; free of vulnerabilities &ndash; is challenging due to the lack of large datasetsand real data. Researchers have been looking at these issues and buildingdatasets. However, these datasets usually miss natural language artifacts andprogramming language diversity. We scraped the entire CVE details database forGitHub references and augmented the data with 3 security-related datasets. Weused the data to create a ground-truth dataset of natural language artifacts(such as commit messages, commits comments, and summaries), meta-data and codechanges. Our dataset integrates a total of 8057 security-relevant commits &ndash;the equivalent to 5942 security patches &ndash; from 1339 different projectsspanning 146 different types of vulnerabilities and 20 languages. A dataset of110k non-security-related commits is also provided. Data and scripts are allavailable on GitHub. Data is stored in a .CSV file. Codebases can be downloadedusing our scripts. Our dataset is a valuable asset to answer research questionson different topics such as the identification of security-relevant informationusing NLP models; software engineering and security best practices; and,vulnerability detection and patching; and, security program analysis.</div></details><blockquote><p><strong><em>2021-09-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.12406v1><strong>MINIMAL: Mining Models for Data Free Universal Adversarial Triggers</strong></a></p><p><em>Swapnil Parekh, Yaman Singla Kumar, Somesh Singh, Changyou Chen, Balaji Krishnamurthy, Rajiv Ratn Shah</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: It is well known that natural language models are vulnerable to adversarialattacks, which are mostly input-specific in nature. Recently, it has been shownthat there also exist input-agnostic attacks in NLP models, called universaladversarial triggers. However, existing methods to craft universal triggers aredata intensive. They require large amounts of data samples to generateadversarial triggers, which are typically inaccessible by attackers. Forinstance, previous works take 3000 data samples per class for the SNLI datasetto generate adversarial triggers. In this paper, we present a novel data-freeapproach, MINIMAL, to mine input-agnostic adversarial triggers from models.Using the triggers produced with our data-free algorithm, we reduce theaccuracy of Stanford Sentiment Treebank&rsquo;s positive class from 93.6% to 9.6%.Similarly, for the Stanford Natural Language Inference (SNLI), our single-wordtrigger reduces the accuracy of the entailment class from 90.95% to less than0.6%. Despite being completely data-free, we get equivalent accuracy drops asdata-dependent methods.</div></details><blockquote><p><strong><em>2021-09-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.14583v3><strong>Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages</strong></a></p><p><em>Nay San, Martijn Bartelds, Mitchell Browne, Lily Clifford, Fiona Gibson, John Mansfield, David Nash, Jane Simpson, Myfany Turpin, Maria Vollmer, Sasha Wilmoth, Dan Jurafsky</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained speech representations like wav2vec 2.0 are a powerful tool forautomatic speech recognition (ASR). Yet many endangered languages lacksufficient data for pre-training such models, or are predominantly oralvernaculars without a standardised writing system, precluding fine-tuning.Query-by-example spoken term detection (QbE-STD) offers an alternative foriteratively indexing untranscribed speech corpora by locating spoken queryterms. Using data from 7 Australian Aboriginal languages and a regional varietyof Dutch, all of which are endangered or vulnerable, we show that QbE-STD canbe improved by leveraging representations developed for ASR (wav2vec 2.0: theEnglish monolingual model and XLSR53 multilingual model). Surprisingly, theEnglish model outperformed the multilingual model on 4 Australian languagedatasets, raising questions around how to optimally leverage self-supervisedspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0representations (either English or XLSR53) offer large improvements (56-86%relative) over state-of-the-art approaches on our endangered language datasets.</div></details><blockquote><p><strong><em>2021-09-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.00157v4><strong>Active Learning Under Malicious Mislabeling and Poisoning Attacks</strong></a></p><p><em>Jing Lin, Ryan Luley, Kaiqi Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural networks usually require large labeled datasets for training toachieve state-of-the-art performance in many tasks, such as imageclassification and natural language processing. Although a lot of data iscreated each day by active Internet users, most of these data are unlabeled andare vulnerable to data poisoning attacks. In this paper, we develop anefficient active learning method that requires fewer labeled instances andincorporates the technique of adversarial retraining in which additionallabeled artificial data are generated without increasing the budget of thelabeling. The generated adversarial examples also provide a way to measure thevulnerability of the model. To check the performance of the proposed methodunder an adversarial setting, i.e., malicious mislabeling and data poisoningattacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset,which contains only two classes: airplane and frog. Our experimental resultsdemonstrate that the proposed active learning method is efficient for defendingagainst malicious mislabeling and data poisoning attacks. Specifically, whereasthe baseline active learning method based on the random sampling strategyperforms poorly (about 50%) under a malicious mislabeling attack, the proposedactive learning method can achieve the desired accuracy of 89% using onlyone-third of the dataset on average.</div></details><blockquote><p><strong><em>2021-08-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.04907v2><strong>Disentangled Contrastive Learning for Learning Robust Textual Representations</strong></a></p><p><em>Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang, Huajun Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although the self-supervised pre-training of transformer models has resultedin the revolutionizing of natural language processing (NLP) applications andthe achievement of state-of-the-art results with regard to various benchmarks,this process is still vulnerable to small and imperceptible permutationsoriginating from legitimate inputs. Intuitively, the representations should besimilar in the feature space with subtle input permutations, while largevariations occur with different meanings. This motivates us to investigate thelearning of robust textual representation in a contrastive manner. However, itis non-trivial to obtain opposing semantic instances for textual samples. Inthis study, we propose a disentangled contrastive learning method thatseparately optimizes the uniformity and alignment of representations withoutnegative sampling. Specifically, we introduce the concept of momentumrepresentation consistency to align features and leverage power normalizationwhile conforming the uniformity. Our experimental results for the NLPbenchmarks demonstrate that our approach can obtain better results comparedwith the baselines, as well as achieve promising improvements with invariancetests and adversarial attacks. The code is available inhttps://github.com/zxlzr/DCL.</div></details><blockquote><p><strong><em>2021-08-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.06590v1><strong>Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models</strong></a></p><p><em>Guanqun Yang, Shay Dineen, Zhipeng Lin, Xueqing Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Public security vulnerability reports (e.g., CVE reports) play an importantrole in the maintenance of computer and network systems. Security companies andadministrators rely on information from these reports to prioritize tasks ondeveloping and deploying patches to their customers. Since these reports areunstructured texts, automatic information extraction (IE) can help scale up theprocessing by converting the unstructured reports to structured forms, e.g.,software names and versions and vulnerability types. Existing works onautomated IE for security vulnerability reports often rely on a large number oflabeled training samples. However, creating massive labeled training set isboth expensive and time consuming. In this work, for the first time, we proposeto investigate this problem where only a small number of labeled trainingsamples are available. In particular, we investigate the performance offine-tuning several state-of-the-art pre-trained language models on our smalltraining dataset. The results show that with pre-trained language models andcarefully tuned hyperparameters, we have reached or slightly outperformed thestate-of-the-art system on this task. Consistent with previous two-step processof first fine-tuning on main category and then transfer learning to others asin [7], if otherwise following our proposed approach, the number of requiredlabeled samples substantially decrease in both stages: 90% reduction infine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64labeled samples per category. Our experiments thus demonstrate theeffectiveness of few-sample learning on NER for security vulnerability report.This result opens up multiple research opportunities for few-sample learningfor security vulnerability reports, which is discussed in the paper. Code:https://github.com/guanqun-yang/FewVulnerability.</div></details><blockquote><p><strong><em>2021-08-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.06082v1><strong>Asteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection</strong></a></p><p><em>Shouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, Zhiqiang Shi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Binary code similarity detection is a fundamental technique for many securityapplications such as vulnerability search, patch analysis, and malwaredetection. There is an increasing need to detect similar code for vulnerabilitysearch across architectures with the increase of critical vulnerabilities inIoT devices. The variety of IoT hardware architectures and software platformsrequires to capture semantic equivalence of code fragments in the similaritydetection. However, existing approaches are insufficient in capturing thesemantic similarity. We notice that the abstract syntax tree (AST) of afunction contains rich semantic information. Inspired by successfulapplications of natural language processing technologies in sentence semanticunderstanding, we propose a deep learning-based AST-encoding method, namedASTERIA, to measure the semantic equivalence of functions in differentplatforms. Our method leverages the Tree-LSTM network to learn the semanticrepresentation of a function from its AST. Then the similarity detection can beconducted efficiently and accurately by measuring the similarity between tworepresentation vectors. We have implemented an open-source prototype ofASTERIA. The Tree-LSTM model is trained on a dataset with 1,022,616 functionpairs and evaluated on a dataset with 95,078 function pairs. Evaluation resultsshow that our method outperforms the AST-based tool Diaphora andthe-state-of-art method Gemini by large margins with respect to the binarysimilarity detection. And our method is several orders of magnitude faster thanDiaphora and Gemini for the similarity calculation. In the application ofvulnerability search, our tool successfully identified 75 vulnerable functionsin 5,979 IoT firmware images.</div></details><blockquote><p><strong><em>2021-08-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.02340v1><strong>Robust Transfer Learning with Pretrained Language Models through Adapters</strong></a></p><p><em>Wenjuan Han, Bo Pang, Yingnian Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Transfer learning with large pretrained transformer-based language modelslike BERT has become a dominating approach for most NLP tasks. Simplyfine-tuning those large language models on downstream tasks or combining itwith task-specific pretraining is often not robust. In particular, theperformance considerably varies as the random seed changes or the number ofpretraining and/or fine-tuning iterations varies, and the fine-tuned model isvulnerable to adversarial attack. We propose a simple yet effectiveadapter-based approach to mitigate these issues. Specifically, we insert smallbottleneck layers (i.e., adapter) within each layer of a pretrained model, thenfix the pretrained layers and train the adapter layers on the downstream taskdata, with (1) task-specific unsupervised pretraining and then (2)task-specific supervised training (e.g., classification, sequence labeling).Our experiments demonstrate that such a training scheme leads to improvedstability and adversarial robustness in transfer learning to various downstreamtasks.</div></details><blockquote><p><strong><em>2021-08-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.02044v1><strong>A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python</strong></a></p><p><em>Amirreza Bagheri, P√©ter Heged≈±s</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the age of big data and machine learning, at a time when the techniquesand methods of software development are evolving rapidly, a problem has arisen:programmers can no longer detect all the security flaws and vulnerabilities intheir code manually. To overcome this problem, developers can now rely onautomatic techniques, like machine learning based prediction models, to detectsuch issues. An inherent property of such approaches is that they work withnumeric vectors (i.e., feature vectors) as inputs. Therefore, one needs totransform the source code into such feature vectors, often referred to as codeembedding. A popular approach for code embedding is to adapt natural languageprocessing techniques, like text representation, to automatically derive thenecessary features from the source code. However, the suitability andcomparison of different text representation techniques for solving SoftwareEngineering (SE) problems is rarely studied systematically. In this paper, wepresent a comparative study on three popular text representation methods,word2vec, fastText, and BERT applied to the SE task of detectingvulnerabilities in Python code. Using a data mining approach, we collected alarge volume of Python source code in both vulnerable and fixed forms that weembedded with word2vec, fastText, and BERT to vectors and used a LongShort-Term Memory network to train on them. Using the same LSTM architecture,we could compare the efficiency of the different embeddings in derivingmeaningful feature vectors. Our findings show that all the text representationmethods are suitable for code representation in this particular task, but theBERT model is the most promising as it is the least time consuming and the LSTMmodel based on it achieved the best overall accuracy(93.8%) in predictingPython source code vulnerabilities.</div></details><blockquote><p><strong><em>2021-06-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2107.00003v1><strong>Understanding Adversarial Examples Through Deep Neural Network&rsquo;s Response Surface and Uncertainty Regions</strong></a></p><p><em>Juan Shu, Bowei Xi, Charles Kamhoua</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural network (DNN) is a popular model implemented in many systems tohandle complex tasks such as image classification, object recognition, naturallanguage processing etc. Consequently DNN structural vulnerabilities becomepart of the security vulnerabilities in those systems. In this paper we studythe root cause of DNN adversarial examples. We examine the DNN response surfaceto understand its classification boundary. Our study reveals the structuralproblem of DNN classification boundary that leads to the adversarial examples.Existing attack algorithms can generate from a handful to a few hundredadversarial examples given one clean image. We show there are infinitely manyadversarial images given one clean sample, all within a small neighborhood ofthe clean sample. We then define DNN uncertainty regions and showtransferability of adversarial examples is not universal. We also argue thatgeneralization error, the large sample theoretical guarantee established forDNN, cannot adequately capture the phenomenon of adversarial examples. We neednew theory to measure DNN robustness.</div></details><blockquote><p><strong><em>2021-06-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.07805v2><strong>Extracting Training Data from Large Language Models</strong></a></p><p><em>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: It has become common to publish large (billion parameter) language modelsthat have been trained on private datasets. This paper demonstrates that insuch settings, an adversary can perform a training data extraction attack torecover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes ofthe public Internet, and are able to extract hundreds of verbatim textsequences from the model&rsquo;s training data. These extracted examples include(public) personally identifiable information (names, phone numbers, and emailaddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possibleeven though each of the above sequences are included in just one document inthe training data. We comprehensively evaluate our extraction attack to understand the factorsthat contribute to its success. Worryingly, we find that larger models are morevulnerable than smaller models. We conclude by drawing lessons and discussingpossible safeguards for training large language models.</div></details><p><a href=http://arxiv.org/abs/1909.06723v4><strong>Natural Language Adversarial Defense through Synonym Encoding</strong></a></p><p><em>Xiaosen Wang, Hao Jin, Yichen Yang, Kun He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the area of natural language processing, deep learning models are recentlyknown to be vulnerable to various types of adversarial perturbations, butrelatively few works are done on the defense side. Especially, there exists feweffective defense method against the successful synonym substitution basedattacks that preserve the syntactic structure and semantic information of theoriginal text while fooling the deep learning models. We contribute in thisdirection and propose a novel adversarial defense method called SynonymEncoding Method (SEM). Specifically, SEM inserts an encoder before the inputlayer of the target model to map each cluster of synonyms to a unique encodingand trains the model to eliminate possible adversarial perturbations withoutmodifying the network architecture or adding extra data. Extensive experimentsdemonstrate that SEM can effectively defend the current synonym substitutionbased attacks and block the transferability of adversarial examples. SEM isalso easy and efficient to scale to large models and big datasets.</div></details><blockquote><p><strong><em>2021-05-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.14357v1><strong>Constructing Flow Graphs from Procedural Cybersecurity Texts</strong></a></p><p><em>Kuntal Kumar Pal, Kazuaki Kashihara, Pratyay Banerjee, Swaroop Mishra, Ruoyu Wang, Chitta Baral</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Following procedural texts written in natural languages is challenging. Wemust read the whole text to identify the relevant information or identify theinstruction flows to complete a task, which is prone to failures. If such textsare structured, we can readily visualize instruction-flows, reason or infer aparticular step, or even build automated systems to help novice agents achievea goal. However, this structure recovery task is a challenge because of suchtexts&rsquo; diverse nature. This paper proposes to identify relevant informationfrom such texts and generate information flows between sentences. We built alarge annotated procedural text dataset (CTFW) in the cybersecurity domain(3154 documents). This dataset contains valuable instructions regardingsoftware vulnerability analysis experiences. We performed extensive experimentson CTFW with our LM-GNN model variants in multiple settings. To show thegeneralizability of both this task and our method, we also experimented withprocedural texts from two other domains (Maintenance Manual and Cooking), whichare substantially different from cybersecurity. Our experiments show that GraphConvolution Network with BERT sentence embeddings outperforms BERT in all threedomains</div></details><blockquote><p><strong><em>2021-03-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.02329v4><strong>InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective</strong></a></p><p><em>Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale language models such as BERT have achieved state-of-the-artperformance across a wide range of NLP tasks. Recent studies, however, showthat such BERT-based models are vulnerable facing the threats of textualadversarial attacks. We aim to address this problem from aninformation-theoretic perspective, and propose InfoBERT, a novel learningframework for robust fine-tuning of pre-trained language models. InfoBERTcontains two mutual-information-based regularizers for model training: (i) anInformation Bottleneck regularizer, which suppresses noisy mutual informationbetween the input and the feature representation; and (ii) a Robust Featureregularizer, which increases the mutual information between local robustfeatures and global features. We provide a principled way to theoreticallyanalyze and improve the robustness of representation learning for languagemodels in both standard and adversarial training. Extensive experimentsdemonstrate that InfoBERT achieves state-of-the-art robust accuracy overseveral adversarial datasets on Natural Language Inference (NLI) and QuestionAnswering (QA) tasks. Our code is available athttps://github.com/AI-secure/InfoBERT.</div></details><blockquote><p><strong><em>2021-03-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.11316v1><strong>Automated Software Vulnerability Assessment with Concept Drift</strong></a></p><p><em>Triet H. M. Le, Bushra Sabir, M. Ali Babar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Software Engineering researchers are increasingly using Natural LanguageProcessing (NLP) techniques to automate Software Vulnerabilities (SVs)assessment using the descriptions in public repositories. However, the existingNLP-based approaches suffer from concept drift. This problem is caused by alack of proper treatment of new (out-of-vocabulary) terms for the evaluation ofunseen SVs over time. To perform automated SVs assessment with concept driftusing SVs&rsquo; descriptions, we propose a systematic approach that combines bothcharacter and word features. The proposed approach is used to predict sevenVulnerability Characteristics (VCs). The optimal model of each VC is selectedusing our customized time-based cross-validation method from a list of eightNLP representations and six well-known Machine Learning models. We have usedthe proposed approach to conduct large-scale experiments on more than 100,000SVs in the National Vulnerability Database (NVD). The results show that ourapproach can effectively tackle the concept drift issue of the SVs&rsquo;descriptions reported from 2000 to 2018 in NVD even without retraining themodel. In addition, our approach performs competitively compared to theexisting word-only method. We also investigate how to build compactconcept-drift-aware models with much fewer features and give somerecommendations on the choice of classifiers and NLP representations for SVsassessment.</div></details><blockquote><p><strong><em>2021-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2102.07995v1><strong>D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis</strong></a></p><p><em>Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti, Edward Epstein, Bo Yang, Jim Laredo, Alessandro Morari, Zhong Su</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Static analysis tools are widely used for vulnerability detection as theyunderstand programs with complex behavior and millions of lines of code.Despite their popularity, static analysis tools are known to generate an excessof false positives. The recent ability of Machine Learning models to understandprogramming languages opens new possibilities when applied to static analysis.However, existing datasets to train models for vulnerability identificationsuffer from multiple limitations such as limited bug context, limited size, andsynthetic and unrealistic source code. We propose D2A, a differential analysisbased approach to label issues reported by static analysis tools. The D2Adataset is built by analyzing version pairs from multiple open source projects.From each project, we select bug fixing commits and we run static analysis onthe versions before and after such commits. If some issues detected in abefore-commit version disappear in the corresponding after-commit version, theyare very likely to be real bugs that got fixed by the commit. We use D2A togenerate a large labeled dataset to train models for vulnerabilityidentification. We show that the dataset can be used to build a classifier toidentify possible false alarms among the issues reported by static analysis,hence helping developers prioritize and investigate potential true positivesfirst.</div></details><blockquote><p><strong><em>2020-11-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.12254v5><strong>Privacy in Deep Learning: A Survey</strong></a></p><p><em>Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ever-growing advances of deep learning in many areas including vision,recommendation systems, natural language processing, etc., have led to theadoption of Deep Neural Networks (DNNs) in production systems. The availabilityof large datasets and high computational power are the main contributors tothese advances. The datasets are usually crowdsourced and may contain sensitiveinformation. This poses serious privacy concerns as this data can be misused orleaked through various vulnerabilities. Even if the cloud provider and thecommunication link is trusted, there are still threats of inference attackswhere an attacker could speculate properties of the data used for training, orfind the underlying model architecture and parameters. In this survey, wereview the privacy concerns brought by deep learning, and the mitigatingtechniques introduced to tackle these issues. We also show that there is a gapin the literature regarding test-time inference privacy, and propose possiblefuture research directions.</div></details><blockquote><p><strong><em>2020-10-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.02220v3><strong>You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion</strong></a></p><p><em>Roei Schuster, Congzheng Song, Eran Tromer, Vitaly Shmatikov</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Code autocompletion is an integral feature of modern code editors and IDEs.The latest generation of autocompleters uses neural language models, trained onpublic open-source code repositories, to suggest likely (not just staticallyfeasible) completions given the current context. We demonstrate that neural code autocompleters are vulnerable to poisoningattacks. By adding a few specially-crafted files to the autocompleter&rsquo;straining corpus (data poisoning), or else by directly fine-tuning theautocompleter on these files (model poisoning), the attacker can influence itssuggestions for attacker-chosen contexts. For example, the attacker can &ldquo;teach"the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3for the SSL/TLS protocol version, or a low iteration count for password-basedencryption. Moreover, we show that these attacks can be targeted: anautocompleter poisoned by a targeted attack is much more likely to suggest theinsecure completion for files from a specific repo or specific developer. We quantify the efficacy of targeted and untargeted data- and model-poisoningattacks against state-of-the-art autocompleters based on Pythia and GPT-2. Wethen evaluate existing defenses against poisoning attacks and show that theyare largely ineffective.</div></details><blockquote><p><strong><em>2020-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.11178v1><strong>Optimal policies for mitigating pandemic costs</strong></a></p><p><em>M. Serra, S. al-Mosleh, S. Ganga Prasath, V. Raju, S. Mantena, J. Chandra, S. Iams, L. Mahadevan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Several non-pharmaceutical interventions have been proposed to control thespread of the COVID-19 pandemic. On the large scale, these empirical solutions,often associated with extended and complete lockdowns, attempt to minimize thecosts associated with mortality, economic losses and social factors, whilebeing subject to constraints such as finite hospital capacity. Here we pose thequestion of how to mitigate pandemic costs subject to constraints by adoptingthe language of optimal control theory. This allows us to determine top-downpolicies for the nature and dynamics of social contact rates given anage-structured model for the dynamics of the disease. Depending on the relativeweights allocated to life and socioeconomic losses, we see that the optimalstrategies range from long-term social-distancing only for the most vulnerable,to partial lockdown to ensure not over-running hospitals, to alternating-shiftswith significant reduction in life and/or socioeconomic losses. Crucially,commonly used strategies that involve long periods of broad lockdown are almostnever optimal, as they are highly unstable to reopening and entail highsocioeconomic costs. Using parameter estimates from data available for Germanyand the USA, we quantify these policies and use sensitivity analysis in therelevant model parameters and initial conditions to determine the range ofrobustness of our policies. Finally we also discuss how bottom-up behavioralchanges can also change the dynamics of the pandemic and show how this intandem with top-down control policies can mitigate pandemic costs even moreeffectively.</div></details><blockquote><p><strong><em>2020-06-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2006.11627v1><strong>Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble</strong></a></p><p><em>Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite neural networks have achieved prominent performance on many naturallanguage processing (NLP) tasks, they are vulnerable to adversarial examples.In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomizedsmoothing method for training a robust model to defense substitution-basedattacks. During training, DNE forms virtual sentences by sampling embeddingvectors for each word in an input sentence from a convex hull spanned by theword and its synonyms, and it augments them with the training data. In such away, the model is robust to adversarial attacks while maintaining theperformance on the original clean data. DNE is agnostic to the networkarchitectures and scales to large models for NLP applications. We demonstratethrough extensive experimentation that our method consistently outperformsrecently proposed defense methods by a significant margin across differentnetwork architectures and multiple data sets.</div></details><blockquote><p><strong><em>2020-04-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.08994v2><strong>Adversarial Training for Large Neural Language Models</strong></a></p><p><em>Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generalization and robustness are both key desiderata for designing machinelearning methods. Adversarial training can enhance robustness, but past workoften finds it hurts generalization. In natural language processing (NLP),pre-training large neural language models such as BERT have demonstratedimpressive gain in generalization for a variety of tasks, with furtherimprovement from adversarial fine-tuning. However, these models are stillvulnerable to adversarial attacks. In this paper, we show that adversarialpre-training can improve both generalization and robustness. We propose ageneral algorithm ALUM (Adversarial training for large neural LangUage Models),which regularizes the training objective by applying perturbations in theembedding space that maximizes the adversarial loss. We present the firstcomprehensive study of adversarial training in all stages, includingpre-training from scratch, continual pre-training on a well-trained model, andtask-specific fine-tuning. ALUM obtains substantial gains over BERT on a widerange of NLP tasks, in both regular and adversarial scenarios. Even for modelsthat have been well trained on extremely large text corpora, such as RoBERTa,ALUM can still produce significant gains from continual pre-training, whereasconventional non-adversarial methods can not. ALUM can be further combined withtask-specific fine-tuning to attain additional gains. The ALUM code is publiclyavailable at <a href=https://github.com/namisan/mt-dnn>https://github.com/namisan/mt-dnn</a>.</div></details><blockquote><p><strong><em>2020-04-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.03742v1><strong>Towards Evaluating the Robustness of Chinese BERT Classifiers</strong></a></p><p><em>Boxin Wang, Boyuan Pan, Xin Li, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in large-scale language representation models such as BERThave improved the state-of-the-art performances in many NLP tasks. Meanwhile,character-level Chinese NLP models, including BERT for Chinese, have alsodemonstrated that they can outperform the existing models. In this paper, weshow that, however, such BERT-based models are vulnerable under character-leveladversarial attacks. We propose a novel Chinese char-level attack methodagainst BERT-based classifiers. Essentially, we generate &ldquo;small&rdquo; perturbationon the character level in the embedding space and guide the charactersubstitution procedure. Extensive experiments show that the classificationaccuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating lessthan 2 characters on average based on the proposed attack. Human evaluationsalso confirm that our generated Chinese adversarial examples barely affecthuman performance on these NLP tasks.</div></details><blockquote><p><strong><em>2020-03-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2003.05987v1><strong>√ÜGIS: Shielding Vulnerable Smart Contracts Against Attacks</strong></a></p><p><em>Christof Ferreira Torres, Mathis Baden, Robert Norvill, Beltran Borja Fiz Pontiveros, Hugo Jonker, Sjouke Mauw</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, smart contracts have suffered major exploits, costingmillions of dollars. Unlike traditional programs, smart contracts are deployedon a blockchain. As such, they cannot be modified once deployed. Though varioustools have been proposed to detect vulnerable smart contracts, the majorityfails to protect vulnerable contracts that have already been deployed on theblockchain. Only very few solutions have been proposed so far to tackle theissue of post-deployment. However, these solutions suffer from low precisionand are not generic enough to prevent any type of attack. In this work, we introduce {\AE}GIS, a dynamic analysis tool that protectssmart contracts from being exploited during runtime. Its capability ofdetecting new vulnerabilities can easily be extended through so-called attackpatterns. These patterns are written in a domain-specific language that istailored to the execution model of Ethereum smart contracts. The languageenables the description of malicious control and data flows. In addition, wepropose a novel mechanism to streamline and speed up the process of managingattack patterns. Patterns are voted upon and stored via a smart contract, thusleveraging the benefits of tamper-resistance and transparency provided by theblockchain. We compare {\AE}GIS to current state-of-the-art tools anddemonstrate that our solution achieves higher precision in detecting attacks.Finally, we perform a large-scale analysis on the first 4.5 million blocks ofthe Ethereum blockchain, thereby confirming the occurrences of well reportedand yet unreported attacks in the wild.</div></details><blockquote><p><strong><em>2019-11-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1911.07707v1><strong>Building Fast Fuzzers</strong></a></p><p><em>Rahul Gopinath, Andreas Zeller</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fuzzing is one of the key techniques for evaluating the robustness ofprograms against attacks. Fuzzing has to be effective in producing inputs thatcover functionality and find vulnerabilities. But it also has to be efficientin producing such inputs quickly. Random fuzzers are very efficient, as theycan quickly generate random inputs; but they are not very effective, as thelarge majority of inputs generated is syntactically invalid. Grammar-basedfuzzers make use of a grammar (or another model for the input language) toproduce syntactically correct inputs, and thus can quickly cover input spaceand associated functionality. Existing grammar-based fuzzers are surprisinglyinefficient, though: Even the fastest grammar fuzzer Dharma still producesinputs about a thousand times slower than the fastest random fuzzer. So far,one can have an effective or an efficient fuzzer, but not both. In this paper, we describe how to build fast grammar fuzzers from the groundup, treating the problem of fuzzing from a programming language implementationperspective. Starting with a Python textbook approach, we adopt and adaptoptimization techniques from functional programming and virtual machineimplementation techniques together with other novel domain-specificoptimizations in a step-by-step fashion. In our F1 prototype fuzzer, theseimprove production speed by a factor of 100&ndash;300 over the fastest grammarfuzzer Dharma. As F1 is even 5&ndash;8 times faster than a lexical random fuzzer, wecan find bugs faster and test with much larger valid inputs than previouslypossible.</div></details><blockquote><p><strong><em>2019-10-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1910.06826v1><strong>Statically Detecting Vulnerabilities by Processing Programming Languages as Natural Languages</strong></a></p><p><em>Ib√©ria Medeiros, Nuno Neves, Miguel Correia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Web applications continue to be a favorite target for hackers due to acombination of wide adoption and rapid deployment cycles, which often lead tothe introduction of high impact vulnerabilities. Static analysis tools areimportant to search for bugs automatically in the program source code,supporting developers on their removal. However, building these tools requiresprogramming the knowledge on how to discover the vulnerabilities. This paperpresents an alternative approach in which tools learn to detect flawsautomatically by resorting to artificial intelligence concepts, more concretelyto natural language processing. The approach employs a sequence model to learnto characterize vulnerabilities based on an annotated corpus. Afterwards, themodel is utilized to discover and identify vulnerabilities in the source code.It was implemented in the DEKANT tool and evaluated experimentally with a largeset of PHP applications and WordPress plugins. Overall, we found severalhundred vulnerabilities belonging to 12 classes of input validationvulnerabilities, where 62 of them were zero-day.</div></details><blockquote><p><strong><em>2018-06-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1806.09339v1><strong>SAQL: A Stream-based Query System for Real-Time Abnormal System Behavior Detection</strong></a></p><p><em>Peng Gao, Xusheng Xiao, Ding Li, Zhichun Li, Kangkook Jee, Zhenyu Wu, Chung Hwan Kim, Sanjeev R. Kulkarni, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, advanced cyber attacks, which consist of a sequence of steps thatinvolve many vulnerabilities and hosts, compromise the security of manywell-protected businesses. This has led to the solutions that ubiquitouslymonitor system activities in each host (big data) as a series of events, andsearch for anomalies (abnormal behaviors) for triaging risky events. Sincefighting against these attacks is a time-critical mission to prevent furtherdamage, these solutions face challenges in incorporating expert knowledge toperform timely anomaly detection over the large-scale provenance data. To address these challenges, we propose a novel stream-based query systemthat takes as input, a real-time event feed aggregated from multiple hosts inan enterprise, and provides an anomaly query engine that queries the event feedto identify abnormal behaviors based on the specified anomalies. To facilitatethe task of expressing anomalies based on expert knowledge, our system providesa domain-specific query language, SAQL, which allows analysts to express modelsfor (1) rule-based anomalies, (2) time-series anomalies, (3) invariant-basedanomalies, and (4) outlier-based anomalies. We deployed our system in NEC LabsAmerica comprising 150 hosts and evaluated it using 1.1TB of real systemmonitoring data (containing 3.3 billion events). Our evaluations on a broad setof attack behaviors and micro-benchmarks show that our system has a lowdetection latency (&lt;2s) and a high system throughput (110,000 events/s;supporting ~4000 hosts), and is more efficient in memory utilization than theexisting stream-based complex event processing systems.</div></details></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>