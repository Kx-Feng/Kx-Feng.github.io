<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Arxiv Papers: Machine Unlearning#2024-03-20
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam
abstractabstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF)."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content="Machine Unlearning"><meta property="og:description" content="Arxiv Papers: Machine Unlearning#2024-03-20
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam
abstractabstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF)."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/arxiv_papers/machine_unlearning/"><meta property="article:section" content="docs"><title>Machine Unlearning | Xiaoyan Feng</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=http://localhost:1313/docs/arxiv_papers/machine_unlearning/><link rel=stylesheet href=/book.min.16628528a7a2a88d96389ec90a07d3f66879aff48e8cbc1f53ddc5b7ddb7ab85.css integrity="sha256-FmKFKKeiqI2WOJ7JCgfT9mh5r/SOjLwfU93Ft923q4U=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.57e922f92c43b5724126f423f53e9865b21cce50b42c923f98d85542bdce8cfc.js integrity="sha256-V+ki+SxDtXJBJvQj9T6YZbIczlC0LJI/mNhVQr3OjPw=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Xiaoyan Feng</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/about_me/>Home</a></li><li><span>Arxiv Papers</span><ul><li><a href=/docs/arxiv_papers/llm_copyright/>LLM with Copyright</a></li><li><a href=/docs/arxiv_papers/llm_privacy/>LLM with Privacy</a></li><li><a href=/docs/arxiv_papers/machine_unlearning/ class=active>Machine Unlearning</a></li><li><a href=/docs/arxiv_papers/vulnerable_llm/>Vulnerable LLM</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Machine Unlearning</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><h1 id=arxiv-papers-machine-unlearning>Arxiv Papers: Machine Unlearning
<a class=anchor href=#arxiv-papers-machine-unlearning>#</a></h1><blockquote><p><strong><em>2024-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.13682v1><strong>Threats, Attacks, and Defenses in Machine Unlearning: A Survey</strong></a></p><p><em>Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF).Therefore, strategic knowledge removal mitigates the risk of harmful outcomes,safeguarding against biases, misinformation, and unauthorized dataexploitation, thereby enhancing the ethical use and reliability of AI systems.Efforts have been made to design efficient unlearning approaches, with MUservices being examined for integration with existing machine learning as aservice (MLaaS), allowing users to submit requests to erase data. However,recent research highlights vulnerabilities in machine unlearning systems, suchas information leakage and malicious unlearning requests, that can lead tosignificant security and privacy concerns. Moreover, extensive researchindicates that unlearning methods and prevalent attacks fulfill diverse roleswithin MU systems. For instance, unlearning can act as a mechanism to recovermodels from backdoor attacks, while backdoor attacks themselves can serve as anevaluation metric for unlearning effectiveness. This underscores the intricaterelationship and complex interplay between these elements in maintaining systemfunctionality and safety. Therefore, this survey seeks to bridge the gapbetween the extensive number of studies on threats, attacks, and defenses inmachine unlearning and the absence of a comprehensive review that categorizestheir taxonomy, methods, and solutions, thus offering valuable insights forfuture research directions and practical implementations.</div></details><blockquote><p><strong><em>2024-03-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.12830v1><strong>Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects</strong></a></p><p><em>Cheng-Long Wang, Qi Li, Zihang Xiang, Di Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The growing concerns surrounding data privacy and security have underscoredthe critical necessity for machine unlearning&ndash;aimed at fully removing datalineage from machine learning models. MLaaS providers expect this to be theirultimate safeguard for regulatory compliance. Despite its critical importance,the pace at which privacy communities have been developing and implementingstrong methods to verify the effectiveness of machine unlearning has beendisappointingly slow, with this vital area often receiving insufficient focus.This paper seeks to address this shortfall by introducing well-defined andeffective metrics for black-box unlearning auditing tasks. We transform theauditing challenge into a question of non-membership inference and developefficient metrics for auditing. By relying exclusively on the original andunlearned models&ndash;eliminating the need to train additional shadow models&ndash;ourapproach simplifies the evaluation of unlearning at the individual data pointlevel. Utilizing these metrics, we conduct an in-depth analysis of currentapproximate machine unlearning algorithms, identifying three key directionswhere these approaches fall short: utility, resilience, and equity. Our aim isthat this work will greatly improve our understanding of approximate machineunlearning methods, taking a significant stride towards converting thetheoretical right to data erasure into a auditable reality.</div></details><p><a href=http://arxiv.org/abs/2403.13130v1><strong>Self-generated Replay Memories for Continual Neural Machine Translation</strong></a></p><p><em>Michele Resta, Davide Bacciu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern Neural Machine Translation systems exhibit strong performance inseveral different languages and are constantly improving. Their ability tolearn continuously is, however, still severely limited by the catastrophicforgetting issue. In this work, we leverage a key property of encoder-decoderTransformers, i.e. their generative ability, to propose a novel approach tocontinually learning Neural Machine Translation systems. We show how this caneffectively learn on a stream of experiences comprising different languages, byleveraging a replay memory populated by using the model itself as a generatorof parallel sentences. We empirically demonstrate that our approach cancounteract catastrophic forgetting without requiring explicit memorization oftraining data. Code will be publicly available upon publication. Code:https://github.com/m-resta/sg-rep</div></details><p><a href=http://arxiv.org/abs/2403.02628v2><strong>Interactive Continual Learning: Fast and Slow Thinking</strong></a></p><p><em>Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Advanced life forms, sustained by the synergistic interaction of neuralcognitive mechanisms, continually acquire and transfer knowledge throughouttheir lifespan. In contrast, contemporary machine learning paradigms exhibitlimitations in emulating the facets of continual learning (CL). Nonetheless,the emergence of large language models (LLMs) presents promising avenues forrealizing CL via interactions with these models. Drawing on ComplementaryLearning System theory, this paper presents a novel Interactive ContinualLearning (ICL) framework, enabled by collaborative interactions among models ofvarious sizes. Specifically, we assign the ViT model as System1 and multimodalLLM as System2. To enable the memory module to deduce tasks from classinformation and enhance Set2Set retrieval, we propose the Class-Knowledge-TaskMulti-Head Attention (CKT-MHA). Additionally, to improve memory retrieval inSystem1 through enhanced geometric representation, we introduce the CL-vMFmechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, weintroduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI)strategy to identify hard examples, thus enhancing collaboration betweenSystem1 and System2 for complex reasoning realization. Comprehensive evaluationof our proposed ICL demonstrates significant resistance to forgetting andsuperior performance relative to existing methods. Code is available atgithub.com/ICL.</div></details><blockquote><p><strong><em>2024-03-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.11288v1><strong>Benefits of non-adiabatic quantum control in quantum computation through spin qubit systems</strong></a></p><p><em>Nirupam Dutta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This is evident that the controllable quantum systems can be the reliablebuilding blocks for Quantum computation. In reality we are witnessing theprogress towards making the idea tractable enough, though optimistic but thethreshold is not very near to us. The dawn of quantum computation has begun. Inthe future, we hope to see a full fledged operationally stable quantum computerwhich can solve the problems beyond the scope of classical digital computers.We may call it quantum supremacy. Nevertheless, we should not forget that thereare problems which demand classical computers to be in the game for a betterperformance in comparison to the same through quantum devices. In the currentstage of computing technology, the most beneficial area is nothing but anhybrid approach and that is for no doubt will reign the market for the nextfive to ten years. This hybrid aspect has several directions such as simulatingquantum computation on a classical computer. Keeping both the aspect,computation through real physical devices and simulation on a classicalcomputer by accessing available quantum computers for cloud computing, someadvantages have been discussed in this article which will be elaborated as wellin future articles. These advantages are inherent if we can achieve propernon-adiabatic control over the spin system in the laboratory. Otherwise theseaspects can always be simulated by using quantum algorithms to see whether theycan be useful in comparison to a purely classical computing machine. This is nodoubt a new window for progress in the direction of quantum computation.</div></details><blockquote><p><strong><em>2024-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.11656v5><strong>SIFU: Sequential Informed Federated Unlearning for Efficient and Provable Client Unlearning in Federated Optimization</strong></a></p><p><em>Yann Fraboni, Martin Van Waerebeke, Kevin Scaman, Richard Vidal, Laetitia Kameni, Marco Lorenzi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning (MU) is an increasingly important topic in machinelearning safety, aiming at removing the contribution of a given data point froma training procedure. Federated Unlearning (FU) consists in extending MU tounlearn a given client&rsquo;s contribution from a federated training routine. Whileseveral FU methods have been proposed, we currently lack a general approachproviding formal unlearning guarantees to the FedAvg routine, while ensuringscalability and generalization beyond the convex assumption on the clients&rsquo;loss functions. We aim at filling this gap by proposing SIFU (SequentialInformed Federated Unlearning), a new FU method applying to both convex andnon-convex optimization regimes. SIFU naturally applies to FedAvg withoutadditional computational cost for the clients and provides formal guarantees onthe quality of the unlearning task. We provide a theoretical analysis of theunlearning properties of SIFU, and practically demonstrate its effectiveness ascompared to a panel of unlearning methods from the state-of-the-art.</div></details><blockquote><p><strong><em>2024-03-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.07061v3><strong>Machine Unlearning: Solutions and Challenges</strong></a></p><p><em>Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models may inadvertently memorize sensitive, unauthorized,or malicious data, posing risks of privacy breaches, security vulnerabilities,and performance degradation. To address these issues, machine unlearning hasemerged as a critical technique to selectively remove specific training datapoints&rsquo; influence on trained models. This paper provides a comprehensivetaxonomy and analysis of the solutions in machine unlearning. We categorizeexisting solutions into exact unlearning approaches that remove data influencethoroughly and approximate unlearning approaches that efficiently minimize datainfluence. By comprehensively reviewing solutions, we identify and discusstheir strengths and limitations. Furthermore, we propose future directions toadvance machine unlearning and establish it as an essential capability fortrustworthy and adaptive machine learning models. This paper providesresearchers with a roadmap of open problems, encouraging impactfulcontributions to address real-world needs for selective data removal.</div></details><p><a href=http://arxiv.org/abs/2403.10461v1><strong>Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness</strong></a></p><p><em>Mohamed elShehaby, Aditya Kotha, Ashraf Matrawy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Learning (ML) is susceptible to adversarial attacks that aim to trickML models, making them produce faulty predictions. Adversarial training wasfound to increase the robustness of ML models against these attacks. However,in network and cybersecurity, obtaining labeled training and adversarialtraining data is challenging and costly. Furthermore, concept drift deepens thechallenge, particularly in dynamic domains like network and cybersecurity, andrequires various models to conduct periodic retraining. This letter introducesAdaptive Continuous Adversarial Training (ACAT) to continuously integrateadversarial training samples into the model during ongoing learning sessions,using real-world detected adversarial data, to enhance model resilience againstevolving adversarial threats. ACAT is an adaptive defense mechanism thatutilizes periodic retraining to effectively counter adversarial attacks whilemitigating catastrophic forgetting. Our approach also reduces the total timerequired for adversarial sample detection, especially in environments such asnetwork security where the rate of attacks could be very high. Traditionaldetection processes that involve two stages may result in lengthy procedures.Experimental results using a SPAM detection dataset demonstrate that with ACAT,the accuracy of the SPAM filter increased from 69% to over 88% after just threeretraining sessions. Furthermore, ACAT outperforms conventional adversarialsample detectors, providing faster decision times, up to four times faster insome cases.</div></details><blockquote><p><strong><em>2024-03-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.08554v1><strong>Federated Knowledge Graph Unlearning via Diffusion Model</strong></a></p><p><em>Bingchen Liu, Yuanyuan Fang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) promotes the development and application ofartificial intelligence technologies by enabling model sharing andcollaboration while safeguarding data privacy. Knowledge graph (KG) embeddingrepresentation provides a foundation for knowledge reasoning and applicationsby mapping entities and relations into vector space. Federated KG embeddingenables the utilization of knowledge from diverse client sources whilesafeguarding the privacy of local data. However, due to demands such as privacyprotection and the need to adapt to dynamic data changes, investigations intomachine unlearning (MU) have been sparked. However, it is challenging tomaintain the performance of KG embedding models while forgetting the influenceof specific forgotten data on the model. In this paper, we propose FedDM, anovel framework tailored for machine unlearning in federated knowledge graphs.Leveraging diffusion models, we generate noisy data to sensibly mitigate theinfluence of specific knowledge on FL models while preserving the overallperformance concerning the remaining data. We conduct experimental evaluationson benchmark datasets to assess the efficacy of the proposed model. Extensiveexperiments demonstrate that FedDM yields promising results in knowledgeforgetting.</div></details><p><a href=http://arxiv.org/abs/2403.08254v1><strong>Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects</strong></a></p><p><em>Na Li, Chunyi Zhou, Yansong Gao, Hui Chen, Anmin Fu, Zhi Zhang, Yu Shui</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Personal digital data is a critical asset, and governments worldwide haveenforced laws and regulations to protect data privacy. Data users have beenendowed with the right to be forgotten of their data. In the course of machinelearning (ML), the forgotten right requires a model provider to delete userdata and its subsequent impact on ML models upon user requests. Machineunlearning emerges to address this, which has garnered ever-increasingattention from both industry and academia. While the area has developedrapidly, there is a lack of comprehensive surveys to capture the latestadvancements. Recognizing this shortage, we conduct an extensive exploration tomap the landscape of machine unlearning including the (fine-grained) taxonomyof unlearning algorithms under centralized and distributed settings, debate onapproximate unlearning, verification and evaluation metrics, challenges andsolutions for unlearning under different applications, as well as attackstargeting machine unlearning. The survey concludes by outlining potentialdirections for future research, hoping to serve as a guide for interestedscholars.</div></details><blockquote><p><strong><em>2024-03-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.07611v1><strong>Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning</strong></a></p><p><em>Vinay Chakravarthi Gogineni, Esmaeil S. Nadimi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has garnered significant attention due to its ability toselectively erase knowledge obtained from specific training data samples in analready trained machine learning model. This capability enables data holders toadhere strictly to data protection regulations. However, existing unlearningtechniques face practical constraints, often causing performance degradation,demanding brief fine-tuning post unlearning, and requiring significant storage.In response, this paper introduces a novel class of machine unlearningalgorithms. First method is partial amnesiac unlearning, integration oflayer-wise pruning with amnesiac unlearning. In this method, updates made tothe model during training are pruned and stored, subsequently used to forgetspecific data from trained model. The second method assimilates layer-wisepartial-updates into label-flipping and optimization-based unlearning tomitigate the adverse effects of data deletion on model efficacy. Through adetailed experimental evaluation, we showcase the effectiveness of proposedunlearning methods. Experimental results highlight that the partial amnesiacunlearning not only preserves model efficacy but also eliminates the necessityfor brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,employing layer-wise partial updates in label-flipping and optimization-basedunlearning techniques demonstrates superiority in preserving model efficacycompared to their naive counterparts.</div></details><p><a href=http://arxiv.org/abs/2403.07362v1><strong>Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning</strong></a></p><p><em>Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The trustworthy machine learning (ML) community is increasingly recognizingthe crucial need for models capable of selectively &lsquo;unlearning&rsquo; data pointsafter training. This leads to the problem of machine unlearning (MU), aiming toeliminate the influence of chosen data points on model performance, while stillmaintaining the model&rsquo;s utility post-unlearning. Despite various MU methods fordata influence erasure, evaluations have largely focused on random dataforgetting, ignoring the vital inquiry into which subset should be chosen totruly gauge the authenticity of unlearning performance. To tackle this issue,we introduce a new evaluative angle for MU from an adversarial viewpoint. Wepropose identifying the data subset that presents the most significantchallenge for influence erasure, i.e., pinpointing the worst-case forget set.Utilizing a bi-level optimization principle, we amplify unlearning challengesat the upper optimization level to emulate worst-case scenarios, whilesimultaneously engaging in standard training and unlearning at the lower level,achieving a balance between data influence erasure and model utility. Ourproposal offers a worst-case evaluation of MU&rsquo;s resilience and effectiveness.Through extensive experiments across different datasets (including CIFAR-10,100, CelebA, Tiny ImageNet, and ImageNet) and models (including both imageclassifiers and generative models), we expose critical pros and cons inexisting (approximate) unlearning strategies. Our results illuminate thecomplex challenges of MU in practice, guiding the future development of moreaccurate and robust unlearning algorithms. The code is available athttps://github.com/OPTML-Group/Unlearn-WorstCase.</div></details><p><a href=http://arxiv.org/abs/2401.06187v2><strong>Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks</strong></a></p><p><em>Jing Wu, Mehrtash Harandi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has become a pivotal task to erase the influence of datafrom a trained model. It adheres to recent data regulation standards andenhances the privacy and security of machine learning applications. In thiswork, we present a new machine unlearning approach Scissorhands. Initially,Scissorhands identifies the most pertinent parameters in the given modelrelative to the forgetting data via connection sensitivity. By reinitializingthe most influential top-k percent of these parameters, a trimmed model forerasing the influence of the forgetting data is obtained. Subsequently,Scissorhands fine-tunes the trimmed model with a gradient projection-basedapproach, seeking parameters that preserve information on the remaining datawhile discarding information related to the forgetting data. Our experimentalresults, conducted across image classification and image generation tasks,demonstrate that Scissorhands, showcases competitive performance when comparedto existing methods.</div></details><p><a href=http://arxiv.org/abs/2403.08124v1><strong>Towards Independence Criterion in Machine Unlearning of Features and Labels</strong></a></p><p><em>Ling Han, Nanqing Luo, Hao Huang, Jing Chen, Mary-Anne Hartley</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This work delves into the complexities of machine unlearning in the face ofdistributional shifts, particularly focusing on the challenges posed bynon-uniform feature and label removal. With the advent of regulations like theGDPR emphasizing data privacy and the right to be forgotten, machine learningmodels face the daunting task of unlearning sensitive information withoutcompromising their integrity or performance. Our research introduces a novelapproach that leverages influence functions and principles of distributionalindependence to address these challenges. By proposing a comprehensiveframework for machine unlearning, we aim to ensure privacy protection whilemaintaining model performance and adaptability across varying distributions.Our method not only facilitates efficient data removal but also dynamicallyadjusts the model to preserve its generalization capabilities. Throughextensive experimentation, we demonstrate the efficacy of our approach inscenarios characterized by significant distributional shifts, makingsubstantial contributions to the field of machine unlearning. This researchpaves the way for developing more resilient and adaptable unlearningtechniques, ensuring models remain robust and accurate in the dynamic landscapeof data privacy and machine learning.</div></details><p><a href=http://arxiv.org/abs/2403.07851v1><strong>12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning</strong></a></p><p><em>Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael Hersche, Leo Zhao, Abbas Rahimi, Luca Benini</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systemsto expand their inference capabilities to new classes using only a few labeledexamples, without forgetting the previously learned classes. Classicalbackpropagation-based learning and its variants are often unsuitable forbattery-powered, memory-constrained systems at the extreme edge. In this work,we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on alightweight model consisting of a pretrained and metalearned feature extractorand an expandable explicit memory storing the class prototypes. Thearchitecture is pretrained with a novel feature orthogonality regularizationand metalearned with a multi-margin loss. For learning a new class, ourapproach extends the explicit memory with novel class prototypes, while theremaining architecture is kept frozen. This allows learning previously unseenclasses based on only a few examples with one single pass (hence online).O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,achieving state-of-the-art results. Tailored for ultra-low-power platforms, weimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating onlinelearning capabilities within just 12 mJ per new class.</div></details><blockquote><p><strong><em>2024-03-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.14412v2><strong>Task-Aware Machine Unlearning and Its Application in Load Forecasting</strong></a></p><p><em>Wangkun Xu, Fei Teng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Data privacy and security have become a non-negligible factor in loadforecasting. Previous researches mainly focus on training stage enhancement.However, once the model is trained and deployed, it may need to `forget&rsquo; (i.e.,remove the impact of) part of training data if the these data are found to bemalicious or as requested by the data owner. This paper introduces the conceptof machine unlearning which is specifically designed to remove the influence ofpart of the dataset on an already trained forecaster. However, directunlearning inevitably degrades the model generalization ability. To balancebetween unlearning completeness and model performance, a performance-awarealgorithm is proposed by evaluating the sensitivity of local model parameterchange using influence function and sample re-weighting. Furthermore, weobserve that the statistical criterion such as mean squared error, cannot fullyreflect the operation cost of the downstream tasks in power system. Therefore,a task-aware machine unlearning is proposed whose objective is a trileveloptimization with dispatch and redispatch problems considered. We theoreticallyprove the existence of the gradient of such an objective, which is key tore-weighting the remaining samples. We tested the unlearning algorithms onlinear, CNN, and MLP-Mixer based load forecasters with a realistic loaddataset. The simulation demonstrates the balance between unlearningcompleteness and operational cost. All codes can be found athttps://github.com/xuwkk/task_aware_machine_unlearning.</div></details><blockquote><p><strong><em>2024-03-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.06288v1><strong>Probing Image Compression For Class-Incremental Learning</strong></a></p><p><em>Justin Yang, Zhihao Duan, Andrew Peng, Yuning Huang, Jiangpeng He, Fengqing Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Image compression emerges as a pivotal tool in the efficient handling andtransmission of digital images. Its ability to substantially reduce file sizenot only facilitates enhanced data storage capacity but also potentially bringsadvantages to the development of continual machine learning (ML) systems, whichlearn new knowledge incrementally from sequential data. Continual ML systemsoften rely on storing representative samples, also known as exemplars, within alimited memory constraint to maintain the performance on previously learneddata. These methods are known as memory replay-based algorithms and have proveneffective at mitigating the detrimental effects of catastrophic forgetting.Nonetheless, the limited memory buffer size often falls short of adequatelyrepresenting the entire data distribution. In this paper, we explore the use ofimage compression as a strategy to enhance the buffer&rsquo;s capacity, therebyincreasing exemplar diversity. However, directly using compressed exemplarsintroduces domain shift during continual ML, marked by a discrepancy betweencompressed training data and uncompressed testing data. Additionally, it isessential to determine the appropriate compression algorithm and select themost effective rate for continual ML systems to balance the trade-off betweenexemplar quality and quantity. To this end, we introduce a new framework toincorporate image compression for continual ML including a pre-processing datacompression step and an efficient compression rate/algorithm selection method.We conduct extensive experiments on CIFAR-100 and ImageNet datasets and showthat our method significantly improves image classification accuracy incontinual ML settings.</div></details><blockquote><p><strong><em>2024-03-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16789v3><strong>Detecting Pretraining Data from Large Language Models</strong></a></p><p><em>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.</div></details><blockquote><p><strong><em>2024-03-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.12971v2><strong>Cooperative data-driven modeling</strong></a></p><p><em>Aleksandr Dekhovich, O. Taylan Turan, Jiaxiang Yi, Miguel A. Bessa</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Data-driven modeling in mechanics is evolving rapidly based on recent machinelearning advances, especially on artificial neural networks. As the fieldmatures, new data and models created by different groups become available,opening possibilities for cooperative modeling. However, artificial neuralnetworks suffer from catastrophic forgetting, i.e. they forget how to performan old task when trained on a new one. This hinders cooperation becauseadapting an existing model for a new task affects the performance on a previoustask trained by someone else. The authors developed a continual learning methodthat addresses this issue, applying it here for the first time to solidmechanics. In particular, the method is applied to recurrent neural networks topredict history-dependent plasticity behavior, although it can be used on anyother architecture (feedforward, convolutional, etc.) and to predict otherphenomena. This work intends to spawn future developments on continual learningthat will foster cooperative strategies among the mechanics community to solveincreasingly challenging problems. We show that the chosen continual learningstrategy can sequentially learn several constitutive laws without forgettingthem, using less data to achieve the same error as standard (non-cooperative)training of one law per model.</div></details><blockquote><p><strong><em>2024-03-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.05592v1><strong>Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten</strong></a></p><p><em>Meem Arafat Manab</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As we keep rapidly advancing toward an era where artificial intelligence is aconstant and normative experience for most of us, we must also be aware of whatthis vision and this progress entail. By first approximating neural connectionsand activities in computer circuits and then creating more and moresophisticated versions of this crude approximation, we are now facing an age tocome where modern deep learning-based artificial intelligence systems canrightly be called thinking machines, and they are sometimes even lauded fortheir emergent behavior and black-box approaches. But as we create morepowerful electronic brains, with billions of neural connections and parameters,can we guarantee that these mammoths built of artificial neurons will be ableto forget the data that we store in them? If they are at some level like abrain, can the right to be forgotten still be protected while dealing withthese AIs? The essential gap between machine learning and the RTBF is exploredin this article, with a premonition of far-reaching conclusions if the gap isnot bridged or reconciled any time soon. The core argument is that deeplearning models, due to their structure and size, cannot be expected to forgetor delete a data as it would be expected from a tabular database, and theyshould be treated more like a mechanical brain, albeit still in development.</div></details><p><a href=http://arxiv.org/abs/2305.05738v4><strong>DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors</strong></a></p><p><em>Chia-Hao Li, Niraj K. Jha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern advances in machine learning (ML) and wearable medical sensors (WMSs)in edge devices have enabled ML-driven disease detection for smart healthcare.Conventional ML-driven methods for disease detection rely on customizingindividual models for each disease and its corresponding WMS data. However,such methods lack adaptability to distribution shifts and new taskclassification classes. In addition, they need to be rearchitected andretrained from scratch for each new disease. Moreover, installing multiple MLmodels in an edge device consumes excessive memory, drains the battery faster,and complicates the detection process. To address these challenges, we proposeDOCTOR, a multi-disease detection continual learning (CL) framework based onWMSs. It employs a multi-headed deep neural network (DNN) and a replay-style CLalgorithm. The CL algorithm enables the framework to continually learn newmissions where different data distributions, classification classes, anddisease detection tasks are introduced sequentially. It counteractscatastrophic forgetting with a data preservation method and a synthetic datageneration (SDG) module. The data preservation method preserves the mostinformative subset of real training data from previous missions for exemplarreplay. The SDG module models the probability distribution of the real trainingdata and generates synthetic data for generative replay while retaining dataprivacy. The multi-headed DNN enables DOCTOR to detect multiple diseasessimultaneously based on user WMS data. We demonstrate DOCTOR&rsquo;s efficacy inmaintaining high disease classification accuracy with a single DNN model invarious CL experiments. In complex scenarios, DOCTOR achieves 1.43 times betteraverage test accuracy, 1.25 times better F1-score, and 0.41 higher backwardtransfer than the naive fine-tuning framework with a small model size of lessthan 350KB.</div></details><blockquote><p><strong><em>2024-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.02537v1><strong>Demolition and Reinforcement of Memories in Spin-Glass-like Neural Networks</strong></a></p><p><em>Enrico Ventura</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Statistical mechanics has made significant contributions to the study ofbiological neural systems by modeling them as recurrent networks ofinterconnected units with adjustable interactions. Several algorithms have beenproposed to optimize the neural connections to enable network tasks such asinformation storage (i.e. associative memory) and learning probabilitydistributions from data (i.e. generative modeling). Among these methods, theUnlearning algorithm, aligned with emerging theories of synaptic plasticity,was introduced by John Hopfield and collaborators. The primary objective ofthis thesis is to understand the effectiveness of Unlearning in bothassociative memory models and generative models. Initially, we demonstrate thatthe Unlearning algorithm can be simplified to a linear perceptron model whichlearns from noisy examples featuring specific internal correlations. Theselection of structured training data enables an associative memory model toretrieve concepts as attractors of a neural dynamics with considerable basinsof attraction. Subsequently, a novel regularization technique for BoltzmannMachines is presented, proving to outperform previously developed methods inlearning hidden probability distributions from data-sets. The Unlearning ruleis derived from this new regularized algorithm and is showed to be comparable,in terms of inferential performance, to traditional Boltzmann-Machine learning.</div></details><blockquote><p><strong><em>2024-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.01267v1><strong>Dissecting Language Models: Machine Unlearning via Selective Pruning</strong></a></p><p><em>Nicholas Pochinkov, Nandi Schoots</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) isincreasingly important as applications become more powerful and more frequentlyadopted. This paper introduces a machine unlearning method specificallydesigned for LLMs. We introduce a selective pruning method for LLMs thatremoves neurons based on their relative importance on a targeted capabilitycompared to overall network performance. This approach is a compute- anddata-efficient method for identifying and removing neurons that enable specificbehaviours. Our findings reveal that both feed-forward and attention neurons inLLMs are specialized; that is, for specific tasks, certain neurons are morecrucial than others.</div></details><blockquote><p><strong><em>2024-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.12508v4><strong>SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation</strong></a></p><p><em>Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With evolving data regulations, machine unlearning (MU) has become animportant tool for fostering trust and safety in today&rsquo;s AI models. However,existing MU methods focusing on data and/or weight perspectives often sufferlimitations in unlearning accuracy, stability, and cross-domain applicability.To address these challenges, we introduce the concept of &lsquo;weight saliency&rsquo; forMU, drawing parallels with input saliency in model explanation. This innovationdirects MU&rsquo;s attention toward specific model weights rather than the entiremodel, improving effectiveness and efficiency. The resultant method that wecall saliency unlearning (SalUn) narrows the performance gap with &rsquo;exact&rsquo;unlearning (model retraining from scratch after removing the forgetting datapoints). To the best of our knowledge, SalUn is the first principled MUapproach that can effectively erase the influence of forgetting data, classes,or concepts in both image classification and generation tasks. As highlightedbelow, For example, SalUn yields a stability advantage in high-variance randomdata forgetting, e.g., with a 0.2% gap compared to exact unlearning on theCIFAR-10 dataset. Moreover, in preventing conditional diffusion models fromgenerating harmful images, SalUn achieves nearly 100% unlearning accuracy,outperforming current state-of-the-art baselines like Erased Stable Diffusionand Forget-Me-Not. Codes are available athttps://github.com/OPTML-Group/Unlearn-Saliency. (WARNING: This paper containsmodel outputs that may be offensive in nature.)</div></details><blockquote><p><strong><em>2024-02-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.19308v1><strong>Loss-Free Machine Unlearning</strong></a></p><p><em>Jack Foster, Stefan Schoepf, Alexandra Brintrup</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We present a machine unlearning approach that is both retraining- andlabel-free. Most existing machine unlearning approaches require a model to befine-tuned to remove information while preserving performance. This iscomputationally expensive and necessitates the storage of the whole dataset forthe lifetime of the model. Retraining-free approaches often utilise Fisherinformation, which is derived from the loss and requires labelled data whichmay not be available. Thus, we present an extension to the Selective SynapticDampening algorithm, substituting the diagonal of the Fisher information matrixfor the gradient of the l2 norm of the model output to approximate sensitivity.We evaluate our method in a range of experiments using ResNet18 and VisionTransformer. Results show our label-free method is competitive with existingstate-of-the-art approaches.</div></details><p><a href=http://arxiv.org/abs/2312.16731v2><strong>Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization</strong></a></p><p><em>Sebastian Dziadzio, Çağatay Yıldız, Gido M. van de Ven, Tomasz Trzciński, Tinne Tuytelaars, Matthias Bethge</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ability of machine learning systems to learn continually is hindered bycatastrophic forgetting, the tendency of neural networks to overwrite existingknowledge when learning a new task. Continual learning methods alleviate thisproblem through regularization, parameter isolation, or rehearsal, but they aretypically evaluated on benchmarks comprising only a handful of tasks. Incontrast, humans are able to learn continually in dynamic, open-worldenvironments, effortlessly achieving one-shot memorization of unfamiliarobjects and reliably recognizing them under various transformations. To makeprogress towards closing this gap, we introduce Infinite dSprites, aparsimonious tool for creating continual classification and disentanglementbenchmarks of arbitrary length and with full control over generative factors.We show that over a sufficiently long time horizon, the performance of allmajor types of continual learning methods deteriorates on this simplebenchmark. Thus, Infinite dSprites highlights an important aspect of continuallearning that has not received enough attention so far: given a finitemodelling capacity and an arbitrarily long learning horizon, efficient learningrequires memorizing class-specific information and accumulating knowledge aboutgeneral mechanisms. In a simple setting with direct supervision on thegenerative factors, we show how learning class-agnostic transformations offersa way to circumvent catastrophic forgetting and improve classification accuracyover time. Our approach sets the stage for continual learning over hundreds oftasks with explicit control over memorization and forgetting, emphasizingopen-set classification and one-shot generalization.</div></details><blockquote><p><strong><em>2024-02-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.15159v2><strong>Machine Unlearning of Pre-trained Large Language Models</strong></a></p><p><em>Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This study investigates the concept of the `right to be forgotten&rsquo; within thecontext of large language models (LLMs). We explore machine unlearning as apivotal solution, with a focus on pre-trained models&ndash;a notablyunder-researched area. Our research delineates a comprehensive framework formachine unlearning in pre-trained LLMs, encompassing a critical analysis ofseven diverse unlearning methods. Through rigorous evaluation using curateddatasets from arXiv, books, and GitHub, we establish a robust benchmark forunlearning performance, demonstrating that these methods are over $10^5$ timesmore computationally efficient than retraining. Our results show thatintegrating gradient ascent with gradient descent on in-distribution dataimproves hyperparameter robustness. We also provide detailed guidelines forefficient hyperparameter tuning in the unlearning process. Our findings advancethe discourse on ethical AI practices, offering substantive insights into themechanics of machine unlearning for pre-trained LLMs and underscoring thepotential for responsible AI development.</div></details><blockquote><p><strong><em>2024-02-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.16835v1><strong>Eight Methods to Evaluate Robust Unlearning in LLMs</strong></a></p><p><em>Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan Hadfield-Menell</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning can be useful for removing harmful capabilities andmemorized text from large language models (LLMs), but there are not yetstandardized methods for rigorously evaluating it. In this paper, we firstsurvey techniques and limitations of existing unlearning evaluations. Second,we apply a comprehensive set of tests for the robustness and competitiveness ofunlearning in the &ldquo;Who&rsquo;s Harry Potter&rdquo; (WHP) model from Eldan and Russinovich(2023). While WHP&rsquo;s unlearning generalizes well when evaluated with the"Familiarity" metric from Eldan and Russinovich, we find i)higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHPperforms on par with the original model on Harry Potter Q&amp;A tasks, iii) itrepresents latent knowledge comparably to the original model, and iv) there iscollateral unlearning in related domains. Overall, our results highlight theimportance of comprehensive unlearning evaluation that avoids ad-hoc metrics.</div></details><p><a href=http://arxiv.org/abs/2402.11846v2><strong>UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models</strong></a></p><p><em>Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid advancement of diffusion models (DMs) has not only transformedvarious real-world industries but has also introduced negative societalconcerns, including the generation of harmful content, copyright disputes, andthe rise of stereotypes and biases. To mitigate these issues, machineunlearning (MU) has emerged as a potential solution, demonstrating its abilityto remove undesired generative capabilities of DMs in various applications.However, by examining existing MU evaluation methods, we uncover several keychallenges that can result in incomplete, inaccurate, or biased evaluations forMU in DMs. To address them, we enhance the evaluation metrics for MU, includingthe introduction of an often-overlooked retainability measurement for DMspost-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensivehigh-resolution stylized image dataset that facilitates us to evaluate theunlearning of artistic painting styles in conjunction with associated imageobjects. We show that this dataset plays a pivotal role in establishing astandardized and automated evaluation framework for MU techniques on DMs,featuring 7 quantitative metrics to address various aspects of unlearningeffectiveness. Through extensive experiments, we benchmark 5 state-of-the-artMU methods, revealing novel insights into their pros and cons, and theunderlying unlearning mechanisms. Furthermore, we demonstrate the potential ofUnlearnCanvas to benchmark other generative modeling tasks, such as styletransfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce allthe results in this work can be found athttps://github.com/OPTML-Group/UnlearnCanvas.</div></details><p><a href=http://arxiv.org/abs/2402.16933v1><strong>Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation</strong></a></p><p><em>Nicki Barari, Xin Lian, Christopher J. MacLellan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural networks have excelled in machine learning, particularly invision tasks, however, they often suffer from catastrophic forgetting whenlearning new tasks sequentially. In this work, we propose Cobweb4V, a novelvisual classification approach that builds on Cobweb, a human like learningsystem that is inspired by the way humans incrementally learn new concepts overtime. In this research, we conduct a comprehensive evaluation, showcasing theproficiency of Cobweb4V in learning visual concepts, requiring less data toachieve effective learning outcomes compared to traditional methods,maintaining stable performance over time, and achieving commendable asymptoticbehavior, without catastrophic forgetting effects. These characteristics alignwith learning strategies in human cognition, positioning Cobweb4V as apromising alternative to neural network approaches.</div></details><blockquote><p><strong><em>2024-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.04385v2><strong>Machine unlearning through fine-grained model parameters perturbation</strong></a></p><p><em>Zhiwei Zuo, Zhuo Tang, Kenli Li, Anwitaman Datta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning techniques, which involve retracting data records andreducing influence of said data on trained models, help with the user privacyprotection objective but incur significant computational costs. Weightperturbation-based unlearning is a general approach, but it typically involvesglobally modifying the parameters. We propose fine-grained Top-K and Random-kparameters perturbed inexact machine unlearning strategies that address theprivacy needs while keeping the computational costs tractable. In order to demonstrate the efficacy of our strategies we also tackle thechallenge of evaluating the effectiveness of machine unlearning by consideringthe model&rsquo;s generalization performance across both unlearning and remainingdata. To better assess the unlearning effect and model generalization, wepropose novel metrics, namely, the forgetting rate and memory retention rate.However, for inexact machine unlearning, current metrics are inadequate inquantifying the degree of forgetting that occurs after unlearning strategiesare applied. To address this, we introduce SPD-GAN, which subtly perturbs thedistribution of data targeted for unlearning. Then, we evaluate the degree ofunlearning by measuring the performance difference of the models on theperturbed unlearning data before and after the unlearning process. Byimplementing these innovative techniques and metrics, we achievecomputationally efficacious privacy protection in machine learning applicationswithout significant sacrifice of model performance. Furthermore, this approachprovides a novel method for evaluating the degree of unlearning.</div></details><p><a href=http://arxiv.org/abs/2402.15109v1><strong>Machine Unlearning by Suppressing Sample Contribution</strong></a></p><p><em>Xinwen Cheng, Zhehao Huang, Xiaolin Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning (MU) is to forget data from a well-trained model, which ispractically important due to the &ldquo;right to be forgotten&rdquo;. In this paper, westart from the fundamental distinction between training data and unseen data ontheir contribution to the model: the training data contributes to the finalmodel while the unseen data does not. We theoretically discover that the inputsensitivity can approximately measure the contribution and practically designan algorithm, called MU-Mis (machine unlearning via minimizing inputsensitivity), to suppress the contribution of the forgetting data. Experimentalresults demonstrate that MU-Mis outperforms state-of-the-art MU methodssignificantly. Additionally, MU-Mis aligns more closely with the application ofMU as it does not require the use of remaining data.</div></details><blockquote><p><strong><em>2024-02-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.18574v2><strong>Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning</strong></a></p><p><em>Zheyuan Liu, Guangyao Dou, Yijun Tian, Chunhui Zhang, Eli Chien, Ziwei Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning (MU) algorithms have become increasingly critical due tothe imperative adherence to data privacy regulations. The primary objective ofMU is to erase the influence of specific data samples on a given model withoutthe need to retrain it from scratch. Accordingly, existing methods focus onmaximizing user privacy protection. However, there are different degrees ofprivacy regulations for each real-world web-based application. Exploring thefull spectrum of trade-offs between privacy, model utility, and runtimeefficiency is critical for practical unlearning scenarios. Furthermore,designing the MU algorithm with simple control of the aforementioned trade-offis desirable but challenging due to the inherent complex interaction. Toaddress the challenges, we present Controllable Machine Unlearning (ConMU), anovel framework designed to facilitate the calibration of MU. The ConMUframework contains three integral modules: an important data selection modulethat reconciles the runtime efficiency and model generalization, a progressiveGaussian mechanism module that balances privacy and model generalization, andan unlearning proxy that controls the trade-offs between privacy and runtimeefficiency. Comprehensive experiments on various benchmark datasets havedemonstrated the robust adaptability of our control mechanism and itssuperiority over established unlearning methods. ConMU explores the fullspectrum of the Privacy-Utility-Efficiency trade-off and allows practitionersto account for different real-world regulations. Source code available at:https://github.com/guangyaodou/ConMU.</div></details><p><a href=http://arxiv.org/abs/2402.13463v2><strong>RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models</strong></a></p><p><em>Jianhao Yan, Yun Luo, Yue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The application scope of large language models (LLMs) is increasinglyexpanding. In practical use, users might provide feedback based on the model&rsquo;soutput, hoping for a responsive model that can complete responses according totheir feedback. Whether the model can appropriately respond to users&rsquo; refutingfeedback and consistently follow through with execution has not been thoroughlyanalyzed. In light of this, this paper proposes a comprehensive benchmark,RefuteBench, covering tasks such as question answering, machine translation,and email writing. The evaluation aims to assess whether models can positivelyaccept feedback in form of refuting instructions and whether they canconsistently adhere to user demands throughout the conversation. We conductevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibitinclination to their internal knowledge, often failing to comply with userfeedback. Additionally, as the length of the conversation increases, modelsgradually forget the user&rsquo;s stated feedback and roll back to their ownresponses. We further propose a recall-and-repeat prompts as a simple andeffective way to enhance the model&rsquo;s responsiveness to feedback.</div></details><blockquote><p><strong><em>2024-02-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.14015v1><strong>Corrective Machine Unlearning</strong></a></p><p><em>Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Learning models increasingly face data integrity challenges due tothe use of large-scale training datasets drawn from the internet. We study whatmodel developers can do if they detect that some data was manipulated orincorrect. Such manipulated data can cause adverse effects like vulnerabilityto backdoored samples, systematic biases, and in general, reduced accuracy oncertain input domains. Often, all manipulated training samples are not known,and only a small, representative subset of the affected data is flagged. We formalize &ldquo;Corrective Machine Unlearning&rdquo; as the problem of mitigating theimpact of data affected by unknown manipulations on a trained model, possiblyknowing only a subset of impacted samples. We demonstrate that the problem ofcorrective unlearning has significantly different requirements from traditionalprivacy-oriented unlearning. We find most existing unlearning methods,including the gold-standard retraining-from-scratch, require most of themanipulated data to be identified for effective corrective unlearning. However,one approach, SSD, achieves limited success in unlearning adverse effects withjust a small portion of the manipulated samples, showing the tractability ofthis setting. We hope our work spurs research towards developing better methodsfor corrective unlearning and offers practitioners a new strategy to handledata integrity challenges arising from web-scale training.</div></details><blockquote><p><strong><em>2024-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.01421v2><strong>Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting</strong></a></p><p><em>Elena Agliari, Francesco Alemanno, Miriam Aquaro, Alberto Fachechi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this work we approach attractor neural networks from a machine learningperspective: we look for optimal network parameters by applying a gradientdescent over a regularized loss function. Within this framework, the optimalneuron-interaction matrices turn out to be a class of matrices which correspondto Hebbian kernels revised by a reiterated unlearning protocol. Remarkably, theextent of such unlearning is proved to be related to the regularizationhyperparameter of the loss function and to the training time. Thus, we candesign strategies to avoid overfitting that are formulated in terms ofregularization and early-stopping tuning. The generalization capabilities ofthese attractor networks are also investigated: analytical results are obtainedfor random synthetic datasets, next, the emerging picture is corroborated bynumerical experiments that highlight the existence of several regimes (i.e.,overfitting, failure and success) as the dataset parameters are varied.</div></details><p><a href=http://arxiv.org/abs/2402.12987v1><strong>Towards Robust Graph Incremental Learning on Evolving Graphs</strong></a></p><p><em>Junwei Su, Difan Zou, Zijun Zhang, Chuan Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Incremental learning is a machine learning approach that involves training amodel on a sequence of tasks, rather than all tasks at once. This ability tolearn incrementally from a stream of tasks is crucial for many real-worldapplications. However, incremental learning is a challenging problem ongraph-structured data, as many graph-related problems involve prediction tasksfor each individual node, known as Node-wise Graph Incremental Learning (NGIL).This introduces non-independent and non-identically distributed characteristicsin the sample data generation process, making it difficult to maintain theperformance of the model as new tasks are added. In this paper, we focus on theinductive NGIL problem, which accounts for the evolution of graph structure(structural shift) induced by emerging tasks. We provide a formal formulationand analysis of the problem, and propose a novel regularization-based techniquecalled Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of thestructural shift on catastrophic forgetting of the inductive NGIL problem. Weshow that the structural shift can lead to a shift in the input distributionfor the existing tasks, and further lead to an increased risk of catastrophicforgetting. Through comprehensive empirical studies with several benchmarkdatasets, we demonstrate that our proposed method,Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt toimprove the performance of state-of-the-art GNN incremental learning frameworksin the inductive setting.</div></details><blockquote><p><strong><em>2024-02-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.15910v4><strong>Reinforcement Unlearning</strong></a></p><p><em>Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi, Sheng Shen, Wanlei Zhou, Minhui Xue</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning refers to the process of mitigating the influence ofspecific training data on machine learning models based on removal requestsfrom data owners. However, one important area that has been largely overlookedin the research of unlearning is reinforcement learning. Reinforcement learningfocuses on training an agent to make optimal decisions within an environment tomaximize its cumulative rewards. During the training, the agent tends tomemorize the features of the environment, which raises a significant concernabout privacy. As per data protection regulations, the owner of the environmentholds the right to revoke access to the agent&rsquo;s training data, thusnecessitating the development of a novel and pressing research field, known as\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revokingentire environments rather than individual data samples. This uniquecharacteristic presents three distinct challenges: 1) how to propose unlearningschemes for environments; 2) how to avoid degrading the agent&rsquo;s performance inremaining environments; and 3) how to evaluate the effectiveness of unlearning.To tackle these challenges, we propose two reinforcement unlearning methods.The first method is based on decremental reinforcement learning, which aims toerase the agent&rsquo;s previously acquired knowledge gradually. The second methodleverages environment poisoning attacks, which encourage the agent to learnnew, albeit incorrect, knowledge to remove the unlearning environment.Particularly, to tackle the third challenge, we introduce the concept of``environment inference attack&rsquo;&rsquo; to evaluate the unlearning outcomes.</div></details><p><a href=http://arxiv.org/abs/2402.11984v1><strong>Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks</strong></a></p><p><em>Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neuromorphic computing with spiking neural networks is promising forenergy-efficient artificial intelligence (AI) applications. However, differentfrom humans who continually learn different tasks in a lifetime, neural networkmodels suffer from catastrophic forgetting. How could neuronal operations solvethis problem is an important question for AI and neuroscience. Many previousstudies draw inspiration from observed neuroscience phenomena and proposeepisodic replay or synaptic metaplasticity, but they are not guaranteed toexplicitly preserve knowledge for neuron populations. Other works focus onmachine learning methods with more mathematical grounding, e.g., orthogonalprojection on high dimensional spaces, but there is no neural correspondencefor neuromorphic computing. In this work, we develop a new method with neuronaloperations based on lateral connections and Hebbian learning, which can protectknowledge by projecting activity traces of neurons into an orthogonal subspaceso that synaptic weight update will not interfere with old tasks. We show thatHebbian and anti-Hebbian learning on recurrent lateral connections caneffectively extract the principal subspace of neural activities and enableorthogonal projection. This provides new insights into how neural circuits andHebbian learning can help continual learning, and also how the concept oforthogonal projection can be realized in neuronal systems. Our method is alsoflexible to utilize arbitrary training methods based on presynapticactivities/traces. Experiments show that our method consistently solvesforgetting for spiking neural networks with nearly zero forgetting undervarious supervised training methods with different error propagationapproaches, and outperforms previous approaches under various settings. Ourmethod can pave a solid path for building continual neuromorphic computingsystems.</div></details><blockquote><p><strong><em>2024-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.14754v2><strong>Fair Machine Unlearning: Data Removal while Mitigating Disparities</strong></a></p><p><em>Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The Right to be Forgotten is a core principle outlined by regulatoryframeworks such as the EU&rsquo;s General Data Protection Regulation (GDPR). Thisprinciple allows individuals to request that their personal data be deletedfrom deployed machine learning models. While &ldquo;forgetting&rdquo; can be naivelyachieved by retraining on the remaining dataset, it is computationallyexpensive to do to so with each new request. As such, several machineunlearning methods have been proposed as efficient alternatives to retraining.These methods aim to approximate the predictive performance of retraining, butfail to consider how unlearning impacts other properties critical to real-worldapplications such as fairness. In this work, we demonstrate that most efficientunlearning methods cannot accommodate popular fairness interventions, and wepropose the first fair machine unlearning method that can efficiently unlearndata instances from a fair objective. We derive theoretical results whichdemonstrate that our method can provably unlearn data and provably maintainfairness performance. Extensive experimentation with real-world datasetshighlight the efficacy of our method at unlearning data instances whilepreserving fairness.</div></details><p><a href=http://arxiv.org/abs/2401.05146v2><strong>Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics</strong></a></p><p><em>Nicolò Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) enables collaborative training of a Machine Learning(ML) model across multiple parties, facilitating the preservation of users&rsquo; andinstitutions&rsquo; privacy by keeping data stored locally. Instead of centralizingraw data, FL exchanges locally refined model parameters to build a global modelincrementally. While FL is more compliant with emerging regulations such as theEuropean General Data Protection Regulation (GDPR), ensuring the right to beforgotten in this context - allowing FL participants to remove their datacontributions from the learned model - remains unclear. In addition, it isrecognized that malicious clients may inject backdoors into the global modelthrough updates, e.g. to generate mispredictions on specially crafted dataexamples. Consequently, there is the need for mechanisms that can guaranteeindividuals the possibility to remove their data and erase maliciouscontributions even after aggregation, without compromising the already acquired"good" knowledge. This highlights the necessity for novel Federated Unlearning(FU) algorithms, which can efficiently remove specific clients&rsquo; contributionswithout full model retraining. This survey provides background concepts,empirical evidence, and practical guidelines to design/implement efficient FUschemes. Our study includes a detailed analysis of the metrics for evaluatingunlearning in FL and presents an in-depth literature review categorizingstate-of-the-art FU contributions under a novel taxonomy. Finally, we outlinethe most relevant and still open technical challenges, by identifying the mostpromising research directions in the field.</div></details><blockquote><p><strong><em>2024-02-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.08787v2><strong>Rethinking Machine Unlearning for Large Language Models</strong></a></p><p><em>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.</div></details><blockquote><p><strong><em>2024-02-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.06864v2><strong>Discriminative Adversarial Unlearning</strong></a></p><p><em>Rohan Sharma, Shijie Zhou, Kaiyi Ji, Changyou Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce a novel machine unlearning framework founded upon theestablished principles of the min-max optimization paradigm. We capitalize onthe capabilities of strong Membership Inference Attacks (MIA) to facilitate theunlearning of specific samples from a trained model. We consider the scenarioof two networks, the attacker $\mathbf{A}$ and the trained defender$\mathbf{D}$ pitted against each other in an adversarial objective, wherein theattacker aims at teasing out the information of the data to be unlearned inorder to infer membership, and the defender unlearns to defend the networkagainst the attack, whilst preserving its general performance. The algorithmcan be trained end-to-end using backpropagation, following the well knowniterative min-max approach in updating the attacker and the defender. Weadditionally incorporate a self-supervised objective effectively addressing thefeature space discrepancies between the forget set and the validation set,enhancing unlearning performance. Our proposed algorithm closely approximatesthe ideal benchmark of retraining from scratch for both random sampleforgetting and class-wise forgetting schemes on standard machine-unlearningdatasets. Specifically, on the class unlearning scheme, the method demonstratesnear-optimal performance and comprehensively overcomes known methods over therandom sample forgetting scheme across all metrics and multiple network pruningstrategies.</div></details><p><a href=http://arxiv.org/abs/2402.08255v1><strong>Distal Interference: Exploring the Limits of Model-Based Continual Learning</strong></a></p><p><em>Heinrich van Deventer, Anna Sergeevna Bosman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is the sequential learning of different tasks by a machinelearning model. Continual learning is known to be hindered by catastrophicinterference or forgetting, i.e. rapid unlearning of earlier learned tasks whennew tasks are learned. Despite their practical success, artificial neuralnetworks (ANNs) are prone to catastrophic interference. This study analyses howgradient descent and overlapping representations between distant input pointslead to distal interference and catastrophic interference. Distal interferencerefers to the phenomenon where training a model on a subset of the domain leadsto non-local changes on other subsets of the domain. This study shows thatuniformly trainable models without distal interference must be exponentiallylarge. A novel antisymmetric bounded exponential layer B-spline ANNarchitecture named ABEL-Spline is proposed that can approximate any continuousfunction, is uniformly trainable, has polynomial computational complexity, andprovides some guarantees for distal interference. Experiments are presented todemonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are alsoevaluated on benchmark regression problems. It is concluded that the weakerdistal interference guarantees in ABEL-Splines are insufficient for model-onlycontinual learning. It is conjectured that continual learning with polynomialcomplexity models requires augmentation of the training data or algorithm.</div></details><blockquote><p><strong><em>2024-02-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.09824v2><strong>On the Costs and Benefits of Adopting Lifelong Learning for Software Analytics &ndash; Empirical Study on Brown Build and Risk Prediction</strong></a></p><p><em>Doriane Olewicki, Sarra Habchi, Mathieu Nayrolles, Mojtaba Faramarzi, Sarath Chandar, Bram Adams</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Nowadays, software analytics tools using machine learning (ML) models to, forexample, predict the risk of a code change are well established. However, asthe goals of a project shift over time, and developers and their habits change,the performance of said models tends to degrade (drift) over time. Currentretraining practices typically require retraining a new model from scratch on alarge updated dataset when performance decay is observed, thus incurring acomputational cost; also there is no continuity between the models as the pastmodel is discarded and ignored during the new model training. Even though theliterature has taken interest in online learning approaches, those have rarelybeen integrated and evaluated in industrial environments. This paper evaluatesthe use of lifelong learning (LL) for industrial use cases at Ubisoft,evaluating both the performance and the required computational effort incomparison to the retraining-from-scratch approaches commonly used by theindustry. LL is used to continuously build and maintain ML-based softwareanalytics tools using an incremental learner that progressively updates the oldmodel using new data. To avoid so-called &ldquo;catastrophic forgetting&rdquo; of importantolder data points, we adopt a replay buffer of older data, which still allowsus to drastically reduce the size of the overall training dataset, and hencemodel training time.</div></details><blockquote><p><strong><em>2024-02-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09418v2><strong>Unlearning regularization for Boltzmann Machines</strong></a></p><p><em>Enrico Ventura, Simona Cocco, Rémi Monasson, Francesco Zamponi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Boltzmann Machines (BMs) are graphical models with interconnected binaryunits, employed for the unsupervised modeling of data distributions. Whentrained on real data, BMs show the tendency to behave like critical systems,displaying a high susceptibility of the model under a small rescaling of theinferred parameters. This behaviour is not convenient for the purpose ofgenerating data, because it slows down the sampling process, and induces themodel to overfit the training-data. In this study, we introduce aregularization method for BMs to improve the robustness of the model underrescaling of the parameters. The new technique shares formal similarities withthe unlearning algorithm, an iterative procedure used to improve memoryassociativity in Hopfield-like neural networks. We test our unlearningregularization on synthetic data generated by two simple models, theCurie-Weiss ferromagnetic model and the Sherrington-Kirkpatrick spin glassmodel, and we show that it outperforms $L_p$-norm schemes. Finally, we discussthe role of parameter initialization.</div></details><blockquote><p><strong><em>2024-02-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.05813v1><strong>Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models</strong></a></p><p><em>Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The aim of this study is to investigate Machine Unlearning (MU), a burgeoningfield focused on addressing concerns related to neural models inadvertentlyretaining personal or sensitive data. Here, a novel approach is introduced toachieve precise and selective forgetting within language models. Unlikeprevious methodologies that adopt completely opposing training objectives, thisapproach aims to mitigate adverse effects on language model performance,particularly in generation tasks. Furthermore, two innovative evaluationmetrics are proposed: Sensitive Information Extraction Likelihood (S-EL) andSensitive Information Memory Accuracy (S-MA), designed to gauge theeffectiveness of sensitive information elimination. To reinforce the forgettingframework, an effective method for annotating sensitive scopes is presented,involving both online and offline strategies. The online selection mechanismleverages language probability scores to ensure computational efficiency, whilethe offline annotation entails a robust two-stage process based on LargeLanguage Models (LLMs).</div></details><p><a href=http://arxiv.org/abs/2402.10941v1><strong>Text2Data: Low-Resource Data Generation with Textual Control</strong></a></p><p><em>Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Natural language serves as a common and straightforward control signal forhumans to interact seamlessly with machines. Recognizing the importance of thisinterface, the machine learning community is investing considerable effort ingenerating data that is semantically coherent with textual instructions. Whilestrides have been made in text-to-data generation spanning image editing, audiosynthesis, video creation, and beyond, low-resource areas characterized byexpensive annotations or complex data structures, such as molecules, motiondynamics, and time series, often lack textual labels. This deficiency impedessupervised learning, thereby constraining the application of advancedgenerative models for text-to-data tasks. In response to these challenges inthe low-resource scenario, we propose Text2Data, a novel approach that utilizesunlabeled data to understand the underlying data distribution through anunsupervised diffusion model. Subsequently, it undergoes controllablefinetuning via a novel constraint optimization-based learning objective thatensures controllability and effectively counteracts catastrophic forgetting.Comprehensive experiments demonstrate that Text2Data is able to achieveenhanced performance regarding controllability across various modalities,including molecules, motions and time series, when compared to existingbaselines.</div></details><blockquote><p><strong><em>2024-02-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.10371v4><strong>Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning</strong></a></p><p><em>Eli Chien, Haoyu Wang, Ziang Chen, Pan Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has raised significant interest with the adoption of lawsensuring the ``right to be forgotten&rsquo;&rsquo;. Researchers have provided aprobabilistic notion of approximate unlearning under a similar definition ofDifferential Privacy (DP), where privacy is defined as statisticalindistinguishability to retraining from scratch. We propose Langevinunlearning, an unlearning framework based on noisy gradient descent withprivacy guarantees for approximate unlearning problems. Langevin unlearningunifies the DP learning process and the privacy-certified unlearning processwith many algorithmic benefits. These include approximate certified unlearningfor non-convex problems, complexity saving compared to retraining, sequentialand batch unlearning for multiple unlearning requests. We verify thepracticality of Langevin unlearning by studying its privacy-utility-complexitytrade-off via experiments on benchmark datasets, and also demonstrate itssuperiority against gradient-decent-plus-output-perturbation based approximateunlearning.</div></details><p><a href=http://arxiv.org/abs/2403.09681v1><strong>ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied to Vision Transformers</strong></a></p><p><em>Ikhyun Cho, Changyeon Park, Julia Hockenmaier</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning (MUL) is an arising field in machine learning that seeksto erase the learned information of specific training data points from atrained model. Despite the recent active research in MUL within computervision, the majority of work has focused on ResNet-based models. Given thatVision Transformers (ViT) have become the predominant model architecture, adetailed study of MUL specifically tailored to ViT is essential. In this paper,we present comprehensive experiments on ViTs using recent MUL algorithms anddatasets. We anticipate that our experiments, ablation studies, and findingscould provide valuable insights and inspire further research in this field.</div></details><p><a href=http://arxiv.org/abs/2402.05007v1><strong>Example-based Explanations for Random Forests using Machine Unlearning</strong></a></p><p><em>Tanmay Surve, Romila Pradhan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Tree-based machine learning models, such as decision trees and randomforests, have been hugely successful in classification tasks primarily becauseof their predictive power in supervised learning tasks and ease ofinterpretation. Despite their popularity and power, these models have beenfound to produce unexpected or discriminatory outcomes. Given theiroverwhelming success for most tasks, it is of interest to identify sources oftheir unexpected and discriminatory behavior. However, there has not been muchwork on understanding and debugging tree-based classifiers in the context offairness. We introduce FairDebugger, a system that utilizes recent advances in machineunlearning research to identify training data subsets responsible for instancesof fairness violations in the outcomes of a random forest classifier.FairDebugger generates top-$k$ explanations (in the form of coherent trainingdata subsets) for model unfairness. Toward this goal, FairDebugger firstutilizes machine unlearning to estimate the change in the tree structures ofthe random forest when parts of the underlying training data are removed, andthen leverages the Apriori algorithm from frequent itemset mining to reduce thesubset search space. We empirically evaluate our approach on three real-worlddatasets, and demonstrate that the explanations generated by FairDebugger areconsistent with insights from prior studies on these datasets.</div></details><blockquote><p><strong><em>2024-02-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.10098v1><strong>Parameter-tuning-free data entry error unlearning with adaptive selective synaptic dampening</strong></a></p><p><em>Stefan Schoepf, Jack Foster, Alexandra Brintrup</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Data entry constitutes a fundamental component of the machine learningpipeline, yet it frequently results in the introduction of labelling errors.When a model has been trained on a dataset containing such errors itsperformance is reduced. This leads to the challenge of efficiently unlearningthe influence of the erroneous data to improve the model performance withoutneeding to completely retrain the model. While model editing methods exist forcases in which the correct label for a wrong entry is known, we focus on thecase of data entry errors where we do not know the correct labels for theerroneous data. Our contribution is twofold. First, we introduce an extensionto the selective synaptic dampening unlearning method that removes the need forparameter tuning, making unlearning accessible to practitioners. We demonstratethe performance of this extension, adaptive selective synaptic dampening(ASSD), on various ResNet18 and Vision Transformer unlearning tasks. Second, wedemonstrate the performance of ASSD in a supply chain delay prediction problemwith labelling errors using real-world data where we randomly introduce variouslevels of labelling errors. The application of this approach is particularlycompelling in industrial settings, such as supply chain management, where asignificant portion of data entry occurs manually through Excel sheets,rendering it error-prone. ASSD shows strong performance on general unlearningbenchmarks and on the error correction problem where it outperforms fine-tuningfor error correction.</div></details><blockquote><p><strong><em>2024-02-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01401v2><strong>Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization</strong></a></p><p><em>Jack Foster, Kyle Fogarty, Stefan Schoepf, Cengiz Öztireli, Alexandra Brintrup</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: To comply with AI and data regulations, the need to forget private orcopyrighted information from trained machine learning models is increasinglyimportant. The key challenge in unlearning is forgetting the necessary data ina timely manner, while preserving model performance. In this work, we addressthe zero-shot unlearning scenario, whereby an unlearning algorithm must be ableto remove data given only a trained model and the data to be forgotten. Undersuch a definition, existing state-of-the-art methods are insufficient. Buildingon the concepts of Lipschitz continuity, we present a method that inducessmoothing of the forget sample&rsquo;s output, with respect to perturbations of thatsample. We show this smoothing successfully results in forgetting whilepreserving general model performance. We perform extensive empirical evaluationof our method over a range of contemporary benchmarks, verifying that ourmethod achieves state-of-the-art performance under the strict constraints ofzero-shot unlearning.</div></details><blockquote><p><strong><em>2024-02-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.02028v1><strong>Unlearnable Examples For Time Series</strong></a></p><p><em>Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Unlearnable examples (UEs) refer to training samples modified to beunlearnable to Deep Neural Networks (DNNs). These examples are usuallygenerated by adding error-minimizing noises that can fool a DNN model intobelieving that there is nothing (no error) to learn from the data. The conceptof UE has been proposed as a countermeasure against unauthorized dataexploitation on personal data. While UE has been extensively studied on images,it is unclear how to craft effective UEs for time series data. In this work, weintroduce the first UE generation method to protect time series data fromunauthorized training by deep learning models. To this end, we propose a newform of error-minimizing noise that can be \emph{selectively} applied tospecific segments of time series, rendering them unlearnable to DNN modelswhile remaining imperceptible to human observers. Through extensive experimentson a wide range of time series datasets, we demonstrate that the proposed UEgeneration method is effective in both classification and generation tasks. Itcan protect time series data against unauthorized exploitation, whilepreserving their utility for legitimate usage, thereby contributing to thedevelopment of secure and trustworthy machine learning systems.</div></details><p><a href=http://arxiv.org/abs/2402.05947v1><strong>Separable Multi-Concept Erasure from Diffusion Models</strong></a></p><p><em>Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong, Baocai Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale diffusion models, known for their impressive image generationcapabilities, have raised concerns among researchers regarding social impacts,such as the imitation of copyrighted artistic styles. In response, existingapproaches turn to machine unlearning techniques to eliminate unsafe conceptsfrom pre-trained models. However, these methods compromise the generativeperformance and neglect the coupling among multi-concept erasures, as well asthe concept restoration problem. To address these issues, we propose aSeparable Multi-concept Eraser (SepME), which mainly includes two parts: thegeneration of concept-irrelevant representations and the weight decoupling. Theformer aims to avoid unlearning substantial information that is irrelevant toforgotten concepts. The latter separates optimizable model weights, making eachweight increment correspond to a specific concept erasure without affectinggenerative performance on other concepts. Specifically, the weight incrementfor erasing a specified concept is formulated as a linear combination ofsolutions calculated based on other known undesirable concepts. Extensiveexperiments indicate the efficacy of our approach in eliminating concepts,preserving model performance, and offering flexibility in the erasure orrecovery of various concepts.</div></details><p><a href=http://arxiv.org/abs/2305.13654v3><strong>Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis</strong></a></p><p><em>Oscar Chew, Hsuan-Tien Lin, Kai-Wei Chang, Kuan-Hao Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent research has revealed that machine learning models have a tendency toleverage spurious correlations that exist in the training set but may not holdtrue in general circumstances. For instance, a sentiment classifier mayerroneously learn that the token &ldquo;performances&rdquo; is commonly associated withpositive movie reviews. Relying on these spurious correlations degrades theclassifiers performance when it deploys on out-of-distribution data. In thispaper, we examine the implications of spurious correlations through a novelperspective called neighborhood analysis. The analysis uncovers how spuriouscorrelations lead unrelated words to erroneously cluster together in theembedding space. Driven by the analysis, we design a metric to detect spurioustokens and also propose a family of regularization methods, NFL (doN&rsquo;t Forgetyour Language) to mitigate spurious correlations in text classification.Experiments show that NFL can effectively prevent erroneous clusters andsignificantly improve the robustness of classifiers without auxiliary data. Thecode is publicly available athttps://github.com/oscarchew/doNt-Forget-your-Language.</div></details><blockquote><p><strong><em>2024-02-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00351v2><strong>Machine Unlearning for Image-to-Image Generative Models</strong></a></p><p><em>Guihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has emerged as a new paradigm to deliberately forget datasamples from a given model in order to adhere to stringent regulations.However, existing machine unlearning methods have been primarily focused onclassification models, leaving the landscape of unlearning for generativemodels relatively unexplored. This paper serves as a bridge, addressing the gapby providing a unifying framework of machine unlearning for image-to-imagegenerative models. Within this framework, we propose acomputationally-efficient algorithm, underpinned by rigorous theoreticalanalysis, that demonstrates negligible performance degradation on the retainsamples, while effectively removing the information from the forget samples.Empirical studies on two large-scale datasets, ImageNet-1K and Places-365,further show that our algorithm does not rely on the availability of the retainsamples, which further complies with data retention policy. To our bestknowledge, this work is the first that represents systemic, theoretical,empirical explorations of machine unlearning specifically tailored forimage-to-image generative models. Our code is available athttps://github.com/jpmorganchase/l2l-generator-unlearning.</div></details><p><a href=http://arxiv.org/abs/2309.10283v3><strong>FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning</strong></a></p><p><em>Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Xiaofeng Zhu, Qing Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning is an emerging field that addresses data privacy issues byenabling the removal of private or irrelevant data from the Machine Learningprocess. Challenges related to privacy and model efficiency arise from the useof outdated, private, and irrelevant data. These issues compromise both theaccuracy and the computational efficiency of models in both Machine Learningand Unlearning. To mitigate these challenges, we introduce a novel framework,Attention-based Machine Unlearning using Federated Reinforcement Learning(FRAMU). This framework incorporates adaptive learning mechanisms, privacypreservation techniques, and optimization strategies, making it a well-roundedsolution for handling various data sources, either single-modality ormulti-modality, while maintaining accuracy and privacy. FRAMU&rsquo;s strength liesin its adaptability to fluctuating data landscapes, its ability to unlearnoutdated, private, or irrelevant data, and its support for continual modelevolution without compromising privacy. Our experiments, conducted on bothsingle-modality and multi-modality datasets, revealed that FRAMU significantlyoutperformed baseline models. Additional assessments of convergence behaviorand optimization strategies further validate the framework&rsquo;s utility infederated learning applications. Overall, FRAMU advances Machine Unlearning byoffering a robust, privacy-preserving solution that optimizes model performancewhile also addressing key challenges in dynamic data environments.</div></details><blockquote><p><strong><em>2024-02-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16136v2><strong>ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach</strong></a></p><p><em>Yuke Hu, Jian Lou, Jiaqi Liu, Wangze Ni, Feng Lin, Zhan Qin, Kui Ren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Over the past years, Machine Learning-as-a-Service (MLaaS) has received asurging demand for supporting Machine Learning-driven services to offerrevolutionized user experience across diverse application areas. MLaaS providesinference service with low inference latency based on an ML model trained usinga dataset collected from numerous individual data owners. Recently, for thesake of data owners&rsquo; privacy and to comply with the &ldquo;right to be forgotten(RTBF)&rdquo; as enacted by data protection legislation, many machine unlearningmethods have been proposed to remove data owners&rsquo; data from trained models upontheir unlearning requests. However, despite their promising efficiency, almostall existing machine unlearning methods handle unlearning requestsindependently from inference requests, which unfortunately introduces a newsecurity issue of inference service obsolescence and a privacy vulnerability ofundesirable exposure for machine unlearning in MLaaS. In this paper, we propose the ERASER framework for machinE unleaRning inMLaAS via an inferencE seRving-aware approach. ERASER strategically chooseappropriate unlearning execution timing to address the inference serviceobsolescence issue. A novel inference consistency certification mechanism isproposed to avoid the violation of RTBF principle caused by postponedunlearning executions, thereby mitigating the undesirable exposurevulnerability. ERASER offers three groups of design choices to allow fortailor-made variants that best suit the specific environments and preferencesof various MLaaS systems. Extensive empirical evaluations across varioussettings confirm ERASER&rsquo;s effectiveness, e.g., it can effectively save up to99% of inference latency and 31% of computation overhead over theinference-oblivion baseline.</div></details><p><a href=http://arxiv.org/abs/2402.00751v1><strong>Unlearnable Algorithms for In-context Learning</strong></a></p><p><em>Andrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning is a desirable operation as models get increasinglydeployed on data with unknown provenance. However, achieving exact unlearning&ndash; obtaining a model that matches the model distribution when the data to beforgotten was never used &ndash; is challenging or inefficient, often requiringsignificant retraining. In this paper, we focus on efficient unlearning methodsfor the task adaptation phase of a pretrained large language model (LLM). Weobserve that an LLM&rsquo;s ability to do in-context learning for task adaptationallows for efficient exact unlearning of task adaptation training data. Weprovide an algorithm for selecting few-shot training examples to prepend to theprompt given to an LLM (for task adaptation), ERASE, whose unlearning operationcost is independent of model and dataset size, meaning it scales to largemodels and datasets. We additionally compare our approach to fine-tuningapproaches and discuss the trade-offs between the two approaches. This leads usto propose a new holistic measure of unlearning cost which accounts for varyinginference costs, and conclude that in-context learning can often be morefavourable than fine-tuning for deployments involving unlearning requests.</div></details><p><a href=http://arxiv.org/abs/2305.06360v6><strong>Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy</strong></a></p><p><em>Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Xiaofeng Zhu, Qing Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning (MU) is gaining increasing attention due to the need toremove or modify predictions made by machine learning (ML) models. Whiletraining models have become more efficient and accurate, the importance ofunlearning previously learned information has become increasingly significantin fields such as privacy, security, and fairness. This paper presents acomprehensive survey of MU, covering current state-of-the-art techniques andapproaches, including data deletion, perturbation, and model updates. Inaddition, commonly used metrics and datasets are also presented. The paper alsohighlights the challenges that need to be addressed, including attacksophistication, standardization, transferability, interpretability, trainingdata, and resource constraints. The contributions of this paper includediscussions about the potential benefits of MU and its future directions.Additionally, the paper emphasizes the need for researchers and practitionersto continue exploring and refining unlearning techniques to ensure that MLmodels can adapt to changing circumstances while maintaining user trust. Theimportance of unlearning is further highlighted in making ArtificialIntelligence (AI) more trustworthy and transparent, especially with theincreasing importance of AI in various domains that involve large amounts ofpersonal user data.</div></details><blockquote><p><strong><em>2024-01-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00195v1><strong>Dataset Condensation Driven Machine Unlearning</strong></a></p><p><em>Junaid Iqbal Khan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The current trend in data regulation requirements and privacy-preservingmachine learning has emphasized the importance of machine unlearning. The naiveapproach to unlearning training data by retraining over the complement of theforget samples is susceptible to computational challenges. These challengeshave been effectively addressed through a collection of techniques fallingunder the umbrella of machine unlearning. However, there still exists a lack ofsufficiency in handling persistent computational challenges in harmony with theutility and privacy of unlearned model. We attribute this to the lack of workon improving the computational complexity of approximate unlearning from theperspective of the training dataset. In this paper, we aim to fill this gap byintroducing dataset condensation as an essential component of machineunlearning in the context of image classification. To achieve this goal, wepropose new dataset condensation techniques and an innovative unlearning schemethat strikes a balance between machine unlearning privacy, utility, andefficiency. Furthermore, we present a novel and effective approach toinstrumenting machine unlearning and propose its application in defendingagainst membership inference and model inversion attacks. Additionally, weexplore a new application of our approach, which involves removing data from`condensed model&rsquo;, which can be employed to quickly train any arbitrary modelwithout being influenced by unlearning samples.</div></details><p><a href=http://arxiv.org/abs/2201.09196v2><strong>Learning to Predict Gradients for Semi-Supervised Continual Learning</strong></a></p><p><em>Yan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A key challenge for machine intelligence is to learn new visual conceptswithout forgetting the previously acquired knowledge. Continual learning isaimed towards addressing this challenge. However, there is a gap betweenexisting supervised continual learning and human-like intelligence, where humanis able to learn from both labeled and unlabeled data. How unlabeled dataaffects learning and catastrophic forgetting in the continual learning processremains unknown. To explore these issues, we formulate a new semi-supervisedcontinual learning method, which can be generically applied to existingcontinual learning models. Specifically, a novel gradient learner learns fromlabeled data to predict gradients on unlabeled data. Hence, the unlabeled datacould fit into the supervised continual learning method. Different fromconventional semi-supervised settings, we do not hypothesize that theunderlying classes, which are associated to the unlabeled data, are known tothe learning process. In other words, the unlabeled data could be very distinctfrom the labeled data. We evaluate the proposed method on mainstream continuallearning, adversarial continual learning, and semi-supervised learning tasks.The proposed method achieves state-of-the-art performance on classificationaccuracy and backward transfer in the continual learning setting whileachieving desired performance on classification accuracy in the semi-supervisedlearning setting. This implies that the unlabeled images can enhance thegeneralizability of continual learning models on the predictive ability onunseen data and significantly alleviate catastrophic forgetting. The code isavailable at \url{https://github.com/luoyan407/grad_prediction.git}.</div></details><blockquote><p><strong><em>2024-01-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.17504v1><strong>CaMU: Disentangling Causal Effects in Deep Model Unlearning</strong></a></p><p><em>Shaofei Shen, Chenhao Zhang, Alina Bialkowski, Weitong Chen, Miao Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning requires removing the information of forgetting data whilekeeping the necessary information of remaining data. Despite recentadvancements in this area, existing methodologies mainly focus on the effect ofremoving forgetting data without considering the negative impact this can haveon the information of the remaining data, resulting in significant performancedegradation after data removal. Although some methods try to repair theperformance of remaining data after removal, the forgotten information can alsoreturn after repair. Such an issue is due to the intricate intertwining of theforgetting and remaining data. Without adequately differentiating the influenceof these two kinds of data on the model, existing algorithms take the risk ofeither inadequate removal of the forgetting data or unnecessary loss ofvaluable information from the remaining data. To address this shortcoming, thepresent study undertakes a causal analysis of the unlearning and introduces anovel framework termed Causal Machine Unlearning (CaMU). This framework addsintervention on the information of remaining data to disentangle the causaleffects between forgetting data and remaining data. Then CaMU eliminates thecausal impact associated with forgetting data while concurrently preserving thecausal relevance of the remaining data. Comprehensive empirical results onvarious datasets and models suggest that CaMU enhances performance on theremaining data and effectively minimizes the influences of forgetting data.Notably, this work is the first to interpret deep model unlearning tasks from anew perspective of causality and provide a solution based on causal analysis,which opens up new possibilities for future research in deep model unlearning.</div></details><blockquote><p><strong><em>2024-01-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.20448v2><strong>A Survey on Federated Unlearning: Challenges, Methods, and Future Directions</strong></a></p><p><em>Ziyao Liu, Yu Jiang, Jiyuan Shen, Minyi Peng, Kwok-Yan Lam, Xingliang Yuan, Xiaoning Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, the notion of &ldquo;the right to be forgotten&rdquo; (RTBF) has evolvedinto a fundamental element of data privacy regulations, affording individualsthe ability to request the removal of their personal data from digital records.Consequently, given the extensive adoption of data-intensive machine learning(ML) algorithms and increasing concerns for personal data privacy protection,the concept of machine unlearning (MU) has gained considerable attention. MUempowers an ML model to selectively eliminate sensitive or personallyidentifiable information it acquired during the training process. Evolving fromthe foundational principles of MU, federated unlearning (FU) has emerged toconfront the challenge of data erasure within the domain of federated learning(FL) settings. This empowers the FL model to unlearn an FL client oridentifiable information pertaining to the client while preserving theintegrity of the decentralized learning process. Nevertheless, unliketraditional MU, the distinctive attributes of federated learning introducespecific challenges for FU techniques. These challenges lead to the need fortailored design when designing FU algorithms. Therefore, this comprehensivesurvey delves into the techniques, methodologies, and recent advancements infederated unlearning. It provides an overview of fundamental concepts andprinciples, evaluates existing federated unlearning algorithms, reviewsoptimizations tailored to federated learning, engages in discussions regardingpractical applications, along with an assessment of their limitations, andoutlines promising directions for future research.</div></details><p><a href=http://arxiv.org/abs/2401.15917v1><strong>Blockchain-enabled Trustworthy Federated Unlearning</strong></a></p><p><em>Yijing Lin, Zhipeng Gao, Hongyang Du, Jinke Ren, Zhiqiang Xie, Dusit Niyato</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated unlearning is a promising paradigm for protecting the dataownership of distributed clients. It allows central servers to removehistorical data effects within the machine learning model as well as addressthe &ldquo;right to be forgotten&rdquo; issue in federated learning. However, existingworks require central servers to retain the historical model parameters fromdistributed clients, such that allows the central server to utilize theseparameters for further training even, after the clients exit the trainingprocess. To address this issue, this paper proposes a new blockchain-enabledtrustworthy federated unlearning framework. We first design a proof offederated unlearning protocol, which utilizes the Chameleon hash function toverify data removal and eliminate the data contributions stored in otherclients&rsquo; models. Then, an adaptive contribution-based retraining mechanism isdeveloped to reduce the computational overhead and significantly improve thetraining efficiency. Extensive experiments demonstrate that the proposedframework can achieve a better data removal effect than the state-of-the-artframeworks, marking a significant stride towards trustworthy federatedunlearning.</div></details><blockquote><p><strong><em>2024-01-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.04934v13><strong>Model Sparsity Can Simplify Machine Unlearning</strong></a></p><p><em>Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In response to recent data regulation requirements, machine unlearning (MU)has emerged as a critical process to remove the influence of specific examplesfrom a given model. Although exact unlearning can be achieved through completemodel retraining using the remaining dataset, the associated computationalcosts have driven the development of efficient, approximate unlearningtechniques. Moving beyond data-centric MU approaches, our study introduces anovel model-based perspective: model sparsification via weight pruning, whichis capable of reducing the gap between exact unlearning and approximateunlearning. We show in both theory and practice that model sparsity can boostthe multi-criteria unlearning performance of an approximate unlearner, closingthe approximation gap, while continuing to be efficient. This leads to a new MUparadigm, termed prune first, then unlearn, which infuses a sparse model priorinto the unlearning process. Building on this insight, we also develop asparsity-aware unlearning method that utilizes sparsity regularization toenhance the training process of approximate unlearning. Extensive experimentsshow that our proposals consistently benefit MU in various unlearningscenarios. A notable highlight is the 77% unlearning efficacy gain offine-tuning (one of the simplest unlearning methods) when using sparsity-awareunlearning. Furthermore, we demonstrate the practical impact of our proposed MUmethods in addressing other machine learning challenges, such as defendingagainst backdoor attacks and enhancing transfer learning. Codes are availableat <a href=https://github.com/OPTML-Group/Unlearn-Sparse>https://github.com/OPTML-Group/Unlearn-Sparse</a>.</div></details><blockquote><p><strong><em>2024-01-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.12857v1><strong>Simultaneous exercise recognition and evaluation in prescribed routines: Approach to virtual coaches</strong></a></p><p><em>Sara García-de-Villa, David Casillas-Pérez, Ana Jiménez-Martín, Juan Jesús García-Domínguez</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Home-based physical therapies are effective if the prescribed exercises arecorrectly executed and patients adhere to these routines. This is speciallyimportant for older adults who can easily forget the guidelines fromtherapists. Inertial Measurement Units (IMUs) are commonly used for trackingexercise execution giving information of patients&rsquo; motion data. In this work,we propose the use of Machine Learning techniques to recognize which exerciseis being carried out and to assess if the recognized exercise is properlyexecuted by using data from four IMUs placed on the person limbs. To the bestof our knowledge, both tasks have never been addressed together as a uniquecomplex task before. However, their combination is needed for the completecharacterization of the performance of physical therapies. We evaluate theperformance of six machine learning classifiers in three contexts: recognitionand evaluation in a single classifier, recognition of correct exercises,excluding the wrongly performed exercises, and a two-stage approach that firstrecognizes the exercise and then evaluates it. We apply our proposal to a setof 8 exercises of the upper-and lower-limbs designed for maintaining elderlypeople health status. To do so, the motion of volunteers were monitored with 4IMUs. We obtain accuracies of 88.4 % and the 91.4 % in the two initialscenarios. In the third one, the recognition provides an accuracy of 96.2 %,whereas the exercise evaluation varies between 93.6 % and 100.0 %. This workproves the feasibility of IMUs for a complete monitoring of physical therapiesin which we can get information of which exercise is being performed and itsquality, as a basis for designing virtual coaches.</div></details><blockquote><p><strong><em>2024-01-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.11760v1><strong>Towards Effective and General Graph Unlearning via Mutual Evolution</strong></a></p><p><em>Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid advancement of AI applications, the growing needs for dataprivacy and model robustness have highlighted the importance of machineunlearning, especially in thriving graph-based scenarios. However, mostexisting graph unlearning strategies primarily rely on well-designedarchitectures or manual process, rendering them less user-friendly and posingchallenges in terms of deployment efficiency. Furthermore, striking a balancebetween unlearning performance and framework generalization is also a pivotalconcern. To address the above issues, we propose \underline{\textbf{M}}utual\underline{\textbf{E}}volution \underline{\textbf{G}}raph\underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm thatsimultaneously evolves the predictive and unlearning capacities of graphunlearning. By incorporating aforementioned two components, MEGU ensurescomplementary optimization in a unified training framework that aligns with theprediction and unlearning requirements. Extensive experiments on 9 graphbenchmark datasets demonstrate the superior performance of MEGU in addressingunlearning requirements at the feature, node, and edge levels. Specifically,MEGU achieves average performance improvements of 2.7%, 2.5%, and 3.2%across these three levels of unlearning tasks when compared to state-of-the-artbaselines. Furthermore, MEGU exhibits satisfactory training efficiency,reducing time and space overhead by an average of 159.8x and 9.6x,respectively, in comparison to retraining GNN from scratch.</div></details><blockquote><p><strong><em>2024-01-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.10458v1><strong>Contrastive Unlearning: A Contrastive Approach to Machine Unlearning</strong></a></p><p><em>Hong kyu Lee, Qiuchen Zhang, Carl Yang, Jian Lou, Li Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning aims to eliminate the influence of a subset of trainingsamples (i.e., unlearning samples) from a trained model. Effectively andefficiently removing the unlearning samples without negatively impacting theoverall model performance is still challenging. In this paper, we propose acontrastive unlearning framework, leveraging the concept of representationlearning for more effective unlearning. It removes the influence of unlearningsamples by contrasting their embeddings against the remaining samples so thatthey are pushed away from their original classes and pulled toward otherclasses. By directly optimizing the representation space, it effectivelyremoves the influence of unlearning samples while maintaining therepresentations learned from the remaining samples. Experiments on a variety ofdatasets and models on both class unlearning and sample unlearning showed thatcontrastive unlearning achieves the best unlearning effects and efficiency withthe lowest performance loss compared with the state-of-the-art algorithms.</div></details><blockquote><p><strong><em>2024-01-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.16467v2><strong>Compositional Program Generation for Few-Shot Systematic Generalization</strong></a></p><p><em>Tim Klinger, Luke Liu, Soham Dan, Maxwell Crouse, Parikshit Ram, Alexander Gray</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Compositional generalization is a key ability of humans that enables us tolearn new concepts from only a handful examples. Neural machine learningmodels, including the now ubiquitous Transformers, struggle to generalize inthis way, and typically require thousands of examples of a concept duringtraining in order to generalize meaningfully. This difference in abilitybetween humans and artificial neural architectures, motivates this study on aneuro-symbolic architecture called the Compositional Program Generator (CPG).CPG has three key features: \textit{modularity}, \textit{composition}, and\textit{abstraction}, in the form of grammar rules, that enable it togeneralize both systematically to new concepts in a few-shot manner, as well asproductively by length on various sequence-to-sequence language tasks. For eachinput, CPG uses a grammar of the input language and a parser to generate aparse in which each grammar rule is assigned its own unique semantic module, aprobabilistic copy or substitution program. Instances with the same parse arealways processed with the same composed modules, while those with differentparses may be processed with different modules. CPG learns parameters for themodules and is able to learn the semantics for new rules and typesincrementally, without forgetting or retraining on rules it&rsquo;s already seen. Itachieves perfect generalization on both the SCAN and COGS benchmarks using just14 examples for SCAN and 22 examples for COGS &ndash; state-of-the-art accuracy witha 1000x improvement in sample efficiency.</div></details><blockquote><p><strong><em>2024-01-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.08998v1><strong>Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization</strong></a></p><p><em>Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With growing concerns surrounding privacy and regulatory compliance, theconcept of machine unlearning has gained prominence, aiming to selectivelyforget or erase specific learned information from a trained model. In responseto this critical need, we introduce a novel approach called Attack-and-Resetfor Unlearning (ARU). This algorithm leverages meticulously crafted adversarialnoise to generate a parameter mask, effectively resetting certain parametersand rendering them unlearnable. ARU outperforms current state-of-the-artresults on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC.In particular, we present the steps involved in attacking and masking thatstrategically filter and re-initialize network parameters biased towards theforget set. Our work represents a significant advancement in rendering dataunexploitable to deep learning models through parameter re-initialization,achieved by harnessing adversarial noise to craft a mask.</div></details><p><a href=http://arxiv.org/abs/2401.10942v1><strong>Machine Unlearning for Recommendation Systems: An Insight</strong></a></p><p><em>Bhavika Sachdeva, Harshita Rathee, Sristi, Arun Sharma, Witold Wydmański</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This review explores machine unlearning (MUL) in recommendation systems,addressing adaptability, personalization, privacy, and bias challenges. Unliketraditional models, MUL dynamically adjusts system knowledge based on shifts inuser preferences and ethical considerations. The paper critically examinesMUL&rsquo;s basics, real-world applications, and challenges like algorithmictransparency. It sifts through literature, offering insights into how MUL couldtransform recommendations, discussing user trust, and suggesting paths forfuture research in responsible and user-focused artificial intelligence (AI).The document guides researchers through challenges involving the trade-offbetween personalization and privacy, encouraging contributions to meetpractical demands for targeted data removal. Emphasizing MUL&rsquo;s role in secureand adaptive machine learning, the paper proposes ways to push its boundaries.The novelty of this paper lies in its exploration of the limitations of themethods, which highlights exciting prospects for advancing the field.</div></details><blockquote><p><strong><em>2024-01-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.12056v9><strong>Continual learning under domain transfer with sparse synaptic bursting</strong></a></p><p><em>Shawn L. Beaulieu, Jeff Clune, Nick Cheney</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Existing machines are functionally specific tools that were made for easyprediction and control. Tomorrow&rsquo;s machines may be closer to biological systemsin their mutability, resilience, and autonomy. But first they must be capableof learning and retaining new information without being exposed to itarbitrarily often. Past efforts to engineer such systems have sought to buildor regulate artificial neural networks using disjoint sets of weights that areuniquely sensitive to specific tasks or inputs. This has not yet enabledcontinual learning over long sequences of previously unseen data withoutcorrupting existing knowledge: a problem known as catastrophic forgetting. Inthis paper, we introduce a system that can learn sequentially over previouslyunseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This isdone by controlling the activity of weights in a convolutional neural networkon the basis of inputs using top-down regulation generated by a secondfeed-forward neural network. We find that our method learns continually underdomain transfer with sparse bursts of activity in weights that are recycledacross tasks, rather than by maintaining task-specific modules. Sparse synapticbursting is found to balance activity and suppression such that new functionscan be learned without corrupting extant knowledge, thus mirroring the balanceof order and disorder in systems at the edge of chaos. This behavior emergesduring a prior pre-training (or &lsquo;meta-learning&rsquo;) phase in which regulatedsynapses are selectively disinhibited, or grown, from an initial state ofuniform suppression through prediction error minimization.</div></details><blockquote><p><strong><em>2024-01-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.08230v2><strong>A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services</strong></a></p><p><em>Hongsheng Hu, Shuo Wang, Jiamin Chang, Haonan Zhong, Ruoxi Sun, Shuang Hao, Haojin Zhu, Minhui Xue</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten requires the removal or &ldquo;unlearning&rdquo; of a user&rsquo;sdata from machine learning models. However, in the context of Machine Learningas a Service (MLaaS), retraining a model from scratch to fulfill the unlearningrequest is impractical due to the lack of training data on the serviceprovider&rsquo;s side (the server). Furthermore, approximate unlearning furtherembraces a complex trade-off between utility (model performance) and privacy(unlearning performance). In this paper, we try to explore the potentialthreats posed by unlearning services in MLaaS, specifically over-unlearning,where more information is unlearned than expected. We propose two strategiesthat leverage over-unlearning to measure the impact on the trade-off balancing,under black-box access settings, in which the existing machine unlearningattacks are not applicable. The effectiveness of these strategies is evaluatedthrough extensive experiments on benchmark datasets, across various modelarchitectures and representative unlearning approaches. Results indicatesignificant potential for both strategies to undermine model efficacy inunlearning scenarios. This study uncovers an underexplored gap betweenunlearning and contemporary MLaaS, highlighting the need for carefulconsiderations in balancing data unlearning, model utility, and security.</div></details><blockquote><p><strong><em>2024-01-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.18252v2><strong>Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI</strong></a></p><p><em>Dawen Zhang, Boming Xia, Yue Liu, Xiwei Xu, Thong Hoang, Zhenchang Xing, Mark Staples, Qinghua Lu, Liming Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of Generative AI has marked a significant milestone in artificialintelligence, demonstrating remarkable capabilities in generating realisticimages, texts, and data patterns. However, these advancements come withheightened concerns over data privacy and copyright infringement, primarily dueto the reliance on vast datasets for model training. Traditional approacheslike differential privacy, machine unlearning, and data poisoning only offerfragmented solutions to these complex issues. Our paper delves into themultifaceted challenges of privacy and copyright protection within the datalifecycle. We advocate for integrated approaches that combines technicalinnovation with ethical foresight, holistically addressing these concerns byinvestigating and devising solutions that are informed by the lifecycleperspective. This work aims to catalyze a broader discussion and inspireconcerted efforts towards data privacy and copyright integrity in GenerativeAI.</div></details><blockquote><p><strong><em>2024-01-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.03350v2><strong>To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods</strong></a></p><p><em>Dawen Zhang, Shidong Pan, Thong Hoang, Zhenchang Xing, Mark Staples, Xiwei Xu, Lina Yao, Qinghua Lu, Liming Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten (RTBF) is motivated by the desire of people not tobe perpetually disadvantaged by their past deeds. For this, data deletion needsto be deep and permanent, and should be removed from machine learning models.Researchers have proposed machine unlearning algorithms which aim to erasespecific data from trained models more efficiently. However, these methodsmodify how data is fed into the model and how training is done, which maysubsequently compromise AI ethics from the fairness perspective. To helpsoftware engineers make responsible decisions when adopting these unlearningmethods, we present the first study on machine unlearning methods to revealtheir fairness implications. We designed and conducted experiments on twotypical machine unlearning methods (SISA and AmnesiacML) along with aretraining method (ORTR) as baseline using three fairness datasets under threedifferent deletion strategies. Experimental results show that under non-uniformdata deletion, SISA leads to better fairness compared with ORTR and AmnesiacML,while initial training and uniform data deletion do not necessarily affect thefairness of all three methods. These findings have exposed an importantresearch problem in software engineering, and can help practitioners betterunderstand the potential trade-offs on fairness when considering solutions forRTBF.</div></details><blockquote><p><strong><em>2024-01-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.02457v1><strong>eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning</strong></a></p><p><em>Zhiwei Zuo, Zhuo Tang, Bin Wang, Kenli Li, Anwitaman Datta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: New categories may be introduced over time, or existing categories may needto be reclassified. Class incremental learning (CIL) is employed for thegradual acquisition of knowledge about new categories while preservinginformation about previously learned ones in such dynamic environments. Itmight also be necessary to also eliminate the influence of related categorieson the model to adapt to reclassification. We thus introduce class-levelmachine unlearning (MU) within CIL. Typically, MU methods tend to betime-consuming and can potentially harm the model&rsquo;s performance. A continuousstream of unlearning requests could lead to catastrophic forgetting. To addressthese issues, we propose a non-destructive eCIL-MU framework based on embeddingtechniques to map data into vectors and then be stored in vector databases. Ourapproach exploits the overlap between CIL and MU tasks for acceleration.Experiments demonstrate the capability of achieving unlearning effectivenessand orders of magnitude (upto $\sim 278\times$) of acceleration.</div></details><blockquote><p><strong><em>2023-12-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.16823v1><strong>Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation</strong></a></p><p><em>Hyunjune Kim, Sangyong Lee, Simon S. Woo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, serious concerns have been raised about the privacy issues relatedto training datasets in machine learning algorithms when including personaldata. Various regulations in different countries, including the GDPR grantindividuals to have personal data erased, known as &rsquo;the right to be forgotten&rsquo;or &rsquo;the right to erasure&rsquo;. However, there has been less research on effectivelyand practically deleting the requested personal data from the training setwhile not jeopardizing the overall machine learning performance. In this work,we propose a fast and novel machine unlearning paradigm at the layer levelcalled layer attack unlearning, which is highly accurate and fast compared toexisting machine unlearning algorithms. We introduce the Partial-PGD algorithmto locate the samples to forget efficiently. In addition, we only use the lastlayer of the model inspired by the Forward-Forward algorithm for unlearningprocess. Lastly, we use Knowledge Distillation (KD) to reliably learn thedecision boundaries from the teacher using soft label information to improveaccuracy performance. We conducted extensive experiments with SOTA machineunlearning models and demonstrated the effectiveness of our approach foraccuracy and end-to-end unlearning performance.</div></details><blockquote><p><strong><em>2023-12-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.02240v2><strong>Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems</strong></a></p><p><em>Dasol Choi, Dongbin Na</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning is a crucial tool for enabling a classification model toforget specific data that are used in the training time. Recently, variousstudies have presented machine unlearning algorithms and evaluated theirmethods on several datasets. However, most of the current machine unlearningalgorithms have been evaluated solely on traditional computer vision datasetssuch as CIFAR-10, MNIST, and SVHN. Furthermore, previous studies generallyevaluate the unlearning methods in the class-unlearning setup. Most previouswork first trains the classification models and then evaluates the machineunlearning performance of machine unlearning algorithms by forgetting selectedimage classes (categories) in the experiments. Unfortunately, theseclass-unlearning settings might not generalize to real-world scenarios. In thiswork, we propose a machine unlearning setting that aims to unlearn specificinstance that contains personal privacy (identity) while maintaining theoriginal task of a given model. Specifically, we propose two machine unlearningbenchmark datasets, MUFAC and MUCAC, that are greatly useful to evaluate theperformance and robustness of a machine unlearning algorithm. In our benchmarkdatasets, the original model performs facial feature recognition tasks: faceage estimation (multi-class classification) and facial attribute classification(binary class classification), where a class does not depend on any singletarget subject (personal identity), which can be a realistic setting. Moreover,we also report the performance of the state-of-the-art machine unlearningmethods on our proposed benchmark datasets. All the datasets, source codes, andtrained models are publicly available athttps://github.com/ndb796/MachineUnlearning.</div></details><blockquote><p><strong><em>2023-12-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.14895v1><strong>FAST: Feature Aware Similarity Thresholding for Weak Unlearning in Black-Box Generative Models</strong></a></p><p><em>Subhodip Panda, Prathosh AP</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The heightened emphasis on the regulation of deep generative models,propelled by escalating concerns pertaining to privacy and compliance withregulatory frameworks, underscores the imperative need for precise controlmechanisms over these models. This urgency is particularly underscored byinstances in which generative models generate outputs that encompassobjectionable, offensive, or potentially injurious content. In response,machine unlearning has emerged to selectively forget specific knowledge orremove the influence of undesirable data subsets from pre-trained models.However, modern machine unlearning approaches typically assume access to modelparameters and architectural details during unlearning, which is not alwaysfeasible. In multitude of downstream tasks, these models function as black-boxsystems, with inaccessible pre-trained parameters, architectures, and trainingdata. In such scenarios, the possibility of filtering undesired outputs becomesa practical alternative. The primary goal of this study is twofold: first, toelucidate the relationship between filtering and unlearning processes, andsecond, to formulate a methodology aimed at mitigating the display ofundesirable outputs generated from models characterized as black-box systems.Theoretical analysis in this study demonstrates that, in the context ofblack-box models, filtering can be seen as a form of weak unlearning. Ourproposed \textbf{\textit{Feature Aware Similarity Thresholding(FAST)}} methodeffectively suppresses undesired outputs by systematically encoding therepresentation of unwanted features in the latent space.</div></details><p><a href=http://arxiv.org/abs/2312.14923v1><strong>Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models</strong></a></p><p><em>Guihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid growth of machine learning has spurred legislative initiatives suchas <code>the Right to be Forgotten,'' allowing users to request data removal. Inresponse, </code>machine unlearning&rsquo;&rsquo; proposes the selective removal of unwanteddata without the need for retraining from scratch. While theNeural-Tangent-Kernel-based (NTK-based) unlearning method excels inperformance, it suffers from significant computational complexity, especiallyfor large-scale models and datasets. Our work introduces ``Fast-NTK,&rsquo;&rsquo; a novelNTK-based unlearning algorithm that significantly reduces the computationalcomplexity by incorporating parameter-efficient fine-tuning methods, such asfine-tuning batch normalization layers in a CNN or visual prompts in a visiontransformer. Our experimental results demonstrate scalability to much largerneural networks and datasets (e.g., 88M parameters; 5k images), surpassing thelimitations of previous full-model NTK-based approaches designed for smallercases (e.g., 8M parameters; 500 images). Notably, our approach maintains aperformance comparable to the traditional method of retraining on the retainset alone. Fast-NTK can thus enable for practical and scalable NTK-basedunlearning in deep neural networks.</div></details><blockquote><p><strong><em>2023-12-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.11030v1><strong>Predicting depinning dynamics of elastic interfaces by machine learning</strong></a></p><p><em>Valtteri Haavisto, Marcin Mińkowski, Lasse Laurson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Predicting the future behaviour of complex systems exhibiting critical-likedynamics is often considered to be an intrinsically hard task. Here, we studythe predictability of the depinning dynamics of elastic interfaces in randommedia driven by a slowly increasing external force, a paradigmatic complexsystem exhibiting critical avalanche dynamics linked to a continuousnon-equilibrium depinning phase transition. To this end, we train a variety ofmachine learning models to infer the mapping from features of the initialrelaxed line shape and the random pinning landscape to predict thesample-dependent staircase-like force-displacement curve that emerges from thedepinning process. Even if for a given realization of the quenched randommedium the dynamics are in principle deterministic, we find that there is anexponential decay of the predictability with the displacement of the line,quantifying how the system forgets its initial state as it nears the depinningtransition from below. Our analysis on how the related displacement scaledepends on the system size and the dimensionality of the input descriptorreveals that the onset of the depinning phase transition gives rise tofundamental limits to predictability.</div></details><blockquote><p><strong><em>2023-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.10336v1><strong>Certified Minimax Unlearning with Generalization Rates and Deletion Capacity</strong></a></p><p><em>Jiaqi Liu, Jian Lou, Zhan Qin, Kui Ren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of $(\epsilon,\delta)$-certified machine unlearning forminimax models. Most of the existing works focus on unlearning from standardstatistical learning models that have a single variable and their unlearningsteps hinge on the direct Hessian-based conventional Newton update. We developa new $(\epsilon,\delta)$-certified machine unlearning algorithm for minimaxmodels. It proposes a minimax unlearning step consisting of atotal-Hessian-based complete Newton update and the Gaussian mechanism borrowedfrom differential privacy. To obtain the unlearning certification, our methodinjects calibrated Gaussian noises by carefully analyzing the &ldquo;sensitivity&rdquo; ofthe minimax unlearning step (i.e., the closeness between the minimax unlearningvariables and the retraining-from-scratch variables). We derive thegeneralization rates in terms of population strong and weak primal-dual riskfor three different cases of loss functions, i.e.,(strongly-)convex-(strongly-)concave losses. We also provide the deletioncapacity to guarantee that a desired population risk can be maintained as longas the number of deleted samples does not exceed the derived amount. Withtraining samples $n$ and model dimension $d$, it yields the order $\mathcalO(n/d^{1/4})$, which shows a strict gap over the baseline method ofdifferentially private minimax learning that has $\mathcal O(n/d^{1/2})$. Inaddition, our rates of generalization and deletion capacity match thestate-of-the-art rates derived previously for standard statistical learningmodels.</div></details><p><a href=http://arxiv.org/abs/2308.06764v2><strong>Few-shot Class-incremental Learning: A Survey</strong></a></p><p><em>Jinghua Zhang, Li Liu, Olli Silvén, Matti Pietikäinen, Dewen Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge inMachine Learning (ML), as it necessitates the Incremental Learning (IL) of newclasses from sparsely labeled training samples without forgetting previousknowledge. While this field has seen recent progress, it remains an activeexploration area. This paper aims to provide a comprehensive and systematicreview of FSCIL. In our in-depth examination, we delve into various facets ofFSCIL, encompassing the problem definition, the discussion of the primarychallenges of unreliable empirical risk minimization and thestability-plasticity dilemma, general schemes, and relevant problems of IL andFew-shot Learning (FSL). Besides, we offer an overview of benchmark datasetsand evaluation metrics. Furthermore, we introduce the Few-shotClass-incremental Classification (FSCIC) methods from data-based,structure-based, and optimization-based approaches and the Few-shotClass-incremental Object Detection (FSCIOD) methods from anchor-free andanchor-based approaches. Beyond these, we present several promising researchdirections within FSCIL that merit further investigation.</div></details><blockquote><p><strong><em>2023-12-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.09651v1><strong>What to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection</strong></a></p><p><em>Xiaohui Zhang, Jiangyan Yi, Chenglong Wang, Chuyuan Zhang, Siding Zeng, Jianhua Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid evolution of speech synthesis and voice conversion has raisedsubstantial concerns due to the potential misuse of such technology, promptinga pressing need for effective audio deepfake detection mechanisms. Existingdetection models have shown remarkable success in discriminating known deepfakeaudio, but struggle when encountering new attack types. To address thischallenge, one of the emergent effective approaches is continual learning. Inthis paper, we propose a continual learning approach called Radian WeightModification (RWM) for audio deepfake detection. The fundamental conceptunderlying RWM involves categorizing all classes into two groups: those withcompact feature distributions across tasks, such as genuine audio, and thosewith more spread-out distributions, like various types of fake audio. Thesedistinctions are quantified by means of the in-class cosine distance, whichsubsequently serves as the basis for RWM to introduce a trainable gradientmodification direction for distinct data types. Experimental evaluationsagainst mainstream continual learning methods reveal the superiority of RWM interms of knowledge acquisition and mitigating forgetting in audio deepfakedetection. Furthermore, RWM&rsquo;s applicability extends beyond audio deepfakedetection, demonstrating its potential significance in diverse machine learningdomains such as image recognition.</div></details><blockquote><p><strong><em>2023-12-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.02826v2><strong>Unifilar Machines and the Adjoint Structure of Bayesian Filtering</strong></a></p><p><em>Nathaniel Virgo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We elucidate the mathematical structure of Bayesian filtering, and Bayesianinference more broadly, by applying recent work on category theoreticalprobability, specifically the concept of a strongly representable Markovcategory. We show that filtering, along with related concepts such as conjugatepriors, arise from an adjunction: the process of taking a hidden Markov processis right adjoint to a forgetful functor. This has an interesting consequence.In practice, filtering is usually implemented using parametrised families ofdistributions. The Kalman filter is a particularly important example, whichuses Gaussians. Rather than calculating a new posterior each time, theimplementation only needs to udpate the parameters. This structure arisesnaturally from our adjunction; the correctness of such a model is witnessed bya map from the model into the system being modelled. Conjugate priors arisefrom this construction as a special case. In showing this we define a notion of unifilar machine, which has its originsin the literature on epsilon-machines. Unifilar machines are useful as modelsof the &ldquo;observable behaviour&rdquo; of stochastic systems; we show additionally thatin the Kleisli category of the distribution monad there is a terminal unifilarmachine, and its elements are controlled stochastic processes, mappingsequences of the input alphabet probabilistically to sequences of the outputalphabet.</div></details><blockquote><p><strong><em>2023-12-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.07707v2><strong>Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</strong></a></p><p><em>Jack Foster, Stefan Schoepf, Alexandra Brintrup</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning, the ability for a machine learning model to forget, isbecoming increasingly important to comply with data privacy regulations, aswell as to remove harmful, manipulated, or outdated information. The keychallenge lies in forgetting specific information while protecting modelperformance on the remaining data. While current state-of-the-art methodsperform well, they typically require some level of retraining over the retaineddata, in order to protect or restore model performance. This adds computationaloverhead and mandates that the training data remain available and accessible,which may not be feasible. In contrast, other methods employ a retrain-freeparadigm, however, these approaches are prohibitively computationally expensiveand do not perform on par with their retrain-based counterparts. We presentSelective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-freeapproach to machine unlearning which is fast, performant, and does not requirelong-term storage of the training data. First, SSD uses the Fisher informationmatrix of the training and forgetting data to select parameters that aredisproportionately important to the forget set. Second, SSD induces forgettingby dampening these parameters proportional to their relative importance to theforget set with respect to the wider training data. We evaluate our methodagainst several existing unlearning methods in a range of experiments usingResNet18 and Vision Transformer. Results show that the performance of SSD iscompetitive with retrain-based post hoc methods, demonstrating the viability ofretrain-free post hoc unlearning approaches.</div></details><p><a href=http://arxiv.org/abs/2310.10659v2><strong>Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning System</strong></a></p><p><em>Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, the security issues of artificial intelligence have becomeincreasingly prominent due to the rapid development of deep learning researchand applications. Backdoor attack is an attack targeting the vulnerability ofdeep learning models, where hidden backdoors are activated by triggers embeddedby the attacker, thereby outputting malicious predictions that may not alignwith the intended output for a given input. In this work, we propose a novelblack-box backdoor attack based on machine unlearning. The attacker firstaugments the training set with carefully designed samples, including poison andmitigation data, to train a `benign&rsquo; model. Then, the attacker posts unlearningrequests for the mitigation samples to remove the impact of relevant data onthe model, gradually activating the hidden backdoor. Since backdoors areimplanted during the iterative unlearning process, it significantly increasesthe computational overhead of existing defense methods for backdoor detectionor mitigation. To address this new security threat, we proposes two methods fordetecting or mitigating such malicious unlearning requests. We conduct theexperiment in both exact unlearning and approximate unlearning (i.e., SISA)settings. Experimental results indicate that: 1) our attack approach cansuccessfully implant backdoor into the model, and sharding increases thedifficult of attack; 2) our detection algorithms are effective in identifyingthe mitigation samples, while sharding reduces the effectiveness of ourdetection algorithms.</div></details><p><a href=http://arxiv.org/abs/2312.07861v1><strong>GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks</strong></a></p><p><em>Bang Wu, He Zhang, Xiangwen Yang, Shuo Wang, Minhui Xue, Shirui Pan, Xingliang Yuan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of Graph Neural Networks (GNNs) in graph data analysis andtheir deployment on Machine Learning as a Service platforms have raisedcritical concerns about data misuse during model training. This situation isfurther exacerbated due to the lack of transparency in local trainingprocesses, potentially leading to the unauthorized accumulation of largevolumes of graph data, thereby infringing on the intellectual property rightsof data owners. Existing methodologies often address either data misusedetection or mitigation, and are primarily designed for local GNN models ratherthan cloud-based MLaaS platforms. These limitations call for an effective andcomprehensive solution that detects and mitigates data misuse without requiringexact training data while respecting the proprietary nature of such data. Thispaper introduces a pioneering approach called GraphGuard, to tackle thesechallenges. We propose a training-data-free method that not only detects graphdata misuse but also mitigates its impact via targeted unlearning, all withoutrelying on the original training data. Our innovative misuse detectiontechnique employs membership inference with radioactive data, enhancing thedistinguishability between member and non-member data distributions. Formitigation, we utilize synthetic graphs that emulate the characteristicspreviously learned by the target model, enabling effective unlearning even inthe absence of exact graph data. We conduct comprehensive experiments utilizingfour real-world graph datasets to demonstrate the efficacy of GraphGuard inboth detection and unlearning. We show that GraphGuard attains a near-perfectdetection rate of approximately 100% across these datasets with various GNNmodels. In addition, it performs unlearning by eliminating the impact of theunlearned graph with a marginal decrease in accuracy (less than 5%).</div></details><p><a href=http://arxiv.org/abs/2312.08481v1><strong>Physics-Guided Continual Learning for Accelerating Aqueous Organic Redox Flow Battery Material Discovery</strong></a></p><p><em>Yucheng Fu, Amanda Howard, Chao Zeng, Panos Stinis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Aqueous organic redox flow batteries (AORFBs) have gained popularity inrenewable energy storage due to high energy density, low cost, and scalability.The rapid discovery of aqueous soluble organic (ASO) redox-active materialsnecessitates efficient machine learning surrogates for predicting batteryperformance. The physics-guided continual learning (PGCL) method proposed inthis study can incrementally learn data from new ASO electrolytes whileaddressing catastrophic forgetting issues in conventional machine learning.Using a ASO anolyte database with 1024 materials generated by a large-scale$780 cm^2$ interdigitated cell model, PGCL incorporates AORFB physics tooptimize the continual learning task formation and training process. Thisachieves a 5x training time speedup compared to the non-physics-guidedcontinual learning method while retaining previously learned battery materialknowledge. The trained PGCL demonstrates its capability in predicting batteryperformance when using unseen dihydroxyphenazine isomers in anolytes, thusshowcasing the potential of PGCL to analyze and discover new ASO materials.</div></details><blockquote><p><strong><em>2023-12-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.15766v2><strong>Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges</strong></a></p><p><em>Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, large language models (LLMs) have spurred a new researchparadigm in natural language processing. Despite their excellent capability inknowledge-based question answering and reasoning, their potential to retainfaulty or even harmful knowledge poses risks of malicious application. Thechallenge of mitigating this issue and transforming these models into purerassistants is crucial for their widespread applicability. Unfortunately,Retraining LLMs repeatedly to eliminate undesirable knowledge is impracticaldue to their immense parameters. Knowledge unlearning, derived from analogousstudies on machine unlearning, presents a promising avenue to address thisconcern and is notably advantageous in the context of LLMs. It allows for theremoval of harmful knowledge in an efficient manner, without affectingunrelated knowledge in the model. To this end, we provide a survey of knowledgeunlearning in the era of LLMs. Firstly, we formally define the knowledgeunlearning problem and distinguish it from related works. Subsequently, wecategorize existing knowledge unlearning methods into three classes: thosebased on parameter optimization, parameter merging, and in-context learning,and introduce details of these unlearning methods. We further presentevaluation datasets used in existing methods, and finally conclude this surveyby presenting the ongoing challenges and future directions.</div></details><p><a href=http://arxiv.org/abs/2312.05229v1><strong>Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration</strong></a></p><p><em>Qi-Wei Wang, Da-Wei Zhou, Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Real-world scenarios are usually accompanied by continuously appearingclasses with scare labeled samples, which require the machine learning model toincrementally learn new classes and maintain the knowledge of base classes. Inthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methodseither introduce extra learnable components or rely on a frozen featureextractor to mitigate catastrophic forgetting and overfitting problems.However, we find a tendency for existing methods to misclassify the samples ofnew classes into base classes, which leads to the poor performance of newclasses. In other words, the strong discriminability of base classes distractsthe classification of new classes. To figure out this intriguing phenomenon, weobserve that although the feature extractor is only trained on base classes, itcan surprisingly represent the semantic similarity between the base and unseennew classes. Building upon these analyses, we propose a simple yet effectiveTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability ofnew classes by fusing the new prototypes (i.e., mean features of a class) withweighted base prototypes. In addition to standard benchmarks in FSCIL, TEENdemonstrates remarkable performance and consistent improvements over baselinemethods in the few-shot learning scenario. Code is available at:https://github.com/wangkiw/TEEN</div></details><blockquote><p><strong><em>2023-12-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.04095v1><strong>Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning Interference with Gradient Projection</strong></a></p><p><em>Tuan Hoang, Santu Rana, Sunil Gupta, Svetha Venkatesh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent data-privacy laws have sparked interest in machine unlearning, whichinvolves removing the effect of specific training samples from a learnt modelas if they were never present in the original training dataset. The challengeof machine unlearning is to discard information about the ``forget&rsquo;&rsquo; data inthe learnt model without altering the knowledge about the remaining dataset andto do so more efficiently than the naive retraining approach. To achieve this,we adopt a projected-gradient based learning method, named asProjected-Gradient Unlearning (PGU), in which the model takes steps in theorthogonal direction to the gradient subspaces deemed unimportant for theretaining dataset, so as to its knowledge is preserved. By utilizing StochasticGradient Descent (SGD) to update the model weights, our method can efficientlyscale to any model and dataset size. We provide empirically evidence todemonstrate that our unlearning method can produce models that behave similarto models retrained from scratch across various metrics even when the trainingdataset is no longer accessible. Our code is available athttps://github.com/hnanhtuan/projected_gradient_unlearning.</div></details><blockquote><p><strong><em>2023-12-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.00761v2><strong>Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting</strong></a></p><p><em>Sangamesh Kodge, Gobinda Saha, Kaushik Roy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has emerged as a prominent and challenging area ofinterest, driven in large part by the rising regulatory demands for industriesto delete user data upon request and the heightened awareness of privacy.Existing approaches either retrain models from scratch or use severalfinetuning steps for every deletion request, often constrained by computationalresource limitations and restricted access to the original training data. Inthis work, we introduce a novel class unlearning algorithm designed tostrategically eliminate an entire class or a group of classes from the learnedmodel. To that end, our algorithm first estimates the Retain Space and theForget Space, representing the feature or activation spaces for samples fromclasses to be retained and unlearned, respectively. To obtain these spaces, wepropose a novel singular value decomposition-based technique that requireslayer wise collection of network activations from a few forward passes throughthe network. We then compute the shared information between these spaces andremove it from the forget space to isolate class-discriminatory feature spacefor unlearning. Finally, we project the model weights in the orthogonaldirection of the class-discriminatory space to obtain the unlearned model. Wedemonstrate our algorithm&rsquo;s efficacy on ImageNet using a Vision Transformerwith only $\sim$1.5% drop in retain accuracy compared to the original modelwhile maintaining under 1% accuracy on the unlearned class samples. Further,our algorithm consistently performs well when subject to Membership InferenceAttacks showing 7.8% improvement on average across a variety of imageclassification datasets and network architectures, as compared to otherbaselines while being $\sim$6x more computationally efficient.</div></details><p><a href=http://arxiv.org/abs/2312.02052v1><strong>DUCK: Distance-based Unlearning via Centroid Kinematics</strong></a></p><p><em>Marco Cotogni, Jacopo Bonato, Luigi Sabetta, Francesco Pelosin, Alessandro Nicolosi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning is rising as a new field, driven by the pressing necessityof ensuring privacy in modern artificial intelligence models. This techniqueprimarily aims to eradicate any residual influence of a specific subset of datafrom the knowledge acquired by a neural model during its training. This workintroduces a novel unlearning algorithm, denoted as Distance-based Unlearningvia Centroid Kinematics (DUCK), which employs metric learning to guide theremoval of samples matching the nearest incorrect centroid in the embeddingspace. Evaluation of the algorithm&rsquo;s performance is conducted across variousbenchmark datasets in two distinct scenarios, class removal, and homogeneoussampling removal, obtaining state-of-the-art performance. We introduce a novelmetric, called Adaptive Unlearning Score (AUS), encompassing not only theefficacy of the unlearning process in forgetting target data but alsoquantifying the performance loss relative to the original model. Moreover, wepropose a novel membership inference attack to assess the algorithm&rsquo;s capacityto erase previously acquired knowledge, designed to be adaptable to futuremethodologies.</div></details><p><a href=http://arxiv.org/abs/2210.07876v2><strong>Control, Confidentiality, and the Right to be Forgotten</strong></a></p><p><em>Aloni Cohen, Adam Smith, Marika Swanberg, Prashant Nalini Vasudevan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent digital rights frameworks give users the right to delete their datafrom systems that store and process their personal information (e.g., the"right to be forgotten" in the GDPR). How should deletion be formalized incomplex systems that interact with many users and store derivative information?We argue that prior approaches fall short. Definitions of machine unlearningCao and Yang [2015] are too narrowly scoped and do not apply to generalinteractive settings. The natural approach of deletion-as-confidentiality Garget al. [2020] is too restrictive: by requiring secrecy of deleted data, itrules out social functionalities. We propose a new formalism:deletion-as-control. It allows users&rsquo; data to be freely used before deletion,while also imposing a meaningful requirement after deletion&ndash;thereby givingusers more control. Deletion-as-control provides new ways of achieving deletionin diverse settings. We apply it to social functionalities, and give a newunified view of various machine unlearning definitions from the literature.This is done by way of a new adaptive generalization of history independence.Deletion-as-control also provides a new approach to the goal of machineunlearning, that is, to maintaining a model while honoring users&rsquo; deletionrequests. We show that publishing a sequence of updated models that aredifferentially private under continual release satisfies deletion-as-control.The accuracy of such an algorithm does not depend on the number of deletedpoints, in contrast to the machine unlearning literature.</div></details><blockquote><p><strong><em>2023-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.00237v1><strong>Negotiated Representations to Prevent Forgetting in Machine Learning Applications</strong></a></p><p><em>Nuri Korhan, Ceren Öner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting is a significant challenge in the field of machinelearning, particularly in neural networks. When a neural network learns toperform well on a new task, it often forgets its previously acquired knowledgeor experiences. This phenomenon occurs because the network adjusts its weightsand connections to minimize the loss on the new task, which can inadvertentlyoverwrite or disrupt the representations that were crucial for the previoustasks. As a result, the the performance of the network on earlier tasksdeteriorates, limiting its ability to learn and adapt to a sequence of tasks.In this paper, we propose a novel method for preventing catastrophic forgettingin machine learning applications, specifically focusing on neural networks. Ourapproach aims to preserve the knowledge of the network across multiple taskswhile still allowing it to learn new information effectively. We demonstratethe effectiveness of our method by conducting experiments on various benchmarkdatasets, including Split MNIST, Split CIFAR10, Split Fashion MNIST, and SplitCIFAR100. These datasets are created by dividing the original datasets intoseparate, non overlapping tasks, simulating a continual learning scenario wherethe model needs to learn multiple tasks sequentially without forgetting theprevious ones. Our proposed method tackles the catastrophic forgetting problemby incorporating negotiated representations into the learning process, whichallows the model to maintain a balance between retaining past experiences andadapting to new tasks. By evaluating our method on these challenging datasets,we aim to showcase its potential for addressing catastrophic forgetting andimproving the performance of neural networks in continual learning settings.</div></details><blockquote><p><strong><em>2023-11-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.17276v1><strong>Machine Unlearning in Learned Databases: An Experimental Analysis</strong></a></p><p><em>Meghdad Kurmanji, Eleni Triantafillou, Peter Triantafillou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models based on neural networks (NNs) are enjoyingever-increasing attention in the DB community. However, an important issue hasbeen largely overlooked, namely the challenge of dealing with the highlydynamic nature of DBs, where data updates are fundamental, highly-frequentoperations. Although some recent research has addressed the issues ofmaintaining updated NN models in the presence of new data insertions, theeffects of data deletions (a.k.a., &ldquo;machine unlearning&rdquo;) remain a blind spot.With this work, for the first time to our knowledge, we pose and answer thefollowing key questions: What is the effect of unlearning algorithms onNN-based DB models? How do these effects translate to effects on downstream DBtasks, such as selectivity estimation (SE), approximate query processing (AQP),data generation (DG), and upstream tasks like data classification (DC)? Whatmetrics should we use to assess the impact and efficacy of unlearningalgorithms in learned DBs? Is the problem of machine unlearning in DBsdifferent from that of machine learning in DBs in the face of data insertions?Is the problem of machine unlearning for DBs different from unlearning in theML literature? what are the overhead and efficiency of unlearning algorithms?What is the sensitivity of unlearning on batching delete operations? If we havea suitable unlearning algorithm, can we combine it with an algorithm handlingdata insertions en route to solving the general adaptability/updatabilityrequirement in learned DBs in the face of both data inserts and deletes? Weanswer these questions using a comprehensive set of experiments, variousunlearning algorithms, a variety of downstream DB tasks, and an upstream task(DC), each with different NNs, and using a variety of metrics on a variety ofreal datasets, making this also a first key step towards a benchmark forlearned DB unlearning.</div></details><blockquote><p><strong><em>2023-11-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16362v1><strong>Reducing Gender Bias in Machine Translation through Counterfactual Data Generation</strong></a></p><p><em>Ranjita Naik, Spencer Rarrick, Vishal Chowdhary</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in neural methods have led to substantial improvement in thequality of Neural Machine Translation (NMT) systems. However, these systemsfrequently produce translations with inaccurate gender (Stanovsky et al.,2019), which can be traced to bias in training data. Saunders and Byrne (2020)tackle this problem with a handcrafted dataset containing balanced genderedprofession words. By using this data to fine-tune an existing NMT model, theyshow that gender bias can be significantly mitigated, albeit at the expense oftranslation quality due to catastrophic forgetting. They recover some of thelost quality with modified training objectives or additional models atinference. We find, however, that simply supplementing the handcrafted datasetwith a random sample from the base model training corpus is enough tosignificantly reduce the catastrophic forgetting. We also propose a noveldomain-adaptation technique that leverages in-domain data created with thecounterfactual data generation techniques proposed by Zmigrod et al. (2019) tofurther improve accuracy on the WinoMT challenge test set without significantloss in translation quality. We show its effectiveness in NMT systems fromEnglish into three morphologically rich languages French, Spanish, and Italian.The relevant dataset and code will be available at Github.</div></details><blockquote><p><strong><em>2023-11-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.15268v1><strong>Unlearning via Sparse Representations</strong></a></p><p><em>Vedant Shah, Frederik Träuble, Ashish Malik, Hugo Larochelle, Michael Mozer, Sanjeev Arora, Yoshua Bengio, Anirudh Goyal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine \emph{unlearning}, which involves erasing knowledge about a\emph{forget set} from a trained model, can prove to be costly and infeasibleby existing techniques. We propose a nearly compute-free zero-shot unlearningtechnique based on a discrete representational bottleneck. We show that theproposed technique efficiently unlearns the forget set and incurs negligibledamage to the model&rsquo;s performance on the rest of the data set. We evaluate theproposed technique on the problem of \textit{class unlearning} using threedatasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposedtechnique to SCRUB, a state-of-the-art approach which uses knowledgedistillation for unlearning. Across all three datasets, the proposed techniqueperforms as well as, if not better than SCRUB while incurring almost nocomputational cost.</div></details><blockquote><p><strong><em>2023-11-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16185v1><strong>Enhancing Sentiment Analysis Results through Outlier Detection Optimization</strong></a></p><p><em>Yuetian Chen, Mei Si</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: When dealing with text data containing subjective labels like speakeremotions, inaccuracies or discrepancies among labelers are not uncommon. Suchdiscrepancies can significantly affect the performance of machine learningalgorithms. This study investigates the potential of identifying and addressingoutliers in text data with subjective labels, aiming to enhance classificationoutcomes. We utilized the Deep SVDD algorithm, a one-class classificationmethod, to detect outliers in nine text-based emotion and sentiment analysisdatasets. By employing both a small-sized language model (DistilBERT base modelwith 66 million parameters) and non-deep learning machine learning algorithms(decision tree, KNN, Logistic Regression, and LDA) as the classifier, ourfindings suggest that the removal of outliers can lead to enhanced results inmost cases. Additionally, as outliers in such datasets are not necessarilyunlearnable, we experienced utilizing a large language model &ndash; DeBERTa v3large with 131 million parameters, which can capture very complex patterns indata. We continued to observe performance enhancements across multipledatasets.</div></details><blockquote><p><strong><em>2023-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.13174v1><strong>SecureCut: Federated Gradient Boosting Decision Trees with Efficient Machine Unlearning</strong></a></p><p><em>Jian Zhang, Bowen Li Jie Li, Chentao Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In response to legislation mandating companies to honor the \textit{right tobe forgotten} by erasing user data, it has become imperative to enable dataremoval in Vertical Federated Learning (VFL) where multiple parties provideprivate features for model training. In VFL, data removal, i.e.,\textit{machine unlearning}, often requires removing specific features acrossall samples under privacy guarentee in federated learning. To address thischallenge, we propose \methname, a novel Gradient Boosting Decision Tree (GBDT)framework that effectively enables both \textit{instance unlearning} and\textit{feature unlearning} without the need for retraining from scratch.Leveraging a robust GBDT structure, we enable effective data deletion whilereducing degradation of model performance. Extensive experimental results onpopular datasets demonstrate that our method achieves superior model utilityand forgetfulness compared to \textit{state-of-the-art} methods. To our bestknowledge, this is the first work that investigates machine unlearning in VFLscenarios.</div></details><p><a href=http://arxiv.org/abs/2311.13623v1><strong>Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges</strong></a></p><p><em>Shilin Zhang, Jiahui Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we address the challenges of online Continual Learning (CL) byintroducing a density distribution-based learning framework. CL, especially theClass Incremental Learning, enables adaptation to new test distributions whilecontinuously learning from a single-pass training data stream, which is more inline with the practical application requirements of real-world scenarios.However, existing CL methods often suffer from catastrophic forgetting andhigher computing costs due to complex algorithm designs, limiting theirpractical use. Our proposed framework overcomes these limitations by achievingsuperior average accuracy and time-space efficiency, bridging the performancegap between CL and classical machine learning. Specifically, we adopt anindependent Generative Kernel Density Estimation (GKDE) model for each CL task.During the testing stage, the GKDEs utilize a self-reported max probabilitydensity value to determine which one is responsible for predicting incomingtest instances. A GKDE-based learning objective can ensure that samples withthe same label are grouped together, while dissimilar instances are pushedfarther apart. Extensive experiments conducted on multiple CL datasets validatethe effectiveness of our proposed framework. Our method outperforms popular CLapproaches by a significant margin, while maintaining competitive time-spaceefficiency, making our framework suitable for real-world applications. Codewill be available at <a href=https://github.com/xxxx/xxxx>https://github.com/xxxx/xxxx</a>.</div></details><blockquote><p><strong><em>2023-11-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.12999v1><strong>CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation</strong></a></p><p><em>Ali Abbasi, Chayne Thrash, Elaheh Akbari, Daniel Zhang, Soheil Kolouri</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid progress of AI, combined with its unprecedented public adoption andthe propensity of large neural networks to memorize training data, has givenrise to significant data privacy concerns. To address these concerns, machineunlearning has emerged as an essential technique to selectively remove theinfluence of specific training data points on trained models. In this paper, weapproach the machine unlearning problem through the lens of continual learning.Given a trained model and a subset of training data designated to be forgotten(i.e., the &ldquo;forget set&rdquo;), we introduce a three-step process, named CovarNav, tofacilitate this forgetting. Firstly, we derive a proxy for the model&rsquo;s trainingdata using a model inversion attack. Secondly, we mislabel the forget set byselecting the most probable class that deviates from the actual ground truth.Lastly, we deploy a gradient projection method to minimize the cross-entropyloss on the modified forget set (i.e., learn incorrect labels for this set)while preventing forgetting of the inverted samples. We rigorously evaluateCovarNav on the CIFAR-10 and Vggface2 datasets, comparing our results withrecent benchmarks in the field and demonstrating the efficacy of our proposedapproach.</div></details><p><a href=http://arxiv.org/abs/2311.11908v2><strong>Continual Learning: Applications and the Road Forward</strong></a></p><p><em>Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is a sub-field of machine learning, which aims to allowmachine learning models to continuously learn on new data, by accumulatingknowledge without forgetting what was learned in the past. In this work, wetake a step back, and ask: &ldquo;Why should one care about continual learning in thefirst place?&rdquo;. We set the stage by surveying recent continual learning paperspublished at three major machine learning conferences, and show thatmemory-constrained settings dominate the field. Then, we discuss five openproblems in machine learning, and even though they seem unrelated to continuallearning at first sight, we show that continual learning will inevitably bepart of their solution. These problems are model-editing, personalization,on-device learning, faster (re-)training and reinforcement learning. Finally,by comparing the desiderata from these unsolved problems and the currentassumptions in continual learning, we highlight and discuss four futuredirections for continual learning research. We hope that this work offers aninteresting perspective on the future of continual learning, while displayingits potential value and the paths we have to pursue in order to make itsuccessful. This work is the result of the many discussions the authors had atthe Dagstuhl seminar on Deep Continual Learning, in March 2023.</div></details><blockquote><p><strong><em>2023-11-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.12047v1><strong>Multimodal Machine Unlearning</strong></a></p><p><em>Jiali Cheng, Hadi Amiri</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning is the process of removing specific training data samplesand their corresponding effects from an already trained model. It hassignificant practical benefits, such as purging private, inaccurate, oroutdated information from trained models without the need for completere-training. Unlearning within a multimodal setting presents unique challengesdue to the intrinsic dependencies between different data modalities and theexpensive cost of training on large multimodal datasets and architectures.Current approaches to machine unlearning have not fully addressed thesechallenges. To bridge this gap, we introduce MMUL, a machine unlearningapproach specifically designed for multimodal data and models. MMUL formulatesthe multimodal unlearning task by focusing on three key properties: (a):modality decoupling, which effectively decouples the association betweenindividual unimodal data points within multimodal inputs marked for deletion,rendering them as unrelated data points within the model&rsquo;s context, (b):unimodal knowledge retention, which retains the unimodal representationcapability of the model post-unlearning, and (c): multimodal knowledgeretention, which retains the multimodal representation capability of the modelpost-unlearning. MMUL is efficient to train and is not constrained by therequirement of using a strongly convex loss. Experiments on two multimodalmodels and four multimodal benchmark datasets, including vision-language andgraph-language datasets, show that MMUL outperforms existing baselines, gainingan average improvement of +17.6 points against the best-performing unimodalbaseline in distinguishing between deleted and remaining data. In addition,MMUL can largely maintain pre-existing knowledge of the original model postunlearning, with a performance gap of only 0.3 points compared to retraining anew model from scratch.</div></details><blockquote><p><strong><em>2023-11-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.10448v1><strong>DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal</strong></a></p><p><em>Jiaeli Shi, Najah Ghalyan, Kostis Gourgoulias, John Buford, Sean Moran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models trained on sensitive or private data caninadvertently memorize and leak that information. Machine unlearning seeks toretroactively remove such details from model weights to protect privacy. Wecontribute a lightweight unlearning algorithm that leverages the FisherInformation Matrix (FIM) for selective forgetting. Prior work in this arearequires full retraining or large matrix inversions, which are computationallyexpensive. Our key insight is that the diagonal elements of the FIM, whichmeasure the sensitivity of log-likelihood to changes in weights, containsufficient information for effective forgetting. Specifically, we compute theFIM diagonal over two subsets &ndash; the data to retain and forget &ndash; for alltrainable weights. This diagonal representation approximates the complete FIMwhile dramatically reducing computation. We then use it to selectively updateweights to maximize forgetting of the sensitive subset while minimizing impacton the retained subset. Experiments show that our algorithm can successfullyforget any randomly selected subsets of training data across neural networkarchitectures. By leveraging the FIM diagonal, our approach provides aninterpretable, lightweight, and efficient solution for machine unlearning withpractical privacy benefits.</div></details><blockquote><p><strong><em>2023-11-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.08719v1><strong>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</strong></a></p><p><em>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkableperformance in long-term human-machine interactions, which basically relies oniterative recalling and reasoning of history to generate high-qualityresponses. However, such repeated recall-reason steps easily produce biasedthoughts, \textit{i.e.}, inconsistent reasoning results when recalling the samehistory for different questions. On the contrary, humans can keep thoughts inthe memory and recall them without repeated reasoning. Motivated by this humancapability, we propose a novel memory mechanism called TiM (Think-in-Memory)that enables LLMs to maintain an evolved memory for storing historical thoughtsalong the conversation stream. The TiM framework consists of two crucialstages: (1) before generating a response, a LLM agent recalls relevant thoughtsfrom memory, and (2) after generating a response, the LLM agent post-thinks andincorporates both historical and new thoughts to update the memory. Thus, TiMcan eliminate the issue of repeated reasoning by saving the post-thinkingthoughts as the history. Besides, we formulate the basic principles to organizethe thoughts in memory based on the well-established operations,(\textit{i.e.}, insert, forget, and merge operations), allowing for dynamicupdates and evolution of the thoughts. Furthermore, we introduceLocality-Sensitive Hashing into TiM to achieve efficient retrieval for thelong-term conversations. We conduct qualitative and quantitative experiments onreal-world and simulated dialogues covering a wide range of topics,demonstrating that equipping existing LLMs with TiM significantly enhancestheir performance in generating responses for long-term interactions.</div></details><blockquote><p><strong><em>2023-11-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.08538v1><strong>Extending Multilingual Machine Translation through Imitation Learning</strong></a></p><p><em>Wen Lai, Viktor Hangya, Alexander Fraser</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite the growing variety of languages supported by existing multilingualneural machine translation (MNMT) models, most of the world&rsquo;s languages arestill being left behind. We aim to extend large-scale MNMT models to a newlanguage, allowing for translation between the newly added and all of thealready supported languages in a challenging scenario: using only a parallelcorpus between the new language and English. Previous approaches, such ascontinued training on parallel data including the new language, suffer fromcatastrophic forgetting (i.e., performance on other languages is reduced). Ournovel approach Imit-MNMT treats the task as an imitation learning process,which mimicks the behavior of an expert, a technique widely used in thecomputer vision area, but not well explored in NLP. More specifically, weconstruct a pseudo multi-parallel corpus of the new and the original languagesby pivoting through English, and imitate the output distribution of theoriginal MNMT model. Extensive experiments show that our approach significantlyimproves the translation performance between the new and the originallanguages, without severe catastrophic forgetting. We also demonstrate that ourapproach is capable of solving copy and off-target problems, which are twocommon issues existence in current large-scale MNMT models.</div></details><blockquote><p><strong><em>2023-11-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.07790v1><strong>Leveraging Hamilton-Jacobi PDEs with time-dependent Hamiltonians for continual scientific machine learning</strong></a></p><p><em>Paula Chen, Tingwei Meng, Zongren Zou, Jérôme Darbon, George Em Karniadakis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We address two major challenges in scientific machine learning (SciML):interpretability and computational efficiency. We increase the interpretabilityof certain learning processes by establishing a new theoretical connectionbetween optimization problems arising from SciML and a generalized Hopfformula, which represents the viscosity solution to a Hamilton-Jacobi partialdifferential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we showthat when we solve certain regularized learning problems with integral-typelosses, we actually solve an optimal control problem and its associated HJ PDEwith time-dependent Hamiltonian. This connection allows us to reinterpretincremental updates to learned models as the evolution of an associated HJ PDEand optimal control problem in time, where all of the previous information isintrinsically encoded in the solution to the HJ PDE. As a result, existing HJPDE solvers and optimal control algorithms can be reused to design newefficient training approaches for SciML that naturally coincide with thecontinual learning framework, while avoiding catastrophic forgetting. As afirst exploration of this connection, we consider the special case of linearregression and leverage our connection to develop a new Riccati-basedmethodology for solving these learning problems that is amenable to continuallearning applications. We also provide some corresponding numerical examplesthat demonstrate the potential computational and memory advantages ourRiccati-based approach can provide.</div></details><blockquote><p><strong><em>2023-11-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.12560v3><strong>Fast Model Debias with Machine Unlearning</strong></a></p><p><em>Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent discoveries have revealed that deep neural networks might behave in abiased manner in many real-world scenarios. For instance, deep networks trainedon a large-scale face recognition dataset CelebA tend to predict blonde hairfor females and black hair for males. Such biases not only jeopardize therobustness of models but also perpetuate and amplify social biases, which isespecially concerning for automated decision-making processes in healthcare,recruitment, etc., as they could exacerbate unfair economic and socialinequalities among different groups. Existing debiasing methods suffer fromhigh costs in bias labeling or model re-training, while also exhibiting adeficiency in terms of elucidating the origins of biases within the model. Tothis respect, we propose a fast model debiasing framework (FMD) which offers anefficient approach to identify, evaluate and remove biases inherent in trainedmodels. The FMD identifies biased attributes through an explicit counterfactualconcept and quantifies the influence of data samples with influence functions.Moreover, we design a machine unlearning-based strategy to efficiently andeffectively remove the bias in a trained model with a small counterfactualdataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasetsalong with experiments with large language models demonstrate that our methodachieves superior or competing accuracies compared with state-of-the-artmethods while attaining significantly fewer biases and requiring much lessdebiasing cost. Notably, our method requires only a small external dataset andupdating a minimal amount of model parameters, without the requirement ofaccess to training data that may be too large or unavailable in practice.</div></details><blockquote><p><strong><em>2023-10-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.20268v1><strong>Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning</strong></a></p><p><em>Fuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Few-shot class-incremental learning (FSCIL) aims to build machine learningmodel that can continually learn new concepts from a few data samples, withoutforgetting knowledge of old classes. The challenges of FSCIL lies in the limited data of new classes, which notonly lead to significant overfitting issues but also exacerbates the notoriouscatastrophic forgetting problems. As proved in early studies, building samplerelationships is beneficial for learning from few-shot samples. In this paper,we promote the idea to the incremental scenario, and propose a Sample-to-Class(S2C) graph learning method for FSCIL. Specifically, we propose a Sample-level Graph Network (SGN) that focuses onanalyzing sample relationships within a single session. This network helpsaggregate similar samples, ultimately leading to the extraction of more refinedclass-level features. Then, we present a Class-level Graph Network (CGN) that establishesconnections across class-level features of both new and old classes. Thisnetwork plays a crucial role in linking the knowledge between differentsessions and helps improve overall learning in the FSCIL scenario. Moreover, wedesign a multi-stage strategy for training S2C model, which mitigates thetraining challenges posed by limited data in the incremental process. The multi-stage training strategy is designed to build S2C graph from base tofew-shot stages, and improve the capacity via an extra pseudo-incrementalstage. Experiments on three popular benchmark datasets show that our methodclearly outperforms the baselines and sets new state-of-the-art results inFSCIL.</div></details><blockquote><p><strong><em>2023-10-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.09880v3><strong>Towards Unbounded Machine Unlearning</strong></a></p><p><em>Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, Eleni Triantafillou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep machine unlearning is the problem of <code>removing' from a trained neuralnetwork a subset of its training set. This problem is very timely and has manyapplications, including the key tasks of removing biases (RB), resolvingconfusion (RC) (caused by mislabelled data in trained models), as well asallowing users to exercise their </code>right to be forgotten&rsquo; to protect UserPrivacy (UP). This paper is the first, to our knowledge, to study unlearningfor different applications (RB, RC, UP), with the view that each has its owndesiderata, definitions for `forgetting&rsquo; and associated metrics for forgetquality. For UP, we propose a novel adaptation of a strong Membership InferenceAttack for unlearning. We also propose SCRUB, a novel unlearning algorithm,which is the only method that is consistently a top performer for forgetquality across the different application-dependent metrics for RB, RC, and UP.At the same time, SCRUB is also consistently a top performer on metrics thatmeasure model utility (i.e. accuracy on retained data and generalization), andis more efficient than previous work. The above are substantiated through acomprehensive empirical evaluation against previous state-of-the-art.</div></details><p><a href=http://arxiv.org/abs/2210.03647v3><strong>Learnware: Small Models Do Big</strong></a></p><p><em>Zhi-Hua Zhou, Zhi-Hao Tan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: There are complaints about current machine learning techniques such as therequirement of a huge amount of training data and proficient training skills,the difficulty of continual learning, the risk of catastrophic forgetting, theleaking of data privacy/proprietary, etc. Most research efforts have beenfocusing on one of those concerned issues separately, paying less attention tothe fact that most issues are entangled in practice. The prevailing big modelparadigm, which has achieved impressive results in natural language processingand computer vision applications, has not yet addressed those issues, whereasbecoming a serious source of carbon emissions. This article offers an overviewof the learnware paradigm, which attempts to enable users not need to buildmachine learning models from scratch, with the hope of reusing small models todo things even beyond their original purposes, where the key ingredient is thespecification which enables a trained model to be adequately identified toreuse according to the requirement of future users who know nothing about themodel in advance.</div></details><blockquote><p><strong><em>2023-10-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.18885v1><strong>A foundational neural operator that continuously learns without forgetting</strong></a></p><p><em>Tapas Tripura, Souvik Chakraborty</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning has witnessed substantial growth, leading to the developmentof advanced artificial intelligence models crafted to address a wide range ofreal-world challenges spanning various domains, such as computer vision,natural language processing, and scientific computing. Nevertheless, thecreation of custom models for each new task remains a resource-intensiveundertaking, demanding considerable computational time and memory resources. Inthis study, we introduce the concept of the Neural Combinatorial Wavelet NeuralOperator (NCWNO) as a foundational model for scientific computing. This modelis specifically designed to excel in learning from a diverse spectrum ofphysics and continuously adapt to the solution operators associated withparametric partial differential equations (PDEs). The NCWNO leverages a gatedstructure that employs local wavelet experts to acquire shared features acrossmultiple physical systems, complemented by a memory-based ensembling approachamong these local wavelet experts. This combination enables rapid adaptation tonew challenges. The proposed foundational model offers two key advantages: (i)it can simultaneously learn solution operators for multiple parametric PDEs,and (ii) it can swiftly generalize to new parametric PDEs with minimalfine-tuning. The proposed NCWNO is the first foundational operator learningalgorithm distinguished by its (i) robustness against catastrophic forgetting,(ii) the maintenance of positive transfer for new parametric PDEs, and (iii)the facilitation of knowledge transfer across dissimilar tasks. Through anextensive set of benchmark examples, we demonstrate that the NCWNO canoutperform task-specific baseline operator learning frameworks with minimalhyperparameter tuning at the prediction stage. We also show that with minimalfine-tuning, the NCWNO performs accurate combinatorial learning of newparametric PDEs.</div></details><blockquote><p><strong><em>2023-10-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.07512v2><strong>Learn to Unlearn: A Survey on Machine Unlearning</strong></a></p><p><em>Youyang Qu, Xin Yuan, Ming Ding, Wei Ni, Thierry Rakotoarivelo, David Smith</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Learning (ML) models have been shown to potentially leak sensitiveinformation, thus raising privacy concerns in ML-driven applications. Thisinspired recent research on removing the influence of specific data samplesfrom a trained ML model. Such efficient removal would enable ML to comply withthe &ldquo;right to be forgotten&rdquo; in many legislation, and could also addressperformance bottlenecks from low-quality or poisonous samples. In that context,machine unlearning methods have been proposed to erase the contributions ofdesignated data samples on models, as an alternative to the often impracticableapproach of retraining models from scratch. This article presents acomprehensive review of recent machine unlearning techniques, verificationmechanisms, and potential attacks. We further highlight emerging challenges andprospective research directions (e.g. resilience and fairness concerns). We aimfor this paper to provide valuable resources for integrating privacy, equity,andresilience into ML systems and help them &ldquo;learn to unlearn&rdquo;.</div></details><blockquote><p><strong><em>2023-10-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16419v1><strong>Open Knowledge Base Canonicalization with Multi-task Unlearning</strong></a></p><p><em>Bingchen Liu, Shihao Hou, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The construction of large open knowledge bases (OKBs) is integral to manyapplications in the field of mobile computing. Noun phrases and relationalphrases in OKBs often suffer from redundancy and ambiguity, which calls for theinvestigation on OKB canonicalization. However, in order to meet therequirements of some privacy protection regulations and to ensure thetimeliness of the data, the canonicalized OKB often needs to remove somesensitive information or outdated data. The machine unlearning in OKBcanonicalization is an excellent solution to the above problem. Currentsolutions address OKB canonicalization by devising advanced clusteringalgorithms and using knowledge graph embedding (KGE) to further facilitate thecanonicalization process. Effective schemes are urgently needed to fullysynergise machine unlearning with clustering and KGE learning. To this end, weput forward a multi-task unlearning framework, namely MulCanon, to tacklemachine unlearning problem in OKB canonicalization. Specifically, the noisecharacteristics in the diffusion model are utilized to achieve the effect ofmachine unlearning for data in OKB. MulCanon unifies the learning objectives ofdiffusion model, KGE and clustering algorithms, and adopts a two-stepmulti-task learning paradigm for training. A thorough experimental study onpopular OKB canonicalization datasets validates that MulCanon achieves advancedmachine unlearning effects.</div></details><p><a href=http://arxiv.org/abs/2310.16996v1><strong>Towards Continually Learning Application Performance Models</strong></a></p><p><em>Ray A. O. Sinurat, Anurag Daram, Haryadi S. Gunawi, Robert B. Ross, Sandeep Madireddy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning-based performance models are increasingly being used tobuild critical job scheduling and application optimization decisions.Traditionally, these models assume that data distribution does not change asmore samples are collected over time. However, owing to the complexity andheterogeneity of production HPC systems, they are susceptible to hardwaredegradation, replacement, and/or software patches, which can lead to drift inthe data distribution that can adversely affect the performance models. To thisend, we develop continually learning performance models that account for thedistribution drift, alleviate catastrophic forgetting, and improvegeneralizability. Our best model was able to retain accuracy, regardless ofhaving to learn the new distribution of data inflicted by system changes, whiledemonstrating a 2x improvement in the prediction accuracy of the whole datasequence in comparison to the naive approach.</div></details><p><a href=http://arxiv.org/abs/2306.11974v3><strong>Universal adversarial perturbations for multiple classification tasks with quantum classifiers</strong></a></p><p><em>Yun-Zhong Qiu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Quantum adversarial machine learning is an emerging field that studies thevulnerability of quantum learning systems against adversarial perturbations anddevelops possible defense strategies. Quantum universal adversarialperturbations are small perturbations, which can make different input samplesinto adversarial examples that may deceive a given quantum classifier. This isa field that was rarely looked into but worthwhile investigating becauseuniversal perturbations might simplify malicious attacks to a large extent,causing unexpected devastation to quantum machine learning models. In thispaper, we take a step forward and explore the quantum universal perturbationsin the context of heterogeneous classification tasks. In particular, we findthat quantum classifiers that achieve almost state-of-the-art accuracy on twodifferent classification tasks can be both conclusively deceived by onecarefully-crafted universal perturbation. This result is explicitlydemonstrated with well-designed quantum continual learning models with elasticweight consolidation method to avoid catastrophic forgetting, as well asreal-life heterogeneous datasets from hand-written digits and medical MRIimages. Our results provide a simple and efficient way to generate universalperturbations on heterogeneous classification tasks and thus would providevaluable guidance for future quantum learning technologies.</div></details><blockquote><p><strong><em>2023-10-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.08130v2><strong>A Survey on Few-Shot Class-Incremental Learning</strong></a></p><p><em>Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Xin Ning, Prayag Tiwari</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large deep learning models are impressive, but they struggle when real-timedata is not available. Few-shot class-incremental learning (FSCIL) poses asignificant challenge for deep neural networks to learn new tasks from just afew labeled samples without forgetting the previously learned ones. This setupeasily leads to catastrophic forgetting and overfitting problems, severelyaffecting model performance. Studying FSCIL helps overcome deep learning modellimitations on data volume and acquisition time, while improving practicalityand adaptability of machine learning models. This paper provides acomprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesizefew-shot learning and incremental learning, focusing on introducing FSCIL fromtwo perspectives, while reviewing over 30 theoretical research studies and morethan 20 applied research studies. From the theoretical perspective, we providea novel categorization approach that divides the field into five subcategories,including traditional machine learning methods, meta-learning based methods,feature and feature space-based methods, replay-based methods, and dynamicnetwork structure-based methods. We also evaluate the performance of recenttheoretical research on benchmark datasets of FSCIL. From the applicationperspective, FSCIL has achieved impressive achievements in various fields ofcomputer vision such as image classification, object detection, and imagesegmentation, as well as in natural language processing and graph. We summarizethe important applications. Finally, we point out potential future researchdirections, including applications, problem setups, and theory development.Overall, this paper offers a comprehensive analysis of the latest advances inFSCIL from a methodological, performance, and application perspective.</div></details><blockquote><p><strong><em>2023-10-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.05521v3><strong>Federated Unlearning: How to Efficiently Erase a Client in FL?</strong></a></p><p><em>Anisa Halimi, Swanand Kadhe, Ambrish Rawat, Nathalie Baracaldo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With privacy legislation empowering the users with the right to be forgotten,it has become essential to make a model amenable for forgetting some of itstraining data. However, existing unlearning methods in the machine learningcontext can not be directly applied in the context of distributed settings likefederated learning due to the differences in learning protocol and the presenceof multiple actors. In this paper, we tackle the problem of federatedunlearning for the case of erasing a client by removing the influence of theirentire local data from the trained global model. To erase a client, we proposeto first perform local unlearning at the client to be erased, and then use thelocally unlearned model as the initialization to run very few rounds offederated learning between the server and the remaining clients to obtain theunlearned global model. We empirically evaluate our unlearning method byemploying multiple performance measures on three datasets, and demonstrate thatour unlearning method achieves comparable performance as the gold standardunlearning method of federated retraining from scratch, while beingsignificantly efficient. Unlike prior works, our unlearning method neitherrequires global access to the data used for training nor the history of theparameter updates to be stored by the server or any of the clients.</div></details><blockquote><p><strong><em>2023-10-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.02084v2><strong>Efficient Model Adaptation for Continual Learning at the Edge</strong></a></p><p><em>Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Most machine learning (ML) systems assume stationary and matching datadistributions during training and deployment. This is often a false assumption.When ML models are deployed on real devices, data distributions often shiftover time due to changes in environmental factors, sensor characteristics, andtask-of-interest. While it is possible to have a human-in-the-loop to monitorfor distribution shifts and engineer new architectures in response to theseshifts, such a setup is not cost-effective. Instead, non-stationary automatedML (AutoML) models are needed. This paper presents theEncoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learningunder domain shifts. The EAR framework uses a fixed deep neural network (DNN)feature encoder and trains shallow networks on top of the encoder to handlenovel data. The EAR framework is capable of 1) detecting when new data isout-of-distribution (OOD) by combining DNNs with hyperdimensional computing(HDC), 2) identifying low-parameter neural adaptors to adapt the model to theOOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizingcatastrophic forgetting on previous tasks by progressively growing the neuralarchitecture as needed and dynamically routing data through the appropriateadaptors and reconfigurators for handling domain-incremental andclass-incremental continual learning. We systematically evaluate our approachon several benchmark datasets for domain adaptation and demonstrate strongperformance compared to state-of-the-art algorithms for OOD detection andfew-/zero-shot NAS.</div></details><blockquote><p><strong><em>2023-10-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.07579v2><strong>In-Context Unlearning: Language Models as Few Shot Unlearners</strong></a></p><p><em>Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning, the study of efficiently removing the impact of specifictraining points on the trained model, has garnered increased attention of late,driven by the need to comply with privacy regulations like the Right to beForgotten. Although unlearning is particularly relevant for LLMs in light ofthe copyright issues they raise, achieving precise unlearning iscomputationally infeasible for very large models. To this end, recent work hasproposed several algorithms which approximate the removal of training datawithout retraining the model. These algorithms crucially rely on access to themodel parameters in order to update them, an assumption that may not hold inpractice due to computational constraints or when the LLM is accessed via API.In this work, we propose a new class of unlearning methods for LLMs we call&rsquo;&lsquo;In-Context Unlearning&rsquo;&rsquo;, providing inputs in context and without having toupdate model parameters. To unlearn a particular training instance, we providethe instance alongside a flipped label and additional correctly labelledinstances which are prepended as inputs to the LLM at inference time. Ourexperimental results demonstrate that these contexts effectively removespecific information from the training set while maintaining performance levelsthat are competitive with (or in some cases exceed) state-of-the-art unlearningmethods that require access to the LLM parameters.</div></details><blockquote><p><strong><em>2023-10-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.05331v1><strong>Unlearning with Fisher Masking</strong></a></p><p><em>Yufang Liu, Changzhi Sun, Yuanbin Wu, Aimin Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning aims to revoke some training data after learning inresponse to requests from users, model developers, and administrators. Mostprevious methods are based on direct fine-tuning, which may neither remove datacompletely nor retain full performances on the remain data. In this work, wefind that, by first masking some important parameters before fine-tuning, theperformances of unlearning could be significantly improved. We propose a newmasking strategy tailored to unlearning based on Fisher information.Experiments on various datasets and network structures show the effectivenessof the method: without any fine-tuning, the proposed Fisher masking couldunlearn almost completely while maintaining most of the performance on theremain data. It also exhibits stronger stability compared to other unlearningbaselines</div></details><p><a href=http://arxiv.org/abs/2205.08821v4><strong>Lessons Learned: Defending Against Property Inference Attacks</strong></a></p><p><em>Joshua Stock, Jens Wettlaufer, Daniel Demmler, Hannes Federrath</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This work investigates and evaluates multiple defense strategies againstproperty inference attacks (PIAs), a privacy attack against machine learningmodels. Given a trained machine learning model, PIAs aim to extract statisticalproperties of its underlying training data, e.g., reveal the ratio of men andwomen in a medical training data set. While for other privacy attacks likemembership inference, a lot of research on defense mechanisms has beenpublished, this is the first work focusing on defending against PIAs. With theprimary goal of developing a generic mitigation strategy against white-boxPIAs, we propose the novel approach property unlearning. Extensive experimentswith property unlearning show that while it is very effective when defendingtarget models against specific adversaries, property unlearning is not able togeneralize, i.e., protect against a whole class of PIAs. To investigate thereasons behind this limitation, we present the results of experiments with theexplainable AI tool LIME. They show how state-of-the-art property inferenceadversaries with the same objective focus on different parts of the targetmodel. We further elaborate on this with a follow-up experiment, in which weuse the visualization technique t-SNE to exhibit how severely statisticaltraining data properties are manifested in machine learning models. Based onthis, we develop the conjecture that post-training techniques like propertyunlearning might not suffice to provide the desirable generic protectionagainst PIAs. As an alternative, we investigate the effects of simpler trainingdata preprocessing methods like adding Gaussian noise to images of a trainingdata set on the success rate of PIAs. We conclude with a discussion of thedifferent defense approaches, summarize the lessons learned and providedirections for future work.</div></details><p><a href=http://arxiv.org/abs/2204.04297v2><strong>Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning</strong></a></p><p><em>Jinyung Hong, Theodore P. Pavlic</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Existing Continual Learning (CL) approaches have focused on addressingcatastrophic forgetting by leveraging regularization methods, replay buffers,and task-specific components. However, realistic CL solutions must be shapednot only by metrics of catastrophic forgetting but also by computationalefficiency and running time. Here, we introduce a novel neural networkarchitecture inspired by neuromodulation in biological nervous systems toeconomically and efficiently address catastrophic forgetting and provide newavenues for interpreting learned representations. Neuromodulation is abiological mechanism that has received limited attention in machine learning;it dynamically controls and fine tunes synaptic dynamics in real time to trackthe demands of different behavioral contexts. Inspired by this, our proposedarchitecture learns a relatively small set of parameters per task context that\emph{neuromodulates} the activity of unchanging, randomized weights thattransform the input. We show that this approach has strong learning performanceper task despite the very small number of learnable parameters. Furthermore,because context vectors are so compact, multiple networks can be storedconcurrently with no interference and little spatial footprint, thus completelyeliminating catastrophic forgetting and accelerating the training process.</div></details><p><a href=http://arxiv.org/abs/2310.05343v1><strong>Investigating Continuous Learning in Spiking Neural Networks</strong></a></p><p><em>C. Tanner Fredieu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, the use of third-generation machine learning, also known asspiking neural network architecture, for continuous learning was investigatedand compared to conventional models. The experimentation was divided into threeseparate phases. The first phase focused on training the conventional modelsvia transfer learning. The second phase trains a Nengo model from theirlibrary. Lastly, each conventional model is converted into a spiking neuralnetwork and trained. Initial results from phase 1 are inline with knownknowledge about continuous learning within current machine learning literature.All models were able to correctly identify the current classes, but they wouldimmediately see a sharp performance drop in previous classes due tocatastrophic forgetting. However, the SNN models were able to retain someinformation about previous classes. Although many of the previous classes werestill identified as the current trained classes, the output probabilitiesshowed a higher than normal value to the actual class. This indicates that theSNN models do have potential to overcome catastrophic forgetting but much workis still needed.</div></details><blockquote><p><strong><em>2023-10-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.02164v2><strong>A Survey of Graph Unlearning</strong></a></p><p><em>Anwar Said, Tyler Derr, Mudassir Shabbir, Waseem Abbas, Xenofon Koutsoukos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Graph unlearning emerges as a crucial advancement in the pursuit ofresponsible AI, providing the means to remove sensitive data traces fromtrained models, thereby upholding the right to be forgotten. It is evident thatgraph machine learning exhibits sensitivity to data privacy and adversarialattacks, necessitating the application of graph unlearning techniques toaddress these concerns effectively. In this comprehensive survey paper, wepresent the first systematic review of graph unlearning approaches,encompassing a diverse array of methodologies and offering a detailed taxonomyand up-to-date literature overview to facilitate the understanding ofresearchers new to this field. Additionally, we establish the vital connectionsbetween graph unlearning and differential privacy, augmenting our understandingof the relevance of privacy-preserving techniques in this context. To ensureclarity, we provide lucid explanations of the fundamental concepts andevaluation measures used in graph unlearning, catering to a broader audiencewith varying levels of expertise. Delving into potential applications, weexplore the versatility of graph unlearning across various domains, includingbut not limited to social networks, adversarial settings, andresource-constrained environments like the Internet of Things (IoT),illustrating its potential impact in safeguarding data privacy and enhancing AIsystems&rsquo; robustness. Finally, we shed light on promising research directions,encouraging further progress and innovation within the domain of graphunlearning. By laying a solid foundation and fostering continued progress, thissurvey seeks to inspire researchers to further advance the field of graphunlearning, thereby instilling confidence in the ethical growth of AI systemsand reinforcing the responsible application of machine learning techniques invarious domains.</div></details><p><a href=http://arxiv.org/abs/2309.13546v2><strong>DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning</strong></a></p><p><em>Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) is a privacy-constrained decentralized machinelearning paradigm in which clients enable collaborative training withoutcompromising private data. However, how to learn a robust global model in thedata-heterogeneous and model-heterogeneous FL scenarios is challenging. Toaddress it, we resort to data-free knowledge distillation to propose a new FLmethod (namely DFRD). DFRD equips a conditional generator on the server toapproximate the training space of the local models uploaded by clients, andsystematically investigates its training in terms of fidelity, transferability}and diversity. To overcome the catastrophic forgetting of the global modelcaused by the distribution shifts of the generator across communication rounds,we maintain an exponential moving average copy of the generator on the server.Additionally, we propose dynamic weighting and label sampling to accuratelyextract knowledge from local models. Finally, our extensive experiments onvarious image classification tasks illustrate that DFRD achieves significantperformance gains compared to SOTA baselines.</div></details><blockquote><p><strong><em>2023-10-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.01165v1><strong>Towards guarantees for parameter isolation in continual learning</strong></a></p><p><em>Giulia Lanzillotta, Sidak Pal Singh, Benjamin F. Grewe, Thomas Hofmann</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning has proved to be a successful paradigm for solving manychallenges in machine learning. However, deep neural networks fail when trainedsequentially on multiple tasks, a shortcoming known as catastrophic forgettingin the continual learning literature. Despite a recent flourish of learningalgorithms successfully addressing this problem, we find that provableguarantees against catastrophic forgetting are lacking. In this work, we studythe relationship between learning and forgetting by looking at the geometry ofneural networks&rsquo; loss landscape. We offer a unifying perspective on a family ofcontinual learning algorithms, namely methods based on parameter isolation, andwe establish guarantees on catastrophic forgetting for some of them.</div></details><blockquote><p><strong><em>2023-09-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.04344v2><strong>ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation</strong></a></p><p><em>Jiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Since real-world machine systems are running in non-stationary environments,Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trainedmodel to continually changing target domains. Recently, existing methods mainlyfocus on model-based adaptation, which aims to leverage a self-training mannerto extract the target domain knowledge. However, pseudo labels can be noisy andthe updated model parameters are unreliable under dynamic data distributions,leading to error accumulation and catastrophic forgetting in the continualadaptation process. To tackle these challenges and maintain the modelplasticity, we tactfully design a Visual Domain Adapter (ViDA) for CTTA,explicitly handling both domain-specific and domain-shared knowledge.Specifically, we first comprehensively explore the different domainrepresentations of the adapters with trainable high-rank or low-rank embeddingspaces. Then we inject ViDAs into the pre-trained model, which leverageshigh-rank and low-rank features to adapt the current domain distribution andmaintain the continual domain-shared knowledge, respectively. To exploit thelow-rank and high-rank ViDAs more effectively, we further propose a HomeostaticKnowledge Allotment (HKA) strategy, which adaptively combines differentknowledge from each ViDA. Extensive experiments conducted on four widely usedbenchmarks demonstrate that our proposed method achieves state-of-the-artperformance in both classification and segmentation CTTA tasks. Note that, ourmethod can be regarded as a novel transfer paradigm for large-scale models,delivering promising results in adaptation to continually changingdistributions.</div></details><blockquote><p><strong><em>2023-09-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.14054v1><strong>Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks</strong></a></p><p><em>Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The increased attention to regulating the outputs of deep generative models,driven by growing concerns about privacy and regulatory compliance, hashighlighted the need for effective control over these models. This necessityarises from instances where generative models produce outputs containingundesirable, offensive, or potentially harmful content. To tackle thischallenge, the concept of machine unlearning has emerged, aiming to forgetspecific learned information or to erase the influence of undesired datasubsets from a trained model. The objective of this work is to prevent thegeneration of outputs containing undesired features from a pre-trained GANwhere the underlying training data set is inaccessible. Our approach isinspired by a crucial observation: the parameter space of GANs exhibitsmeaningful directions that can be leveraged to suppress specific undesiredfeatures. However, such directions usually result in the degradation of thequality of generated samples. Our proposed method, known as&rsquo;Adapt-then-Unlearn,&rsquo; excels at unlearning such undesirable features while alsomaintaining the quality of generated samples. This method unfolds in twostages: in the initial stage, we adapt the pre-trained GAN using negativesamples provided by the user, while in the subsequent stage, we focus onunlearning the undesired feature. During the latter phase, we train thepre-trained GAN using positive samples, incorporating a repulsion regularizer.This regularizer encourages the model&rsquo;s parameters to be away from theparameters associated with the adapted model from the first stage while alsomaintaining the quality of generated samples. To the best of our knowledge, ourapproach stands as first method addressing unlearning in GANs. We validate theeffectiveness of our method through comprehensive experiments.</div></details><blockquote><p><strong><em>2023-09-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.03941v3><strong>Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions</strong></a></p><p><em>Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.</div></details><blockquote><p><strong><em>2023-09-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.06676v3><strong>A Tagging Solution to Discover IoT Devices in Apartments</strong></a></p><p><em>Berkay Kaplan, Jingyu Qian, Israel J Lopez-Toledo, Carl A. Gunter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The number of IoT devices in smart homes is increasing. This broad adoptionfacilitates users&rsquo; lives, but it also brings problems. One such issue is thatsome IoT devices may invade users&rsquo; privacy. Some reasons for this invasion canstem from obscure data collection practices or hidden devices. Specific IoTdevices can exist out of sight and still collect user data to send to thirdparties via the Internet. Owners can easily forget the location or even theexistence of these devices, especially if the owner is a landlord who managesseveral properties. The landlord-owner scenario creates multi-user problems asdesigners build machines for single users. We developed tags that use wirelessprotocols, buzzers, and LED lighting to lead users to solve the issue of devicediscovery in shared spaces and accommodate multi-user scenarios. They areattached to IoT devices inside a unit during their installation to be laterdiscovered by a tenant. These tags have similar functionalities as the popularTile models or Airtag, but our tags have different features based on ourprivacy use case. Our tags do not require pairing; multiple users can interactwith them through our Android application. Although researchers developedseveral other tools, such as thermal cameras or virtual reality (VR), fordiscovering devices in environments, they have not used wireless protocols as asolution. We measured specific performance metrics of our tags to analyze theirfeasibility for this problem. We also conducted a user study to measure theparticipants&rsquo; comfort levels while finding objects with our tags attached. Ourresults indicate that wireless tags can be viable for device tracking inresidential properties.</div></details><blockquote><p><strong><em>2023-09-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.16700v2><strong>Forgetting 1-Limited Automata</strong></a></p><p><em>Giovanni Pighizzini, Luca Prigioniero</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce and investigate forgetting 1-limited automata, which aresingle-tape Turing machines that, when visiting a cell for the first time,replace the input symbol in it by a fixed symbol, so forgetting the originalcontents. These devices have the same computational power as finite automata,namely they characterize the class of regular languages. We study the cost insize of the conversions of forgetting 1-limited automata, in bothnondeterministic and deterministic cases, into equivalent one-waynondeterministic and deterministic automata, providing optimal bounds in termsof exponential or superpolynomial functions. We also discuss the sizerelationships with two-way finite automata. In this respect, we prove theexistence of a language for which forgetting 1-limited automata areexponentially larger than equivalent minimal deterministic two-way automata.</div></details><p><a href=http://arxiv.org/abs/2309.08353v1><strong>Continual Learning with Deep Streaming Regularized Discriminant Analysis</strong></a></p><p><em>Joe Khawand, Peter Hanappe, David Colliaux</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is increasingly sought after in real world machinelearning applications, as it enables learning in a more human-like manner.Conventional machine learning approaches fail to achieve this, as incrementallyupdating the model with non-identically distributed data leads to catastrophicforgetting, where existing representations are overwritten. Althoughtraditional continual learning methods have mostly focused on batch learning,which involves learning from large collections of labeled data sequentially,this approach is not well-suited for real-world applications where we wouldlike new data to be integrated directly. This necessitates a paradigm shifttowards streaming learning. In this paper, we propose a streaming version ofregularized discriminant analysis as a solution to this challenge. We combineour algorithm with a convolutional neural network and demonstrate that itoutperforms both batch learning and existing streaming learning algorithms onthe ImageNet ILSVRC-2012 dataset.</div></details><blockquote><p><strong><em>2023-09-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.03609v5><strong>PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning</strong></a></p><p><em>Junfeng Guo, Ang Li, Cong Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While real-world applications of reinforcement learning are becoming popular,the security and robustness of RL systems are worthy of more attention andexploration. In particular, recent works have revealed that, in a multi-agentRL environment, backdoor trigger actions can be injected into a victim agent(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as itsees the backdoor trigger action. To ensure the security of RL agents againstmalicious backdoors, in this work, we propose the problem of Backdoor Detectionin a multi-agent competitive reinforcement learning system, with the objectiveof detecting Trojan agents as well as the corresponding potential triggeractions, and further trying to mitigate their Trojan behavior. In order tosolve this problem, we propose PolicyCleanse that is based on the property thatthe activated Trojan agents accumulated rewards degrade noticeably afterseveral timesteps. Along with PolicyCleanse, we also design a machineunlearning-based approach that can effectively mitigate the detected backdoor.Extensive experiments demonstrate that the proposed methods can accuratelydetect Trojan agents, and outperform existing backdoor mitigation baselineapproaches by at least 3% in winning rate across various types of agents andenvironments.</div></details><p><a href=http://arxiv.org/abs/2309.07429v1><strong>Semantic Parsing in Limited Resource Conditions</strong></a></p><p><em>Zhuang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This thesis explores challenges in semantic parsing, specifically focusing onscenarios with limited data and computational resources. It offers solutionsusing techniques like automatic data curation, knowledge transfer, activelearning, and continual learning. For tasks with no parallel training data, the thesis proposes generatingsynthetic training examples from structured database schemas. When there isabundant data in a source domain but limited parallel data in a target domain,knowledge from the source is leveraged to improve parsing in the target domain. For multilingual situations with limited data in the target languages, thethesis introduces a method to adapt parsers using a limited human translationbudget. Active learning is applied to select source-language samples for manualtranslation, maximizing parser performance in the target language. In addition,an alternative method is also proposed to utilize machine translation services,supplemented by human-translated data, to train a more effective parser. When computational resources are limited, a continual learning approach isintroduced to minimize training time and computational memory. This maintainsthe parser&rsquo;s efficiency in previously learned tasks while adapting it to newtasks, mitigating the problem of catastrophic forgetting. Overall, the thesis provides a comprehensive set of methods to improvesemantic parsing in resource-constrained conditions.</div></details><blockquote><p><strong><em>2023-09-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.01120v2><strong>Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics</strong></a></p><p><em>Hany Abdulsamad, Peter Nickl, Pascal Klink, Jan Peters</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Well-calibrated probabilistic regression models are a crucial learningcomponent in robotics applications as datasets grow rapidly and tasks becomemore complex. Unfortunately, classical regression models are usually eitherprobabilistic kernel machines with a flexible structure that does not scalegracefully with data or deterministic and vastly scalable automata, albeit witha restrictive parametric form and poor regularization. In this paper, weconsider a probabilistic hierarchical modeling paradigm that combines thebenefits of both worlds to deliver computationally efficient representationswith inherent complexity regularization. The presented approaches areprobabilistic interpretations of local regression techniques that approximatenonlinear functions through a set of local linear or polynomial units.Importantly, we rely on principles from Bayesian nonparametrics to formulateflexible models that adapt their complexity to the data and can potentiallyencompass an infinite number of components. We derive two efficient variationalinference techniques to learn these representations and highlight theadvantages of hierarchical infinite local regression models, such as dealingwith non-smooth functions, mitigating catastrophic forgetting, and enablingparameter sharing and fast predictions. Finally, we validate this approach onlarge inverse dynamics datasets and test the learned models in real-worldcontrol scenarios.</div></details><blockquote><p><strong><em>2023-09-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.00886v1><strong>Tight Bounds for Machine Unlearning via Differential Privacy</strong></a></p><p><em>Yiyang Huang, Clément L. Canonne</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We consider the formulation of &ldquo;machine unlearning&rdquo; of Sekhari, Acharya,Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called &ldquo;right to beforgotten&rdquo; by requiring that a trained model, upon request, should be able to"unlearn" a number of points from the training data, as if they had never beenincluded in the first place. Sekhari et al. established some positive andnegative results about the number of data points that can be successfullyunlearnt by a trained model without impacting the model&rsquo;s accuracy (the"deletion capacity"), showing that machine unlearning could be achieved byusing differentially private (DP) algorithms. However, their results left opena gap between upper and lower bounds on the deletion capacity of thesealgorithms: our work fully closes this gap, obtaining tight bounds on thedeletion capacity achievable by DP-based machine unlearning algorithms.</div></details><blockquote><p><strong><em>2023-08-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.00023v1><strong>Continual Learning From a Stream of APIs</strong></a></p><p><em>Enneng Yang, Zhenyi Wang, Li Shen, Nan Yin, Tongliang Liu, Guibing Guo, Xingwei Wang, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) aims to learn new tasks without forgetting previoustasks. However, existing CL methods require a large amount of raw data, whichis often unavailable due to copyright considerations and privacy risks.Instead, stakeholders usually release pre-trained machine learning models as aservice (MLaaS), which users can access via APIs. This paper considers twopractical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL(DFCL-APIs), which achieve CL from a stream of APIs with partial or no rawdata. Performing CL under these two new settings faces several challenges:unavailable full raw data, unknown model parameters, heterogeneous models ofarbitrary architecture and scale, and catastrophic forgetting of previous APIs.To overcome these issues, we propose a novel data-free cooperative continualdistillation learning framework that distills knowledge from a stream of APIsinto a CL model by generating pseudo data, just by querying APIs. Specifically,our framework includes two cooperative generators and one CL model, formingtheir training as an adversarial game. We first use the CL model and thecurrent API as fixed discriminators to train generators via a derivative-freemethod. Generators adversarially generate hard and diverse synthetic data tomaximize the response gap between the CL model and the API. Next, we train theCL model by minimizing the gap between the responses of the CL model and theblack-box API on synthetic data, to transfer the API&rsquo;s knowledge to the CLmodel. Furthermore, we propose a new regularization term based on networksimilarity to prevent catastrophic forgetting of previous APIs.Our methodperforms comparably to classic CL with full raw data on the MNIST and SVHN inthe DFCL-APIs setting. In the DECL-APIs setting, our method achieves 0.97x,0.75x and 0.69x performance of classic CL on CIFAR10, CIFAR100, andMiniImageNet.</div></details><blockquote><p><strong><em>2023-08-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.09153v2><strong>An Empirical Investigation of the Role of Pre-training in Lifelong Learning</strong></a></p><p><em>Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The lifelong learning paradigm in machine learning is an attractivealternative to the more prominent isolated learning scheme not only due to itsresemblance to biological learning but also its potential to reduce energywaste by obviating excessive model re-training. A key challenge to thisparadigm is the phenomenon of catastrophic forgetting. With the increasingpopularity and success of pre-trained models in machine learning, we pose thequestion: What role does pre-training play in lifelong learning, specificallywith respect to catastrophic forgetting? We investigate existing methods in thecontext of large, pre-trained models and evaluate their performance on avariety of text and image classification tasks, including a large-scale studyusing a novel data set of 15 diverse NLP tasks. Across all settings, we observethat generic pre-training implicitly alleviates the effects of catastrophicforgetting when learning multiple tasks sequentially compared to randomlyinitialized models. We then further investigate why pre-training alleviatesforgetting in this setting. We study this phenomenon by analyzing the losslandscape, finding that pre-trained weights appear to ease forgetting byleading to wider minima. Based on this insight, we propose jointly optimizingfor current task loss and loss basin sharpness to explicitly encourage widerbasins during sequential fine-tuning. We show that this optimization approachoutperforms several state-of-the-art task-sequential continual learningalgorithms across multiple settings, occasionally even without retaining amemory that scales in size with the number of tasks.</div></details><blockquote><p><strong><em>2023-08-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.13269v2><strong>Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation</strong></a></p><p><em>Guanhua Ye, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As some recent information security legislation endowed users withunconditional rights to be forgotten by any trained machine learning model,personalized IoT service providers have to put unlearning functionality intotheir consideration. The most straightforward method to unlearn users&rsquo;contribution is to retrain the model from the initial state, which is notrealistic in high throughput applications with frequent unlearning requests.Though some machine unlearning frameworks have been proposed to speed up theretraining process, they fail to match decentralized learning scenarios. Inthis paper, we design a decentralized unlearning framework called HDUS, whichuses distilled seed models to construct erasable ensembles for all clients.Moreover, the framework is compatible with heterogeneous on-device models,representing stronger scalability in real-world applications. Extensiveexperiments on three real-world datasets show that our HDUS achievesstate-of-the-art performance.</div></details><p><a href=http://arxiv.org/abs/2308.14322v1><strong>Machine Unlearning Methodology base on Stochastic Teacher Network</strong></a></p><p><em>Xulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, Jing Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rise of the phenomenon of the &ldquo;right to be forgotten&rdquo; has promptedresearch on machine unlearning, which grants data owners the right to activelywithdraw data that has been used for model training, and requires theelimination of the contribution of that data to the model. A simple method toachieve this is to use the remaining data to retrain the model, but this is notacceptable for other data owners who continue to participate in training.Existing machine unlearning methods have been found to be ineffective inquickly removing knowledge from deep learning models. This paper proposes usinga stochastic network as a teacher to expedite the mitigation of the influencecaused by forgotten data on the model. We performed experiments on threedatasets, and the findings demonstrate that our approach can efficientlymitigate the influence of target data on the model within a single epoch. Thisallows for one-time erasure and reconstruction of the model, and thereconstruction model achieves the same performance as the retrained model.</div></details><blockquote><p><strong><em>2023-08-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.13559v1><strong>Machine Unlearning for Causal Inference</strong></a></p><p><em>Vikas Ramachandra, Mohit Sethi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models play a vital role in making predictions and derivinginsights from data and are being increasingly used for causal inference. Topreserve user privacy, it is important to enable the model to forget some ofits learning/captured information about a given user (machine unlearning). Thispaper introduces the concept of machine unlearning for causal inference,particularly propensity score matching and treatment effect estimation, whichaims to refine and improve the performance of machine learning models forcausal analysis given the above unlearning requirements. The paper presents amethodology for machine unlearning using a neural network-based propensityscore model. The dataset used in the study is the Lalonde dataset, a widelyused dataset for evaluating the effectiveness i.e. the treatment effect of jobtraining programs. The methodology involves training an initial propensityscore model on the original dataset and then creating forget sets byselectively removing instances, as well as matched instance pairs. based onpropensity score matching. These forget sets are used to evaluate the retrainedmodel, allowing for the elimination of unwanted associations. The actualretraining of the model is performed using the retain set. The experimentalresults demonstrate the effectiveness of the machine unlearning approach. Thedistribution and histogram analysis of propensity scores before and afterunlearning provide insights into the impact of the unlearning process on thedata. This study represents the first attempt to apply machine unlearningtechniques to causal inference.</div></details><p><a href=http://arxiv.org/abs/2308.10422v2><strong>Split Unlearning</strong></a></p><p><em>Guangsheng Yu, Xu Wang, Caijun Sun, Qin Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Split learning is emerging as a powerful approach to decentralized machinelearning, but the urgent task of unlearning to address privacy issues presentssignificant challenges. Conventional methods of retraining from scratch orgradient ascending require all clients&rsquo; involvement, incurring highcomputational and communication overhead, particularly in public networks whereclients lack resources and may be reluctant to participate in unlearningprocesses they have no interest. In this short article, we propose\textsc{SplitWiper}, a new framework that integrates the concept of SISA toreduce retraining costs and ensures no interference between the unlearningclient and others in public networks. Recognizing the inherent sharding insplit learning, we first establish the SISA-based design of\textsc{SplitWiper}. This forms the premise for conceptualizing two unlearningstrategies for label-sharing and non-label-sharing scenarios. This articlerepresents an earlier edition, with extensive experiments being conducted forthe forthcoming full version.</div></details><p><a href=http://arxiv.org/abs/2308.12674v1><strong>Improving Translation Faithfulness of Large Language Models via Augmenting Instructions</strong></a></p><p><em>Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) present strong general capabilities, and acurrent compelling challenge is stimulating their specialized capabilities,such as machine translation, through low-cost instruction tuning. The standardinstruction-following data is sequentially organized as the concatenation of aninstruction, an input, and a response. As the attention mechanism of LLMs haslimitations on local focus, LLMs tend to focus more on the words or sentencesnearby at each position. This leads to a high risk of instruction forgettingduring decoding. To alleviate the above issues, We propose SWIE(Segment-Weighted Instruction Embedding) and an instruction-following datasetOVERMISS. SWIE improves the model instruction understanding by adding a globalinstruction representation on the following input and response representations.OVERMISS improves model faithfulness by comparing over-translation andmiss-translation results with the correct translation. We apply our methods totwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental resultsdemonstrate significant improvements in translation performance with SWIE basedon BLOOMZ-3b, particularly in zero-shot and long text translations due toreduced instruction forgetting risk. Additionally, OVERMISS outperforms thebaseline in translation performance (e.g. an increase in BLEU scores from 0.69to 3.12 and an average improvement of 0.48 percentage comet scores forLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE(e.g. the BLUE scores increase up to 0.56 from English to German across threedifferent backbones), and both exhibit improvements in the faithfulness metricbased on word alignment.</div></details><p><a href=http://arxiv.org/abs/2308.12679v1><strong>A Continual Learning Approach for Cross-Domain White Blood Cell Classification</strong></a></p><p><em>Ario Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Accurate classification of white blood cells in peripheral blood is essentialfor diagnosing hematological diseases. Due to constantly evolving clinicalsettings, data sources, and disease classifications, it is necessary to updatemachine learning classification models regularly for practical real-world use.Such models significantly benefit from sequentially learning from incoming datastreams without forgetting previously acquired knowledge. However, models cansuffer from catastrophic forgetting, causing a drop in performance on previoustasks when fine-tuned on new data. Here, we propose a rehearsal-based continuallearning approach for class incremental and domain incremental scenarios inwhite blood cell classification. To choose representative samples from previoustasks, we employ exemplar set selection based on the model&rsquo;s predictions. Thisinvolves selecting the most confident samples and the most challenging samplesidentified through uncertainty estimation of the model. We thoroughly evaluatedour proposed approach on three white blood cell classification datasets thatdiffer in color, resolution, and class composition, including scenarios wherenew domains or new classes are introduced to the model with every task. We alsotest a long class incremental experiment with both new domains and new classes.Our results demonstrate that our approach outperforms established baselines incontinual learning, including existing iCaRL and EWC methods for classifyingwhite blood cells in cross-domain environments.</div></details><blockquote><p><strong><em>2023-08-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.08747v2><strong>An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning</strong></a></p><p><em>Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting (CF) is a phenomenon that occurs in machine learningwhen a model forgets previously learned information as it learns newinformation. As large language models (LLMs) have shown excellent performance,it is interesting to uncover whether CF exists in the continual fine-tuning ofLLMs. In this study, we empirically evaluate the forgetting phenomenon in LLMs&rsquo;knowledge, from the perspectives of domain knowledge, reasoning, and readingcomprehension. The experiments demonstrate that catastrophic forgetting isgenerally observed in LLMs ranging from 1b to 7b. Furthermore, as the scaleincreases, the severity of forgetting also intensifies. Comparing thedecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ suffersless forgetting and maintains more knowledge. We also observe that LLMs canmitigate language bias (e.g. gender bias) during continual fine-tuning.Moreover, we find that ALPACA can maintain more knowledge and capacity comparedwith LLAMA during the continual fine-tuning, which implies that generalinstruction tuning can help mitigate the forgetting phenomenon of LLMs in thefurther fine-tuning process.</div></details><blockquote><p><strong><em>2023-08-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.09881v1><strong>Generative Adversarial Networks Unlearning</strong></a></p><p><em>Hui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As machine learning continues to develop, and data misuse scandals becomemore prevalent, individuals are becoming increasingly concerned about theirpersonal information and are advocating for the right to remove their data.Machine unlearning has emerged as a solution to erase training data fromtrained machine learning models. Despite its success in classifiers, researchon Generative Adversarial Networks (GANs) is limited due to their uniquearchitecture, including a generator and a discriminator. One challenge pertainsto generator unlearning, as the process could potentially disrupt thecontinuity and completeness of the latent space. This disruption mightconsequently diminish the model&rsquo;s effectiveness after unlearning. Anotherchallenge is how to define a criterion that the discriminator should performfor the unlearning images. In this paper, we introduce a substitution mechanismand define a fake label to effectively mitigate these challenges. Based on thesubstitution mechanism and fake label, we propose a cascaded unlearningapproach for both item and class unlearning within GAN models, in which theunlearning and learning processes run in a cascaded manner. We conducted acomprehensive evaluation of the cascaded unlearning technique using the MNISTand CIFAR-10 datasets. Experimental results demonstrate that this approachachieves significantly improved item and class unlearning efficiency, reducingthe required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets,respectively, in comparison to retraining from scratch. Notably, although themodel&rsquo;s performance experiences minor degradation after unlearning, thisreduction is negligible when dealing with a minimal number of images (e.g., 64)and has no adverse effects on downstream tasks such as classification.</div></details><p><a href=http://arxiv.org/abs/2308.03810v2><strong>AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning</strong></a></p><p><em>Xingyu Li, Bo Tang, Haifeng Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual lifelong learning is an machine learning framework inspired byhuman learning, where learners are trained to continuously acquire newknowledge in a sequential manner. However, the non-stationary nature ofstreaming training data poses a significant challenge known as catastrophicforgetting, which refers to the rapid forgetting of previously learnedknowledge when new tasks are introduced. While some approaches, such asexperience replay (ER), have been proposed to mitigate this issue, theirperformance remains limited, particularly in the class-incremental scenariowhich is considered natural and highly challenging. In this paper, we present anovel algorithm, called adaptive-experience replay (AdaER), to address thechallenge of continual lifelong learning. AdaER consists of two stages: memoryreplay and memory update. In the memory replay stage, AdaER introduces acontextually-cued memory recall (C-CMR) strategy, which selectively replaysmemories that are most conflicting with the current input data in terms of bothdata and task. Additionally, AdaER incorporates an entropy-balanced reservoirsampling (E-BRS) strategy to enhance the performance of the memory buffer bymaximizing information entropy. To evaluate the effectiveness of AdaER, weconduct experiments on established supervised continual lifelong learningbenchmarks, specifically focusing on class-incremental learning scenarios. Theresults demonstrate that AdaER outperforms existing continual lifelong learningbaselines, highlighting its efficacy in mitigating catastrophic forgetting andimproving learning performance.</div></details><blockquote><p><strong><em>2023-08-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.15470v2><strong>Learning to Learn: How to Continuously Teach Humans and Machines</strong></a></p><p><em>Parantak Singh, You Li, Ankur Sikarwar, Weixian Lei, Daniel Gao, Morgan Bruce Talbot, Ying Sun, Mike Zheng Shou, Gabriel Kreiman, Mengmi Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Curriculum design is a fundamental component of education. For example, whenwe learn mathematics at school, we build upon our knowledge of addition tolearn multiplication. These and other concepts must be mastered before ourfirst algebra lesson, which also reinforces our addition and multiplicationskills. Designing a curriculum for teaching either a human or a machine sharesthe underlying goal of maximizing knowledge transfer from earlier to latertasks, while also minimizing forgetting of learned tasks. Prior research oncurriculum design for image classification focuses on the ordering of trainingexamples during a single offline task. Here, we investigate the effect of theorder in which multiple distinct tasks are learned in a sequence. We focus onthe online class-incremental continual learning setting, where algorithms orhumans must learn image classes one at a time during a single pass through adataset. We find that curriculum consistently influences learning outcomes forhumans and for multiple continual machine learning algorithms across severalbenchmark datasets. We introduce a novel-object recognition dataset for humancurriculum learning experiments and observe that curricula that are effectivefor humans are highly correlated with those that are effective for machines. Asan initial step towards automated curriculum design for onlineclass-incremental learning, we propose a novel algorithm, dubbed CurriculumDesigner (CD), that designs and ranks curricula based on inter-class featuresimilarities. We find significant overlap between curricula that areempirically highly effective and those that are highly ranked by our CD. Ourstudy establishes a framework for further research on teaching humans andmachines to learn continuously using optimized curricula.</div></details><blockquote><p><strong><em>2023-08-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.08378v1><strong>Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation</strong></a></p><p><em>Jingrui Hou, Georgina Cosma, Axel Finke</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning refers to the capability of a machine learning model tolearn and adapt to new information, without compromising its performance onpreviously learned tasks. Although several studies have investigated continuallearning methods for information retrieval tasks, a well-defined taskformulation is still lacking, and it is unclear how typical learning strategiesperform in this context. To address this challenge, a systematic taskformulation of continual neural information retrieval is presented, along witha multiple-topic dataset that simulates continuous information retrieval. Acomprehensive continual neural information retrieval framework consisting oftypical retrieval models and continual learning strategies is then proposed.Empirical evaluations illustrate that the proposed framework can successfullyprevent catastrophic forgetting in neural information retrieval and enhanceperformance on previously learned tasks. The results indicate thatembedding-based retrieval models experience a decline in their continuallearning performance as the topic shift distance and dataset volume of newtasks increase. In contrast, pretraining-based models do not show any suchcorrelation. Adopting suitable learning strategies can mitigate the effects oftopic shift and data augmentation.</div></details><blockquote><p><strong><em>2023-08-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.04539v1><strong>Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures</strong></a></p><p><em>Sandeep Madireddy, Angel Yanguas-Gil, Prasanna Balaprakash</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ability to learn continuously from an incoming data stream withoutcatastrophic forgetting is critical to designing intelligent systems. Manyapproaches to continual learning rely on stochastic gradient descent and itsvariants that employ global error updates, and hence need to adopt strategiessuch as memory buffers or replay to circumvent its stability, greed, andshort-term memory limitations. To address this limitation, we have developed abiologically inspired lightweight neural network architecture that incorporatessynaptic plasticity mechanisms and neuromodulation and hence learns throughlocal error signals to enable online continual learning without stochasticgradient descent. Our approach leads to superior online continual learning performance onSplit-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to othermemory-constrained learning approaches and matches that of the state-of-the-artmemory-intensive replay-based approaches. We further demonstrate theeffectiveness of our approach by integrating key design concepts into otherbackpropagation-based continual learning algorithms, significantly improvingtheir accuracy. Our results provide compelling evidence for the importance ofincorporating biological principles into machine learning models and offerinsights into how we can leverage them to design more efficient and robustsystems for online continual learning.</div></details><blockquote><p><strong><em>2023-08-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.11577v4><strong>Machine Unlearning of Features and Labels</strong></a></p><p><em>Alexander Warnecke, Lukas Pirch, Christian Wressnegger, Konrad Rieck</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Removing information from a machine learning model is a non-trivial task thatrequires to partially revert the training process. This task is unavoidablewhen sensitive data, such as credit card numbers or passwords, accidentallyenter the model and need to be removed afterwards. Recently, different conceptsfor machine unlearning have been proposed to address this problem. While theseapproaches are effective in removing individual data points, they do not scaleto scenarios where larger groups of features and labels need to be reverted. Inthis paper, we propose the first method for unlearning features and labels. Ourapproach builds on the concept of influence functions and realizes unlearningthrough closed-form updates of model parameters. It enables to adapt theinfluence of training data on a learning model retrospectively, therebycorrecting data leaks and privacy issues. For learning models with stronglyconvex loss functions, our method provides certified unlearning withtheoretical guarantees. For models with non-convex losses, we empirically showthat unlearning features and labels is effective and significantly faster thanother strategies.</div></details><blockquote><p><strong><em>2023-08-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.15402v3><strong>Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship</strong></a></p><p><em>Quanziang Wang, Renzhen Wang, Yuexiang Li, Dong Wei, Kai Ma, Yefeng Zheng, Deyu Meng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is a promising machine learning paradigm to learn newtasks while retaining previously learned knowledge over streaming trainingdata. Till now, rehearsal-based methods, keeping a small part of data from oldtasks as a memory buffer, have shown good performance in mitigatingcatastrophic forgetting for previously learned knowledge. However, most ofthese methods typically treat each new task equally, which may not adequatelyconsider the relationship or similarity between old and new tasks. Furthermore,these methods commonly neglect sample importance in the continual trainingprocess and result in sub-optimal performance on certain tasks. To address thischallenging problem, we propose Relational Experience Replay (RER), a bi-levellearning framework, to adaptively tune task-wise relationships and sampleimportance within each task to achieve a better <code>stability' and </code>plasticity&rsquo;trade-off. As such, the proposed method is capable of accumulating newknowledge while consolidating previously learned old knowledge during continuallearning. Extensive experiments conducted on three publicly available datasets(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed methodcan consistently improve the performance of all baselines and surpass currentstate-of-the-art methods.</div></details><blockquote><p><strong><em>2023-07-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.11228v1><strong>From Adaptive Query Release to Machine Unlearning</strong></a></p><p><em>Enayat Ullah, Raman Arora</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We formalize the problem of machine unlearning as design of efficientunlearning algorithms corresponding to learning algorithms which perform aselection of adaptive queries from structured query classes. We give efficientunlearning algorithms for linear and prefix-sum query classes. As applications,we show that unlearning in many problems, in particular, stochastic convexoptimization (SCO), can be reduced to the above, yielding improved guaranteesfor the problem. In particular, for smooth Lipschitz losses and any $\rho>0$,our results yield an unlearning algorithm with excess population risk of$\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$ with unlearningquery (gradient) complexity $\tilde O(\rho \cdot \text{RetrainingComplexity})$, where $d$ is the model dimensionality and $n$ is the initialnumber of samples. For non-smooth Lipschitz losses, we give an unlearningalgorithm with excess population risk $\tildeO\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}\big)^{1/2}\big)$ with thesame unlearning query (gradient) complexity. Furthermore, in the special caseof Generalized Linear Models (GLMs), such as those in linear and logisticregression, we get dimension-independent rates of $\tildeO\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{2/3}}\big)$ and $\tildeO\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{1/3}}\big)$ for smooth Lipschitzand non-smooth Lipschitz losses respectively. Finally, we give generalizationsof the above from one unlearning request to \textit{dynamic} streams consistingof insertions and deletions.</div></details><p><a href=http://arxiv.org/abs/2307.10562v1><strong>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</strong></a></p><p><em>Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Backdoor attacks are serious security threats to machine learning modelswhere an adversary can inject poisoned samples into the training set, causing abackdoored model which predicts poisoned samples with particular triggers toparticular target classes, while behaving normally on benign samples. In thispaper, we explore the task of purifying a backdoored model using a small cleandataset. By establishing the connection between backdoor risk and adversarialrisk, we derive a novel upper bound for backdoor risk, which mainly capturesthe risk on the shared adversarial examples (SAEs) between the backdoored modeland the purified model. This upper bound further suggests a novel bi-leveloptimization problem for mitigating backdoor using adversarial trainingtechniques. To solve it, we propose Shared Adversarial Unlearning (SAU).Specifically, SAU first generates SAEs, and then, unlearns the generated SAEssuch that they are either correctly classified by the purified model and/ordifferently classified by the two models, such that the backdoor effect in thebackdoored model will be mitigated in the purified model. Experiments onvarious benchmark datasets and network architectures show that our proposedmethod achieves state-of-the-art performance for backdoor defense.</div></details><p><a href=http://arxiv.org/abs/2307.08122v2><strong>Tangent Transformers for Composition, Privacy and Removal</strong></a></p><p><em>Tian Yu Liu, Aditya Golatkar, Stefano Soatto</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuninglinearized transformers obtained by computing a First-order Taylor Expansionaround a pre-trained initialization. We show that the Jacobian-Vector Productresulting from linearization can be computed efficiently in a single forwardpass, reducing training and inference cost to the same order of magnitude asits original non-linear counterpart, while using the same number of parameters.Furthermore, we show that, when applied to various downstream visualclassification tasks, the resulting Tangent Transformer fine-tuned with TAFTcan perform comparably with fine-tuning the original non-linear network. SinceTangent Transformers are linear with respect to the new set of weights, and theresulting fine-tuning loss is convex, we show that TAFT enjoys severaladvantages compared to non-linear fine-tuning when it comes to modelcomposition, parallel training, machine unlearning, and differential privacy.</div></details><blockquote><p><strong><em>2023-07-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.04550v2><strong>Gradient Surgery for One-shot Unlearning on Generative Model</strong></a></p><p><em>Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent regulation on right-to-be-forgotten emerges tons of interest inunlearning pre-trained machine learning models. While approximating astraightforward yet expensive approach of retrain-from-scratch, recent machineunlearning methods unlearn a sample by updating weights to remove its influenceon the weight parameters. In this paper, we introduce a simple yet effectiveapproach to remove a data influence on the deep generative model. Inspired byworks in multi-task learning, we propose to manipulate gradients to regularizethe interplay of influence among samples by projecting gradients onto thenormal plane of the gradients to be retained. Our work is agnostic tostatistics of the removal samples, outperforming existing baselines whileproviding theoretical analysis for the first time in unlearning a generativemodel.</div></details><blockquote><p><strong><em>2023-07-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.12440v3><strong>Certifying Model Accuracy under Distribution Shifts</strong></a></p><p><em>Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Certified robustness in machine learning has primarily focused on adversarialperturbations of the input with a fixed attack budget for each point in thedata distribution. In this work, we present provable robustness guarantees onthe accuracy of a model under bounded Wasserstein shifts of the datadistribution. We show that a simple procedure that randomizes the input of themodel within a transformation space is provably robust to distributional shiftsunder the transformation. Our framework allows the datum-specific perturbationsize to vary across different points in the input distribution and is generalenough to include fixed-sized perturbations as well. Our certificates produceguaranteed lower bounds on the performance of the model for any (natural oradversarial) shift of the input distribution within a Wasserstein ball aroundthe original distribution. We apply our technique to: (i) certify robustnessagainst natural (non-adversarial) transformations of images such as colorshifts, hue shifts and changes in brightness and saturation, (ii) certifyrobustness against adversarial shifts of the input distribution, and (iii) showprovable lower bounds (hardness results) on the performance of models trainedon so-called &ldquo;unlearnable&rdquo; datasets that have been poisoned to interfere withmodel training.</div></details><blockquote><p><strong><em>2023-07-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.07083v1><strong>A Scenario-Based Functional Testing Approach to Improving DNN Performance</strong></a></p><p><em>Hong Zhu, Thi Minh Tam Tran, Aduen Benjumea, Andrew Bradley</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper proposes a scenario-based functional testing approach forenhancing the performance of machine learning (ML) applications. The proposedmethod is an iterative process that starts with testing the ML model on variousscenarios to identify areas of weakness. It follows by a further testing on thesuspected weak scenarios and statistically evaluate the model&rsquo;s performance onthe scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios isconfirmed by test results, the treatment of the model is performed byretraining the model using a transfer learning technique with the originalmodel as the base and applying a set of training data specifically targetingthe treated scenarios plus a subset of training data selected at random fromthe original train dataset to prevent the so-call catastrophic forgettingeffect. Finally, after the treatment, the model is assessed and evaluated againby testing on the treated scenarios as well as other scenarios to check if thetreatment is effective and no side effect caused. The paper reports a casestudy with a real ML deep neural network (DNN) model, which is the perceptionsystem of an autonomous racing car. It is demonstrated that the method iseffective in the sense that DNN model&rsquo;s performance can be improved. Itprovides an efficient method of enhancing ML model&rsquo;s performance with much lesshuman and compute resource than retrain from scratch.</div></details><blockquote><p><strong><em>2023-07-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.04905v1><strong>FedYolo: Augmenting Federated Learning with Pretrained Transformers</strong></a></p><p><em>Xuechen Zhang, Mingchen Li, Xiangyu Chang, Jiasi Chen, Amit K. Roy-Chowdhury, Ananda Theertha Suresh, Samet Oymak</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The growth and diversity of machine learning applications motivate arethinking of learning with mobile and edge devices. How can we address diverseclient goals and learn with scarce heterogeneous data? While federated learningaims to address these issues, it has challenges hindering a unified solution.Large transformer models have been shown to work across a variety of tasksachieving remarkable few-shot adaptation. This raises the question: Can clientsuse a single general-purpose model, rather than custom models for each task,while obeying device and network constraints? In this work, we investigatepretrained transformers (PTF) to achieve these on-device learning goals andthoroughly explore the roles of model size and modularity, where the latterrefers to adaptation through modules such as prompts or adapters. Focusing onfederated learning, we demonstrate that: (1) Larger scale shrinks the accuracygaps between alternative approaches and improves heterogeneity robustness.Scale allows clients to run more local SGD epochs which can significantlyreduce the number of communication rounds. At the extreme, clients can achieverespectable accuracy locally highlighting the potential of fully-locallearning. (2) Modularity, by design, enables $>$100$\times$ less communicationin bits. Surprisingly, it also boosts the generalization capability of localadaptation methods and the robustness of smaller PTFs. Finally, it enablesclients to solve multiple unrelated tasks simultaneously using a single PTF,whereas full updates are prone to catastrophic forgetting. These insights onscale and modularity motivate a new federated learning approach we call &ldquo;YouOnly Load Once&rdquo; (FedYolo): The clients load a full PTF model once and allfuture updates are accomplished through communication-efficient modules withlimited catastrophic-forgetting, where each task is assigned to its own module.</div></details><blockquote><p><strong><em>2023-07-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.03363v1><strong>Federated Unlearning via Active Forgetting</strong></a></p><p><em>Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Jiaming Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The increasing concerns regarding the privacy of machine learning models havecatalyzed the exploration of machine unlearning, i.e., a process that removesthe influence of training data on machine learning models. This concern alsoarises in the realm of federated learning, prompting researchers to address thefederated unlearning problem. However, federated unlearning remainschallenging. Existing unlearning methods can be broadly categorized into twoapproaches, i.e., exact unlearning and approximate unlearning. Firstly,implementing exact unlearning, which typically relies on thepartition-aggregation framework, in a distributed manner does not improve timeefficiency theoretically. Secondly, existing federated (approximate) unlearningmethods suffer from imprecise data influence estimation, significantcomputational burden, or both. To this end, we propose a novel federatedunlearning framework based on incremental learning, which is independent ofspecific models and federated settings. Our framework differs from existingfederated unlearning methods that rely on approximate retraining or datainfluence estimation. Instead, we leverage new memories to overwrite old ones,imitating the process of \textit{active forgetting} in neurology. Specifically,the model, intended to unlearn, serves as a student model that continuouslylearns from randomly initiated teacher models. To preserve catastrophicforgetting of non-target data, we utilize elastic weight consolidation toelastically constrain weight change. Extensive experiments on three benchmarkdatasets demonstrate the efficiency and effectiveness of our proposed method.The result of backdoor attacks demonstrates that our proposed method achievessatisfying completeness.</div></details><p><a href=http://arxiv.org/abs/2302.10325v2><strong>Adaptive Sparse Gaussian Process</strong></a></p><p><em>Vanessa Gómez-Verdejo, Emilio Parrado-Hernández, Manel Martínez-Ramón</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Adaptive learning is necessary for non-stationary environments where thelearning machine needs to forget past data distribution. Efficient algorithmsrequire a compact model update to not grow in computational burden with theincoming data and with the lowest possible computational cost for onlineparameter updating. Existing solutions only partially cover these needs. Here,we propose the first adaptive sparse Gaussian Process (GP) able to address allthese issues. We first reformulate a variational sparse GP algorithm to make itadaptive through a forgetting factor. Next, to make the model inference assimple as possible, we propose updating a single inducing point of the sparseGP model together with the remaining model parameters every time a new samplearrives. As a result, the algorithm presents a fast convergence of theinference process, which allows an efficient model update (with a singleinference iteration) even in highly non-stationary environments. Experimentalresults demonstrate the capabilities of the proposed algorithm and its goodperformance in modeling the predictive posterior in mean and confidenceinterval estimation compared to state-of-the-art approaches.</div></details><blockquote><p><strong><em>2023-07-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.03216v2><strong>Unlearning Graph Classifiers with Limited Data Resources</strong></a></p><p><em>Chao Pan, Eli Chien, Olgica Milenkovic</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As the demand for user privacy grows, controlled data removal (machineunlearning) is becoming an important feature of machine learning models fordata-sensitive Web applications such as social networks and recommendersystems. Nevertheless, at this point it is still largely unknown how to performefficient machine unlearning of graph neural networks (GNNs); this isespecially the case when the number of training samples is small, in which caseunlearning can seriously compromise the performance of the model. To addressthis issue, we initiate the study of unlearning the Graph Scattering Transform(GST), a mathematical framework that is efficient, provably stable underfeature or graph topology perturbations, and offers graph classificationperformance comparable to that of GNNs. Our main contribution is the firstknown nonlinear approximate graph unlearning method based on GSTs. Our secondcontribution is a theoretical analysis of the computational complexity of theproposed unlearning mechanism, which is hard to replicate for deep neuralnetworks. Our third contribution are extensive simulation results which showthat, compared to complete retraining of GNNs after each removal request, thenew GST-based approach offers, on average, a 10.38x speed-up and leads to a2.6% increase in test accuracy during unlearning of 90 out of 100 traininggraphs from the IMDB dataset (10% training ratio). Our implementation isavailable online at <a href=https://doi.org/10.5281/zenodo.7613150>https://doi.org/10.5281/zenodo.7613150</a>.</div></details><p><a href=http://arxiv.org/abs/2210.16424v2><strong>Machine Unlearning of Federated Clusters</strong></a></p><p><em>Chao Pan, Jin Sima, Saurav Prakash, Vishal Rana, Olgica Milenkovic</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated clustering (FC) is an unsupervised learning problem that arises ina number of practical applications, including personalized recommender andhealthcare systems. With the adoption of recent laws ensuring the &ldquo;right to beforgotten&rdquo;, the problem of machine unlearning for FC methods has become ofsignificant importance. We introduce, for the first time, the problem ofmachine unlearning for FC, and propose an efficient unlearning mechanism for acustomized secure FC framework. Our FC framework utilizes specialinitialization procedures that we show are well-suited for unlearning. Toprotect client data privacy, we develop the secure compressed multisetaggregation (SCMA) framework that addresses sparse secure federated learning(FL) problems encountered during clustering as well as more general problems.To simultaneously facilitate low communication complexity and secret sharingprotocols, we integrate Reed-Solomon encoding with special evaluation pointsinto our SCMA pipeline, and prove that the client communication cost islogarithmic in the vector dimension. Additionally, to demonstrate the benefitsof our unlearning mechanism over complete retraining, we provide a theoreticalanalysis for the unlearning performance of our approach. Simulation resultsshow that the new FC framework exhibits superior clustering performancecompared to previously reported FC baselines when the cluster sizes are highlyimbalanced. Compared to completely retraining K-means++ locally and globallyfor each removal request, our unlearning procedure offers an average speed-upof roughly 84x across seven datasets. Our implementation for the proposedmethod is available at <a href=https://github.com/thupchnsky/mufc>https://github.com/thupchnsky/mufc</a>.</div></details><blockquote><p><strong><em>2023-06-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2006.10909v2><strong>Neural Topic Modeling with Continual Lifelong Learning</strong></a></p><p><em>Pankaj Gupta, Yatin Chaudhary, Thomas Runkler, Hinrich Schütze</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Lifelong learning has recently attracted attention in building machinelearning systems that continually accumulate and transfer knowledge to helpfuture learning. Unsupervised topic modeling has been popularly used todiscover topics from document collections. However, the application of topicmodeling is challenging due to data sparsity, e.g., in a small collection of(short) documents and thus, generate incoherent topics and sub-optimal documentrepresentations. To address the problem, we propose a lifelong learningframework for neural topic modeling that can continuously process streams ofdocument collections, accumulate topics and guide future topic modeling tasksby knowledge transfer from several sources to better deal with the sparse data.In the lifelong process, we particularly investigate jointly: (1) sharinggenerative homologies (latent topics) over lifetime to transfer priorknowledge, and (2) minimizing catastrophic forgetting to retain the pastlearning via novel selective data augmentation, co-training and topicregularization approaches. Given a stream of document collections, we apply theproposed Lifelong Neural Topic Modeling (LNTM) framework in modeling threesparse document collections as future tasks and demonstrate improvedperformance quantified by perplexity, topic coherence and information retrievaltask.</div></details><blockquote><p><strong><em>2023-06-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.14126v1><strong>Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training</strong></a></p><p><em>Fan Liu, Weijia Zhang, Hao Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning-based forecasting models are commonly used in IntelligentTransportation Systems (ITS) to predict traffic patterns and provide city-wideservices. However, most of the existing models are susceptible to adversarialattacks, which can lead to inaccurate predictions and negative consequencessuch as congestion and delays. Therefore, improving the adversarial robustnessof these models is crucial for ITS. In this paper, we propose a novel frameworkfor incorporating adversarial training into spatiotemporal traffic forecastingtasks. We demonstrate that traditional adversarial training methods designatedfor static domains cannot be directly applied to traffic forecasting tasks, asthey fail to effectively defend against dynamic adversarial attacks. Then, wepropose a reinforcement learning-based method to learn the optimal nodeselection strategy for adversarial examples, which simultaneously strengthensthe dynamic attack defense capability and reduces the model overfitting.Additionally, we introduce a self-knowledge distillation regularization moduleto overcome the &ldquo;forgetting issue&rdquo; caused by continuously changing adversarialnodes during training. We evaluate our approach on two real-world trafficdatasets and demonstrate its superiority over other baselines. Our methodeffectively enhances the adversarial robustness of spatiotemporal trafficforecasting models. The source code for our framework is available athttps://github.com/usail-hkust/RDAT.</div></details><blockquote><p><strong><em>2023-06-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.12015v1><strong>Federated Self-Learning with Weak Supervision for Speech Recognition</strong></a></p><p><em>Milind Rao, Gopinath Chennupati, Gautam Tiwari, Anit Kumar Sahu, Anirudh Raju, Ariya Rastrow, Jasha Droppo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automatic speech recognition (ASR) models with low-footprint are increasinglybeing deployed on edge devices for conversational agents, which enhancesprivacy. We study the problem of federated continual incremental learning forrecurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancingscheme of learning on-device, without access to ground truth human transcriptsor machine transcriptions from a stronger ASR model. In particular, we studythe performance of a self-learning based scheme, with a paired teacher modelupdated through an exponential moving average of ASR models. Further, wepropose using possibly noisy weak-supervision signals such as feedback scoresand natural language understanding semantics determined from user behavioracross multiple turns in a session of interactions with the conversationalagent. These signals are leveraged in a multi-task policy-gradient trainingapproach to improve the performance of self-learning for ASR. Finally, we showhow catastrophic forgetting can be mitigated by combining on-device learningwith a memory-replay approach using selected historical datasets. Theseinnovations allow for 10% relative improvement in WER on new use cases withminimal degradation on other test sets in the absence of strong-supervisionsignals such as ground-truth transcriptions.</div></details><blockquote><p><strong><em>2023-06-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.05670v1><strong>One-Shot Machine Unlearning with Mnemonic Code</strong></a></p><p><em>Tomoya Yamashita, Masanori Yamada, Takashi Shibata</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning has achieved significant improvements in accuracy and has beenapplied to various fields. With the spread of deep learning, a new problem hasalso emerged; deep learning models can sometimes have undesirable informationfrom an ethical standpoint. This problem must be resolved if deep learning isto make sensitive decisions such as hiring and prison sentencing. Machineunlearning (MU) is the research area that responds to such demands. MU aims atforgetting about undesirable training data from a trained deep learning model.A naive MU approach is to re-train the whole model with the training data fromwhich the undesirable data has been removed. However, re-training the wholemodel can take a huge amount of time and consumes significant computerresources. To make MU even more practical, a simple-yet-effective MU method isrequired. In this paper, we propose a one-shot MU method, which does not needadditional training. To design one-shot MU, we add noise to the modelparameters that are sensitive to undesirable information. In our proposedmethod, we use the Fisher information matrix (FIM) to estimate the sensitivemodel parameters. Training data were usually used to evaluate the FIM inexisting methods. In contrast, we avoid the need to retain the training datafor calculating the FIM by using class-specific synthetic signals calledmnemonic code. Extensive experiments using artificial and natural datasetsdemonstrate that our method outperforms the existing methods.</div></details><blockquote><p><strong><em>2023-06-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.04410v1><strong>Meta-Learning in Spiking Neural Networks with Reward-Modulated STDP</strong></a></p><p><em>Arsham Gholamzadeh Khoee, Alireza Javaheri, Saeed Reza Kheradpisheh, Mohammad Ganjtabesh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The human brain constantly learns and rapidly adapts to new situations byintegrating acquired knowledge and experiences into memory. Developing thiscapability in machine learning models is considered an important goal of AIresearch since deep neural networks perform poorly when there is limited dataor when they need to adapt quickly to new unseen tasks. Meta-learning modelsare proposed to facilitate quick learning in low-data regimes by employingabsorbed information from the past. Although some models have recently beenintroduced that reached high-performance levels, they are not biologicallyplausible. We have proposed a bio-plausible meta-learning model inspired by thehippocampus and the prefrontal cortex using spiking neural networks with areward-based learning system. Our proposed model includes a memory designed toprevent catastrophic forgetting, a phenomenon that occurs when meta-learningmodels forget what they have learned as soon as the new task begins. Also, ournew model can easily be applied to spike-based neuromorphic devices and enablesfast learning in neuromorphic hardware. The final analysis will discuss theimplications and predictions of the model for solving few-shot classificationtasks. In solving these tasks, our model has demonstrated the ability tocompete with the existing state-of-the-art meta-learning techniques.</div></details><blockquote><p><strong><em>2023-06-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.03558v1><strong>Machine Unlearning: A Survey</strong></a></p><p><em>Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, Philip S. Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning has attracted widespread attention and evolved into anenabling technology for a wide range of highly successful applications, such asintelligent computer vision, speech recognition, medical diagnosis, and more.Yet a special need has arisen where, due to privacy, usability, and/or theright to be forgotten, information about some specific samples needs to beremoved from a model, called machine unlearning. This emerging technology hasdrawn significant interest from both academics and industry due to itsinnovation and practicality. At the same time, this ambitious problem has ledto numerous research efforts aimed at confronting its challenges. To the bestof our knowledge, no study has analyzed this complex topic or compared thefeasibility of existing unlearning solutions in different kinds of scenarios.Accordingly, with this survey, we aim to capture the key concepts of unlearningtechniques. The existing solutions are classified and summarized based on theircharacteristics within an up-to-date and comprehensive review of eachcategory&rsquo;s advantages and limitations. The survey concludes by highlightingsome of the outstanding issues with unlearning techniques, along with somefeasible directions for new research opportunities.</div></details><p><a href=http://arxiv.org/abs/2306.03500v1><strong>Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory</strong></a></p><p><em>Aliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Interactive machine learning (IML) is a beneficial learning paradigm in casesof limited data availability, as human feedback is incrementally integratedinto the training process. In this paper, we present an IML pipeline for imagecaptioning which allows us to incrementally adapt a pre-trained imagecaptioning model to a new data distribution based on user input. In order toincorporate user input into the model, we explore the use of a combination ofsimple data augmentation methods to obtain larger data batches for each newlyannotated data instance and implement continual learning methods to preventcatastrophic forgetting from repeated updates. For our experiments, we split adomain-specific image captioning dataset, namely VizWiz, into non-overlappingparts to simulate an incremental input flow for continually adapting the modelto new data. We find that, while data augmentation worsens results, even whenrelatively small amounts of data are available, episodic memory is an effectivestrategy to retain knowledge from previously seen clusters.</div></details><p><a href=http://arxiv.org/abs/2306.03542v1><strong>Masked Autoencoders are Efficient Continual Federated Learners</strong></a></p><p><em>Subarnaduti Paul, Lars-Joel Frey, Roshni Kamath, Kristian Kersting, Martin Mundt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning is typically framed from a perspective of i.i.d., and moreimportantly, isolated data. In parts, federated learning lifts this assumption,as it sets out to solve the real-world challenge of collaboratively learning ashared model from data distributed across clients. However, motivated primarilyby privacy and computational constraints, the fact that data may change,distributions drift, or even tasks advance individually on clients, is seldomtaken into account. The field of continual learning addresses this separatechallenge and first steps have recently been taken to leverage synergies indistributed supervised settings, in which several clients learn to solvechanging classification tasks over time without forgetting previously seenones. Motivated by these prior works, we posit that such federated continuallearning should be grounded in unsupervised learning of representations thatare shared across clients; in the loose spirit of how humans can indirectlyleverage others&rsquo; experience without exposure to a specific task. For thispurpose, we demonstrate that masked autoencoders for distribution estimationare particularly amenable to this setup. Specifically, their masking strategycan be seamlessly integrated with task attention mechanisms to enable selectiveknowledge transfer between clients. We empirically corroborate the latterstatement through several continual federated scenarios on both image andbinary datasets.</div></details><p><a href=http://arxiv.org/abs/2306.03715v1><strong>Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability</strong></a></p><p><em>Jianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, Bo Han</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Out-of-distribution (OOD) detection is an indispensable aspect of secure AIwhen deploying machine learning models in real-world applications. Previousparadigms either explore better scoring functions or utilize the knowledge ofoutliers to equip the models with the ability of OOD detection. However, few ofthem pay attention to the intrinsic OOD detection capability of the givenmodel. In this work, we generally discover the existence of an intermediatestage of a model trained on in-distribution (ID) data having higher OODdetection performance than that of its final stage across different settings,and further identify one critical data-level attribution to be learning withthe atypical samples. Based on such insights, we propose a novel method,Unleashing Mask, which aims to restore the OOD discriminative capabilities ofthe well-trained model with ID data. Our method utilizes a mask to figure outthe memorized atypical samples, and then finetune the model or prune it withthe introduced mask to forget them. Extensive experiments and analysisdemonstrate the effectiveness of our method. The code is available at:https://github.com/tmlr-group/Unleashing-Mask.</div></details><blockquote><p><strong><em>2023-06-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.05911v2><strong>Lifelong Machine Learning Potentials</strong></a></p><p><em>Marco Eckhoff, Markus Reiher</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning potentials (MLPs) trained on accurate quantum chemical datacan retain the high accuracy, while inflicting little computational demands. Onthe downside, they need to be trained for each individual system. In recentyears, a vast number of MLPs has been trained from scratch because learningadditional data typically requires to train again on all data to not forgetpreviously acquired knowledge. Additionally, most common structural descriptorsof MLPs cannot represent efficiently a large number of different chemicalelements. In this work, we tackle these problems by introducingelement-embracing atom-centered symmetry functions (eeACSFs) which combinestructural properties and element information from the periodic table. TheseeeACSFs are a key for our development of a lifelong machine learning potential(lMLP). Uncertainty quantification can be exploited to transgress a fixed,pre-trained MLP to arrive at a continuously adapting lMLP, because a predefinedlevel of accuracy can be ensured. To extend the applicability of an lMLP to newsystems, we apply continual learning strategies to enable autonomous andon-the-fly training on a continuous stream of new data. For the training ofdeep neural networks, we propose the continual resilient (CoRe) optimizer andincremental learning strategies relying on rehearsal of data, regularization ofparameters, and the architecture of the model.</div></details><blockquote><p><strong><em>2023-06-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.02216v1><strong>Forgettable Federated Linear Learning with Certified Data Removal</strong></a></p><p><em>Ruinan Jin, Minghui Chen, Qiong Zhang, Xiaoxiao Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) is a trending distributed learning framework thatenables collaborative model training without data sharing. Machine learningmodels trained on datasets can potentially expose the private information ofthe training data, revealing details about individual data records. In thisstudy, we focus on the FL paradigm that grants clients the ``right to beforgotten&rsquo;&rsquo;. The forgettable FL framework should bleach its global modelweights as it has never seen that client and hence does not reveal anyinformation about the client. To this end, we propose the Forgettable FederatedLinear Learning (2F2L) framework featured with novel training and data removalstrategies. The training pipeline, named Federated linear training, employslinear approximation on the model parameter space to enable our 2F2L frameworkwork for deep neural networks while achieving comparable results with canonicalneural network training. We also introduce FedRemoval, an efficient andeffective removal strategy that tackles the computational challenges in FL byapproximating the Hessian matrix using public server data from the pretrainedmodel. Unlike the previous uncertified and heuristic machine unlearning methodsin FL, we provide theoretical guarantees by bounding the differences of modelweights by our FedRemoval and that from retraining from scratch. Experimentalresults on MNIST and Fashion-MNIST datasets demonstrate the effectiveness ofour method in achieving a balance between model accuracy and informationremoval, outperforming baseline strategies and approaching retraining fromscratch.</div></details><blockquote><p><strong><em>2023-05-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.08096v2><strong>Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher</strong></a></p><p><em>Vikram S Chundawat, Ayush K Tarun, Murari Mandal, Mohan Kankanhalli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has become an important area of research due to anincreasing need for machine learning (ML) applications to comply with theemerging data privacy regulations. It facilitates the provision for removal ofcertain set or class of data from an already trained ML model without requiringretraining from scratch. Recently, several efforts have been put in to makeunlearning to be effective and efficient. We propose a novel machine unlearningmethod by exploring the utility of competent and incompetent teachers in astudent-teacher framework to induce forgetfulness. The knowledge from thecompetent and incompetent teachers is selectively transferred to the student toobtain a model that doesn&rsquo;t contain any information about the forget data. Weexperimentally show that this method generalizes well, is fast and effective.Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluateany unlearning method. Unlike the existing unlearning metrics, the ZRF scoredoes not depend on the availability of the expensive retrained model. Thismakes it useful for analysis of the unlearned model after deployment as well.We present results of experiments conducted for random subset forgetting andclass forgetting on various deep networks and across different applicationdomains.~Source code is at:https://github.com/vikram2000b/bad-teaching-unlearning</div></details><p><a href=http://arxiv.org/abs/2210.08196v2><strong>Deep Regression Unlearning</strong></a></p><p><em>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: <a href=https://github.com/ayu987/deep-regression-unlearning>https://github.com/ayu987/deep-regression-unlearning</a></div></details><p><a href=http://arxiv.org/abs/2201.05629v3><strong>Zero-Shot Machine Unlearning</strong></a></p><p><em>Vikram S Chundawat, Ayush K Tarun, Murari Mandal, Mohan Kankanhalli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern privacy regulations grant citizens the right to be forgotten byproducts, services and companies. In case of machine learning (ML)applications, this necessitates deletion of data not only from storage archivesbut also from ML models. Due to an increasing need for regulatory compliancerequired for ML applications, machine unlearning is becoming an emergingresearch problem. The right to be forgotten requests come in the form ofremoval of a certain set or class of data from the already trained ML model.Practical considerations preclude retraining of the model from scratch afterdiscarding the deleted data. The few existing studies use either the wholetraining data, or a subset of training data, or some metadata stored duringtraining to update the model weights for unlearning. However, in many cases, nodata related to the training process or training samples may be accessible forthe unlearning purpose. We therefore ask the question: is it possible toachieve unlearning with zero training samples? In this paper, we introduce thenovel problem of zero-shot machine unlearning that caters for the extreme butpractical scenario where zero original data samples are available for use. Wethen propose two novel solutions for zero-shot machine unlearning based on (a)error minimizing-maximizing noise and (b) gated knowledge transfer. Thesemethods remove the information of the forget data from the model whilemaintaining the model efficacy on the retain data. The zero-shot approachoffers good protection against the model inversion attacks and membershipinference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN)to effectively measure the quality of the unlearning method. The experimentsshow promising results for unlearning in deep learning models on benchmarkvision data-sets. The source code is available here:https://github.com/ayu987/zero-shot-unlearning</div></details><p><a href=http://arxiv.org/abs/2111.08947v5><strong>Fast Yet Effective Machine Unlearning</strong></a></p><p><em>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Unlearning the data observed during the training of a machine learning (ML)model is an important task that can play a pivotal role in fortifying theprivacy and security of ML-based applications. This paper raises the followingquestions: (i) can we unlearn a single or multiple class(es) of data from a MLmodel without looking at the full training data even once? (ii) can we make theprocess of unlearning fast and scalable to large datasets, and generalize it todifferent deep networks? We introduce a novel machine unlearning framework witherror-maximizing noise generation and impair-repair based weight manipulationthat offers an efficient solution to the above questions. An error-maximizingnoise matrix is learned for the class to be unlearned using the original model.The noise matrix is used to manipulate the model weights to unlearn thetargeted class of data. We introduce impair and repair steps for a controlledmanipulation of the network weights. In the impair step, the noise matrix alongwith a very high learning rate is used to induce sharp unlearning in the model.Thereafter, the repair step is used to regain the overall performance. Withvery few update steps, we show excellent unlearning while substantiallyretaining the overall model accuracy. Unlearning multiple classes requires asimilar number of update steps as for a single class, making our approachscalable to large problems. Our method is quite efficient in comparison to theexisting methods, works for multi-class unlearning, does not put anyconstraints on the original optimization mechanism or network design, and workswell in both small and large-scale vision tasks. This work is an important steptowards fast and easy implementation of unlearning in deep networks. Sourcecode: <a href=https://github.com/vikram2000b/Fast-Machine-Unlearning>https://github.com/vikram2000b/Fast-Machine-Unlearning</a></div></details><blockquote><p><strong><em>2023-05-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.17244v1><strong>Mitigating Catastrophic Forgetting in Long Short-Term Memory Networks</strong></a></p><p><em>Ketaki Joshi, Raghavendra Pradyumna Pothukuchi, Andre Wibisono, Abhishek Bhattacharjee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning on sequential data is critical for many machine learning(ML) deployments. Unfortunately, LSTM networks, which are commonly used tolearn on sequential data, suffer from catastrophic forgetting and are limitedin their ability to learn multiple tasks continually. We discover thatcatastrophic forgetting in LSTM networks can be overcome in two novel andreadily-implementable ways &ndash; separating the LSTM memory either for each taskor for each target label. Our approach eschews the need for explicitregularization, hypernetworks, and other complex methods. We quantify thebenefits of our approach on recently-proposed LSTM networks for computer memoryaccess prefetching, an important sequential learning problem in ML-basedcomputer system optimization. Compared to state-of-the-art weightregularization methods to mitigate catastrophic forgetting, our approach issimple, effective, and enables faster learning. We also show that our proposalenables the use of small, non-regularized LSTM networks for complex naturallanguage processing in the offline learning scenario, which was previouslyconsidered difficult.</div></details><blockquote><p><strong><em>2023-05-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.16424v1><strong>SketchOGD: Memory-Efficient Continual Learning</strong></a></p><p><em>Benjamin Wright, Youngjae Min, Jeremy Bernstein, Navid Azizan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: When machine learning models are trained continually on a sequence of tasks,they are liable to forget what they learned on previous tasks &ndash; a phenomenonknown as catastrophic forgetting. Proposed solutions to catastrophic forgettingtend to involve storing information about past tasks, meaning that memory usageis a chief consideration in determining their practicality. This paper proposesa memory-efficient solution to catastrophic forgetting, improving upon anestablished algorithm known as orthogonal gradient descent (OGD). OGD utilizesprior model gradients to find weight updates that preserve performance on priordatapoints. However, since the memory cost of storing prior model gradientsgrows with the runtime of the algorithm, OGD is ill-suited to continuallearning over arbitrarily long time horizons. To address this problem, thispaper proposes SketchOGD. SketchOGD employs an online sketching algorithm tocompress model gradients as they are encountered into a matrix of a fixed,user-determined size. In contrast to existing memory-efficient variants of OGD,SketchOGD runs online without the need for advance knowledge of the totalnumber of tasks, is simple to implement, and is more amenable to analysis. Weprovide theoretical guarantees on the approximation error of the relevantsketches under a novel metric suited to the downstream task of OGD.Experimentally, we find that SketchOGD tends to outperform currentstate-of-the-art variants of OGD given a fixed memory budget.</div></details><blockquote><p><strong><em>2023-05-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.15242v1><strong>Machine Unlearning: its nature, scope, and importance for a &ldquo;delete culture&rdquo;</strong></a></p><p><em>Luciano Floridi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them &lsquo;forget&rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.</div></details><blockquote><p><strong><em>2023-05-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.12320v1><strong>Random Relabeling for Efficient Machine Unlearning</strong></a></p><p><em>Junde Li, Swaroop Ghosh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Learning algorithms and data are the driving forces for machine learning tobring about tremendous transformation of industrial intelligence. However,individuals&rsquo; right to retract their personal data and relevant data privacyregulations pose great challenges to machine learning: how to design anefficient mechanism to support certified data removals. Removal of previouslyseen data known as machine unlearning is challenging as these data points wereimplicitly memorized in training process of learning algorithms. Retrainingremaining data from scratch straightforwardly serves such deletion requests,however, this naive method is not often computationally feasible. We proposethe unlearning scheme random relabeling, which is applicable to genericsupervised learning algorithms, to efficiently deal with sequential dataremoval requests in the online setting. A less constraining removalcertification method based on probability distribution similarity with naiveunlearning is further developed for logit-based classifiers.</div></details><blockquote><p><strong><em>2023-05-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.11191v1><strong>Towards Generalizable Data Protection With Transferable Unlearnable Examples</strong></a></p><p><em>Bin Fang, Bo Li, Shuang Wu, Tianyi Zheng, Shouhong Ding, Ran Yi, Lizhuang Ma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial Intelligence (AI) is making a profound impact in almost everydomain. One of the crucial factors contributing to this success has been theaccess to an abundance of high-quality data for constructing machine learningmodels. Lately, as the role of data in artificial intelligence has beensignificantly magnified, concerns have arisen regarding the secure utilizationof data, particularly in the context of unauthorized data usage. To mitigatedata exploitation, data unlearning have been introduced to render dataunexploitable. However, current unlearnable examples lack the generalizationrequired for wide applicability. In this paper, we present a novel,generalizable data protection method by generating transferable unlearnableexamples. To the best of our knowledge, this is the first solution thatexamines data privacy from the perspective of data distribution. Throughextensive experimentation, we substantiate the enhanced generalizableprotection capabilities of our proposed method.</div></details><p><a href=http://arxiv.org/abs/2305.10691v1><strong>Re-thinking Data Availablity Attacks Against Deep Neural Networks</strong></a></p><p><em>Bin Fang, Bo Li, Shuang Wu, Ran Yi, Shouhong Ding, Lizhuang Ma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The unauthorized use of personal data for commercial purposes and theclandestine acquisition of private data for training machine learning modelscontinue to raise concerns. In response to these issues, researchers haveproposed availability attacks that aim to render data unexploitable. However,many current attack methods are rendered ineffective by adversarial training.In this paper, we re-examine the concept of unlearnable examples and discernthat the existing robust error-minimizing noise presents an inaccurateoptimization objective. Building on these observations, we introduce a noveloptimization paradigm that yields improved protection results with reducedcomputational time requirements. We have conducted extensive experiments tosubstantiate the soundness of our approach. Moreover, our method establishes arobust foundation for future research in this area.</div></details><blockquote><p><strong><em>2023-05-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.09738v1><strong>CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning</strong></a></p><p><em>Sanyam Jain</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Training machine learning models in an incremental fashion is not onlyimportant but also an efficient way to achieve artificial general intelligence.The ability that humans possess of continuous or lifelong learning helps themto not forget previously learned tasks. However, current neural network modelsare prone to catastrophic forgetting when it comes to continual learning. Manyresearchers have come up with several techniques in order to reduce the effectof forgetting from neural networks, however, all techniques are studiedclassically with a very less focus on changing the machine learning modelarchitecture. In this research paper, we show that it is not only possible tocircumvent catastrophic forgetting in continual learning with novel hybridclassical-quantum neural networks, but also explains what features are mostimportant to learn for classification. In addition, we also claim that if themodel is trained with these explanations, it tends to give better performanceand learn specific features that are far from the decision boundary. Finally,we present the experimental results to show comparisons between classical andclassical-quantum hybrid architectures on benchmark MNIST and CIFAR-10datasets. After successful runs of learning procedure, we found hybrid neuralnetwork outperforms classical one in terms of remembering the right evidencesof the class-specific features.</div></details><p><a href=http://arxiv.org/abs/2305.14365v1><strong>Continually Learned Pavlovian Signalling Without Forgetting for Human-in-the-Loop Robotic Control</strong></a></p><p><em>Adam S. R. Parker, Michael R. Dawson, Patrick M. Pilarski</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial limbs are sophisticated devices to assist people with tasks ofdaily living. Despite advanced robotic prostheses demonstrating similar motioncapabilities to biological limbs, users report them difficult and non-intuitiveto use. Providing more effective feedback from the device to the user hastherefore become a topic of increased interest. In particular, predictionlearning methods from the field of reinforcement learning &ndash; specifically, anapproach termed Pavlovian signalling &ndash; have been proposed as one approach forbetter modulating feedback in prostheses since they can adapt during continuoususe. One challenge identified in these learning methods is that they can forgetpreviously learned predictions when a user begins to successfully act upondelivered feedback. The present work directly addresses this challenge,contributing new evidence on the impact of algorithmic choices, such as on- oroff-policy methods and representation choices, on the Pavlovian signalling froma machine to a user during their control of a robotic arm. Two conditions ofalgorithmic differences were studied using different scenarios of controlling arobotic arm: an automated motion system and human participant piloting.Contrary to expectations, off-policy learning did not provide the expectedsolution to the forgetting problem. We instead identified beneficial propertiesof a look-ahead state representation that made existing approaches able tolearn (and not forget) predictions in support of Pavlovian signalling. Thiswork therefore contributes new insight into the challenges of providing learnedpredictive feedback from a prosthetic device, and demonstrates avenues for moredynamic signalling in future human-machine interactions.</div></details><blockquote><p><strong><em>2023-05-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.06535v1><strong>KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment</strong></a></p><p><em>Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, Hongzhi Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent legislation of the &ldquo;right to be forgotten&rdquo; has led to the interest inmachine unlearning, where the learned models are endowed with the function toforget information about specific training instances as if they have neverexisted in the training set. Previous work mainly focuses on computer visionscenarios and largely ignores the essentials of unlearning in NLP field, wheretext data contains more explicit and sensitive personal information thanimages. In this paper, we propose a general unlearning framework called KGA toinduce forgetfulness. Different from previous work that tries to recovergradients or forces models to perform close to one specific distribution, KGAmaintains distribution differences (i.e., knowledge gap). This relaxes thedistribution assumption. Furthermore, we first apply the unlearning method tovarious NLP tasks (i.e., classification, translation, response generation) andpropose several unlearning evaluation metrics with pertinence. Experiments onlarge-scale datasets show that KGA yields comprehensive improvements overbaselines, where extensive analyses further validate the effectiveness of KGAand provide insight into unlearning for NLP tasks.</div></details><p><a href=http://arxiv.org/abs/2211.12701v2><strong>Continual Learning of Natural Language Processing Tasks: A Survey</strong></a></p><p><em>Zixuan Ke, Bing Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) is a learning paradigm that emulates the humancapability of learning and accumulating knowledge continually withoutforgetting the previously learned knowledge and also transferring the learnedknowledge to help learn new tasks better. This survey presents a comprehensivereview and analysis of the recent progress of CL in NLP, which has significantdifferences from CL in computer vision and machine learning. It covers (1) allCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting(CF) prevention, (3) knowledge transfer (KT), which is particularly importantfor NLP tasks; and (4) some theory and the hidden challenge of inter-task classseparation (ICS). (1), (3) and (4) have not been included in the existingsurvey. Finally, a list of future directions is discussed.</div></details><blockquote><p><strong><em>2023-05-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.06448v1><strong>Continual Facial Expression Recognition: A Benchmark</strong></a></p><p><em>Nikhil Churamani, Tolga Dimlioglu, German I. Parisi, Hatice Gunes</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Understanding human affective behaviour, especially in the dynamics ofreal-world settings, requires Facial Expression Recognition (FER) models tocontinuously adapt to individual differences in user expression, contextualattributions, and the environment. Current (deep) Machine Learning (ML)-basedFER approaches pre-trained in isolation on benchmark datasets fail to capturethe nuances of real-world interactions where data is available onlyincrementally, acquired by the agent or robot during interactions. New learningcomes at the cost of previous knowledge, resulting in catastrophic forgetting.Lifelong or Continual Learning (CL), on the other hand, enables adaptability inagents by being sensitive to changing data distributions, integrating newinformation without interfering with previously learnt knowledge. Positing CLas an effective learning paradigm for FER, this work presents the ContinualFacial Expression Recognition (ConFER) benchmark that evaluates popular CLtechniques on FER tasks. It presents a comparative analysis of several CL-basedapproaches on popular FER datasets such as CK+, RAF-DB, and AffectNet andpresent strategies for a successful implementation of ConFER for AffectiveComputing (AC) research. CL techniques, under different learning settings, areshown to achieve state-of-the-art (SOTA) performance across several datasets,thus motivating a discussion on the benefits of applying CL principles towardshuman behaviour understanding, particularly from facial expressions, as wellthe challenges entailed.</div></details><blockquote><p><strong><em>2023-05-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.00099v2><strong>Measuring Forgetting of Memorized Training Examples</strong></a></p><p><em>Matthew Jagielski, Om Thakkar, Florian Tramèr, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models &ldquo;forget&rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.</div></details><blockquote><p><strong><em>2023-04-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.04662v2><strong>Cognitively Inspired Learning of Incremental Drifting Concepts</strong></a></p><p><em>Mohammad Rostami, Aram Galstyan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans continually expand their learned knowledge to new domains and learnnew concepts without any interference with past learned experiences. Incontrast, machine learning models perform poorly in a continual learningsetting, where input data distribution changes over time. Inspired by thenervous system learning mechanisms, we develop a computational model thatenables a deep neural network to learn new concepts and expand its learnedknowledge to new domains incrementally in a continual learning setting. We relyon the Parallel Distributed Processing theory to encode abstract concepts in anembedding space in terms of a multimodal distribution. This embedding space ismodeled by internal data representations in a hidden network layer. We alsoleverage the Complementary Learning Systems theory to equip the model with amemory mechanism to overcome catastrophic forgetting through implementingpseudo-rehearsal. Our model can generate pseudo-data points for experiencereplay and accumulate new experiences to past learned experiences withoutcausing cross-task interference.</div></details><p><a href=http://arxiv.org/abs/2304.10857v1><strong>SequeL: A Continual Learning Library in PyTorch and JAX</strong></a></p><p><em>Nikolaos Dimitriadis, Francois Fleuret, Pascal Frossard</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Learning is an important and challenging problem in machinelearning, where models must adapt to a continuous stream of new data withoutforgetting previously acquired knowledge. While existing frameworks are builton PyTorch, the rising popularity of JAX might lead to divergent codebases,ultimately hindering reproducibility and progress. To address this problem, weintroduce SequeL, a flexible and extensible library for Continual Learning thatsupports both PyTorch and JAX frameworks. SequeL provides a unified interfacefor a wide range of Continual Learning algorithms, includingregularization-based approaches, replay-based approaches, and hybridapproaches. The library is designed towards modularity and simplicity, makingthe API suitable for both researchers and practitioners. We releaseSequeL\footnote{\url{https://github.com/nik-dim/sequel}} as an open-sourcelibrary, enabling researchers and developers to easily experiment and extendthe library for their own purposes.</div></details><blockquote><p><strong><em>2023-04-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.10199v1><strong>Selective and Collaborative Influence Function for Efficient Recommendation Unlearning</strong></a></p><p><em>Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent regulations on the Right to be Forgotten have greatly influenced theway of running a recommender system, because users now have the right towithdraw their private data. Besides simply deleting the target data in thedatabase, unlearning the associated data lineage e.g., the learned personalfeatures and preferences in the model, is also necessary for data withdrawal.Existing unlearning methods are mainly devised for generalized machine learningmodels in classification tasks. In this paper, we first identify two maindisadvantages of directly applying existing unlearning methods in the contextof recommendation, i.e., (i) unsatisfactory efficiency for large-scalerecommendation models and (ii) destruction of collaboration across users anditems. To tackle the above issues, we propose an extra-efficient recommendationunlearning method based on Selective and Collaborative Influence Function(SCIF). Our proposed method can (i) avoid any kind of retraining which iscomputationally prohibitive for large-scale systems, (ii) further enhanceefficiency by selectively updating user embedding and (iii) preserve thecollaboration across the remaining users and items. Furthermore, in order toevaluate the unlearning completeness, we define a Membership Inference Oracle(MIO), which can justify whether the unlearned data points were in the trainingset of the model, i.e., whether a data point was completely unlearned.Extensive experiments on two benchmark datasets demonstrate that our proposedmethod can not only greatly enhance unlearning efficiency, but also achieveadequate unlearning completeness. More importantly, our proposed methodoutperforms the state-of-the-art unlearning method regarding comprehensiverecommendation metrics.</div></details><p><a href=http://arxiv.org/abs/2304.10638v1><strong>Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning</strong></a></p><p><em>Manaar Alam, Hithem Lamri, Michail Maniatakos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) enables collaborative deep learning training acrossmultiple participants without exposing sensitive personal data. However, thedistributed nature of FL and the unvetted participants&rsquo; data makes itvulnerable to backdoor attacks. In these attacks, adversaries inject maliciousfunctionality into the centralized model during training, leading tointentional misclassifications for specific adversary-chosen inputs. Whileprevious research has demonstrated successful injections of persistentbackdoors in FL, the persistence also poses a challenge, as their existence inthe centralized model can prompt the central aggregation server to takepreventive measures to penalize the adversaries. Therefore, this paper proposesa methodology that enables adversaries to effectively remove backdoors from thecentralized model upon achieving their objectives or upon suspicion of possibledetection. The proposed approach extends the concept of machine unlearning andpresents strategies to preserve the performance of the centralized model andsimultaneously prevent over-unlearning of information unrelated to backdoorpatterns, making the adversaries stealthy while removing backdoors. To the bestof our knowledge, this is the first work that explores machine unlearning in FLto remove backdoors to the benefit of adversaries. Exhaustive evaluationconsidering image classification scenarios demonstrates the efficacy of theproposed method in efficient backdoor removal from the centralized model,injected by state-of-the-art attacks across multiple configurations.</div></details><blockquote><p><strong><em>2023-04-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.03093v2><strong>Inductive Graph Unlearning</strong></a></p><p><em>Cheng-Long Wang, Mengdi Huai, Di Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As a way to implement the &ldquo;right to be forgotten&rdquo; in machine learning,\textit{machine unlearning} aims to completely remove the contributions andinformation of the samples to be deleted from a trained model without affectingthe contributions of other samples. Recently, many frameworks for machineunlearning have been proposed, and most of them focus on image and text data.To extend machine unlearning to graph data, \textit{GraphEraser} has beenproposed. However, a critical issue is that \textit{GraphEraser} isspecifically designed for the transductive graph setting, where the graph isstatic and attributes and edges of test nodes are visible during training. Itis unsuitable for the inductive setting, where the graph could be dynamic andthe test graph information is invisible in advance. Such inductive capabilityis essential for production machine learning systems with evolving graphs likesocial media and transaction networks. To fill this gap, we propose the\underline{{\bf G}}\underline{{\bf U}}ided \underline{{\bf I}}n\underline{{\bfD}}uctiv\underline{{\bf E}} Graph Unlearning framework (GUIDE). GUIDE consistsof three components: guided graph partitioning with fairness and balance,efficient subgraph repair, and similarity-based aggregation. Empirically, weevaluate our method on several inductive benchmarks and evolving transactiongraphs. Generally speaking, GUIDE can be efficiently implemented on theinductive graph learning tasks for its low graph partition cost, no matter oncomputation or structure information. The code will be available here:https://github.com/Happy2Git/GUIDE.</div></details><blockquote><p><strong><em>2023-04-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.02835v1><strong>GIF: A General Graph Unlearning Strategy via Influence Function</strong></a></p><p><em>Jiancan Wu, Yi Yang, Yuchun Qian, Yongduo Sui, Xiang Wang, Xiangnan He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the greater emphasis on privacy and security in our society, the problemof graph unlearning &ndash; revoking the influence of specific data on the trainedGNN model, is drawing increasing attention. However, ranging from machineunlearning to recently emerged graph unlearning methods, existing effortseither resort to retraining paradigm, or perform approximate erasure that failsto consider the inter-dependency between connected neighbors or imposesconstraints on GNN structure, therefore hard to achieve satisfyingperformance-complexity trade-offs. In this work, we explore the influence function tailored for graphunlearning, so as to improve the unlearning efficacy and efficiency for graphunlearning. We first present a unified problem formulation of diverse graphunlearning tasks \wrt node, edge, and feature. Then, we recognize the crux tothe inability of traditional influence function for graph unlearning, anddevise Graph Influence Function (GIF), a model-agnostic unlearning method thatcan efficiently and accurately estimate parameter changes in response to a$\epsilon$-mass perturbation in deleted data. The idea is to supplement theobjective of the traditional influence function with an additional loss term ofthe influenced neighbors due to the structural dependency. Further deductionson the closed-form solution of parameter changes provide a better understandingof the unlearning mechanism. We conduct extensive experiments on fourrepresentative GNN models and three benchmark datasets to justify thesuperiority of GIF for diverse graph unlearning tasks in terms of unlearningefficacy, model utility, and unlearning efficiency. Our implementations areavailable at \url{https://github.com/wujcan/GIF-torch/}.</div></details><p><a href=http://arxiv.org/abs/2304.02350v2><strong>Unfolded Self-Reconstruction LSH: Towards Machine Unlearning in Approximate Nearest Neighbour Search</strong></a></p><p><em>Kim Yong Tan, Yueming Lyu, Yew Soon Ong, Ivor W. Tsang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Approximate nearest neighbour (ANN) search is an essential component ofsearch engines, recommendation systems, etc. Many recent works focus onlearning-based data-distribution-dependent hashing and achieve good retrievalperformance. However, due to increasing demand for users&rsquo; privacy and security,we often need to remove users&rsquo; data information from Machine Learning (ML)models to satisfy specific privacy and security requirements. This needrequires the ANN search algorithm to support fast online data deletion andinsertion. Current learning-based hashing methods need retraining the hashfunction, which is prohibitable due to the vast time-cost of large-scale data.To address this problem, we propose a novel data-dependent hashing method namedunfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSHunfolded the optimization update for instance-wise data reconstruction, whichis better for preserving data information than data-independent LSH. Moreover,our USR-LSH supports fast online data deletion and insertion withoutretraining. To the best of our knowledge, we are the first to address themachine unlearning of retrieval problems. Empirically, we demonstrate thatUSR-LSH outperforms the state-of-the-art data-distribution-independent LSH inANN tasks in terms of precision and recall. We also show that USR-LSH hassignificantly faster data deletion and insertion time than learning-baseddata-dependent hashing.</div></details><blockquote><p><strong><em>2023-04-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.02049v1><strong>Multi-Class Explainable Unlearning for Image Classification via Weight Filtering</strong></a></p><p><em>Samuele Poppi, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Unlearning has recently been emerging as a paradigm for selectivelyremoving the impact of training datapoints from a network. While existingapproaches have focused on unlearning either a small subset of the trainingdata or a single class, in this paper we take a different path and devise aframework that can unlearn all classes of an image classification network in asingle untraining round. Our proposed technique learns to modulate the innercomponents of an image classification network through memory matrices so that,after training, the same network can selectively exhibit an unlearning behaviorover any of the classes. By discovering weights which are specific to each ofthe classes, our approach also recovers a representation of the classes whichis explainable by-design. We test the proposed framework, which we name WeightFiltering network (WF-Net), on small-scale and medium-scale imageclassification datasets, with both CNN and Transformer-based backbones. Ourwork provides interesting insights in the development of explainable solutionsfor unlearning and could be easily extended to other vision tasks.</div></details><p><a href=http://arxiv.org/abs/2212.04842v2><strong>PIVOT: Prompting for Video Continual Learning</strong></a></p><p><em>Andrés Villa, Juan León Alcázar, Motasem Alfarra, Kumail Alhamoud, Julio Hurtado, Fabian Caba Heilbron, Alvaro Soto, Bernard Ghanem</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern machine learning pipelines are limited due to data availability,storage quotas, privacy regulations, and expensive annotation processes. Theseconstraints make it difficult or impossible to train and update large-scalemodels on such dynamic annotated sets. Continual learning directly approachesthis problem, with the ultimate goal of devising methods where a deep neuralnetwork effectively learns relevant patterns for new (unseen) classes, withoutsignificantly altering its performance on previously learned ones. In thispaper, we address the problem of continual learning for video data. Weintroduce PIVOT, a novel method that leverages extensive knowledge inpre-trained models from the image domain, thereby reducing the number oftrainable parameters and the associated forgetting. Unlike previous methods,ours is the first approach that effectively uses prompting mechanisms forcontinual learning without any in-domain pre-training. Our experiments showthat PIVOT improves state-of-the-art methods by a significant 27% on the20-task ActivityNet setup.</div></details><blockquote><p><strong><em>2023-04-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.17269v2><strong>A Closer Look at Rehearsal-Free Continual Learning</strong></a></p><p><em>James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, Zsolt Kira</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is a setting where machine learning models learn novelconcepts from continuously shifting training data, while simultaneouslyavoiding degradation of knowledge on previously seen classes which maydisappear from the training data for extended periods of time (a phenomenonknown as the catastrophic forgetting problem). Current approaches for continuallearning of a single expanding task (aka class-incremental continual learning)require extensive rehearsal of previously seen data to avoid this degradationof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it mayalso violate data-privacy. Instead, we explore combining knowledge distillationand parameter regularization in new ways to achieve strong continual learningperformance without rehearsal. Specifically, we take a deep dive into commoncontinual learning techniques: prediction distillation, feature distillation,L2 parameter regularization, and EWC parameter regularization. We firstdisprove the common assumption that parameter regularization techniques failfor rehearsal-free continual learning of a single, expanding task. Next, weexplore how to leverage knowledge from a pre-trained model in rehearsal-freecontinual learning and find that vanilla L2 parameter regularizationoutperforms EWC parameter regularization and feature distillation. Finally, weexplore the recently popular ImageNet-R benchmark, and show that L2 parameterregularization implemented in self-attention blocks of a ViT transformeroutperforms recent popular prompting for continual learning methods.</div></details><blockquote><p><strong><em>2023-03-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.14423v1><strong>Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation</strong></a></p><p><em>Yuliang Cai, Jesse Thomason, Mohammad Rostami</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The size and the computational load of fine-tuning large-scale pre-trainedneural network are becoming two major obstacles in adopting machine learning inmany applications. Continual learning (CL) can serve as a remedy throughenabling knowledge-transfer across sequentially arriving tasks which relaxesthe need to fine-tune all network weights from scratch. However, existing CLalgorithms primarily consider learning unimodal vision-only or language-onlytasks. We develop a transformer-based CL architecture for learning bimodalvision-and-language tasks based on increasing the number of the learnableparameters dynamically and using knowledge distillation. The new additionalparameters are used to specialize the network for each task. Our approachenables sharing information between the tasks while addressing the challenge ofcatastrophic forgetting. Our approach is scalable learning to a large number oftasks because it requires little memory and time overhead. Our model reachesstate-of-the-art performance on challenging vision-and-language tasks.</div></details><blockquote><p><strong><em>2023-03-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.13862v1><strong>Two-level Graph Network for Few-Shot Class-Incremental Learning</strong></a></p><p><em>Hao Chen, Linyan Li, Fan Lyu, Fuyuan Hu, Zhenping Xia, Fenglei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Few-shot class-incremental learning (FSCIL) aims to design machine learningalgorithms that can continually learn new concepts from a few data points,without forgetting knowledge of old classes. The difficulty lies in thatlimited data from new classes not only lead to significant overfitting issuesbut also exacerbates the notorious catastrophic forgetting problems. However,existing FSCIL methods ignore the semantic relationships between sample-leveland class-level. % Using the advantage that graph neural network (GNN) can minerich information among few samples, In this paper, we designed a two-levelgraph network for FSCIL named Sample-level and Class-level Graph Neural Network(SCGN). Specifically, a pseudo incremental learning paradigm is designed inSCGN, which synthesizes virtual few-shot tasks as new tasks to optimize SCGNmodel parameters in advance. Sample-level graph network uses the relationshipof a few samples to aggregate similar samples and obtains refined class-levelfeatures. Class-level graph network aims to mitigate the semantic conflictbetween prototype features of new classes and old classes. SCGN buildstwo-level graph networks to guarantee the latent semantic of each few-shotclass can be effectively represented in FSCIL. Experiments on three popularbenchmark datasets show that our method significantly outperforms the baselinesand sets new state-of-the-art results with remarkable advantages.</div></details><blockquote><p><strong><em>2023-03-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2301.01217v4><strong>Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples</strong></a></p><p><em>Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yu-Gang Jiang, Yaowei Wang, Changsheng Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: There is a growing interest in developing unlearnable examples (UEs) againstvisual privacy leaks on the Internet. UEs are training samples added withinvisible but unlearnable noise, which have been found can prevent unauthorizedtraining of machine learning models. UEs typically are generated via a bileveloptimization framework with a surrogate model to remove (minimize) errors fromthe original samples, and then applied to protect the data against unknowntarget models. However, existing UE generation methods all rely on an idealassumption called label-consistency, where the hackers and protectors areassumed to hold the same label for a given sample. In this work, we propose andpromote a more practical label-agnostic setting, where the hackers may exploitthe protected data quite differently from the protectors. E.g., a m-classunlearnable dataset held by the protector may be exploited by the hacker as an-class dataset. Existing UE generation methods are rendered ineffective inthis challenging setting. To tackle this challenge, we present a noveltechnique called Unlearnable Clusters (UCs) to generate label-agnosticunlearnable examples with cluster-wise perturbations. Furthermore, we proposeto leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as thesurrogate model to improve the transferability of the crafted UCs to diversedomains. We empirically verify the effectiveness of our proposed approach undera variety of settings with different datasets, target models, and evencommercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is availableat \url{https://github.com/jiamingzhang94/Unlearnable-Clusters}.</div></details><blockquote><p><strong><em>2023-03-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.12267v1><strong>AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection</strong></a></p><p><em>Puning Yang, Jian Liang, Jie Cao, Ran He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Out-of-distribution (OOD) detection is a crucial aspect of deploying machinelearning models in open-world applications. Empirical evidence suggests thattraining with auxiliary outliers substantially improves OOD detection. However,such outliers typically exhibit a distribution gap compared to the test OODdata and do not cover all possible test OOD scenarios. Additionally,incorporating these outliers introduces additional training burdens. In thispaper, we introduce a novel paradigm called test-time OOD detection, whichutilizes unlabeled online data directly at test time to improve OOD detectionperformance. While this paradigm is efficient, it also presents challenges suchas catastrophic forgetting. To address these challenges, we propose adaptiveoutlier optimization (AUTO), which consists of an in-out-aware filter, an IDmemory bank, and a semantically-consistent objective. AUTO adaptively minespseudo-ID and pseudo-OOD samples from test data, utilizing them to optimizenetworks in real time during inference. Extensive results on CIFAR-10,CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhancesOOD detection performance.</div></details><blockquote><p><strong><em>2023-03-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.11570v1><strong>Boundary Unlearning</strong></a></p><p><em>Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, Chen Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The practical needs of the ``right to be forgotten&rsquo;&rsquo; and poisoned dataremoval call for efficient \textit{machine unlearning} techniques, which enablemachine learning models to unlearn, or to forget a fraction of training dataand its lineage. Recent studies on machine unlearning for deep neural networks(DNNs) attempt to destroy the influence of the forgetting data by scrubbing themodel parameters. However, it is prohibitively expensive due to the largedimension of the parameter space. In this paper, we refocus our attention fromthe parameter space to the decision space of the DNN model, and proposeBoundary Unlearning, a rapid yet effective way to unlearn an entire class froma trained DNN model. The key idea is to shift the decision boundary of theoriginal DNN model to imitate the decision behavior of the model retrained fromscratch. We develop two novel boundary shift methods, namely Boundary Shrinkand Boundary Expanding, both of which can rapidly achieve the utility andprivacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 andVggface2 datasets, and the results show that Boundary Unlearning caneffectively forget the forgetting class on image classification and facerecognition tasks, with an expected speed-up of $17\times$ and $19\times$,respectively, compared with retraining from the scratch.</div></details><blockquote><p><strong><em>2023-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.09126v2><strong>Verifiable and Provably Secure Machine Unlearning</strong></a></p><p><em>Thorsten Eisenhofer, Doreen Riepel, Varun Chandrasekaran, Esha Ghosh, Olga Ohrimenko, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning aims to remove points from the training dataset of amachine learning model after training; for example when a user requests theirdata to be deleted. While many machine unlearning methods have been proposed,none of them enable users to audit the procedure. Furthermore, recent workshows a user is unable to verify if their data was unlearnt from an inspectionof the model alone. Rather than reasoning about model parameters, we propose toview verifiable unlearning as a security problem. To this end, we present thefirst cryptographic definition of verifiable unlearning to formally capture theguarantees of a machine unlearning system. In this framework, the server firstcomputes a proof that the model was trained on a dataset $D$. Given a user datapoint $d$ requested to be deleted, the server updates the model using anunlearning algorithm. It then provides a proof of the correct execution ofunlearning and that $d \notin D&rsquo;$, where $D&rsquo;$ is the new training dataset. Ourframework is generally applicable to different unlearning techniques that weabstract as admissible functions. We instantiate the framework, based oncryptographic assumptions, using SNARKs and hash chains. Finally, we implementthe protocol for three different unlearning techniques (retraining-based,amnesiac, and optimization-based) to validate its feasibility for linearregression, logistic regression, and neural networks.</div></details><blockquote><p><strong><em>2023-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.11076v1><strong>From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning</strong></a></p><p><em>Kamil Faber, Dominik Zurek, Marcin Pietron, Nathalie Japkowicz, Antonio Vergari, Roberto Corizzo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) is one of the most promising trends in recent machinelearning research. Its goal is to go beyond classical assumptions in machinelearning and develop models and learning strategies that present highrobustness in dynamic environments. The landscape of CL research is fragmentedinto several learning evaluation protocols, comprising different learningtasks, datasets, and evaluation metrics. Additionally, the benchmarks adoptedso far are still distant from the complexity of real-world scenarios, and areusually tailored to highlight capabilities specific to certain strategies. Insuch a landscape, it is hard to objectively assess strategies. In this work, wefill this gap for CL on image data by introducing two novel CL benchmarks thatinvolve multiple heterogeneous tasks from six image datasets, with varyinglevels of complexity and quality. Our aim is to fairly evaluate currentstate-of-the-art CL strategies on a common ground that is closer to complexreal-world scenarios. We additionally structure our benchmarks so that tasksare presented in increasing and decreasing order of complexity &ndash; according toa curriculum &ndash; in order to evaluate if current CL models are able to exploitstructure across tasks. We devote particular emphasis to providing the CLcommunity with a rigorous and reproducible evaluation protocol for measuringthe ability of a model to generalize and not to forget while learning.Furthermore, we provide an extensive experimental evaluation showing thatpopular CL strategies, when challenged with our benchmarks, yield sub-parperformance, high levels of forgetting, and present a limited ability toeffectively leverage curriculum task ordering. We believe that these resultshighlight the need for rigorous comparisons in future CL works as well as pavethe way to design new CL strategies that are able to deal with more complexscenarios.</div></details><blockquote><p><strong><em>2023-03-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.14307v3><strong>GradMA: A Gradient-Memory-based Accelerated Federated Learning with Alleviated Catastrophic Forgetting</strong></a></p><p><em>Kangyang Luo, Xiang Li, Yunshi Lan, Ming Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) has emerged as a de facto machine learning area andreceived rapid increasing research interests from the community. However,catastrophic forgetting caused by data heterogeneity and partial participationposes distinctive challenges for FL, which are detrimental to the performance.To tackle the problems, we propose a new FL approach (namely GradMA), whichtakes inspiration from continual learning to simultaneously correct theserver-side and worker-side update directions as well as take full advantage ofserver&rsquo;s rich computing and memory resources. Furthermore, we elaborate amemory reduction strategy to enable GradMA to accommodate FL with a large scaleof workers. We then analyze convergence of GradMA theoretically under thesmooth non-convex setting and show that its convergence rate achieves a linearspeed up w.r.t the increasing number of sampled active workers. At last, ourextensive experiments on various image classification tasks show that GradMAachieves significant performance gains in accuracy and communication efficiencycompared to SOTA baselines.</div></details><blockquote><p><strong><em>2023-03-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.15567v2><strong>Few-Shot Unlearning by Model Inversion</strong></a></p><p><em>Youngsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, Jungseul Ok</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We consider a practical scenario of machine unlearning to erase a targetdataset, which causes unexpected behavior from the trained model. The targetdataset is often assumed to be fully identifiable in a standard unlearningscenario. Such a flawless identification, however, is almost impossible if thetraining dataset is inaccessible at the time of unlearning. Unlike previousapproaches requiring a complete set of targets, we consider few-shot unlearningscenario when only a few samples of target data are available. To this end, weformulate the few-shot unlearning problem specifying intentions behind theunlearning request (e.g., purely unlearning, mislabel correction, privacyprotection), and we devise a straightforward framework that (i) retrieves aproxy of the training data via model inversion fully exploiting informationavailable in the context of unlearning; (ii) adjusts the proxy according to theunlearning intention; and (iii) updates the model with the adjusted proxy. Wedemonstrate that our method using only a subset of target data can outperformthe state-of-the-art unlearning methods even with a complete indication oftarget data.</div></details><blockquote><p><strong><em>2023-03-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.06544v1><strong>Informative regularization for a multi-layer perceptron RR Lyrae classifier under data shift</strong></a></p><p><em>Francisco Pérez-Galarce, Karim Pichara, Pablo Huijse, Márcio Catelan, Domingo Mery</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent decades, machine learning has provided valuable models andalgorithms for processing and extracting knowledge from time-series surveys.Different classifiers have been proposed and performed to an excellentstandard. Nevertheless, few papers have tackled the data shift problem inlabeled training sets, which occurs when there is a mismatch between the datadistribution in the training set and the testing set. This drawback can damagethe prediction performance in unseen data. Consequently, we propose a scalableand easily adaptable approach based on an informative regularization and anad-hoc training procedure to mitigate the shift problem during the training ofa multi-layer perceptron for RR Lyrae classification. We collect ranges forcharacteristic features to construct a symbolic representation of priorknowledge, which was used to model the informative regularizer component.Simultaneously, we design a two-step back-propagation algorithm to integratethis knowledge into the neural network, whereby one step is applied in eachepoch to minimize classification error, while another is applied to ensureregularization. Our algorithm defines a subset of parameters (a mask) for eachloss function. This approach handles the forgetting effect, which stems from atrade-off between these loss functions (learning from data versus learningexpert knowledge) during training. Experiments were conducted using recentlyproposed shifted benchmark sets for RR Lyrae stars, outperforming baselinemodels by up to 3% through a more reliable classifier. Our method provides anew path to incorporate knowledge from characteristic features into artificialneural networks to manage the underlying data shift problem.</div></details><p><a href=http://arxiv.org/abs/2303.06580v1><strong>Towards General Purpose Medical AI: Continual Learning Medical Foundation Model</strong></a></p><p><em>Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Inevitable domain and task discrepancies in real-world scenarios can impairthe generalization performance of the pre-trained deep models for medical data.Therefore, we audaciously propose that we should build a general-purposemedical AI system that can be seamlessly adapted to downstream domains/tasks.Since the domain/task adaption procedures usually involve additional labelingwork for the target data, designing a data-efficient adaption algorithm isdesired to save the cost of transferring the learned knowledge. Our recent workfound that vision-language models (VLMs) are efficient learners withextraordinary cross-domain ability. Therefore, in this work, we further explorethe possibility of leveraging pre-trained VLMs as medical foundation models forbuilding general-purpose medical AI, where we thoroughly investigate threemachine-learning paradigms, i.e., domain/task-specialized learning, jointlearning, and continual learning, for training the VLMs and evaluate theirgeneralization performance on cross-domain and cross-task test sets. Toalleviate the catastrophic forgetting during sequential training, we employrehearsal learning and receive a sharp boost in terms of generalizationcapability. In a nutshell, our empirical evidence suggests that continuallearning may be a practical and efficient learning paradigm for the medicalfoundation model. And we hope researchers can use our empirical evidence asbasement to further explore the path toward medical foundation model.</div></details><blockquote><p><strong><em>2023-03-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.05985v1><strong>KubeEdge-Sedna v0.3: Towards Next-Generation Automatically Customized AI Engineering Scheme</strong></a></p><p><em>Zimu Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The scale of the global edge AI market continues to grow. The currenttechnical challenges that hinder the large-scale replication of edge AI aremainly small samples on the edge and heterogeneity of edge data. In addition,edge AI customers often have requirements for data security compliance andoffline autonomy of edge AI services. Based on the lifelong learning method inthe academic world, we formally define the problem of edge-cloud collaborativelifelong learning for the first time, and release the industry&rsquo;s firstopen-source edge-cloud collaborative lifelong learning. Edge-cloudcollaborative lifelong learning adapts to data heterogeneity at different edgelocations through (1) multi-task transfer learning to achieve accurateprediction of &ldquo;thousands of people and thousands of faces&rdquo;; (2) incrementalprocessing of unknown tasks, the more systems learn and the smarter systems arewith small samples, gradually realize AI engineering and automation; (3) Usethe cloud-side knowledge base to remember new situational knowledge to avoidcatastrophic forgetting; (4) The edge-cloud collaborative architecture enablesdata security compliance and edge AI services to be offline autonomy whileapplying cloud resources. This work hopes to help fundamentally solve theabove-mentioned challenges of edge-cloud collaborative machine learning.</div></details><blockquote><p><strong><em>2023-03-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.04183v1><strong>Robustness-preserving Lifelong Learning via Dataset Condensation</strong></a></p><p><em>Jinghan Jia, Yihua Zhang, Dogyoon Song, Sijia Liu, Alfred Hero</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Lifelong learning (LL) aims to improve a predictive model as the data sourceevolves continuously. Most work in this learning paradigm has focused onresolving the problem of &lsquo;catastrophic forgetting,&rsquo; which refers to a notoriousdilemma between improving model accuracy over new data and retaining accuracyover previous data. Yet, it is also known that machine learning (ML) models canbe vulnerable in the sense that tiny, adversarial input perturbations candeceive the models into producing erroneous predictions. This motivates theresearch objective of this paper - specification of a new LL framework that cansalvage model robustness (against adversarial attacks) from catastrophicforgetting. Specifically, we propose a new memory-replay LL strategy thatleverages modern bi-level optimization techniques to determine the &lsquo;coreset&rsquo; ofthe current data (i.e., a small amount of data to be memorized) for ease ofpreserving adversarial robustness over time. We term the resulting LL framework&rsquo;Data-Efficient Robustness-Preserving LL&rsquo; (DERPLL). The effectiveness of DERPLLis evaluated for class-incremental image classification using ResNet-18 overthe CIFAR-10 dataset. Experimental results show that DERPLL outperforms theconventional coreset-guided LL baseline and achieves a substantial improvementin both standard accuracy and robust accuracy.</div></details><blockquote><p><strong><em>2023-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.12641v3><strong>Prototype-Guided Memory Replay for Continual Learning</strong></a></p><p><em>Stella Ho, Ming Liu, Lan Du, Longxiang Gao, Yong Xiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) refers to a machine learning paradigm that learnscontinuously without forgetting previously acquired knowledge. Thereby, majordifficulty in CL is catastrophic forgetting of preceding tasks, caused byshifts in data distributions. Existing CL models often save a large number ofold examples and stochastically revisit previously seen data to retain oldknowledge. However, the occupied memory size keeps enlarging along withaccumulating seen data. Hereby, we propose a memory-efficient CL method bystoring a few samples to achieve good performance. We devise a dynamicprototype-guided memory replay module and incorporate it into an onlinemeta-learning model. We conduct extensive experiments on text classificationand investigate the effect of training set orders on CL model performance. Theexperimental results testify the superiority of our method in terms offorgetting mitigation and efficiency.</div></details><blockquote><p><strong><em>2023-02-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.02069v2><strong>Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning</strong></a></p><p><em>Xiangrong Zhu, Guangyao Li, Wei Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) recently emerges as a paradigm to train a globalmachine learning model across distributed clients without sharing raw data.Knowledge Graph (KG) embedding represents KGs in a continuous vector space,serving as the backbone of many knowledge-driven applications. As a promisingcombination, federated KG embedding can fully take advantage of knowledgelearned from different clients while preserving the privacy of local data.However, realistic problems such as data heterogeneity and knowledge forgettingstill remain to be concerned. In this paper, we propose FedLU, a novel FLframework for heterogeneous KG embedding learning and unlearning. To cope withthe drift between local optimization and global convergence caused by dataheterogeneity, we propose mutual knowledge distillation to transfer localknowledge to global, and absorb global knowledge back. Moreover, we present anunlearning method based on cognitive neuroscience, which combines retroactiveinterference and passive decay to erase specific knowledge from local clientsand propagate to the global model by reusing knowledge distillation. Weconstruct new datasets for assessing realistic performance of thestate-of-the-arts. Extensive experiments show that FedLU achieves superiorresults in both link prediction and knowledge forgetting.</div></details><blockquote><p><strong><em>2023-02-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.12448v1><strong>Subspace based Federated Unlearning</strong></a></p><p><em>Guanghao Li, Li Shen, Yan Sun, Yue Hu, Han Hu, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) enables multiple clients to train a machine learningmodel collaboratively without exchanging their local data. Federated unlearningis an inverse FL process that aims to remove a specified target client&rsquo;scontribution in FL to satisfy the user&rsquo;s right to be forgotten. Most existingfederated unlearning algorithms require the server to store the history of theparameter updates, which is not applicable in scenarios where the serverstorage resource is constrained. In this paper, we propose asimple-yet-effective subspace based federated unlearning method, dubbed SFU,that lets the global model perform gradient ascent in the orthogonal space ofinput gradient spaces formed by other clients to eliminate the target client&rsquo;scontribution without requiring additional storage. Specifically, the serverfirst collects the gradients generated from the target client after performinggradient ascent, and the input representation matrix is computed locally by theremaining clients. We also design a differential privacy method to protect theprivacy of the representation matrix. Then the server merges thoserepresentation matrices to get the input gradient subspace and updates theglobal model in the orthogonal subspace of the input gradient subspace tocomplete the forgetting task with minimal model performance degradation.Experiments on MNIST, CIFAR10, and CIFAR100 show that SFU outperforms severalstate-of-the-art (SOTA) federated unlearning algorithms by a large margin invarious settings.</div></details><blockquote><p><strong><em>2023-02-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.06640v3><strong>Towards Adversarial Evaluations for Inexact Machine Unlearning</strong></a></p><p><em>Shashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, Ponnurangam Kumaraguru</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Learning models face increased concerns regarding the storage ofpersonal user data and adverse impacts of corrupted data like backdoors orsystematic bias. Machine Unlearning can address these by allowing post-hocdeletion of affected training data from a learned model. Achieving this taskexactly is computationally expensive; consequently, recent works have proposedinexact unlearning algorithms to solve this approximately as well as evaluationmethods to test the effectiveness of these algorithms. In this work, we first outline some necessary criteria for evaluation methodsand show no existing evaluation satisfies them all. Then, we design a strongerblack-box evaluation method called the Interclass Confusion (IC) test whichadversarially manipulates data during training to detect the insufficiency ofunlearning procedures. We also propose two analytically motivated baselinemethods~(EU-k and CF-k) which outperform several popular inexact unlearningmethods. Overall, we demonstrate how adversarial evaluation strategies can helpin analyzing various unlearning phenomena which can guide the development ofstronger unlearning algorithms.</div></details><blockquote><p><strong><em>2023-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.09813v1><strong>Audit to Forget: A Unified Method to Revoke Patients&rsquo; Private Data in Intelligent Healthcare</strong></a></p><p><em>Juexiao Zhou, Haoyang Li, Xingyu Liao, Bin Zhang, Wenjia He, Zhongxiao Li, Longxi Zhou, Xin Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Revoking personal private data is one of the basic human rights, which hasalready been sheltered by several privacy-preserving laws in many countries.However, with the development of data science, machine learning and deeplearning techniques, this right is usually neglected or violated as more andmore patients&rsquo; data are being collected and used for model training, especiallyin intelligent healthcare, thus making intelligent healthcare a sector wheretechnology must meet the law, regulations, and privacy principles to ensurethat the innovation is for the common good. In order to secure patients&rsquo; rightto be forgotten, we proposed a novel solution by using auditing to guide theforgetting process, where auditing means determining whether a dataset has beenused to train the model and forgetting requires the information of a querydataset to be forgotten from the target model. We unified these two tasks byintroducing a new approach called knowledge purification. To implement oursolution, we developed AFS, a unified open-source software, which is able toevaluate and revoke patients&rsquo; private data from pre-trained deep learningmodels. We demonstrated the generality of AFS by applying it to four tasks ondifferent datasets with various data sizes and architectures of deep learningnetworks. The software is publicly available at\url{https://github.com/JoshuaChou2018/AFS}.</div></details><blockquote><p><strong><em>2023-02-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.09310v1><strong>On Handling Catastrophic Forgetting for Incremental Learning of Human Physical Activity on the Edge</strong></a></p><p><em>Jingwei Zuo, George Arvanitakis, Hakim Hacid</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human activity recognition (HAR) has been a classic research problem. Inparticular, with recent machine learning (ML) techniques, the recognition taskhas been largely investigated by companies and integrated into their productsfor customers. However, most of them apply a predefined activity set andconduct the learning process on the cloud, hindering specific personalizationsfrom end users (i.e., edge devices). Even though recent progress in IncrementalLearning allows learning new-class data on the fly, the learning process isgenerally conducted on the cloud, requiring constant data exchange betweencloud and edge devices, thus leading to data privacy issues. In this paper, wepropose PILOTE, which pushes the incremental learning process to the extremeedge, while providing reliable data privacy and practical utility, e.g., lowprocessing latency, personalization, etc. In particular, we consider thepractical challenge of extremely limited data during the incremental learningprocess on edge, where catastrophic forgetting is required to be handled in apractical way. We validate PILOTE with extensive experiments on human activitydata collected from mobile sensors. The results show PILOTE can work on edgedevices with extremely limited resources while providing reliable performance.</div></details><blockquote><p><strong><em>2023-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.12018v1><strong>Gaussian Switch Sampling: A Second Order Approach to Active Learning</strong></a></p><p><em>Ryan Benkert, Mohit Prabhushankar, Ghassan AlRegib, Armin Pacharmi, Enrique Corona</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In active learning, acquisition functions define informativeness directly onthe representation position within the model manifold. However, for mostmachine learning models (in particular neural networks) this representation isnot fixed due to the training pool fluctuations in between active learningrounds. Therefore, several popular strategies are sensitive to experimentparameters (e.g. architecture) and do not consider model robustness toout-of-distribution settings. To alleviate this issue, we propose a groundedsecond-order definition of information content and sample importance within thecontext of active learning. Specifically, we define importance by how often aneural network &ldquo;forgets&rdquo; a sample during training - artifacts of second orderrepresentation shifts. We show that our definition produces highly accurateimportance scores even when the model representations are constrained by thelack of training data. Motivated by our analysis, we develop Gaussian SwitchSampling (GauSS). We show that GauSS is setup agnostic and robust to anomalousdistributions with exhaustive experiments on three in-distribution benchmarks,three out-of-distribution benchmarks, and three different architectures. Wereport an improvement of up to 5% when compared against four popular querystrategies.</div></details><blockquote><p><strong><em>2023-02-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.08911v2><strong>Forget Unlearning: Towards True Data-Deletion in Machine Learning</strong></a></p><p><em>Rishav Chourasia, Neil Shah</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Unlearning algorithms aim to remove deleted data&rsquo;s influence from trainedmodels at a cost lower than full retraining. However, prior guarantees ofunlearning in literature are flawed and don&rsquo;t protect the privacy of deletedrecords. We show that when users delete their data as a function of publishedmodels, records in a database become interdependent. So, even retraining afresh model after deletion of a record doesn&rsquo;t ensure its privacy. Secondly,unlearning algorithms that cache partial computations to speed up theprocessing can leak deleted information over a series of releases, violatingthe privacy of deleted records in the long run. To address these, we propose asound deletion guarantee and show that the privacy of existing records isnecessary for the privacy of deleted records. Under this notion, we propose anaccurate, computationally efficient, and secure machine unlearning algorithmbased on noisy gradient descent.</div></details><blockquote><p><strong><em>2023-02-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.00353v3><strong>Towards Label-Efficient Incremental Learning: A Survey</strong></a></p><p><em>Mert Kilickaya, Joost van de Weijer, Yuki M. Asano</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The current dominant paradigm when building a machine learning model is toiterate over a dataset over and over until convergence. Such an approach isnon-incremental, as it assumes access to all images of all categories at once.However, for many applications, non-incremental learning is unrealistic. Tothat end, researchers study incremental learning, where a learner is requiredto adapt to an incoming stream of data with a varying distribution whilepreventing forgetting of past knowledge. Significant progress has been made,however, the vast majority of works focus on the fully supervised setting,making these algorithms label-hungry thus limiting their real-life deployment.To that end, in this paper, we make the first attempt to survey recentlygrowing interest in label-efficient incremental learning. We identify threesubdivisions, namely semi-, few-shot- and self-supervised learning to reducelabeling efforts. Finally, we identify novel directions that can furtherenhance label-efficiency and improve incremental learning scalability. Projectwebsite: <a href=https://github.com/kilickaya/label-efficient-il>https://github.com/kilickaya/label-efficient-il</a>.</div></details><blockquote><p><strong><em>2023-02-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.03528v1><strong>Efficiently Upgrading Multilingual Machine Translation Models to Support More Languages</strong></a></p><p><em>Simeng Sun, Maha Elbayad, Anna Sun, James Cross</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With multilingual machine translation (MMT) models continuing to grow in sizeand number of supported languages, it is natural to reuse and upgrade existingmodels to save computation as data becomes available in more languages.However, adding new languages requires updating the vocabulary, whichcomplicates the reuse of embeddings. The question of how to reuse existingmodels while also making architectural changes to provide capacity for both oldand new languages has also not been closely studied. In this work, we introducethree techniques that help speed up effective learning of the new languages andalleviate catastrophic forgetting despite vocabulary and architecturemismatches. Our results show that by (1) carefully initializing the network,(2) applying learning rate scaling, and (3) performing data up-sampling, it ispossible to exceed the performance of a same-sized baseline model with 30%computation and recover the performance of a larger model trained from scratchwith over 50% reduction in computation. Furthermore, our analysis reveals thatthe introduced techniques help learn the new directions more effectively andalleviate catastrophic forgetting at the same time. We hope our work will guideresearch into more efficient approaches to growing languages for these MMTmodels and ultimately maximize the reuse of existing models.</div></details><p><a href=http://arxiv.org/abs/2302.03648v1><strong>Deep Class-Incremental Learning: A Survey</strong></a></p><p><em>Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep models, e.g., CNNs and Vision Transformers, have achieved impressiveachievements in many vision tasks in the closed world. However, novel classesemerge from time to time in our ever-changing world, requiring a learningsystem to acquire new knowledge continually. For example, a robot needs tounderstand new instructions, and an opinion monitoring system should analyzeemerging topics every day. Class-Incremental Learning (CIL) enables the learnerto incorporate the knowledge of new classes incrementally and build a universalclassifier among all seen classes. Correspondingly, when directly training themodel with new class instances, a fatal problem occurs &ndash; the model tends tocatastrophically forget the characteristics of former ones, and its performancedrastically degrades. There have been numerous efforts to tackle catastrophicforgetting in the machine learning community. In this paper, we surveycomprehensively recent advances in deep class-incremental learning andsummarize these methods from three aspects, i.e., data-centric, model-centric,and algorithm-centric. We also provide a rigorous and unified evaluation of 16methods in benchmark image classification tasks to find out the characteristicsof different algorithms empirically. Furthermore, we notice that the currentcomparison protocol ignores the influence of memory budget in model storage,which may result in unfair comparison and biased results. Hence, we advocatefair comparison by aligning the memory budget in evaluation, as well as severalmemory-agnostic performance measures. The source code to reproduce theseevaluations is available at <a href=https://github.com/zhoudw-zdw/CIL_Survey/>https://github.com/zhoudw-zdw/CIL_Survey/</a></div></details><blockquote><p><strong><em>2023-02-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.03011v5><strong>LEAF: Navigating Concept Drift in Cellular Networks</strong></a></p><p><em>Shinan Liu, Francesco Bronzino, Paul Schmitt, Arjun Nitin Bhagoji, Nick Feamster, Hector Garcia Crespo, Timothy Coyle, Brian Ward</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Operational networks commonly rely on machine learning models for many tasks,including detecting anomalies, inferring application performance, andforecasting demand. Yet, model accuracy can degrade due to concept drift,whereby the relationship between the features and the target to be predictedchanges. Mitigating concept drift is an essential part of operationalizingmachine learning models in general, but is of particular importance innetworking&rsquo;s highly dynamic deployment environments. In this paper, we firstcharacterize concept drift in a large cellular network for a major metropolitanarea in the United States. We find that concept drift occurs across manyimportant key performance indicators (KPIs), independently of the model,training set size, and time interval &ndash; thus necessitating practical approachesto detect, explain, and mitigate it. We then show that frequent modelretraining with newly available data is not sufficient to mitigate conceptdrift, and can even degrade model accuracy further. Finally, we develop a newmethodology for concept drift mitigation, Local Error Approximation of Features(LEAF). LEAF works by detecting drift; explaining the features and timeintervals that contribute the most to drift; and mitigates it using forgettingand over-sampling. We evaluate LEAF against industry-standard mitigationapproaches (notably, periodic retraining) with more than four years of cellularKPI data. Our initial tests with a major cellular provider in the US show thatLEAF consistently outperforms periodic and triggered retraining on complex,real-world data while reducing costly retraining operations.</div></details><blockquote><p><strong><em>2023-01-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2301.13261v1><strong>Emergence of Maps in the Memories of Blind Navigation Agents</strong></a></p><p><em>Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, Dhruv Batra</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Animal navigation research posits that organisms build and maintain internalspatial representations, or maps, of their environment. We ask if machines &ndash;specifically, artificial intelligence (AI) navigation agents &ndash; also buildimplicit (or &lsquo;mental&rsquo;) maps. A positive answer to this question would (a)explain the surprising phenomenon in recent literature of ostensibly map-freeneural-networks achieving strong performance, and (b) strengthen the evidenceof mapping as a fundamental mechanism for navigation by intelligent embodiedagents, whether they be biological or artificial. Unlike animal navigation, wecan judiciously design the agent&rsquo;s perceptual system and control the learningparadigm to nullify alternative navigation mechanisms. Specifically, we train&rsquo;blind&rsquo; agents &ndash; with sensing limited to only egomotion and no other sensingof any kind &ndash; to perform PointGoal navigation (&lsquo;go to $\Delta$ x, $\Delta$ y&rsquo;)via reinforcement learning. Our agents are composed of navigation-agnosticcomponents (fully-connected and recurrent neural networks), and ourexperimental setup provides no inductive bias towards mapping. Despite theseharsh conditions, we find that blind agents are (1) surprisingly effectivenavigators in new environments (~95% success); (2) they utilize memory overlong horizons (remembering ~1,000 steps of past experience in an episode); (3)this memory enables them to exhibit intelligent behavior (following walls,detecting collisions, taking shortcuts); (4) there is emergence of maps andcollision detection neurons in the representations of the environment built bya blind agent as it navigates; and (5) the emergent maps are selective and taskdependent (e.g. the agent &lsquo;forgets&rsquo; exploratory detours). Overall, this paperpresents no new techniques for the AI audience, but a surprising finding, aninsight, and an explanation.</div></details><blockquote><p><strong><em>2023-01-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.04640v2><strong>Memory Efficient Continual Learning with Transformers</strong></a></p><p><em>Beyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, Cedric Archambeau</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In many real-world scenarios, data to train machine learning models becomesavailable over time. Unfortunately, these models struggle to continually learnnew concepts without forgetting what has been learnt in the past. Thisphenomenon is known as catastrophic forgetting and it is difficult to preventdue to practical constraints. For instance, the amount of data that can bestored or the computational resources that can be used might be limited.Moreover, applications increasingly rely on large pre-trained neural networks,such as pre-trained Transformers, since the resources or data might not beavailable in sufficiently large quantities to practitioners to train the modelfrom scratch. In this paper, we devise a method to incrementally train a modelon a sequence of tasks using pre-trained Transformers and extending them withAdapters. Different than the existing approaches, our method is able to scaleto a large number of tasks without significant overhead and allows sharinginformation across tasks. On both image and text classification tasks, weempirically demonstrate that our method maintains a good predictive performancewithout retraining the model or increasing the number of model parameters overtime. The resulting model is also significantly faster at inference timecompared to Adapter-based state-of-the-art methods.</div></details><blockquote><p><strong><em>2022-12-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.07276v3><strong>Carousel Memory: Rethinking the Design of Episodic Memory for Continual Learning</strong></a></p><p><em>Soobee Lee, Minindu Weerakoon, Jonghyun Choi, Minjia Zhang, Di Wang, Myeongjae Jeon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Learning (CL) is an emerging machine learning paradigm that aims tolearn from a continuous stream of tasks without forgetting knowledge learnedfrom the previous tasks. To avoid performance decrease caused by forgetting,prior studies exploit episodic memory (EM), which stores a subset of the pastobserved samples while learning from new non-i.i.d. data. Despite the promisingresults, since CL is often assumed to execute on mobile or IoT devices, the EMsize is bounded by the small hardware memory capacity and makes it infeasibleto meet the accuracy requirements for real-world applications. Specifically,all prior CL methods discard samples overflowed from the EM and can neverretrieve them back for subsequent training steps, incurring loss of informationthat would exacerbate catastrophic forgetting. We explore a novel hierarchicalEM management strategy to address the forgetting issue. In particular, inmobile and IoT devices, real-time data can be stored not just in high-speedRAMs but in internal storage devices as well, which offer significantly largercapacity than the RAMs. Based on this insight, we propose to exploit theabundant storage to preserve past experiences and alleviate the forgetting byallowing CL to efficiently migrate samples between memory and storage withoutbeing interfered by the slow access speed of the storage. We call it CarouselMemory (CarM). As CarM is complementary to existing CL methods, we conductextensive evaluations of our method with seven popular CL methods and show thatCarM significantly improves the accuracy of the methods across differentsettings by large margins in final average accuracy (up to 28.4%) whileretaining the same training efficiency.</div></details><blockquote><p><strong><em>2022-12-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.12643v1><strong>Utilizing Priming to Identify Optimal Class Ordering to Alleviate Catastrophic Forgetting</strong></a></p><p><em>Gabriel Mantione-Holmes, Justin Leo, Jugal Kalita</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In order for artificial neural networks to begin accurately mimickingbiological ones, they must be able to adapt to new exigencies withoutforgetting what they have learned from previous training. Lifelong learningapproaches to artificial neural networks attempt to strive towards this goal,yet have not progressed far enough to be realistically deployed for naturallanguage processing tasks. The proverbial roadblock of catastrophic forgettingstill gate-keeps researchers from an adequate lifelong learning model. Whileefforts are being made to quell catastrophic forgetting, there is a lack ofresearch that looks into the importance of class ordering when training on newclasses for incremental learning. This is surprising as the ordering of"classes" that humans learn is heavily monitored and incredibly important.While heuristics to develop an ideal class order have been researched, thispaper examines class ordering as it relates to priming as a scheme forincremental class learning. By examining the connections between variousmethods of priming found in humans and how those are mimicked yet remainunexplained in life-long machine learning, this paper provides a betterunderstanding of the similarities between our biological systems and thesynthetic systems while simultaneously improving current practices to combatcatastrophic forgetting. Through the merging of psychological priming practiceswith class ordering, this paper is able to identify a generalizable method forclass ordering in NLP incremental learning tasks that consistently outperformsrandom class ordering.</div></details><blockquote><p><strong><em>2022-12-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.09679v2><strong>A Review of Deep Transfer Learning and Recent Advancements</strong></a></p><p><em>Mohammadreza Iman, Khaled Rasheed, Hamid R. Arabnia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning has been the answer to many machine learning problems duringthe past two decades. However, it comes with two major constraints: dependencyon extensive labeled data and training costs. Transfer learning in deeplearning, known as Deep Transfer Learning (DTL), attempts to reduce suchdependency and costs by reusing an obtained knowledge from a source data/taskin training on a target data/task. Most applied DTL techniques arenetwork/model-based approaches. These methods reduce the dependency of deeplearning models on extensive training data and drastically decrease trainingcosts. As a result, researchers detected Covid-19 infection on chest X-Rayswith high accuracy at the beginning of the pandemic with minimal data using DTLtechniques. Also, the training cost reduction makes DTL viable on edge deviceswith limited resources. Like any new advancement, DTL methods have their ownlimitations, and a successful transfer depends on some adjustments fordifferent scenarios. In this paper, we review the definition and taxonomy ofdeep transfer learning and well-known methods. Then we investigate the DTLapproaches by reviewing recent applied DTL techniques in the past five years.Further, we review some experimental analyses of DTLs to learn the bestpractice for applying DTL in different scenarios. Moreover, the limitations ofDTLs (catastrophic forgetting dilemma and overly biased pre-trained models) arediscussed, along with possible solutions and research trends.</div></details><blockquote><p><strong><em>2022-12-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.10717v1><strong>Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</strong></a></p><p><em>Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce camouflaged data poisoning attacks, a new attack vector thatarises in the context of machine unlearning and other settings when modelretraining may be induced. An adversary first adds a few carefully craftedpoints to the training dataset such that the impact on the model&rsquo;s predictionsis minimal. The adversary subsequently triggers a request to remove a subset ofthe introduced points at which point the attack is unleashed and the model&rsquo;spredictions are negatively affected. In particular, we consider clean-labeltargeted attacks (in which the goal is to cause the model to misclassify aspecific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.This attack is realized by constructing camouflage datapoints that mask theeffect of a poisoned dataset.</div></details><blockquote><p><strong><em>2022-12-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.09573v1><strong>Privacy Adhering Machine Un-learning in NLP</strong></a></p><p><em>Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, Dan Roth</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Regulations introduced by General Data Protection Regulation (GDPR) in the EUor California Consumer Privacy Act (CCPA) in the US have included provisions onthe \textit{right to be forgotten} that mandates industry applications toremove data related to an individual from their systems. In several real worldindustry applications that use Machine Learning to build models on user data,such mandates require significant effort both in terms of data cleansing aswell as model retraining while ensuring the models do not deteriorate inprediction quality due to removal of data. As a result, continuous removal ofdata and model retraining steps do not scale if these applications receive suchrequests at a very high frequency. Recently, a few researchers proposed theidea of \textit{Machine Unlearning} to tackle this challenge. Despite thesignificant importance of this task, the area of Machine Unlearning isunder-explored in Natural Language Processing (NLP) tasks. In this paper, weexplore the Unlearning framework on various GLUE tasks \cite{Wang:18}, such as,QQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC andSISA-A) to perform \textit{guaranteed} Unlearning that provides significantreduction in terms of both memory (90-95%), time (100x) and space consumption(99%) in comparison to the baselines while keeping model performance constant.</div></details><blockquote><p><strong><em>2022-12-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.12232v4><strong>Bounding Membership Inference</strong></a></p><p><em>Anvith Thudi, Ilia Shumailov, Franziska Boenisch, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Differential Privacy (DP) is the de facto standard for reasoning about theprivacy guarantees of a training algorithm. Despite the empirical observationthat DP reduces the vulnerability of models to existing membership inference(MI) attacks, a theoretical underpinning as to why this is the case is largelymissing in the literature. In practice, this means that models need to betrained with DP guarantees that greatly decrease their accuracy. In this paper, we provide a tighter bound on the positive accuracy (i.e.,attack precision) of any MI adversary when a training algorithm provides$(\varepsilon, \delta)$-DP. Our bound informs the design of a novel privacyamplification scheme: an effective training set is sub-sampled from a largerset prior to the beginning of training. We find this greatly reduces the boundon MI positive accuracy. As a result, our scheme allows the use of looser DPguarantees to limit the success of any MI adversary; this ensures that themodel&rsquo;s accuracy is less impacted by the privacy guarantee. While this clearlybenefits entities working with far more data than they need to train on, it canalso improve the accuracy-privacy trade-off on benchmarks studied in theacademic literature. Consequently, we also find that subsampling decreases theeffectiveness of a state-of-the-art MI attack (LiRA) much more effectively thantraining with stronger DP guarantees on MNIST and CIFAR10. We conclude bydiscussing implications of our MI bound on the field of machine unlearning.</div></details><blockquote><p><strong><em>2022-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.02704v2><strong>Fundamental limits to learning closed-form mathematical models from data</strong></a></p><p><em>Oscar Fajardo-Fontiveros, Ignasi Reichardt, Harry R. De Los Rios, Jordi Duch, Marta Sales-Pardo, Roger Guimera</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Given a finite and noisy dataset generated with a closed-form mathematicalmodel, when is it possible to learn the true generating model from the dataalone? This is the question we investigate here. We show that thismodel-learning problem displays a transition from a low-noise phase in whichthe true model can be learned, to a phase in which the observation noise is toohigh for the true model to be learned by any method. Both in the low-noisephase and in the high-noise phase, probabilistic model selection leads tooptimal generalization to unseen data. This is in contrast to standard machinelearning approaches, including artificial neural networks, which in thisparticular problem are limited, in the low-noise phase, by their ability tointerpolate. In the transition region between the learnable and unlearnablephases, generalization is hard for all approaches including probabilistic modelselection.</div></details><blockquote><p><strong><em>2022-12-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.07876v1><strong>Forgetful Forests: high performance learning data structures for streaming data under concept drift</strong></a></p><p><em>Zhehu Yuan, Yinqi Sun, Dennis Shasha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Database research can help machine learning performance in many ways. One wayis to design better data structures. This paper combines the use of incrementalcomputation and sequential and probabilistic filtering to enable &ldquo;forgetful"tree-based learning algorithms to cope with concept drift data (i.e., datawhose function from input to classification changes over time). The forgetful algorithms described in this paper achieve high timeperformance while maintaining high quality predictions on streaming data.Specifically, the algorithms are up to 24 times faster than state-of-the-artincremental algorithms with at most a 2% loss of accuracy, or at least twicefaster without any loss of accuracy. This makes such structures suitable forhigh volume streaming applications.</div></details><blockquote><p><strong><em>2022-12-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.04603v1><strong>System Design for an Integrated Lifelong Reinforcement Learning Agent for Real-Time Strategy Games</strong></a></p><p><em>Indranil Sur, Zachary Daniels, Abrar Rahman, Kamil Faber, Gianmarco J. Gallardo, Tyler L. Hayes, Cameron E. Taylor, Mustafa Burak Gurbuz, James Smith, Sahana Joshi, Nathalie Japkowicz, Michael Baron, Zsolt Kira, Christopher Kanan, Roberto Corizzo, Ajay Divakaran, Michael Piacentino, Jesse Hostetler, Aswin Raghavan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As Artificial and Robotic Systems are increasingly deployed and relied uponfor real-world applications, it is important that they exhibit the ability tocontinually learn and adapt in dynamically-changing environments, becomingLifelong Learning Machines. Continual/lifelong learning (LL) involvesminimizing catastrophic forgetting of old tasks while maximizing a model&rsquo;scapability to learn new tasks. This paper addresses the challenging lifelongreinforcement learning (L2RL) setting. Pushing the state-of-the-art forward inL2RL and making L2RL useful for practical applications requires more thandeveloping individual L2RL algorithms; it requires making progress at thesystems-level, especially research into the non-trivial problem of how tointegrate multiple L2RL algorithms into a common framework. In this paper, weintroduce the Lifelong Reinforcement Learning Components Framework (L2RLCF),which standardizes L2RL systems and assimilates different continual learningcomponents (each addressing different aspects of the lifelong learning problem)into a unified system. As an instantiation of L2RLCF, we develop a standard APIallowing easy integration of novel lifelong learning components. We describe acase study that demonstrates how multiple independently-developed LL componentscan be integrated into a single realized system. We also introduce anevaluation environment in order to measure the effect of combining varioussystem components. Our evaluation environment employs different LL scenarios(sequences of tasks) consisting of Starcraft-2 minigames and allows for thefair, comprehensive, and quantitative comparison of different combinations ofcomponents within a challenging common evaluation environment.</div></details><blockquote><p><strong><em>2022-12-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.03334v1><strong>Pre-trained Encoders in Self-Supervised Learning Improve Secure and Privacy-preserving Supervised Learning</strong></a></p><p><em>Hongbin Liu, Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Classifiers in supervised learning have various security and privacy issues,e.g., 1) data poisoning attacks, backdoor attacks, and adversarial examples onthe security side as well as 2) inference attacks and the right to be forgottenfor the training data on the privacy side. Various secure andprivacy-preserving supervised learning algorithms with formal guarantees havebeen proposed to address these issues. However, they suffer from variouslimitations such as accuracy loss, small certified security guarantees, and/orinefficiency. Self-supervised learning is an emerging technique to pre-trainencoders using unlabeled data. Given a pre-trained encoder as a featureextractor, supervised learning can train a simple yet accurate classifier usinga small amount of labeled training data. In this work, we perform the firstsystematic, principled measurement study to understand whether and when apre-trained encoder can address the limitations of secure or privacy-preservingsupervised learning algorithms. Our key findings are that a pre-trained encodersubstantially improves 1) both accuracy under no attacks and certified securityguarantees against data poisoning and backdoor attacks of state-of-the-artsecure learning algorithms (i.e., bagging and KNN), 2) certified securityguarantees of randomized smoothing against adversarial examples withoutsacrificing its accuracy under no attacks, 3) accuracy of differentiallyprivate classifiers, and 4) accuracy and/or efficiency of exact machineunlearning.</div></details><p><a href=http://arxiv.org/abs/2212.02800v1><strong>Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation</strong></a></p><p><em>Yang Zhao, Junnan Zhu, Lu Xiang, Jiajun Zhang, Yu Zhou, Feifei Zhai, Chengqing Zong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A common scenario of Multilingual Neural Machine Translation (MNMT) is thateach translation task arrives in a sequential manner, and the training data ofprevious tasks is unavailable. In this scenario, the current methods sufferheavily from catastrophic forgetting (CF). To alleviate the CF, we investigateknowledge distillation based life-long learning methods. Specifically, inone-tomany scenario, we propose a multilingual distillation method to make thenew model (student) jointly learn multilingual output from old model (teacher)and new task. In many-to one scenario, we find that direct distillation facesthe extreme partial distillation problem, and we propose two different methodsto address it: pseudo input distillation and reverse teacher distillation. Theexperimental results on twelve translation tasks show that the proposed methodscan better consolidate the previous knowledge and sharply alleviate the CF.</div></details><p><a href=http://arxiv.org/abs/2212.02974v1><strong>CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain</strong></a></p><p><em>Markus Bayer, Philipp Kuehn, Ramin Shanehsaz, Christian Reuter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The field of cybersecurity is evolving fast. Experts need to be informedabout past, current and - in the best case - upcoming threats, because attacksare becoming more advanced, targets bigger and systems more complex. As thiscannot be addressed manually, cybersecurity experts need to rely on machinelearning techniques. In the texutual domain, pre-trained language models likeBERT have shown to be helpful, by providing a good baseline for furtherfine-tuning. However, due to the domain-knowledge and many technical terms incybersecurity general language models might miss the gist of textualinformation, hence doing more harm than good. For this reason, we create ahigh-quality dataset and present a language model specifically tailored to thecybersecurity domain, which can serve as a basic building block forcybersecurity systems that deal with natural language. The model is comparedwith other models based on 15 different domain-dependent extrinsic andintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On theone hand, the results of the intrinsic tasks show that our model improves theinternal representation space of words compared to the other models. On theother hand, the extrinsic, domain-dependent tasks, consisting of sequencetagging and classification, show that the model is best in specific applicationscenarios, in contrast to the others. Furthermore, we show that our approachagainst catastrophic forgetting works, as the model is able to retrieve thepreviously trained domain-independent knowledge. The used dataset and trainedmodel are made publicly available</div></details><blockquote><p><strong><em>2022-12-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.10956v4><strong>CIRCLE: Continual Repair across Programming Languages</strong></a></p><p><em>Wei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong Hao, Hongzhi Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automatic Program Repair (APR) aims at fixing buggy source code with lessmanual debugging efforts, which plays a vital role in improving softwarereliability and development productivity. Recent APR works have achievedremarkable progress via applying deep learning (DL), particularly neuralmachine translation (NMT) techniques. However, we observe that existingDL-based APR models suffer from at least two severe drawbacks: (1) Most of themcan only generate patches for a single programming language, as a result, torepair multiple languages, we have to build and train many repairing models.(2) Most of them are developed in an offline manner. Therefore, they won&rsquo;tfunction when there are new-coming requirements. To address the above problems,a T5-based APR framework equipped with continual learning ability acrossmultiple programming languages is proposed, namely \emph{C}ont\emph{I}nual\emph{R}epair a\emph{C}ross Programming \emph{L}anguag\emph{E}s(\emph{CIRCLE}). Specifically, (1) CIRCLE utilizes a prompting function tonarrow the gap between natural language processing (NLP) pre-trained tasks andAPR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achievelifelong learning for APR without access to the full historical data. (3) Anelastic regularization method is employed to strengthen CIRCLE&rsquo;s continuallearning ability further, preventing it from catastrophic forgetting. (4)CIRCLE applies a simple but effective re-repairing method to revise generatederrors caused by crossing multiple programming languages. We train CIRCLE forfour languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on fivecommonly used benchmarks. The experimental results demonstrate that CIRCLE notonly effectively and efficiently repairs multiple programming languages incontinual learning settings, but also achieves state-of-the-art performancewith a single repair model.</div></details><blockquote><p><strong><em>2022-11-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.14177v1><strong>Overcoming Catastrophic Forgetting by XAI</strong></a></p><p><em>Giang Nguyen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Explaining the behaviors of deep neural networks, usually considered as blackboxes, is critical especially when they are now being adopted over diverseaspects of human life. Taking the advantages of interpretable machine learning(interpretable ML), this work proposes a novel tool called CatastrophicForgetting Dissector (or CFD) to explain catastrophic forgetting in continuallearning settings. We also introduce a new method called Critical Freezingbased on the observations of our tool. Experiments on ResNet articulate howcatastrophic forgetting happens, particularly showing which components of thisfamous network are forgetting. Our new continual learning algorithm defeatsvarious recent techniques by a significant margin, proving the capability ofthe investigation. Critical freezing not only attacks catastrophic forgettingbut also exposes explainability.</div></details><blockquote><p><strong><em>2022-11-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.07725v1><strong>Hierarchically Structured Task-Agnostic Continual Learning</strong></a></p><p><em>Heinke Hihn, Daniel A. Braun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: One notable weakness of current machine learning algorithms is the poorability of models to solve new problems without forgetting previously acquiredknowledge. The Continual Learning paradigm has emerged as a protocol tosystematically investigate settings where the model sequentially observessamples generated by a series of tasks. In this work, we take a task-agnosticview of continual learning and develop a hierarchical information-theoreticoptimality principle that facilitates a trade-off between learning andforgetting. We derive this principle from a Bayesian perspective and show itsconnections to previous approaches to continual learning. Based on thisprinciple, we propose a neural network layer, called theMixture-of-Variational-Experts layer, that alleviates forgetting by creating aset of information processing paths through the network which is governed by agating policy. Equipped with a diverse and specialized set of parameters, eachpath can be regarded as a distinct sub-network that learns to solve tasks. Toimprove expert allocation, we introduce diversity objectives, which we evaluatein additional ablation studies. Importantly, our approach can operate in atask-agnostic way, i.e., it does not require task-specific knowledge, as is thecase with many existing continual learning algorithms. Due to the generalformulation based on generic utility functions, we can apply this optimalityprinciple to a large variety of learning problems, including supervisedlearning, reinforcement learning, and generative modeling. We demonstrate thecompetitive performance of our method on continual reinforcement learning andvariants of the MNIST, CIFAR-10, and CIFAR-100 datasets.</div></details><blockquote><p><strong><em>2022-11-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.09930v3><strong>BayesPCN: A Continually Learnable Predictive Coding Associative Memory</strong></a></p><p><em>Jason Yoo, Frank Wood</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Associative memory plays an important role in human intelligence and itsmechanisms have been linked to attention in machine learning. While the machinelearning community&rsquo;s interest in associative memories has recently beenrekindled, most work has focused on memory recall ($read$) over memory learning($write$). In this paper, we present BayesPCN, a hierarchical associativememory capable of performing continual one-shot memory writes withoutmeta-learning. Moreover, BayesPCN is able to gradually forget past observations($forget$) to free its memory. Experiments show that BayesPCN can recallcorrupted i.i.d. high-dimensional data observed hundreds to a thousand``timesteps&rsquo;&rsquo; ago without a large drop in recall ability compared to thestate-of-the-art offline-learned parametric memory models.</div></details><blockquote><p><strong><em>2022-11-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.03300v1><strong>HFedMS: Heterogeneous Federated Learning with Memorable Data Semantics in Industrial Metaverse</strong></a></p><p><em>Shenglai Zeng, Zonghang Li, Hongfang Yu, Zhihao Zhang, Long Luo, Bo Li, Dusit Niyato</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL), as a rapidly evolving privacy-preservingcollaborative machine learning paradigm, is a promising approach to enable edgeintelligence in the emerging Industrial Metaverse. Even though many successfuluse cases have proved the feasibility of FL in theory, in the industrialpractice of Metaverse, the problems of non-independent and identicallydistributed (non-i.i.d.) data, learning forgetting caused by streamingindustrial data, and scarce communication bandwidth remain key barriers torealize practical FL. Facing the above three challenges simultaneously, thispaper presents a high-performance and efficient system named HFEDMS forincorporating practical FL into Industrial Metaverse. HFEDMS reduces dataheterogeneity through dynamic grouping and training mode conversion (DynamicSequential-to-Parallel Training, STP). Then, it compensates for the forgottenknowledge by fusing compressed historical data semantics and calibratesclassifier parameters (Semantic Compression and Compensation, SCC). Finally,the network parameters of the feature extractor and classifier are synchronizedin different frequencies (Layer-wiseAlternative Synchronization Protocol, LASP)to reduce communication costs. These techniques make FL more adaptable to theheterogeneous streaming data continuously generated by industrial equipment,and are also more efficient in communication than traditional methods (e.g.,Federated Averaging). Extensive experiments have been conducted on the streamednon-i.i.d. FEMNIST dataset using 368 simulated devices. Numerical results showthat HFEDMS improves the classification accuracy by at least 6.4% compared with8 benchmarks and saves both the overall runtime and transfer bytes by up to98%, proving its superiority in precision and efficiency.</div></details><blockquote><p><strong><em>2022-11-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.01542v2><strong>Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions</strong></a></p><p><em>Shuhao Gu, Bojie Hu, Yang Feng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper considers continual learning of large-scale pretrained neuralmachine translation model without accessing the previous training data orintroducing model separation. We argue that the widely usedregularization-based methods, which perform multi-objective learning with anauxiliary loss, suffer from the misestimate problem and cannot always achieve agood balance between the previous and new tasks. To solve the problem, wepropose a two-stage training method based on the local features of the realloss. We first search low forgetting risk regions, where the model can retainthe performance on the previous task as the parameters are updated, to avoidthe catastrophic forgetting problem. Then we can continually train the modelwithin this region only with the new training data to fit the new task.Specifically, we propose two methods to search the low forgetting risk regions,which are based on the curvature of loss and the impacts of the parameters onthe model output, respectively. We conduct experiments on domain adaptation andmore challenging language adaptation tasks, and the experimental results showthat our method can achieve significant improvements compared with severalstrong baselines.</div></details><blockquote><p><strong><em>2022-10-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.12967v2><strong>Anti-Retroactive Interference for Lifelong Learning</strong></a></p><p><em>Runqi Wang, Yuxiang Bao, Baochang Zhang, Jianzhuang Liu, Wentao Zhu, Guodong Guo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans can continuously learn new knowledge. However, machine learning modelssuffer from drastic dropping in performance on previous tasks after learningnew tasks. Cognitive science points out that the competition of similarknowledge is an important cause of forgetting. In this paper, we design aparadigm for lifelong learning based on meta-learning and associative mechanismof the brain. It tackles the problem from two aspects: extracting knowledge andmemorizing knowledge. First, we disrupt the sample&rsquo;s background distributionthrough a background attack, which strengthens the model to extract the keyfeatures of each task. Second, according to the similarity between incrementalknowledge and base knowledge, we design an adaptive fusion of incrementalknowledge, which helps the model allocate capacity to the knowledge ofdifferent difficulties. It is theoretically analyzed that the proposed learningparadigm can make the models of different tasks converge to the same optimum.The proposed method is validated on the MNIST, CIFAR100, CUB200 and ImageNet100datasets.</div></details><blockquote><p><strong><em>2022-10-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.16023v1><strong>LegoNet: A Fast and Exact Unlearning Architecture</strong></a></p><p><em>Sihao Yu, Fei Sun, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning aims to erase the impact of specific training samples upondeleted requests from a trained model. Re-training the model on the retaineddata after deletion is an effective but not efficient way due to the hugenumber of model parameters and re-training samples. To speed up, a natural wayis to reduce such parameters and samples. However, such a strategy typicallyleads to a loss in model performance, which poses the challenge that increasingthe unlearning efficiency while maintaining acceptable performance. In thispaper, we present a novel network, namely \textit{LegoNet}, which adopts theframework of ``fixed encoder + multiple adapters&rsquo;&rsquo;. We fix the encoder~(\ie thebackbone for representation learning) of LegoNet to reduce the parameters thatneed to be re-trained during unlearning. Since the encoder occupies a majorpart of the model parameters, the unlearning efficiency is significantlyimproved. However, fixing the encoder empirically leads to a significantperformance drop. To compensate for the performance loss, we adopt the ensembleof multiple adapters, which are independent sub-models adopted to infer theprediction by the encoding~(\ie the output of the encoder). Furthermore, wedesign an activation mechanism for the adapters to further trade off unlearningefficiency against model performance. This mechanism guarantees that eachsample can only impact very few adapters, so during unlearning, parameters andsamples that need to be re-trained are both reduced. The empirical experimentsverify that LegoNet accomplishes fast and exact unlearning while maintainingacceptable performance, synthetically outperforming unlearning baselines.</div></details><p><a href=http://arxiv.org/abs/2206.09140v2><strong>Certified Graph Unlearning</strong></a></p><p><em>Eli Chien, Chao Pan, Olgica Milenkovic</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Graph-structured data is ubiquitous in practice and often processed usinggraph neural networks (GNNs). With the adoption of recent laws ensuring the``right to be forgotten&rsquo;&rsquo;, the problem of graph data removal has become ofsignificant importance. To address the problem, we introduce the first knownframework for \emph{certified graph unlearning} of GNNs. In contrast tostandard machine unlearning, new analytical and heuristic unlearning challengesarise when dealing with complex graph data. First, three different types ofunlearning requests need to be considered, including node feature, edge andnode unlearning. Second, to establish provable performance guarantees, oneneeds to address challenges associated with feature mixing during propagation.The underlying analysis is illustrated on the example of simple graphconvolutions (SGC) and their generalized PageRank (GPR) extensions, therebylaying the theoretical foundation for certified unlearning of GNNs. Ourempirical studies on six benchmark datasets demonstrate excellentperformance-complexity trade-offs when compared to complete retraining methodsand approaches that do not leverage graph information. For example, whenunlearning $20%$ of the nodes on the Cora dataset, our approach suffers only a$0.1%$ loss in test accuracy while offering a $4$-fold speed-up compared tocomplete retraining. Our scheme also outperforms unlearning methods that do notleverage graph information with a $12%$ increase in test accuracy for acomparable time complexity.</div></details><blockquote><p><strong><em>2022-10-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.12647v2><strong>Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation</strong></a></p><p><em>Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we explore the challenging problem of performing a generativetask in a target language when labeled data is only available in English, usingsummarization as a case study. We assume a strict setting with no access toparallel data or machine translation and find that common transfer learningapproaches struggle in this setting, as a generative multilingual modelfine-tuned purely on English catastrophically forgets how to generatenon-English. Given the recent rise of parameter-efficient adaptationtechniques, we conduct the first investigation into how one such method, prompttuning (Lester et al., 2021), can overcome catastrophic forgetting to enablezero-shot cross-lingual generation. Our experiments show thatparameter-efficient prompt tuning provides gains over standard fine-tuning whentransferring between less-related languages, e.g., from English to Thai.However, a significant gap still remains between these methods andfully-supervised baselines. To improve cross-lingual transfer further, weexplore several approaches, including: (1) mixing in unlabeled multilingualdata, and (2) explicitly factoring prompts into recombinable language and taskcomponents. Our approaches can provide further quality gains, suggesting thatrobust zero-shot cross-lingual generation is within reach.</div></details><blockquote><p><strong><em>2022-10-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.02299v5><strong>A Survey of Machine Unlearning</strong></a></p><p><em>Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, Quoc Viet Hung Nguyen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Today, computer systems hold large amounts of personal data. Yet while suchan abundance of data allows breakthroughs in artificial intelligence, andespecially machine learning (ML), its existence can be a threat to userprivacy, and it can weaken the bonds of trust between humans and AI. Recentregulations now require that, on request, private information about a user mustbe removed from both computer systems and from ML models, i.e. ``the right tobe forgotten&rsquo;&rsquo;). While removing data from back-end databases should bestraightforward, it is not sufficient in the AI context as ML models often`remember&rsquo; the old data. Contemporary adversarial attacks on trained modelshave proven that we can learn whether an instance or an attribute belonged tothe training data. This phenomenon calls for a new paradigm, namely machineunlearning, to make ML models forget about particular data. It turns out thatrecent works on machine unlearning have not been able to completely solve theproblem due to the lack of common frameworks and resources. Therefore, thispaper aspires to present a comprehensive examination of machine unlearning&rsquo;sconcepts, scenarios, methods, and applications. Specifically, as a categorycollection of cutting-edge studies, the intention behind this article is toserve as a comprehensive resource for researchers and practitioners seeking anintroduction to machine unlearning and its formulations, design criteria,removal requests, algorithms, and applications. In addition, we aim tohighlight the key findings, current trends, and new research areas that havenot yet featured the use of machine unlearning but could benefit greatly fromit. We hope this survey serves as a valuable resource for ML researchers andthose seeking to innovate privacy technologies. Our resources are publiclyavailable at <a href=https://github.com/tamlhp/awesome-machine-unlearning>https://github.com/tamlhp/awesome-machine-unlearning</a>.</div></details><p><a href=http://arxiv.org/abs/2210.11334v2><strong>Proof of Unlearning: Definitions and Instantiation</strong></a></p><p><em>Jiasi Weng, Shenglong Yao, Yuefeng Du, Junjie Huang, Jian Weng, Cong Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The &ldquo;Right to be Forgotten&rdquo; rule in machine learning (ML) practice enablessome individual data to be deleted from a trained model, as pursued by recentlydeveloped machine unlearning techniques. To truly comply with the rule, anatural and necessary step is to verify if the individual data are indeeddeleted after unlearning. Yet, previous parameter-space verification metricsmay be easily evaded by a distrustful model trainer. Thus, Thudi et al.recently present a call to action on algorithm-level verification in USENIXSecurity'22. We respond to the call, by reconsidering the unlearning problem in thescenario of machine learning as a service (MLaaS), and proposing a newdefinition framework for Proof of Unlearning (PoUL) on algorithm level.Specifically, our PoUL definitions (i) enforce correctness properties on boththe pre and post phases of unlearning, so as to prevent the state-of-the-artforging attacks; (ii) highlight proper practicality requirements of both theprover and verifier sides with minimal invasiveness to the off-the-shelfservice pipeline and computational workloads. Under the definition framework,we subsequently present a trusted hardware-empowered instantiation using SGXenclave, by logically incorporating an authentication layer for tracing thedata lineage with a proving layer for supporting the audit of learning. Wecustomize authenticated data structures to support large out-of-enclave storagewith simple operation logic, and meanwhile, enable proving complex unlearninglogic with affordable memory footprints in the enclave. We finally validate thefeasibility of the proposed instantiation with a proof-of-conceptimplementation and multi-dimensional performance evaluation.</div></details><blockquote><p><strong><em>2022-09-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.15276v1><strong>Machine Unlearning Method Based On Projection Residual</strong></a></p><p><em>Zihao Cao, Jianzong Wang, Shijing Si, Zhangcheng Huang, Jing Xiao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models (mainly neural networks) are used more and more inreal life. Users feed their data to the model for training. But these processesare often one-way. Once trained, the model remembers the data. Even when datais removed from the dataset, the effects of these data persist in the model.With more and more laws and regulations around the world protecting dataprivacy, it becomes even more important to make models forget this datacompletely through machine unlearning. This paper adopts the projection residual method based on Newton iterationmethod. The main purpose is to implement machine unlearning tasks in thecontext of linear regression models and neural network models. This methodmainly uses the iterative weighting method to completely forget the data andits corresponding influence, and its computational cost is linear in thefeature dimension of the data. This method can improve the current machinelearning method. At the same time, it is independent of the size of thetraining set. Results were evaluated by feature injection testing (FIT).Experiments show that this method is more thorough in deleting data, which isclose to model retraining.</div></details><blockquote><p><strong><em>2022-09-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.03853v2><strong>Project and Forget: Solving Large-Scale Metric Constrained Problems</strong></a></p><p><em>Rishi Sonthalia, Anna C. Gilbert</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Given a set of dissimilarity measurements amongst data points, determiningwhat metric representation is most &ldquo;consistent&rdquo; with the input measurements orthe metric that best captures the relevant geometric features of the data is akey step in many machine learning algorithms. Existing methods are restrictedto specific kinds of metrics or small problem sizes because of the large numberof metric constraints in such problems. In this paper, we provide an active setalgorithm, Project and Forget, that uses Bregman projections, to solve metricconstrained problems with many (possibly exponentially) inequality constraints.We provide a theoretical analysis of \textsc{Project and Forget} and prove thatour algorithm converges to the global optimal solution and that the $L_2$distance of the current iterate to the optimal solution decays asymptoticallyat an exponential rate. We demonstrate that using our method we can solve largeproblem instances of three types of metric constrained problems: general weightcorrelation clustering, metric nearness, and metric learning; in each case,out-performing the state of the art methods with respect to CPU times andproblem sizes.</div></details><blockquote><p><strong><em>2022-09-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.12269v1><strong>Algorithms that Approximate Data Removal: New Results and Limitations</strong></a></p><p><em>Vinith M. Suriyakumar, Ashia C. Wilson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of deleting user data from machine learning modelstrained using empirical risk minimization. Our focus is on learning algorithmswhich return the empirical risk minimizer and approximate unlearning algorithmsthat comply with deletion requests that come streaming minibatches. Leveragingthe infintesimal jacknife, we develop an online unlearning algorithm that isboth computationally and memory efficient. Unlike prior memory efficientunlearning algorithms, we target models that minimize objectives withnon-smooth regularizers, such as the commonly used $\ell_1$, elastic net, ornuclear norm penalties. We also provide generalization, deletion capacity, andunlearning guarantees that are consistent with state of the art methods. Acrossa variety of benchmark datasets, our algorithm empirically improves upon theruntime of prior methods while maintaining the same memory requirements andtest accuracy. Finally, we open a new direction of inquiry by proving that allapproximate unlearning algorithms introduced so far fail to unlearn in problemsettings where common hyperparameter tuning methods, such as cross-validation,have been used to select models.</div></details><blockquote><p><strong><em>2022-09-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.10836v2><strong>Evaluating Machine Unlearning via Epistemic Uncertainty</strong></a></p><p><em>Alexander Becker, Thomas Liebig</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: There has been a growing interest in Machine Unlearning recently, primarilydue to legal requirements such as the General Data Protection Regulation (GDPR)and the California Consumer Privacy Act. Thus, multiple approaches werepresented to remove the influence of specific target data points from a trainedmodel. However, when evaluating the success of unlearning, current approacheseither use adversarial attacks or compare their results to the optimalsolution, which usually incorporates retraining from scratch. We argue thatboth ways are insufficient in practice. In this work, we present an evaluationmetric for Machine Unlearning algorithms based on epistemic uncertainty. Thisis the first definition of a general evaluation metric for Machine Unlearningto our best knowledge.</div></details><blockquote><p><strong><em>2022-09-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.14991v2><strong>Graph Unlearning</strong></a></p><p><em>Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning is a process of removing the impact of some training datafrom the machine learning (ML) models upon receiving removal requests. Whilestraightforward and legitimate, retraining the ML model from scratch incurs ahigh computational overhead. To address this issue, a number of approximatealgorithms have been proposed in the domain of image and text data, among whichSISA is the state-of-the-art solution. It randomly partitions the training setinto multiple shards and trains a constituent model for each shard. However,directly applying SISA to the graph data can severely damage the graphstructural information, and thereby the resulting ML model utility. In thispaper, we propose GraphEraser, a novel machine unlearning framework tailored tograph data. Its contributions include two novel graph partition algorithms anda learning-based aggregation method. We conduct extensive experiments on fivereal-world graph datasets to illustrate the unlearning efficiency and modelutility of GraphEraser. It achieves 2.06$\times$ (small dataset) to35.94$\times$ (large dataset) unlearning time improvement. On the other hand,GraphEraser achieves up to $62.5%$ higher F1 score and our proposedlearning-based aggregation method achieves up to $112%$ higher F1score.\footnote{Our code is available at\url{https://github.com/MinChen00/Graph-Unlearning}.}</div></details><blockquote><p><strong><em>2022-09-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.03227v2><strong>Challenges and Pitfalls of Bayesian Unlearning</strong></a></p><p><em>Ambrish Rawat, James Requeima, Wessel Bruinsma, Richard Turner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning refers to the task of removing a subset of training data,thereby removing its contributions to a trained model. Approximate unlearningare one class of methods for this task which avoid the need to retrain themodel from scratch on the retained data. Bayes&rsquo; rule can be used to castapproximate unlearning as an inference problem where the objective is to obtainthe updated posterior by dividing out the likelihood of deleted data. Howeverthis has its own set of challenges as one often doesn&rsquo;t have access to theexact posterior of the model parameters. In this work we examine the use of theLaplace approximation and Variational Inference to obtain the updatedposterior. With a neural network trained for a regression task as the guidingexample, we draw insights on the applicability of Bayesian unlearning inpractical scenarios.</div></details><blockquote><p><strong><em>2022-09-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.04425v1><strong>The Role Of Biology In Deep Learning</strong></a></p><p><em>Robert Bain</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial neural networks took a lot of inspiration from their biologicalcounterparts in becoming our best machine perceptual systems. This worksummarizes some of that history and incorporates modern theoreticalneuroscience into experiments with artificial neural networks from the field ofdeep learning. Specifically, iterative magnitude pruning is used to trainsparsely connected networks with 33x fewer weights without loss in performance.These are used to test and ultimately reject the hypothesis that weightsparsity alone improves image noise robustness. Recent work mitigatedcatastrophic forgetting using weight sparsity, activation sparsity, and activedendrite modeling. This paper replicates those findings, and extends the methodto train convolutional neural networks on a more challenging continual learningtask. The code has been made publicly available.</div></details><blockquote><p><strong><em>2022-09-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.01501v1><strong>Meta-Learning with Less Forgetting on Large-Scale Non-Stationary Task Distributions</strong></a></p><p><em>Zhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Donglin Zhan, Tiehang Duan, Mingchen Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The paradigm of machine intelligence moves from purely supervised learning toa more practical scenario when many loosely related unlabeled data areavailable and labeled data is scarce. Most existing algorithms assume that theunderlying task distribution is stationary. Here we consider a more realisticand challenging setting in that task distributions evolve over time. We namethis problem as Semi-supervised meta-learning with Evolving Task diStributions,abbreviated as SETS. Two key challenges arise in this more realistic setting:(i) how to use unlabeled data in the presence of a large amount of unlabeledout-of-distribution (OOD) data; and (ii) how to prevent catastrophic forgettingon previously learned task distributions due to the task distribution shift. Wepropose an OOD Robust and knowleDge presErved semi-supeRvised meta-learningapproach (ORDER), to tackle these two major challenges. Specifically, our ORDERintroduces a novel mutual information regularization to robustify the modelwith unlabeled OOD data and adopts an optimal transport regularization toremember previously learned knowledge in feature space. In addition, we testour method on a very challenging dataset: SETS on large-scale non-stationarysemi-supervised task distributions consisting of (at least) 72K tasks. Withextensive experiments, we demonstrate the proposed ORDER alleviates forgettingon evolving task distributions and is more robust to OOD data than relatedstrong baselines.</div></details><blockquote><p><strong><em>2022-09-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.00939v1><strong>An Introduction to Machine Unlearning</strong></a></p><p><em>Salvatore Mercuri, Raad Khraishi, Ramin Okhrati, Devesh Batra, Conor Hamill, Taha Ghasempour, Andrew Nowlan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Removing the influence of a specified subset of training data from a machinelearning model may be required to address issues such as privacy, fairness, anddata quality. Retraining the model from scratch on the remaining data afterremoval of the subset is an effective but often infeasible option, due to itscomputational expense. The past few years have therefore seen several novelapproaches towards efficient removal, forming the field of &ldquo;machineunlearning&rdquo;, however, many aspects of the literature published thus far aredisparate and lack consensus. In this paper, we summarise and compare sevenstate-of-the-art machine unlearning algorithms, consolidate definitions of coreconcepts used in the field, reconcile different approaches for evaluatingalgorithms, and discuss issues related to applying machine unlearning inpractice.</div></details><blockquote><p><strong><em>2022-08-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.14307v1><strong>Beyond Supervised Continual Learning: a Review</strong></a></p><p><em>Benedikt Bagus, Alexander Gepperth, Timothée Lesort</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Learning (CL, sometimes also termed incremental learning) is aflavor of machine learning where the usual assumption of stationary datadistribution is relaxed or omitted. When naively applying, e.g., DNNs in CLproblems, changes in the data distribution can cause the so-called catastrophicforgetting (CF) effect: an abrupt loss of previous knowledge. Although manysignificant contributions to enabling CL have been made in recent years, mostworks address supervised (classification) problems. This article reviewsliterature that study CL in other settings, such as learning with reducedsupervision, fully unsupervised learning, and reinforcement learning. Besidesproposing a simple schema for classifying CL approaches w.r.t. their level ofautonomy and supervision, we discuss the specific challenges associated witheach setting and the potential contributions to the field of CL in general.</div></details><blockquote><p><strong><em>2022-08-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.06875v1><strong>Forgetting Fast in Recommender Systems</strong></a></p><p><em>Wenyan Liu, Juncheng Wan, Xiaoling Wang, Weinan Zhang, Dell Zhang, Hang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Users of a recommender system may want part of their data being deleted, notonly from the data repository but also from the underlying machine learningmodel, for privacy or utility reasons. Such right-to-be-forgotten requestscould be fulfilled by simply retraining the recommendation model from scratch,but that would be too slow and too expensive in practice. In this paper, weinvestigate fast machine unlearning techniques for recommender systems that canremove the effect of a small amount of training data from the recommendationmodel without incurring the full cost of retraining. A natural idea to speedthis process up is to fine-tune the current recommendation model on theremaining training data instead of starting from a random initialization. Thiswarm-start strategy indeed works for neural recommendation models usingstandard 1st-order neural network optimizers (like AdamW). However, we havefound that even greater acceleration could be achieved by employing 2nd-order(Newton or quasi-Newton) optimization methods instead. To overcome theprohibitively high computational cost of 2nd-order optimizers, we propose a newrecommendation unlearning approach AltEraser which divides the optimizationproblem of unlearning into many small tractable sub-problems. Extensiveexperiments on three real-world recommendation datasets show promising resultsof AltEraser in terms of consistency (forgetting thoroughness), accuracy(recommendation effectiveness), and efficiency (unlearning speed). To ourknowledge, this work represents the first attempt at fast approximate machineunlearning for state-of-the-art neural recommendation models.</div></details><p><a href=http://arxiv.org/abs/1905.10696v4><strong>Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting</strong></a></p><p><em>Alexander Ororbia, Ankur Mali, Daniel Kifer, C. Lee Giles</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In lifelong learning systems based on artificial neural networks, one of thebiggest obstacles is the inability to retain old knowledge as new informationis encountered. This phenomenon is known as catastrophic forgetting. In thispaper, we propose a new kind of connectionist architecture, the SequentialNeural Coding Network, that is robust to forgetting when learning from streamsof data points and, unlike networks of today, does not learn via the popularback-propagation of errors. Grounded in the neurocognitive theory of predictiveprocessing, our model adapts synapses in a biologically-plausible fashion whileanother neural system learns to direct and control this cortex-like structure,mimicking some of the task-executive control functionality of the basalganglia. In our experiments, we demonstrate that our self-organizing systemexperiences significantly less forgetting compared to standard neural models,outperforming a swath of previously proposed methods, including rehearsal/databuffer-based methods, on both standard (SplitMNIST, Split Fashion MNIST, etc.)and custom benchmarks even though it is trained in a stream-like fashion. Ourwork offers evidence that emulating mechanisms in real neuronal systems, e.g.,local learning, lateral competition, can yield new directions and possibilitiesfor tackling the grand challenge of lifelong machine learning.</div></details><blockquote><p><strong><em>2022-08-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.05217v1><strong>Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation</strong></a></p><p><em>Zhijing Wu, Hua Xu, Jingliang Fang, Kai Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Machine Reading Comprehension aims to incrementally learn from acontinuous data stream across time without access the previous seen data, whichis crucial for the development of real-world MRC systems. However, it is agreat challenge to learn a new domain incrementally without catastrophicallyforgetting previous knowledge. In this paper, MA-MRC, a continual MRC modelwith uncertainty-aware fixed Memory and Adversarial domain adaptation, isproposed. In MA-MRC, a fixed size memory stores a small number of samples inprevious domain data along with an uncertainty-aware updating strategy when newdomain data arrives. For incremental learning, MA-MRC not only keeps a stableunderstanding by learning both memory and new domain data, but also makes fulluse of the domain adaptation relationship between them by adversarial learningstrategy. The experimental results show that MA-MRC is superior to strongbaselines and has a substantial incremental learning ability withoutcatastrophically forgetting under two different continual MRC settings.</div></details><blockquote><p><strong><em>2022-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.01004v3><strong>Explaining How Deep Neural Networks Forget by Deep Visualization</strong></a></p><p><em>Giang Nguyen, Shuan Chen, Tae Joon Jun, Daeyoung Kim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Explaining the behaviors of deep neural networks, usually considered as blackboxes, is critical especially when they are now being adopted over diverseaspects of human life. Taking the advantages of interpretable machine learning(interpretable ML), this paper proposes a novel tool called CatastrophicForgetting Dissector (or CFD) to explain catastrophic forgetting in continuallearning settings. We also introduce a new method called Critical Freezingbased on the observations of our tool. Experiments on ResNet articulate howcatastrophic forgetting happens, particularly showing which components of thisfamous network are forgetting. Our new continual learning algorithm defeatsvarious recent techniques by a significant margin, proving the capability ofthe investigation. Critical freezing not only attacks catastrophic forgettingbut also exposes explainability.</div></details><blockquote><p><strong><em>2022-07-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.10659v1><strong>Novel Class Discovery without Forgetting</strong></a></p><p><em>K J Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, Vineeth N Balasubramanian</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans possess an innate ability to identify and differentiate instances thatthey are not familiar with, by leveraging and adapting the knowledge that theyhave acquired so far. Importantly, they achieve this without deteriorating theperformance on their earlier learning. Inspired by this, we identify andformulate a new, pragmatic problem setting of NCDwF: Novel Class Discoverywithout Forgetting, which tasks a machine learning model to incrementallydiscover novel categories of instances from unlabeled data, while maintainingits performance on the previously seen categories. We propose 1) a method togenerate pseudo-latent representations which act as a proxy for (no longeravailable) labeled data, thereby alleviating forgetting, 2) amutual-information based regularizer which enhances unsupervised discovery ofnovel classes, and 3) a simple Known Class Identifier which aids generalizedinference when the testing data contains instances form both seen and unseencategories. We introduce experimental protocols based on CIFAR-10, CIFAR-100and ImageNet-1000 to measure the trade-off between knowledge retention andnovel class discovery. Our extensive evaluations reveal that existing modelscatastrophically forget previously seen categories while identifying novelcategories, while our method is able to effectively balance between thecompeting objectives. We hope our work will attract further research into thisnewly identified pragmatic problem setting.</div></details><blockquote><p><strong><em>2022-07-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.08180v1><strong>Federated Learning and catastrophic forgetting in pervasive computing: demonstration in HAR domain</strong></a></p><p><em>Anastasiia Usmanova, François Portet, Philippe Lalanda, German Vega</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning has been introduced as a new machine learning paradigmenhancing the use of local devices. At a server level, FL regularly aggregatesmodels learned locally on distributed clients to obtain a more general model.In this way, no private data is sent over the network, and the communicationcost is reduced. However, current solutions rely on the availability of largeamounts of stored data at the client side in order to fine-tune the models sentby the server. Such setting is not realistic in mobile pervasive computingwhere data storage must be kept low and data characteristic (distribution) canchange dramatically. To account for this variability, a solution is to use thedata regularly collected by the client to progressively adapt the receivedmodel. But such naive approach exposes clients to the well-known problem ofcatastrophic forgetting. The purpose of this paper is to demonstrate thisproblem in the mobile human activity recognition context on smartphones.</div></details><p><a href=http://arxiv.org/abs/2207.08181v1><strong>Federated Continual Learning through distillation in pervasive computing</strong></a></p><p><em>Anastasiia Usmanova, François Portet, Philippe Lalanda, German Vega</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning has been introduced as a new machine learning paradigmenhancing the use of local devices. At a server level, FL regularly aggregatesmodels learned locally on distributed clients to obtain a more general model.Current solutions rely on the availability of large amounts of stored data atthe client side in order to fine-tune the models sent by the server. Suchsetting is not realistic in mobile pervasive computing where data storage mustbe kept low and data characteristic can change dramatically. To account forthis variability, a solution is to use the data regularly collected by theclient to progressively adapt the received model. But such naive approachexposes clients to the well-known problem of catastrophic forgetting. Toaddress this problem, we have defined a Federated Continual Learning approachwhich is mainly based on distillation. Our approach allows a better use ofresources, eliminating the need to retrain from scratch at the arrival of newdata and reducing memory usage by limiting the amount of data to be stored.This proposal has been evaluated in the Human Activity Recognition (HAR) domainand has shown to effectively reduce the catastrophic forgetting effect.</div></details><blockquote><p><strong><em>2022-07-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.10681v3><strong>Online Continual Learning for Embedded Devices</strong></a></p><p><em>Tyler L. Hayes, Christopher Kanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Real-time on-device continual learning is needed for new applications such ashome robots, user personalization on smartphones, and augmented/virtual realityheadsets. However, this setting poses unique challenges: embedded devices havelimited memory and compute capacity and conventional machine learning modelssuffer from catastrophic forgetting when updated on non-stationary datastreams. While several online continual learning models have been developed,their effectiveness for embedded applications has not been rigorously studied.In this paper, we first identify criteria that online continual learners mustmeet to effectively perform real-time, on-device learning. We then study theefficacy of several online continual learning methods when used with mobileneural networks. We measure their performance, memory usage, computerequirements, and ability to generalize to out-of-domain inputs.</div></details><blockquote><p><strong><em>2022-07-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.07655v2><strong>Deep Unlearning via Randomized Conditionally Independent Hessians</strong></a></p><p><em>Ronak Mehta, Sourav Pal, Vikas Singh, Sathya N. Ravi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent legislation has led to interest in machine unlearning, i.e., removingspecific training samples from a predictive model as if they never existed inthe training dataset. Unlearning may also be required due tocorrupted/adversarial data or simply a user&rsquo;s updated privacy requirement. Formodels which require no training (k-NN), simply deleting the closest originalsample can be effective. But this idea is inapplicable to models which learnricher representations. Recent ideas leveraging optimization-based updatesscale poorly with the model dimension d, due to inverting the Hessian of theloss function. We use a variant of a new conditional independence coefficient,L-CODEC, to identify a subset of the model parameters with the most semanticoverlap on an individual sample level. Our approach completely avoids the needto invert a (possibly) huge matrix. By utilizing a Markov blanket selection, wepremise that L-CODEC is also suitable for deep unlearning, as well as otherapplications in vision. Compared to alternatives, L-CODEC makes approximateunlearning possible in settings that would otherwise be infeasible, includingvision models used for face recognition, person re-identification and NLPmodels that may require unlearning samples identified for exclusion. Code canbe found at <a href=https://github.com/vsingh-group/LCODEC-deep-unlearning/>https://github.com/vsingh-group/LCODEC-deep-unlearning/</a></div></details><blockquote><p><strong><em>2022-07-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.06233v1><strong>Continual Learning with Deep Learning Methods in an Application-Oriented Context</strong></a></p><p><em>Benedikt Pfülb</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Abstract knowledge is deeply grounded in many computer-based applications. Animportant research area of Artificial Intelligence (AI) deals with theautomatic derivation of knowledge from data. Machine learning offers theaccording algorithms. One area of research focuses on the development ofbiologically inspired learning algorithms. The respective machine learningmethods are based on neurological concepts so that they can systematicallyderive knowledge from data and store it. One type of machine learningalgorithms that can be categorized as &ldquo;deep learning&rdquo; model is referred to asDeep Neural Networks (DNNs). DNNs consist of multiple artificial neuronsarranged in layers that are trained by using the backpropagation algorithm.These deep learning methods exhibit amazing capabilities for inferring andstoring complex knowledge from high-dimensional data. However, DNNs areaffected by a problem that prevents new knowledge from being added to anexisting base. The ability to continuously accumulate knowledge is an importantfactor that contributed to evolution and is therefore a prerequisite for thedevelopment of strong AIs. The so-called &ldquo;catastrophic forgetting&rdquo; (CF) effectcauses DNNs to immediately loose already derived knowledge after a few trainingiterations on a new data distribution. Only an energetically expensiveretraining with the joint data distribution of past and new data enables theabstraction of the entire new set of knowledge. In order to counteract theeffect, various techniques have been and are still being developed with thegoal to mitigate or even solve the CF problem. These published CF avoidancestudies usually imply the effectiveness of their approaches for variouscontinual learning tasks. This dissertation is set in the context of continualmachine learning with deep learning methods. The first part deals with thedevelopment of an &mldr;</div></details><blockquote><p><strong><em>2022-06-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.14085v1><strong>Continual Learning with Transformers for Image Classification</strong></a></p><p><em>Beyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, Cedric Archambeau</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In many real-world scenarios, data to train machine learning models becomeavailable over time. However, neural network models struggle to continuallylearn new concepts without forgetting what has been learnt in the past. Thisphenomenon is known as catastrophic forgetting and it is often difficult toprevent due to practical constraints, such as the amount of data that can bestored or the limited computation sources that can be used. Moreover, traininglarge neural networks, such as Transformers, from scratch is very costly andrequires a vast amount of training data, which might not be available in theapplication domain of interest. A recent trend indicates that dynamicarchitectures based on an expansion of the parameters can reduce catastrophicforgetting efficiently in continual learning, but this needs complex tuning tobalance the growing number of parameters and barely share any informationacross tasks. As a result, they struggle to scale to a large number of taskswithout significant overhead. In this paper, we validate in the computer visiondomain a recent solution called Adaptive Distillation of Adapters (ADA), whichis developed to perform continual learning using pre-trained Transformers andAdapters on text classification tasks. We empirically demonstrate on differentclassification tasks that this method maintains a good predictive performancewithout retraining the model or increasing the number of model parameters overthe time. Besides it is significantly faster at inference time compared to thestate-of-the-art methods.</div></details><p><a href=http://arxiv.org/abs/2206.14048v1><strong>Short-Term Plasticity Neurons Learning to Learn and Forget</strong></a></p><p><em>Hector Garcia Rodriguez, Qinghai Guo, Timoleon Moraitis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Short-term plasticity (STP) is a mechanism that stores decaying memories insynapses of the cerebral cortex. In computing practice, STP has been used, butmostly in the niche of spiking neurons, even though theory predicts that it isthe optimal solution to certain dynamic tasks. Here we present a new type ofrecurrent neural unit, the STP Neuron (STPN), which indeed turns out strikinglypowerful. Its key mechanism is that synapses have a state, propagated throughtime by a self-recurrent connection-within-the-synapse. This formulationenables training the plasticity with backpropagation through time, resulting ina form of learning to learn and forget in the short term. The STPN outperformsall tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, anddifferentiable plasticity. We confirm this in both supervised and reinforcementlearning (RL), and in tasks such as Associative Retrieval, Maze Exploration,Atari video games, and MuJoCo robotics. Moreover, we calculate that, inneuromorphic or biological circuits, the STPN minimizes energy consumptionacross models, as it depresses individual synapses dynamically. Based on these,biological STP may have been a strong evolutionary attractor that maximizesboth efficiency and computational power. The STPN now brings these neuromorphicadvantages also to a broad spectrum of machine learning practice. Code isavailable at <a href=https://github.com/NeuromorphicComputing/stpn>https://github.com/NeuromorphicComputing/stpn</a></div></details><blockquote><p><strong><em>2022-06-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.13336v1><strong>Continual Learning of Dynamical Systems with Competitive Federated Reservoir Computing</strong></a></p><p><em>Leonard Bereska, Efstratios Gavves</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning recently proved efficient in learning differential equationsand dynamical systems from data. However, the data is commonly assumed tooriginate from a single never-changing system. In contrast, when modelingreal-world dynamical processes, the data distribution often shifts due tochanges in the underlying system dynamics. Continual learning of theseprocesses aims to rapidly adapt to abrupt system changes without forgettingprevious dynamical regimes. This work proposes an approach to continuallearning based on reservoir computing, a state-of-the-art method for trainingrecurrent neural networks on complex spatiotemporal dynamical systems.Reservoir computing fixes the recurrent network weights - hence these cannot beforgotten - and only updates linear projection heads to the output. We proposeto train multiple competitive prediction heads concurrently. Inspired byneuroscience&rsquo;s predictive coding, only the most predictive heads activate,laterally inhibiting and thus protecting the inactive heads from forgettinginduced by interfering parameter updates. We show that this multi-headreservoir minimizes interference and catastrophic forgetting on severaldynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenzattractor, and the high-dimensional Lorenz-96 weather model. Our resultssuggest that reservoir computing is a promising candidate framework for thecontinual learning of dynamical systems. We provide our code for datageneration, method, and comparisons at\url{https://github.com/leonardbereska/multiheadreservoir}.</div></details><p><a href=http://arxiv.org/abs/2206.13399v1><strong>Transfer Learning via Test-Time Neural Networks Aggregation</strong></a></p><p><em>Bruno Casella, Alessio Barbaro Chisari, Sebastiano Battiato, Mario Valerio Giuffrida</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: It has been demonstrated that deep neural networks outperform traditionalmachine learning. However, deep networks lack generalisability, that is, theywill not perform as good as in a new (testing) set drawn from a differentdistribution due to the domain shift. In order to tackle this known issue,several transfer learning approaches have been proposed, where the knowledge ofa trained model is transferred into another to improve performance withdifferent data. However, most of these approaches require additional trainingsteps, or they suffer from catastrophic forgetting that occurs when a trainedmodel has overwritten previously learnt knowledge. We address both problemswith a novel transfer learning approach that uses network aggregation. We traindataset-specific networks together with an aggregation network in a unifiedframework. The loss function includes two main components: a task-specific loss(such as cross-entropy) and an aggregation loss. The proposed aggregation lossallows our model to learn how trained deep network parameters can be aggregatedwith an aggregation operator. We demonstrate that the proposed approach learnsmodel aggregation at test time without any further training step, reducing theburden of transfer learning to a simple arithmetical operation. The proposedapproach achieves comparable performance w.r.t. the baseline. Besides, if theaggregation operator has an inverse, we will show that our model alsoinherently allows for selective forgetting, i.e., the aggregated model canforget one of the datasets it was trained on, retaining information on theothers.</div></details><blockquote><p><strong><em>2022-06-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.12016v1><strong>Unsupervised Learning Algorithms for Keyword Extraction in an Undergraduate Thesis</strong></a></p><p><em>Fred Torres-Cruz, Edelfre Flores, William E. Arcaya, Irenio L. Chagua, Marga I. Ingaluque</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The amount of data managed in many academic institutions has increased inrecent years, particularly in all the research work done by undergraduatestudents, who simply use empirical techniques for keyword selection, forgettingexisting technical methods to assist their students in this process.Information and communication technologies, such as the platform for integratedresearch and academic work with responsibility (PILAR), which recordsinformation about research projects, such as titles, summaries, and keywords intheir various modalities, have gained relevance and importance in themanagement of these. We proved algorithms with these records of researchprojects that have been analysed in this study, and predictions were made foreach of the nine (09) models of unsupervised machine learning algorithms thatwere implemented for each of the 7430 records from the dataset. The mostefficient way of extracting keywords for this dataset was the TF-IDF method,obtaining 72% accuracy and [0.4786, SD 0.0501] in average extraction time foreach thesis file processed by this model.</div></details><blockquote><p><strong><em>2022-06-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.10469v2><strong>The Privacy Onion Effect: Memorization is Relative</strong></a></p><p><em>Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, Florian Tramer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models trained on private datasets have been shown to leaktheir private data. While recent work has found that the average data point israrely leaked, the outlier samples are frequently subject to memorization and,consequently, privacy leakage. We demonstrate and analyse an Onion Effect ofmemorization: removing the &ldquo;layer&rdquo; of outlier points that are most vulnerableto a privacy attack exposes a new layer of previously-safe points to the sameattack. We perform several experiments to study this effect, and understand whyit occurs. The existence of this effect has various consequences. For example,it suggests that proposals to defend against memorization without training withrigorous privacy guarantees are unlikely to be effective. Further, it suggeststhat privacy-enhancing technologies such as machine unlearning could actuallyharm the privacy of other users.</div></details><blockquote><p><strong><em>2022-06-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1908.05672v5><strong>Towards Making the Most of BERT in Neural Machine Translation</strong></a></p><p><em>Jiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Yong Yu, Weinan Zhang, Lei Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: GPT-2 and BERT demonstrate the effectiveness of using pre-trained languagemodels (LMs) on various natural language processing tasks. However, LMfine-tuning often suffers from catastrophic forgetting when applied toresource-rich tasks. In this work, we introduce a concerted training framework(CTNMT) that is the key to integrate the pre-trained LMs to neural machinetranslation (NMT). Our proposed CTNMT consists of three techniques: a)asymptotic distillation to ensure that the NMT model can retain the previouspre-trained knowledge; b) a dynamic switching gate to avoid catastrophicforgetting of pre-trained knowledge; and c) a strategy to adjust the learningpaces according to a scheduled policy. Our experiments in machine translationshow CTNMT gains of up to 3 BLEU score on the WMT14 English-German languagepair which even surpasses the previous state-of-the-art pre-training aided NMTby 1.4 BLEU score. While for the large WMT14 English-French task with 40millions of sentence-pairs, our base model still significantly improves uponthe state-of-the-art Transformer big model by more than 1 BLEU score. The codeand model can be downloaded from <a href=https://github.com/bytedance/neurst/tree/master/examples/ctnmt>https://github.com/bytedance/neurst/tree/master/examples/ctnmt</a>.</div></details><p><a href=http://arxiv.org/abs/2112.00791v2><strong>Controlling Conditional Language Models without Catastrophic Forgetting</strong></a></p><p><em>Tomasz Korbak, Hady Elsahar, German Kruszewski, Marc Dymetman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning is shifting towards general-purpose pretrained generativemodels, trained in a self-supervised manner on large amounts of data, which canthen be applied to solve a large number of tasks. However, due to their generictraining methodology, these models often fail to meet some of the downstreamrequirements (e.g., hallucinations in abstractive summarization or styleviolations in code generation). This raises the important question of how toadapt pre-trained generative models to meet all requirements without destroyingtheir general capabilities (&ldquo;catastrophic forgetting&rdquo;). Recent work hasproposed to solve this problem by representing task-specific requirementsthrough energy-based models (EBMs) and approximating these EBMs usingdistributional policy gradients (DPG). Despite its effectiveness, this approachis however limited to unconditional distributions. In this paper, we extend DPGto conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG onfour different control objectives across three tasks (translation,summarization and code generation) and two pretrained models (T5 and GPT-Neo).Our results show that fine-tuning using CDPG robustly moves these pretrainedmodels closer towards meeting control objectives and &ndash; in contrast withbaseline approaches &ndash; does not result in catastrophic forgetting.</div></details><blockquote><p><strong><em>2022-06-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.06904v2><strong>Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks</strong></a></p><p><em>Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In adversarial machine learning, new defenses against attacks on deeplearning systems are routinely broken soon after their release by more powerfulattacks. In this context, forensic tools can offer a valuable complement toexisting defenses, by tracing back a successful attack to its root cause, andoffering a path forward for mitigation to prevent similar attacks in thefuture. In this paper, we describe our efforts in developing a forensic tracebacktool for poison attacks on deep neural networks. We propose a novel iterativeclustering and pruning solution that trims &ldquo;innocent&rdquo; training samples, untilall that remains is the set of poisoned data responsible for the attack. Ourmethod clusters training samples based on their impact on model parameters,then uses an efficient data unlearning method to prune innocent clusters. Weempirically demonstrate the efficacy of our system on three types ofdirty-label (backdoor) poison attacks and three types of clean-label poisonattacks, across domains of computer vision and malware classification. Oursystem achieves over 98.4% precision and 96.8% recall across all attacks. Wealso show that our system is robust against four anti-forensics measuresspecifically designed to attack it.</div></details><blockquote><p><strong><em>2022-06-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.04823v1><strong>Membership Inference via Backdooring</strong></a></p><p><em>Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently issued data privacy regulations like GDPR (General Data ProtectionRegulation) grant individuals the right to be forgotten. In the context ofmachine learning, this requires a model to forget about a training data sampleif requested by the data owner (i.e., machine unlearning). As an essential stepprior to machine unlearning, it is still a challenge for a data owner to tellwhether or not her data have been used by an unauthorized party to train amachine learning model. Membership inference is a recently emerging techniqueto identify whether a data sample was used to train a target model, and seemsto be a promising solution to this challenge. However, straightforward adoptionof existing membership inference approaches fails to address the challengeeffectively due to being originally designed for attacking membership privacyand suffering from several severe limitations such as low inference accuracy onwell-generalized models. In this paper, we propose a novel membership inferenceapproach inspired by the backdoor technology to address the said challenge.Specifically, our approach of Membership Inference via Backdooring (MIB)leverages the key observation that a backdoored model behaves very differentlyfrom a clean model when predicting on deliberately marked samples created by adata owner. Appealingly, MIB requires data owners&rsquo; marking a small number ofsamples for membership inference and only black-box access to the target model,with theoretical guarantees for inference results. We perform extensiveexperiments on various datasets and deep neural network architectures, and theresults validate the efficacy of our approach, e.g., marking only 0.1% of thetraining dataset is practically sufficient for effective membership inference.</div></details><blockquote><p><strong><em>2022-06-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.14032v2><strong>Quantum continual learning of quantum data realizing knowledge backward transfer</strong></a></p><p><em>Haozhen Situ, Tianxiang Lu, Minghua Pan, Lvzhou Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: For the goal of strong artificial intelligence that can mimic human-levelintelligence, AI systems would have the ability to adapt to ever-changingscenarios and learn new knowledge continuously without forgetting previouslyacquired knowledge. When a machine learning model is consecutively trained onmultiple tasks that come in sequence, its performance on previously learnedtasks may drop dramatically during the learning process of the newly seen task.To avoid this phenomenon termed catastrophic forgetting, continual learning,also known as lifelong learning, has been proposed and become one of the mostup-to-date research areas of machine learning. As quantum machine learningblossoms in recent years, it is interesting to develop quantum continuallearning. This paper focuses on the case of quantum models for quantum datawhere the computation model and the data to be processed are both quantum. Thegradient episodic memory method is incorporated to design a quantum continuallearning scheme that overcomes catastrophic forgetting and realizes knowledgebackward transfer. Specifically, a sequence of quantum state classificationtasks is continually learned by a variational quantum classifier whoseparameters are optimized by a classical gradient-based optimizer. The gradientof the current task is projected to the closest gradient, avoiding the increaseof the loss at previous tasks, but allowing the decrease. Numerical simulationresults show that our scheme not only overcomes catastrophic forgetting, butalso realize knowledge backward transfer, which means the classifier&rsquo;sperformance on previous tasks is enhanced rather than compromised whilelearning a new task.</div></details><blockquote><p><strong><em>2022-05-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.13066v1><strong>Semi-supervised Drifted Stream Learning with Short Lookback</strong></a></p><p><em>Weijieying Ren, Pengyang Wang, Xiaolin Li, Charles E. Hughes, Yanjie Fu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In many scenarios, 1) data streams are generated in real time; 2) labeleddata are expensive and only limited labels are available in the beginning; 3)real-world data is not always i.i.d. and data drift over time gradually; 4) thestorage of historical streams is limited and model updating can only beachieved based on a very short lookback window. This learning setting limitsthe applicability and availability of many Machine Learning (ML) algorithms. Wegeneralize the learning task under such setting as a semi-supervised driftedstream learning with short lookback problem (SDSL). SDSL imposes twounder-addressed challenges on existing methods in semi-supervised learning,continuous learning, and domain adaptation: 1) robust pseudo-labeling undergradual shifts and 2) anti-forgetting adaptation with short lookback. To tacklethese challenges, we propose a principled and generic generation-replayframework to solve SDSL. The framework is able to accomplish: 1) robustpseudo-labeling in the generation step; 2) anti-forgetting adaption in thereplay step. To achieve robust pseudo-labeling, we develop a novel pseudo-labelclassification model to leverage supervised knowledge of previously labeleddata, unsupervised knowledge of new data, and, structure knowledge of invariantlabel semantics. To achieve adaptive anti-forgetting model replay, we proposeto view the anti-forgetting adaptation task as a flat region search problem. Wepropose a novel minimax game-based replay objective function to solve the flatregion search problem and develop an effective optimization solver. Finally, wepresent extensive experiments to demonstrate our framework can effectivelyaddress the task of anti-forgetting learning in drifted streams with shortlookback.</div></details><p><a href=http://arxiv.org/abs/2205.10937v2><strong>muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems</strong></a></p><p><em>Andrea Gesmundo, Jeff Dean</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Most uses of machine learning today involve training a model from scratch fora particular task, or sometimes starting with a model pretrained on a relatedtask and then fine-tuning on a downstream task. Both approaches offer limitedknowledge transfer between different tasks, time-consuming human-drivencustomization to individual tasks and high computational costs especially whenstarting from randomly initialized models. We propose a method that uses thelayers of a pretrained deep neural network as building blocks to construct anML system that can jointly solve an arbitrary number of tasks. The resultingsystem can leverage cross tasks knowledge transfer, while being immune fromcommon drawbacks of multitask approaches such as catastrophic forgetting,gradients interference and negative transfer. We define an evolutionaryapproach designed to jointly select the prior knowledge relevant for each task,choose the subset of the model parameters to train and dynamically auto-tuneits hyperparameters. Furthermore, a novel scale control method is employed toachieve quality/size trade-offs that outperform common fine-tuning techniques.Compared with standard fine-tuning on a benchmark of 10 diverse imageclassification tasks, the proposed model improves the average accuracy by 2.39%while using 47% less parameters per task.</div></details><blockquote><p><strong><em>2022-05-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.08013v2><strong>Continual learning on 3D point clouds with random compressed rehearsal</strong></a></p><p><em>Maciej Zamorski, Michał Stypułkowski, Konrad Karanowski, Tomasz Trzciński, Maciej Zięba</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Contemporary deep neural networks offer state-of-the-art results when appliedto visual reasoning, e.g., in the context of 3D point cloud data. Point cloudsare important datatype for precise modeling of three-dimensional environments,but effective processing of this type of data proves to be challenging. In theworld of large, heavily-parameterized network architectures andcontinuously-streamed data, there is an increasing need for machine learningmodels that can be trained on additional data. Unfortunately, currentlyavailable models cannot fully leverage training on additional data withoutlosing their past knowledge. Combating this phenomenon, called catastrophicforgetting, is one of the main objectives of continual learning. Continuallearning for deep neural networks has been an active field of research,primarily in 2D computer vision, natural language processing, reinforcementlearning, and robotics. However, in 3D computer vision, there are hardly anycontinual learning solutions specifically designed to take advantage of pointcloud structure. This work proposes a novel neural network architecture capableof continual learning on 3D point cloud data. We utilize point cloud structureproperties for preserving a heavily compressed set of past data. By usingrehearsal and reconstruction as regularization methods of the learning process,our approach achieves a significant decrease of catastrophic forgettingcompared to the existing solutions on several most popular point cloud datasetsconsidering two continual learning settings: when a task is known beforehand,and in the challenging scenario of when task information is unknown to themodel.</div></details><blockquote><p><strong><em>2022-05-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.09357v1><strong>Continual Pre-Training Mitigates Forgetting in Language and Vision</strong></a></p><p><em>Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, Davide Bacciu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained models are nowadays a fundamental component of machine learningresearch. In continual learning, they are commonly used to initialize the modelbefore training on the stream of non-stationary data. However, pre-training israrely applied during continual learning. We formalize and investigate thecharacteristics of the continual pre-training scenario in both language andvision environments, where a model is continually pre-trained on a stream ofincoming data and only later fine-tuned to different downstream tasks. We showthat continually pre-trained models are robust against catastrophic forgettingand we provide strong empirical evidence supporting the fact thatself-supervised pre-training is more effective in retaining previous knowledgethan supervised protocols. Code is provided athttps://github.com/AndreaCossu/continual-pretraining-nlp-vision .</div></details><p><a href=http://arxiv.org/abs/2205.09875v1><strong>Incremental Learning with Differentiable Architecture and Forgetting Search</strong></a></p><p><em>James Seale Smith, Zachary Seymour, Han-Pang Chiu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As progress is made on training machine learning models on incrementallyexpanding classification tasks (i.e., incremental learning), a next step is totranslate this progress to industry expectations. One technique missing fromincremental learning is automatic architecture design via Neural ArchitectureSearch (NAS). In this paper, we show that leveraging NAS for incrementallearning results in strong performance gains for classification tasks.Specifically, we contribute the following: first, we create a strong baselineapproach for incremental learning based on Differentiable Architecture Search(DARTS) and state-of-the-art incremental learning strategies, outperformingmany existing strategies trained with similar-sized popular architectures;second, we extend the idea of architecture search to regularize architectureforgetting, boosting performance past our proposed baseline. We evaluate ourmethod on both RF signal and image classification tasks, and demonstrate we canachieve up to a 10% performance increase over state-of-the-art methods. Mostimportantly, our contribution enables learning from continuous distributions onreal-world application data for which the complexity of the data distributionis unknown, or the modality less explored (such as RF signal classification).</div></details><blockquote><p><strong><em>2022-05-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.06369v1><strong>How to Combine Membership-Inference Attacks on Multiple Updated Models</strong></a></p><p><em>Matthew Jagielski, Stanley Wu, Alina Oprea, Jonathan Ullman, Roxana Geambasu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A large body of research has shown that machine learning models arevulnerable to membership inference (MI) attacks that violate the privacy of theparticipants in the training data. Most MI research focuses on the case of asingle standalone model, while production machine-learning platforms oftenupdate models over time, on data that often shifts in distribution, giving theattacker more information. This paper proposes new attacks that take advantageof one or more model updates to improve MI. A key part of our approach is toleverage rich information from standalone MI attacks mounted separately againstthe original and updated models, and to combine this information in specificways to improve attack effectiveness. We propose a set of combination functionsand tuning methods for each, and present both analytical and quantitativejustification for various options. Our results on four public datasets showthat our attacks are effective at using update information to give theadversary a significant advantage over attacks on standalone models, but alsocompared to a prior MI attack that takes advantage of model updates in arelated machine-unlearning setting. We perform the first measurements of theimpact of distribution shift on MI attacks with model updates, and show that amore drastic distribution shift results in significantly higher MI risk than agradual shift. Our code is available athttps://www.github.com/stanleykywu/model-updates.</div></details><blockquote><p><strong><em>2022-04-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.14106v4><strong>Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning</strong></a></p><p><em>Francesco Pelosin, Andrea Torsello</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The design of machines and algorithms capable of learning in a dynamicallychanging environment has become an increasingly topical problem with theincrease of the size and heterogeneity of data available to learning systems.As a consequence, the key issue of Continual Learning has become that ofaddressing the stability-plasticity dilemma of connectionist systems, as theyneed to adapt their model without forgetting previously acquired knowledge.Within this context, rehearsal-based methods i.e., solutions in where thelearner exploits memory to revisit past data, has proven to be very effective,leading to performance at the state-of-the-art. In our study, we propose ananalysis of the memory quantity/quality trade-off adopting various datareduction approaches to increase the number of instances storable in memory. Inparticular, we investigate complex instance compression techniques such as deepencoders, but also trivial approaches such as image resizing and lineardimensionality reduction. Our findings suggest that the optimal trade-off isseverely skewed toward instance quantity, where rehearsal approaches withseveral heavily compressed instances easily outperform state-of-the-artapproaches with the same amount of memory at their disposal. Further, in highmemory configurations, deep approaches extracting spatial structure combinedwith extreme resizing (of the order of $8\times8$ images) yield the bestresults, while in memory-constrained configurations where deep approachescannot be used due to their memory requirement in training, Extreme LearningMachines (ELM) offer a clear advantage.</div></details><blockquote><p><strong><em>2022-04-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.13691v1><strong>Foundations for learning from noisy quantum experiments</strong></a></p><p><em>Hsin-Yuan Huang, Steven T. Flammia, John Preskill</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Understanding what can be learned from experiments is central to scientificprogress. In this work, we use a learning-theoretic perspective to study thetask of learning physical operations in a quantum machine when all operations(state preparation, dynamics, and measurement) are a priori unknown. We provethat, without any prior knowledge, if one can explore the full quantum statespace by composing the operations, then every operation can be learned. Whenone cannot explore the full state space but all operations are approximatelyknown and noise in Clifford gates is gate-independent, we find an efficientalgorithm for learning all operations up to a single unlearnable parametercharacterizing the fidelity of the initial state. For learning a noise channelon Clifford gates to a fixed accuracy, our algorithm uses quadratically fewerexperiments than previously known protocols. Under more general conditions, thetrue description of the noise can be unlearnable; for example, we prove that nobenchmarking protocol can learn gate-dependent Pauli noise on Clifford+T gateseven under perfect state preparation and measurement. Despite not being able tolearn the noise, we show that a noisy quantum computer that performs entangledmeasurements on multiple copies of an unknown state can yield a large advantagein learning properties of the state compared to a noiseless device thatmeasures individual copies and then processes the measurement data using aclassical computer. Concretely, we prove that noisy quantum computers withtwo-qubit gate error rate $\epsilon$ can achieve a learning task using $N$copies of the state, while $N^{\Omega(1/\epsilon)}$ copies are requiredclassically.</div></details><p><a href=http://arxiv.org/abs/2202.11295v2><strong>Continual learning-based probabilistic slow feature analysis for multimode dynamic process monitoring</strong></a></p><p><em>Jingxin Zhang, Donghua Zhou, Maoyin Chen, Xia Hong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, a novel multimode dynamic process monitoring approach isproposed by extending elastic weight consolidation (EWC) to probabilistic slowfeature analysis (PSFA) in order to extract multimode slow features for onlinemonitoring. EWC was originally introduced in the setting of machine learning ofsequential multi-tasks with the aim of avoiding catastrophic forgetting issue,which equally poses as a major challenge in multimode dynamic processmonitoring. When a new mode arrives, a set of data should be collected so thatthis mode can be identified by PSFA and prior knowledge. Then, a regularizationterm is introduced to prevent new data from significantly interfering with thelearned knowledge, where the parameter importance measures are estimated. Theproposed method is denoted as PSFA-EWC, which is updated continually andcapable of achieving excellent performance for successive modes. Different fromtraditional multimode monitoring algorithms, PSFA-EWC furnishes backward andforward transfer ability. The significant features of previous modes areretained while consolidating new information, which may contribute to learningnew relevant modes. Compared with several known methods, the effectiveness ofthe proposed method is demonstrated via a continuous stirred tank heater and apractical coal pulverizing system.</div></details><blockquote><p><strong><em>2022-04-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.13295v2><strong>Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations</strong></a></p><p><em>Tao Guo, Song Guo, Jiewei Zhang, Wenchao Xu, Junxiao Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, the enactment of privacy regulations has promoted the rise of themachine unlearning paradigm. Existing studies of machine unlearning mainlyfocus on sample-wise unlearning, such that a learnt model will not exposeuser&rsquo;s privacy at the sample level. Yet we argue that such ability of selectiveremoval should also be presented at the attribute level, especially for theattributes irrelevant to the main task, e.g., whether a person recognized in aface recognition system wears glasses or the age range of that person. Througha comprehensive literature review, it is found that existing studies onattribute-related problems like fairness and de-biasing learning cannot addressthe above concerns properly. To bridge this gap, we propose a paradigm ofselectively removing input attributes from feature representations which wename `attribute unlearning&rsquo;. In this paradigm, certain attributes will beaccurately captured and detached from the learned feature representations atthe stage of training, according to their mutual information. The particularattributes will be progressively eliminated along with the training proceduretowards convergence, while the rest of attributes related to the main task arepreserved for achieving competitive model performance. Considering thecomputational complexity during the training process, we not only give atheoretically approximate training method, but also propose an accelerationscheme to speed up the training process. We validate our method by spanningseveral datasets and models and demonstrate that our design can preserve modelfidelity and reach prevailing unlearning efficacy with high efficiency. Theproposed unlearning paradigm builds a foundation for future machine unlearningsystem and will become an essential component of the latest privacy-relatedlegislation.</div></details><blockquote><p><strong><em>2022-04-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.06047v2><strong>Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning</strong></a></p><p><em>Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, Daniel Rubin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning is an emerging research paradigm enabling collaborativetraining of machine learning models among different organizations while keepingdata private at each institution. Despite recent progress, there remainfundamental challenges such as the lack of convergence and the potential forcatastrophic forgetting across real-world heterogeneous devices. In this paper,we demonstrate that self-attention-based architectures (e.g., Transformers) aremore robust to distribution shifts and hence improve federated learning overheterogeneous data. Concretely, we conduct the first rigorous empiricalinvestigation of different neural architectures across a range of federatedalgorithms, real-world benchmarks, and heterogeneous data splits. Ourexperiments show that simply replacing convolutional networks with Transformerscan greatly reduce catastrophic forgetting of previous devices, accelerateconvergence, and reach a better global model, especially when dealing withheterogeneous data. We release our code and pretrained models athttps://github.com/Liangqiong/ViT-FL-main to encourage future exploration inrobust architectures as an alternative to current research efforts on theoptimization front.</div></details><p><a href=http://arxiv.org/abs/2204.06216v1><strong>Sapinet: A sparse event-based spatiotemporal oscillator for learning in the wild</strong></a></p><p><em>Ayon Borthakur</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce Sapinet &ndash; a spike timing (event)-based multilayer neuralnetwork for \textit{learning in the wild} &ndash; that is: one-shot online learningof multiple inputs without catastrophic forgetting, and without the need fordata-specific hyperparameter retuning. Key features of Sapinet include dataregularization, model scaling, data classification, and denoising. The modelalso supports stimulus similarity mapping. We propose a systematic method totune the network for performance. We studied the model performance on differentlevels of odor similarity, gaussian and impulse noise. Sapinet achieved highclassification accuracies on standard machine olfaction datasets without therequirement of fine tuning for a specific dataset.</div></details><blockquote><p><strong><em>2022-03-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.14544v1><strong>Gradient-Matching Coresets for Rehearsal-Based Continual Learning</strong></a></p><p><em>Lukas Balles, Giovanni Zappella, Cédric Archambeau</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The goal of continual learning (CL) is to efficiently update a machinelearning model with new data without forgetting previously-learned knowledge.Most widely-used CL methods rely on a rehearsal memory of data points to bereused while training on new data. Curating such a rehearsal memory to maintaina small, informative subset of all the data seen so far is crucial to thesuccess of these methods. We devise a coreset selection method forrehearsal-based continual learning. Our method is based on the idea of gradientmatching: The gradients induced by the coreset should match, as closely aspossible, those induced by the original training dataset. Inspired by theneural tangent kernel theory, we perform this gradient matching across themodel&rsquo;s initialization distribution, allowing us to extract a coreset withouthaving to train the model first. We evaluate the method on a wide range ofcontinual learning scenarios and demonstrate that it improves the performanceof rehearsal-based CL methods compared to competing memory managementstrategies such as reservoir sampling.</div></details><blockquote><p><strong><em>2022-03-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.14383v1><strong>Continual learning: a feature extraction formalization, an efficient algorithm, and fundamental obstructions</strong></a></p><p><em>Binghui Peng, Andrej Risteski</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning is an emerging paradigm in machine learning, wherein amodel is exposed in an online fashion to data from multiple differentdistributions (i.e. environments), and is expected to adapt to the distributionchange. Precisely, the goal is to perform well in the new environment, whilesimultaneously retaining the performance on the previous environments (i.e.avoid &ldquo;catastrophic forgetting&rdquo;) &ndash; without increasing the size of the model. While this setup has enjoyed a lot of attention in the applied community,there hasn&rsquo;t be theoretical work that even formalizes the desired guarantees.In this paper, we propose a framework for continual learning through theframework of feature extraction &ndash; namely, one in which features, as well as aclassifier, are being trained with each environment. When the features arelinear, we design an efficient gradient-based algorithm $\mathsf{DPGD}$, thatis guaranteed to perform well on the current environment, as well as avoidcatastrophic forgetting. In the general case, when the features are non-linear,we show such an algorithm cannot exist, whether efficient or not.</div></details><blockquote><p><strong><em>2022-03-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.13591v1><strong>Continual Test-Time Domain Adaptation</strong></a></p><p><em>Qin Wang, Olga Fink, Luc Van Gool, Dengxin Dai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Test-time domain adaptation aims to adapt a source pre-trained model to atarget domain without using any source data. Existing works mainly consider thecase where the target domain is static. However, real-world machine perceptionsystems are running in non-stationary and continually changing environmentswhere the target domain distribution can change over time. Existing methods,which are mostly based on self-training and entropy regularization, can sufferfrom these non-stationary environments. Due to the distribution shift over timein the target domain, pseudo-labels become unreliable. The noisy pseudo-labelscan further lead to error accumulation and catastrophic forgetting. To tacklethese issues, we propose a continual test-time adaptation approach~(CoTTA)which comprises two parts. Firstly, we propose to reduce the error accumulationby using weight-averaged and augmentation-averaged predictions which are oftenmore accurate. On the other hand, to avoid catastrophic forgetting, we proposeto stochastically restore a small part of the neurons to the source pre-trainedweights during each iteration to help preserve source knowledge in thelong-term. The proposed method enables the long-term adaptation for allparameters in the network. CoTTA is easy to implement and can be readilyincorporated in off-the-shelf pre-trained models. We demonstrate theeffectiveness of our approach on four classification tasks and a segmentationtask for continual test-time adaptation, on which we outperform existingmethods. Our code is available at \url{https://qin.ee/cotta}.</div></details><p><a href=http://arxiv.org/abs/2204.09594v1><strong>Predicting Clinical Intent from Free Text Electronic Health Records</strong></a></p><p><em>Kawsar Noor, Katherine Smith, Julia Bennett, Jade OConnell, Jessica Fisk, Monika Hunt, Gary Philippo, Teresa Xu, Simon Knight, Luis Romao, Richard JB Dobson, Wai Keong Wong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: After a patient consultation, a clinician determines the steps in themanagement of the patient. A clinician may for example request to see thepatient again or refer them to a specialist. Whilst most clinicians will recordtheir intent as &ldquo;next steps&rdquo; in the patient&rsquo;s clinical notes, in some cases theclinician may forget to indicate their intent as an order or request, e.g.failure to place the follow-up order. This consequently results in patientsbecoming lost-to-follow up and may in some cases lead to adverse consequences.In this paper we train a machine learning model to detect a clinician&rsquo;s intentto follow up with a patient from the patient&rsquo;s clinical notes. Annotatorssystematically identified 22 possible types of clinical intent and annotated3000 Bariatric clinical notes. The annotation process revealed a classimbalance in the labeled data and we found that there was only sufficientlabeled data to train 11 out of the 22 intents. We used the data to train aBERT based multilabel classification model and reported the following averageaccuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,macro-f1: 0.90.</div></details><blockquote><p><strong><em>2022-03-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.12964v1><strong>Knowledge Removal in Sampling-based Bayesian Inference</strong></a></p><p><em>Shaopeng Fu, Fengxiang He, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten has been legislated in many countries, but itsenforcement in the AI industry would cause unbearable costs. When single datadeletion requests come, companies may need to delete the whole models learnedwith massive resources. Existing works propose methods to remove knowledgelearned from data for explicitly parameterized models, which however are notappliable to the sampling-based Bayesian inference, i.e., Markov chain MonteCarlo (MCMC), as MCMC can only infer implicit distributions. In this paper, wepropose the first machine unlearning algorithm for MCMC. We first convert theMCMC unlearning problem into an explicit optimization problem. Based on thisproblem conversion, an {\it MCMC influence function} is designed to provablycharacterize the learned knowledge from data, which then delivers the MCMCunlearning algorithm. Theoretical analysis shows that MCMC unlearning would notcompromise the generalizability of the MCMC models. Experiments on Gaussianmixture models and Bayesian neural networks confirm the effectiveness of theproposed algorithm. The code is available at\url{https://github.com/fshp971/mcmc-unlearning}.</div></details><blockquote><p><strong><em>2022-03-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.11491v1><strong>Making Recommender Systems Forget: Learning and Unlearning for Erasable Recommendation</strong></a></p><p><em>Yuyuan Li, Xiaolin Zheng, Chaochao Chen, Junlin Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy laws and regulations enforce data-driven systems, e.g., recommendersystems, to erase the data that concern individuals. As machine learning modelspotentially memorize the training data, data erasure should also unlearn thedata lineage in models, which raises increasing interest in the problem ofMachine Unlearning (MU). However, existing MU methods cannot be directlyapplied into recommendation. The basic idea of most recommender systems iscollaborative filtering, but existing MU methods ignore the collaborativeinformation across users and items. In this paper, we propose a generalerasable recommendation framework, namely LASER, which consists of Group moduleand SeqTrain module. Firstly, Group module partitions users into balancedgroups based on their similarity of collaborative embedding learned viahypergraph. Then SeqTrain module trains the model sequentially on all groupswith curriculum learning. Both theoretical analysis and experiments on tworeal-world datasets demonstrate that LASER can not only achieve efficientunlearning, but also outperform the state-of-the-art unlearning framework interms of model utility.</div></details><p><a href=http://arxiv.org/abs/2104.06951v2><strong>Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey</strong></a></p><p><em>Danielle Saunders</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The development of deep learning techniques has allowed Neural MachineTranslation (NMT) models to become extremely powerful, given sufficienttraining data and training time. However, systems struggle when translatingtext from a new domain with a distinct style or vocabulary. Fine-tuning onin-domain data allows good domain adaptation, but requires sufficient relevantbilingual data. Even if this is available, simple fine-tuning can causeoverfitting to new data and `catastrophic forgetting&rsquo; of previously learnedbehaviour. We concentrate on robust approaches to domain adaptation for NMT,particularly where a system may need to translate across multiple domains. Wedivide techniques into those revolving around data selection or generation,model architecture, parameter adaptation procedure, and inference procedure. Wefinally highlight the benefits of domain adaptation and multi-domain adaptationtechniques to other lines of NMT research.</div></details><blockquote><p><strong><em>2022-03-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.03910v2><strong>Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation</strong></a></p><p><em>Chenze Shao, Yang Feng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural networks tend to gradually forget the previously learned knowledgewhen learning multiple tasks sequentially from dynamic data distributions. Thisproblem is called \textit{catastrophic forgetting}, which is a fundamentalchallenge in the continual learning of neural networks. In this work, weobserve that catastrophic forgetting not only occurs in continual learning butalso affects the traditional static training. Neural networks, especiallyneural machine translation models, suffer from catastrophic forgetting even ifthey learn from a static training set. To be specific, the final model paysimbalanced attention to training samples, where recently exposed samplesattract more attention than earlier samples. The underlying cause is thattraining samples do not get balanced training in each model update, so we namethis problem \textit{imbalanced training}. To alleviate this problem, wepropose Complementary Online Knowledge Distillation (COKD), which usesdynamically updated teacher models trained on specific data orders toiteratively provide complementary knowledge to the student model. Experimentalresults on multiple machine translation tasks show that our method successfullyalleviates the problem of imbalanced training and achieves substantialimprovements over strong baseline systems.</div></details><blockquote><p><strong><em>2022-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.08619v1><strong>An Independently Learnable Hierarchical Model for Bilateral Control-Based Imitation Learning Applications</strong></a></p><p><em>Kazuki Hayashi, Sho Sakaino, Toshiaki Tsuji</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, motion generation by machine learning has been actively researchedto automate various tasks. Imitation learning is one such method that learnsmotions from data collected in advance. However, executing long-term tasksremains challenging. Therefore, a novel framework for imitation learning isproposed to solve this problem. The proposed framework comprises upper andlower layers, where the upper layer model, whose timescale is long, and lowerlayer model, whose timescale is short, can be independently trained. In thismodel, the upper layer learns long-term task planning, and the lower layerlearns motion primitives. The proposed method was experimentally compared tohierarchical RNN-based methods to validate its effectiveness. Consequently, theproposed method showed a success rate equal to or greater than that ofconventional methods. In addition, the proposed method required less than 1/20of the training time compared to conventional methods. Moreover, it succeededin executing unlearned tasks by reusing the trained lower layer.</div></details><blockquote><p><strong><em>2022-03-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.13069v2><strong>Continual Active Learning Using Pseudo-Domains for Limited Labelling Resources and Changing Acquisition Characteristics</strong></a></p><p><em>Matthias Perkonigg, Johannes Hofmanninger, Christian Herold, Helmut Prosch, Georg Langs</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning in medical imaging during clinical routine is impaired bychanges in scanner protocols, hardware, or policies resulting in aheterogeneous set of acquisition settings. When training a deep learning modelon an initial static training set, model performance and reliability sufferfrom changes of acquisition characteristics as data and targets may becomeinconsistent. Continual learning can help to adapt models to the changingenvironment by training on a continuous data stream. However, continual manualexpert labelling of medical imaging requires substantial effort. Thus, ways touse labelling resources efficiently on a well chosen sub-set of new examples isnecessary to render this strategy feasible. Here, we propose a method for continual active learning operating on a streamof medical images in a multi-scanner setting. The approach automaticallyrecognizes shifts in image acquisition characteristics - new domains -, selectsoptimal examples for labelling and adapts training accordingly. Labelling issubject to a limited budget, resembling typical real world scenarios. Todemonstrate generalizability, we evaluate the effectiveness of our method onthree tasks: cardiac segmentation, lung nodule detection and brain ageestimation. Results show that the proposed approach outperforms other activelearning methods, while effectively counteracting catastrophic forgetting.</div></details><blockquote><p><strong><em>2022-03-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.07320v1><strong>The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining</strong></a></p><p><em>Yi Liu, Lei Xu, Xingliang Yuan, Cong Wang, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In Machine Learning, the emergence of \textit{the right to be forgotten} gavebirth to a paradigm named \textit{machine unlearning}, which enables dataholders to proactively erase their data from a trained model. Existing machineunlearning techniques focus on centralized training, where access to allholders&rsquo; training data is a must for the server to conduct the unlearningprocess. It remains largely underexplored about how to achieve unlearning whenfull access to all training data becomes unavailable. One noteworthy example isFederated Learning (FL), where each participating data holder trains locally,without sharing their training data to the central server. In this paper, weinvestigate the problem of machine unlearning in FL systems. We start with aformal definition of the unlearning problem in FL and propose a rapidretraining approach to fully erase data samples from a trained FL model. Theresulting design allows data holders to jointly conduct the unlearning processefficiently while keeping their training data locally. Our formal convergenceand complexity analysis demonstrate that our design can preserve model utilitywith high efficiency. Extensive evaluations on four real-world datasetsillustrate the effectiveness and performance of our proposed realization.</div></details><p><a href=http://arxiv.org/abs/2203.06953v1><strong>Forward Compatible Few-Shot Class-Incremental Learning</strong></a></p><p><em>Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, De-Chuan Zhan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Novel classes frequently arise in our dynamically changing world, e.g., newusers in the authentication system, and a machine learning model shouldrecognize new classes without forgetting old ones. This scenario becomes morechallenging when new class instances are insufficient, which is called few-shotclass-incremental learning (FSCIL). Current methods handle incremental learningretrospectively by making the updated model similar to the old one. Bycontrast, we suggest learning prospectively to prepare for future updates, andpropose ForwArd Compatible Training (FACT) for FSCIL. Forward compatibilityrequires future new classes to be easily incorporated into the current modelbased on the current stage data, and we seek to realize it by reservingembedding space for future new classes. In detail, we assign virtual prototypesto squeeze the embedding of known classes and reserve for new ones. Besides, weforecast possible new classes and prepare for the updating process. The virtualprototypes allow the model to accept possible updates in the future, which actas proxies scattered among embedding space to build a stronger classifierduring inference. FACT efficiently incorporates new classes with forwardcompatibility and meanwhile resists forgetting of old ones. Extensiveexperiments validate FACT&rsquo;s state-of-the-art performance. Code is available at:https://github.com/zhoudw-zdw/CVPR22-Fact</div></details><blockquote><p><strong><em>2022-03-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.05692v1><strong>Lifelong Adaptive Machine Learning for Sensor-based Human Activity Recognition Using Prototypical Networks</strong></a></p><p><em>Rebecca Adaimi, Edison Thomaz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning, also known as lifelong learning, is an emerging researchtopic that has been attracting increasing interest in the field of machinelearning. With human activity recognition (HAR) playing a key role in enablingnumerous real-world applications, an essential step towards the long-termdeployment of such recognition systems is to extend the activity model todynamically adapt to changes in people&rsquo;s everyday behavior. Current research incontinual learning applied to HAR domain is still under-explored withresearchers exploring existing methods developed for computer vision in HAR.Moreover, analysis has so far focused on task-incremental or class-incrementallearning paradigms where task boundaries are known. This impedes theapplicability of such methods for real-world systems since data is presented ina randomly streaming fashion. To push this field forward, we build on recentadvances in the area of continual machine learning and design a lifelongadaptive learning framework using Prototypical Networks, LAPNet-HAR, thatprocesses sensor-based data streams in a task-free data-incremental fashion andmitigates catastrophic forgetting using experience replay and continualprototype adaptation. Online learning is further facilitated using contrastiveloss to enforce inter-class separation. LAPNet-HAR is evaluated on 5 publiclyavailable activity datasets in terms of the framework&rsquo;s ability to acquire newinformation while preserving previous knowledge. Our extensive empiricalresults demonstrate the effectiveness of LAPNet-HAR in task-free continuallearning and uncover useful insights for future challenges.</div></details><blockquote><p><strong><em>2022-03-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.02645v1><strong>Acceleration of Federated Learning with Alleviated Forgetting in Local Training</strong></a></p><p><em>Chencheng Xu, Zhiwei Hong, Minlie Huang, Tao Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) enables distributed optimization of machine learningmodels while protecting privacy by independently training local models on eachclient and then aggregating parameters on a central server, thereby producingan effective global model. Although a variety of FL algorithms have beenproposed, their training efficiency remains low when the data are notindependently and identically distributed (non-i.i.d.) across differentclients. We observe that the slow convergence rates of the existing methods are(at least partially) caused by the catastrophic forgetting issue during thelocal training stage on each individual client, which leads to a large increasein the loss function concerning the previous training data at the otherclients. Here, we propose FedReg, an algorithm to accelerate FL with alleviatedknowledge forgetting in the local training stage by regularizing locallytrained parameters with the loss on generated pseudo data, which encode theknowledge of previous training data learned by the global model. Ourcomprehensive experiments demonstrate that FedReg not only significantlyimproves the convergence rate of FL, especially when the neural networkarchitecture is deep and the clients&rsquo; data are extremely non-i.i.d., but isalso able to protect privacy better in classification problems and more robustagainst gradient inversion attacks. The code is available at:https://github.com/Zoesgithub/FedReg.</div></details><blockquote><p><strong><em>2022-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.13398v2><strong>Unrolling SGD: Understanding Factors Influencing Machine Unlearning</strong></a></p><p><em>Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning is the process through which a deployed machine learningmodel is made to forget about some of its training data points. While naivelyretraining the model from scratch is an option, it is almost always associatedwith large computational overheads for deep learning models. Thus, severalapproaches to approximately unlearn have been proposed along with correspondingmetrics that formalize what it means for a model to forget about a data point.In this work, we first taxonomize approaches and metrics of approximateunlearning. As a result, we identify verification error, i.e., the L2difference between the weights of an approximately unlearned and a naivelyretrained model, as an approximate unlearning metric that should be optimizedfor as it subsumes a large class of other metrics. We theoretically analyze thecanonical training algorithm, stochastic gradient descent (SGD), to surface thevariables which are relevant to reducing the verification error of approximateunlearning for SGD. From this analysis, we first derive an easy-to-computeproxy for verification error (termed unlearning error). The analysis alsoinforms the design of a new training objective penalty that limits the overallchange in weights during SGD and as a result facilitates approximate unlearningwith lower verification error. We validate our theoretical work through anempirical evaluation on learning with CIFAR-10, CIFAR-100, and IMDB sentimentanalysis.</div></details><blockquote><p><strong><em>2022-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.12667v4><strong>Mixture-of-Variational-Experts for Continual Learning</strong></a></p><p><em>Heinke Hihn, Daniel A. Braun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: One weakness of machine learning algorithms is the poor ability of models tosolve new problems without forgetting previously acquired knowledge. TheContinual Learning (CL) paradigm has emerged as a protocol to systematicallyinvestigate settings where the model sequentially observes samples generated bya series of tasks. In this work, we take a task-agnostic view of continuallearning and develop a hierarchical information-theoretic optimality principlethat facilitates a trade-off between learning and forgetting. We discuss thisprinciple from a Bayesian perspective and show its connections to previousapproaches to CL. Based on this principle, we propose a neural network layer,called the Mixture-of-Variational-Experts layer, that alleviates forgetting bycreating a set of information processing paths through the network which isgoverned by a gating policy. Due to the general formulation based on genericutility functions, we can apply this optimality principle to a large variety oflearning problems, including supervised learning, reinforcement learning, andgenerative modeling. We demonstrate the competitive performance of our methodin continual supervised learning and in continual reinforcement learning.</div></details><blockquote><p><strong><em>2022-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.13585v1><strong>Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten</strong></a></p><p><em>Quoc Phong Nguyen, Ryutaro Oikawa, Dinil Mon Divakaran, Mun Choon Chan, Bryan Kian Hsiang Low</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As the use of machine learning (ML) models is becoming increasingly popularin many real-world applications, there are practical challenges that need to beaddressed for model maintenance. One such challenge is to &lsquo;undo&rsquo; the effect ofa specific subset of dataset used for training a model. This specific subsetmay contain malicious or adversarial data injected by an attacker, whichaffects the model performance. Another reason may be the need for a serviceprovider to remove data pertaining to a specific user to respect the user&rsquo;sprivacy. In both cases, the problem is to &lsquo;unlearn&rsquo; a specific subset of thetraining data from a trained model without incurring the costly procedure ofretraining the whole model from scratch. Towards this goal, this paper presentsa Markov chain Monte Carlo-based machine unlearning (MCU) algorithm. MCU helpsto effectively and efficiently unlearn a trained model from subsets of trainingdataset. Furthermore, we show that with MCU, we are able to explain the effectof a subset of a training dataset on the model prediction. Thus, MCU is usefulfor examining subsets of data to identify the adversarial data to be removed.Similarly, MCU can be used to erase the lineage of a user&rsquo;s personal data fromtrained ML models, thus upholding a user&rsquo;s &ldquo;right to be forgotten&rdquo;. Weempirically evaluate the performance of our proposed MCU algorithm onreal-world phishing and diabetes datasets. Results show that MCU can achieve adesirable performance by efficiently removing the effect of a subset oftraining dataset and outperform an existing algorithm that utilizes theremaining dataset.</div></details><blockquote><p><strong><em>2022-02-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.10203v1><strong>Learning Bayesian Sparse Networks with Full Experience Replay for Continual Learning</strong></a></p><p><em>Dong Gong, Qingsen Yan, Yuhang Liu, Anton van den Hengel, Javen Qinfeng Shi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Learning (CL) methods aim to enable machine learning models tolearn new tasks without catastrophic forgetting of those that have beenpreviously mastered. Existing CL approaches often keep a buffer ofpreviously-seen samples, perform knowledge distillation, or use regularizationtechniques towards this goal. Despite their performance, they still suffer frominterference across tasks which leads to catastrophic forgetting. To amelioratethis problem, we propose to only activate and select sparse neurons forlearning current and past tasks at any stage. More parameters space and modelcapacity can thus be reserved for the future tasks. This minimizes theinterference between parameters for different tasks. To do so, we propose aSparse neural Network for Continual Learning (SNCL), which employs variationalBayesian sparsity priors on the activations of the neurons in all layers. FullExperience Replay (FER) provides effective supervision in learning the sparseactivations of the neurons in different layers. A loss-aware reservoir-samplingstrategy is developed to maintain the memory buffer. The proposed method isagnostic as to the network structures and the task boundaries. Experiments ondifferent datasets show that our approach achieves state-of-the-art performancefor mitigating forgetting.</div></details><blockquote><p><strong><em>2022-02-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.11891v2><strong>On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning</strong></a></p><p><em>Anvith Thudi, Hengrui Jia, Ilia Shumailov, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning, i.e. having a model forget about some of its trainingdata, has become increasingly more important as privacy legislation promotesvariants of the right-to-be-forgotten. In the context of deep learning,approaches for machine unlearning are broadly categorized into two classes:exact unlearning methods, where an entity has formally removed the data point&rsquo;simpact on the model by retraining the model from scratch, and approximateunlearning, where an entity approximates the model parameters one would obtainby exact unlearning to save on compute costs. In this paper, we first show thatthe definition that underlies approximate unlearning, which seeks to prove theapproximately unlearned model is close to an exactly retrained model, isincorrect because one can obtain the same model using different datasets. Thusone could unlearn without modifying the model at all. We then turn to exactunlearning approaches and ask how to verify their claims of unlearning. Ourresults show that even for a given training trajectory one cannot formallyprove the absence of certain data points used during training. We thus concludethat unlearning is only well-defined at the algorithmic level, where anentity&rsquo;s only possible auditable claim to unlearning is that they used aparticular algorithm designed to allow for external scrutiny during an audit.</div></details><blockquote><p><strong><em>2022-02-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.08266v2><strong>Hard to Forget: Poisoning Attacks on Certified Machine Unlearning</strong></a></p><p><em>Neil G. Marchant, Benjamin I. P. Rubinstein, Scott Alfeld</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to erasure requires removal of a user&rsquo;s information from data heldby organizations, with rigorous interpretations extending to downstreamproducts such as learned models. Retraining from scratch with the particularuser&rsquo;s data omitted fully removes its influence on the resulting model, butcomes with a high computational cost. Machine &ldquo;unlearning&rdquo; mitigates the costincurred by full retraining: instead, models are updated incrementally,possibly only requiring retraining when approximation errors accumulate. Rapidprogress has been made towards privacy guarantees on the indistinguishabilityof unlearned and retrained models, but current formalisms do not placepractical bounds on computation. In this paper we demonstrate how an attackercan exploit this oversight, highlighting a novel attack surface introduced bymachine unlearning. We consider an attacker aiming to increase thecomputational cost of data removal. We derive and empirically investigate apoisoning attack on certified machine unlearning where strategically designedtraining data triggers complete retraining when removed.</div></details><blockquote><p><strong><em>2022-02-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.03460v1><strong>Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning</strong></a></p><p><em>Ji Gao, Sanjam Garg, Mohammad Mahmoody, Prashant Nalini Vasudevan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy attacks on machine learning models aim to identify the data that isused to train such models. Such attacks, traditionally, are studied on staticmodels that are trained once and are accessible by the adversary. Motivated tomeet new legal requirements, many machine learning methods are recentlyextended to support machine unlearning, i.e., updating models as if certainexamples are removed from their training sets, and meet new legal requirements.However, privacy attacks could potentially become more devastating in this newsetting, since an attacker could now access both the original model beforedeletion and the new model after the deletion. In fact, the very act ofdeletion might make the deleted record more vulnerable to privacy attacks. Inspired by cryptographic definitions and the differential privacy framework,we formally study privacy implications of machine unlearning. We formalize(various forms of) deletion inference and deletion reconstruction attacks, inwhich the adversary aims to either identify which record is deleted or toreconstruct (perhaps part of) the deleted records. We then present successfuldeletion inference and reconstruction attacks for a variety of machine learningmodels and tasks such as classification, regression, and language models.Finally, we show that our attacks would provably be precluded if the schemessatisfy (variants of) Deletion Compliance (Garg, Goldwasser, and Vasudevan,Eurocrypt&rsquo; 20).</div></details><blockquote><p><strong><em>2022-02-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.03576v1><strong>Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations</strong></a></p><p><em>Weiqi Peng, Jinghui Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Owing much to the revolution of information technology, the recent progressof deep learning benefits incredibly from the vastly enhanced access to dataavailable in various digital formats. However, in certain scenarios, people maynot want their data being used for training commercial models and thus studiedhow to attack the learnability of deep learning models. Previous works onlearnability attack only consider the goal of preventing unauthorizedexploitation on the specific dataset but not the process of restoring thelearnability for authorized cases. To tackle this issue, this paper introducesand investigates a new concept called &ldquo;learnability lock&rdquo; for controlling themodel&rsquo;s learnability on a specific dataset with a special key. In particular,we propose adversarial invertible transformation, that can be viewed as amapping from image to image, to slightly modify data samples so that theybecome &ldquo;unlearnable&rdquo; by machine learning models with negligible loss of visualfeatures. Meanwhile, one can unlock the learnability of the dataset and trainmodels normally using the corresponding key. The proposed learnability lockleverages class-wise perturbation that applies a universal transformationfunction on data samples of the same label. This ensures that the learnabilitycan be easily restored with a simple inverse transformation while remainingdifficult to be detected or reverse-engineered. We empirically demonstrate thesuccess and practicability of our method on visual classification tasks.</div></details><blockquote><p><strong><em>2022-02-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.00155v1><strong>Fortuitous Forgetting in Connectionist Networks</strong></a></p><p><em>Hattie Zhou, Ankit Vani, Hugo Larochelle, Aaron Courville</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Forgetting is often seen as an unwanted characteristic in both human andmachine learning. However, we propose that forgetting can in fact be favorableto learning. We introduce &ldquo;forget-and-relearn&rdquo; as a powerful paradigm forshaping the learning trajectories of artificial neural networks. In thisprocess, the forgetting step selectively removes undesirable information fromthe model, and the relearning step reinforces features that are consistentlyuseful under different conditions. The forget-and-relearn framework unifiesmany existing iterative training algorithms in the image classification andlanguage emergence literature, and allows us to understand the success of thesealgorithms in terms of the disproportionate forgetting of undesirableinformation. We leverage this understanding to improve upon existing algorithmsby designing more targeted forgetting operations. Insights from our analysisprovide a coherent view on the dynamics of iterative training in neuralnetworks and offer a clear path towards performance improvements.</div></details><blockquote><p><strong><em>2022-01-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.06820v2><strong>Recommendation Unlearning</strong></a></p><p><em>Chong Chen, Fei Sun, Min Zhang, Bolin Ding</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recommender systems provide essential web services by learning users&rsquo;personal preferences from collected data. However, in many cases, systems alsoneed to forget some training data. From the perspective of privacy, severalprivacy regulations have recently been proposed, requiring systems to eliminateany impact of the data whose owner requests to forget. From the perspective ofutility, if a system&rsquo;s utility is damaged by some bad data, the system needs toforget these data to regain utility. From the perspective of usability, userscan delete noise and incorrect entries so that a system can provide more usefulrecommendations. While unlearning is very important, it has not beenwell-considered in existing recommender systems. Although there are someresearches have studied the problem of machine unlearning in the domains ofimage and text data, existing methods can not been directly applied torecommendation as they are unable to consider the collaborative information. In this paper, we propose RecEraser, a general and efficient machineunlearning framework tailored to recommendation task. The main idea ofRecEraser is to partition the training set into multiple shards and train aconstituent model for each shard. Specifically, to keep the collaborativeinformation of the data, we first design three novel data partition algorithmsto divide training data into balanced groups based on their similarity. Then,considering that different shard models do not uniformly contribute to thefinal prediction, we further propose an adaptive aggregation method to improvethe global model utility. Experimental results on three public benchmarks showthat RecEraser can not only achieve efficient unlearning, but also outperformthe state-of-the-art unlearning methods in terms of model utility. The sourcecode can be found at <a href=https://github.com/chenchongthu/Recommendation-Unlearning>https://github.com/chenchongthu/Recommendation-Unlearning</a></div></details><blockquote><p><strong><em>2022-01-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.09538v1><strong>Backdoor Defense with Machine Unlearning</strong></a></p><p><em>Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Backdoor injection attack is an emerging threat to the security of neuralnetworks, however, there still exist limited effective defense methods againstthe attack. In this paper, we propose BAERASE, a novel method that can erasethe backdoor injected into the victim model through machine unlearning.Specifically, BAERASE mainly implements backdoor defense in two key steps.First, trigger pattern recovery is conducted to extract the trigger patternsinfected by the victim model. Here, the trigger pattern recovery problem isequivalent to the one of extracting an unknown noise distribution from thevictim model, which can be easily resolved by the entropy maximization basedgenerative model. Subsequently, BAERASE leverages these recovered triggerpatterns to reverse the backdoor injection procedure and induce the victimmodel to erase the polluted memories through a newly designed gradient ascentbased machine unlearning method. Compared with the previous machine unlearningsolutions, the proposed approach gets rid of the reliance on the full access totraining data for retraining and shows higher effectiveness on backdoor erasingthan existing fine-tuning or pruning methods. Moreover, experiments show thatBAERASE can averagely lower the attack success rates of three kinds ofstate-of-the-art backdoor attacks by 99% on four benchmark datasets.</div></details><blockquote><p><strong><em>2021-12-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.14146v1><strong>Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience</strong></a></p><p><em>David McCaffary</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The innate capacity of humans and other animals to learn a diverse, and ofteninterfering, range of knowledge and skills throughout their lifespan is ahallmark of natural intelligence, with obvious evolutionary motivations. Inparallel, the ability of artificial neural networks (ANNs) to learn across arange of tasks and domains, combining and re-using learned representationswhere required, is a clear goal of artificial intelligence. This capacity,widely described as continual learning, has become a prolific subfield ofresearch in machine learning. Despite the numerous successes of deep learningin recent years, across domains ranging from image recognition to machinetranslation, such continual task learning has proved challenging. Neuralnetworks trained on multiple tasks in sequence with stochastic gradient descentoften suffer from representational interference, whereby the learned weightsfor a given task effectively overwrite those of previous tasks in a processtermed catastrophic forgetting. This represents a major impediment to thedevelopment of more generalised artificial learning systems, capable ofaccumulating knowledge over time and task space, in a manner analogous tohumans. A repository of selected papers and implementations accompanying thisreview can be found at <a href=https://github.com/mccaffary/continual-learning>https://github.com/mccaffary/continual-learning</a>.</div></details><blockquote><p><strong><em>2021-12-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.14609v2><strong>An Investigation on Learning, Polluting, and Unlearning the Spam Emails for Lifelong Learning</strong></a></p><p><em>Nishchal Parne, Kyathi Puppaala, Nithish Bhupathi, Ripon Patgiri</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning for security is studied in this context. Several spamemail detection methods exist, each of which employs a different algorithm todetect undesired spam emails. But these models are vulnerable to attacks. Manyattackers exploit the model by polluting the data, which are trained to themodel in various ways. So to act deftly in such situations model needs toreadily unlearn the polluted data without the need for retraining. Retrainingis impractical in most cases as there is already a massive amount of datatrained to the model in the past, which needs to be trained again just forremoving a small amount of polluted data, which is often significantly lessthan 1%. This problem can be solved by developing unlearning frameworks for allspam detection models. In this research, unlearning module is integrated intospam detection models that are based on Naive Bayes, Decision trees, and RandomForests algorithms. To assess the benefits of unlearning over retraining, threespam detection models are polluted and exploited by taking attackers&rsquo; positionsand proving models&rsquo; vulnerability. Reduction in accuracy and true positiverates are shown in each case showing the effect of pollution on models. Thenunlearning modules are integrated into the models, and polluted data isunlearned; on testing the models after unlearning, restoration of performanceis seen. Also, unlearning and retraining times are compared with differentpollution data sizes on all models. On analyzing the findings, it can beconcluded that unlearning is considerably superior to retraining. Results showthat unlearning is fast, easy to implement, easy to use, and effective.</div></details><blockquote><p><strong><em>2021-12-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2107.07617v2><strong>Algorithmic insights on continual learning from fruit flies</strong></a></p><p><em>Yang Shen, Sanjoy Dasgupta, Saket Navlakha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning in computational systems is challenging due tocatastrophic forgetting. We discovered a two layer neural circuit in the fruitfly olfactory system that addresses this challenge by uniquely combining sparsecoding and associative learning. In the first layer, odors are encoded usingsparse, high dimensional representations, which reduces memory interference byactivating non overlapping populations of neurons for different odors. In thesecond layer, only the synapses between odor activated neurons and the outputneuron associated with the odor are modified during learning; the rest of theweights are frozen to prevent unrelated memories from being overwritten. Weshow empirically and analytically that this simple and lightweight algorithmsignificantly boosts continual learning performance. The fly associativelearning algorithm is strikingly similar to the classic perceptron learningalgorithm, albeit two modifications, which we show are critical for reducingcatastrophic forgetting. Overall, fruit flies evolved an efficient lifelonglearning algorithm, and circuit mechanisms from neuroscience can be translatedto improve machine computation.</div></details><p><a href=http://arxiv.org/abs/2112.11944v1><strong>Continual learning of longitudinal health records</strong></a></p><p><em>J. Armstrong, D. Clifton</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning denotes machine learning methods which can adapt to newenvironments while retaining and reusing knowledge gained from pastexperiences. Such methods address two issues encountered by models innon-stationary environments: ungeneralisability to new data, and thecatastrophic forgetting of previous knowledge when retrained. This is apervasive problem in clinical settings where patient data exhibits covariateshift not only between populations, but also continuously over time. However,while continual learning methods have seen nascent success in the imagingdomain, they have been little applied to the multi-variate sequential datacharacteristic of critical care patient recordings. Here we evaluate a variety of continual learning methods on longitudinal ICUdata in a series of representative healthcare scenarios. We find that whileseveral methods mitigate short-term forgetting, domain shift remains achallenging problem over large series of tasks, with only replay based methodsachieving stable long-term performance. Code for reproducing all experiments can be found athttps://github.com/iacobo/continual</div></details><blockquote><p><strong><em>2021-12-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2112.05095v1><strong>Provable Continual Learning via Sketched Jacobian Approximations</strong></a></p><p><em>Reinhard Heckel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: An important problem in machine learning is the ability to learn tasks in asequential manner. If trained with standard first-order methods most modelsforget previously learned tasks when trained on a new task, which is oftenreferred to as catastrophic forgetting. A popular approach to overcomeforgetting is to regularize the loss function by penalizing models that performpoorly on previous tasks. For example, elastic weight consolidation (EWC)regularizes with a quadratic form involving a diagonal matrix build based onpast data. While EWC works very well for some setups, we show that, even underotherwise ideal conditions, it can provably suffer catastrophic forgetting ifthe diagonal matrix is a poor approximation of the Hessian matrix of previoustasks. We propose a simple approach to overcome this: Regularizing training ofa new task with sketches of the Jacobian matrix of past data. This provablyenables overcoming catastrophic forgetting for linear models and for wideneural networks, at the cost of memory. The overarching goal of this paper isto provided insights on when regularization-based continual learning algorithmswork and under what memory costs.</div></details><p><a href=http://arxiv.org/abs/2112.04728v1><strong>Reducing Catastrophic Forgetting in Self Organizing Maps with Internally-Induced Generative Replay</strong></a></p><p><em>Hitesh Vaidya, Travis Desell, Alexander Ororbia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A lifelong learning agent is able to continually learn from potentiallyinfinite streams of pattern sensory data. One major historic difficulty inbuilding agents that adapt in this way is that neural systems struggle toretain previously-acquired knowledge when learning from new samples. Thisproblem is known as catastrophic forgetting (interference) and remains anunsolved problem in the domain of machine learning to this day. Whileforgetting in the context of feedforward networks has been examined extensivelyover the decades, far less has been done in the context of alternativearchitectures such as the venerable self-organizing map (SOM), an unsupervisedneural model that is often used in tasks such as clustering and dimensionalityreduction. Although the competition among its internal neurons might carry thepotential to improve memory retention, we observe that a fixed-sized SOMtrained on task incremental data, i.e., it receives data points related tospecific classes at certain temporal increments, experiences significantforgetting. In this study, we propose the continual SOM (c-SOM), a model thatis capable of reducing its own forgetting when processing information.</div></details><blockquote><p><strong><em>2021-11-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.10831v1><strong>Learning by Active Forgetting for Neural Networks</strong></a></p><p><em>Jian Peng, Xian Sun, Min Deng, Chao Tao, Bo Tang, Wenbo Li, Guohua Wu, QingZhu, Yu Liu, Tao Lin, Haifeng Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Remembering and forgetting mechanisms are two sides of the same coin in ahuman learning-memory system. Inspired by human brain memory mechanisms, modernmachine learning systems have been working to endow machine with lifelonglearning capability through better remembering while pushing the forgetting asthe antagonist to overcome. Nevertheless, this idea might only see the halfpicture. Up until very recently, increasing researchers argue that a brain isborn to forget, i.e., forgetting is a natural and active process for abstract,rich, and flexible representations. This paper presents a learning model byactive forgetting mechanism with artificial neural networks. The activeforgetting mechanism (AFM) is introduced to a neural network via a"plug-and-play" forgetting layer (P&amp;PF), consisting of groups of inhibitoryneurons with Internal Regulation Strategy (IRS) to adjust the extinction rateof themselves via lateral inhibition mechanism and External Regulation Strategy(ERS) to adjust the extinction rate of excitatory neurons via inhibitionmechanism. Experimental studies have shown that the P&amp;PF offers surprisingbenefits: self-adaptive structure, strong generalization, long-term learningand memory, and robustness to data and parameter perturbation. This work shedslight on the importance of forgetting in the learning process and offers newperspectives to understand the underlying mechanisms of neural networks.</div></details><blockquote><p><strong><em>2021-11-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.05528v1><strong>Lightweight machine unlearning in neural network</strong></a></p><p><em>Kongyang Chen, Yiwen Wang, Yao Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, machine learning neural network has penetrated deeply intopeople&rsquo;s life. As the price of convenience, people&rsquo;s private information alsohas the risk of disclosure. The &ldquo;right to be forgotten&rdquo; was introduced in atimely manner, stipulating that individuals have the right to withdraw theirconsent from personal information processing activities based on their consent.To solve this problem, machine unlearning is proposed, which allows the modelto erase all memory of private information. Previous studies, includingretraining and incremental learning to update models, often take up extrastorage space or are difficult to apply to neural networks. Our method onlyneeds to make a small perturbation of the weight of the target model and makeit iterate in the direction of the model trained with the remaining data subsetuntil the contribution of the unlearning data to the model is completelyeliminated. In this paper, experiments on five datasets prove the effectivenessof our method for machine unlearning, and our method is 15 times faster thanretraining.</div></details><blockquote><p><strong><em>2021-10-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.15008v1><strong>Risk-utility tradeoff shapes memory strategies for evolving patterns</strong></a></p><p><em>Oskar H Schnaack, Luca Peliti, Armita Nourmohammad</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Keeping a memory of evolving stimuli is ubiquitous in biology, an example ofwhich is immune memory for evolving pathogens. However, learning and memorystorage for dynamic patterns still pose challenges in machine learning. Here,we introduce an analytical energy-based framework to address this problem. Byaccounting for the tradeoff between utility in keeping a high-affinity memoryand the risk in forgetting some of the diverse stimuli, we show that a moderatetolerance for risk enables a repertoire to robustly classify evolving patterns,without much fine-tuning. Our approach offers a general guideline for learningand memory storage in systems interacting with diverse and evolving stimuli.</div></details><blockquote><p><strong><em>2021-10-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.10255v1><strong>A Simple Approach to Continual Learning by Transferring Skill Parameters</strong></a></p><p><em>K. R. Zentner, Ryan Julian, Ujjwal Puri, Yulun Zhang, Gaurav S. Sukhatme</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In order to be effective general purpose machines in real world environments,robots not only will need to adapt their existing manipulation skills to newcircumstances, they will need to acquire entirely new skills on-the-fly. Agreat promise of continual learning is to endow robots with this ability, byusing their accumulated knowledge and experience from prior skills. We take afresh look at this problem, by considering a setting in which the robot islimited to storing that knowledge and experience only in the form of learnedskill policies. We show that storing skill policies, careful pre-training, andappropriately choosing when to transfer those skill policies is sufficient tobuild a continual learner in the context of robotic manipulation. We analyzewhich conditions are needed to transfer skills in the challenging Meta-Worldsimulation benchmark. Using this analysis, we introduce a pair-wise metricrelating skills that allows us to predict the effectiveness of skill transferbetween tasks, and use it to reduce the problem of continual learning tocurriculum selection. Given an appropriate curriculum, we show how tocontinually acquire robotic manipulation skills without forgetting, and usingfar fewer samples than needed to train them from scratch.</div></details><blockquote><p><strong><em>2021-10-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.09574v1><strong>Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters</strong></a></p><p><em>Asa Cooper Stickland, Alexandre Bérard, Vassilina Nikoulina</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Adapter layers are lightweight, learnable units inserted between transformerlayers. Recent work explores using such layers for neural machine translation(NMT), to adapt pre-trained models to new domains or language pairs, trainingonly a small set of parameters for each new setting (language pair or domain).In this work we study the compositionality of language and domain adapters inthe context of Machine Translation. We aim to study, 1) parameter-efficientadaptation to multiple domains and languages simultaneously (full-resourcescenario) and 2) cross-lingual transfer in domains where parallel data isunavailable for certain language pairs (partial-resource scenario). We findthat in the partial resource scenario a naive combination of domain-specificand language-specific adapters often results in `catastrophic forgetting&rsquo; ofthe missing languages. We study other ways to combine the adapters to alleviatethis issue and maximize cross-lingual transfer. With our best adaptercombinations, we obtain improvements of 3-4 BLEU on average for sourcelanguages that do not have in-domain data. For target languages withoutin-domain data, we achieve a similar improvement by combining adapters withback-translation. Supplementary material is available athttps://tinyurl.com/r66stbxj</div></details><blockquote><p><strong><em>2021-10-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.08857v1><strong>Growing Representation Learning</strong></a></p><p><em>Ryan King, Bobak Mortazavi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning continues to grow in popularity due to its ability to learnincreasingly complex tasks. However, for many supervised models, the shift in adata distribution or the appearance of a new event can result in a severedecrease in model performance. Retraining a model from scratch with updateddata can be resource intensive or impossible depending on the constraintsplaced on an organization or system. Continual learning methods attempt toadapt models to new classes instead of retraining. However, many of thesemethods do not have a detection method for new classes or make assumptionsabout the distribution of classes. In this paper, we develop an attention basedGaussian Mixture, called GMAT, that learns interpretable representations ofdata with or without labels. We incorporate this method with existing NeuralArchitecture Search techniques to develop an algorithm for detection new eventsfor an optimal number of representations through an iterative process oftraining a growing. We show that our method is capable learning newrepresentations of data without labels or assumptions about the distributionsof labels. We additionally develop a method that allows our model to utilizelabels to more accurately develop representations. Lastly, we show that ourmethod can avoid catastrophic forgetting by replaying samples from learnedrepresentations.</div></details><blockquote><p><strong><em>2021-09-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.08445v2><strong>DRILL: Dynamic Representations for Imbalanced Lifelong Learning</strong></a></p><p><em>Kyra Ahrens, Fares Abawi, Stefan Wermter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual or lifelong learning has been a long-standing challenge in machinelearning to date, especially in natural language processing (NLP). Althoughstate-of-the-art language models such as BERT have ushered in a new era in thisfield due to their outstanding performance in multitask learning scenarios,they suffer from forgetting when being exposed to a continuous stream of datawith shifting data distributions. In this paper, we introduce DRILL, a novelcontinual learning architecture for open-domain text classification. DRILLleverages a biologically inspired self-organizing neural architecture toselectively gate latent language representations from BERT in atask-incremental manner. We demonstrate in our experiments that DRILLoutperforms current methods in a realistic scenario of imbalanced,non-stationary data without prior knowledge about task boundaries. To the bestof our knowledge, DRILL is the first of its kind to use a self-organizingneural architecture for open-domain lifelong learning in NLP.</div></details><blockquote><p><strong><em>2021-09-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.02205v2><strong>When Machine Unlearning Jeopardizes Privacy</strong></a></p><p><em>Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten states that a data owner has the right to erasetheir data from an entity storing it. In the context of machine learning (ML),the right to be forgotten requires an ML model owner to remove the data owner&rsquo;sdata from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of thedata owner, we argue that machine unlearning may leave some imprint of the datain the ML model and thus create unintended privacy risks. In this paper, weperform the first study on investigating the unintended information leakagecaused by machine unlearning. We propose a novel membership inference attackthat leverages the different outputs of an ML model&rsquo;s two versions to inferwhether a target sample is part of the training set of the original model butout of the training set of the corresponding unlearned model. Our experimentsdemonstrate that the proposed membership inference attack achieves strongperformance. More importantly, we show that our attack in multiple casesoutperforms the classical membership inference attack on the original ML model,which indicates that machine unlearning can have counterproductive effects onprivacy. We notice that the privacy degradation is especially significant forwell-generalized ML models where classical membership inference does notperform well. We further investigate four mechanisms to mitigate the newlydiscovered privacy risks and show that releasing the predicted label only,temperature scaling, and differential privacy are effective. We believe thatour results can help improve privacy protection in practical implementations ofmachine unlearning. Our code is available athttps://github.com/MinChen00/UnlearningLeaks.</div></details><p><a href=http://arxiv.org/abs/2104.08771v2><strong>Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation</strong></a></p><p><em>Mozhdeh Gheini, Xiang Ren, Jonathan May</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the power of cross-attention in the Transformer architecture withinthe context of transfer learning for machine translation, and extend thefindings of studies into cross-attention when training from scratch. We conducta series of experiments through fine-tuning a translation model on data whereeither the source or target language has changed. These experiments reveal thatfine-tuning only the cross-attention parameters is nearly as effective asfine-tuning all parameters (i.e., the entire translation model). We provideinsights into why this is the case and observe that limiting fine-tuning inthis manner yields cross-lingually aligned embeddings. The implications of thisfinding for researchers and practitioners include a mitigation of catastrophicforgetting, the potential for zero-shot translation, and the ability to extendmachine translation models to several new language pairs with reduced parameterstorage overhead.</div></details><blockquote><p><strong><em>2021-09-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.04197v1><strong>A distillation-based approach integrating continual learning and federated learning for pervasive services</strong></a></p><p><em>Anastasiia Usmanova, François Portet, Philippe Lalanda, German Vega</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning, a new machine learning paradigm enhancing the use of edgedevices, is receiving a lot of attention in the pervasive community to supportthe development of smart services. Nevertheless, this approach still needs tobe adapted to the specificity of the pervasive domain. In particular, issuesrelated to continual learning need to be addressed. In this paper, we present adistillation-based approach dealing with catastrophic forgetting in federatedlearning scenario. Specifically, Human Activity Recognition tasks are used as ademonstration domain.</div></details><blockquote><p><strong><em>2021-09-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.01558v1><strong>Learning Neural Models for Natural Language Processing in the Face of Distributional Shift</strong></a></p><p><em>Paul Michel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The dominating NLP paradigm of training a strong neural predictor to performone task on a specific dataset has led to state-of-the-art performance in avariety of applications (eg. sentiment classification, span-prediction basedquestion answering or machine translation). However, it builds upon theassumption that the data distribution is stationary, ie. that the data issampled from a fixed distribution both at training and test time. This way oftraining is inconsistent with how we as humans are able to learn from andoperate within a constantly changing stream of information. Moreover, it isill-adapted to real-world use cases where the data distribution is expected toshift over the course of a model&rsquo;s lifetime. The first goal of this thesis is to characterize the different forms thisshift can take in the context of natural language processing, and proposebenchmarks and evaluation metrics to measure its effect on current deeplearning architectures. We then proceed to take steps to mitigate the effect ofdistributional shift on NLP models. To this end, we develop methods based onparametric reformulations of the distributionally robust optimizationframework. Empirically, we demonstrate that these approaches yield more robustmodels as demonstrated on a selection of realistic problems. In the third andfinal part of this thesis, we explore ways of efficiently adapting existingmodels to new domains or tasks. Our contribution to this topic takesinspiration from information geometry to derive a new gradient update rulewhich alleviate catastrophic forgetting issues during adaptation.</div></details><blockquote><p><strong><em>2021-08-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.15093v3><strong>Certifiable Machine Unlearning for Linear Models</strong></a></p><p><em>Ananth Mahadevan, Michael Mathioudakis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning is the task of updating machine learning (ML) models aftera subset of the training data they were trained on is deleted. Methods for thetask are desired to combine effectiveness and efficiency, i.e., they shouldeffectively &ldquo;unlearn&rdquo; deleted data, but in a way that does not requireexcessive computation effort (e.g., a full retraining) for a small amount ofdeletions. Such a combination is typically achieved by tolerating some amountof approximation in the unlearning. In addition, laws and regulations in thespirit of &ldquo;the right to be forgotten&rdquo; have given rise to requirements forcertifiability, i.e., the ability to demonstrate that the deleted data hasindeed been unlearned by the ML model. In this paper, we present an experimental study of the three state-of-the-artapproximate unlearning methods for linear models and demonstrate the trade-offsbetween efficiency, effectiveness and certifiability offered by each method. Inimplementing the study, we extend some of the existing works and describe acommon ML pipeline to compare and evaluate the unlearning methods on sixreal-world datasets and a variety of settings. We provide insights into theeffect of the quantity and distribution of the deleted data on ML models andthe performance of each unlearning method in different settings. We alsopropose a practical online strategy to determine when the accumulated errorfrom approximate unlearning is large enough to warrant a full retrain of the MLmodel.</div></details><p><a href=http://arxiv.org/abs/2108.06131v2><strong>The Forgotten Threat of Voltage Glitching: A Case Study on Nvidia Tegra X2 SoCs</strong></a></p><p><em>Otto Bittner, Thilo Krachenfels, Andreas Galauner, Jean-Pierre Seifert</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Voltage fault injection (FI) is a well-known attack technique that can beused to force faulty behavior in processors during their operation. Glitchingthe supply voltage can cause data value corruption, skip security checks, orenable protected code paths. At the same time, modern systems on a chip (SoCs)are used in security-critical applications, such as self-driving cars andautonomous machines. Since these embedded devices are often physicallyaccessible by attackers, vendors must consider device tampering in their threatmodels. However, while the threat of voltage FI is known since the early 2000s,it seems as if vendors still forget to integrate countermeasures. This workshows how the entire boot security of an Nvidia SoC, used in Tesla&rsquo;s autopilotand Mercedes-Benz&rsquo;s infotainment system, can be circumvented using voltage FI.We uncover a hidden bootloader that is only available to the manufacturer fortesting purposes and disabled by fuses in shipped products. We demonstrate howto re-enable this bootloader using FI to gain code execution with the highestprivileges, enabling us to extract the bootloader&rsquo;s firmware and decryptionkeys used in later boot stages. Using a hardware implant, an adversary mightmisuse the hidden bootloader to bypass trusted code execution even during thesystem&rsquo;s regular operation.</div></details><blockquote><p><strong><em>2021-08-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.06758v1><strong>An Investigation of Replay-based Approaches for Continual Learning</strong></a></p><p><em>Benedikt Bagus, Alexander Gepperth</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) is a major challenge of machine learning (ML) anddescribes the ability to learn several tasks sequentially without catastrophicforgetting (CF). Recent works indicate that CL is a complex topic, even more sowhen real-world scenarios with multiple constraints are involved. Severalsolution classes have been proposed, of which so-called replay-based approachesseem very promising due to their simplicity and robustness. Such approachesstore a subset of past samples in a dedicated memory for later processing:while this does not solve all problems, good results have been obtained. Inthis article, we empirically investigate replay-based approaches of continuallearning and assess their potential for applications. Selected recentapproaches as well as own proposals are compared on a common set of benchmarks,with a particular focus on assessing the performance of different sampleselection strategies. We find that the impact of sample selection increaseswhen a smaller number of samples is stored. Nevertheless, performance variesstrongly between different replay approaches. Surprisingly, we find that themost naive rehearsal-based approaches that we propose here can outperformrecent state-of-the-art methods.</div></details><blockquote><p><strong><em>2021-08-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.12141v2><strong>Dimension reduction in recurrent networks by canonicalization</strong></a></p><p><em>Lyudmila Grigoryeva, Juan-Pablo Ortega</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Many recurrent neural network machine learning paradigms can be formulatedusing state-space representations. The classical notion of canonicalstate-space realization is adapted in this paper to accommodate semi-infiniteinputs so that it can be used as a dimension reduction tool in the recurrentnetworks setup. The so-called input forgetting property is identified as thekey hypothesis that guarantees the existence and uniqueness (up to systemisomorphisms) of canonical realizations for causal and time-invariantinput/output systems with semi-infinite inputs. Additionally, the notion ofoptimal reduction coming from the theory of symmetric Hamiltonian systems isimplemented in our setup to construct canonical realizations out of inputforgetting but not necessarily canonical ones. These two procedures are studiedin detail in the framework of linear fading memory input/output systems.Finally, the notion of implicit reduction using reproducing kernel Hilbertspaces (RKHS) is introduced which allows, for systems with linear readouts, toachieve dimension reduction without the need to actually compute the reducedspaces introduced in the first part of the paper.</div></details><blockquote><p><strong><em>2021-08-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.03913v1><strong>Unified Regularity Measures for Sample-wise Learning and Generalization</strong></a></p><p><em>Chi Zhang, Xiaoning Ma, Yu Liu, Le Wang, Yuanqi Su, Yuehu Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fundamental machine learning theory shows that different samples contributeunequally both in learning and testing processes. Contemporary studies on DNNimply that such sample difference is rooted on the distribution of intrinsicpattern information, namely sample regularity. Motivated by the recentdiscovery on network memorization and generalization, we proposed a pair ofsample regularity measures for both processes with a formulation-consistentrepresentation. Specifically, cumulative binary training/generalizing loss(CBTL/CBGL), the cumulative number of correct classiffcations of thetraining/testing sample within training stage, is proposed to quantize thestability in memorization-generalization process; whileforgetting/mal-generalizing events, i.e., the mis-classification of previouslylearned or generalized sample, are utilized to represent the uncertainty ofsample regularity with respect to optimization dynamics. Experiments validatedthe effectiveness and robustness of the proposed approaches for mini-batch SGDoptimization. Further applications on training/testing sample selection showthe proposed measures sharing the unified computing procedure could benefit forboth tasks.</div></details><blockquote><p><strong><em>2021-08-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2108.02786v1><strong>Quantum Continual Learning Overcoming Catastrophic Forgetting</strong></a></p><p><em>Wenjie Jiang, Zhide Lu, Dong-Ling Deng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting describes the fact that machine learning models willlikely forget the knowledge of previously learned tasks after the learningprocess of a new one. It is a vital problem in the continual learning scenarioand recently has attracted tremendous concern across different communities. Inthis paper, we explore the catastrophic forgetting phenomena in the context ofquantum machine learning. We find that, similar to those classical learningmodels based on neural networks, quantum learning systems likewise suffer fromsuch forgetting problem in classification tasks emerging from variousapplication scenes. We show that based on the local geometrical information inthe loss function landscape of the trained model, a uniform strategy can beadapted to overcome the forgetting problem in the incremental learning setting.Our results uncover the catastrophic forgetting phenomena in quantum machinelearning and offer a practical method to overcome this problem, which opens anew avenue for exploring potential quantum advantages towards continuallearning.</div></details><blockquote><p><strong><em>2021-08-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2003.10933v3><strong>Learn to Forget: Machine Unlearning via Neuron Masking</strong></a></p><p><em>Yang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip Yu, Kui Ren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Nowadays, machine learning models, especially neural networks, becomeprevalent in many real-world applications.These models are trained based on aone-way trip from user data: as long as users contribute their data, there isno way to withdraw; and it is well-known that a neural network memorizes itstraining data. This contradicts the &ldquo;right to be forgotten&rdquo; clause of GDPR,potentially leading to law violations. To this end, machine unlearning becomesa popular research topic, which allows users to eliminate memorization of theirprivate data from a trained machine learning model.In this paper, we proposethe first uniform metric called for-getting rate to measure the effectivenessof a machine unlearning method. It is based on the concept of membershipinference and describes the transformation rate of the eliminated data from"memorized" to &ldquo;unknown&rdquo; after conducting unlearning. We also propose a novelunlearning method calledForsaken. It is superior to previous work in eitherutility or efficiency (when achieving the same forgetting rate). We benchmarkForsaken with eight standard datasets to evaluate its performance. Theexperimental results show that it can achieve more than 90% forgetting rate onaverage and only causeless than 5% accuracy loss.</div></details><p><a href=http://arxiv.org/abs/2103.07492v4><strong>Continual Learning for Recurrent Neural Networks: an Empirical Evaluation</strong></a></p><p><em>Andrea Cossu, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Learning continuously during all model lifetime is fundamental to deploymachine learning solutions robust to drifts in the data distribution. Advancesin Continual Learning (CL) with recurrent neural networks could pave the way toa large number of applications where incoming data is non stationary, likenatural language processing and robotics. However, the existing body of work onthe topic is still fragmented, with approaches which are application-specificand whose assessment is based on heterogeneous learning protocols and datasets.In this paper, we organize the literature on CL for sequential data processingby providing a categorization of the contributions and a review of thebenchmarks. We propose two new benchmarks for CL with sequential data based onexisting datasets, whose characteristics resemble real-world applications. Wealso provide a broad empirical evaluation of CL and Recurrent Neural Networksin class-incremental scenario, by testing their ability to mitigate forgettingwith a number of different strategies which are not specific to sequential dataprocessing. Our results highlight the key role played by the sequence lengthand the importance of a clear specification of the CL scenario.</div></details><blockquote><p><strong><em>2021-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.03279v2><strong>Remember What You Want to Forget: Algorithms for Machine Unlearning</strong></a></p><p><em>Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of unlearning datapoints from a learnt model. Thelearner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,and outputs a model $\widehat{w}$ that performs well on unseen samples from thesame distribution. However, at some point in the future, any training datapoint$z \in S$ can request to be unlearned, thus prompting the learner to modify itsoutput model while still ensuring the same accuracy guarantees. We initiate arigorous study of generalization in machine unlearning, where the goal is toperform well on previously unseen datapoints. Our focus is on bothcomputational and storage complexity. For the setting of convex losses, we provide an unlearning algorithm that canunlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. Incomparison, in general, differentially private learning (which impliesunlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. Thisdemonstrates a novel separation between differential privacy and machineunlearning.</div></details><blockquote><p><strong><em>2021-07-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.00926v4><strong>CLeaR: An Adaptive Continual Learning Framework for Regression Tasks</strong></a></p><p><em>Yujiang He, Bernhard Sick</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting means that a trained neural network model graduallyforgets the previously learned tasks when being retrained on new tasks.Overcoming the forgetting problem is a major problem in machine learning.Numerous continual learning algorithms are very successful in incrementallearning of classification tasks, where new samples with their labels appearfrequently. However, there is currently no research that addresses thecatastrophic forgetting problem in regression tasks as far as we know. Thisproblem has emerged as one of the primary constraints in some applications,such as renewable energy forecasts. This article clarifies problem-relateddefinitions and proposes a new methodological framework that can forecasttargets and update itself by means of continual learning. The frameworkconsists of forecasting neural networks and buffers, which store newlycollected data from a non-stationary data stream in an application. The changedprobability distribution of the data stream, which the framework hasidentified, will be learned sequentially. The framework is called CLeaR(Continual Learning for Regression Tasks), where components can be flexiblycustomized for a specific application scenario. We design two sets ofexperiments to evaluate the CLeaR framework concerning fitting error(training), prediction error (test), and forgetting ratio. The first one isbased on an artificial time series to explore how hyperparameters affect theCLeaR framework. The second one is designed with data collected from Europeanwind farms to evaluate the CLeaR framework&rsquo;s performance in a real-worldapplication. The experimental results demonstrate that the CLeaR framework cancontinually acquire knowledge in the data stream and improve the predictionaccuracy. The article concludes with further research issues arising fromrequirements to extend the framework.</div></details><blockquote><p><strong><em>2021-07-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.01616v3><strong>Towards Lifelong Learning of End-to-end ASR</strong></a></p><p><em>Heng-Jui Chang, Hung-yi Lee, Lin-shan Lee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automatic speech recognition (ASR) technologies today are primarily optimizedfor given datasets; thus, any changes in the application environment (e.g.,acoustic conditions or topic domains) may inevitably degrade the performance.We can collect new data describing the new environment and fine-tune thesystem, but this naturally leads to higher error rates for the earlierdatasets, referred to as catastrophic forgetting. The concept of lifelonglearning (LLL) aiming to enable a machine to sequentially learn new tasks fromnew datasets describing the changing real world without forgetting thepreviously learned knowledge is thus brought to attention. This paper reports,to our knowledge, the first effort to extensively consider and analyze the useof various approaches of LLL in end-to-end (E2E) ASR, including proposing novelmethods in saving data for past domains to mitigate the catastrophic forgettingproblem. An overall relative reduction of 28.7% in WER was achieved compared tothe fine-tuning baseline when sequentially learning on three very differentbenchmark corpora. This can be the first step toward the highly desired ASRtechnologies capable of synchronizing with the continuously changing realworld.</div></details><p><a href=http://arxiv.org/abs/2107.00940v1><strong>Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks</strong></a></p><p><em>Suryanarayana Maddu, Dominik Sturm, Christian L. Müller, Ivo F. Sbalzarini</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We characterize and remedy a failure mode that may arise from multi-scaledynamics with scale imbalances during training of deep neural networks, such asPhysics Informed Neural Networks (PINNs). PINNs are popular machine-learningtemplates that allow for seamless integration of physical equation models withdata. Their training amounts to solving an optimization problem over a weightedsum of data-fidelity and equation-fidelity objectives. Conflicts betweenobjectives can arise from scale imbalances, heteroscedasticity in the data,stiffness of the physical equation, or from catastrophic interference duringsequential training. We explain the training pathology arising from this andpropose a simple yet effective inverse-Dirichlet weighting strategy toalleviate the issue. We compare with Sobolev training of neural networks,providing the baseline of analytically $\boldsymbol{\epsilon}$-optimaltraining. We demonstrate the effectiveness of inverse-Dirichlet weighting invarious applications, including a multi-scale model of active turbulence, wherewe show orders of magnitude improvement in accuracy and convergence overconventional PINN training. For inverse modeling using sequential training, wefind that inverse-Dirichlet weighting protects a PINN against catastrophicforgetting.</div></details><blockquote><p><strong><em>2021-06-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.15721v2><strong>Coded Machine Unlearning</strong></a></p><p><em>Nasser Aldaghri, Hessam Mahdavifar, Ahmad Beirami</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: There are applications that may require removing the trace of a sample fromthe system, e.g., a user requests their data to be deleted, or corrupted datais discovered. Simply removing a sample from storage units does not necessarilyremove its entire trace since downstream machine learning models may store someinformation about the samples used to train them. A sample can be perfectlyunlearned if we retrain all models that used it from scratch with that sampleremoved from their training dataset. When multiple such unlearning requests areexpected to be served, unlearning by retraining becomes prohibitivelyexpensive. Ensemble learning enables the training data to be split into smallerdisjoint shards that are assigned to non-communicating weak learners. Eachshard is used to produce a weak model. These models are then aggregated toproduce the final central model. This setup introduces an inherent trade-offbetween performance and unlearning cost, as reducing the shard size reduces theunlearning cost but may cause degradation in performance. In this paper, wepropose a coded learning protocol where we utilize linear encoders to encodethe training data into shards prior to the learning phase. We also present thecorresponding unlearning protocol and show that it satisfies the perfectunlearning criterion. Our experimental results show that the proposed codedmachine unlearning provides a better performance versus unlearning costtrade-off compared to the uncoded baseline.</div></details><blockquote><p><strong><em>2021-06-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.05872v1><strong>Knowing when we do not know: Bayesian continual learning for sensing-based analysis tasks</strong></a></p><p><em>Sandra Servia-Rodriguez, Cecilia Mascolo, Young D. Kwon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite much research targeted at enabling conventional machine learningmodels to continually learn tasks and data distributions sequentially withoutforgetting the knowledge acquired, little effort has been devoted to accountfor more realistic situations where learning some tasks accurately might bemore critical than forgetting previous ones. In this paper we propose aBayesian inference based framework to continually learn a set of real-world,sensing-based analysis tasks that can be tuned to prioritize the remembering ofpreviously learned tasks or the learning of new ones. Our experiments prove therobustness and reliability of the learned models to adapt to the changingsensing environment, and show the suitability of using uncertainty of thepredictions to assess their reliability.</div></details><blockquote><p><strong><em>2021-06-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.02793v1><strong>Solving hybrid machine learning tasks by traversing weight space geodesics</strong></a></p><p><em>Guruprasad Raghavan, Matt Thomson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning problems have an intrinsic geometric structure as centralobjects including a neural network&rsquo;s weight space and the loss functionassociated with a particular task can be viewed as encoding the intrinsicgeometry of a given machine learning problem. Therefore, geometric concepts canbe applied to analyze and understand theoretical properties of machine learningstrategies as well as to develop new algorithms. In this paper, we addressthree seemingly unrelated open questions in machine learning by viewing themthrough a unified framework grounded in differential geometry. Specifically, weview the weight space of a neural network as a manifold endowed with aRiemannian metric that encodes performance on specific tasks. By defining ametric, we can construct geodesic, minimum length, paths in weight space thatrepresent sets of networks of equivalent or near equivalent functionalperformance on a specific task. We, then, traverse geodesic paths whileidentifying networks that satisfy a second objective. Inspired by the geometricinsight, we apply our geodesic framework to 3 major applications: (i) Networksparsification (ii) Mitigating catastrophic forgetting by constructing networkswith high performance on a series of objectives and (iii) Finding high-accuracypaths connecting distinct local optima of deep networks in the non-convex losslandscape. Our results are obtained on a wide range of network architectures(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce ageometric framework that unifies a range of machine learning objectives andthat can be applied to multiple classes of neural network architectures.</div></details><blockquote><p><strong><em>2021-06-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.00265v1><strong>A unified PAC-Bayesian framework for machine unlearning via information risk minimization</strong></a></p><p><em>Sharu Theresa Jose, Osvaldo Simeone</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning refers to mechanisms that can remove the influence of asubset of training data upon request from a trained model without incurring thecost of re-training from scratch. This paper develops a unified PAC-Bayesianframework for machine unlearning that recovers the two recent design principles- variational unlearning (Nguyen et.al., 2020) and forgetting Lagrangian(Golatkar et.al., 2020) - as information risk minimization problems(Zhang,2006). Accordingly, both criteria can be interpreted as PAC-Bayesianupper bounds on the test loss of the unlearned model that take the form of freeenergy metrics.</div></details><blockquote><p><strong><em>2021-05-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.06209v1><strong>DeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep Neural Networks</strong></a></p><p><em>Yingzhe He, Guozhu Meng, Kai Chen, Jinwen He, Xingbo Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine unlearning has great significance in guaranteeing model security andprotecting user privacy. Additionally, many legal provisions clearly stipulatethat users have the right to demand model providers to delete their own datafrom training set, that is, the right to be forgotten. The naive way ofunlearning data is to retrain the model without it from scratch, which becomesextremely time and resource consuming at the modern scale of deep neuralnetworks. Other unlearning approaches by refactoring model or training datastruggle to gain a balance between overhead and model usability. In this paper, we propose an approach, dubbed as DeepObliviate, to implementmachine unlearning efficiently, without modifying the normal training mode. Ourapproach improves the original training process by storing intermediate modelson the hard disk. Given a data point to unlearn, we first quantify its temporalresidual memory left in stored models. The influenced models will be retrainedand we decide when to terminate the retraining based on the trend of residualmemory on-the-fly. Last, we stitch an unlearned model by combining theretrained models and uninfluenced models. We extensively evaluate our approachon five datasets and deep learning models. Compared to the method of retrainingfrom scratch, our approach can achieve 99.0%, 95.0%, 91.9%, 96.7%, 74.1%accuracy rates and 66.7$\times$, 75.0$\times$, 33.3$\times$, 29.4$\times$,13.7$\times$ speedups on the MNIST, SVHN, CIFAR-10, Purchase, and ImageNetdatasets, respectively. Compared to the state-of-the-art unlearning approach,we improve 5.8% accuracy, 32.5$\times$ prediction speedup, and reach acomparable retrain speedup under identical settings on average on thesedatasets. Additionally, DeepObliviate can also pass the backdoor-basedunlearning verification.</div></details><blockquote><p><strong><em>2021-05-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2102.07503v2><strong>One-shot learning for the long term: consolidation with an artificial hippocampal algorithm</strong></a></p><p><em>Gideon Kowadlo, Abdelrahman Ahmed, David Rawlinson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Standard few-shot experiments involve learning to efficiently matchpreviously unseen samples by class. We claim that few-shot learning should belong term, assimilating knowledge for the future, without forgetting previousconcepts. In the mammalian brain, the hippocampus is understood to play asignificant role in this process, by learning rapidly and consolidatingknowledge to the neocortex incrementally over a short period. In this researchwe tested whether an artificial hippocampal algorithm (AHA), could be used witha conventional Machine Learning (ML) model that learns incrementally analogousto the neocortex, to achieve one-shot learning both short and long term. Theresults demonstrated that with the addition of AHA, the system could learn inone-shot and consolidate the knowledge for the long term without catastrophicforgetting. This study is one of the first examples of using a CLS model ofhippocampus to consolidate memories, and it constitutes a step toward few-shotcontinual learning.</div></details><blockquote><p><strong><em>2021-05-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.13891v3><strong>Federated Unlearning</strong></a></p><p><em>Gaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, Jiangchuan Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) has recently emerged as a promising distributedmachine learning (ML) paradigm. Practical needs of the &ldquo;right to be forgotten"and countering data poisoning attacks call for efficient techniques that canremove, or unlearn, specific training data from the trained FL model. Existingunlearning techniques in the context of ML, however, are no longer in effectfor FL, mainly due to the inherent distinction in the way how FL and ML learnfrom data. Therefore, how to enable efficient data removal from FL modelsremains largely under-explored. In this paper, we take the first step to fillthis gap by presenting FedEraser, the first federated unlearning methodologythat can eliminate the influence of a federated client&rsquo;s data on the global FLmodel while significantly reducing the time used for constructing the unlearnedFL model.The basic idea of FedEraser is to trade the central server&rsquo;s storagefor unlearned model&rsquo;s construction time, where FedEraser reconstructs theunlearned model by leveraging the historical parameter updates of federatedclients that have been retained at the central server during the trainingprocess of FL. A novel calibration method is further developed to calibrate theretained updates, which are further used to promptly construct the unlearnedmodel, yielding a significant speed-up to the reconstruction of the unlearnedmodel while maintaining the model efficacy. Experiments on four realisticdatasets demonstrate the effectiveness of FedEraser, with an expected speed-upof $4\times$ compared with retraining from the scratch. We envision our work asan early step in FL towards compliance with legal and ethical criteria in afair and transparent manner.</div></details><blockquote><p><strong><em>2021-05-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.01946v1><strong>Continual Learning on the Edge with TensorFlow Lite</strong></a></p><p><em>Giorgos Demosthenous, Vassilis Vassiliades</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deploying sophisticated deep learning models on embedded devices with thepurpose of solving real-world problems is a struggle using today&rsquo;s technology.Privacy and data limitations, network connection issues, and the need for fastmodel adaptation are some of the challenges that constitute today&rsquo;s approachesunfit for many applications on the edge and make real-time on-device training anecessity. Google is currently working on tackling these challenges byembedding an experimental transfer learning API to their TensorFlow Lite,machine learning library. In this paper, we show that although transferlearning is a good first step for on-device model training, it suffers fromcatastrophic forgetting when faced with more realistic scenarios. We presentthis issue by testing a simple transfer learning model on the CORe50 benchmarkas well as by demonstrating its limitations directly on an Android applicationwe developed. In addition, we expand the TensorFlow Lite library to includecontinual learning capabilities, by integrating a simple replay approach intothe head of the current transfer learning model. We test our continual learningmodel on the CORe50 benchmark to show that it tackles catastrophic forgetting,and we demonstrate its ability to continually learn, even under non-idealconditions, using the application we developed. Finally, we open-source thecode of our Android application to enable developers to integrate continuallearning to their own smartphone applications, as well as to facilitate furtherdevelopment of continual learning functionality into the TensorFlow Liteenvironment.</div></details><blockquote><p><strong><em>2021-05-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.00157v1><strong>A Deep Learning Framework for Lifelong Machine Learning</strong></a></p><p><em>Charles X. Ling, Tanner Bohn</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong machine learning,few-shot learning, and transfer learning attempt to capture these properties.However, most previous approaches can only demonstrate subsets of theseproperties, often by different complex mechanisms. In this work, we propose asimple yet powerful unified deep learning framework that supports almost all ofthese properties and approaches through one central mechanism. Experiments ontoy examples support our claims. We also draw connections between manypeculiarities of human learning (such as memory loss and &ldquo;rain man&rdquo;) and ourframework. As academics, we often lack resources required to build and train, deepneural networks with billions of parameters on hundreds of TPUs. Thus, whileour framework is still conceptual, and our experiment results are surely notSOTA, we hope that this unified lifelong learning framework inspires new worktowards large-scale experiments and understanding human learning in general. This paper is summarized in two short YouTube videos:https://youtu.be/gCuUyGETbTU (part 1) and <a href=https://youtu.be/XsaGI01b-1o>https://youtu.be/XsaGI01b-1o</a> (part2).</div></details><blockquote><p><strong><em>2021-04-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.11861v1><strong>Class-Incremental Experience Replay for Continual Learning under Concept Drift</strong></a></p><p><em>Łukasz Korycki, Bartosz Krawczyk</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern machine learning systems need to be able to cope with constantlyarriving and changing data. Two main areas of research dealing with suchscenarios are continual learning and data stream mining. Continual learningfocuses on accumulating knowledge and avoiding forgetting, assuming informationonce learned should be stored. Data stream mining focuses on adaptation toconcept drift and discarding outdated information, assuming that only the mostrecent data is relevant. While these two areas are mainly being developed inseparation, they offer complementary views on the problem of learning fromdynamic data. There is a need for unifying them, by offering architecturescapable of both learning and storing new information, as well as revisiting andadapting to changes in previously seen concepts. We propose a novel continuallearning approach that can handle both tasks. Our experience replay method isfueled by a centroid-driven memory storing diverse instances of incrementallyarriving classes. This is enhanced with a reactive subspace buffer that tracksconcept drift occurrences in previously seen classes and adapts clustersaccordingly. The proposed architecture is thus capable of both rememberingvalid and forgetting outdated information, offering a holistic framework forcontinual learning under concept drift.</div></details><blockquote><p><strong><em>2021-04-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.11604v1><strong>Learning in Deep Neural Networks Using a Biologically Inspired Optimizer</strong></a></p><p><em>Giorgia Dellaferrera, Stanislaw Wozniak, Giacomo Indiveri, Angeliki Pantazi, Evangelos Eleftheriou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Plasticity circuits in the brain are known to be influenced by thedistribution of the synaptic weights through the mechanisms of synapticintegration and local regulation of synaptic strength. However, the complexinterplay of stimulation-dependent plasticity with local learning signals isdisregarded by most of the artificial neural network training algorithmsdevised so far. Here, we propose a novel biologically inspired optimizer forartificial (ANNs) and spiking neural networks (SNNs) that incorporates keyprinciples of synaptic integration observed in dendrites of cortical neurons:GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals).GRAPES implements a weight-distribution dependent modulation of the errorsignal at each node of the neural network. We show that this biologicallyinspired mechanism leads to a systematic improvement of the convergence rate ofthe network, and substantially improves classification accuracy of ANNs andSNNs with both feedforward and recurrent architectures. Furthermore, wedemonstrate that GRAPES supports performance scalability for models ofincreasing complexity and mitigates catastrophic forgetting by enablingnetworks to generalize to unseen tasks based on previously acquired knowledge.The local characteristics of GRAPES minimize the required memory resources,making it optimally suited for dedicated hardware implementations. Overall, ourwork indicates that reconciling neurophysiology insights with machineintelligence is key to boosting the performance of neural networks.</div></details><blockquote><p><strong><em>2021-04-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1912.08124v4><strong>SpaRCe: Improved Learning of Reservoir Computing Systems through Sparse Representations</strong></a></p><p><em>Luca Manneschi, Andrew C. Lin, Eleni Vasilaki</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: &ldquo;Sparse&rdquo; neural networks, in which relatively few neurons or connections areactive, are common in both machine learning and neuroscience. Whereas inmachine learning, &ldquo;sparsity&rdquo; is related to a penalty term that leads to someconnecting weights becoming small or zero, in biological brains, sparsity isoften created when high spiking thresholds prevent neuronal activity. Here weintroduce sparsity into a reservoir computing network via neuron-specificlearnable thresholds of activity, allowing neurons with low thresholds tocontribute to decision-making but suppressing information from neurons withhigh thresholds. This approach, which we term &ldquo;SpaRCe&rdquo;, optimises the sparsitylevel of the reservoir without affecting the reservoir dynamics. The read-outweights and the thresholds are learned by an on-line gradient rule thatminimises an error function on the outputs of the network. Threshold learningoccurs by the balance of two opposing forces: reducing inter-neuronalcorrelations in the reservoir by deactivating redundant neurons, whileincreasing the activity of neurons participating in correct decisions. We testSpaRCe on classification problems and find that threshold learning improvesperformance compared to standard reservoir computing. SpaRCe alleviates theproblem of catastrophic forgetting, a problem most evident in standard echostate networks and recurrent neural networks in general, due to increasing thenumber of task-specialised neurons that are included in the network decisions.</div></details><blockquote><p><strong><em>2021-04-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.07446v1><strong>Rehearsal revealed: The limits and merits of revisiting samples in continual learning</strong></a></p><p><em>Eli Verwimp, Matthias De Lange, Tinne Tuytelaars</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Learning from non-stationary data streams and overcoming catastrophicforgetting still poses a serious challenge for machine learning research.Rather than aiming to improve state-of-the-art, in this work we provide insightinto the limits and merits of rehearsal, one of continual learning&rsquo;s mostestablished methods. We hypothesize that models trained sequentially withrehearsal tend to stay in the same low-loss region after a task has finished,but are at risk of overfitting on its sample memory, hence harminggeneralization. We provide both conceptual and strong empirical evidence onthree benchmarks for both behaviors, bringing novel insights into the dynamicsof rehearsal and continual learning in general. Finally, we interpret importantcontinual learning works in the light of our findings, allowing for a deeperunderstanding of their successes.</div></details><blockquote><p><strong><em>2021-04-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.13678v2><strong>Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation</strong></a></p><p><em>Shuhao Gu, Yang Feng, Wanying Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Domain Adaptation is widely used in practical applications of neural machinetranslation, which aims to achieve good performance on both the general-domainand in-domain. However, the existing methods for domain adaptation usuallysuffer from catastrophic forgetting, domain divergence, and model explosion. Toaddress these three problems, we propose a method of &ldquo;divide and conquer&rdquo; whichis based on the importance of neurons or parameters in the translation model.In our method, we first prune the model and only keep the important neurons orparameters, making them responsible for both general-domain and in-domaintranslation. Then we further train the pruned model supervised by the originalunpruned model with the knowledge distillation method. Last we expand the modelto the original size and fine-tune the added parameters for the in-domaintranslation. We conduct experiments on different languages and domains and theresults show that our method can achieve significant improvements compared withseveral strong baselines.</div></details><blockquote><p><strong><em>2021-04-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.03047v1><strong>Few-Shot Incremental Learning with Continually Evolved Classifiers</strong></a></p><p><em>Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Few-shot class-incremental learning (FSCIL) aims to design machine learningalgorithms that can continually learn new concepts from a few data points,without forgetting knowledge of old classes. The difficulty lies in thatlimited data from new classes not only lead to significant overfitting issuesbut also exacerbate the notorious catastrophic forgetting problems. Moreover,as training data come in sequence in FSCIL, the learned classifier can onlyprovide discriminative information in individual sessions, while FSCIL requiresall classes to be involved for evaluation. In this paper, we address the FSCILproblem from two aspects. First, we adopt a simple but effective decoupledlearning strategy of representations and classifiers that only the classifiersare updated in each incremental session, which avoids knowledge forgetting inthe representations. By doing so, we demonstrate that a pre-trained backboneplus a non-parametric class mean classifier can beat state-of-the-art methods.Second, to make the classifiers learned on individual sessions applicable toall classes, we propose a Continually Evolved Classifier (CEC) that employs agraph model to propagate context information between classifiers foradaptation. To enable the learning of CEC, we design a pseudo incrementallearning paradigm that episodically constructs a pseudo incremental learningtask to optimize the graph parameters by sampling data from the base dataset.Experiments on three popular benchmark datasets, including CIFAR100,miniImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our methodsignificantly outperforms the baselines and sets new state-of-the-art resultswith remarkable advantages.</div></details><blockquote><p><strong><em>2021-04-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1912.03049v4><strong>Regularization Shortcomings for Continual Learning</strong></a></p><p><em>Timothée Lesort, Andrei Stoian, David Filliat</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In most machine learning algorithms, training data is assumed to beindependent and identically distributed (iid). When it is not the case, thealgorithm&rsquo;s performances are challenged, leading to the famous phenomenon ofcatastrophic forgetting. Algorithms dealing with it are gathered in theContinual Learning research field. In this paper, we study the regularizationbased approaches to continual learning and show that those approaches can notlearn to discriminate classes from different tasks in an elemental continualbenchmark: the class-incremental scenario. We make theoretical reasoning toprove this shortcoming and illustrate it with examples and experiments.Moreover, we show that it can have some important consequences on continualmulti-tasks reinforcement learning or in pre-trained models used for continuallearning. We believe that highlighting and understanding the shortcomings ofregularization strategies will help us to use them more efficiently.</div></details><blockquote><p><strong><em>2021-03-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.12294v1><strong>Gradient Regularized Contrastive Learning for Continual Domain Adaptation</strong></a></p><p><em>Shixiang Tang, Peng Su, Dapeng Chen, Wanli Ouyang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human beings can quickly adapt to environmental changes by leveraginglearning experience. However, adapting deep neural networks to dynamicenvironments by machine learning algorithms remains a challenge. To betterunderstand this issue, we study the problem of continual domain adaptation,where the model is presented with a labelled source domain and a sequence ofunlabelled target domains. The obstacles in this problem are both domain shiftand catastrophic forgetting. We propose Gradient Regularized ContrastiveLearning (GRCL) to solve the obstacles. At the core of our method, gradientregularization plays two key roles: (1) enforcing the gradient not to harm thediscriminative ability of source features which can, in turn, benefit theadaptation ability of the model to target domains; (2) constraining thegradient not to increase the classification loss on old target domains, whichenables the model to preserve the performance on old target domains whenadapting to an in-coming target domain. Experiments on Digits, DomainNet andOffice-Caltech benchmarks demonstrate the strong performance of our approachwhen compared to the other state-of-the-art methods.</div></details><blockquote><p><strong><em>2021-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2006.03887v2><strong>Multiple-timescale Neural Networks: Generation of Context-dependent Sequences and Inference through Autonomous Bifurcations</strong></a></p><p><em>Tomoki Kurikawa, Kunihiko Kaneko</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Sequential transitions between metastable states are ubiquitously observed inthe neural system and underlie various cognitive functions. Although a numberof studies with asymmetric Hebbian connectivity have investigated how suchsequences are generated, the focused sequences are simple Markov ones. On theother hand, supervised machine learning methods can generate complex non-Markovsequences, but these sequences are vulnerable against perturbations. Further,concatenation of newly learned sequence to the already learned one is difficultdue to catastrophe forgetting, although concatenation is essential forcognitive functions such as inference. How stable complex sequences aregenerated still remains unclear. We have developed a neural network with fastand slow dynamics, which are inspired by the experiments. The slow dynamicsstore history of inputs and outputs and affect the fast dynamics depending onthe stored history. We show the learning rule that requires only localinformation can form the network generating the complex and robust sequences inthe fast dynamics. The slow dynamics work as bifurcation parameters for thefast one, wherein they stabilize the next pattern of the sequence before thecurrent pattern is destabilized. This co-existence period leads to the stabletransition between the current and the next pattern in the sequence. We furtherfind that timescale balance is critical to this period. Our study provides anovel mechanism generating the robust complex sequences with multipletimescales in neural dynamics. Considering the multiple timescales are widelyobserved, the mechanism advances our understanding of temporal processing inthe neural system.</div></details><blockquote><p><strong><em>2021-02-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2102.13179v1><strong>Machine Unlearning via Algorithmic Stability</strong></a></p><p><em>Enayat Ullah, Tung Mai, Anup Rao, Ryan Rossi, Raman Arora</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of machine unlearning and identify a notion ofalgorithmic stability, Total Variation (TV) stability, which we argue, issuitable for the goal of exact unlearning. For convex risk minimizationproblems, we design TV-stable algorithms based on noisy Stochastic GradientDescent (SGD). Our key contribution is the design of corresponding efficientunlearning algorithms, which are based on constructing a (maximal) coupling ofMarkov chains for the noisy SGD procedure. To understand the trade-offs betweenaccuracy and unlearning efficiency, we give upper and lower bounds on excessempirical and populations risk of TV stable algorithms for convex riskminimization. Our techniques generalize to arbitrary non-convex functions, andour algorithms are differentially private as well.</div></details><blockquote><p><strong><em>2021-02-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.06417v2><strong>Bayesian Inference Forgetting</strong></a></p><p><em>Shaopeng Fu, Fengxiang He, Yue Xu, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten has been legislated in many countries but theenforcement in machine learning would cause unbearable costs: companies mayneed to delete whole models learned from massive resources due to singleindividual requests. Existing works propose to remove the knowledge learnedfrom the requested data via its influence function which is no longer naturallywell-defined in Bayesian inference. This paper proposes a {\it Bayesianinference forgetting} (BIF) framework to realize the right to be forgotten inBayesian inference. In the BIF framework, we develop forgetting algorithms forvariational inference and Markov chain Monte Carlo. We show that our algorithmscan provably remove the influence of single datums on the learned models.Theoretical analysis demonstrates that our algorithms have guaranteedgeneralizability. Experiments of Gaussian mixture models on the syntheticdataset and Bayesian neural networks on the real-world data verify thefeasibility of our methods. The source code package is available at\url{https://github.com/fshp971/BIF}.</div></details><blockquote><p><strong><em>2021-01-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2102.00464v1><strong>Beyond the Command: Feminist STS Research and Critical Issues for the Design of Social Machines</strong></a></p><p><em>Kelly B. Wagman, Lisa Parks</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machines, from artificially intelligent digital assistants to embodiedrobots, are becoming more pervasive in everyday life. Drawing on feministscience and technology studies (STS) perspectives, we demonstrate how machinedesigners are not just crafting neutral objects, but relationships betweenmachines and humans that are entangled in human social issues such as genderand power dynamics. Thus, in order to create a more ethical and just future,the dominant assumptions currently underpinning the design of thesehuman-machine relations must be challenged and reoriented toward relations ofjustice and inclusivity. This paper contributes the &ldquo;social machine&rdquo; as a modelfor technology designers who seek to recognize the importance, diversity andcomplexity of the social in their work, and to engage with the agential powerof machines. In our model, the social machine is imagined as a potentiallyequitable relationship partner that has agency and as an &ldquo;other&rdquo; that isdistinct from, yet related to, humans, objects, and animals. We criticallyexamine and contrast our model with tendencies in robotics that consider robotsas tools, human companions, animals or creatures, and/or slaves. In doing so,we demonstrate ingrained dominant assumptions about human-machine relations andreveal the challenges of radical thinking in the social machine design space.Finally, we present two design challenges based on non-anthropomorphicfiguration and mutuality, and call for experimentation, unlearning dominanttendencies, and reimagining of sociotechnical futures.</div></details><blockquote><p><strong><em>2021-01-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.11828v1><strong>Adaptive Decision Forest: An Incremental Machine Learning Framework</strong></a></p><p><em>Md Geaur Rahman, Md Zahidul Islam</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this study, we present an incremental machine learning framework calledAdaptive Decision Forest (ADF), which produces a decision forest to classifynew records. Based on our two novel theorems, we introduce a new splittingstrategy called iSAT, which allows ADF to classify new records even if they areassociated with previously unseen classes. ADF is capable of identifying andhandling concept drift; it, however, does not forget previously gainedknowledge. Moreover, ADF is capable of handling big data if the data can bedivided into batches. We evaluate ADF on five publicly available natural datasets and one synthetic data set, and compare the performance of ADF against theperformance of eight state-of-the-art techniques. Our experimental results,including statistical sign test and Nemenyi test analyses, indicate a clearsuperiority of the proposed framework over the state-of-the-art techniques.</div></details><blockquote><p><strong><em>2021-01-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.03163v1><strong>Slow manifolds in recurrent networks encode working memory efficiently and robustly</strong></a></p><p><em>Elham Ghazizadeh, ShiNung Ching</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Working memory is a cognitive function involving the storage and manipulationof latent information over brief intervals of time, thus making it crucial forcontext-dependent computation. Here, we use a top-down modeling approach toexamine network-level mechanisms of working memory, an enigmatic issue andcentral topic of study in neuroscience and machine intelligence. We trainthousands of recurrent neural networks on a working memory task and thenperform dynamical systems analysis on the ensuing optimized networks, whereinwe find that four distinct dynamical mechanisms can emerge. In particular, weshow the prevalence of a mechanism in which memories are encoded along slowstable manifolds in the network state space, leading to a phasic neuronalactivation profile during memory periods. In contrast to mechanisms in whichmemories are directly encoded at stable attractors, these networks naturallyforget stimuli over time. Despite this seeming functional disadvantage, theyare more efficient in terms of how they leverage their attractor landscape andparadoxically, are considerably more robust to noise. Our results provide newdynamical hypotheses regarding how working memory function is encoded in bothnatural and artificial neural networks.</div></details><blockquote><p><strong><em>2020-12-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.15497v1><strong>Incremental Embedding Learning via Zero-Shot Translation</strong></a></p><p><em>Kun Wei, Cheng Deng, Xu Yang, Maosen Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern deep learning methods have achieved great success in machine learningand computer vision fields by learning a set of pre-defined datasets. Howerver,these methods perform unsatisfactorily when applied into real-world situations.The reason of this phenomenon is that learning new tasks leads the trainedmodel quickly forget the knowledge of old tasks, which is referred to ascatastrophic forgetting. Current state-of-the-art incremental learning methodstackle catastrophic forgetting problem in traditional classification networksand ignore the problem existing in embedding networks, which are the basicnetworks for image retrieval, face recognition, zero-shot learning, etc.Different from traditional incremental classification networks, the semanticgap between the embedding spaces of two adjacent tasks is the main challengefor embedding networks under incremental learning setting. Thus, we propose anovel class-incremental method for embedding network, named as zero-shottranslation class-incremental method (ZSTCI), which leverages zero-shottranslation to estimate and compensate the semantic gap without any exemplars.Then, we try to learn a unified representation for two adjacent tasks insequential learning process, which captures the relationships of previousclasses and current classes precisely. In addition, ZSTCI can easily becombined with existing regularization-based incremental learning methods tofurther improve performance of embedding networks. We conduct extensiveexperiments on CUB-200-2011 and CIFAR100, and the experiment results prove theeffectiveness of our method. The code of our method has been released.</div></details><blockquote><p><strong><em>2020-12-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.12178v2><strong>Incremental Real-Time Personalization in Human Activity Recognition Using Domain Adaptive Batch Normalization</strong></a></p><p><em>Alan Mazankiewicz, Klemens Böhm, Mario Bergés</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human Activity Recognition (HAR) from devices like smartphone accelerometersis a fundamental problem in ubiquitous computing. Machine learning basedrecognition models often perform poorly when applied to new users that were notpart of the training data. Previous work has addressed this challenge bypersonalizing general recognition models to the unique motion pattern of a newuser in a static batch setting. They require target user data to be availableupfront. The more challenging online setting has received less attention. Nosamples from the target user are available in advance, but they arrivesequentially. Additionally, the motion pattern of users may change over time.Thus, adapting to new and forgetting old information must be traded off.Finally, the target user should not have to do any work to use the recognitionsystem by, say, labeling any activities. Our work addresses all of thesechallenges by proposing an unsupervised online domain adaptation algorithm.Both classification and personalization happen continuously and incrementallyin real time. Our solution works by aligning the feature distributions of allsubjects, be they sources or the target, in hidden neural network layers. Tothis end, we normalize the input of a layer with user-specific mean andvariance statistics. During training, these statistics are computed overuser-specific batches. In the online phase, they are estimated incrementallyfor any new target user.</div></details><blockquote><p><strong><em>2020-12-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1912.03817v3><strong>Machine Unlearning</strong></a></p><p><em>Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Once users have shared their data online, it is generally difficult for themto revoke access and ask for the data to be deleted. Machine learning (ML)exacerbates this problem because any model trained with said data may havememorized it, putting users at risk of a successful privacy attack exposingtheir information. Yet, having models unlearn is notoriously difficult. Weintroduce SISA training, a framework that expedites the unlearning process bystrategically limiting the influence of a data point in the training procedure.While our framework is applicable to any learning algorithm, it is designed toachieve the largest improvements for stateful algorithms like stochasticgradient descent for deep neural networks. SISA training reduces thecomputational overhead associated with unlearning, even in the worst-casesetting where unlearning requests are made uniformly across the training set.In some cases, the service provider may have a prior on the distribution ofunlearning requests that will be issued by users. We may take this prior intoaccount to partition and order data accordingly, and further decrease overheadfrom unlearning. Our evaluation spans several datasets from different domains,with corresponding motivations for unlearning. Under no distributionalassumptions, for simple learning tasks, we observe that SISA training improvestime to unlearn points from the Purchase dataset by 4.63x, and 2.45x for theSVHN dataset, over retraining from scratch. SISA training also provides aspeed-up of 1.36x in retraining for complex learning tasks such as ImageNetclassification; aided by transfer learning, this results in a small degradationin accuracy. Our work contributes to practical data governance in machineunlearning.</div></details><blockquote><p><strong><em>2020-12-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.00487v3><strong>Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes</strong></a></p><p><em>Timothée Lesort</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans learn all their life long. They accumulate knowledge from a sequenceof learning experiences and remember the essential concepts without forgettingwhat they have learned previously. Artificial neural networks struggle to learnsimilarly. They often rely on data rigorously preprocessed to learn solutionsto specific problems such as classification or regression. In particular, theyforget their past learning experiences if trained on new ones. Therefore,artificial neural networks are often inept to deal with real-life settings suchas an autonomous-robot that has to learn on-line to adapt to new situations andovercome new problems without forgetting its past learning-experiences.Continual learning (CL) is a branch of machine learning addressing this type ofproblem. Continual algorithms are designed to accumulate and improve knowledgein a curriculum of learning-experiences without forgetting. In this thesis, wepropose to explore continual algorithms with replay processes. Replay processesgather together rehearsal methods and generative replay methods. GenerativeReplay consists of regenerating past learning experiences with a generativemodel to remember them. Rehearsal consists of saving a core-set of samples frompast learning experiences to rehearse them later. The replay processes makepossible a compromise between optimizing the current learning objective and thepast ones enabling learning without forgetting in sequences of tasks settings.We show that they are very promising methods for continual learning. Notably,they enable the re-evaluation of past data with new knowledge and theconfrontation of data from different learning-experiences. We demonstrate theirability to learn continually through unsupervised learning, supervised learningand reinforcement learning tasks.</div></details><blockquote><p><strong><em>2020-12-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2003.04247v2><strong>Towards Probabilistic Verification of Machine Unlearning</strong></a></p><p><em>David Marco Sommer, Liwei Song, Sameer Wagh, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The right to be forgotten, also known as the right to erasure, is the rightof individuals to have their data erased from an entity storing it. The statusof this long held notion was legally solidified recently by the General DataProtection Regulation (GDPR) in the European Union. Consequently, there is aneed for mechanisms whereby users can verify if service providers comply withtheir deletion requests. In this work, we take the first step in proposing aformal framework to study the design of such verification mechanisms for datadeletion requests &ndash; also known as machine unlearning &ndash; in the context ofsystems that provide machine learning as a service (MLaaS). Our frameworkallows the rigorous quantification of any verification mechanism based onstandard hypothesis testing. Furthermore, we propose a novel backdoor-basedverification mechanism and demonstrate its effectiveness in certifying datadeletion with high confidence, thus providing a basis for quantitativelyinferring machine unlearning. We evaluate our approach over a range of network architectures such asmulti-layer perceptrons (MLP), convolutional neural networks (CNN), residualnetworks (ResNet), and long short-term memory (LSTM), as well as over 5different datasets. We demonstrate that our approach has minimal effect on theML service&rsquo;s accuracy but provides high confidence verification of unlearning.Our proposed mechanism works even if only a handful of users employ our systemto ascertain compliance with data deletion requests. In particular, with just5% of users participating, modifying half their data with a backdoor, and withmerely 30 test queries, our verification mechanism has both false positive andfalse negative ratios below $10^{-3}$. We also show the effectiveness of ourapproach by testing it against an adaptive adversary that uses astate-of-the-art backdoor defense method.</div></details><blockquote><p><strong><em>2020-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2011.00678v3><strong>Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation</strong></a></p><p><em>Shuhao Gu, Yang Feng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural machine translation (NMT) models usually suffer from catastrophicforgetting during continual training where the models tend to gradually forgetpreviously learned knowledge and swing to fit the newly added data which mayhave a different distribution, e.g. a different domain. Although many methodshave been proposed to solve this problem, we cannot get to know what causesthis phenomenon yet. Under the background of domain adaptation, we investigatethe cause of catastrophic forgetting from the perspectives of modules andparameters (neurons). The investigation on the modules of the NMT model showsthat some modules have tight relation with the general-domain knowledge whilesome other modules are more essential in the domain adaptation. And theinvestigation on the parameters shows that some parameters are important forboth the general-domain and in-domain translation and the great change of themduring continual training brings about the performance decline ingeneral-domain. We conduct experiments across different language pairs anddomains to ensure the validity and reliability of our findings.</div></details><blockquote><p><strong><em>2020-11-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2011.07035v1><strong>Continual Learning with Deep Artificial Neurons</strong></a></p><p><em>Blake Camp, Jaya Krishna Mandivarapu, Rolando Estrada</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neurons in real brains are enormously complex computational units. Amongother things, they&rsquo;re responsible for transforming inbound electro-chemicalvectors into outbound action potentials, updating the strengths of intermediatesynapses, regulating their own internal states, and modulating the behavior ofother nearby neurons. One could argue that these cells are the only thingsexhibiting any semblance of real intelligence. It is odd, therefore, that themachine learning community has, for so long, relied upon the assumption thatthis complexity can be reduced to a simple sum and fire operation. We ask,might there be some benefit to substantially increasing the computational powerof individual neurons in artificial systems? To answer this question, weintroduce Deep Artificial Neurons (DANs), which are themselves realized as deepneural networks. Conceptually, we embed DANs inside each node of a traditionalneural network, and we connect these neurons at multiple synaptic sites,thereby vectorizing the connections between pairs of cells. We demonstrate thatit is possible to meta-learn a single parameter vector, which we dub a neuronalphenotype, shared by all DANs in the network, which facilitates ameta-objective during deployment. Here, we isolate continual learning as ourmeta-objective, and we show that a suitable neuronal phenotype can endow asingle network with an innate ability to update its synapses with minimalforgetting, using standard backpropagation, without experience replay, norseparate wake/sleep phases. We demonstrate this ability on sequentialnon-linear regression tasks.</div></details><blockquote><p><strong><em>2020-11-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2011.00890v1><strong>Emergent Communication Pretraining for Few-Shot Machine Translation</strong></a></p><p><em>Yaoyiran Li, Edoardo M. Ponti, Ivan Vulić, Anna Korhonen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While state-of-the-art models that rely upon massively multilingualpretrained encoders achieve sample efficiency in downstream applications, theystill require abundant amounts of unlabelled text. Nevertheless, most of theworld&rsquo;s languages lack such resources. Hence, we investigate a more radicalform of unsupervised knowledge transfer in the absence of linguistic data. Inparticular, for the first time we pretrain neural networks via emergentcommunication from referential games. Our key assumption is that groundingcommunication on images&mdash;as a crude approximation of real-worldenvironments&mdash;inductively biases the model towards learning natural languages.On the one hand, we show that this substantially benefits machine translationin few-shot settings. On the other hand, this also provides an extrinsicevaluation protocol to probe the properties of emergent languages ex vitro.Intuitively, the closer they are to natural languages, the higher the gainsfrom pretraining on them should be. For instance, in this work we measure theinfluence of communication success and maximum sequence length on downstreamperformances. Finally, we introduce a customised adapter layer and annealingstrategies for the regulariser of maximum-a-posteriori inference duringfine-tuning. These turn out to be crucial to facilitate knowledge transfer andprevent catastrophic forgetting. Compared to a recurrent baseline, our methodyields gains of $59.0%$$\sim$$147.6%$ in BLEU score with only $500$ NMTtraining instances and $65.1%$$\sim$$196.7%$ with $1,000$ NMT traininginstances across four language pairs. These proof-of-concept results reveal thepotential of emergent communication pretraining for both natural languageprocessing tasks in resource-poor settings and extrinsic evaluation ofartificial languages.</div></details><blockquote><p><strong><em>2020-10-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.15600v1><strong>Three computational models and its equivalence</strong></a></p><p><em>Ciro Ivan Garcia Lopez</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The study of computability has its origin in Hilbert&rsquo;s conference of 1900,where an adjacent question, to the ones he asked, is to give a precisedescription of the notion of algorithm. In the search for a good definitionarose three independent theories: Turing and the Turing machines, G"odel andthe recursive functions, Church and the Lambda Calculus. Later there were established by Kleene that the classic models of computationare equivalent. This fact is widely accepted by many textbooks and the proof isomitted since the proof is tedious and unreadable. We intend to fill this gappresenting the proof in a modern way, without forgetting the mathematicaldetails.</div></details><blockquote><p><strong><em>2020-10-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.10981v1><strong>Amnesiac Machine Learning</strong></a></p><p><em>Laura Graves, Vineel Nagisetty, Vijay Ganesh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The Right to be Forgotten is part of the recently enacted General DataProtection Regulation (GDPR) law that affects any data holder that has data onEuropean Union residents. It gives EU residents the ability to request deletionof their personal data, including training records used to train machinelearning models. Unfortunately, Deep Neural Network models are vulnerable toinformation leaking attacks such as model inversion attacks which extract classinformation from a trained model and membership inference attacks whichdetermine the presence of an example in a model&rsquo;s training data. If a maliciousparty can mount an attack and learn private information that was meant to beremoved, then it implies that the model owner has not properly protected theiruser&rsquo;s rights and their models may not be compliant with the GDPR law. In thispaper, we present two efficient methods that address this question of how amodel owner or data holder may delete personal data from models in such a waythat they may not be vulnerable to model inversion and membership inferenceattacks while maintaining model efficacy. We start by presenting a real-worldthreat model that shows that simply removing training data is insufficient toprotect users. We follow that up with two data removal methods, namelyUnlearning and Amnesiac Unlearning, that enable model owners to protectthemselves against such attacks while being compliant with regulations. Weprovide extensive empirical analysis that show that these methods are indeedefficient, safe to apply, effectively remove learned information aboutsensitive data from trained models while maintaining model efficacy.</div></details><blockquote><p><strong><em>2020-10-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.09403v1><strong>Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation</strong></a></p><p><em>Dušan Variš, Ondřej Bojar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This work presents our ongoing research of unsupervised pretraining in neuralmachine translation (NMT). In our method, we initialize the weights of theencoder and decoder with two language models that are trained with monolingualdata and then fine-tune the model on parallel data using Elastic WeightConsolidation (EWC) to avoid forgetting of the original language modelingtasks. We compare the regularization by EWC with the previous work that focuseson regularization by language modeling objectives. The positive result is thatusing EWC with the decoder achieves BLEU scores similar to the previous work.However, the model converges 2-3 times faster and does not require the originalunlabeled training data during the fine-tuning stage. In contrast, theregularization using EWC is less effective if the original and new tasks arenot closely related. We show that initializing the bidirectional NMT encoderwith a left-to-right language model and forcing the model to remember theoriginal left-to-right language modeling task limits the learning capacity ofthe encoder for the whole bidirectional context.</div></details><blockquote><p><strong><em>2020-10-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.06138v1><strong>Incorporating BERT into Parallel Sequence Decoding with Adapters</strong></a></p><p><em>Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While large scale pre-trained language models such as BERT have achievedgreat success on various natural language understanding tasks, how toefficiently and effectively incorporate them into sequence-to-sequence modelsand the corresponding text generation tasks remains a non-trivial problem. Inthis paper, we propose to address this problem by taking two different BERTmodels as the encoder and decoder respectively, and fine-tuning them byintroducing simple and lightweight adapter modules, which are inserted betweenBERT layers and tuned on the task-specific dataset. In this way, we obtain aflexible and efficient model which is able to jointly leverage the informationcontained in the source-side and target-side BERT models, while bypassing thecatastrophic forgetting problem. Each component in the framework can beconsidered as a plug-in unit, making the framework flexible and task agnostic.Our framework is based on a parallel sequence decoding algorithm namedMask-Predict considering the bi-directional and conditional independent natureof BERT, and can be adapted to traditional autoregressive decoding easily. Weconduct extensive experiments on neural machine translation tasks where theproposed method consistently outperforms autoregressive baselines whilereducing the inference latency by half, and achieves $36.49$/$33.57$ BLEUscores on IWSLT14 German-English/WMT14 German-English translation. When adaptedto autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEUscores on WMT14 English-German/English-French translation, on par with thestate-of-the-art baseline models.</div></details><blockquote><p><strong><em>2020-09-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2009.03632v1><strong>Imbalanced Continual Learning with Partitioning Reservoir Sampling</strong></a></p><p><em>Chris Dongjoo Kim, Jinseo Jeong, Gunhee Kim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning from a sequential stream of data is a crucial challengefor machine learning research. Most studies have been conducted on this topicunder the single-label classification setting along with an assumption ofbalanced label distribution. This work expands this research horizon towardsmulti-label classification. In doing so, we identify unanticipated adversityinnately existent in many multi-label datasets, the long-tailed distribution.We jointly address the two independently solved problems, CatastropicForgetting and the long-tailed label distribution by first empirically showinga new challenge of destructive forgetting of the minority concepts on the tail.Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow thestudy of both intra- and inter-task imbalances. Lastly, we propose a newsampling strategy for replay-based approach named Partitioning ReservoirSampling (PRS), which allows the model to maintain a balanced knowledge of bothhead and tail classes. We publicly release the dataset and the code in ourproject page.</div></details><blockquote><p><strong><em>2020-08-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2008.10874v1><strong>Continual Domain Adaptation for Machine Reading Comprehension</strong></a></p><p><em>Lixin Su, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yanyan Lan, Xueqi Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine reading comprehension (MRC) has become a core component in a varietyof natural language processing (NLP) applications such as question answeringand dialogue systems. It becomes a practical challenge that an MRC model needsto learn in non-stationary environments, in which the underlying datadistribution changes over time. A typical scenario is the domain drift, i.e.different domains of data come one after another, where the MRC model isrequired to adapt to the new domain while maintaining previously learnedability. To tackle such a challenge, in this work, we introduce the\textit{Continual Domain Adaptation} (CDA) task for MRC. So far as we know,this is the first study on the continual learning perspective of MRC. We buildtwo benchmark datasets for the CDA task, by re-organizing existing MRCcollections into different domains with respect to context type and questiontype, respectively. We then analyze and observe the catastrophic forgetting(CF) phenomenon of MRC under the CDA setting. To tackle the CDA task, wepropose several BERT-based continual learning MRC models using eitherregularization-based methodology or dynamic-architecture paradigm. We analyzethe performance of different continual learning MRC models under the CDA taskand show that the proposed dynamic-architecture based model achieves the bestperformance.</div></details><blockquote><p><strong><em>2020-08-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.10803v3><strong>Formant Tracking Using Dilated Convolutional Networks Through Dense Connection with Gating Mechanism</strong></a></p><p><em>Wang Dai, Jinsong Zhang, Yingming Gao, Wei Wei, Dengfeng Ke, Binghuai Lin, Yanlu Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Formant tracking is one of the most fundamental problems in speechprocessing. Traditionally, formants are estimated using signal processingmethods. Recent studies showed that generic convolutional architectures canoutperform recurrent networks on temporal tasks such as speech synthesis andmachine translation. In this paper, we explored the use of TemporalConvolutional Network (TCN) for formant tracking. In addition to theconventional implementation, we modified the architecture from three aspects.First, we turned off the &ldquo;causal&rdquo; mode of dilated convolution, making thedilated convolution see the future speech frames. Second, each hidden layerreused the output information from all the previous layers through denseconnection. Third, we also adopted a gating mechanism to alleviate the problemof gradient disappearance by selectively forgetting unimportant information.The model was validated on the open access formant database VTR. The experimentshowed that our proposed model was easy to converge and achieved an overallmean absolute percent error (MAPE) of 8.2% on speech-labeled frames, comparedto three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9% (TCN).</div></details><blockquote><p><strong><em>2020-08-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2008.02567v1><strong>An Intelligent Non-Invasive Real Time Human Activity Recognition System for Next-Generation Healthcare</strong></a></p><p><em>William Taylor, Syed Aziz Shah, Kia Dashtipour, Adnan Zahid, Qammer H. Abbasi, Muhammad Ali Imran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human motion detection is getting considerable attention in the field ofArtificial Intelligence (AI) driven healthcare systems. Human motion can beused to provide remote healthcare solutions for vulnerable people byidentifying particular movements such as falls, gait and breathing disorders.This can allow people to live more independent lifestyles and still have thesafety of being monitored if more direct care is needed. At present wearabledevices can provide real time monitoring by deploying equipment on a person&rsquo;sbody. However, putting devices on a person&rsquo;s body all the time make ituncomfortable and the elderly tends to forget it to wear as well in addition tothe insecurity of being tracked all the time. This paper demonstrates how humanmotions can be detected in quasi-real-time scenario using a non-invasivemethod. Patterns in the wireless signals presents particular human body motionsas each movement induces a unique change in the wireless medium. These changescan be used to identify particular body motions. This work produces a datasetthat contains patterns of radio wave signals obtained using software definedradios (SDRs) to establish if a subject is standing up or sitting down as atest case. The dataset was used to create a machine learning model, which wasused in a developed application to provide a quasi-real-time classification ofstanding or sitting state. The machine learning model was able to achieve 96.70% accuracy using the Random Forest algorithm using 10 fold cross validation. Abenchmark dataset of wearable devices was compared to the proposed dataset andresults showed the proposed dataset to have similar accuracy of nearly 90 %.The machine learning models developed in this paper are tested for twoactivities but the developed system is designed and applicable for detectingand differentiating x number of activities.</div></details><blockquote><p><strong><em>2020-08-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2008.01679v1><strong>Applying Incremental Deep Neural Networks-based Posture Recognition Model for Injury Risk Assessment in Construction</strong></a></p><p><em>Junqi Zhao, Esther Obonyo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Monitoring awkward postures is a proactive prevention for MusculoskeletalDisorders (MSDs)in construction. Machine Learning (ML) models have shownpromising results for posture recognition from Wearable Sensors. However,further investigations are needed concerning: i) Incremental Learning (IL),where trained models adapt to learn new postures and control the forgetting oflearned postures; ii) MSDs assessment with recognized postures. This studyproposed an incremental Convolutional Long Short-Term Memory (CLN) model,investigated effective IL strategies, and evaluated MSDs assessment usingrecognized postures. Tests with nine workers showed the CLN model with shallowconvolutional layers achieved high recognition performance (F1 Score) underpersonalized (0.87) and generalized (0.84) modeling. Generalized shallow CLNmodel under Many-to-One IL scheme can balance the adaptation (0.73) andforgetting of learnt subjects (0.74). MSDs assessment using postures recognizedfrom incremental CLN model had minor difference with ground-truth, whichdemonstrates the high potential for automated MSDs monitoring in construction.</div></details><blockquote><p><strong><em>2020-07-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.10571v1><strong>AI Tax: The Hidden Cost of AI Data Center Applications</strong></a></p><p><em>Daniel Richins, Dharmisha Doshi, Matthew Blackmore, Aswathy Thulaseedharan Nair, Neha Pathapati, Ankit Patel, Brainard Daguman, Daniel Dobrijalowski, Ramesh Illikkal, Kevin Long, David Zimmerman, Vijay Janapa Reddi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial intelligence and machine learning are experiencing widespreadadoption in industry and academia. This has been driven by rapid advances inthe applications and accuracy of AI through increasingly complex algorithms andmodels; this, in turn, has spurred research into specialized hardware AIaccelerators. Given the rapid pace of advances, it is easy to forget that theyare often developed and evaluated in a vacuum without considering the fullapplication environment. This paper emphasizes the need for a holistic,end-to-end analysis of AI workloads and reveals the &ldquo;AI tax.&rdquo; We deploy andcharacterize Face Recognition in an edge data center. The application is anAI-centric edge video analytics application built using popular open sourceinfrastructure and ML tools. Despite using state-of-the-art AI and MLalgorithms, the application relies heavily on pre-and post-processing code. AsAI-centric applications benefit from the acceleration promised by accelerators,we find they impose stresses on the hardware and software infrastructure:storage and network bandwidth become major bottlenecks with increasing AIacceleration. By specializing for AI applications, we show that a purpose-builtedge data center can be designed for the stresses of accelerated AI at 15%lower TCO than one derived from homogeneous servers and infrastructure.</div></details><blockquote><p><strong><em>2020-07-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2001.05859v5><strong>Deep learning achieves perfect anomaly detection on 108,308 retinal images including unlearned diseases</strong></a></p><p><em>Ayaka Suzuki, Yoshiro Suzuki</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Optical coherence tomography (OCT) scanning is useful in detecting variousretinal diseases. However, there are not enough ophthalmologists who candiagnose retinal OCT images in much of the world. To provide OCT screeninginexpensively and extensively, an automated diagnosis system is indispensable.Although many machine learning techniques have been presented for assistingophthalmologists in diagnosing retinal OCT images, there is no technique thatcan diagnose independently without relying on an ophthalmologist, i.e., thereis no technique that does not overlook any anomaly, including unlearneddiseases. As long as there is a risk of overlooking a disease with a technique,ophthalmologists must double-check even those images that the techniqueclassifies as normal. Here, we show that our deep-learning-based binaryclassifier (normal or abnormal) achieved a perfect classification on 108,308two-dimensional retinal OCT images, i.e., true positive rate = 1.000000 andtrue negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000.Although the test set included three types of diseases, two of these were notused for training. However, all test images were correctly classified.Furthermore, we demonstrated that our scheme was able to cope with differencesin patient race. No conventional approach has achieved the above performances.Our work has a sufficient possibility of raising automated diagnosis techniquesfor retinal OCT images from &ldquo;assistant for ophthalmologists&rdquo; to &ldquo;independentdiagnosis system without ophthalmologists&rdquo;.</div></details><blockquote><p><strong><em>2020-07-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.07400v1><strong>Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics</strong></a></p><p><em>Vinay V. Ramasesh, Ethan Dyer, Maithra Raghu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A central challenge in developing versatile machine learning systems iscatastrophic forgetting: a model trained on tasks in sequence will suffersignificant performance drops on earlier tasks. Despite the ubiquity ofcatastrophic forgetting, there is limited understanding of the underlyingprocess and its causes. In this paper, we address this important knowledge gap,investigating how forgetting affects representations in neural network models.Through representational analysis techniques, we find that deeper layers aredisproportionately the source of forgetting. Supporting this, a study ofmethods to mitigate forgetting illustrates that they act to stabilize deeperlayers. These insights enable the development of an analytic argument andempirical picture relating the degree of forgetting to representationalsimilarity between tasks. Consistent with this picture, we observe maximalforgetting occurs for task sequences with intermediate similarity. We performempirical studies on the standard split CIFAR-10 setup and also introduce anovel CIFAR-100 based task approximating realistic input distribution shift.</div></details><blockquote><p><strong><em>2020-07-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.04498v3><strong>Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</strong></a></p><p><em>Danielle Saunders, Bill Byrne</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Training data for NLP tasks often exhibits gender bias in that fewersentences refer to women than to men. In Neural Machine Translation (NMT)gender bias has been shown to reduce translation quality, particularly when thetarget language has grammatical gender. The recent WinoMT challenge set allowsus to measure this effect directly (Stanovsky et al, 2019). Ideally we would reduce system bias by simply debiasing all data prior totraining, but achieving this effectively is itself a challenge. Rather thanattempt to create a <code>balanced' dataset, we use transfer learning on a small setof trusted, gender-balanced examples. This approach gives strong and consistentimprovements in gender debiasing with much less computational cost thantraining from scratch. A known pitfall of transfer learning on new domains is </code>catastrophicforgetting&rsquo;, which we address both in adaptation and in inference. Duringadaptation we show that Elastic Weight Consolidation allows a performancetrade-off between general translation quality and bias reduction. Duringinference we propose a lattice-rescoring scheme which outperforms all systemsevaluated in Stanovsky et al (2019) on WinoMT with no degradation of generaltest set BLEU, and we show this scheme can be applied to remove gender bias inthe output of <code>black box</code> online commercial MT systems. We demonstrate ourapproach translating from English into three languages with varied linguisticproperties and data availability.</div></details><blockquote><p><strong><em>2020-07-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2002.02730v2><strong>Machine Unlearning: Linear Filtration for Logit-based Classifiers</strong></a></p><p><em>Thomas Baumhauer, Pascal Schöttle, Matthias Zeppelzauer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently enacted legislation grants individuals certain rights to decide inwhat fashion their personal data may be used, and in particular a &ldquo;right to beforgotten&rdquo;. This poses a challenge to machine learning: how to proceed when anindividual retracts permission to use data which has been part of the trainingprocess of a model? From this question emerges the field of machine unlearning,which could be broadly described as the investigation of how to &ldquo;deletetraining data from models&rdquo;. Our work complements this direction of research forthe specific setting of class-wide deletion requests for classification models(e.g. deep neural networks). As a first step, we propose linear filtration as aintuitive, computationally efficient sanitization method. Our experimentsdemonstrate benefits in an adversarial setting over naive deletion schemes.</div></details><blockquote><p><strong><em>2020-07-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.02639v2><strong>Dynamic memory to alleviate catastrophic forgetting in continuous learning settings</strong></a></p><p><em>Johannes Hofmanninger, Matthias Perkonigg, James A. Brink, Oleg Pianykh, Christian Herold, Georg Langs</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In medical imaging, technical progress or changes in diagnostic procedureslead to a continuous change in image appearance. Scanner manufacturer,reconstruction kernel, dose, other protocol specific settings or administeringof contrast agents are examples that influence image content independent of thescanned biology. Such domain and task shifts limit the applicability of machinelearning algorithms in the clinical routine by rendering models obsolete overtime. Here, we address the problem of data shifts in a continuous learningscenario by adapting a model to unseen variations in the source domain whilecounteracting catastrophic forgetting effects. Our method uses a dynamic memoryto facilitate rehearsal of a diverse training data subset to mitigateforgetting. We evaluated our approach on routine clinical CT data obtained withtwo different scanner protocols and synthetic classification tasks. Experimentsshow that dynamic memory counters catastrophic forgetting in a setting withmultiple data shifts without the necessity for explicit knowledge about whenthese shifts occur.</div></details><blockquote><p><strong><em>2020-06-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1911.09704v4><strong>A Conceptual Framework for Lifelong Learning</strong></a></p><p><em>Charles X. Ling, Tanner Bohn</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong learning, few-shotlearning, and transfer learning, attempt to capture these properties. However,most previous approaches can only demonstrate subsets of these properties,often by different complex mechanisms. In this work, we propose a simple yetpowerful unified framework that supports almost all of these properties andapproaches through one central mechanism. We also draw connections between manypeculiarities of human learning (such as memory loss and &ldquo;rain man&rdquo;) and ourframework. While we do not present any state-of-the-art results, we hope thatthis conceptual framework provides a novel perspective on existing work andproposes many new research directions.</div></details><blockquote><p><strong><em>2020-06-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2006.11413v1><strong>Visualizing and Understanding Vision System</strong></a></p><p><em>Feng Qi, Guanjun Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: How the human vision system addresses the object identity-preservingrecognition problem is largely unknown. Here, we use a visionrecognition-reconstruction network (RRN) to investigate the development,recognition, learning and forgetting mechanisms, and achieve similarcharacteristics to electrophysiological measurements in monkeys. First, innetwork development study, the RRN also experiences critical developmentalstages characterized by specificities in neuron types, synapse and activationpatterns, and visual task performance from the early stage of coarse saliencemap recognition to mature stage of fine structure recognition. In digitrecognition study, we witness that the RRN could maintain object invariancerepresentation under various viewing conditions by coordinated adjustment ofresponses of population neurons. And such concerted population responsescontained untangled object identity and properties information that could beaccurately extracted via high-level cortices or even a simple weightedsummation decoder. In the learning and forgetting study, novel structurerecognition is implemented by adjusting entire synapses in low magnitude whilepattern specificities of original synaptic connectivity are preserved, whichguaranteed a learning process without disrupting the existing functionalities.This work benefits the understanding of the human visual processing mechanismand the development of human-like machine intelligence.</div></details><blockquote><p><strong><em>2020-05-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2005.06369v1><strong>Progressive growing of self-organized hierarchical representations for exploration</strong></a></p><p><em>Mayalen Etcheverry, Pierre-Yves Oudeyer, Chris Reinke</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Designing agent that can autonomously discover and learn a diversity ofstructures and skills in unknown changing environments is key for lifelongmachine learning. A central challenge is how to learn incrementallyrepresentations in order to progressively build a map of the discoveredstructures and re-use it to further explore. To address this challenge, weidentify and target several key functionalities. First, we aim to build lastingrepresentations and avoid catastrophic forgetting throughout the explorationprocess. Secondly we aim to learn a diversity of representations allowing todiscover a &ldquo;diversity of diversity&rdquo; of structures (and associated skills) incomplex high-dimensional environments. Thirdly, we target representations thatcan structure the agent discoveries in a coarse-to-fine manner. Finally, wetarget the reuse of such representations to drive exploration toward an"interesting" type of diversity, for instance leveraging human guidance.Current approaches in state representation learning rely generally onmonolithic architectures which do not enable all these functionalities.Therefore, we present a novel technique to progressively construct a Hierarchyof Observation Latent Models for Exploration Stratification, called HOLMES.This technique couples the use of a dynamic modular model architecture forrepresentation learning with intrinsically-motivated goal exploration processes(IMGEPs). The paper shows results in the domain of automated discovery ofdiverse self-organized patterns, considering as testbed the experimentalframework from Reinke et al. (2019).</div></details><blockquote><p><strong><em>2020-04-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.04077v1><strong>Continual Learning with Gated Incremental Memories for sequential data processing</strong></a></p><p><em>Andrea Cossu, Antonio Carta, Davide Bacciu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ability to learn in dynamic, nonstationary environments withoutforgetting previous knowledge, also known as Continual Learning (CL), is a keyenabler for scalable and trustworthy deployments of adaptive solutions. Whilethe importance of continual learning is largely acknowledged in machine visionand reinforcement learning problems, this is mostly under-documented forsequence processing tasks. This work proposes a Recurrent Neural Network (RNN)model for CL that is able to deal with concept drift in input distributionwithout forgetting previously acquired knowledge. We also implement and test apopular CL approach, Elastic Weight Consolidation (EWC), on top of twodifferent types of RNNs. Finally, we compare the performances of our enhancedarchitecture against EWC and RNNs on a set of standard CL benchmarks, adaptedto the sequential data processing scenario. Results show the superiorperformance of our architecture and highlight the need for special solutionsdesigned to address CL in RNNs.</div></details><blockquote><p><strong><em>2020-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2003.09114v1><strong>Online Continual Learning on Sequences</strong></a></p><p><em>German I. Parisi, Vincenzo Lomonaco</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Online continual learning (OCL) refers to the ability of a system to learnover time from a continuous stream of data without having to revisit previouslyencountered training samples. Learning continually in a single data pass iscrucial for agents and robots operating in changing environments and requiredto acquire, fine-tune, and transfer increasingly complex representations fromnon-i.i.d. input distributions. Machine learning models that address OCL mustalleviate \textit{catastrophic forgetting} in which hidden representations aredisrupted or completely overwritten when learning from streams of novel input.In this chapter, we summarize and discuss recent deep learning models thataddress OCL on sequential input through the use (and combination) of synapticregularization, structural plasticity, and experience replay. Differentimplementations of replay have been proposed that alleviate catastrophicforgetting in connectionists architectures via the re-occurrence of (latentrepresentations of) input sequences and that functionally resemble mechanismsof hippocampal replay in the mammalian brain. Empirical evidence shows thatarchitectures endowed with experience replay typically outperform architectureswithout in (online) incremental learning tasks.</div></details><blockquote><p><strong><em>2020-03-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1912.10084v4><strong>SensAI+Expanse Adaptation on Human Behaviour Towards Emotional Valence Prediction</strong></a></p><p><em>Nuno A. C. Henriques, Helder Coelho, Leonel Garcia-Marques</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: An agent, artificial or human, must be continuously adjusting its behaviourin order to thrive in a more or less demanding environment. An artificial agentwith the ability to predict human emotional valence in a geospatial andtemporal context requires proper adaptation to its mobile device environmentwith resource consumption strict restrictions (e.g., power from battery). Thedeveloped distributed system includes a mobile device embodied agent (SensAI)plus Cloud-expanded (Expanse) cognition and memory resources. The system isdesigned with several adaptive mechanisms in a best effort for the agent tocope with its interacting humans and to be resilient on collecting data formachine learning towards prediction. These mechanisms encompasshomeostatic-like adjustments such as auto recovering from an unexpected failurein the mobile device, forgetting repeated data to save local memory, adjustingactions to a proper moment (e.g., notify only when human is interacting), andthe Expanse complementary learning algorithms&rsquo; parameters with autoadjustments. Regarding emotional valence prediction performance, results from acomparison study between state-of-the-art algorithms revealed Extreme GradientBoosting on average the best model for prediction with efficient energy use,and explainable using feature importance inspection. Therefore, this workcontributes with a smartphone sensing-based system, distributed in the Cloud,robust to unexpected behaviours from humans and the environment, able topredict emotional valence states with very good performance.</div></details><blockquote><p><strong><em>2020-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2002.09571v2><strong>Learning to Continually Learn</strong></a></p><p><em>Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, Nick Cheney</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual lifelong learning requires an agent or model to learn manysequentially ordered tasks, building on previous knowledge withoutcatastrophically forgetting it. Much work has gone towards preventing thedefault tendency of machine learning models to catastrophically forget, yetvirtually all such work involves manually-designed solutions to the problem. Weinstead advocate meta-learning a solution to catastrophic forgetting, allowingAI to learn to continually learn. Inspired by neuromodulatory processes in thebrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). Itdifferentiates through a sequential learning process to meta-learn anactivation-gating function that enables context-dependent selective activationwithin a deep neural network. Specifically, a neuromodulatory (NM) neuralnetwork gates the forward pass of another (otherwise normal) neural networkcalled the prediction learning network (PLN). The NM network also thusindirectly controls selective plasticity (i.e. the backward pass of) the PLN.ANML enables continual learning without catastrophic forgetting at scale: itproduces state-of-the-art continual learning performance, sequentially learningas many as 600 classes (over 9,000 SGD updates).</div></details><p><a href=http://arxiv.org/abs/2003.02097v1><strong>A Snooze-less User-Aware Notification System for Proactive Conversational Agents</strong></a></p><p><em>Yara Rizk, Vatche Isahagian, Merve Unuvar, Yasaman Khazaeni</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ubiquity of smart phones and electronic devices has placed a wealth ofinformation at the fingertips of consumers as well as creators of digitalcontent. This has led to millions of notifications being issued each secondfrom alerts about posted YouTube videos to tweets, emails and personalmessages. Adding work related notifications and we can see how quickly thenumber of notifications increases. Not only does this cause reducedproductivity and concentration but has also been shown to cause alert fatigue.This condition makes users desensitized to notifications, causing them toignore or miss important alerts. Depending on what domain users work in, thecost of missing a notification can vary from a mere inconvenience to life anddeath. Therefore, in this work, we propose an alert and notification frameworkthat intelligently issues, suppresses and aggregates notifications, based onevent severity, user preferences, or schedules, to minimize the need for usersto ignore, or snooze their notifications and potentially forget aboutaddressing important ones. Our framework can be deployed as a backend service,but is better suited to be integrated into proactive conversational agents, afield receiving a lot of attention with the digital transformation era, emailservices, news services and others. However, the main challenge lies indeveloping the right machine learning algorithms that can learn models from awide set of users while customizing these models to individual users&rsquo;preferences.</div></details><blockquote><p><strong><em>2020-01-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2001.09822v1><strong>Uncertainty-based Modulation for Lifelong Learning</strong></a></p><p><em>Andrew Brna, Ryan Brown, Patrick Connolly, Stephen Simons, Renee Shimizu, Mario Aguilar-Simon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The creation of machine learning algorithms for intelligent agents capable ofcontinuous, lifelong learning is a critical objective for algorithms beingdeployed on real-life systems in dynamic environments. Here we present analgorithm inspired by neuromodulatory mechanisms in the human brain thatintegrates and expands upon Stephen Grossberg's ground-breaking AdaptiveResonance Theory proposals. Specifically, it builds on the concept ofuncertainty, and employs a series of neuromodulatory mechanisms to enablecontinuous learning, including self-supervised and one-shot learning. Algorithmcomponents were evaluated in a series of benchmark experiments that demonstratestable learning without catastrophic forgetting. We also demonstrate thecritical role of developing these systems in a closed-loop manner where theenvironment and the agent's behaviors constrain and guide the learningprocess. To this end, we integrated the algorithm into an embodied simulateddrone agent. The experiments show that the algorithm is capable of continuouslearning of new tasks and under changed conditions with high classificationaccuracy (greater than 94 percent) in a virtual environment, withoutcatastrophic forgetting. The algorithm accepts high dimensional inputs from anystate-of-the-art detection and feature extraction algorithms, making it aflexible addition to existing systems. We also describe future developmentefforts focused on imbuing the algorithm with mechanisms to seek out newknowledge as well as employ a broader range of neuromodulatory processes.</div></details><blockquote><p><strong><em>2020-01-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1906.01120v3><strong>An Adaptive Random Path Selection Approach for Incremental Learning</strong></a></p><p><em>Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, Ming-Hsuan Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In a conventional supervised learning setting, a machine learning model hasaccess to examples of all object classes that are desired to be recognizedduring the inference stage. This results in a fixed model that lacks theflexibility to adapt to new learning tasks. In practical settings, learningtasks often arrive in a sequence and the models must continually learn toincrement their previously acquired knowledge. Existing incremental learningapproaches fall well below the state-of-the-art cumulative models that use alltraining classes at once. In this paper, we propose a random path selectionalgorithm, called Adaptive RPS-Net, that progressively chooses optimal pathsfor the new tasks while encouraging parameter sharing between tasks. Weintroduce a new network capacity measure that enables us to automaticallyswitch paths if the already used resources are saturated. Since the proposedpath-reuse strategy ensures forward knowledge transfer, our approach isefficient and has considerably less computation overhead. As an added novelty,the proposed model integrates knowledge distillation and retrospection alongwith the path selection strategy to overcome catastrophic forgetting. In orderto maintain an equilibrium between previous and newly acquired knowledge, wepropose a simple controller to dynamically balance the model plasticity.Through extensive experiments, we demonstrate that the Adaptive RPS-Net methodsurpasses the state-of-the-art performance for incremental learning and byutilizing parallel computation this method can run in constant time with nearlythe same efficiency as a conventional deep convolutional neural network.</div></details><p><a href=http://arxiv.org/abs/1410.1490v3><strong>Spaced Repetition and Mnemonics Enable Recall of Multiple Strong Passwords</strong></a></p><p><em>Jeremiah Blocki, Saranga Komanduri, Lorrie Cranor, Anupam Datta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We report on a user study that provides evidence that spaced repetition and aspecific mnemonic technique enable users to successfully recall multiple strongpasswords over time. Remote research participants were asked to memorize 4Person-Action-Object (PAO) stories where they chose a famous person from adrop-down list and were given machine-generated random action-object pairs.Users were also shown a photo of a scene and asked to imagine the PAO storytaking place in the scene (e.g., Bill Gates&mdash;swallowing&mdash;bike on a beach).Subsequently, they were asked to recall the action-object pairs when promptedwith the associated scene-person pairs following a spaced repetition scheduleover a period of 127+ days. While we evaluated several spaced repetitionschedules, the best results were obtained when users initially returned after12 hours and then in $1.5\times$ increasing intervals: 77% of the participantssuccessfully recalled all 4 stories in 10 tests over a period of 158 days. Muchof the forgetting happened in the first test period (12 hours): 89% ofparticipants who remembered their stories during the first test periodsuccessfully remembered them in every subsequent round. These findings, coupledwith recent results on naturally rehearsing password schemes, suggest that 4PAO stories could be used to create usable and strong passwords for 14sensitive accounts following this spaced repetition schedule, possibly with afew extra upfront rehearsals. In addition, we find that there is aninterference effect across multiple PAO stories: the recall rate of 100% (resp.90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAOstories) is significantly better than the recall rate for participants who wereasked to memorize 4 PAO stories. These findings yield concrete advice forimproving constructions of password management schemes and future user studies.</div></details><blockquote><p><strong><em>2019-12-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1907.13517v3><strong>An Enhanced Machine Learning-based Biometric Authentication System Using RR-Interval Framed Electrocardiograms</strong></a></p><p><em>Amang Song-Kyoo Kim, Chan Yeob Yeun, Paul D. Yoo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper is targeted in the area of biometric data enabled security systembased on the machine learning for the digital health. The disadvantages oftraditional authentication systems include the risks of forgetfulness, loss,and theft. Biometric authentication is therefore rapidly replacing traditionalauthentication methods and is becoming an everyday part of life. Theelectrocardiogram (ECG) was recently introduced as a biometric authenticationsystem suitable for security checks. The proposed authentication system helpsinvestigators studying ECG-based biometric authentication techniques to reshapeinput data by slicing based on the RR-interval, and defines the OverallPerformance (OP), which is the combined performance metric of multipleauthentication measures. We evaluated the performance of the proposed systemusing a confusion matrix and achieved up to 95% accuracy by compact dataanalysis. We also used the Amang ECG (amgecg) toolbox in MATLAB to investigatethe upper-range control limit (UCL) based on the mean square error, whichdirectly affects three authentication performance metrics: the accuracy, thenumber of accepted samples, and the OP. Using this approach, we found that theOP can be optimized by using a UCL of 0.0028, which indicates 61 acceptedsamples out of 70 and ensures that the proposed authentication system achievesan accuracy of 95%.</div></details><blockquote><p><strong><em>2019-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1907.00182v3><strong>Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges</strong></a></p><p><em>Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, Natalia Díaz-Rodríguez</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning (CL) is a particular machine learning paradigm where thedata distribution and learning objective changes through time, or where all thetraining data and objective criteria are never available at once. The evolutionof the learning process is modeled by a sequence of learning experiences wherethe goal is to be able to learn new skills all along the sequence withoutforgetting what has been previously learned. Continual learning also aims atthe same time at optimizing the memory, the computation power and the speedduring the learning process. An important challenge for machine learning is not necessarily findingsolutions that work in the real world but rather finding stable algorithms thatcan learn in real world. Hence, the ideal approach would be tackling the realworld in a embodied platform: an autonomous agent. Continual learning wouldthen be effective in an autonomous agent or robot, which would learnautonomously through time about the external world, and incrementally develop aset of complex skills and knowledge. Robotic agents have to learn to adapt and interact with their environmentusing a continuous stream of observations. Some recent approaches aim attackling continual learning for robotics, but most recent papers on continuallearning only experiment approaches in simulation or with static datasets.Unfortunately, the evaluation of those algorithms does not provide insights onwhether their solutions may help continual learning in the context of robotics.This paper aims at reviewing the existing state of the art of continuallearning, summarizing existing benchmarks and metrics, and proposing aframework for presenting and evaluating both robotics and non roboticsapproaches in a way that makes transfer between both fields easier.</div></details><blockquote><p><strong><em>2019-11-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1911.06565v1><strong>Feedback Linearization based on Gaussian Processes with event-triggered Online Learning</strong></a></p><p><em>Jonas Umlauft, Sandra Hirche</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Combining control engineering with nonparametric modeling techniques frommachine learning allows to control systems without analytic description usingdata-driven models. Most existing approaches separate learning, i.e. the systemidentification based on a fixed dataset, and control, i.e. the execution of themodel-based control law. This separation makes the performance highly sensitiveto the initial selection of training data and possibly requires very largedatasets. This article proposes a learning feedback linearizing control lawusing online closed-loop identification. The employed Gaussian process modelupdates its training data only if the model uncertainty becomes too large. Thisevent-triggered online learning ensures high data efficiency and therebyreduces the computational complexity, which is a major barrier for usingGaussian processes under real-time constraints. We propose safe forgettingstrategies of data points to adhere to budget constraint and to furtherincrease data-efficiency. We show asymptotic stability for the tracking errorunder the proposed event-triggering law and illustrate the effectiveidentification and control in simulation.</div></details><blockquote><p><strong><em>2019-11-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1911.04695v1><strong>Learning from the Past: Continual Meta-Learning via Bayesian Graph Modeling</strong></a></p><p><em>Yadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Mahsa Baktashmotlagh, Yang Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Meta-learning for few-shot learning allows a machine to leverage previouslyacquired knowledge as a prior, thus improving the performance on novel taskswith only small amounts of data. However, most mainstream models suffer fromcatastrophic forgetting and insufficient robustness issues, thereby failing tofully retain or exploit long-term knowledge while being prone to cause severeerror accumulation. In this paper, we propose a novel Continual Meta-Learningapproach with Bayesian Graph Neural Networks (CML-BGNN) that mathematicallyformulates meta-learning as continual learning of a sequence of tasks. Witheach task forming as a graph, the intra- and inter-task correlations can bewell preserved via message-passing and history transition. To remedytopological uncertainty from graph initialization, we utilize Bayes by Backpropstrategy that approximates the posterior distribution of task-specificparameters with amortized inference networks, which are seamlessly integratedinto the end-to-end edge learning. Extensive experiments conducted on theminiImageNet and tieredImageNet datasets demonstrate the effectiveness andefficiency of the proposed method, improving the performance by 42.8% comparedwith state-of-the-art on the miniImageNet 5-way 1-shot classification task.</div></details><blockquote><p><strong><em>2019-10-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1908.04742v3><strong>Online Continual Learning with Maximally Interfered Retrieval</strong></a></p><p><em>Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, Tinne Tuytelaars</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning, the setting where a learning agent is faced with a neverending stream of data, continues to be a great challenge for modern machinelearning systems. In particular the online or &ldquo;single-pass through the data"setting has gained attention recently as a natural setting that is difficult totackle. Methods based on replay, either generative or from a stored memory,have been shown to be effective approaches for continual learning, matching orexceeding the state of the art in a number of standard benchmarks. Theseapproaches typically rely on randomly selecting samples from the replay memoryor from a generative model, which is suboptimal. In this work, we consider acontrolled sampling of memories for replay. We retrieve the samples which aremost interfered, i.e. whose prediction will be most negatively impacted by theforeseen parameters update. We show a formulation for this sampling criterionin both the generative replay and the experience replay setting, producingconsistent gains in performance and greatly reduced forgetting. We release animplementation of our method athttps://github.com/optimass/Maximally_Interfered_Retrieval.</div></details><blockquote><p><strong><em>2019-10-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1910.02718v2><strong>Continual Learning in Neural Networks</strong></a></p><p><em>Rahaf Aljundi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial neural networks have exceeded human-level performance inaccomplishing several individual tasks (e.g. voice recognition, objectrecognition, and video games). However, such success remains modest compared tohuman intelligence that can learn and perform an unlimited number of tasks.Humans&rsquo; ability of learning and accumulating knowledge over their lifetime isan essential aspect of their intelligence. Continual machine learning aims at ahigher level of machine intelligence through providing the artificial agentswith the ability to learn online from a non-stationary and never-ending streamof data. A key component of such a never-ending learning process is to overcomethe catastrophic forgetting of previously seen data, a problem that neuralnetworks are well known to suffer from. The work described in this thesis hasbeen dedicated to the investigation of continual learning and solutions tomitigate the forgetting phenomena in neural networks. To approach the continuallearning problem, we first assume a task incremental setting where tasks arereceived one at a time and data from previous tasks are not stored. Since thetask incremental setting can&rsquo;t be assumed in all continual learning scenarios,we also study the more general online continual setting. We consider aninfinite stream of data drawn from a non-stationary distribution with asupervisory or self-supervisory training signal. The proposed methods in thisthesis have tackled important aspects of continual learning. They wereevaluated on different benchmarks and over various learning sequences. Advancesin the state of the art of continual learning have been shown and challengesfor bringing continual learning into application were critically identified.</div></details><blockquote><p><strong><em>2019-09-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1805.10768v3><strong>Deep Trustworthy Knowledge Tracing</strong></a></p><p><em>Heonseok Ha, Uiwon Hwang, Yongjun Hong, Jahee Jang, Sungroh Yoon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Knowledge tracing (KT), a key component of an intelligent tutoring system, isa machine learning technique that estimates the mastery level of a studentbased on his/her past performance. The objective of KT is to predict astudent&rsquo;s response to the next question. Compared with traditional KT models,deep learning-based KT (DLKT) models show better predictive performance becauseof the representation power of deep neural networks. Various methods have beenproposed to improve the performance of DLKT, but few studies have beenconducted on the reliability of DLKT. In this work, we claim that the existingDLKTs are not reliable in real education environments. To substantiate theclaim, we show limitations of DLKT from various perspectives such as knowledgestate update failure, catastrophic forgetting, and non-interpretability. Wethen propose a novel regularization to address these problems. The proposedmethod allows us to achieve trustworthy DLKT. In addition, the proposed modelwhich is trained on scenarios with forgetting can also be easily extended toscenarios without forgetting.</div></details><blockquote><p><strong><em>2019-09-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1909.07115v1><strong>AdaBoost-assisted Extreme Learning Machine for Efficient Online Sequential Classification</strong></a></p><p><em>Yi-Ta Chen, Yu-Chuan Chuang, An-Yeu, Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we propose an AdaBoost-assisted extreme learning machine forefficient online sequential classification (AOS-ELM). In order to achievebetter accuracy in online sequential learning scenarios, we utilize thecost-sensitive algorithm-AdaBoost, which diversifying the weak classifiers, andadding the forgetting mechanism, which stabilizing the performance during thetraining procedure. Hence, AOS-ELM adapts better to sequentially arrived datacompared with other voting based methods. The experiment results show AOS-ELMcan achieve 94.41% accuracy on MNIST dataset, which is the theoretical accuracybound performed by an original batch learning algorithm, AdaBoost-ELM.Moreover, with the forgetting mechanism, the standard deviation of accuracyduring the online sequential learning process is reduced to 8.26x.</div></details><blockquote><p><strong><em>2019-09-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1909.01380v1><strong>The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</strong></a></p><p><em>Elena Voita, Rico Sennrich, Ivan Titov</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We seek to understand how the representations of individual tokens and thestructure of the learned feature space evolve between layers in deep neuralnetworks under different learning objectives. We focus on the Transformers forour analysis as they have been shown effective on various tasks, includingmachine translation (MT), standard left-to-right language models (LM) andmasked language modeling (MLM). Previous work used black-box probing tasks toshow that the representations learned by the Transformer differ significantlydepending on the objective. In this work, we use canonical correlation analysisand mutual information estimators to study how information flows acrossTransformer layers and how this process depends on the choice of learningobjective. For example, as you go from bottom to top layers, information aboutthe past in left-to-right language models gets vanished and predictions aboutthe future get formed. In contrast, for MLM, representations initially acquireinformation about the context around the token, partially forgetting the tokenidentity and producing a more generalized token representation. The tokenidentity then gets recreated at the top MLM layers.</div></details><blockquote><p><strong><em>2019-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1904.09144v2><strong>The peculiar statistical mechanics of Optimal Learning Machines</strong></a></p><p><em>Matteo Marsili</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Optimal Learning Machines (OLM) are systems that extract maximallyinformative representation of the environment they are in contact with, or ofthe data they are presented. It has recently been suggested that these systemsare characterised by an exponential distribution of energy levels. In order tounderstand the peculiar properties of OLM within a broader framework, Iconsider an ensemble of optimisation problems over functions of many variables,part of which describe a sub-system and the rest account for its interactionwith a random environment. The number of states of the sub-system with a givenvalue of the objective function obeys a stretched exponential distribution,with exponent $\gamma$, and the interaction part is drawn at random from thesame distribution, independently for each configuration of the whole system.Systems with $\gamma=1$ then correspond to OLM, and we find that they sit atthe boundary between two regions with markedly different properties. For all$\gamma>0$ the system exhibits a freezing phase transition. The transition isdiscontinuous for $\gamma&lt;1$ and it is continuous for $\gamma>1$. The region$\gamma>1$ corresponds to learnable energy landscapes and the behaviour of thesub-system becomes predictable as the size of the environment exceeds acritical threshold. For $\gamma&lt;1$, instead, the energy landscape isunlearnable and the behaviour of the system becomes more and more unpredictableas the size of the environment increases. Sub-systems with $\gamma=1$ (OLM)feature a behaviour which is independent of the relative size of theenvironment. This is consistent with the expectation that efficientrepresentations should be largely independent of the level of detail of thedescription of the environment.</div></details><blockquote><p><strong><em>2019-06-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1903.00058v2><strong>Non-Parametric Adaptation for Neural Machine Translation</strong></a></p><p><em>Ankur Bapna, Orhan Firat</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural Networks trained with gradient descent are known to be susceptible tocatastrophic forgetting caused by parameter shift during the training process.In the context of Neural Machine Translation (NMT) this results in poorperformance on heterogeneous datasets and on sub-tasks like rare phrasetranslation. On the other hand, non-parametric approaches are immune toforgetting, perfectly complementing the generalization ability of NMT. However,attempts to combine non-parametric or retrieval based approaches with NMT haveonly been successful on narrow domains, possibly due to over-reliance onsentence level retrieval. We propose a novel n-gram level retrieval approachthat relies on local phrase level similarities, allowing us to retrieveneighbors that are useful for translation even when overall sentence similarityis low. We complement this with an expressive neural network, allowing ourmodel to extract information from the noisy retrieved context. We evaluate oursemi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT,JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets.The semi-parametric nature of our approach opens the door for non-parametricdomain adaptation, demonstrating strong inference-time adaptation performanceon new domains without the need for any parameter updates.</div></details><blockquote><p><strong><em>2019-06-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1808.01009v2><strong>Data-driven Local Control Design using Optimization and Machine Learning Techniques</strong></a></p><p><em>Stavros Karagiannopoulos, Petros Aristidou, Gabriela Hug</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The optimal control of distribution networks often requires monitoring andcommunication infrastructure, either centralized or distributed. However, mostof the current distribution systems lack this kind of infrastructure and relyon suboptimal, fit-and-forget, local controls to ensure the security of thenetwork. In this paper, we propose a data-driven algorithm that uses historicaldata, advanced optimization techniques, and machine learning methods, to designlocal controls that emulate the optimal behavior without the use of anycommunication. We demonstrate the performance of the optimized local control ona three-phase, unbalanced, low-voltage, distribution network. The results showthat our data-driven methodology clearly outperforms standard industry localcontrol and successfully imitates an optimal-power-flow-based control.</div></details><blockquote><p><strong><em>2019-05-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1905.13260v1><strong>Large Scale Incremental Learning</strong></a></p><p><em>Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Yun Fu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern machine learning suffers from catastrophic forgetting when learningnew classes incrementally. The performance dramatically degrades due to themissing data of old classes. Incremental learning methods have been proposed toretain the knowledge acquired from the old classes, by using knowledgedistilling and keeping a few exemplars from the old classes. However, thesemethods struggle to scale up to a large number of classes. We believe this isbecause of the combination of two factors: (a) the data imbalance between theold and new classes, and (b) the increasing number of visually similar classes.Distinguishing between an increasing number of visually similar classes isparticularly challenging, when the training data is unbalanced. We propose asimple and effective method to address this data imbalance issue. We found thatthe last fully connected layer has a strong bias towards the new classes, andthis bias can be corrected by a linear model. With two bias parameters, ourmethod performs remarkably well on two large datasets: ImageNet (1000 classes)and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithmsby 11.1% and 13.2% respectively.</div></details><blockquote><p><strong><em>2019-05-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1904.00310v3><strong>Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting</strong></a></p><p><em>Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Addressing catastrophic forgetting is one of the key challenges in continuallearning where machine learning systems are trained with sequential orstreaming tasks. Despite recent remarkable progress in state-of-the-art deeplearning, deep neural networks (DNNs) are still plagued with the catastrophicforgetting problem. This paper presents a conceptually simple yet general andeffective framework for handling catastrophic forgetting in continual learningwith DNNs. The proposed method consists of two components: a neural structureoptimization component and a parameter learning and/or fine-tuning component.By separating the explicit neural structure learning and the parameterestimation, not only is the proposed method capable of evolving neuralstructures in an intuitively meaningful way, but also shows strong capabilitiesof alleviating catastrophic forgetting in experiments. Furthermore, theproposed method outperforms all other baselines on the permuted MNIST dataset,the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continuallearning setting.</div></details><blockquote><p><strong><em>2019-04-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1904.10644v1><strong>Facilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients</strong></a></p><p><em>Yu Chen, Tom Diethe, Neil Lawrence</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual learning aims to enable machine learning models to learn a generalsolution space for past and future tasks in a sequential manner. Conventionalmodels tend to forget the knowledge of previous tasks while learning a newtask, a phenomenon known as catastrophic forgetting. When using Bayesian modelsin continual learning, knowledge from previous tasks can be retained in twoways: 1). posterior distributions over the parameters, containing the knowledgegained from inference in previous tasks, which then serve as the priors for thefollowing task; 2). coresets, containing knowledge of data distributions ofprevious tasks. Here, we show that Bayesian continual learning can befacilitated in terms of these two means through the use of natural gradientsand Stein gradients respectively.</div></details><blockquote><p><strong><em>2019-04-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1904.07734v1><strong>Three scenarios for continual learning</strong></a></p><p><em>Gido M. van de Ven, Andreas S. Tolias</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Standard artificial neural networks suffer from the well-known issue ofcatastrophic forgetting, making continual or lifelong learning difficult formachine learning. In recent years, numerous methods have been proposed forcontinual learning, but due to differences in evaluation protocols it isdifficult to directly compare their performance. To enable more structuredcomparisons, we describe three continual learning scenarios based on whether attest time task identity is provided and&ndash;in case it is not&ndash;whether it must beinferred. Any sequence of well-defined tasks can be performed according to eachscenario. Using the split and permuted MNIST task protocols, for each scenariowe carry out an extensive comparison of recently proposed continual learningmethods. We demonstrate substantial differences between the three scenarios interms of difficulty and in terms of how efficient different methods are. Inparticular, when task identity must be inferred (i.e., class incrementallearning), we find that regularization-based approaches (e.g., elastic weightconsolidation) fail and that replaying representations of previous experiencesseems required for solving this scenario.</div></details><blockquote><p><strong><em>2019-04-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1904.03178v1><strong>Reducing catastrophic forgetting when evolving neural networks</strong></a></p><p><em>Joseph Early</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A key stepping stone in the development of an artificial general intelligence(a machine that can perform any task), is the production of agents that canperform multiple tasks at once instead of just one. Unfortunately, canonicalmethods are very prone to catastrophic forgetting (CF) - the act of overwritingprevious knowledge about a task when learning a new task. Recent efforts havedeveloped techniques for overcoming CF in learning systems, but no attempt hasbeen made to apply these new techniques to evolutionary systems. This researchpresents a novel technique, weight protection, for reducing CF in evolutionarysystems by adapting a method from learning systems. It is used in conjunctionwith other evolutionary approaches for overcoming CF and is shown to beeffective at alleviating CF when applied to a suite of reinforcement learningtasks. It is speculated that this work could indicate the potential for a widerapplication of existing learning-based approaches to evolutionary systems andthat evolutionary techniques may be competitive with or better than learningsystems when it comes to reducing CF.</div></details><blockquote><p><strong><em>2019-03-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1811.09455v2><strong>Designing ground states of Hopfield networks for quantum state preparation</strong></a></p><p><em>Clemens Dlaska, Lukas M. Sieberer, Wolfgang Lechner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We present a protocol to store a polynomial number of arbitrary bit strings,encoded as spin configurations, in the approximately degenerate low-energymanifold of an all-to-all connected Ising spin glass. The iterative protocol isinspired by machine learning techniques utilizing $k$-local Hopfield networkstrained with $k$-local Hebbian learning and unlearning. The trained Hamiltonianis the basis of a quantum state-preparation scheme to create quantum many-bodysuperpositions with tunable squared amplitudes using resources available innear term experiments. We find that the number of configurations that can bestored in the ground states and thus turned into superposition scales with the$k$-locality of the Ising interaction.</div></details><blockquote><p><strong><em>2019-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1903.06070v1><strong>Attention-Based Structural-Plasticity</strong></a></p><p><em>Soheil Kolouri, Nicholas Ketz, Xinyun Zou, Jeffrey Krichmar, Praveen Pilly</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting/interference is a critical problem for lifelonglearning machines, which impedes the agents from maintaining their previouslylearned knowledge while learning new tasks. Neural networks, in particular,suffer plenty from the catastrophic forgetting phenomenon. Recently there hasbeen several efforts towards overcoming catastrophic forgetting in neuralnetworks. Here, we propose a biologically inspired method toward overcomingcatastrophic forgetting. Specifically, we define an attention-based selectiveplasticity of synapses based on the cholinergic neuromodulatory system in thebrain. We define synaptic importance parameters in addition to synaptic weightsand then use Hebbian learning in parallel with backpropagation algorithm tolearn synaptic importances in an online and seamless manner. We test ourproposed method on benchmark tasks including the Permuted MNIST and the SplitMNIST problems and show competitive performance compared to thestate-of-the-art methods.</div></details><blockquote><p><strong><em>2019-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1809.05922v2><strong>Memory Efficient Experience Replay for Streaming Learning</strong></a></p><p><em>Tyler L. Hayes, Nathan D. Cahill, Christopher Kanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In supervised machine learning, an agent is typically trained once and thendeployed. While this works well for static settings, robots often operate inchanging environments and must quickly learn new things from data streams. Inthis paradigm, known as streaming learning, a learner is trained online, in asingle pass, from a data stream that cannot be assumed to be independent andidentically distributed (iid). Streaming learning will cause conventional deepneural networks (DNNs) to fail for two reasons: 1) they need multiple passesthrough the entire dataset; and 2) non-iid data will cause catastrophicforgetting. An old fix to both of these issues is rehearsal. To learn a newexample, rehearsal mixes it with previous examples, and then this mixture isused to update the DNN. Full rehearsal is slow and memory intensive because itstores all previously observed examples, and its effectiveness for preventingcatastrophic forgetting has not been studied in modern DNNs. Here, we describethe ExStream algorithm for memory efficient rehearsal and compare it toalternatives. We find that full rehearsal can eliminate catastrophic forgettingin a variety of streaming learning settings, with ExStream performing wellusing far less memory and computation.</div></details><blockquote><p><strong><em>2019-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1903.03511v1><strong>Realizing Continual Learning through Modeling a Learning System as a Fiber Bundle</strong></a></p><p><em>Zhenfeng Cao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A human brain is capable of continual learning by nature; however the currentmainstream deep neural networks suffer from a phenomenon named catastrophicforgetting (i.e., learning a new set of patterns suddenly and completely wouldresult in fully forgetting what has already been learned). In this paper wepropose a generic learning model, which regards a learning system as a fiberbundle. By comparing the learning performance of our model with conventionalones whose neural networks are multilayer perceptrons through a variety ofmachine-learning experiments, we found our proposed model not only enjoys adistinguished capability of continual learning but also bears a highinformation capacity. In addition, we found in some learning scenarios thelearning performance can be further enhanced by making the learning time-awareto mimic the episodic memory in human brain. Last but not least, we found thatthe properties of forgetting in our model correspond well to those of humanmemory. This work may shed light on how a human brain learns.</div></details><blockquote><p><strong><em>2019-02-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1802.07569v4><strong>Continual Lifelong Learning with Neural Networks: A Review</strong></a></p><p><em>German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, Stefan Wermter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humans and animals have the ability to continually acquire, fine-tune, andtransfer knowledge and skills throughout their lifespan. This ability, referredto as lifelong learning, is mediated by a rich set of neurocognitive mechanismsthat together contribute to the development and specialization of oursensorimotor skills as well as to long-term memory consolidation and retrieval.Consequently, lifelong learning capabilities are crucial for autonomous agentsinteracting in the real world and processing continuous streams of information.However, lifelong learning remains a long-standing challenge for machinelearning and neural network models since the continual acquisition ofincrementally available information from non-stationary data distributionsgenerally leads to catastrophic forgetting or interference. This limitationrepresents a major drawback for state-of-the-art deep neural network modelsthat typically learn representations from stationary batches of training data,thus without accounting for situations in which information becomesincrementally available over time. In this review, we critically summarize themain challenges linked to lifelong learning for artificial learning systems andcompare existing neural network approaches that alleviate, to differentextents, catastrophic forgetting. We discuss well-established and emergingresearch motivated by lifelong learning factors in biological systems such asstructural plasticity, memory replay, curriculum and transfer learning,intrinsic motivation, and multisensory integration.</div></details><blockquote><p><strong><em>2018-12-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1806.02942v3><strong>SupportNet: solving catastrophic forgetting in class incremental learning with support data</strong></a></p><p><em>Yu Li, Zhongxiao Li, Lizhong Ding, Yijie Pan, Chao Huang, Yuhui Hu, Wei Chen, Xin Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A plain well-trained deep learning model often does not have the ability tolearn new knowledge without forgetting the previously learned knowledge, whichis known as catastrophic forgetting. Here we propose a novel method,SupportNet, to efficiently and effectively solve the catastrophic forgettingproblem in the class incremental learning scenario. SupportNet combines thestrength of deep learning and support vector machine (SVM), where SVM is usedto identify the support data from the old data, which are fed to the deeplearning model together with the new data for further training so that themodel can review the essential information of the old data when learning thenew information. Two powerful consolidation regularizers are applied tostabilize the learned representation and ensure the robustness of the learnedmodel. We validate our method with comprehensive experiments on various tasks,which show that SupportNet drastically outperforms the state-of-the-artincremental learning methods and even reaches similar performance as the deeplearning model trained from scratch on both old and new data. Our program isaccessible at: <a href=https://github.com/lykaust15/SupportNet>https://github.com/lykaust15/SupportNet</a></div></details><blockquote><p><strong><em>2018-12-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1812.01716v1><strong>Learning to Unlearn: Building Immunity to Dataset Bias in Medical Imaging Studies</strong></a></p><p><em>Ahmed Ashraf, Shehroz Khan, Nikhil Bhagwat, Mallar Chakravarty, Babak Taati</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Medical imaging machine learning algorithms are usually evaluated on a singledataset. Although training and testing are performed on different subsets ofthe dataset, models built on one study show limited capability to generalize toother studies. While database bias has been recognized as a serious problem inthe computer vision community, it has remained largely unnoticed in medicalimaging research. Transfer learning thus remains confined to the re-use offeature representations requiring re-training on the new dataset. As a result,machine learning models do not generalize even when trained on imaging datasetsthat were captured to study the same variable of interest. The ability totransfer knowledge gleaned from one study to another, without the need forre-training, if possible, would provide reassurance that the models arelearning knowledge fundamental to the problem under study instead of latchingonto the idiosyncracies of a dataset. In this paper, we situate the problem ofdataset bias in the context of medical imaging studies. We show empiricalevidence that such a problem exists in medical datasets. We then present aframework to unlearn study membership as a means to handle the problem ofdatabase bias. Our main idea is to take the data from the original featurespace to an intermediate space where the data points are indistinguishable interms of which study they come from, while maintaining the recognitioncapability with respect to the variable of interest. This will promote modelswhich learn the more general properties of the etiology under study instead ofaligning to dataset-specific peculiarities. Essentially, our proposed modellearns to unlearn the dataset bias.</div></details><blockquote><p><strong><em>2018-11-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1809.04933v2><strong>Identifying Real Estate Opportunities using Machine Learning</strong></a></p><p><em>Alejandro Baldominos, Iván Blanco, Antonio José Moreno, Rubén Iturrarte, Óscar Bernárdez, Carlos Afonso</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The real estate market is exposed to many fluctuations in prices because ofexisting correlations with many variables, some of which cannot be controlledor might even be unknown. Housing prices can increase rapidly (or in somecases, also drop very fast), yet the numerous listings available online wherehouses are sold or rented are not likely to be updated that often. In somecases, individuals interested in selling a house (or apartment) might includeit in some online listing, and forget about updating the price. In other cases,some individuals might be interested in deliberately setting a price below themarket price in order to sell the home faster, for various reasons. In thispaper, we aim at developing a machine learning application that identifiesopportunities in the real estate market in real time, i.e., houses that arelisted with a price substantially below the market price. This program can beuseful for investors interested in the housing market. We have focused in a usecase considering real estate assets located in the Salamanca district in Madrid(Spain) and listed in the most relevant Spanish online site for home sales andrentals. The application is formally implemented as a regression problem thattries to estimate the market price of a house given features retrieved frompublic online listings. For building this application, we have performed afeature engineering stage in order to discover relevant features that allowsfor attaining a high predictive performance. Several machine learningalgorithms have been tested, including regression trees, k-nearest neighbors,support vector machines and neural networks, identifying advantages andhandicaps of each of them.</div></details><blockquote><p><strong><em>2018-11-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1811.02361v1><strong>Kalman Filter Modifier for Neural Networks in Non-stationary Environments</strong></a></p><p><em>Honglin Li, Frieder Ganz, Shirin Enshaeifar, Payam Barnaghi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Learning in a non-stationary environment is an inevitable problem whenapplying machine learning algorithm to real world environment. Learning newtasks without forgetting the previous knowledge is a challenge issue in machinelearning. We propose a Kalman Filter based modifier to maintain the performanceof Neural Network models under non-stationary environments. The result showsthat our proposed model can preserve the key information and adapts better tothe changes. The accuracy of proposed model decreases by 0.4% in ourexperiments, while the accuracy of conventional model decreases by 90% in thedrifts environment.</div></details><blockquote><p><strong><em>2018-10-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1810.12546v1><strong>Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks</strong></a></p><p><em>Biao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, Huiji Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we propose an additionsubtraction twin-gated recurrent network(ATR) to simplify neural machine translation. The recurrent units of ATR areheavily simplified to have the smallest number of weight matrices among unitsof all existing gated RNNs. With the simple addition and subtraction operation,we introduce a twin-gated mechanism to build input and forget gates which arehighly correlated. Despite this simplification, the essential non-linearitiesand capability of modeling long-distance dependencies are preserved.Additionally, the proposed ATR is more transparent than LSTM/GRU due to thesimplification. Forward self-attention can be easily established in ATR, whichmakes the proposed network interpretable. Experiments on WMT14 translationtasks demonstrate that ATR-based neural machine translation can yieldcompetitive performance on English- German and English-French language pairs interms of both translation quality and speed. Further experiments on NISTChinese-English translation, natural language inference and Chinese wordsegmentation verify the generality and applicability of ATR on differentnatural language processing tasks.</div></details><blockquote><p><strong><em>2018-09-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1805.11154v2><strong>Refining Source Representations with Relation Networks for Neural Machine Translation</strong></a></p><p><em>Wen Zhang, Jiawei Hu, Yang Feng, Qun Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although neural machine translation with the encoder-decoder framework hasachieved great success recently, it still suffers drawbacks of forgettingdistant information, which is an inherent disadvantage of recurrent neuralnetwork structure, and disregarding relationship between source words duringencoding step. Whereas in practice, the former information and relationship areoften useful in current step. We target on solving these problems and thusintroduce relation networks to learn better representations of the source. Therelation networks are able to facilitate memorization capability of recurrentneural network via associating source words with each other, this would alsohelp retain their relationships. Then the source representations and all therelations are fed into the attention component together while decoding, withthe main encoder-decoder framework unchanged. Experiments on several datasetsshow that our method can improve the translation performance significantly overthe conventional encoder-decoder model and even outperform the approachinvolving supervised syntactic knowledge.</div></details><blockquote><p><strong><em>2018-07-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1807.10211v1><strong>Robust Tracking via Weighted Online Extreme Learning Machine</strong></a></p><p><em>Jing Zhang, Huibing Wang, Yonggong Ren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The tracking method based on the extreme learning machine (ELM) is efficientand effective. ELM randomly generates input weights and biases in the hiddenlayer, and then calculates and computes the output weights by reducing theiterative solution to the problem of linear equations. Therefore, ELM offersthe satisfying classification performance and fast training time than otherdiscriminative models in tracking. However, the original ELM method oftensuffers from the problem of the imbalanced classification distribution, whichis caused by few target objects, leading to under-fitting and more backgroundsamples leading to over-fitting. Worse still, it reduces the robustness oftracking under special conditions including occlusion, illumination, etc. Toaddress above problems, in this paper, we present a robust tracking algorithm.First, we introduce the local weight matrix that is the dynamic creation fromthe data distribution at the current frame in the original ELM so as to balancebetween the empirical and structure risk, and fully learn the target object toenhance the classification performance. Second, we improve it to theincremental learning method ensuring tracking real-time and efficient. Finally,the forgetting factor is used to strengthen the robustness for changing of theclassification distribution with time. Meanwhile, we propose a novel optimizedmethod to obtain the optimal sample as the target object, which avoids trackingdrift resulting from noisy samples. Therefore, our tracking method can fullylearn both of the target object and background information to enhance thetracking performance, and it is evaluated in 20 challenge image sequences withdifferent attributes including illumination, occlusion, deformation, etc.,which achieves better performance than several state-of-the-art methods interms of effectiveness and robustness.</div></details><blockquote><p><strong><em>2018-07-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1807.08942v1><strong>Example Mining for Incremental Learning in Medical Imaging</strong></a></p><p><em>Pratyush Kumar, Muktabh Mayank Srivastava</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Incremental Learning is well known machine learning approach wherein theweights of the learned model are dynamically and gradually updated togeneralize on new unseen data without forgetting the existing knowledge.Incremental learning proves to be time as well as resource-efficient solutionfor deployment of deep learning algorithms in real world as the model canautomatically and dynamically adapt to new data as and when annotated databecomes available. The development and deployment of Computer Aided Diagnosis(CAD) tools in medical domain is another scenario, where incremental learningbecomes very crucial as collection and annotation of a comprehensive datasetspanning over multiple pathologies and imaging machines might take years.However, not much has so far been explored in this direction. In the currentwork, we propose a robust and efficient method for incremental learning inmedical imaging domain. Our approach makes use of Hard Example Mining technique(which is commonly used as a solution to heavy class imbalance) toautomatically select a subset of dataset to fine-tune the existing networkweights such that it adapts to new data while retaining existing knowledge. Wedevelop our approach for incremental learning of our already under test modelfor detecting dental caries. Further, we apply our approach to one publiclyavailable dataset and demonstrate that our approach reaches the accuracy oftraining on entire dataset at once, while availing the benefits of incrementallearning scenario.</div></details><blockquote><p><strong><em>2018-07-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1807.03486v2><strong>An Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm</strong></a></p><p><em>Shin Kamada, Takumi Ichimura</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep Belief Network (DBN) has a deep architecture that represents multiplefeatures of input patterns hierarchically with the pre-trained RestrictedBoltzmann Machines (RBM). A traditional RBM or DBN model cannot change itsnetwork structure during the learning phase. Our proposed adaptive learningmethod can discover the optimal number of hidden neurons and weights and/orlayers according to the input space. The model is an important method to takeaccount of the computational cost and the model stability. The regularities tohold the sparse structure of network is considerable problem, since theextraction of explicit knowledge from the trained network should be required.In our previous research, we have developed the hybrid method of adaptivestructural learning method of RBM and Learning Forgetting method to the trainedRBM. In this paper, we propose the adaptive learning method of DBN that candetermine the optimal number of layers during the learning. We evaluated ourproposed model on some benchmark data sets.</div></details><blockquote><p><strong><em>2018-05-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1709.03980v3><strong>Refining Source Representations with Relation Networks for Neural Machine Translation</strong></a></p><p><em>Wen Zhang, Jiawei Hu, Yang Feng, Qun Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although neural machine translation (NMT) with the encoder-decoder frameworkhas achieved great success in recent times, it still suffers from somedrawbacks: RNNs tend to forget old information which is often useful and theencoder only operates through words without considering word relationship. Tosolve these problems, we introduce a relation networks (RN) into NMT to refinethe encoding representations of the source. In our method, the RN firstaugments the representation of each source word with its neighbors and reasonsall the possible pairwise relations between them. Then the sourcerepresentations and all the relations are fed to the attention module and thedecoder together, keeping the main encoder-decoder architecture unchanged.Experiments on two Chinese-to-English data sets in different scales both showthat our method can outperform the competitive baselines significantly.</div></details><blockquote><p><strong><em>2018-05-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1803.01364v2><strong>SAFE: Spectral Evolution Analysis Feature Extraction for Non-Stationary Time Series Prediction</strong></a></p><p><em>Arief Koesdwiady, Fakhri Karray</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper presents a practical approach for detecting non-stationarity intime series prediction. This method is called SAFE and works by monitoring theevolution of the spectral contents of time series through a distance function.This method is designed to work in combination with state-of-the-art machinelearning methods in real time by informing the online predictors to performnecessary adaptation when a non-stationarity presents. We also propose analgorithm to proportionally include some past data in the adaption process toovercome the Catastrophic Forgetting problem. To validate our hypothesis andtest the effectiveness of our approach, we present comprehensive experiments indifferent elements of the approach involving artificial and real-worlddatasets. The experiments show that the proposed method is able tosignificantly save computational resources in term of processor or GPU cycleswhile maintaining high prediction performances.</div></details><blockquote><p><strong><em>2018-05-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1802.08250v2><strong>SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural Networks by Selective Network Augmentation</strong></a></p><p><em>Abel S. Zacarias, Luís A. Alexandre</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Lifelong learning aims to develop machine learning systems that can learn newtasks while preserving the performance on previous learned tasks. In this paperwe present a method to overcome catastrophic forgetting on convolutional neuralnetworks, that learns new tasks and preserves the performance on old taskswithout accessing the data of the original model, by selective networkaugmentation. The experiment results showed that SeNA-CNN, in some scenarios,outperforms the state-of-art Learning without Forgetting algorithm. Resultsalso showed that in some situations it is better to use SeNA-CNN instead oftraining a neural network using isolated learning.</div></details><blockquote><p><strong><em>2018-04-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1804.09458v1><strong>Dynamic Few-Shot Visual Learning without Forgetting</strong></a></p><p><em>Spyros Gidaris, Nikos Komodakis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The human visual system has the remarkably ability to be able to effortlesslylearn novel concepts from only a few examples. Mimicking the same behavior onmachine learning vision systems is an interesting and very challenging researchproblem with many practical advantages on real world vision applications. Inthis context, the goal of our work is to devise a few-shot visual learningsystem that during test time it will be able to efficiently learn novelcategories from only a few training data while at the same time it will notforget the initial categories on which it was trained (here called basecategories). To achieve that goal we propose (a) to extend an objectrecognition system with an attention based few-shot classification weightgenerator, and (b) to redesign the classifier of a ConvNet model as the cosinesimilarity function between feature representations and classification weightvectors. The latter, apart from unifying the recognition of both novel and basecategories, it also leads to feature representations that generalize better on"unseen" categories. We extensively evaluate our approach on Mini-ImageNetwhere we manage to improve the prior state-of-the-art on few-shot recognition(i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settingsrespectively) while at the same time we do not sacrifice any accuracy on thebase categories, which is a characteristic that most prior approaches lack.Finally, we apply our approach on the recently introduced few-shot benchmark ofBharath and Girshick [4] where we also achieve state-of-the-art results. Thecode and models of our paper will be published on:https://github.com/gidariss/FewShotWithoutForgetting</div></details><blockquote><p><strong><em>2018-04-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1804.04286v1><strong>Combating catastrophic forgetting with developmental compression</strong></a></p><p><em>Shawn L. E. Beaulieu, Sam Kriegman, Josh C. Bongard</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generally intelligent agents exhibit successful behavior across problems inseveral settings. Endemic in approaches to realize such intelligence inmachines is catastrophic forgetting: sequential learning corrupts knowledgeobtained earlier in the sequence, or tasks antagonistically compete for systemresources. Methods for obviating catastrophic forgetting have sought toidentify and preserve features of the system necessary to solve one problemwhen learning to solve another, or to enforce modularity such that minimallyoverlapping sub-functions contain task specific knowledge. While successful,both approaches scale poorly because they require larger architectures as thenumber of training instances grows, causing different parts of the system tospecialize for separate subsets of the data. Here we present a method foraddressing catastrophic forgetting called developmental compression. Itexploits the mild impacts of developmental mutations to lessen adverse changesto previously-evolved capabilities and `compresses&rsquo; specialized neural networksinto a generalized one. In the absence of domain knowledge, developmentalcompression produces systems that avoid overt specialization, alleviating theneed to engineer a bespoke system for every task permutation and suggestingbetter scalability than existing approaches. We validate this method on a robotcontrol problem and hope to extend this approach to other machine learningdomains in the future.</div></details><blockquote><p><strong><em>2018-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1803.00111v1><strong>Predicting Recall Probability to Adaptively Prioritize Study</strong></a></p><p><em>Shane Mooney, Karen Sun, Eric Bomgardner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Students have a limited time to study and are typically ineffective atallocating study time. Machine-directed study strategies that identify whichitems need reinforcement and dictate the spacing of repetition have been shownto help students optimize mastery (Mozer & Lindsey 2017). The large volume ofresearch on this matter is typically conducted in constructed experimentalsettings with fixed instruction, content, and scheduling; in contrast, we aimto develop methods that can address any demographic, subject matter, or studyschedule. We show two methods that model item-specific recall probability foruse in a discrepancy-reduction instruction strategy. The first model predictsitem recall probability using a multiple logistic regression (MLR) model basedon previous answer correctness and temporal spacing of study. Prompted byliterature suggesting that forgetting is better modeled by the power law thanan exponential decay (Wickelgren 1974), we compare the MLR approach with aRecurrent Power Law (RPL) model which adaptively fits a forgetting curve. Wethen discuss the performance of these models against study datasets comprisedof millions of answers and show that the RPL approach is more accurate andflexible than the MLR model. Finally, we give an overview of promising futureapproaches to knowledge modeling.</div></details><blockquote><p><strong><em>2017-11-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1708.02072v4><strong>Measuring Catastrophic Forgetting in Neural Networks</strong></a></p><p><em>Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, Christopher Kanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep neural networks are used in many state-of-the-art systems for machineperception. Once a network is trained to do a specific task, e.g., birdclassification, it cannot easily be trained to do new tasks, e.g.,incrementally learning to recognize additional bird species or learning anentirely different task such as flower recognition. When new tasks are added,typical deep neural networks are prone to catastrophically forgetting previoustasks. Networks that are capable of assimilating new information incrementally,much like how humans form new memories over time, will be more efficient thanre-training the model from scratch each time a new task needs to be learned.There have been multiple attempts to develop schemes that mitigate catastrophicforgetting, but these methods have not been directly compared, the tests usedto evaluate them vary considerably, and these methods have only been evaluatedon small-scale problems (e.g., MNIST). In this paper, we introduce new metricsand benchmarks for directly comparing five different mechanisms designed tomitigate catastrophic forgetting in neural networks: regularization,ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments onreal-world images and sounds show that the mechanism(s) that are critical foroptimal performance vary based on the incremental training paradigm and type ofdata being used, but they all demonstrate that the catastrophic forgettingproblem has yet to be solved.</div></details><blockquote><p><strong><em>2017-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1503.04069v2><strong>LSTM: A Search Space Odyssey</strong></a></p><p><em>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random search,and their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs ($\approx 15$ yearsof CPU time), which makes our study the largest of its kind on LSTM networks.Our results show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.</div></details><blockquote><p><strong><em>2017-09-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1705.02609v2><strong>Emptiness Problems for Distributed Automata</strong></a></p><p><em>Antti Kuusisto, Fabian Reiter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We investigate the decidability of the emptiness problem for three classes ofdistributed automata. These devices operate on finite directed graphs, actingas networks of identical finite-state machines that communicate in an infinitesequence of synchronous rounds. The problem is shown to be decidable inLogSpace for a class of forgetful automata, where the nodes see the messagesreceived from their neighbors but cannot remember their own state. Whenrestricted to the appropriate families of graphs, these forgetful automata areequivalent to classical finite word automata, but strictly more expressive thanfinite tree automata. On the other hand, we also show that the emptinessproblem is undecidable in general. This already holds for two heavilyrestricted classes of distributed automata: those that reject immediately ifthey receive more than one message per round, and those whose state diagrammust be acyclic except for self-loops.</div></details><blockquote><p><strong><em>2017-07-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1705.00744v2><strong>A Strategy for an Uncompromising Incremental Learner</strong></a></p><p><em>Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan, Baoxin Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multi-class supervised learning systems require the knowledge of the entirerange of labels they predict. Often when learnt incrementally, they suffer fromcatastrophic forgetting. To avoid this, generous leeways have to be made to thephilosophy of incremental learning that either forces a part of the machine tonot learn, or to retrain the machine again with a selection of the historicdata. While these hacks work to various degrees, they do not adhere to thespirit of incremental learning. In this article, we redefine incrementallearning with stringent conditions that do not allow for any undesirablerelaxations and assumptions. We design a strategy involving generative modelsand the distillation of dark knowledge as a means of hallucinating data alongwith appropriate targets from past distributions. We call this technique,phantom sampling.We show that phantom sampling helps avoid catastrophicforgetting during incremental learning. Using an implementation based on deepneural networks, we demonstrate that phantom sampling dramatically avoidscatastrophic forgetting. We apply these strategies to competitive multi-classincremental learning of deep neural networks. Using various benchmark datasetsand through our strategy, we demonstrate that strict incremental learning couldbe achieved. We further put our strategy to test on challenging cases,including cross-domain increments and incrementing on a novel label space. Wealso propose a trivial extension to unbounded-continual learning and identifypotential for future development.</div></details><blockquote><p><strong><em>2015-09-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1509.03185v1><strong>Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine</strong></a></p><p><em>Andrew J. R. Simpson</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In a recent article we described a new type of deep neural network - aPerpetual Learning Machine (PLM) - which is capable of learning &lsquo;on the fly&rsquo;like a brain by existing in a state of Perpetual Stochastic Gradient Descent(PSGD). Here, by simulating the process of practice, we demonstrate bothselective memory and selective forgetting when we introduce statistical recallbiases during PSGD. Frequently recalled memories are remembered, whilstmemories recalled rarely are forgotten. This results in a &lsquo;use it or lose it&rsquo;stimulus driven memory process that is similar to human memory.</div></details><blockquote><p><strong><em>2015-09-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1412.5732v2><strong>Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data</strong></a></p><p><em>Changsheng Li, Fan Wei, Weishan Dong, Qingshan Liu, Xiangfeng Wang, Xin Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Online multiple-output regression is an important machine learning techniquefor modeling, predicting, and compressing multi-dimensional correlated datastreams. In this paper, we propose a novel online multiple-output regressionmethod, called MORES, for stream data. MORES can \emph{dynamically} learn thestructure of the coefficients change in each update step to facilitate themodel&rsquo;s continuous refinement. We observe that limited expressive ability ofthe regression model, especially in the preliminary stage of online update,often leads to the variables in the residual errors being dependent. In lightof this point, MORES intends to \emph{dynamically} learn and leverage thestructure of the residual errors to improve the prediction accuracy. Moreover,we define three statistical variables to \emph{exactly} represent all the seensamples for \emph{incrementally} calculating prediction loss in each onlineupdate round, which can avoid loading all the training data into memory forupdating model, and also effectively prevent drastic fluctuation of the modelin the presence of noise. Furthermore, we introduce a forgetting factor to setdifferent weights on samples so as to track the data streams&rsquo; evolvingcharacteristics quickly from the latest samples. Experiments on one syntheticdataset and three real-world datasets validate the effectiveness of theproposed method. In addition, the update speed of MORES is at least 2000samples processed per second on the three real-world datasets, more than 15times faster than the state-of-the-art online learning algorithm.</div></details><blockquote><p><strong><em>2015-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1312.6211v3><strong>An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</strong></a></p><p><em>Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Catastrophic forgetting is a problem faced by many machine learning modelsand algorithms. When trained on one task, then trained on a second task, manymachine learning models &ldquo;forget&rdquo; how to perform the first task. This is widelybelieved to be a serious problem for neural networks. Here, we investigate theextent to which the catastrophic forgetting problem occurs for modern neuralnetworks, comparing both established and recent gradient-based trainingalgorithms and activation functions. We also examine the effect of therelationship between the first task and the second task on catastrophicforgetting. We find that it is always best to train using the dropoutalgorithm&ndash;the dropout algorithm is consistently best at adapting to the newtask, remembering the old task, and has the best tradeoff curve between thesetwo extremes. We find that different tasks and relationships between tasksresult in very different rankings of activation function performance. Thissuggests the choice of activation function should always be cross-validated.</div></details><blockquote><p><strong><em>2014-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1402.5988v2><strong>Incremental Learning of Event Definitions with Inductive Logic Programming</strong></a></p><p><em>Nikos Katzouris, Alexander Artikis, George Paliouras</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Event recognition systems rely on properly engineered knowledge bases ofevent definitions to infer occurrences of events in time. The manualdevelopment of such knowledge is a tedious and error-prone task, thusevent-based applications may benefit from automated knowledge constructiontechniques, such as Inductive Logic Programming (ILP), which combines machinelearning with the declarative and formal semantics of First-Order Logic.However, learning temporal logical formalisms, which are typically utilized bylogic-based Event Recognition systems is a challenging task, which most ILPsystems cannot fully undertake. In addition, event-based data is usuallymassive and collected at different times and under various circumstances.Ideally, systems that learn from temporal data should be able to operate in anincremental mode, that is, revise prior constructed knowledge in the face ofnew evidence. Most ILP systems are batch learners, in the sense that in orderto account for new evidence they have no alternative but to forget pastknowledge and learn from scratch. Given the increased inherent complexity ofILP and the volumes of real-life temporal data, this results to algorithms thatscale poorly. In this work we present an incremental method for learning andrevising event-based knowledge, in the form of Event Calculus programs. Theproposed algorithm relies on abductive-inductive learning and comprises ascalable clause refinement methodology, based on a compressive summarization ofclause coverage in a stream of examples. We present an empirical evaluation ofour approach on real and synthetic data from activity recognition and citytransport applications.</div></details><blockquote><p><strong><em>2013-12-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1312.5734v1><strong>Time-varying Learning and Content Analytics via Sparse Factor Analysis</strong></a></p><p><em>Andrew S. Lan, Christoph Studer, Richard G. Baraniuk</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner&rsquo;s concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.</div></details><blockquote><p><strong><em>2012-06-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1112.2988v2><strong>Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns</strong></a></p><p><em>Tsvi Achler</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Matching animal-like flexibility in recognition and the ability to quicklyincorporate new information remains difficult. Limits are yet to be adequatelyaddressed in neural models and recognition algorithms. This work proposes aconfiguration for recognition that maintains the same function of conventionalalgorithms but avoids combinatorial problems. Feedforward recognitionalgorithms such as classical artificial neural networks and machine learningalgorithms are known to be subject to catastrophic interference and forgetting.Modifying or learning new information (associations between patterns andlabels) causes loss of previously learned information. I demonstrate usingmathematical analysis how supervised generative models, with feedforward andfeedback connections, can emulate feedforward algorithms yet avoid catastrophicinterference and forgetting. Learned information in generative models is storedin a more intuitive form that represents the fixed points or solutions of thenetwork and moreover displays similar difficulties as cognitive phenomena.Brain-like capabilities and limits associated with generative models suggestthe brain may perform recognition and store information using a similarapproach. Because of the central role of recognition, progress understandingthe underlying principles may reveal significant insight on how to better studyand integrate with the brain.</div></details><blockquote><p><strong><em>2005-10-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/astro-ph/0410037v2><strong>Monster Redshift Surveys through Dispersive Slitless Imaging: The Baryon Oscillation Probe</strong></a></p><p><em>Karl Glazebrook, Ivan Baldry, Warren Moos, Jeff Kruk, Stephan McCandliss</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Wide-field imaging from space should not forget the dispersive dimension. Weconsider the capability of space-based imaging with a slitless grism: becauseof the low near-infrared background in space and the high sky-density of highredshift emission line galaxies this makes for a very powerful redshift machinewith no moving parts. A small 1m space telescope with a 0.5 degree field ofview could measure redshifts for 10^7 galaxies at 0.5&lt;z&lt;2 per year, this is aMIDEX class concept which we have dubbed `The Baryon Oscillation Probe&rsquo; as theprimary science case would be constraining dark energy evolution viameasurement of the baryonic oscillations in the galaxy power spectrum. Theseideas are generalizable to other missions such as SNAP and DESTINY.</div></details></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>