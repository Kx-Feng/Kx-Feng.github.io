<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="2024-03-20
MELTing point: Mobile Evaluation of Language Transformers
Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
abstractabstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence&rsquo;&rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs)."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="2024-03-20
MELTing point: Mobile Evaluation of Language Transformers
Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
abstractabstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence&rsquo;&rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs)."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/arxiv_papers/data_top_llm_privacy_none/"><meta property="article:section" content="docs"><title>Data Top Llm Privacy None | Xiaoyan Feng</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=http://localhost:1313/docs/arxiv_papers/data_top_llm_privacy_none/><link rel=stylesheet href=/book.min.16628528a7a2a88d96389ec90a07d3f66879aff48e8cbc1f53ddc5b7ddb7ab85.css integrity="sha256-FmKFKKeiqI2WOJ7JCgfT9mh5r/SOjLwfU93Ft923q4U=" crossorigin=anonymous><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.9f674b6230a4fba0f1529d8771a004997eb45f4d74eea985b65dc83c9c60ab68.js integrity="sha256-n2dLYjCk+6DxUp2HcaAEmX60X0107qmFtl3IPJxgq2g=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Xiaoyan Feng</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/about_me/>Home</a></li><li><span>Arxiv Papers</span><ul><li><a href=/docs/arxiv_papers/llm_copyright/>LLM with Copyright</a></li><li><a href=/docs/arxiv_papers/machine_unlearning/>Machine Unlearning</a></li><li><a href=/docs/arxiv_papers/vulnerable_llm/>Vulnerable LLM</a></li><li><a href=/docs/arxiv_papers/data_top_llm_privacy_none/ class=active>Data Top Llm Privacy None</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Data Top Llm Privacy None</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><blockquote><p><strong><em>2024-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.12844v2><strong>MELTing point: Mobile Evaluation of Language Transformers</strong></a></p><p><em>Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence&rsquo;&rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs). To achievethis, we have created our own automation infrastructure, MELT, which supportsthe headless execution and benchmarking of LLMs on device, supporting differentmodels, devices and frameworks, including Android, iOS and Nvidia Jetsondevices. We evaluate popular instruction fine-tuned LLMs and leverage differentframeworks to measure their end-to-end and granular performance, tracing theirmemory and energy requirements along the way. Our analysis is the first systematic study of on-device LLM execution,quantifying performance, energy efficiency and accuracy across variousstate-of-the-art models and showcases the state of on-device intelligence inthe era of hyperscale models. Results highlight the performance heterogeneityacross targets and corroborates that LLM inference is largely memory-bound.Quantization drastically reduces memory requirements and renders executionviable, but at a non-negligible accuracy cost. Drawing from its energyfootprint and thermal behavior, the continuous execution of LLMs remainselusive, as both factors negatively affect user experience. Last, ourexperience shows that the ecosystem is still in its infancy, and algorithmic aswell as hardware breakthroughs can significantly shift the execution cost. Weexpect NPU acceleration, and framework-hardware co-design to be the biggest bettowards efficient standalone execution, with the alternative of offloadingtailored towards edge deployments.</div></details><blockquote><p><strong><em>2024-03-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.08666v2><strong>Radiology-GPT: A Large Language Model for Radiology</strong></a></p><p><em>Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, Tianming Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce Radiology-GPT, a large language model for radiology. Using aninstruction tuning approach on an extensive dataset of radiology domainknowledge, Radiology-GPT demonstrates superior performance compared to generallanguage models such as StableLM, Dolly and LLaMA. It exhibits significantversatility in radiological diagnosis, research, and communication. This workserves as a catalyst for future developments in clinical NLP. The successfulimplementation of Radiology-GPT is indicative of the potential of localizinggenerative large language models, specifically tailored for distinctive medicalspecialties, while ensuring adherence to privacy standards such as HIPAA. Theprospect of developing individualized, large-scale language models that caterto specific needs of various hospitals presents a promising direction. Thefusion of conversational competence and domain-specific knowledge in thesemodels is set to foster future development in healthcare AI. A demo ofRadiology-GPT is available athttps://huggingface.co/spaces/allen-eric/radiology-gpt.</div></details><p><a href=http://arxiv.org/abs/2403.12503v1><strong>Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices</strong></a></p><p><em>Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have significantly transformed the landscape ofNatural Language Processing (NLP). Their impact extends across a diversespectrum of tasks, revolutionizing how we approach language understanding andgenerations. Nevertheless, alongside their remarkable utility, LLMs introducecritical security and risk considerations. These challenges warrant carefulexamination to ensure responsible deployment and safeguard against potentialvulnerabilities. This research paper thoroughly investigates security andprivacy concerns related to LLMs from five thematic perspectives: security andprivacy concerns, vulnerabilities against adversarial attacks, potential harmscaused by misuses of LLMs, mitigation strategies to address these challengeswhile identifying limitations of current strategies. Lastly, the paperrecommends promising avenues for future research to enhance the security andrisk management of LLMs.</div></details><p><a href=http://arxiv.org/abs/2305.11414v3><strong>Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models</strong></a></p><p><em>Sixing Yu, J. Pablo Mu√±oz, Ali Jannesari</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, havedemonstrated remarkable success in a wide range of applications, driven bytheir ability to leverage vast amounts of data for pre-training. However,optimizing FMs often requires access to sensitive data, raising privacyconcerns and limiting their applicability in many domains. In this paper, wepropose the Federated Foundation Models (FFMs) paradigm, which combines thebenefits of FMs and Federated Learning (FL) to enable privacy-preserving andcollaborative learning across multiple end-users. We discuss the potentialbenefits and challenges of integrating FL into the lifespan of FMs, coveringpre-training, fine-tuning, and application. We further outline potential futureresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, andfederated prompt tuning, which allow the development of more personalized andcontext-aware models while ensuring data privacy. Moreover, we explore thepossibility of continual/lifelong learning in FFMs, as increased computationalpower at the edge may unlock the potential for optimizing FMs using newlygenerated private data close to the data source. The proposed FFM conceptsoffer a flexible and scalable framework for training large language models in aprivacy-preserving manner, setting the stage for subsequent advancements inboth FM training and federated learning.</div></details><blockquote><p><strong><em>2024-03-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.11838v1><strong>Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models</strong></a></p><p><em>Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) exhibit impressive capabilities but also presentrisks such as biased content generation and privacy issues. One of the currentalignment techniques includes principle-driven integration, but it faceschallenges arising from the imprecision of manually crafted rules andinadequate risk perception in models without safety training. To address these,we introduce Guide-Align, a two-stage approach. Initially, a safety-trainedmodel identifies potential risks and formulates specific guidelines for variousinputs, thereby establishing a comprehensive library of guidelines and modelsfor input-guidelines retrieval. Subsequently, the retrieval model correlatesnew inputs with pertinent guidelines, guiding LLMs in response generation toensure safe and high-quality outputs, thus aligning with human values. Anadditional optional stage involves fine-tuning a model with new well-aligneddatasets generated through the process implemented in the second stage. Ourmethod customizes guidelines to accommodate diverse inputs, thereby enhancingthe fine-grainedness and comprehensiveness of the guideline library.Furthermore, it incorporates safety expertise from a safety-trained LLM througha lightweight retrieval model. We evaluated our approach on three benchmarks,demonstrating significant improvements in LLM security and quality. Notably,our fine-tuned model, Labrador, even at 13 billion parameters, outperformsGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.</div></details><p><a href=http://arxiv.org/abs/2401.05561v4><strong>TrustLLM: Trustworthiness in Large Language Models</strong></a></p><p><em>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), exemplified by ChatGPT, have gainedconsiderable attention for their excellent natural language processingcapabilities. Nonetheless, these LLMs present many challenges, particularly inthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMsemerges as an important topic. This paper introduces TrustLLM, a comprehensivestudy of trustworthiness in LLMs, including principles for different dimensionsof trustworthiness, established benchmark, evaluation, and analysis oftrustworthiness for mainstream LLMs, and discussion of open challenges andfuture directions. Specifically, we first propose a set of principles fortrustworthy LLMs that span eight different dimensions. Based on theseprinciples, we further establish a benchmark across six dimensions includingtruthfulness, safety, fairness, robustness, privacy, and machine ethics. Wethen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting ofover 30 datasets. Our findings firstly show that in general trustworthiness andutility (i.e., functional effectiveness) are positively related. Secondly, ourobservations reveal that proprietary LLMs generally outperform most open-sourcecounterparts in terms of trustworthiness, raising concerns about the potentialrisks of widely accessible open-source LLMs. However, a few open-source LLMscome very close to proprietary ones. Thirdly, it is important to note that someLLMs may be overly calibrated towards exhibiting trustworthiness, to the extentthat they compromise their utility by mistakenly treating benign prompts asharmful and consequently not responding. Finally, we emphasize the importanceof ensuring transparency not only in the models themselves but also in thetechnologies that underpin trustworthiness. Knowing the specific trustworthytechnologies that have been employed is crucial for analyzing theireffectiveness.</div></details><blockquote><p><strong><em>2024-03-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.03724v2><strong>DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer</strong></a></p><p><em>Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have emerged as dominant tools for varioustasks, particularly when tailored for a specific target by prompt tuning.Nevertheless, concerns surrounding data privacy present obstacles due to thetuned prompts&rsquo; dependency on sensitive private information. A practicalsolution is to host a local LLM and optimize a soft prompt privately usingdata. Yet, hosting a local model becomes problematic when model ownership isprotected. Alternative methods, like sending data to the model&rsquo;s provider fortraining, intensify these privacy issues facing an untrusted provider. In thispaper, we present a novel solution called Differentially-Private Offsite PromptTuning (DP-OPT) to address this challenge. Our approach involves tuning adiscrete prompt on the client side and then applying it to the desired cloudmodels. We demonstrate that prompts suggested by LLMs themselves can betransferred without compromising performance significantly. To ensure that theprompts do not leak private information, we introduce the first private promptgeneration mechanism, by a differentially-private (DP) ensemble of in-contextlearning with private demonstrations. With DP-OPT, generatingprivacy-preserving prompts by Vicuna-7b can yield competitive performancecompared to non-private in-context learning on GPT3.5 or local private prompttuning. Codes are available at <a href=https://github.com/VITA-Group/DP-OPT>https://github.com/VITA-Group/DP-OPT</a> .</div></details><blockquote><p><strong><em>2024-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.14769v4><strong>Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning</strong></a></p><p><em>Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated embodied agent learning protects the data privacy of individualvisual environments by keeping data locally at each client (the individualenvironment) during training. However, since the local data is inaccessible tothe server under federated learning, attackers may easily poison the trainingdata of the local client to build a backdoor in the agent without notice.Deploying such an agent raises the risk of potential harm to humans, as theattackers may easily navigate and control the agent as they wish via thebackdoor. Towards Byzantine-robust federated embodied agent learning, in thispaper, we study the attack and defense for the task of vision-and-languagenavigation (VLN), where the agent is required to follow natural languageinstructions to navigate indoor environments. First, we introduce a simple buteffective attack strategy, Navigation as Wish (NAW), in which the maliciousclient manipulates local trajectory data to implant a backdoor into the globalmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easilynavigate the deployed VLN agent regardless of the language instruction, withoutaffecting its performance on normal test sets. Then, we propose a newPrompt-Based Aggregation (PBA) to defend against the NAW attack in federatedVLN, which provides the server with a &lsquo;&lsquo;prompt&rsquo;&rsquo; of the vision-and-languagealignment variance between the benign and malicious clients so that they can bedistinguished during training. We validate the effectiveness of the PBA methodon protecting the global model from the NAW attack, which outperforms otherstate-of-the-art defense methods by a large margin in the defense metrics onR2R and RxR.</div></details><p><a href=http://arxiv.org/abs/2401.01519v3><strong>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review</strong></a></p><p><em>Luoma Ke, Song Tong, Peng Cheng, Kaiping Peng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper explores the frontiers of large language models (LLMs) inpsychology applications. Psychology has undergone several theoretical changes,and the current use of Artificial Intelligence (AI) and Machine Learning,particularly LLMs, promises to open up new research directions. We provide adetailed exploration of how LLMs like ChatGPT are transforming psychologicalresearch. It discusses the impact of LLMs across various branches ofpsychology, including cognitive and behavioral, clinical and counseling,educational and developmental, and social and cultural psychology, highlightingtheir potential to simulate aspects of human cognition and behavior. The paperdelves into the capabilities of these models to emulate human-like textgeneration, offering innovative tools for literature review, hypothesisgeneration, experimental design, experimental subjects, data analysis, academicwriting, and peer review in psychology. While LLMs are essential in advancingresearch methodologies in psychology, the paper also cautions about theirtechnical and ethical challenges. There are issues like data privacy, theethical implications of using LLMs in psychological research, and the need fora deeper understanding of these models&rsquo; limitations. Researchers shouldresponsibly use LLMs in psychological studies, adhering to ethical standardsand considering the potential consequences of deploying these technologies insensitive areas. Overall, the article provides a comprehensive overview of thecurrent state of LLMs in psychology, exploring potential benefits andchallenges. It serves as a call to action for researchers to leverage LLMs&rsquo;advantages responsibly while addressing associated risks.</div></details><blockquote><p><strong><em>2024-03-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.05720v4><strong>Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning</strong></a></p><p><em>Jianwei Li, Sheng Liu, Qi Lei</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models trained via federated learning (FL) demonstrate impressivecapabilities in handling complex tasks while protecting user privacy. Recentstudies indicate that leveraging gradient information and prior knowledge canpotentially reveal training samples within FL setting. However, theseinvestigations have overlooked the potential privacy risks tied to theintrinsic architecture of the models. This paper presents a two-stage privacyattack strategy that targets the vulnerabilities in the architecture ofcontemporary language models, significantly enhancing attack performance byinitially recovering certain feature directions as additional supervisorysignals. Our comparative experiments demonstrate superior attack performanceacross various datasets and scenarios, highlighting the privacy leakage riskassociated with the increasingly complex architectures of language models. Wecall for the community to recognize and address these potential privacy risksin designing large language models.</div></details><p><a href=http://arxiv.org/abs/2403.10351v1><strong>TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale</strong></a></p><p><em>Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of large language models (LLMs) has significantly advanced naturallanguage processing tasks like text summarization. However, their large sizeand computational demands, coupled with privacy concerns in data transmission,limit their use in resource-constrained and privacy-centric settings. Toovercome this, we introduce TriSum, a framework for distilling LLMs&rsquo; textsummarization abilities into a compact, local model. Initially, LLMs extract aset of aspect-triple rationales and summaries, which are refined using adual-scoring method for quality. Next, a smaller local model is trained withthese tasks, employing a curriculum learning strategy that evolves from simpleto complex tasks. Our method enhances local model performance on variousbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability byproviding insights into the summarization rationale.</div></details><p><a href=http://arxiv.org/abs/2403.10408v1><strong>SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores</strong></a></p><p><em>Vidminas Vizgirda, Rui Zhao, Naman Goel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We present SocialGenPod, a decentralised and privacy-friendly way ofdeploying generative AI Web applications. Unlike centralised Web and dataarchitectures that keep user data tied to application and service providers, weshow how one can use Solid &ndash; a decentralised Web specification &ndash; to decoupleuser data from generative AI applications. We demonstrate SocialGenPod using aprototype that allows users to converse with different Large Language Models,optionally leveraging Retrieval Augmented Generation to generate answersgrounded in private documents stored in any Solid Pod that the user is allowedto access, directly or indirectly. SocialGenPod makes use of Solid accesscontrol mechanisms to give users full control of determining who has access todata stored in their Pods. SocialGenPod keeps all user data (chat history, appconfiguration, personal documents, etc) securely in the user&rsquo;s personal Pod;separate from specific model or application providers. Besides better privacycontrols, this approach also enables portability across different services andapplications. Finally, we discuss challenges, posed by the large computerequirements of state-of-the-art models, that future research in this areashould address. Our prototype is open-source and available at:https://github.com/Vidminas/socialgenpod/.</div></details><p><a href=http://arxiv.org/abs/2401.07348v4><strong>Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity</strong></a></p><p><em>Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of Generative AI, particularly through Large Language Models(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AIlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,thereby broadening their application scope. However, the complexity andemergent autonomy of these models introduce challenges in predictability andlegal compliance. This paper delves into the legal and regulatory implicationsof Generative AI and LLMs in the European Union context, analyzing aspects ofliability, privacy, intellectual property, and cybersecurity. It criticallyexamines the adequacy of the existing and proposed EU legislation, includingthe Artificial Intelligence Act (AIA) draft, in addressing the uniquechallenges posed by Generative AI in general and LLMs in particular. The paperidentifies potential gaps and shortcomings in the legislative framework andproposes recommendations to ensure the safe and compliant deployment ofgenerative models, ensuring they align with the EU&rsquo;s evolving digital landscapeand legal standards.</div></details><p><a href=http://arxiv.org/abs/2403.09017v2><strong>AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic</strong></a></p><p><em>Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The swift progress and widespread acceptance of artificial intelligence (AI)systems highlight a pressing requirement to comprehend both the capabilitiesand potential risks associated with AI. Given the linguistic complexity,cultural richness, and underrepresented status of Arabic in AI research, thereis a pressing need to focus on Large Language Models (LLMs) performance andsafety for Arabic related tasks. Despite some progress in their development,there is a lack of comprehensive trustworthiness evaluation benchmarks whichpresents a major challenge in accurately assessing and improving the safety ofLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the firstcomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises516 human-written multiple-choice questions addressing diverse dimensionsrelated to truthfulness, ethics, safety, physical health, mental health,unfairness, illegal activities, privacy, and offensive language. We evaluated aset of LLMs against our benchmark to assess their trustworthiness. GPT-4 wasthe most trustworthy LLM, while open-source models, particularly AceGPT 7B andJais 13B, struggled to achieve a score of 60% in our benchmark.</div></details><blockquote><p><strong><em>2024-03-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.05156v2><strong>On Protecting the Data Privacy of Large Language Models (LLMs): A Survey</strong></a></p><p><em>Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, Xiuzhen Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are complex artificial intelligence systemscapable of understanding, generating and translating human language. They learnlanguage patterns by analyzing large amounts of text data, allowing them toperform writing, conversation, summarizing and other language tasks. When LLMsprocess and generate large amounts of data, there is a risk of leakingsensitive information, which may threaten data privacy. This paper concentrateson elucidating the data privacy concerns associated with LLMs to foster acomprehensive understanding. Specifically, a thorough investigation isundertaken to delineate the spectrum of data privacy threats, encompassing bothpassive privacy leakage and active privacy attacks within LLMs. Subsequently,we conduct an assessment of the privacy protection mechanisms employed by LLMsat various stages, followed by a detailed examination of their efficacy andconstraints. Finally, the discourse extends to delineate the challengesencountered and outline prospective directions for advancement in the realm ofLLM privacy protection.</div></details><p><a href=http://arxiv.org/abs/2403.08664v2><strong>Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records</strong></a></p><p><em>Erlend Frayling, Jake Lever, Graham McDonald</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The challenge of accessing historical patient data for clinical research,while adhering to privacy regulations, is a significant obstacle in medicalscience. An innovative approach to circumvent this issue involves utilisingsynthetic medical records that mirror real patient data without compromisingindividual privacy. The creation of these synthetic datasets, particularlywithout using actual patient data to train Large Language Models (LLMs),presents a novel solution as gaining access to sensitive patient information totrain models is also a challenge. This study assesses the capability of theLlama 2 LLM to create synthetic medical records that accurately reflect realpatient information, employing zero-shot and few-shot prompting strategies forcomparison against fine-tuned methodologies that do require sensitive patientdata during training. We focus on generating synthetic narratives for theHistory of Present Illness section, utilising data from the MIMIC-IV datasetfor comparison. In this work introduce a novel prompting technique thatleverages a chain-of-thought approach, enhancing the model&rsquo;s ability togenerate more accurate and contextually relevant medical narratives withoutprior fine-tuning. Our findings suggest that this chain-of-thought promptedapproach allows the zero-shot model to achieve results on par with those offine-tuned models, based on Rouge metrics evaluation.</div></details><p><a href=http://arxiv.org/abs/2310.18961v7><strong>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</strong></a></p><p><em>Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Zero-shot anomaly detection (ZSAD) requires detection models trained usingauxiliary data to detect anomalies without any training sample in a targetdataset. It is a crucial task when training data is not accessible due tovarious concerns, eg, data privacy, yet it is challenging since the models needto generalize to anomalies across different domains where the appearance offoreground objects, abnormal regions, and background features, such asdefects/tumors on different products/organs, can vary significantly. Recentlylarge pre-trained vision-language models (VLMs), such as CLIP, havedemonstrated strong zero-shot recognition ability in various vision tasks,including anomaly detection. However, their ZSAD performance is weak since theVLMs focus more on modeling the class semantics of the foreground objectsrather than the abnormality/normality in the images. In this paper we introducea novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD acrossdifferent domains. The key insight of AnomalyCLIP is to learn object-agnostictext prompts that capture generic normality and abnormality in an imageregardless of its foreground objects. This allows our model to focus on theabnormal image regions rather than the object semantics, enabling generalizednormality and abnormality recognition on diverse types of objects. Large-scaleexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIPachieves superior zero-shot performance of detecting and segmenting anomaliesin datasets of highly diverse class semantics from various defect inspectionand medical imaging domains. Code will be made available athttps://github.com/zqhang/AnomalyCLIP.</div></details><blockquote><p><strong><em>2024-03-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.08481v1><strong>SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks</strong></a></p><p><em>Guy Amit, Abigail Goldsteen, Ariel Farkash</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Natural language processing models have experienced a significant upsurge inrecent years, with numerous applications being built upon them. Many of theseapplications require fine-tuning generic base models on customized, proprietarydatasets. This fine-tuning data is especially likely to contain personal orsensitive information about individuals, resulting in increased privacy risk.Membership inference attacks are the most commonly employed attack to assessthe privacy leakage of a machine learning model. However, limited research isavailable on the factors that affect the vulnerability of language models tothis kind of attack, or on the applicability of different defense strategies inthe language domain. We provide the first systematic review of thevulnerability of fine-tuned large language models to membership inferenceattacks, the various factors that come into play, and the effectiveness ofdifferent defense strategies. We find that some training methods providesignificantly reduced privacy risk, with the combination of differentialprivacy and low-rank adaptors achieving the best privacy protection againstthese attacks.</div></details><p><a href=http://arxiv.org/abs/2403.10557v1><strong>Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models</strong></a></p><p><em>Kang Gu, Md Rafi Ur Rashid, Najrin Sultana, Shagufta Mehnaz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid development of Large Language Models (LLMs), we have witnessedintense competition among the major LLM products like ChatGPT, LLaMa, andGemini. However, various issues (e.g. privacy leakage and copyright violation)of the training corpus still remain underexplored. For example, the Times suedOpenAI and Microsoft for infringing on its copyrights by using millions of itsarticles for training. From the perspective of LLM practitioners, handling suchunintended privacy violations can be challenging. Previous work addressed the``unlearning" problem of LLMs using gradient information, while they mostlyintroduced significant overheads like data preprocessing or lacked robustness.In this paper, contrasting with the methods based on first-order information,we revisit the unlearning problem via the perspective of second-orderinformation (Hessian). Our unlearning algorithms, which are inspired by classicNewton update, are not only data-agnostic/model-agnostic but also proven to berobust in terms of utility preservation or privacy guarantee. Through acomprehensive evaluation with four NLP datasets as well as a case study onreal-world datasets, our methods consistently show superiority over thefirst-order methods.</div></details><p><a href=http://arxiv.org/abs/2304.12479v5><strong>AGI: Artificial General Intelligence for Education</strong></a></p><p><em>Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, Xiaoming Zhai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial general intelligence (AGI) has gained global recognition as afuture technology due to the emergence of breakthrough large language modelsand chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventionalAI models, typically designed for a limited range of tasks, demand significantamounts of domain-specific data for training and may not always considerintricate interpersonal dynamics in education. AGI, driven by the recent largepre-trained models, represents a significant leap in the capability of machinesto perform tasks that require human-level intelligence, such as reasoning,problem-solving, decision-making, and even understanding human emotions andsocial interactions. This position paper reviews AGI&rsquo;s key concepts,capabilities, scope, and potential within future education, including achievingfuture educational goals, designing pedagogy and curriculum, and performingassessments. It highlights that AGI can significantly improve intelligenttutoring systems, educational assessment, and evaluation procedures. AGIsystems can adapt to individual student needs, offering tailored learningexperiences. They can also provide comprehensive feedback on studentperformance and dynamically adjust teaching methods based on student progress.The paper emphasizes that AGI&rsquo;s capabilities extend to understanding humanemotions and social interactions, which are critical in educational settings.The paper discusses that ethical issues in education with AGI include databias, fairness, and privacy and emphasizes the need for codes of conduct toensure responsible AGI use in academic settings like homework, teaching, andrecruitment. We also conclude that the development of AGI necessitatesinterdisciplinary collaborations between educators and AI engineers to advanceresearch and application efforts.</div></details><p><a href=http://arxiv.org/abs/2403.08694v1><strong>TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning</strong></a></p><p><em>Shangding Gu, Alois Knoll, Ming Jin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The development of Large Language Models (LLMs) often confronts challengesstemming from the heavy reliance on human annotators in the reinforcementlearning with human feedback (RLHF) framework, or the frequent and costlyexternal queries tied to the self-instruct paradigm. In this work, we pivot toReinforcement Learning (RL) &ndash; but with a twist. Diverging from the typicalRLHF, which refines LLMs following instruction data training, we use RL todirectly generate the foundational instruction dataset that alone suffices forfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations andrules, prioritizing the diversification of training datasets. It facilitatesthe generation of high-quality data without excessive reliance on externaladvanced models, paving the way for a single fine-tuning step and negating theneed for subsequent RLHF stages. Our findings highlight key advantages of ourapproach: reduced need for human involvement and fewer model queries (only$5.73%$ of WizardLM&rsquo;s total), along with enhanced capabilities of LLMs incrafting and comprehending complex instructions compared to strong baselines,and substantially improved model privacy protection.</div></details><p><a href=http://arxiv.org/abs/2403.08271v1><strong>Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification</strong></a></p><p><em>Long Lan, Fengxiang Wang, Shuyan Li, Xiangtao Zheng, Zengmao Wang, Xinwang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fine-grained ship classification in remote sensing (RS-FGSC) poses asignificant challenge due to the high similarity between classes and thelimited availability of labeled data, limiting the effectiveness of traditionalsupervised classification methods. Recent advancements in large pre-trainedVision-Language Models (VLMs) have demonstrated impressive capabilities infew-shot or zero-shot learning, particularly in understanding image content.This study delves into harnessing the potential of VLMs to enhanceclassification accuracy for unseen ship categories, which holds considerablesignificance in scenarios with restricted data due to cost or privacyconstraints. Directly fine-tuning VLMs for RS-FGSC often encounters thechallenge of overfitting the seen classes, resulting in suboptimalgeneralization to unseen classes, which highlights the difficulty indifferentiating complex backgrounds and capturing distinct ship features. Toaddress these issues, we introduce a novel prompt tuning technique that employsa hierarchical, multi-granularity prompt design. Our approach integrates remotesensing ship priors through bias terms, learned from a small trainable network.This strategy enhances the model&rsquo;s generalization capabilities while improvingits ability to discern intricate backgrounds and learn discriminative shipfeatures. Furthermore, we contribute to the field by introducing acomprehensive dataset, FGSCM-52, significantly expanding existing datasets withmore extensive data and detailed annotations for less common ship classes.Extensive experimental evaluations demonstrate the superiority of our proposedmethod over current state-of-the-art techniques. The source code will be madepublicly available.</div></details><p><a href=http://arxiv.org/abs/2403.09733v1><strong>OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models</strong></a></p><p><em>Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid development of Large Language Models (LLMs) has facilitated avariety of applications from different domains. In this technical report, weexplore the integration of LLMs and the popular academic writing tool,Overleaf, to enhance the efficiency and quality of academic writing. To achievethe above goal, there are three challenges: i) including seamless interactionbetween Overleaf and LLMs, ii) establishing reliable communication with the LLMprovider, and iii) ensuring user privacy. To address these challenges, wepresent OverleafCopilot, the first-ever tool (i.e., a browser extension) thatseamlessly integrates LLMs and Overleaf, enabling researchers to leverage thepower of LLMs while writing papers. Specifically, we first propose an effectiveframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, awebsite for researchers to easily find and share high-quality up-to-dateprompts. Thirdly, we propose an agent command system to help researchersquickly build their customizable agents. OverleafCopilot(<a href=https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb>https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb</a>) has been on the Chrome Extension Store, which now serves thousands ofresearchers. Additionally, the code of PromptGenius is released athttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has thepotential to revolutionize academic writing practices, empowering researchersto produce higher-quality papers in less time.</div></details><blockquote><p><strong><em>2024-03-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.07283v1><strong>A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism</strong></a></p><p><em>Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao, Dianhai Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As Large Language Models (LLMs) gain great success in real-worldapplications, an increasing number of users are seeking to develop and deploytheir customized LLMs through cloud services. Nonetheless, in some specificdomains, there are still concerns regarding cost and trade-offs between privacyissues and accuracy. In this study, we introduce a cost-effective andself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. Withcarefully designed horizontal and vertical shaking operators, we can achievecomparable accuracy results with SOTA privacy-preserving LLM schemes usingCryptography-based or Differential Privacy-based methods. Experiments also showthat with the CypherTalk framework, users can achieve reliable accuracy whenusing optimized shaking operator settings. To our best knowledge, this is thefirst work that considers cost, and trade-off between model utility and privacyin LLM scenarios.</div></details><p><a href=http://arxiv.org/abs/2403.07506v1><strong>Robustness, Security, Privacy, Explainability, Efficiency, and Usability of Large Language Models for Code</strong></a></p><p><em>Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models for code (LLM4Code), which demonstrate strongperformance (e.g., high accuracy) in processing source code, have significantlytransformed software engineering. Many studies separately investigate thenon-functional properties of LM4Code, but there is no systematic review of howthese properties are evaluated and enhanced. This paper fills this gap bythoroughly examining 146 relevant studies, thereby presenting the firstsystematic literature review to identify seven important properties beyondaccuracy, including robustness, security, privacy, explainability, efficiency,and usability. We discuss the current state-of-the-art methods and trends,identify gaps in existing research, and present promising directions for futurestudy.</div></details><p><a href=http://arxiv.org/abs/2403.08100v1><strong>Efficient Language Model Architectures for Differentially Private Federated Learning</strong></a></p><p><em>Jae Hun Ro, Srinadh Bhojanapalli, Zheng Xu, Yanxiang Zhang, Ananda Theertha Suresh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Cross-device federated learning (FL) is a technique that trains a model ondata distributed across typically millions of edge devices without data leavingthe devices. SGD is the standard client optimizer for on device training incross-device FL, favored for its memory and computational efficiency. However,in centralized training of neural language models, adaptive optimizers arepreferred as they offer improved stability and performance. In light of this,we ask if language models can be modified such that they can be efficientlytrained with SGD client optimizers and answer this affirmatively. We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrentnetwork by modifying the sigmoid and tanh activations in the recurrent cell andshow that this new model converges faster and achieves better utility than thestandard CIFG recurrent model in cross-device FL in large scale experiments. Wefurther show that the proposed scale invariant modification also helps infederated learning of larger transformer models. Finally, we demonstrate thescale invariant modification is also compatible with other non-adaptivealgorithms. Particularly, our results suggest an improved privacy utilitytrade-off in federated learning with differential privacy.</div></details><blockquote><p><strong><em>2024-03-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.07088v1><strong>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation</strong></a></p><p><em>Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu, Mingbang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require high computation cost andlarge memory cost. At the same time, LLMs may cause privacy leakage whentraining or prediction procedure contains sensitive information. In this paper,we propose SPA(Side Plugin Adaption), a lightweight architecture for faston-devices inference and privacy retaining on the constraints of stricton-devices computation and memory constraints. Compared with other on-devicesseq2seq generation, SPA could make a fast and stable inference on low-resourceconstraints, allowing it to obtain cost effiency. Our method establish aninteraction between a pretrained LLMs on-cloud and additive parameterson-devices, which could provide the knowledge on both pretrained LLMs andprivate personal feature.Further more, SPA provides a framework to keepfeature-base parameters on private guaranteed but low computational deviceswhile leave the parameters containing general information on the highcomputational devices.</div></details><blockquote><p><strong><em>2024-03-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.06131v1><strong>FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning</strong></a></p><p><em>Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Instruction tuning has proven essential for enhancing the performance oflarge language models (LLMs) in generating human-aligned responses. However,collecting diverse, high-quality instruction data for tuning poses challenges,particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)has emerged as a solution, leveraging federated learning from multiple dataowners while preserving privacy. Yet, it faces challenges due to limitedinstruction data and vulnerabilities to training data extraction attacks. Toaddress these issues, we propose a novel federated algorithm, FedPIT, whichutilizes LLMs&rsquo; in-context learning capability to self-generate task-specificsynthetic data for training autonomously. Our method employs parameter-isolatedtraining to maintain global parameters trained on synthetic data and localparameters trained on augmented local data, effectively thwarting dataextraction attacks. Extensive experiments on real-world medical datademonstrate the effectiveness of FedPIT in improving federated few-shotperformance while preserving privacy and robustness against data heterogeneity.</div></details><blockquote><p><strong><em>2024-03-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16789v3><strong>Detecting Pretraining Data from Large Language Models</strong></a></p><p><em>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although large language models (LLMs) are widely deployed, the data used totrain them is rarely disclosed. Given the incredible scale of this data, up totrillions of tokens, it is all but certain that it includes potentiallyproblematic text such as copyrighted materials, personally identifiableinformation, and test data for widely reported reference benchmarks. However,we currently have no way to know which data of these types is included or inwhat proportions. In this paper, we study the pretraining data detectionproblem: given a piece of text and black-box access to an LLM without knowingthe pretraining data, can we determine if the model was trained on the providedtext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA thatuses data created before and after model training to support gold truthdetection. We also introduce a new detection method Min-K% Prob based on asimple hypothesis: an unseen example is likely to contain a few outlier wordswith low probabilities under the LLM, while a seen example is less likely tohave words with such low probabilities. Min-K% Prob can be applied without anyknowledge about the pretraining corpus or any additional training, departingfrom previous detection methods that require training a reference model on datathat is similar to the pretraining data. Moreover, our experiments demonstratethat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previousmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted bookdetection, contaminated downstream example detection and privacy auditing ofmachine unlearning, and find it a consistently effective solution.</div></details><blockquote><p><strong><em>2024-03-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.05681v1><strong>DP-TabICL: In-Context Learning with Differentially Private Tabular Data</strong></a></p><p><em>Alycia N. Carey, Karuna Bhaila, Kennedy Edemacu, Xintao Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In-context learning (ICL) enables large language models (LLMs) to adapt tonew tasks by conditioning on demonstrations of question-answer pairs and it hasbeen shown to have comparable performance to costly model retraining andfine-tuning. Recently, ICL has been extended to allow tabular data to be usedas demonstration examples by serializing individual records into naturallanguage formats. However, it has been shown that LLMs can leak informationcontained in prompts, and since tabular data often contain sensitiveinformation, understanding how to protect the underlying tabular data used inICL is a critical area of research. This work serves as an initialinvestigation into how to use differential privacy (DP) &ndash; the long-establishedgold standard for data privacy and anonymization &ndash; to protect tabular dataused in ICL. Specifically, we investigate the application of DP mechanisms forprivate tabular ICL via data privatization prior to serialization andprompting. We formulate two private ICL frameworks with provable privacyguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenariosvia injecting noise into individual records or group statistics, respectively.We evaluate our DP-based frameworks on eight real-world tabular datasets andacross multiple ICL and DP settings. Our evaluations show that DP-based ICL canprotect the privacy of the underlying tabular data while achieving comparableperformance to non-LLM baselines, especially under high privacy regimes.</div></details><p><a href=http://arxiv.org/abs/2403.04960v1><strong>SecGPT: An Execution Isolation Architecture for LLM-Based Systems</strong></a></p><p><em>Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begunsupporting third-party applications. These LLM apps leverage the de factonatural language-based automated execution paradigm of LLMs: that is, apps andtheir interactions are defined in natural language, provided access to userdata, and allowed to freely interact with each other and the system. These LLMapp ecosystems resemble the settings of earlier computing platforms, wherethere was insufficient isolation between apps and the system. Becausethird-party apps may not be trustworthy, and exacerbated by the imprecision ofthe natural language interfaces, the current designs pose security and privacyrisks for users. In this paper, we propose SecGPT, an architecture forLLM-based systems that aims to mitigate the security and privacy issues thatarise with the execution of third-party apps. SecGPT&rsquo;s key idea is to isolatethe execution of apps and more precisely mediate their interactions outside oftheir isolated environments. We evaluate SecGPT against a number of case studyattacks and demonstrate that it protects against many security, privacy, andsafety issues that exist in non-isolated LLM-based systems. The performanceoverhead incurred by SecGPT to improve security is under 0.3x forthree-quarters of the tested queries. To foster follow-up research, we releaseSecGPT&rsquo;s source code at <a href=https://github.com/llm-platform-security/SecGPT>https://github.com/llm-platform-security/SecGPT</a>.</div></details><blockquote><p><strong><em>2024-03-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.04451v1><strong>Membership Inference Attacks and Privacy in Topic Modeling</strong></a></p><p><em>Nico Manzonelli, Wanrong Zhang, Salil Vadhan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent research shows that large language models are susceptible to privacyattacks that infer aspects of the training data. However, it is unclear ifsimpler generative models, like topic models, share similar vulnerabilities. Inthis work, we propose an attack against topic models that can confidentlyidentify members of the training data in Latent Dirichlet Allocation. Ourresults suggest that the privacy risks associated with generative modeling arenot restricted to large neural models. Additionally, to mitigate thesevulnerabilities, we explore differentially private (DP) topic modeling. Wepropose a framework for private topic modeling that incorporates DP vocabularyselection as a pre-processing step, and show that it improves privacy whilehaving limited effects on practical utility.</div></details><p><a href=http://arxiv.org/abs/2403.04124v1><strong>Privacy-preserving Fine-tuning of Large Language Models through Flatness</strong></a></p><p><em>Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The privacy concerns associated with the use of Large Language Models (LLMs)have grown recently with the development of LLMs such as ChatGPT. DifferentialPrivacy (DP) techniques are explored in existing work to mitigate their privacyrisks at the cost of generalization degradation. Our paper reveals that theflatness of DP-trained models&rsquo; loss landscape plays an essential role in thetrade-off between their privacy and generalization. We further propose aholistic framework to enforce appropriate weight flatness, which substantiallyimproves model generalization with competitive privacy preservation. Itinnovates from three coarse-to-grained levels, including perturbation-awaremin-max optimization on model weights within a layer, flatness-guided sparseprefix-tuning on weights across layers, and weight knowledge distillationbetween DP & non-DP weights copies. Comprehensive experiments of bothblack-box and white-box scenarios are conducted to demonstrate theeffectiveness of our proposal in enhancing generalization and maintaining DPcharacteristics. For instance, on text classification dataset QNLI, DP-Flatachieves similar performance with non-private full fine-tuning but with DPguarantee under privacy budget $\epsilon=3$, and even better performance givenhigher privacy budgets. Codes are provided in the supplement.</div></details><p><a href=http://arxiv.org/abs/2403.04256v1><strong>Federated Recommendation via Hybrid Retrieval Augmented Generation</strong></a></p><p><em>Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Recommendation (FR) emerges as a novel paradigm that enablesprivacy-preserving recommendations. However, traditional FR systems usuallyrepresent users/items with discrete identities (IDs), suffering fromperformance degradation due to the data sparsity and heterogeneity in FR. Onthe other hand, Large Language Models (LLMs) as recommenders have proveneffective across various recommendation scenarios. Yet, LLM-based recommendersencounter challenges such as low inference efficiency and potentialhallucination, compromising their performance in real-world scenarios. To thisend, we propose GPT-FedRec, a federated recommendation framework leveragingChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.GPT-FedRec is a two-stage solution. The first stage is a hybrid retrievalprocess, mining ID-based user patterns and text-based item features. Next, theretrieved results are converted into text prompts and fed into GPT forre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aimsto extract generalized features from data and exploit pretrained knowledgewithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, theRAG approach also prevents LLM hallucination, improving the recommendationperformance for real-world users. Experimental results on diverse benchmarkdatasets demonstrate the superior performance of GPT-FedRec againststate-of-the-art baseline methods.</div></details><blockquote><p><strong><em>2024-03-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.03397v1><strong>Explaining Genetic Programming Trees using Large Language Models</strong></a></p><p><em>Paula Maddigan, Andrew Lensen, Bing Xue</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Genetic programming (GP) has the potential to generate explainable results,especially when used for dimensionality reduction. In this research, weinvestigate the potential of leveraging eXplainable AI (XAI) and large languagemodels (LLMs) like ChatGPT to improve the interpretability of GP-basednon-linear dimensionality reduction. Our study introduces a novel XAI dashboardnamed GP4NLDR, the first approach to combine state-of-the-art GP with anLLM-powered chatbot to provide comprehensive, user-centred explanations. Weshowcase the system&rsquo;s ability to provide intuitive and insightful narratives onhigh-dimensional data reduction processes through case studies. Our studyhighlights the importance of prompt engineering in eliciting accurate andpertinent responses from LLMs. We also address important considerations arounddata privacy, hallucinatory outputs, and the rapid advancements in generativeAI. Our findings demonstrate its potential in advancing the explainability ofGP algorithms. This opens the door for future research into explaining GPmodels with LLMs.</div></details><p><a href=http://arxiv.org/abs/2403.04024v1><strong>Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification</strong></a></p><p><em>Ricardo Bigolin Lanfredi, Pritam Mukherjee, Ronald Summers</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In chest X-ray (CXR) image analysis, rule-based systems are usually employedto extract labels from reports, but concerns exist about label quality. Thesedatasets typically offer only presence labels, sometimes with binaryuncertainty indicators, which limits their usefulness. In this work, we presentMAPLEZ (Medical report Annotations with Privacy-preserving Large language modelusing Expeditious Zero shot answers), a novel approach leveraging a locallyexecutable Large Language Model (LLM) to extract and enhance findings labels onCXR reports. MAPLEZ extracts not only binary labels indicating the presence orabsence of a finding but also the location, severity, and radiologists&rsquo;uncertainty about the finding. Over eight abnormalities from five test sets, weshow that our method can extract these annotations with an increase of 5percentage points (pp) in F1 score for categorical presence annotations andmore than 30 pp increase in F1 score for the location annotations overcompeting labelers. Additionally, using these improved annotations inclassification supervision, we demonstrate substantial advancements in modelquality, with an increase of 1.7 pp in AUROC over models trained withannotations from the state-of-the-art approach. We share code and annotations.</div></details><p><a href=http://arxiv.org/abs/2403.03536v1><strong>Towards Efficient and Effective Unlearning of Large Language Models for Recommendation</strong></a></p><p><em>Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The significant advancements in large language models (LLMs) give rise to apromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).The efficacy of LLMRec arises from the open-world knowledge and reasoningcapabilities inherent in LLMs. LLMRec acquires the recommendation capabilitiesthrough instruction tuning based on user interaction data. However, in order toprotect user privacy and optimize utility, it is also crucial for LLMRec tointentionally forget specific user data, which is generally referred to asrecommendation unlearning. In the era of LLMs, recommendation unlearning posesnew challenges for LLMRec in terms of \textit{inefficiency} and\textit{ineffectiveness}. Existing unlearning methods require updating billionsof parameters in LLMRec, which is costly and time-consuming. Besides, theyalways impact the model utility during the unlearning process. To this end, wepropose \textbf{E2URec}, the first \underline{E}fficient and\underline{E}ffective \underline{U}nlearning method for LLM\underline{Rec}. Ourproposed E2URec enhances the unlearning efficiency by updating only a fewadditional LoRA parameters, and improves the unlearning effectiveness byemploying a teacher-student framework, where we maintain multiple teachernetworks to guide the unlearning process. Extensive experiments show thatE2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affectingrecommendation performance. The source code is at\url{https://github.com/justarter/E2URec}.</div></details><blockquote><p><strong><em>2024-03-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.03129v1><strong>CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following</strong></a></p><p><em>Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advancement of language models (LMs), their exposure to private datais increasingly inevitable, and their deployment (especially for smaller ones)on personal devices, such as PCs and smartphones, has become a prevailingtrend. In contexts laden with user information, enabling models to bothsafeguard user privacy and execute commands efficiently emerges as an essentialresearch imperative. In this paper, we propose CoGenesis, a collaborativegeneration framework integrating large (hosted on cloud infrastructure) andsmall models (deployed on local devices) to address privacy concerns logically.Initially, we design a pipeline to create personalized writing instructiondatasets enriched with extensive context details as the testbed of thisresearch issue. Subsequently, we introduce two variants of CoGenesis based onsketch and logits respectively. Our experimental findings, based on oursynthesized dataset and two additional open-source datasets, indicate that: 1)Large-scale models perform well when provided with user context but struggle inthe absence of such context. 2) While specialized smaller models fine-tuned onthe synthetic dataset show promise, they still lag behind their largercounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,showcases competitive performance, providing a feasible solution to privacyissues.</div></details><p><a href=http://arxiv.org/abs/2403.02694v1><strong>Privacy-Aware Semantic Cache for Large Language Models</strong></a></p><p><em>Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2have revolutionized natural language processing and search engine dynamics.However, these models incur exceptionally high computational costs. Forinstance, GPT-3 consists of 175 billion parameters and inference on thesemodels also demands billions of floating-point operations. Caching is a naturalsolution to reduce LLM inference costs on repeated queries. However, existingcaching methods are incapable of finding semantic similarities among LLMqueries, leading to unacceptable false hit-and-miss rates. This paper introduces MeanCache, a semantic cache for LLMs that identifiessemantically similar queries to determine cache hit or miss. Using MeanCache,the response to a user&rsquo;s semantically similar query can be retrieved from alocal cache rather than re-querying the LLM, thus reducing costs, serviceprovider load, and environmental impact. MeanCache leverages Federated Learning(FL) to collaboratively train a query similarity model in a distributed manneracross numerous users without violating privacy. By placing a local cache ineach user&rsquo;s device and using FL, MeanCache reduces the latency and costs andenhances model performance, resulting in lower cache false hit rates. Ourexperiments, benchmarked against the GPTCache, reveal that MeanCache attains anapproximately 17% higher F-score and a 20% increase in precision duringsemantic cache hit-and-miss decisions. Furthermore, MeanCache reduces thestorage requirement by 83% and accelerates semantic cache hit-and-missdecisions by 11%, while still surpassing GPTCache.</div></details><blockquote><p><strong><em>2024-03-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.01749v1><strong>Differentially Private Synthetic Data via Foundation Model APIs 2: Text</strong></a></p><p><em>Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Text data has become extremely valuable due to the emergence of machinelearning algorithms that learn from it. A lot of high-quality text datagenerated in the real world is private and therefore cannot be shared or usedfreely due to privacy concerns. Generating synthetic replicas of private textdata with a formal privacy guarantee, i.e., differential privacy (DP), offers apromising and scalable solution. However, existing methods necessitate DPfinetuning of large language models (LLMs) on private data to generate DPsynthetic data. This approach is not viable for proprietary LLMs (e.g.,GPT-3.5) and also demands considerable computational resources for open-sourceLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)algorithm to generate DP synthetic images with only API access to diffusionmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, thatapplies to the complex setting of text. We use API access to an LLM andgenerate DP synthetic text without any model training. We conduct comprehensiveexperiments on three benchmark datasets. Our results demonstrate that Aug-PEproduces DP synthetic text that yields competitive utility with the SOTA DPfinetuning baselines. This underscores the feasibility of relying solely on APIaccess of LLMs to produce high-quality DP synthetic texts, thereby facilitatingmore accessible routes to privacy-preserving LLM applications. Our code anddata are available at <a href=https://github.com/AI-secure/aug-pe>https://github.com/AI-secure/aug-pe</a>.</div></details><p><a href=http://arxiv.org/abs/2309.16739v3><strong>Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities</strong></a></p><p><em>Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen, Kaibin Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), which have shown remarkable capabilities, arerevolutionizing AI development and potentially shaping our future. However,given their multimodality, the status quo cloud-based deployment faces somecritical challenges: 1) long response time; 2) high bandwidth costs; and 3) theviolation of data privacy. 6G mobile edge computing (MEC) systems may resolvethese pressing issues. In this article, we explore the potential of deployingLLMs at the 6G edge. We start by introducing killer applications powered bymultimodal LLMs, including robotics and healthcare, to highlight the need fordeploying LLMs in the vicinity of end users. Then, we identify the criticalchallenges for LLM deployment at the edge and envision the 6G MEC architecturefor LLMs. Furthermore, we delve into two design aspects, i.e., edge trainingand edge inference for LLMs. In both aspects, considering the inherent resourcelimitations at the edge, we discuss various cutting-edge techniques, includingsplit learning/inference, parameter-efficient fine-tuning, quantization, andparameter-sharing inference, to facilitate the efficient deployment of LLMs.This article serves as a position paper for thoroughly identifying themotivation, challenges, and pathway for empowering LLMs at the 6G edge.</div></details><blockquote><p><strong><em>2024-03-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.01384v1><strong>On the Compressibility of Quantized Large Language Models</strong></a></p><p><em>Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, Chun Jason Xue</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offerssignificant benefits, such as enhanced data privacy and real-time processingcapabilities. However, it also faces critical challenges due to the substantialmemory requirement of LLMs. Quantization is an effective way of reducing themodel size while maintaining good performance. However, even afterquantization, LLMs may still be too big to fit entirely into the limited memoryof edge or mobile devices and have to be partially loaded from the storage tocomplete the inference. In this case, the I/O latency of model loading becomesthe bottleneck of the LLM inference latency. In this work, we take apreliminary step of studying applying data compression techniques to reducedata movement and thus speed up the inference of quantized LLM onmemory-constrained devices. In particular, we discussed the compressibility ofquantized LLMs, the trade-off between the compressibility and performance ofquantized LLMs, and opportunities to optimize both of them jointly.</div></details><p><a href=http://arxiv.org/abs/2403.04786v1><strong>Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models</strong></a></p><p><em>Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have become a cornerstone in the field ofNatural Language Processing (NLP), offering transformative capabilities inunderstanding and generating human-like text. However, with their risingprominence, the security and vulnerability aspects of these models havegarnered significant attention. This paper presents a comprehensive survey ofthe various forms of attacks targeting LLMs, discussing the nature andmechanisms of these attacks, their potential impacts, and current defensestrategies. We delve into topics such as adversarial attacks that aim tomanipulate model outputs, data poisoning that affects model training, andprivacy concerns related to training data exploitation. The paper also exploresthe effectiveness of different attack methodologies, the resilience of LLMsagainst these attacks, and the implications for model integrity and user trust.By examining the latest research, we provide insights into the currentlandscape of LLM vulnerabilities and defense mechanisms. Our objective is tooffer a nuanced understanding of LLM attacks, foster awareness within the AIcommunity, and inspire robust solutions to mitigate these risks in futuredevelopments.</div></details><p><a href=http://arxiv.org/abs/2403.01567v1><strong>ReMatch: Retrieval Enhanced Schema Matching with LLMs</strong></a></p><p><em>Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Schema matching is a crucial task in data integration, involving thealignment of a source database schema with a target schema to establishcorrespondence between their elements. This task is challenging due to textualand semantic heterogeneity, as well as differences in schema sizes. Althoughmachine-learning-based solutions have been explored in numerous studies, theyoften suffer from low accuracy, require manual mapping of the schemas for modeltraining, or need access to source schema data which might be unavailable dueto privacy concerns. In this paper we present a novel method, named ReMatch,for matching schemas using retrieval-enhanced Large Language Models (LLMs). Ourmethod avoids the need for predefined mapping, any model training, or access todata in the source database. In the ReMatch method the tables of the targetschema and the attributes of the source schema are first represented asstructured passage-based documents. For each source attribute document, weretrieve $J$ documents, representing target schema tables, according to theirsemantic relevance. Subsequently, we create a prompt for every source table,comprising all its attributes and their descriptions, alongside all attributesfrom the set of top $J$ target tables retrieved previously. We employ LLMsusing this prompt for the matching task, yielding a ranked list of $K$potential matches for each source attribute. Our experimental results on largereal-world schemas demonstrate that ReMatch significantly improves matchingcapabilities and outperforms other machine learning approaches. By eliminatingthe requirement for training data, ReMatch becomes a viable solution forreal-world scenarios.</div></details><blockquote><p><strong><em>2024-03-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.04784v1><strong>Analysis of Privacy Leakage in Federated Large Language Models</strong></a></p><p><em>Minh N. Vu, Truc Nguyen, Tre&rsquo; R. Jeter, My T. Thai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid adoption of Federated Learning (FL) as the training and tuningprotocol for applications utilizing Large Language Models (LLMs), recentresearch highlights the need for significant modifications to FL to accommodatethe large-scale of LLMs. While substantial adjustments to the protocol havebeen introduced as a response, comprehensive privacy analysis for the adaptedFL protocol is currently lacking. To address this gap, our work delves into an extensive examination of theprivacy analysis of FL when used for training LLMs, both from theoretical andpractical perspectives. In particular, we design two active membershipinference attacks with guaranteed theoretical success rates to assess theprivacy leakages of various adapted FL configurations. Our theoretical findingsare translated into practical attacks, revealing substantial privacyvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, andOpenAI&rsquo;s GPTs, across multiple real-world language datasets. Additionally, weconduct thorough experiments to evaluate the privacy leakage of these modelswhen data is protected by state-of-the-art differential privacy (DP)mechanisms.</div></details><p><a href=http://arxiv.org/abs/2309.11852v2><strong>Knowledge Sanitization of Large Language Models</strong></a></p><p><em>Yoichi Ishibashi, Hidetoshi Shimodaira</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We explore a knowledge sanitization approach to mitigate the privacy concernsassociated with large language models (LLMs). LLMs trained on a large corpus ofWeb data can memorize and potentially reveal sensitive or confidentialinformation, raising critical security concerns. Our technique efficientlyfine-tunes these models using the Low-Rank Adaptation (LoRA) method, promptingthem to generate harmless responses such as ``I don&rsquo;t know&rsquo;&rsquo; when queried aboutspecific information. Experimental results in a closed-book question-answeringtask show that our straightforward method not only minimizes particularknowledge leakage but also preserves the overall performance of LLMs. These twoadvantages strengthen the defense against extraction attacks and reduces theemission of harmful content such as hallucinations.</div></details><p><a href=http://arxiv.org/abs/2403.01218v1><strong>Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy</strong></a></p><p><em>Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel&rsquo;s training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to their<code>U-MIA'' counterparts). We propose a categorization of existing U-MIAs into</code>population U-MIAs&rsquo;&rsquo;, where the same attacker is instantiated for allexamples, and ``per-example U-MIAs&rsquo;&rsquo;, where a dedicated attacker isinstantiated for each example. We show that the latter category, wherein theattacker tailors its membership prediction to each example under attack, issignificantly stronger. Indeed, our results show that the commonly used U-MIAsin the unlearning literature overestimate the privacy protection afforded byexisting unlearning techniques on both vision and language models. Ourinvestigation reveals a large variance in the vulnerability of differentexamples to per-example U-MIAs. In fact, several unlearning algorithms lead toa reduced vulnerability for some, but not all, examples that we wish tounlearn, at the expense of increasing it for other examples. Notably, we findthat the privacy protection for the remaining training examples may worsen as aconsequence of unlearning. We also discuss the fundamental difficulty ofequally protecting all examples using existing unlearning schemes, due to thedifferent rates at which examples are unlearned. We demonstrate that naiveattempts at tailoring unlearning stopping criteria to different examples failto alleviate these issues.</div></details><p><a href=http://arxiv.org/abs/2403.01133v1><strong>Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data</strong></a></p><p><em>Aritra Hota, Soumyajit Chatterjee, Sandip Chakraborty</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Traditional human-in-the-loop-based annotation for time-series data likeinertial data often requires access to alternate modalities like video or audiofrom the environment. These alternate sources provide the necessary informationto the human annotator, as the raw numeric data is often too obfuscated evenfor an expert. However, this traditional approach has many concerns surroundingoverall cost, efficiency, storage of additional modalities, time, scalability,and privacy. Interestingly, recent large language models (LLMs) are alsotrained with vast amounts of publicly available alphanumeric data, which allowsthem to comprehend and perform well on tasks beyond natural languageprocessing. Naturally, this opens up a potential avenue to explore LLMs asvirtual annotators where the LLMs will be directly provided the raw sensor datafor annotation instead of relying on any alternate modality. Naturally, thiscould mitigate the problems of the traditional human-in-the-loop approach.Motivated by this observation, we perform a detailed study in this paper toassess whether the state-of-the-art (SOTA) LLMs can be used as virtualannotators for labeling time-series physical sensing data. To perform this in aprincipled manner, we segregate the study into two major phases. In the firstphase, we investigate the challenges an LLM like GPT-4 faces in comprehendingraw sensor data. Considering the observations from phase 1, in the next phase,we investigate the possibility of encoding the raw sensor data using SOTA SSLapproaches and utilizing the projected time-series data to get annotations fromthe LLM. Detailed evaluation with four benchmark HAR datasets shows thatSSL-based encoding and metric-based guidance allow the LLM to make morereasonable decisions and provide accurate annotations without requiringcomputationally expensive fine-tuning or sophisticated prompt engineering.</div></details><blockquote><p><strong><em>2024-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2403.00932v1><strong>Differentially Private Knowledge Distillation via Synthetic Text Generation</strong></a></p><p><em>James Flemings, Murali Annavaram</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language models (LLMs) are achieving state-of-the-art performance inmany different downstream tasks. However, the increasing urgency of dataprivacy requires LLMs to train with Differential Privacy (DP) on private data.Concurrently it is also necessary to compress LLMs for real-life deployments onresource-constrained devices or latency-sensitive applications. Differentialprivacy and model compression generally must trade off utility loss to achievetheir objectives. Moreover, concurrently achieving both can result in even moreutility loss. To this end, we propose a novel differentially private knowledgedistillation algorithm that exploits synthetic data generated by adifferentially private LLM. The knowledge of a teacher model is transferredonto the student in two ways: one way from the synthetic data itself, the hardlabels, and the other way by the output distribution of the teacher modelevaluated on the synthetic data, the soft labels. Furthermore, if the teacherand student share a similar architectural structure, we can further distillknowledge by exploiting hidden representations. Our results show that ourframework substantially improves the utility over existing baselines withstrong privacy parameters, {\epsilon} = 2, validating that we can successfullycompress autoregressive LLMs while preserving the privacy of training data.</div></details><p><a href=http://arxiv.org/abs/2403.00871v1><strong>Teach LLMs to Phish: Stealing Private Information from Language Models</strong></a></p><p><em>Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang, Yaoqing Yang, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: When large language models are trained on private data, it can be asignificant privacy risk for them to memorize and regurgitate sensitiveinformation. In this work, we propose a new practical data extraction attackthat we call &ldquo;neural phishing&rdquo;. This attack enables an adversary to target andextract sensitive or personally identifiable information (PII), e.g., creditcard numbers, from a model trained on user data with upwards of 10% attacksuccess rates, at times, as high as 50%. Our attack assumes only that anadversary can insert as few as 10s of benign-appearing sentences into thetraining dataset using only vague priors on the structure of the user data.</div></details><p><a href=http://arxiv.org/abs/2403.01008v1><strong>BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)</strong></a></p><p><em>Sean Wellington</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: BasedAI is a distributed network of machines which introduces decentralizedinfrastructure capable of integrating Fully Homomorphic Encryption (FHE) withany large language model (LLM) connected to its network. The proposed frameworkembeds a default mechanism, called &ldquo;Cerberus Squeezing&rdquo;, into the miningprocess which enables the transformation of a standard LLMs into encryptedzero-knowledge LLMs, or &ldquo;ZK-LLMs&rdquo;, leveraging insights from generativeadversarial networks for data privacy. This novel quantization mechanismempowers BasedAI miners to process and respond to prompts derived from Userinteraction with LLMs without the need for decrypting either the queries ortheir corresponding responses. The introduction of Cerberus Squeezingsignificantly improves performance degradation caused by quantized functions incurrent FHE-compliant computing environments by proactively optimizing callsbetween users, miners, and validators.</div></details><blockquote><p><strong><em>2024-02-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.19465v1><strong>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</strong></a></p><p><em>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Moststudies concentrate on fully pre-trained LLMs to better understand and improveLLMs&rsquo; trustworthiness. In this paper, to reveal the untapped potential ofpre-training, we pioneer the exploration of LLMs&rsquo; trustworthiness during thisperiod, focusing on five key dimensions: reliability, privacy, toxicity,fairness, and robustness. To begin with, we apply linear probing to LLMs. Thehigh probing accuracy suggests that \textit{LLMs in early pre-training canalready distinguish concepts in each trustworthiness dimension}. Therefore, tofurther uncover the hidden possibilities of pre-training, we extract steeringvectors from a LLM&rsquo;s pre-training checkpoints to enhance the LLM&rsquo;strustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutualinformation estimation is bounded by linear probing accuracy, we also probeLLMs with mutual information to investigate the dynamics of trustworthinessduring pre-training. We are the first to observe a similar two-phasephenomenon: fitting and compression~\citep{shwartz2017opening}. This researchprovides an initial exploration of trustworthiness modeling during LLMpre-training, seeking to unveil new insights and spur further developments inthe field. We will make our code publicly accessible at\url{https://github.com/ChnQ/TracingLLM}.</div></details><blockquote><p><strong><em>2024-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.17491v2><strong>FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing</strong></a></p><p><em>Terence Jie Chua, Wenhan Yu, Jun Zhao, Kwok-Yan Lam</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of foundation models, including language and vision models, hasreshaped AI&rsquo;s landscape, offering capabilities across various applications.Deploying and fine-tuning these large models, like GPT-3 and BERT, presentschallenges, especially in the current foundation model era. We introduceEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, weexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT usesadapters, emulators, and PEFT for federated model tuning, enhancing modelprivacy and memory efficiency. Adapters adjust pre-trained models, whileemulators give a compact representation of original models, addressing bothprivacy and efficiency. Adaptable to various neural networks, our approach alsouses deep reinforcement learning for hyper-parameter optimization. We testedFedPEAT in a unique scenario with a server participating in collaborativefederated tuning, showcasing its potential in tackling foundation modelchallenges.</div></details><p><a href=http://arxiv.org/abs/2403.00830v1><strong>MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices</strong></a></p><p><em>Abdul Basit, Khizar Hussain, Muhammad Abdullah Hanif, Muhammad Shafique</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are revolutionizing various domains with theirremarkable natural language processing (NLP) abilities. However, deploying LLMsin resource-constrained edge computing and embedded systems presentssignificant challenges. Another challenge lies in delivering medical assistancein remote areas with limited healthcare facilities and infrastructure. Toaddress this, we introduce MedAide, an on-premise healthcare chatbot. Itleverages tiny-LLMs integrated with LangChain, providing efficient edge-basedpreliminary medical diagnostics and support. MedAide employs modeloptimizations for minimal memory footprint and latency on embedded edge deviceswithout server infrastructure. The training process is optimized using low-rankadaptation (LoRA). Additionally, the model is trained on diverse medicaldatasets, employing reinforcement learning from human feedback (RLHF) toenhance its domain-specific capabilities. The system is implemented on variousconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77%accuracy in medical consultations and scores 56 in USMLE benchmark, enabling anenergy-efficient healthcare assistance platform that alleviates privacyconcerns due to edge-based deployment, thereby empowering the community.</div></details><blockquote><p><strong><em>2024-02-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.17505v1><strong>BASES: Large-scale Web Search User Simulation with Large Language Model based Agents</strong></a></p><p><em>Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Due to the excellent capacities of large language models (LLMs), it becomesfeasible to develop LLM-based agents for reliable user simulation. Consideringthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,we conduct large-scale user simulation for web search, to improve the analysisand modeling of user search behavior. Specially, we propose BASES, a novel usersimulation framework with LLM-based agents, designed to facilitatecomprehensive simulations of web search user behaviors. Our simulationframework can generate unique user profiles at scale, which subsequently leadsto diverse search behaviors. To demonstrate the effectiveness of BASES, weconduct evaluation experiments based on two human benchmarks in both Chineseand English, demonstrating that BASES can effectively simulate large-scalehuman-like search behaviors. To further accommodate the research on web search,we develop WARRIORS, a new large-scale dataset encompassing web search userbehaviors, including both Chinese and English versions, which can greatlybolster research in the field of information retrieval. Our code and data willbe publicly released soon.</div></details><blockquote><p><strong><em>2024-02-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.15591v4><strong>Privacy-Preserved Neural Graph Databases</strong></a></p><p><em>Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the era of large language models (LLMs), efficient and accurate dataretrieval has become increasingly crucial for the use of domain-specific orprivate data in the retrieval augmented generation (RAG). Neural graphdatabases (NGDBs) have emerged as a powerful paradigm that combines thestrengths of graph databases (GDBs) and neural networks to enable efficientstorage, retrieval, and analysis of graph-structured data which can beadaptively trained with LLMs. The usage of neural embedding storage and Complexneural logical Query Answering (CQA) provides NGDBs with generalizationability. When the graph is incomplete, by extracting latent patterns andrepresentations, neural graph databases can fill gaps in the graph structure,revealing hidden relationships and enabling accurate query answering.Nevertheless, this capability comes with inherent trade-offs, as it introducesadditional privacy risks to the domain-specific or private databases. Maliciousattackers can infer more sensitive information in the database usingwell-designed queries such as from the answer sets of where Turing Awardwinners born before 1950 and after 1940 lived, the living places of TuringAward winner Hinton are probably exposed, although the living places may havebeen deleted in the training stage due to the privacy concerns. In this work,we propose a privacy-preserved neural graph database (P-NGDB) framework toalleviate the risks of privacy leakage in NGDBs. We introduce adversarialtraining techniques in the training stage to enforce the NGDBs to generateindistinguishable answers when queried with private information, enhancing thedifficulty of inferring sensitive information through combinations of multipleinnocuous queries.</div></details><p><a href=http://arxiv.org/abs/2402.16347v1><strong>CodeS: Towards Building Open-source Language Models for Text-to-SQL</strong></a></p><p><em>Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, Hong Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models have shown promising performance on the task of translatingnatural language questions into SQL queries (Text-to-SQL). However, most of thestate-of-the-art (SOTA) approaches rely on powerful yet closed-source largelanguage models (LLMs), such as ChatGPT and GPT-4, which may have thelimitations of unclear model architectures, data privacy risks, and expensiveinference overheads. To address the limitations, we introduce CodeS, a seriesof pre-trained language models with parameters ranging from 1B to 15B,specifically designed for the text-to-SQL task. CodeS is a fully open-sourcelanguage model, which achieves superior accuracy with much smaller parametersizes. This paper studies the research challenges in building CodeS. To enhancethe SQL generation abilities of CodeS, we adopt an incremental pre-trainingapproach using a specifically curated SQL-centric corpus. Based on this, weaddress the challenges of schema linking and rapid domain adaptation throughstrategic prompt construction and a bi-directional data augmentation technique.We conduct comprehensive evaluations on multiple datasets, including the widelyused Spider benchmark, the newly released BIRD benchmark, robustness-diagnosticbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, aswell as two real-world datasets created for financial and academicapplications. The experimental results show that our CodeS achieves new SOTAaccuracy and robustness on nearly all challenging text-to-SQL benchmarks.</div></details><p><a href=http://arxiv.org/abs/2305.14965v2><strong>Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks</strong></a></p><p><em>Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent explorations with commercial Large Language Models (LLMs) have shownthat non-expert users can jailbreak LLMs by simply manipulating their prompts;resulting in degenerate output behavior, privacy and security breaches,offensive outputs, and violations of content regulator policies. Limitedstudies have been conducted to formalize and analyze these attacks and theirmitigations. We bridge this gap by proposing a formalism and a taxonomy ofknown (and possible) jailbreaks. We survey existing jailbreak methods and theireffectiveness on open-source and commercial LLMs (such as GPT-based models,OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreakdetection in terms of their effectiveness against known attacks. For ouranalysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. Wewill make the dataset public along with the model outputs.</div></details><p><a href=http://arxiv.org/abs/2305.01181v2><strong>A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models</strong></a></p><p><em>Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham Fikri Aji, Derek F. Wong, Longyue Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine Translation (MT) has greatly advanced over the years due to thedevelopments in deep neural networks. However, the emergence of Large LanguageModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MTdomain. In this context, we believe that the future of MT is intricately tiedto the capabilities of LLMs. These models not only offer vast linguisticunderstandings but also bring innovative methodologies, such as prompt-basedtechniques, that have the potential to further elevate MT. In this paper, weprovide an overview of the significant enhancements in MT that are influencedby LLMs and advocate for their pivotal role in upcoming MT research andimplementations. We highlight several new MT directions, emphasizing thebenefits of LLMs in scenarios such as Long-Document Translation, StylizedTranslation, and Interactive Translation. Additionally, we address theimportant concern of privacy in LLM-driven MT and suggest essentialprivacy-preserving strategies. By showcasing practical instances, we aim todemonstrate the advantages that LLMs offer, particularly in tasks liketranslating extended documents. We conclude by emphasizing the critical role ofLLMs in guiding the future evolution of MT and offer a roadmap for futureexploration in the sector.</div></details><p><a href=http://arxiv.org/abs/2402.16840v1><strong>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</strong></a></p><p><em>Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: &ldquo;Bigger the better&rdquo; has been the predominant trend in recent Large LanguageModels (LLMs) development. However, LLMs do not suit well for scenarios thatrequire on-device processing, energy efficiency, low memory footprint, andresponse efficiency. These requisites are crucial for privacy, security, andsustainable deployment. This paper explores the &ldquo;less is more&rdquo; paradigm byaddressing the challenge of designing accurate yet efficient Small LanguageModels (SLMs) for resource constrained devices. Our primary contribution is theintroduction of an accurate and fully transparent open-source 0.5 billion(0.5B) parameter SLM, named MobiLlama, catering to the specific needs ofresource-constrained computing with an emphasis on enhanced performance withreduced resource demands. MobiLlama is a SLM design that initiates from alarger model and applies a careful parameter sharing scheme to reduce both thepre-training and the deployment cost. Our work strives to not only bridge thegap in open-source SLMs but also ensures full transparency, where completetraining data pipeline, training code, model weights, and over 300 checkpointsalong with evaluation codes is available at :https://github.com/mbzuai-oryx/MobiLlama.</div></details><p><a href=http://arxiv.org/abs/2401.05200v2><strong>Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking</strong></a></p><p><em>Samuel Kernan Freire, Chaofan Wang, Mina Foosherian, Stefan Wellsandt, Santiago Ruiz-Arenas, Evangelos Niforatos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in natural language processing enable more intelligent waysto support knowledge sharing in factories. In manufacturing, operatingproduction lines has become increasingly knowledge-intensive, putting strain ona factory&rsquo;s capacity to train and support new operators. This paper introducesa Large Language Model (LLM)-based system designed to retrieve information fromthe extensive knowledge contained in factory documentation and knowledge sharedby expert operators. The system aims to efficiently answer queries fromoperators and facilitate the sharing of new knowledge. We conducted a userstudy at a factory to assess its potential impact and adoption, elicitingseveral perceived benefits, namely, enabling quicker information retrieval andmore efficient resolution of issues. However, the study also highlighted apreference for learning from a human expert when such an option is available.Furthermore, we benchmarked several commercial and open-sourced LLMs for thissystem. The current state-of-the-art model, GPT-4, consistently outperformedits counterparts, with open-source models trailing closely, presenting anattractive option given their data privacy and customization benefits. Insummary, this work offers preliminary insights and a system design forfactories considering using LLM tools for knowledge management.</div></details><p><a href=http://arxiv.org/abs/2402.17012v1><strong>Pandora&rsquo;s White-Box: Increased Training Data Leakage in Open LLMs</strong></a></p><p><em>Jeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper we undertake a systematic study of privacy attacks against opensource Large Language Models (LLMs), where an adversary has access to eitherthe model weights, gradients, or losses, and tries to exploit them to learnsomething about the underlying training data. Our headline results are thefirst membership inference attacks (MIAs) against pre-trained LLMs that areable to simultaneously achieve high TPRs and low FPRs, and a pipeline showingthat over $50%$ (!) of the fine-tuning dataset can be extracted from afine-tuned LLM in natural settings. We consider varying degrees of access tothe underlying model, customization of the language model, and resourcesavailable to the attacker. In the pre-trained setting, we propose three newwhite-box MIAs: an attack based on the gradient norm, a supervised neuralnetwork classifier, and a single step loss ratio attack. All outperformexisting black-box baselines, and our supervised attack closes the gap betweenMIA attack success against LLMs and other types of models. In fine-tuning, wefind that given access to the loss of the fine-tuned and base models, afine-tuned loss ratio attack FLoRA is able to achieve near perfect MIApeformance. We then leverage these MIAs to extract fine-tuning data fromfine-tuned language models. We find that the pipeline of generating fromfine-tuned models prompted with a small snippet of the prefix of each trainingexample, followed by using FLoRa to select the most likely training sample,succeeds the majority of the fine-tuning dataset after only $3$ epochs offine-tuning. Taken together, these findings show that highly effective MIAs areavailable in almost all LLM training settings, and highlight that great caremust be taken before LLMs are fine-tuned on highly sensitive data and thendeployed.</div></details><p><a href=http://arxiv.org/abs/2306.11698v5><strong>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</strong></a></p><p><em>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in their capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications such as healthcare and finance &ndash; where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives &ndash; including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially because GPT-4follows (misleading) instructions more precisely. Our work illustrates acomprehensive trustworthiness evaluation of GPT models and sheds light on thetrustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/ ; our dataset can be previewed athttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version ofthis work is at <a href="https://openreview.net/pdf?id=kaHpo8OZw2">https://openreview.net/pdf?id=kaHpo8OZw2</a> .</div></details><p><a href=http://arxiv.org/abs/2402.16664v1><strong>LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery</strong></a></p><p><em>Kexin Chen, Yuyang Du, Tao You, Mobarakol Islam, Ziyu Guo, Yueming Jin, Guangyong Chen, Pheng-Ann Heng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Visual question answering (VQA) can be fundamentally crucial for promotingrobotic-assisted surgical education. In practice, the needs of trainees areconstantly evolving, such as learning more surgical types, adapting todifferent robots, and learning new surgical instruments and techniques for onesurgery. Therefore, continually updating the VQA system by a sequential datastream from multiple resources is demanded in robotic surgery to address newtasks. In surgical scenarios, the storage cost and patient data privacy oftenrestrict the availability of old data when updating the model, necessitating anexemplar-free continual learning (CL) setup. However, prior studies overlookedtwo vital problems of the surgical domain: i) large domain shifts from diversesurgical operations collected from multiple departments or clinical centers,and ii) severe data imbalance arising from the uneven presence of surgicalinstruments or activities during surgical procedures. This paper proposes toaddress these two problems with a multimodal large language model (LLM) and anadaptive weight assignment methodology. We first develop a new multi-teacher CLframework that leverages a multimodal LLM as the additional teacher. The stronggeneralization ability of the LLM can bridge the knowledge gap when domainshifts and data imbalances occur. We then put forth a novel data processingmethod that transforms complex LLM embeddings into logits compatible with ourCL framework. We further design an adaptive weight assignment approach thatbalances the generalization ability of the LLM and the domain expertise of theold CL model. We construct a new dataset for surgical VQA tasks, providingvaluable data resources for future research. Extensive experimental results onthree datasets demonstrate the superiority of our method to other advanced CLmodels.</div></details><blockquote><p><strong><em>2024-02-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.16035v1><strong>Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations</strong></a></p><p><em>Yafei Xiang, Hanyi Yu, Yulu Gong, Shuning Huo, Mengran Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid development of artificial intelligence technology, Transformerstructural pre-training model has become an important tool for large languagemodel (LLM) tasks. In the field of e-commerce, these models are especiallywidely used, from text understanding to generating recommendation systems,which provide powerful technical support for improving user experience andoptimizing service processes. This paper reviews the core application scenariosof Transformer pre-training model in e-commerce text understanding andrecommendation generation, including but not limited to automatic generation ofproduct descriptions, sentiment analysis of user comments, construction ofpersonalized recommendation system and automated processing of customer serviceconversations. Through a detailed analysis of the model&rsquo;s working principle,implementation process, and application effects in specific cases, this paperemphasizes the unique advantages of pre-trained models in understanding complexuser intentions and improving the quality of recommendations. In addition, thechallenges and improvement directions for the future are also discussed, suchas how to further improve the generalization ability of the model, the abilityto handle large-scale data sets, and technical strategies to protect userprivacy. Ultimately, the paper points out that the application of Transformerstructural pre-training models in e-commerce has not only driven technologicalinnovation, but also brought substantial benefits to merchants and consumers,and looking forward, these models will continue to play a key role ine-commerce and beyond.</div></details><blockquote><p><strong><em>2024-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.16893v1><strong>The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)</strong></a></p><p><em>Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitatelanguage model with proprietary and private data, where data privacy is apivotal concern. Whereas extensive research has demonstrated the privacy risksof large language models (LLMs), the RAG technique could potentially reshapethe inherent behaviors of LLM generation, posing new privacy issues that arecurrently under-explored. In this work, we conduct extensive empirical studieswith novel attack methods, which demonstrate the vulnerability of RAG systemson leaking the private retrieval database. Despite the new risk brought by RAGon the retrieval data, we further reveal that RAG can mitigate the leakage ofthe LLMs&rsquo; training data. Overall, we provide new insights in this paper forprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAGsystems builders. Our code is available athttps://github.com/phycholosogy/RAG-privacy.</div></details><p><a href=http://arxiv.org/abs/2310.09266v2><strong>User Inference Attacks on Large Language Models</strong></a></p><p><em>Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fine-tuning is a common and effective method for tailoring large languagemodels (LLMs) to specialized tasks and applications. In this paper, we studythe privacy implications of fine-tuning LLMs on user data. To this end, weconsider a realistic threat model, called user inference, wherein an attackerinfers whether or not a user&rsquo;s data was used for fine-tuning. We design attacksfor performing user inference that require only black-box access to thefine-tuned LLM and a few samples from a user which need not be from thefine-tuning dataset. We find that LLMs are susceptible to user inference acrossa variety of fine-tuning datasets, at times with near perfect attack successrates. Further, we theoretically and empirically investigate the propertiesthat make users vulnerable to user inference, finding that outlier users, userswith identifiable shared features between examples, and users that contribute alarge fraction of the fine-tuning data are most susceptible to attack. Based onthese findings, we identify several methods for mitigating user inferenceincluding training with example-level differential privacy, removingwithin-user duplicate examples, and reducing a user&rsquo;s contribution to thetraining data. While these techniques provide partial mitigation of userinference, we highlight the need to develop methods to fully protect fine-tunedLLMs against this privacy risk.</div></details><blockquote><p><strong><em>2024-02-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.06714v2><strong>Exploring Memorization in Fine-tuned Language Models</strong></a></p><p><em>Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have shown great capabilities in various tasksbut also exhibited memorization of training data, raising tremendous privacyand copyright concerns. While prior works have studied memorization duringpre-training, the exploration of memorization during fine-tuning is ratherlimited. Compared to pre-training, fine-tuning typically involves moresensitive data and diverse objectives, thus may bring distinct privacy risksand unique memorization behaviors. In this work, we conduct the firstcomprehensive analysis to explore language models&rsquo; (LMs) memorization duringfine-tuning across tasks. Our studies with open-sourced and our own fine-tunedLMs across various tasks indicate that memorization presents a strong disparityamong different fine-tuning tasks. We provide an intuitive explanation of thistask disparity via sparse coding theory and unveil a strong correlation betweenmemorization and attention score distribution.</div></details><p><a href=http://arxiv.org/abs/2305.00450v2><strong>SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support</strong></a></p><p><em>Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Developing specialized dialogue systems for mental health support requiresmulti-turn conversation data, which has recently garnered increasing attention.However, gathering and releasing large-scale and real-life multi-turnconversations to facilitate advancements in mental health presents challengesdue to data privacy protection, as well as the time and cost involved. Toaddress the challenges related to data scarcity, we introduce SMILE, asingle-turn to multi-turn inclusive language expansion technique that promptsChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our workbegins with the analysis of language transformation, validating the feasibilityof the proposed method when compared with other baseline methods. We thenconduct a study on dialogue diversity, including lexical features, semanticfeatures, and dialogue topics, demonstrating the effectiveness of our proposedmethod. Furthermore, we implement an expert evaluation and the resultsdemonstrate that the dialogues generated with our proposed method are of higherquality than those generated with other baseline methods. Thus, we employ ourmethod to generate a large-scale, diverse, and high-quality dialogue datasetnamed SmileChat, comprising 55,165 dialogues in total with an average of 10.4turns per dialogue. Finally, we utilize the collected corpus to develop amental health chatbot, MeChat. To better assess the overall quality ofSmileChat, we collect a real-life chat dataset comprising 82 counselingdialogues for model evaluation. Both automatic and human evaluationsdemonstrate that our trained dialogue system exhibits significant improvements,showcasing that SmileChat is high-quality and practical.</div></details><blockquote><p><strong><em>2024-02-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.13659v1><strong>Privacy-Preserving Instructions for Aligning Large Language Models</strong></a></p><p><em>Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Service providers of large language model (LLM) applications collect userinstructions in the wild and use them in further aligning LLMs with users&rsquo;intentions. These instructions, which potentially contain sensitiveinformation, are annotated by human workers in the process. This poses a newprivacy risk not addressed by the typical private optimization. To this end, wepropose using synthetic instructions to replace real instructions in dataannotation and model fine-tuning. Formal differential privacy is guaranteed bygenerating those synthetic instructions using privately fine-tuned generators.Crucial in achieving the desired utility is our novel filtering algorithm thatmatches the distribution of the synthetic instructions to that of the realones. In both supervised fine-tuning and reinforcement learning from humanfeedback, our extensive experiments demonstrate the high utility of the finalset of synthetic instructions by showing comparable results to realinstructions. In supervised fine-tuning, models trained with private syntheticinstructions outperform leading open-source models such as Vicuna.</div></details><p><a href=http://arxiv.org/abs/2402.13846v1><strong>Large Language Models are Advanced Anonymizers</strong></a></p><p><em>Robin Staab, Mark Vero, Mislav Balunoviƒá, Martin Vechev</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent work in privacy research on large language models has shown that theyachieve near human-level performance at inferring personal data from real-worldonline texts. With consistently increasing model capabilities, existing textanonymization methods are currently lacking behind regulatory requirements andadversarial threats. This raises the question of how individuals caneffectively protect their personal data in sharing online texts. In this work,we take two steps to answer this question: We first present a new setting forevaluating anonymizations in the face of adversarial LLMs inferences, allowingfor a natural measurement of anonymization performance while remedying some ofthe shortcomings of previous metrics. We then present our LLM-based adversarialanonymization framework leveraging the strong inferential capabilities of LLMsto inform our anonymization procedure. In our experimental evaluation, we showon real-world and synthetic online texts how adversarial anonymizationoutperforms current industry-grade anonymizers both in terms of the resultingutility and privacy.</div></details><blockquote><p><strong><em>2024-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09538v2><strong>Reducing Privacy Risks in Online Self-Disclosures with Language Models</strong></a></p><p><em>Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Self-disclosure, while being common and rewarding in social mediainteraction, also poses privacy risks. In this paper, we take the initiative toprotect the user-side privacy associated with online self-disclosure throughdetection and abstraction. We develop a taxonomy of 19 self-disclosurecategories and curate a large corpus consisting of 4.8K annotated disclosurespans. We then fine-tune a language model for detection, achieving over 65%partial span F$_1$. We further conduct an HCI user study, with 82% ofparticipants viewing the model positively, highlighting its real-worldapplicability. Motivated by the user feedback, we introduce the task ofself-disclosure abstraction, which is paraphrasing disclosures into lessspecific terms while preserving their utility, e.g., &ldquo;Im 16F&rdquo; to &ldquo;I&rsquo;m a teenagegirl&rdquo;. We explore various fine-tuning strategies, and our best model cangenerate diverse abstractions that moderately reduce privacy risks whilemaintaining high utility according to human evaluation. To help users indeciding which disclosures to abstract, we present a task of rating theirimportance for context understanding. Our fine-tuned model achieves 80%accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we willonly release our corpus to researchers who agree to ethical guidelines.</div></details><p><a href=http://arxiv.org/abs/2312.06717v3><strong>Privacy Issues in Large Language Models: A Survey</strong></a></p><p><em>Seth Neel, Peter Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This is the first survey of the active area of AI research that focuses onprivacy issues in Large Language Models (LLMs). Specifically, we focus on workthat red-teams models to highlight privacy risks, attempts to build privacyinto the training or inference process, enables efficient data deletion fromtrained models to comply with existing privacy regulations, and tries tomitigate copyright issues. Our focus is on summarizing technical research thatdevelops algorithms, proves theorems, and runs empirical evaluations. Whilethere is an extensive body of legal and policy work addressing these challengesfrom a different angle, that is not the focus of our survey. Nevertheless,these works, along with recent legal developments do inform how these technicalproblems are formalized, and so we discuss them briefly in Section 1. While wehave made our best effort to include all the relevant work, due to the fastmoving nature of this research we may have missed some recent work. If we havemissed some of your work please contact us, as we will attempt to keep thissurvey relatively up to date. We are maintaining a repository with the list ofpapers covered in this survey and any relevant code that was publicly availableat <a href=https://github.com/safr-ml-lab/survey-llm>https://github.com/safr-ml-lab/survey-llm</a>.</div></details><p><a href=http://arxiv.org/abs/2312.08617v3><strong>RTLCoder: Outperforming GPT-3.5 in Design RTL Generation with Our Open-Source Dataset and Lightweight Solution</strong></a></p><p><em>Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, Zhiyao Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The automatic generation of RTL code (e.g., Verilog) using natural languageinstructions and large language models (LLMs) has attracted significantresearch interest recently. However, most existing approaches heavily rely oncommercial LLMs such as ChatGPT, while open-source LLMs tailored for thisspecific design generation task exhibit notably inferior performance. Theabsence of high-quality open-source solutions restricts the flexibility anddata privacy of this emerging technique. In this study, we present a newcustomized LLM solution with a modest parameter count of only 7B, achievingbetter performance than GPT-3.5 on two representative benchmarks for RTL codegeneration. This remarkable balance between accuracy and efficiency is madepossible by leveraging our new RTL code dataset and a customized LLM algorithm,both of which will be made fully open-source. Furthermore, we have successfullyquantized our LLM to 4-bit with a total size of 4GB, enabling it to function ona single laptop with only slight performance degradation. This efficiencyallows the RTL generator to serve as a local assistant for engineers, ensuringall design privacy concerns are addressed.</div></details><p><a href=http://arxiv.org/abs/2402.05926v2><strong>On the Convergence of Zeroth-Order Federated Tuning for Large Language Models</strong></a></p><p><em>Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area.</div></details><blockquote><p><strong><em>2024-02-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.14845v1><strong>Purifying Large Language Models by Ensembling a Small Language Model</strong></a></p><p><em>Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emerging success of large language models (LLMs) heavily relies oncollecting abundant training data from external (untrusted) sources. Despitesubstantial efforts devoted to data cleaning and curation, well-constructedLLMs have been reported to suffer from copyright infringement, data poisoning,and/or privacy violations, which would impede practical deployment of LLMs. Inthis study, we propose a simple and easily implementable method for purifyingLLMs from the negative effects caused by uncurated data, namely, throughensembling LLMs with benign and small language models (SLMs). Aside fromtheoretical guarantees, we perform comprehensive experiments to empiricallyconfirm the efficacy of ensembling LLMs with SLMs, which can effectivelypreserve the performance of LLMs while mitigating issues such as copyrightinfringement, data poisoning, and privacy violations.</div></details><p><a href=http://arxiv.org/abs/2402.12022v1><strong>Distilling Large Language Models for Text-Attributed Graph Learning</strong></a></p><p><em>Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents.Graph models can efficiently learn TAGs, but their training heavily relies onhuman-annotated labels, which are scarce or even unavailable in manyapplications. Large language models (LLMs) have recently demonstratedremarkable capabilities in few-shot and zero-shot TAG learning, but they sufferfrom scalability, cost, and privacy issues. Therefore, in this work, we focuson synergizing LLMs and graph models with their complementary strengths bydistilling the power of LLMs to a local graph model on TAG learning. To addressthe inherent gaps between LLMs (generative models for texts) and graph models(discriminative models for graphs), we propose first to let LLMs teach aninterpreter with rich textual rationale and then let a student model mimic theinterpreter&rsquo;s reasoning without LLMs&rsquo; textual rationale. Extensive experimentsvalidate the efficacy of our proposed framework.</div></details><p><a href=http://arxiv.org/abs/2311.06318v2><strong>Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion</strong></a></p><p><em>Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen herring, Sujay Kumar Jauhar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) excel at tackling various natural languagetasks. However, due to the significant costs involved in re-training orfine-tuning them, they remain largely static and difficult to personalize.Nevertheless, a variety of applications could benefit from generations that aretailored to users&rsquo; preferences, goals, and knowledge. Among them is web search,where knowing what a user is trying to accomplish, what they care about, andwhat they know can lead to improved search experiences. In this work, wepropose a novel and general approach that augments an LLM with relevant contextfrom users&rsquo; interaction histories with a search engine in order to personalizeits outputs. Specifically, we construct an entity-centric knowledge store foreach user based on their search and browsing activities on the web, which isthen leveraged to provide contextually relevant LLM prompt augmentations. Thisknowledge store is light-weight, since it only produces user-specific aggregateprojections of interests and knowledge onto public knowledge graphs, andleverages existing search log infrastructure, thereby mitigating the privacy,compliance, and scalability concerns associated with building deep userprofiles for personalization. We validate our approach on the task ofcontextual query suggestion, which requires understanding not only the user&rsquo;scurrent search context but also what they historically know and care about.Through a number of experiments based on human evaluation, we show that ourapproach is significantly better than several other LLM-powered baselines,generating query suggestions that are contextually more relevant, personalized,and useful.</div></details><p><a href=http://arxiv.org/abs/2402.12298v1><strong>Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports</strong></a></p><p><em>Felix J. Dorfner, Liv J√ºrgensen, Leonhard Donle, Fares Al Mohamad, Tobias R. Bodenmann, Mason C. Cleveland, Felix Busch, Lisa C. Adams, James Sato, Thomas Schultz, Albert E. Kim, Jameson Merkow, Keno K. Bressem, Christopher P. Bridge</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Introduction: With the rapid advances in large language models (LLMs), therehave been numerous new open source as well as commercial models. While recentpublications have explored GPT-4 in its application to extracting informationof interest from radiology reports, there has not been a real-world comparisonof GPT-4 to different leading open-source models. Materials and Methods: Two different and independent datasets were used. Thefirst dataset consists of 540 chest x-ray reports that were created at theMassachusetts General Hospital between July 2019 and July 2021. The seconddataset consists of 500 chest x-ray reports from the ImaGenome dataset. We thencompared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to theopen-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accuratelylabel the presence of multiple findings in x-ray text reports using differentprompting techniques. Results: On the ImaGenome dataset, the best performing open-source model wasLlama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shotprompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,respectively. On the institutional dataset, the best performing open-sourcemodel was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- andfew-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and0.973, respectively. Conclusion: In this paper, we show that while GPT-4 is superior toopen-source models in zero-shot report labeling, the implementation of few-shotprompting can bring open-source models on par with GPT-4. This shows thatopen-source models could be a performant and privacy preserving alternative toGPT-4 for the task of radiology report classification.</div></details><p><a href=http://arxiv.org/abs/2402.11882v1><strong>NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization</strong></a></p><p><em>Imjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The discharge summary is a one of critical documents in the patient journey,encompassing all events experienced during hospitalization, including multiplevisits, medications, tests, surgery/procedures, and admissions/discharge.Providing a summary of the patient&rsquo;s progress is crucial, as it significantlyinfluences future care and planning. Consequently, clinicians face thelaborious and resource-intensive task of manually collecting, organizing, andcombining all the necessary data for a discharge summary. Therefore, we propose"NOTE", which stands for &ldquo;Notable generation Of patient Text summaries throughan Efficient approach based on direct preference optimization&rdquo;. NOTE is basedon Medical Information Mart for Intensive Care- III dataset and summarizes asingle hospitalization of a patient. Patient events are sequentially combinedand used to generate a discharge summary for each hospitalization. In thepresent circumstances, large language models&rsquo; application programminginterfaces (LLMs&rsquo; APIs) are widely available, but importing and exportingmedical data presents significant challenges due to privacy protection policiesin healthcare institutions. Moreover, to ensure optimal performance, it isessential to implement a lightweight model for internal server or programwithin the hospital. Therefore, we utilized DPO and parameter efficient finetuning (PEFT) techniques to apply a fine-tuning method that guarantees superiorperformance. To demonstrate the practical application of the developed NOTE, weprovide a webpage-based demonstration software. In the future, we will aim todeploy the software available for actual use by clinicians in hospital. NOTEcan be utilized to generate various summaries not only discharge summaries butalso throughout a patient&rsquo;s journey, thereby alleviating the labor-intensiveworkload of clinicians and aiming for increased efficiency.</div></details><blockquote><p><strong><em>2024-02-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.11505v1><strong>Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources</strong></a></p><p><em>Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) has recently been applied to the parameter-efficientfine-tuning of Large Language Models (LLMs). While promising, it raisessignificant challenges due to the heterogeneous resources and datadistributions of clients.This study introduces FlexLoRA, a simple yet effectiveaggregation scheme for LLM fine-tuning, which mitigates the &ldquo;buckets effect&rdquo; intraditional FL that restricts the potential of clients with ample resources bytying them to the capabilities of the least-resourced participants. FlexLoRAallows for dynamic adjustment of local LoRA ranks, fostering the development ofa global model imbued with broader, less task-specific knowledge. Bysynthesizing a full-size LoRA weight from individual client contributions andemploying Singular Value Decomposition (SVD) for weight redistribution,FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600clients performing diverse NLP tasks, our experiments validate the efficacy ofFlexLoRA, with the federated global model achieving up to a 3.1% averageimprovement in downstream NLP task performance. FlexLoRA&rsquo;s practicality isfurther underscored by its seamless integration with existing LoRA-based FLmethods and theoretical analysis, offering a path toward scalable,privacy-preserving federated tuning for LLMs.</div></details><blockquote><p><strong><em>2024-02-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.09959v2><strong>LLM-based Federated Recommendation</strong></a></p><p><em>Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, Tat-Seng Chua</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs), with their advanced contextual understandingabilities, have demonstrated considerable potential in enhancing recommendationsystems via fine-tuning methods. However, fine-tuning requires users&rsquo; behaviordata, which poses considerable privacy risks due to the incorporation ofsensitive user information. The unintended disclosure of such data couldinfringe upon data protection laws and give rise to ethical issues. To mitigatethese privacy issues, Federated Learning for Recommendation (Fed4Rec) hasemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-basedrecommendation presents two main challenges: first, an increase in theimbalance of performance across clients, affecting the system&rsquo;s efficiency overtime, and second, a high demand on clients&rsquo; computational and storage resourcesfor local training and inference of LLMs. To address these challenges, we introduce a Privacy-Preserving LLM-basedRecommendation (PPLR) framework. The PPLR framework employs two primarystrategies. First, it implements a dynamic balance strategy, which involves thedesign of dynamic parameter aggregation and adjustment of learning speed fordifferent clients during the training phase, to ensure relatively balancedperformance across all clients. Second, PPLR adopts a flexible storagestrategy, selectively retaining certain sensitive layers of the language modelon the client side while offloading non-sensitive layers to the server. Thisapproach aims to preserve user privacy while efficiently saving computationaland storage resources. Experimental results demonstrate that PPLR not onlyachieves a balanced performance among clients but also enhances overall systemperformance in a manner that is both computationally and storage-efficient,while effectively protecting user privacy.</div></details><p><a href=http://arxiv.org/abs/2402.08631v2><strong>Knowledge Editing on Black-box Large Language Models</strong></a></p><p><em>Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Yutao Mou, Jinxu Zhao, Weiran Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Knowledge editing (KE) aims to efficiently and precisely modify the behaviorof large language models (LLMs) to update specific knowledge without negativelyinfluencing other knowledge. Current research primarily focuses on white-boxLLMs editing, overlooking an important scenario: black-box LLMs editing, whereLLMs are accessed through interfaces and only textual output is available. Inthis paper, we first officially introduce KE on black-box LLMs and then proposea comprehensive evaluation framework to overcome the limitations of existingevaluations that are not applicable to black-box LLMs editing and lackcomprehensiveness. To tackle privacy leaks of editing data and styleover-editing in current methods, we introduce a novel postEdit framework,resolving privacy concerns through downstream post-processing and maintainingtextual style consistency via fine-grained editing to original responses.Experiments and analysis on two benchmarks demonstrate that postEditoutperforms all baselines and achieves strong generalization, especially withhuge improvements on style retention (average $+20.82%\uparrow$).</div></details><p><a href=http://arxiv.org/abs/2305.06488v4><strong>A Platform for the Biomedical Application of Large Language Models</strong></a></p><p><em>Sebastian Lobentanzer, Shaohong Feng, The BioChatter Consortium, Andreas Maier, Cankun Wang, Jan Baumbach, Nils Krehl, Qin Ma, Julio Saez-Rodriguez</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Current-generation Large Language Models (LLMs) have stirred enormousinterest in recent months, yielding great potential for accessibility andautomation, while simultaneously posing significant challenges and risk ofmisuse. To facilitate interfacing with LLMs in the biomedical space, while atthe same time safeguarding their functionalities through sensible constraints,we propose a dedicated, open-source framework: BioChatter. Based on open-sourcesoftware packages, we synergise the many functionalities that are currentlydeveloping around LLMs, such as knowledge integration / retrieval-augmentedgeneration, model chaining, and benchmarking, resulting in an easy-to-use andinclusive framework for application in many use cases of biomedicine. We focuson robust and user-friendly implementation, including ways to deployprivacy-preserving local open-source LLMs. We demonstrate use cases via twomulti-purpose web apps (<a href=https://chat.biocypher.org>https://chat.biocypher.org</a>), and provide documentation,support, and an open community.</div></details><p><a href=http://arxiv.org/abs/2402.11353v1><strong>Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention</strong></a></p><p><em>Eunkyung Jo, Yuin Jeong, SoHyun Park, Daniel A. Epstein, Young-Ho Kim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent large language models (LLMs) offer the potential to support publichealth monitoring by facilitating health disclosure through open-endedconversations but rarely preserve the knowledge gained about individuals acrossrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents anopportunity to improve engagement and self-disclosure, but we lack anunderstanding of how LTM impacts people&rsquo;s interaction with LLM-driven chatbotsin public health interventions. We examine the case of CareCall &ndash; anLLM-driven voice chatbot with LTM &ndash; through the analysis of 1,252 call logsand interviews with nine users. We found that LTM enhanced health disclosureand fostered positive perceptions of the chatbot by offering familiarity.However, we also observed challenges in promoting self-disclosure through LTM,particularly around addressing chronic health conditions and privacy concerns.We discuss considerations for LTM integration in LLM-driven chatbots for publichealth monitoring, including carefully deciding what topics need to beremembered in light of public health goals.</div></details><blockquote><p><strong><em>2024-02-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.07764v2><strong>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment</strong></a></p><p><em>Minrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, Khaled B. Letaief</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.</div></details><blockquote><p><strong><em>2024-02-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.09132v2><strong>Exploring the Adversarial Capabilities of Large Language Models</strong></a></p><p><em>Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The proliferation of large language models (LLMs) has sparked widespread andgeneral interest due to their strong language generation capabilities, offeringgreat potential for both industry and research. While previous research delvedinto the security and privacy issues of LLMs, the extent to which these modelscan exhibit adversarial behavior remains largely unexplored. Addressing thisgap, we investigate whether common publicly available LLMs have inherentcapabilities to perturb text samples to fool safety measures, so-calledadversarial examples resp.~attacks. More specifically, we investigate whetherLLMs are inherently able to craft adversarial examples out of benign samples tofool existing safe rails. Our experiments, which focus on hate speechdetection, reveal that LLMs succeed in finding adversarial perturbations,effectively undermining hate speech detection systems. Our findings carrysignificant implications for (semi-)autonomous systems relying on LLMs,highlighting potential challenges in their interaction with existing systemsand safety measures.</div></details><p><a href=http://arxiv.org/abs/2402.10052v1><strong>Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination</strong></a></p><p><em>Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vuliƒá</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While displaying impressive generation capabilities across many tasks, LargeLanguage Models (LLMs) still struggle with crucial issues of privacy violationand unwanted exposure of sensitive data. This raises an essential question: howshould we prevent such undesired behavior of LLMs while maintaining theirstrong generation and natural language understanding (NLU) capabilities? Inthis work, we introduce a novel approach termed deliberate imagination in thecontext of LLM unlearning. Instead of trying to forget memorized data, weemploy a self-distillation framework, guiding LLMs to deliberately imaginealternative scenarios. As demonstrated in a wide range of experiments, theproposed method not only effectively unlearns targeted text but also preservesthe LLMs&rsquo; capabilities in open-ended generation tasks as well as in NLU tasks.Our results demonstrate the usefulness of this approach across different modelsand sizes, and also with parameter-efficient fine-tuning, offering a novelpathway to addressing the challenges with private and sensitive data in LLMapplications.</div></details><p><a href=http://arxiv.org/abs/2402.08787v2><strong>Rethinking Machine Unlearning for Large Language Models</strong></a></p><p><em>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.</div></details><blockquote><p><strong><em>2024-02-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.07601v2><strong>Online Advertisements with LLMs: Opportunities and Challenges</strong></a></p><p><em>Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper explores the potential for leveraging Large Language Models (LLM)in the realm of online advertising systems. We delve into essentialrequirements including privacy, latency, reliability as well as thesatisfaction of users and advertisers which such a system must fulfill. Wefurther introduce a general framework for LLM advertisement, consisting ofmodification, bidding, prediction, and auction modules. Different designconsiderations for each module is presented, with an in-depth examination oftheir practicality and the technical challenges inherent to theirimplementation. Finally, we explore the prospect of LLM-based dynamic creativeoptimization as a means to significantly enhance the appeal of advertisementsto users and discuss its additional challenges.</div></details><p><a href=http://arxiv.org/abs/2402.09611v1><strong>Towards Privacy-Aware Sign Language Translation at Scale</strong></a></p><p><em>Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camg√∂z, Jean Maillard</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A major impediment to the advancement of sign language translation (SLT) isdata scarcity. Much of the sign language data currently available on the webcannot be used for training supervised models due to the lack of alignedcaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bearsprivacy risks due to the presence of biometric information, which theresponsible development of SLT technologies should account for. In this work,we propose a two-stage framework for privacy-aware SLT at scale that addressesboth of these issues. We introduce SSVP-SLT, which leverages self-supervisedvideo pretraining on anonymized and unannotated videos, followed by supervisedSLT finetuning on a curated parallel dataset. SSVP-SLT achievesstate-of-the-art finetuned and zero-shot gloss-free SLT performance on theHow2Sign dataset, outperforming the strongest respective baselines by over 3BLEU-4. Based on controlled experiments, we further discuss the advantages andlimitations of self-supervised pretraining and anonymization via facialobfuscation for SLT.</div></details><p><a href=http://arxiv.org/abs/2310.09639v2><strong>DPZero: Private Fine-Tuning of Language Models without Backpropagation</strong></a></p><p><em>Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The widespread practice of fine-tuning large language models (LLMs) ondomain-specific data faces two major challenges in memory and privacy. First,as the size of LLMs continues to grow, the memory demands of gradient-basedtraining methods via backpropagation become prohibitively high. Second, giventhe tendency of LLMs to memorize training data, it is important to protectpotentially sensitive information in the fine-tuning data from beingregurgitated. Zeroth-order methods, which rely solely on forward passes,substantially reduce memory consumption during training. However, directlycombining them with standard differentially private gradient descent suffersfrom growing model size. To bridge this gap, we introduce DPZero, a novelprivate zeroth-order algorithm with nearly dimension-independent rates. Thememory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa onsix downstream tasks.</div></details><blockquote><p><strong><em>2024-02-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.08761v1><strong>JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models</strong></a></p><p><em>Jillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang, Zaid Harchaoui, Yejin Choi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The permanence of online content combined with the enhanced authorshipidentification techniques calls for stronger computational methods to protectthe identity and privacy of online authorship when needed, e.g., blind reviewsfor scientific papers, anonymous online reviews, or anonymous interactions inthe mental health forums. In this paper, we propose an unsupervisedinference-time approach to authorship obfuscation to address the uniquechallenges of authorship obfuscation: lack of supervision data for diverseauthorship and domains, and the need for a sufficient level of revision beyondsimple paraphrasing to obfuscate the authorship, all the while preserving theoriginal content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm forauthorship obfuscation that can be in principle applied to any text andauthorship. Our approach builds on small language models such as GPT2-XL inorder to help avoid disclosing the original content to proprietary LLM&rsquo;s APIs,while also reducing the performance gap between small and large language modelsvia algorithmic enhancement. The key idea behind our approach is to boost thecreative power of smaller language models through constrained decoding, whilealso allowing for user-specified controls and flexibility. Experimental resultsdemonstrate that our approach based on GPT2-XL outperforms previousstate-of-the-art methods based on comparably small models, while performingcompetitively against GPT3.5 175B, a propriety model that is two orders ofmagnitudes larger.</div></details><p><a href=http://arxiv.org/abs/2402.08323v1><strong>Mapping the Ethics of Generative AI: A Comprehensive Scoping Review</strong></a></p><p><em>Thilo Hagendorff</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of generative artificial intelligence and the widespread adoptionof it in society engendered intensive debates about its ethical implicationsand risks. These risks often differ from those associated with traditionaldiscriminative machine learning. To synthesize the recent discourse and map itsnormative concepts, we conducted a scoping review on the ethics of generativeartificial intelligence, including especially large language models andtext-to-image models. Our analysis provides a taxonomy of 378 normative issuesin 19 topic areas and ranks them according to their prevalence in theliterature. The study offers a comprehensive overview for scholars,practitioners, or policymakers, condensing the ethical debates surroundingfairness, safety, harmful content, hallucinations, privacy, interaction risks,security, alignment, societal impacts, and others. We discuss the results,evaluate imbalances in the literature, and explore unsubstantiated riskscenarios.</div></details><p><a href=http://arxiv.org/abs/2402.08219v1><strong>BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models</strong></a></p><p><em>Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Geminifor specific tasks is challenging. Due to the opacity in their parameters,embeddings, and even output probabilities, existing fine-tuning adaptationmethods are inapplicable. Consequently, adapting these black-box LLMs is onlypossible through their API services, raising concerns about transparency,privacy, and cost. To address these challenges, we introduce BBox-Adapter, anovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes targetand source domain data by treating target data as positive and source data asnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss topromote the likelihood of target domain data while penalizing that of thesource domain. Furthermore, it features an online adaptation mechanism, whichincorporates real-time positive data sampling from ground-truth, human, or AIfeedback, coupled with negative data from previous adaptations. Extensiveexperiments demonstrate BBox-Adapter&rsquo;s effectiveness and cost efficiency. Itimproves model performance by up to 6.77% across diverse tasks and domains,while reducing training and inference costs by 31.30x and 1.84x, respectively.</div></details><blockquote><p><strong><em>2024-02-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.09477v1><strong>PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining</strong></a></p><p><em>Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, S√©bastien Gambs, Mathias L√©cuyer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce a privacy auditing scheme for ML models that relies onmembership inference attacks using generated data as &ldquo;non-members&rdquo;. Thisscheme, which we call PANORAMIA, quantifies the privacy leakage for large-scaleML models without control of the training process or model re-training and onlyrequires access to a subset of the training data. To demonstrate itsapplicability, we evaluate our auditing scheme across multiple ML domains,ranging from image and tabular data classification to large-scale languagemodels.</div></details><p><a href=http://arxiv.org/abs/2402.07792v1><strong>Empowering Federated Learning for Massive Models with NVIDIA FLARE</strong></a></p><p><em>Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the ever-evolving landscape of artificial intelligence (AI) and largelanguage models (LLMs), handling and leveraging data effectively has become acritical challenge. Most state-of-the-art machine learning algorithms aredata-centric. However, as the lifeblood of model performance, necessary datacannot always be centralized due to various factors such as privacy,regulation, geopolitics, copyright issues, and the sheer effort required tomove vast datasets. In this paper, we explore how federated learning enabled byNVIDIA FLARE can address these challenges with easy and scalable integrationcapabilities, enabling parameter-efficient and full supervised fine-tuning ofLLMs for natural language processing and biopharmaceutical applications toenhance their accuracy and robustness.</div></details><p><a href=http://arxiv.org/abs/2402.07367v1><strong>Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code</strong></a></p><p><em>Liming Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Mini-applications, commonly referred to as mini-apps, are compact softwareprograms embedded within larger applications or platforms, offering targetedfunctionality without the need for separate installations. Typically web-basedor cloud-hosted, these mini-apps streamline user experiences by providingfocused services accessible through web browsers or mobile apps. Theirsimplicity, speed, and integration capabilities make them valuable additions tomessaging platforms, social media networks, e-commerce sites, and variousdigital environments. WeChat Mini Programs, a prominent feature of China&rsquo;sleading messaging app, exemplify this trend, offering users a seamless array ofservices without additional downloads. Leveraging WeChat&rsquo;s extensive user baseand payment infrastructure, Mini Programs facilitate efficient transactions andbridge online and offline experiences, shaping China&rsquo;s digital landscapesignificantly. This paper investigates the potential of employing LargeLanguage Models (LLMs) to detect privacy breaches within WeChat Mini Programs.Given the widespread use of Mini Programs and growing concerns about dataprivacy, this research seeks to determine if LLMs can effectively identifyinstances of privacy leakage within this ecosystem. Through meticulous analysisand experimentation, we aim to highlight the efficacy of LLMs in safeguardinguser privacy and security within the WeChat Mini Program environment, therebycontributing to a more secure digital landscape.</div></details><p><a href=http://arxiv.org/abs/2402.07812v1><strong>Retrieval-Augmented Thought Process as Sequential Decision Making</strong></a></p><p><em>Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have demonstrated their strong ability to assistpeople and show &ldquo;sparks of intelligence&rdquo;. However, several open challengeshinder their wider application: such as concerns over privacy, tendencies toproduce hallucinations, and difficulties in handling long contexts. In thiswork, we address those challenges by introducing the Retrieval-AugmentedThought Process (RATP). Given access to external knowledge, RATP formulates thethought generation of LLMs as a multiple-step decision process. To optimizesuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns aQ-value estimator that permits cost-efficient inference. In addressing the taskof question-answering with private data, where ethical and security concernslimit LLM training methods, RATP achieves a 50% improvement over existingin-context retrieval-augmented language models.</div></details><p><a href=http://arxiv.org/abs/2402.05868v2><strong>EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models</strong></a></p><p><em>Guo Lin, Wenyue Hua, Yongfeng Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanism EmojiCryptto protect user privacy. It uses Emoji to encrypt the user inputs beforesending them to LLM, effectively rendering them indecipherable to human orLLM&rsquo;s examination while retaining the original intent of the prompt, thusensuring the model&rsquo;s performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that EmojiCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/EmojiCrypt.</div></details><p><a href=http://arxiv.org/abs/2401.15284v2><strong>Five ethical principles for generative AI in scientific research</strong></a></p><p><em>Zhicheng Lin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generative artificial intelligence tools like large language models arerapidly transforming academic research and real world applications. However,discussions on ethical guidelines for generative AI in science remainfragmented, underscoring the urgent need for consensus based standards. Thispaper offers an initial framework by developing analyses and mitigationstrategies across five key themes: understanding model limitations regardingtruthfulness and bias; respecting privacy, confidentiality, and copyright;avoiding plagiarism and policy violations when incorporating model output;ensuring applications provide overall benefit; and using AI transparently andreproducibly. Common scenarios are outlined to demonstrate potential ethicalviolations. We argue that global consensus coupled with professional trainingand reasonable enforcement are critical to promoting the benefits of AI whilesafeguarding research integrity.</div></details><p><a href=http://arxiv.org/abs/2402.07510v1><strong>Secret Collusion Among Generative AI Agents</strong></a></p><p><em>Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent capability increases in large language models (LLMs) open upapplications in which teams of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both the AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.</div></details><blockquote><p><strong><em>2024-02-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.07334v1><strong>Differentially Private Training of Mixture of Experts Models</strong></a></p><p><em>Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This position paper investigates the integration of Differential Privacy (DP)in the training of Mixture of Experts (MoE) models within the field of naturallanguage processing. As Large Language Models (LLMs) scale to billions ofparameters, leveraging expansive datasets, they exhibit enhanced linguisticcapabilities and emergent abilities. However, this growth raises significantcomputational and privacy concerns. Our study addresses these issues byexploring the potential of MoE models, known for their computationalefficiency, and the application of DP, a standard for privacy preservation. Wepresent the first known attempt to train MoE models under the constraints ofDP, addressing the unique challenges posed by their architecture and thecomplexities of DP integration. Our initial experimental studies demonstratethat MoE models can be effectively trained with DP, achieving performance thatis competitive with their non-private counterparts. This initial study aims toprovide valuable insights and ignite further research in the domain ofprivacy-preserving MoE models, softly laying the groundwork for prospectivedevelopments in this evolving field.</div></details><blockquote><p><strong><em>2024-02-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.06954v1><strong>OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning</strong></a></p><p><em>Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Trained on massive publicly available data, large language models (LLMs) havedemonstrated tremendous success across various fields. While more datacontributes to better performance, a disconcerting reality is that high-qualitypublic data will be exhausted in a few years. In this paper, we offer apotential next step for contemporary LLMs: collaborative and privacy-preservingLLM training on the underutilized distributed private data via federatedlearning (FL), where multiple data owners collaboratively train a shared modelwithout transmitting raw data. To achieve this, we build a concise, integrated,and research-friendly framework/codebase, named OpenFedLLM. It covers federatedinstruction tuning for enhancing instruction-following capability, federatedvalue alignment for aligning with human values, and 7 representative FLalgorithms. Besides, OpenFedLLM supports training on diverse domains, where wecover 8 training datasets; and provides comprehensive evaluations, where wecover 30+ evaluation metrics. Through extensive experiments, we observe thatall FL algorithms outperform local training on training LLMs, demonstrating aclear performance improvement across a variety of settings. Notably, in afinancial benchmark, Llama2-7B fine-tuned by applying any FL algorithm canoutperform GPT-4 by a significant margin while the model obtained throughindividual training cannot, demonstrating strong motivation for clients toparticipate in FL. The code is available athttps://github.com/rui-ye/OpenFedLLM.</div></details><blockquote><p><strong><em>2024-02-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.06806v1><strong>Towards Principled Assessment of Tabular Data Synthesis Algorithms</strong></a></p><p><em>Yuntao Du, Ninghui Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Data synthesis has been advocated as an important approach for utilizing datawhile protecting data privacy. A large number of tabular data synthesisalgorithms (which we call synthesizers) have been proposed. Some synthesizerssatisfy Differential Privacy, while others aim to provide privacy in aheuristic fashion. A comprehensive understanding of the strengths andweaknesses of these synthesizers remains elusive due to lacking principledevaluation metrics and missing head-to-head comparisons of newly developedsynthesizers that take advantage of diffusion models and large language modelswith state-of-the-art marginal-based synthesizers. In this paper, we present a principled and systematic evaluation frameworkfor assessing tabular data synthesis algorithms. Specifically, we examine andcritique existing evaluation metrics, and introduce a set of new metrics interms of fidelity, privacy, and utility to address their limitations. Based onthe proposed metrics, we also devise a unified objective for tuning, which canconsistently improve the quality of synthetic data for all methods. Weconducted extensive evaluations of 8 different types of synthesizers on 12datasets and identified some interesting findings, which offer new directionsfor privacy-preserving data synthesis.</div></details><p><a href=http://arxiv.org/abs/2402.06334v1><strong>ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs</strong></a></p><p><em>Fernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: ExaRanker recently introduced an approach to training information retrieval(IR) models, incorporating natural language explanations as additional labels.The method addresses the challenge of limited labeled examples, leading toimprovements in the effectiveness of IR models. However, the initial resultswere based on proprietary language models such as GPT-3.5, which posedconstraints on dataset size due to its cost and data privacy. In this paper, weintroduce ExaRanker-Open, where we adapt and explore the use of open-sourcelanguage models to generate explanations. The method has been tested usingdifferent LLMs and datasets sizes to better comprehend the effectivecontribution of data augmentation. Our findings reveal that incorporatingexplanations consistently enhances neural rankers, with benefits escalating asthe LLM size increases. Notably, the data augmentation method provesadvantageous even with large datasets, as evidenced by ExaRanker surpassing thetarget baseline by 0.6 nDCG@10 points in our study. To encourage furtheradvancements by the research community, we have open-sourced both the code anddatasets at <a href=https://github.com/unicamp-dl/ExaRanker>https://github.com/unicamp-dl/ExaRanker</a>.</div></details><blockquote><p><strong><em>2024-02-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.11441v2><strong>Social Learning: Towards Collaborative Learning with Large Language Models</strong></a></p><p><em>Amirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka, Matt Sharifi, Blaise Aguera y Arcas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce the framework of &ldquo;social learning&rdquo; in the context of largelanguage models (LLMs), whereby models share knowledge with each other in aprivacy-aware manner using natural language. We present and evaluate twoapproaches for knowledge transfer between LLMs. In the first scenario, we allowthe model to generate abstract prompts aiming to teach the task. In our secondapproach, models transfer knowledge by generating synthetic examples. Weevaluate these methods across diverse datasets and quantify memorization as aproxy for privacy loss. These techniques inspired by social learning yieldpromising results with low memorization of the original data. In particular, weshow that performance using these methods is comparable to results with the useof original labels and prompts. Our work demonstrates the viability of sociallearning for LLMs, establishes baseline approaches and highlights severalunexplored areas for future work.</div></details><p><a href=http://arxiv.org/abs/2306.14263v2><strong>Revolutionizing Cyber Threat Detection with Large Language Models: A privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices</strong></a></p><p><em>Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Merouane Debbah, Thierry Lestable, Narinderjit Singh Thandi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The field of Natural Language Processing (NLP) is currently undergoing arevolutionary transformation driven by the power of pre-trained Large LanguageModels (LLMs) based on groundbreaking Transformer architectures. As thefrequency and diversity of cybersecurity attacks continue to rise, theimportance of incident detection has significantly increased. IoT devices areexpanding rapidly, resulting in a growing need for efficient techniques toautonomously identify network-based attacks in IoT networks with both highprecision and minimal computational requirements. This paper presentsSecurityBERT, a novel architecture that leverages the Bidirectional EncoderRepresentations from Transformers (BERT) model for cyber threat detection inIoT networks. During the training of SecurityBERT, we incorporated a novelprivacy-preserving encoding technique called Privacy-Preserving Fixed-LengthEncoding (PPFLE). We effectively represented network traffic data in astructured format by combining PPFLE with the Byte-level Byte-Pair Encoder(BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperformstraditional Machine Learning (ML) and Deep Learning (DL) methods, such asConvolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), incyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, ourexperimental analysis shows that SecurityBERT achieved an impressive 98.2%overall accuracy in identifying fourteen distinct attack types, surpassingprevious records set by hybrid solutions such as GAN-Transformer-basedarchitectures and CNN-LSTM models. With an inference time of less than 0.15seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERTis ideally suited for real-life traffic analysis and a suitable choice fordeployment on resource-constrained IoT devices.</div></details><p><a href=http://arxiv.org/abs/2402.07940v1><strong>LLMs Among Us: Generative AI Participating in Digital Discourse</strong></a></p><p><em>Kristina Radivojevic, Nicholas Clark, Paul Brenner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of Large Language Models (LLMs) has great potential to reshapethe landscape of many social media platforms. While this can bring promisingopportunities, it also raises many threats, such as biases and privacyconcerns, and may contribute to the spread of propaganda by malicious actors.We developed the &ldquo;LLMs Among Us&rdquo; experimental framework on top of the Mastodonsocial media platform for bot and human participants to communicate withoutknowing the ratio or nature of bot and human participants. We built 10 personaswith three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted threerounds of the experiment and surveyed participants after each round to measurethe ability of LLMs to pose as human participants without human detection. Wefound that participants correctly identified the nature of other users in theexperiment only 42% of the time despite knowing the presence of both bots andhumans. We also found that the choice of persona had substantially more impacton human perception than the choice of mainstream LLMs.</div></details><blockquote><p><strong><em>2024-02-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.04489v1><strong>De-amplifying Bias from Differential Privacy in Language Model Fine-tuning</strong></a></p><p><em>Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fairness and privacy are two important values machine learning (ML)practitioners often seek to operationalize in models. Fairness aims to reducemodel bias for social/demographic sub-groups. Privacy via differential privacy(DP) mechanisms, on the other hand, limits the impact of any individual&rsquo;straining data on the resulting model. The trade-offs between privacy andfairness goals of trustworthy ML pose a challenge to those wishing to addressboth. We show that DP amplifies gender, racial, and religious bias whenfine-tuning large language models (LLMs), producing models more biased thanones fine-tuned without DP. We find the cause of the amplification to be adisparity in convergence of gradients across sub-groups. Through the case ofbinary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),a known method for addressing bias, also mitigates bias amplification by DP. Asa consequence, DP and CDA together can be used to fine-tune models whilemaintaining both fairness and privacy.</div></details><p><a href=http://arxiv.org/abs/2310.08320v3><strong>Defending Our Privacy With Backdoors</strong></a></p><p><em>Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The proliferation of large AI models trained on uncurated, often sensitiveweb-scraped data has raised significant privacy concerns. One of the concernsis that adversaries can extract information about the training data usingprivacy attacks. Unfortunately, the task of removing specific information fromthe models without sacrificing performance is not straightforward and hasproven to be challenging. We propose a rather easy yet effective defense basedon backdoor attacks to remove private information such as names and faces ofindividuals from vision-language models by fine-tuning them for only a fewminutes instead of re-training them from scratch. Specifically, throughstrategic insertion of backdoors into text encoders, we align the embeddings ofsensitive phrases with those of neutral terms-&ldquo;a person&rdquo; instead of theperson&rsquo;s actual name. For image encoders, we map embeddings of individuals tobe removed from the model to a universal, anonymous embedding. Our empiricalresults demonstrate the effectiveness of our backdoor-based defense on CLIP byassessing its performance using a specialized privacy attack for zero-shotclassifiers. Our approach provides not only a new &ldquo;dual-use&rdquo; perspective onbackdoor attacks, but also presents a promising avenue to enhance the privacyof individuals within models trained on uncurated web-scraped data.</div></details><blockquote><p><strong><em>2024-02-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.03907v1><strong>Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy</strong></a></p><p><em>Efe Bozkir, S√ºleyman √ñzdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent developments in computer graphics, hardware, artificial intelligence(AI), and human-computer interaction likely lead to extended reality (XR)devices and setups being more pervasive. While these devices and setups provideusers with interactive, engaging, and immersive experiences with differentsensing modalities, such as eye and hand trackers, many non-player charactersare utilized in a pre-scripted way or by conventional AI techniques. In thispaper, we argue for using large language models (LLMs) in XR by embedding themin virtual avatars or as narratives to facilitate more inclusive experiencesthrough prompt engineering according to user profiles and fine-tuning the LLMsfor particular purposes. We argue that such inclusion will facilitate diversityfor XR use. In addition, we believe that with the versatile conversationalcapabilities of LLMs, users will engage more with XR environments, which mighthelp XR be more used in everyday life. Lastly, we speculate that combining theinformation provided to LLM-powered environments by the users and the biometricdata obtained through the sensors might lead to novel privacy invasions. Whilestudying such possible privacy invasions, user privacy concerns and preferencesshould also be investigated. In summary, despite some challenges, embeddingLLMs into XR is a promising and novel research area with several opportunities.</div></details><p><a href=http://arxiv.org/abs/2401.15656v2><strong>LLsM: Generative Linguistic Steganography with Large Language Model</strong></a></p><p><em>Yihao Wang, Ruiqi Song, Ru Zhang, Jianyi Liu, Lingxiao Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Linguistic Steganography (LS) tasks aim to generate steganographic text(stego) based on secret information. Only authorized recipients can perceivethe existence of secrets in the texts and extract them, thereby preservingprivacy. However, the controllability of the stego generated by existingschemes is poor, and the stego is difficult to contain specific discoursecharacteristics such as style. As a result, the stego is easily detectable,compromising covert communication. To address these problems, this paperproposes LLsM, the first LS with the Large Language Model (LLM). We fine-tunedthe LLaMA2 with a large-scale constructed dataset encompassing rich discoursecharacteristics, which enables the fine-tuned LLM to generate texts withspecific discourse in a controllable manner. Then the discourse is used asguiding information and inputted into the fine-tuned LLM in the form of thePrompt together with secret. On this basis, the constructed candidate pool willbe range encoded and use secret to determine the interval. The same prefix ofthis interval&rsquo;s beginning and ending is the secret embedded at this moment.Experiments show that LLsM performs superior to prevalent LS-task andrelated-task baselines regarding text quality, statistical analysis, discoursematching, and anti-steganalysis. In particular, LLsM&rsquo;s MAUVE matric surpassessome baselines by 70%-80%, and its anti-steganalysis performance is 30%-40%higher. Notably, we also present examples of longer stegos generated by LLsM,showing its potential superiority in long LS tasks.</div></details><p><a href=http://arxiv.org/abs/2402.04401v1><strong>Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning</strong></a></p><p><em>Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Personalization in large language models (LLMs) is increasingly important,aiming to align LLM&rsquo;s interactions, content, and recommendations withindividual user preferences. Recent advances in LLM personalization havespotlighted effective prompt design, by enriching user queries withnon-parametric knowledge through behavior history retrieval and textualprofiles. However, these approaches were limited due to a lack of modelownership, resulting in constrained customization and privacy issues. Moreover,they often failed to accurately capture user behavior patterns, especially incases where user data were complex and dynamic. To address these shortcomings,we introduce One PEFT Per User (OPPU), which employs personalizedparameter-efficient fine-tuning (PEFT) modules, to store user-specific behaviorpatterns and preferences. By plugging in users&rsquo; personal PEFT parameters, theycan own and use their LLMs personally. OPPU integrates parametric userknowledge in the personal PEFT parameters with the non-parametric knowledgeacquired through retrieval and profile. This integration adapts individual LLMsto user behavior shifts. Experimental results demonstrate that OPPUsignificantly outperforms existing prompt-based methods across seven diversetasks in the LaMP benchmark. Further in-depth studies reveal OPPU&rsquo;s enhancedcapabilities in handling user behavior shifts, modeling users at differentactive levels, maintaining robustness across various user history formats, anddisplaying versatility with different PEFT methods.</div></details><blockquote><p><strong><em>2024-02-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.03435v1><strong>Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach</strong></a></p><p><em>Sergi Blanco-Cuaresma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This study explores the use of Large Language Models (LLMs) to analyze textcomments from Reddit users, aiming to achieve two primary objectives: firstly,to pinpoint critical excerpts that support a predefined psychologicalassessment of suicidal risk; and secondly, to summarize the material tosubstantiate the preassigned suicidal risk level. The work is circumscribed tothe use of &ldquo;open-source&rdquo; LLMs that can be run locally, thereby enhancing dataprivacy. Furthermore, it prioritizes models with low computationalrequirements, making it accessible to both individuals and institutionsoperating on limited computing budgets. The implemented strategy only relies ona carefully crafted prompt and a grammar to guide the LLM&rsquo;s text completion.Despite its simplicity, the evaluation metrics show outstanding results, makingit a valuable privacy-focused and cost-effective approach. This work is part ofthe Computational Linguistics and Clinical Psychology (CLPsych) 2024 sharedtask.</div></details><p><a href=http://arxiv.org/abs/2402.02987v1><strong>Conversation Reconstruction Attack Against GPT Models</strong></a></p><p><em>Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent times, significant advancements have been made in the field oflarge language models (LLMs), represented by GPT series models. To optimizetask execution, users often engage in multi-round conversations with GPT modelshosted in cloud environments. These multi-round conversations, potentiallyreplete with private information, require transmission and storage within thecloud. However, this operational paradigm introduces additional attacksurfaces. In this paper, we first introduce a specific ConversationReconstruction Attack targeting GPT models. Our introduced ConversationReconstruction Attack is composed of two steps: hijacking a session andreconstructing the conversations. Subsequently, we offer an exhaustiveevaluation of the privacy risks inherent in conversations when GPT models aresubjected to the proposed attack. However, GPT-4 demonstrates certainrobustness to the proposed attacks. We then introduce two advanced attacksaimed at better reconstructing previous conversations, specifically the UNRattack and the PBU attack. Our experimental findings indicate that the PBUattack yields substantial performance across all models, achieving semanticsimilarity scores exceeding 0.60, while the UNR attack is effective solely onGPT-3.5. Our results reveal the concern about privacy risks associated withconversations involving GPT models and aim to draw the community&rsquo;s attention toprevent the potential misuse of these models&rsquo; remarkable capabilities. We willresponsibly disclose our findings to the suppliers of related large languagemodels.</div></details><p><a href=http://arxiv.org/abs/2402.02975v1><strong>Putting Context in Context: the Impact of Discussion Structure on Text Classification</strong></a></p><p><em>Nicol√≤ Penzo, Antonio Longa, Bruno Lepri, Sara Tonelli, Marco Guerini</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Current text classification approaches usually focus on the content to beclassified. Contextual aspects (both linguistic and extra-linguistic) areusually neglected, even in tasks based on online discussions. Still in manycases the multi-party and multi-turn nature of the context from which theseelements are selected can be fruitfully exploited. In this work, we propose aseries of experiments on a large dataset for stance detection in English, inwhich we evaluate the contribution of different types of contextualinformation, i.e. linguistic, structural and temporal, by feeding them asnatural language input into a transformer-based model. We also experiment withdifferent amounts of training data and analyse the topology of local discussionnetworks in a privacy-compliant way. Results show that structural informationcan be highly beneficial to text classification but only under certaincircumstances (e.g. depending on the amount of training data and on discussionchain complexity). Indeed, we show that contextual information on smallerdatasets from other classification tasks does not yield significantimprovements. Our framework, based on local discussion networks, allows theintegration of structural information, while minimising user profiling, thuspreserving their privacy.</div></details><blockquote><p><strong><em>2024-02-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.02315v1><strong>A Survey of Large Language Models in Finance (FinLLMs)</strong></a></p><p><em>Jean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have shown remarkable capabilities across a widevariety of Natural Language Processing (NLP) tasks and have attracted attentionfrom multiple domains, including financial services. Despite the extensiveresearch into general-domain LLMs, and their immense potential in finance,Financial LLM (FinLLM) research remains limited. This survey provides acomprehensive overview of FinLLMs, including their history, techniques,performance, and opportunities and challenges. Firstly, we present achronological overview of general-domain Pre-trained Language Models (PLMs)through to current FinLLMs, including the GPT-series, selected open-sourceLLMs, and financial LMs. Secondly, we compare five techniques used acrossfinancial PLMs and FinLLMs, including training methods, training data, andfine-tuning methods. Thirdly, we summarize the performance evaluations of sixbenchmark tasks and datasets. In addition, we provide eight advanced financialNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, wediscuss the opportunities and the challenges facing FinLLMs, such ashallucination, privacy, and efficiency. To support AI research in finance, wecompile a collection of accessible datasets and evaluation benchmarks onGitHub.</div></details><blockquote><p><strong><em>2024-02-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01994v1><strong>Human-Centered Privacy Research in the Age of Large Language Models</strong></a></p><p><em>Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, Zhiping Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of large language models (LLMs), and their increased use inuser-facing systems, has led to substantial privacy concerns. To date, researchon these privacy concerns has been model-centered: exploring how LLMs lead toprivacy risks like memorization, or can be used to infer personalcharacteristics about people from their content. We argue that there is a needfor more research focusing on the human aspect of these privacy issues: e.g.,research on how design paradigms for LLMs affect users&rsquo; disclosure behaviors,users&rsquo; mental models and preferences for privacy controls, and the design oftools, systems, and artifacts that empower end-users to reclaim ownership overtheir personal data. To build usable, efficient, and privacy-friendly systemspowered by these models with imperfect privacy properties, our goal is toinitiate discussions to outline an agenda for conducting human-centeredresearch on privacy issues in LLM-powered systems. This Special Interest Group(SIG) aims to bring together researchers with backgrounds in usable securityand privacy, human-AI collaboration, NLP, or any other related domains to sharetheir perspectives and experiences on this problem, to help our communityestablish a collective understanding of the challenges, research opportunities,research methods, and strategies to collaborate with researchers outside ofHCI.</div></details><blockquote><p><strong><em>2024-02-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01117v1><strong>DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models</strong></a></p><p><em>Mohammadreza Pourreza, Davood Rafiei</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Leading models for the text-to-SQL task heavily rely on proprietary LargeLanguage Models (LLMs), posing concerns over data privacy. Closing theperformance gap between small open-source models and large proprietary modelsis crucial to mitigate this reliance. To this end, we introduce a noveltwo-stage fine-tuning approach that decomposes the task into two simpler tasks.Through comprehensive evaluation on two large cross-domain datasets and twosmall LLMs, we show that this approach improves execution accuracy by 3 to 7percent, effectively aligning the performance of open-source models with theirproprietary counterparts.</div></details><p><a href=http://arxiv.org/abs/2402.01931v1><strong>Digits micro-model for accurate and secure transactions</strong></a></p><p><em>Chirag Chhablani, Nikhita Sharma, Jordan Hosier, Vijay K. Gurbani</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Automatic Speech Recognition (ASR) systems are used in the financial domainto enhance the caller experience by enabling natural language understanding andfacilitating efficient and intuitive interactions. Increasing use of ASRsystems requires that such systems exhibit very low error rates. Thepredominant ASR models to collect numeric data are large, general-purposecommercial models &ndash; Google Speech-to-text (STT), or Amazon Transcribe &ndash; oropen source (OpenAI&rsquo;s Whisper). Such ASR models are trained on hundreds ofthousands of hours of audio data and require considerable resources to run.Despite recent progress large speech recognition models, we highlight thepotential of smaller, specialized &ldquo;micro&rdquo; models. Such light models can betrained perform well on number recognition specific tasks, competing withgeneral models like Whisper or Google STT while using less than 80 minutes oftraining time and occupying at least an order of less memory resources. Also,unlike larger speech recognition models, micro-models are trained on carefullyselected and curated datasets, which makes them highly accurate, agile, andeasy to retrain, while using low compute resources. We present our work oncreating micro models for multi-digit number recognition that handle diversespeaking styles reflecting real-world pronunciation patterns. Our workcontributes to domain-specific ASR models, improving digit recognitionaccuracy, and privacy of data. An added advantage, their low resourceconsumption allows them to be hosted on-premise, keeping private data localinstead uploading to an external cloud. Our results indicate that ourmicro-model makes less errors than the best-of-breed commercial or open-sourceASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for ourmodel versus 11 GB VRAM for Whisper).</div></details><blockquote><p><strong><em>2024-02-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01018v1><strong>HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent</strong></a></p><p><em>Weijie Xu, Zicheng Huang, Wenxiang Hu, Xi Fang, Rajesh Kumar Cherukuri, Naumaan Nayyar, Lorenzo Malandri, Srinivasan H. Sengamedu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advancements in Large Language Models (LLMs) have been reshapingNatural Language Processing (NLP) task in several domains. Their use in thefield of Human Resources (HR) has still room for expansions and could bebeneficial for several time consuming tasks. Examples such as time-offsubmissions, medical claims filing, and access requests are noteworthy, butthey are by no means the sole instances. However, the aforementioneddevelopments must grapple with the pivotal challenge of constructing ahigh-quality training dataset. On one hand, most conversation datasets aresolving problems for customers not employees. On the other hand, gatheringconversations with HR could raise privacy concerns. To solve it, we introduceHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HRdomains to evaluate LLM Agent. Our work has the following contributions: (1) Itis the first labeled open-sourced conversation dataset in the HR domain for NLPresearch. (2) It provides a detailed recipe for the data generation procedurealong with data analysis and human evaluations. The data generation pipeline istransferable and can be easily adapted for labeled conversation data generationin other domains. (3) The proposed data-collection pipeline is mostly based onLLMs with minimal human involvement for annotation, which is time andcost-efficient.</div></details><blockquote><p><strong><em>2024-01-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00179v1><strong>De-identification is not always enough</strong></a></p><p><em>Atiquer Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, Xiaoqian Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: For sharing privacy-sensitive data, de-identification is commonly regarded asadequate for safeguarding privacy. Synthetic data is also being considered as aprivacy-preserving alternative. Recent successes with numerical and tabulardata generative models and the breakthroughs in large generative languagemodels raise the question of whether synthetically generated clinical notescould be a viable alternative to real notes for research purposes. In thiswork, we demonstrated that (i) de-identification of real clinical notes doesnot protect records against a membership inference attack, (ii) proposed anovel approach to generate synthetic clinical notes using the currentstate-of-the-art large language models, (iii) evaluated the performance of thesynthetically generated notes in a clinical domain task, and (iv) proposed away to mount a membership inference attack where the target model is trainedwith synthetic data. We observed that when synthetically generated notesclosely match the performance of real data, they also exhibit similar privacyconcerns to the real data. Whether other approaches to synthetically generatedclinical notes could offer better trade-offs and become a better alternative tosensitive real notes warrants further investigation.</div></details><p><a href=http://arxiv.org/abs/2312.06353v3><strong>Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes</strong></a></p><p><em>Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, Shuiguang Deng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained large language models (LLMs) need fine-tuning to improve theirresponsiveness to natural language instructions. Federated learning offers away to fine-tune LLMs using the abundant data on end devices withoutcompromising data privacy. Most existing federated fine-tuning methods for LLMsrely on parameter-efficient fine-tuning techniques, which may not reach theperformance height possible with full-parameter tuning. However, federatedfull-parameter tuning of LLMs is a non-trivial problem due to the immensecommunication cost. This work introduces FedKSeed that employs zeroth-orderoptimization with a finite set of random seeds. It significantly reducestransmission requirements between the server and clients to just a few randomseeds and scalar gradients, amounting to only a few thousand bytes, makingfederated full-parameter tuning of billion-sized LLMs possible on devices.Building on it, we develop a strategy enabling probability-differentiated seedsampling, prioritizing perturbations with greater impact on model accuracy.Experiments across six scenarios with various LLMs, datasets and datapartitions demonstrate that our approach outperforms existing federated LLMfine-tuning methods in both communication efficiency and new taskgeneralization.</div></details><blockquote><p><strong><em>2024-01-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.00888v1><strong>Security and Privacy Challenges of Large Language Models: A Survey</strong></a></p><p><em>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have demonstrated extraordinary capabilities andcontributed to multiple fields, such as generating and summarizing text,language translation, and question-answering. Nowadays, LLM is becoming a verypopular tool in computerized language processing tasks, with the capability toanalyze complicated linguistic patterns and provide relevant and appropriateresponses depending on the context. While offering significant advantages,these models are also vulnerable to security and privacy attacks, such asjailbreaking attacks, data poisoning attacks, and Personally IdentifiableInformation (PII) leakage attacks. This survey provides a thorough review ofthe security and privacy challenges of LLMs for both training data and users,along with the application-based risks in various domains, such astransportation, education, and healthcare. We assess the extent of LLMvulnerabilities, investigate emerging security and privacy attacks for LLMs,and review the potential defense mechanisms. Additionally, the survey outlinesexisting research gaps in this domain and highlights future researchdirections.</div></details><p><a href=http://arxiv.org/abs/2402.01758v1><strong>Aalap: AI Assistant for Legal & Paralegal Functions in India</strong></a></p><p><em>Aman Tiwari, Prathamesh Kalamkar, Atreyo Banerjee, Saurabh Karn, Varun Hemachandran, Smita Gupta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Using proprietary Large Language Models on legal tasks poses challenges dueto data privacy issues, domain data heterogeneity, domain knowledgesophistication, and domain objectives uniqueness. We created Aalalp, afine-tuned Mistral 7B model on instructions data related to specific Indianlegal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31% ofour test data and obtains an equivalent score in 34% of the test data asevaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoningrather than legal recall. Aalap is definitely helpful for the day-to-dayactivities of lawyers, judges, or anyone working in legal systems.</div></details><blockquote><p><strong><em>2024-01-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.05644v2><strong>Towards Building the Federated GPT: Federated Instruction Tuning</strong></a></p><p><em>Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Yufan Zhou, Guoyin Wang, Yiran Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While &ldquo;instruction-tuned&rdquo; generative large language models (LLMs) havedemonstrated an impressive ability to generalize to new tasks, the trainingphases heavily rely on large amounts of diverse and high-quality instructiondata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,especially when it comes to human-written data, can pose significant challengesboth in terms of cost and accessibility. Moreover, concerns related to privacycan further limit access to such data, making the process of obtaining it acomplex and nuanced undertaking. Consequently, this hinders the generality ofthe tuned models and may restrict their effectiveness in certain contexts. Totackle this issue, our study introduces a new approach called FederatedInstruction Tuning (FedIT), which leverages federated learning (FL) as thelearning framework for the instruction tuning of LLMs. This marks the firstexploration of FL-based instruction tuning for LLMs. This is especiallyimportant since text data is predominantly generated by end users. Therefore,it is imperative to design and adapt FL approaches to effectively leveragethese users&rsquo; diverse instructions stored on local devices, while preservingprivacy and ensuring data security. In the current paper, by conducting widelyused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneousand diverse sets of instructions on the client&rsquo;s end with the proposedframework FedIT, we improved the performance of LLMs compared to centralizedtraining with only limited local instructions. Further, in this paper, wedeveloped a Github repository named Shepherd. This repository offers afoundational framework for exploring federated fine-tuning of LLMs usingheterogeneous instructions across diverse categories.</div></details><blockquote><p><strong><em>2024-01-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.11765v2><strong>Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation</strong></a></p><p><em>Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of in-context learning (ICL) with large language models(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leakor regurgitate the private examples demonstrated in the prompt. We propose anovel algorithm that generates synthetic few-shot demonstrations from theprivate dataset with formal differential privacy (DP) guarantees, and showempirically that it can achieve effective ICL. We conduct extensive experimentson standard benchmarks and compare our algorithm with non-private ICL andzero-shot solutions. Our results demonstrate that our algorithm can achievecompetitive performance with strong privacy levels. These results open up newpossibilities for ICL with privacy protection for a broad range ofapplications.</div></details><p><a href=http://arxiv.org/abs/2401.15605v1><strong>AI as a Medical Ally: Evaluating ChatGPT&rsquo;s Usage and Impact in Indian Healthcare</strong></a></p><p><em>Aryaman Raina, Prateek Mishra, Harshit goyal, Dhruv Kumar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This study investigates the integration and impact of Large Language Models(LLMs), like ChatGPT, in India&rsquo;s healthcare sector. Our research employs a dualapproach, engaging both general users and medical professionals through surveysand interviews respectively. Our findings reveal that healthcare professionalsvalue ChatGPT in medical education and preliminary clinical settings, butexercise caution due to concerns about reliability, privacy, and the need forcross-verification with medical references. General users show a preference forAI interactions in healthcare, but concerns regarding accuracy and trustpersist. The study underscores the need for these technologies to complement,not replace, human medical expertise, highlighting the importance of developingLLMs in collaboration with healthcare providers. This paper enhances theunderstanding of LLMs in healthcare, detailing current usage, user trust, andimprovement areas. Our insights inform future research and development,underscoring the need for ethically compliant, user-focused LLM advancementsthat address healthcare-specific challenges.</div></details><p><a href=http://arxiv.org/abs/2401.15657v1><strong>Data-Free Generalized Zero-Shot Learning</strong></a></p><p><em>Bowen Tang, Long Yan, Jing Zhang, Qian Yu, Lu Sheng, Dong Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deep learning models have the ability to extract rich knowledge fromlarge-scale datasets. However, the sharing of data has become increasinglychallenging due to concerns regarding data copyright and privacy. Consequently,this hampers the effective transfer of knowledge from existing data to noveldownstream tasks and concepts. Zero-shot learning (ZSL) approaches aim torecognize new classes by transferring semantic knowledge learned from baseclasses. However, traditional generative ZSL methods often require access toreal images from base classes and rely on manually annotated attributes, whichpresents challenges in terms of data restrictions and model scalability. Tothis end, this paper tackles a challenging and practical problem dubbed asdata-free zero-shot learning (DFZSL), where only the CLIP-based base classesdata pre-trained classifier is available for zero-shot classification.Specifically, we propose a generic framework for DFZSL, which consists of threemain components. Firstly, to recover the virtual features of the base data, wemodel the CLIP features of base class images as samples from a von Mises-Fisher(vMF) distribution based on the pre-trained classifier. Secondly, we leveragethe text features of CLIP as low-cost semantic information and propose afeature-language prompt tuning (FLPT) method to further align the virtual imagefeatures and textual features. Thirdly, we train a conditional generative modelusing the well-aligned virtual image features and corresponding semantic textfeatures, enabling the generation of new classes features and achieve betterzero-shot generalization. Our framework has been evaluated on five commonlyused benchmarks for generalized ZSL, as well as 11 benchmarks for thebase-to-new ZSL. The results demonstrate the superiority and effectiveness ofour approach. Our code is available in <a href=https://github.com/ylong4/DFZSL>https://github.com/ylong4/DFZSL</a></div></details><blockquote><p><strong><em>2024-01-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01725v1><strong>Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models</strong></a></p><p><em>Yunhong He, Jianling Qiu, Wei Zhang, Zhengqing Yuan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advancements in large language models (LLMs) have significantlyenhanced capabilities in natural language processing and artificialintelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionizedtext generation, translation, and question-answering tasks due to thetransformative Transformer model. Despite their widespread use, LLMs presentchallenges such as ethical dilemmas when models are compelled to respondinappropriately, susceptibility to phishing attacks, and privacy violations.This paper addresses these challenges by introducing a multi-pronged approachthat includes: 1) filtering sensitive vocabulary from user input to preventunethical responses; 2) detecting role-playing to halt interactions that couldlead to &lsquo;prison break&rsquo; scenarios; 3) implementing custom rule engines torestrict the generation of prohibited content; and 4) extending thesemethodologies to various LLM derivatives like Multi-Model Large Language Models(MLLMs). Our approach not only fortifies models against unethical manipulationsand privacy breaches but also maintains their high performance across tasks. Wedemonstrate state-of-the-art performance under various attack prompts, withoutcompromising the model&rsquo;s core functionalities. Furthermore, the introduction ofdifferentiated security levels empowers users to control their personal datadisclosure. Our methods contribute to reducing social risks and conflictsarising from technological abuse, enhance data protection, and promote socialequity. Collectively, this research provides a framework for balancing theefficiency of question-answering systems with user privacy and ethicalstandards, ensuring a safer user experience and fostering trust in AItechnology.</div></details><p><a href=http://arxiv.org/abs/2401.15463v1><strong>DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure</strong></a></p><p><em>Junyi Ye, Mengnan Du, Guiling Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper introduces DataFrame question answering (QA), a novel task thatutilizes large language models (LLMs) to generate Pandas queries forinformation retrieval and data analysis on dataframes, emphasizing safe andnon-revealing data handling. Our method, which solely relies on dataframecolumn names, not only ensures data privacy but also significantly reduces thecontext window in the prompt, streamlining information processing andaddressing major challenges in LLM-based data analysis. We propose DataFrame QAas a comprehensive framework that includes safe Pandas query generation andcode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1metric on the renowned WikiSQL and our newly developed &lsquo;UCI-DataFrameQA&rsquo;,tailored for complex data analysis queries. Our findings indicate that GPT-4achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,underscoring its capability in securely retrieving and aggregating dataframevalues and conducting sophisticated data analyses. This approach, deployable ina zero-shot manner without prior training or adjustments, proves to be highlyadaptable and secure for diverse applications.</div></details><blockquote><p><strong><em>2024-01-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.02003v2><strong>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</strong></a></p><p><em>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, Yue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes the papers into &ldquo;The Good&rdquo; (beneficial LLM applications),&ldquo;The Bad&rdquo; (offensive applications), and &ldquo;The Ugly&rdquo; (vulnerabilities of LLMs andtheir defenses). We have some interesting findings. For example, LLMs haveproven to enhance code security (code vulnerability detection) and data privacy(data confidentiality protection), outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, Research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs&rsquo; potential to both bolster and jeopardize cybersecurity.</div></details><blockquote><p><strong><em>2024-01-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2402.01711v1><strong>LLM on FHIR &ndash; Demystifying Health Records</strong></a></p><p><em>Paul Schmiedmayer, Adrit Rao, Philipp Zagar, Vishnu Ravi, Aydin Zahedivash, Arash Fereydooni, Oliver Aalami</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Objective: To enhance health literacy and accessibility of health informationfor a diverse patient population by developing a patient-centered artificialintelligence (AI) solution using large language models (LLMs) and FastHealthcare Interoperability Resources (FHIR) application programming interfaces(APIs). Materials and Methods: The research involved developing LLM on FHIR, anopen-source mobile application allowing users to interact with their healthrecords using LLMs. The app is built on Stanford&rsquo;s Spezi ecosystem and usesOpenAI&rsquo;s GPT-4. A pilot study was conducted with the SyntheticMass patientdataset and evaluated by medical experts to assess the app&rsquo;s effectiveness inincreasing health literacy. The evaluation focused on the accuracy, relevance,and understandability of the LLM&rsquo;s responses to common patient questions.Results: LLM on FHIR demonstrated varying but generally high degrees ofaccuracy and relevance in providing understandable health information topatients. The app effectively translated medical data into patient-friendlylanguage and was able to adapt its responses to different patient profiles.However, challenges included variability in LLM responses and the need forprecise filtering of health data. Discussion and Conclusion: LLMs offersignificant potential in improving health literacy and making health recordsmore accessible. LLM on FHIR, as a pioneering application in this field,demonstrates the feasibility and challenges of integrating LLMs into patientcare. While promising, the implementation and pilot also highlight risks suchas inconsistent responses and the importance of replicable output. Futuredirections include better resource identification mechanisms and executing LLMson-device to enhance privacy and reduce costs.</div></details><blockquote><p><strong><em>2024-01-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.07254v4><strong>Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement</strong></a></p><p><em>Chenghao Li, Dake Chen, Yuke Zhang, Peter A. Beerel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While diffusion models demonstrate a remarkable capability for generatinghigh-quality images, their tendency to `replicate&rsquo; training data raises privacyconcerns. Although recent research suggests that this replication may stem fromthe insufficient generalization of training data captions and duplication oftraining images, effective mitigation strategies remain elusive. To addressthis gap, our paper first introduces a generality score that measures thecaption generality and employ large language model (LLM) to generalize trainingcaptions. Subsequently, we leverage generalized captions and propose a noveldual fusion enhancement approach to mitigate the replication of diffusionmodels. Our empirical results demonstrate that our proposed methods cansignificantly reduce replication by 43.5% compared to the original diffusionmodel while maintaining the diversity and quality of generations. Code isavailable at <a href=https://github.com/HowardLi0816/dual-fusion-diffusion>https://github.com/HowardLi0816/dual-fusion-diffusion</a>.</div></details><p><a href=http://arxiv.org/abs/2401.12453v1><strong>&ldquo;The teachers are confused as well&rdquo;: A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education</strong></a></p><p><em>Kyrie Zhixuan Zhou, Zachary Kilhoffer, Madelyn Rose Sanfilippo, Ted Underwood, Ece Gumusel, Mengyi Wei, Abhinav Choudhry, Jinjun Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are advancing quickly and impacting people&rsquo;slives for better or worse. In higher education, concerns have emerged such asstudents&rsquo; misuse of LLMs and degraded education outcomes. To unpack the ethicalconcerns of LLMs for higher education, we conducted a case study consisting ofstakeholder interviews (n=20) in higher education computer science. We foundthat students use several distinct mental models to interact with LLMs - LLMsserve as a tool for (a) writing, (b) coding, and (c) information retrieval,which differ somewhat in ethical considerations. Students and teachers broughtup ethical issues that directly impact them, such as inaccurate LLM responses,hallucinations, biases, privacy leakage, and academic integrity issues.Participants emphasized the necessity of guidance and rules for the use of LLMsin higher education, including teaching digital literacy, rethinking education,and having cautious and contextual policies. We reflect on the ethicalchallenges and propose solutions.</div></details><p><a href=http://arxiv.org/abs/2401.12915v1><strong>Red Teaming Visual Language Models</strong></a></p><p><em>Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: VLMs (Vision-Language Models) extend the capabilities of LLMs (Large LanguageModels) to accept multimodal inputs. Since it has been verified that LLMs canbe induced to generate harmful or inaccurate content through specific testcases (termed as Red Teaming), how VLMs perform in similar scenarios,especially with their combination of textual and visual inputs, remains aquestion. To explore this problem, we present a novel red teaming datasetRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modaljail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,privacy, safety, fairness). Our RTVLM is the first red-teaming dataset tobenchmark current VLMs in terms of these 4 different aspects. Detailed analysisshows that 10 prominent open-sourced VLMs struggle with the red teaming indifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning(SFT) using RTVLM, and this bolsters the models&rsquo; performance with 10% in RTVLMtest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,overpassing other LLaVA-based models with regular alignment data. This revealsthat current open-sourced VLMs still lack red teaming alignment. Our code anddatasets will be open-source.</div></details><blockquote><p><strong><em>2024-01-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.13894v2><strong>FwdLLM: Efficient FedLLM using Forward Gradient</strong></a></p><p><em>Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, Shangguang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are transforming the landscape of mobileintelligence. Federated Learning (FL), a method to preserve user data privacy,is often employed in fine-tuning LLMs to downstream mobile tasks, an approachknown as FedLLM. Though recent efforts have addressed the network issue inducedby the vast model size, they have not practically mitigated vital challengesconcerning integration with mobile devices, such as significant memoryconsumption and sluggish model convergence. In response to these challenges, this work introduces FwdLLM, an innovativeFL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLMto employ backpropagation (BP)-free training methods, requiring devices only toexecute ``perturbed inferences&rsquo;&rsquo;. Consequently, FwdLLM delivers way bettermemory efficiency and time efficiency (expedited by mobile NPUs and an expandedarray of participant devices). FwdLLM centers around three key designs: (1) itcombines BP-free training with parameter-efficient training methods, anessential way to scale the approach to the LLM era; (2) it systematically andadaptively allocates computational loads across devices, striking a carefulbalance between convergence speed and accuracy; (3) it discriminatively samplesperturbed predictions that are more valuable to model convergence.Comprehensive experiments with five LLMs and three NLP tasks illustrateFwdLLM&rsquo;s significant advantages over conventional methods, including up tothree orders of magnitude faster convergence and a 14.6x reduction in memoryfootprint. Uniquely, FwdLLM paves the way for federated learning ofbillion-parameter LLMs such as LLaMA on COTS mobile devices &ndash; a featpreviously unattained.</div></details><blockquote><p><strong><em>2024-01-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.05163v2><strong>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA</strong></a></p><p><em>Jiawei Chen, Dingkang Yang, Yue Jiang, Yuxuan Lei, Lihua Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Medical visual question answering (VQA) is a challenging multimodal task,where Vision-Language Pre-training (VLP) models can effectively improve thegeneralization performance. However, most methods in the medical field treatVQA as an answer classification task which is difficult to transfer topractical application scenarios. Additionally, due to the privacy of medicalimages and the expensive annotation process, large-scale medical image-textpairs datasets for pretraining are severely lacking. In this paper, we proposea large-scale MultI-task Self-Supervised learning based framework (MISS) formedical VQA tasks. Unlike existing methods, we treat medical VQA as agenerative task. We unify the text encoder and multimodal encoder and alignimage-text features through multi-task learning. Furthermore, we propose aTransfer-and-Caption method that extends the feature space of single-modalimage datasets using large language models (LLMs), enabling those traditionalmedical vision field task data to be applied to VLP. Experiments show that ourmethod achieves excellent results with fewer multimodal datasets anddemonstrates the advantages of generative VQA models. The code and modelweights will be released upon the paper&rsquo;s acceptance.</div></details><p><a href=http://arxiv.org/abs/2311.13158v3><strong>Towards a Responsible AI Metrics Catalogue: A Collection of Metrics for AI Accountability</strong></a></p><p><em>Boming Xia, Qinghua Lu, Liming Zhu, Sung Une Lee, Yue Liu, Zhenchang Xing</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial Intelligence (AI), particularly through the advent of large-scalegenerative AI (GenAI) models such as Large Language Models (LLMs), has become atransformative element in contemporary technology. While these models haveunlocked new possibilities, they simultaneously present significant challenges,such as concerns over data privacy and the propensity to generate misleading orfabricated content. Current frameworks for Responsible AI (RAI) often fallshort in providing the granular guidance necessary for tangible application,especially for Accountability-a principle that is pivotal for ensuringtransparent and auditable decision-making, bolstering public trust, and meetingincreasing regulatory expectations. This study bridges the accountability gapby introducing our effort towards a comprehensive metrics catalogue, formulatedthrough a systematic multivocal literature review (MLR) that integratesfindings from both academic and grey literature. Our catalogue delineatesprocess metrics that underpin procedural integrity, resource metrics thatprovide necessary tools and frameworks, and product metrics that reflect theoutputs of AI systems. This tripartite framework is designed to operationalizeAccountability in AI, with a special emphasis on addressing the intricacies ofGenAI.</div></details><p><a href=http://arxiv.org/abs/2312.09669v3><strong>Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models</strong></a></p><p><em>Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Yuang Qi, Weiming Zhang, Nenghai Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid development of large language models (LLMs) has yielded impressivesuccess in various downstream tasks. However, the vast potential and remarkablecapabilities of LLMs also raise new security and privacy concerns if they areexploited for nefarious purposes due to their open-endedness. For example, LLMsmay be used to plagiarize or imitate writing, thereby infringing the copyrightof the original content, or to create indiscriminate fake information based ona certain source text. In some cases, LLMs can even analyze text from theInternet to infer personal privacy. Unfortunately, previous text protectionresearch could not foresee the emergence of powerful LLMs, rendering it nolonger effective in this new context. To bridge this gap, we introduce SilentGuardian (SG), a text protection mechanism against LLMs, which allows LLMs torefuse to generate response when receiving protected text, preventing themalicious use of text from the source. Specifically, we first propose theconcept of Truncation Protection Examples (TPE). By carefully modifying thetext to be protected, TPE can induce LLMs to first sample the end token, thusdirectly terminating the interaction. In addition, to efficiently construct TPEin the discrete space of text data, we propose a novel optimization algorithmcalled Super Taliored Protection (STP), which is not only highly efficient butalso maintains the semantic consistency of the text during the optimizationprocess. The comprehensive experimental evaluation demonstrates that SG caneffectively protect the target text under various configurations and achievealmost 100% protection success rate in some cases. Notably, SG also exhibitsrelatively good transferability and robustness, making its application inpractical scenarios possible.</div></details><blockquote><p><strong><em>2024-01-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.04124v3><strong>MobileAgent: enhancing mobile control via human-machine interaction and SOP integration</strong></a></p><p><em>Tinghe Ding</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Agents centered around Large Language Models (LLMs) are now capable ofautomating mobile device operations for users. After fine-tuning to learn auser&rsquo;s mobile operations, these agents can adhere to high-level userinstructions online. They execute tasks such as goal decomposition, sequencingof sub-goals, and interactive environmental exploration, until the finalobjective is achieved. However, privacy concerns related to personalized userdata arise during mobile operations, requiring user confirmation. Moreover,users&rsquo; real-world operations are exploratory, with action data being complexand redundant, posing challenges for agent learning. To address these issues,in our practical application, we have designed interactive tasks between agentsand humans to identify sensitive information and align with personalized userneeds. Additionally, we integrated Standard Operating Procedure (SOP)information within the model&rsquo;s in-context learning to enhance the agent&rsquo;scomprehension of complex task execution. Our approach is evaluated on the newdevice control benchmark AitW, which encompasses 30K unique instructions acrossmulti-step tasks, including application operation, web searching, and webshopping. Experimental results show that the SOP-based agent achievesstate-of-the-art performance in LLMs without incurring additional inferencecosts, boasting an overall action success rate of 66.92%. The code and dataexamples are available at <a href=https://github.com/alipay/mobile-agent>https://github.com/alipay/mobile-agent</a>.</div></details><blockquote><p><strong><em>2024-01-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.04076v3><strong>Greening Large Language Models of Code</strong></a></p><p><em>Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models of code have shown remarkable effectiveness acrossvarious software engineering tasks. Despite the availability of many cloudservices built upon these powerful models, there remain several scenarios wheredevelopers cannot take full advantage of them, stemming from factors such asrestricted or unreliable internet access, institutional privacy policies thatprohibit external transmission of code to third-party vendors, and more.Therefore, developing a compact, efficient, and yet energy-saving model fordeployment on developers&rsquo; devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployablemodel from a large language model of code by optimizing it in terms of modelsize, inference latency, energy consumption, and carbon footprint whilemaintaining a comparable level of effectiveness. The key idea of Avatar is toformulate the optimization of language models as a multi-objectiveconfiguration tuning problem and solve it with the help of a SatisfiabilityModulo Theories (SMT) solver and a tailored optimization algorithm. The SMTsolver is used to form an appropriate configuration space, while theoptimization algorithm identifies the Pareto-optimal set of configurations fortraining the optimized models using knowledge distillation. We evaluate Avatarwith two popular language models of code, i.e., CodeBERT and GraphCodeBERT, ontwo popular tasks, i.e., vulnerability prediction and clone detection. We useAvatar to produce optimized models with a small size (3 MB), which is160$\times$ smaller than the original large models. On the two tasks, theoptimized models significantly reduce the energy consumption (up to 184$\times$less), carbon footprint (up to 157$\times$ less), and inference latency (up to76$\times$ faster), with only a negligible loss in effectiveness (1.67% onaverage).</div></details><blockquote><p><strong><em>2024-01-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.04343v1><strong>Private Fine-tuning of Large Language Models with Zeroth-order Optimization</strong></a></p><p><em>Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Fine-tuning large pretrained models on private datasets may run the risk ofviolating privacy. Differential privacy is a framework for mitigating privacyrisks by enforcing algorithmic stability. DP-SGD enables training models withprivate data in a privacy-preserving manner, but raises new obstacles in theform of performance loss and significant engineering challenges. We introduceDP-ZO, a new method for fine-tuning large language models that preserves theprivacy of training data by privatizing zeroth-order optimization. A keyinsight into the design of our method is that the direction of the gradient inSPSA, the zeroth-order algorithm we use, is always random and the onlyinformation that depends on private data is the step size, i.e., a scalar.Therefore, we only need to privatize the scalar step size, which ismemory-efficient. DP-ZO, which can be instantiated with either Laplace orGaussian noise, provides a strong privacy-utility trade-off across differenttasks, and model sizes, under conservative privacy budgets. One noteworthyresult is that DP-ZO exhibits just $1.86%$ performance degradation due toprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samplesfrom SQuAD.</div></details><blockquote><p><strong><em>2024-01-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00793v2><strong>SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models</strong></a></p><p><em>Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the growing use of large language models hosted on cloud platforms tooffer inference services, privacy concerns are escalating, especiallyconcerning sensitive data like investment plans and bank account details.Secure Multi-Party Computing (SMPC) emerges as a promising solution to protectthe privacy of inference data and model parameters. However, the application ofSMPC in Privacy-Preserving Inference (PPI) for large language models,particularly those based on the Transformer architecture, often leads toconsiderable slowdowns or declines in performance. This is largely due to themultitude of nonlinear operations in the Transformer architecture, which arenot well-suited to SMPC and difficult to circumvent or optimize effectively. Toaddress this concern, we introduce an advanced optimization framework calledSecFormer, to achieve fast and accurate PPI for Transformer models. Byimplementing model design optimization, we successfully eliminate the high-costexponential and maximum operations in PPI without sacrificing modelperformance. Additionally, we have developed a suite of efficient SMPCprotocols that utilize segmented polynomials, Fourier series and Goldschmidt&rsquo;smethod to handle other complex nonlinear functions within PPI, such as GeLU,LayerNorm, and Softmax. Our extensive experiments reveal that SecFormeroutperforms MPCFormer in performance, showing improvements of $5.6%$ and$24.2%$ for BERT$<em>{\text{BASE}}$ and BERT$</em>{\text{LARGE}}$, respectively. Interms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma forBERT$<em>{\text{BASE}}$ and BERT$</em>{\text{LARGE}}$, demonstrating its effectivenessand speed.</div></details><blockquote><p><strong><em>2024-01-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.01692v1><strong>Predicting challenge moments from students&rsquo; discourse: A comparison of GPT-4 to two traditional natural language processing approaches</strong></a></p><p><em>Wannapon Suraworachet, Jennifer Seon, Mutlu Cukurova</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Effective collaboration requires groups to strategically regulate themselvesto overcome challenges. Research has shown that groups may fail to regulate dueto differences in members&rsquo; perceptions of challenges which may benefit fromexternal support. In this study, we investigated the potential of leveragingthree distinct natural language processing models: an expert knowledgerule-based model, a supervised machine learning (ML) model and a Large Languagemodel (LLM), in challenge detection and challenge dimension identification(cognitive, metacognitive, emotional and technical/other challenges) fromstudent discourse, was investigated. The results show that the supervised MLand the LLM approaches performed considerably well in both tasks, in contrastto the rule-based approach, whose efficacy heavily relies on the engineeredfeatures by experts. The paper provides an extensive discussion of the threeapproaches&rsquo; performance for automated detection and support of students&rsquo;challenge moments in collaborative learning activities. It argues that,although LLMs provide many advantages, they are unlikely to be the panacea toissues of the detection and feedback provision of socially shared regulation oflearning due to their lack of reliability, as well as issues of validityevaluation, privacy and confabulation. We conclude the paper with a discussionon additional considerations, including model transparency to explore feasibleand meaningful analytical feedback for students and educators using LLMs.</div></details><p><a href=http://arxiv.org/abs/2312.17449v2><strong>DB-GPT: Empowering Database Interactions with Private Large Language Models</strong></a></p><p><em>Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, Faqiang Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The recent breakthroughs in large language models (LLMs) are positioned totransition many areas of software. Database technologies particularly have animportant entanglement with LLMs as efficient and intuitive databaseinteractions are paramount. In this paper, we present DB-GPT, a revolutionaryand production-ready project that integrates LLMs with traditional databasesystems to enhance user experience and accessibility. DB-GPT is designed tounderstand natural language queries, provide context-aware responses, andgenerate complex SQL queries with high accuracy, making it an indispensabletool for users ranging from novice to expert. The core innovation in DB-GPTlies in its private LLM technology, which is fine-tuned on domain-specificcorpora to maintain user privacy and ensure data security while offering thebenefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, whichincludes a novel retrieval augmented generation (RAG) knowledge system, anadaptive learning mechanism to continuously improve performance based on userfeedback and a service-oriented multi-model framework (SMMF) with powerfuldata-driven agents. Our extensive experiments and user studies confirm thatDB-GPT represents a paradigm shift in database interactions, offering a morenatural, efficient, and secure way to engage with data repositories. The paperconcludes with a discussion of the implications of DB-GPT framework on thefuture of human-database interaction and outlines potential avenues for furtherenhancements and applications in the field. The project code is available athttps://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself byinstalling it with the instructionshttps://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minutevideo at <a href="https://www.youtube.com/watch?v=KYs4nTDzEhk">https://www.youtube.com/watch?v=KYs4nTDzEhk</a>.</div></details><blockquote><p><strong><em>2024-01-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00544v2><strong>A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models</strong></a></p><p><em>Vansh Sharma, Venkat Raman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This research explores the integration of large language models (LLMs) intoscientific data assimilation, focusing on combustion science as a case study.Leveraging foundational models integrated with Retrieval-Augmented Generation(RAG) framework, the study introduces an approach to process diverse combustionresearch data, spanning experimental studies, simulations, and literature. Themultifaceted nature of combustion research emphasizes the critical role ofknowledge processing in navigating and extracting valuable information from avast and diverse pool of sources. The developed approach minimizescomputational and economic expenses while optimizing data privacy and accuracy.It incorporates prompt engineering and offline open-source LLMs, offering userautonomy in selecting base models. The study provides a thorough examination oftext segmentation strategies, conducts comparative studies between LLMs, andexplores various optimized prompts to demonstrate the effectiveness of theframework. By incorporating an external database, the framework outperforms aconventional LLM in generating accurate responses and constructing robustarguments. Additionally, the study delves into the investigation of optimizedprompt templates for the purpose of efficient extraction of scientificliterature. The research addresses concerns related to hallucinations and falseresearch articles by introducing a custom workflow developed with a detectionalgorithm to filter out inaccuracies. Despite identified areas for improvement,the framework consistently delivers accurate domain-specific responses withminimal human oversight. The prompt-agnostic approach introduced holds promisefor future deliberations. The study underscores the significance of integratingLLMs and knowledge processing techniques in scientific research, providing afoundation for advancements in data assimilation and utilization.</div></details><blockquote><p><strong><em>2023-12-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2401.00905v1><strong>Opening A Pandora&rsquo;s Box: Things You Should Know in the Era of Custom GPTs</strong></a></p><p><em>Guanhong Tao, Siyuan Cheng, Zhuo Zhang, Junmin Zhu, Guangyu Shen, Xiangyu Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of large language models (LLMs) has significantly acceleratedthe development of a wide range of applications across various fields. There isa growing trend in the construction of specialized platforms based on LLMs,such as the newly introduced custom GPTs by OpenAI. While custom GPTs providevarious functionalities like web browsing and code execution, they alsointroduce significant security threats. In this paper, we conduct acomprehensive analysis of the security and privacy issues arising from thecustom GPT platform. Our systematic examination categorizes potential attackscenarios into three threat models based on the role of the malicious actor,and identifies critical data exchange channels in custom GPTs. Utilizing theSTRIDE threat modeling framework, we identify 26 potential attack vectors, with19 being partially or fully validated in real-world settings. Our findingsemphasize the urgent need for robust security and privacy measures in thecustom GPT ecosystem, especially in light of the forthcoming launch of theofficial GPT store by OpenAI.</div></details><blockquote><p><strong><em>2023-12-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.09130v2><strong>Split-and-Denoise: Protect large language model inference with local differential privacy</strong></a></p><p><em>Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) shows powerful capability in natural languageunderstanding by capturing hidden semantics in vector space. This processenriches the value of the text embeddings for various downstream tasks, therebyfostering the Embedding-as-a-Service (EaaS) business model. However, the directtransmission of text to servers poses a largely unaddressed risk of privacyleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), aninnovative framework that split the model to execute the token embedding layeron the client side at minimal computational cost. This allows the client tointroduce noise prior to transmitting the embeddings to the server, andsubsequently receive and denoise the perturbed output embeddings for downstreamtasks. Our approach is designed for the inference stage of LLMs and requires nomodifications to the model parameters. Extensive experiments demonstrate SnD&rsquo;seffectiveness in optimizing the privacy-utility tradeoff across various LLMarchitectures and diverse downstream tasks. The results reveal a significantperformance improvement under the same privacy budget compared to the baseline,offering clients a privacy-preserving solution for local privacy protection.</div></details><p><a href=http://arxiv.org/abs/2401.00870v1><strong>Teach Large Language Models to Forget Privacy</strong></a></p><p><em>Ran Yan, Yujun Li, Wenqian Li, Peihua Mai, Yan Pang, Yinchuan Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have proven powerful, but the risk of privacyleakage remains a significant concern. Traditional privacy-preserving methods,such as Differential Privacy and Homomorphic Encryption, are inadequate forblack-box API-only settings, demanding either model transparency or heavycomputational resources. We propose Prompt2Forget (P2F), the first frameworkdesigned to tackle the LLM local privacy challenge by teaching LLM to forget.The method involves decomposing full questions into smaller segments,generating fabricated answers, and obfuscating the model&rsquo;s memory of theoriginal input. A benchmark dataset was crafted with questions containingprivacy-sensitive information from diverse fields. P2F achieves zero-shotgeneralization, allowing adaptability across a wide range of use cases withoutmanual adjustments. Experimental results indicate P2F&rsquo;s robust capability toobfuscate LLM&rsquo;s memory, attaining a forgetfulness score of around 90% withoutany utility loss. This represents an enhancement of up to 63% when contrastedwith the naive direct instruction technique, highlighting P2F&rsquo;s efficacy inmitigating memory retention of sensitive information within LLMs. Our findingsestablish the first benchmark in the novel field of the LLM forgetting task,representing a meaningful advancement in privacy preservation in the emergingLLM domain.</div></details><p><a href=http://arxiv.org/abs/2401.00284v1><strong>Evaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models</strong></a></p><p><em>Maximilian Weber, Merle Reichardt</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper explores the use of open generative Large Language Models (LLMs)for annotation tasks in the social sciences. The study highlights thechallenges associated with proprietary models, such as limited reproducibilityand privacy concerns, and advocates for the adoption of open (source) modelsthat can be operated on independent devices. Two examples of annotation tasks,sentiment analysis in tweets and identification of leisure activities inchildhood aspirational essays are provided. The study evaluates the performanceof different prompting strategies and models (neural-chat-7b-v3-2,Starling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). Theresults indicate the need for careful validation and tailored promptengineering. The study highlights the advantages of open models for dataprivacy and reproducibility.</div></details><blockquote><p><strong><em>2023-12-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.17493v1><strong>Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning</strong></a></p><p><em>Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, Meikang Qiu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The surge in interest and application of large language models (LLMs) hassparked a drive to fine-tune these models to suit specific applications, suchas finance and medical science. However, concerns regarding data privacy haveemerged, especially when multiple stakeholders aim to collaboratively enhanceLLMs using sensitive data. In this scenario, federated learning becomes anatural choice, allowing decentralized fine-tuning without exposing raw data tocentral servers. Motivated by this, we investigate how data privacy can beensured in LLM fine-tuning through practical federated learning approaches,enabling secure contributions from multiple parties to enhance LLMs. Yet,challenges arise: 1) despite avoiding raw data exposure, there is a risk ofinferring sensitive information from model outputs, and 2) federated learningfor LLMs incurs notable communication overhead. To address these challenges,this article introduces DP-LoRA, a novel federated learning algorithm tailoredfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism thatadds noise in weight updates, maintaining individual data privacy whilefacilitating collaborative model training. Moreover, DP-LoRA optimizescommunication efficiency via low-rank adaptation, minimizing the transmissionof updated weights during distributed training. The experimental results acrossmedical, financial, and general datasets using various LLMs demonstrate thatDP-LoRA effectively ensures strict privacy constraints while minimizingcommunication overhead.</div></details><blockquote><p><strong><em>2023-12-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.08820v3><strong>How to Raise a Robot &ndash; A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</strong></a></p><p><em>Niklas Hemken, Florian Jacob, Fabian Peller-Konrad, Rainer Kartmann, Tamim Asfour, Hannes Hartenstein</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Humanoid robots will be able to assist humans in their daily life, inparticular due to their versatile action capabilities. However, while theserobots need a certain degree of autonomy to learn and explore, they also shouldrespect various constraints, for access control and beyond. We explore thenovel field of incorporating privacy, security, and access control constraintswith robot task planning approaches. We report preliminary results on theclassical symbolic approach, deep-learned neural networks, and modern ideasusing large language models as knowledge base. From analyzing their trade-offs,we conclude that a hybrid approach is necessary, and thereby present a new usecase for the emerging field of neuro-symbolic artificial intelligence.</div></details><blockquote><p><strong><em>2023-12-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.15926v1><strong>FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models</strong></a></p><p><em>Panlong Wu, Kangshuo Li, Ting Wang, Fangxin Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Foundation models have shown great success in natural language processing,computer vision, and multimodal tasks. FMs have a large number of modelparameters, thus requiring a substantial amount of data to help optimize themodel during the training. Federated learning has revolutionized machinelearning by enabling collaborative learning from decentralized data while stillpreserving the data privacy of clients. Despite the great benefits foundationmodels can have empowered by federated learning, they face severe computation,communication, and statistical challenges. In this paper, we propose a noveltwo-stage federated learning algorithm called FedMS. A global expert is trainedin the first stage and a local expert is trained in the second stage to providebetter personalization. We construct a Mixture of Foundation Models (MoFM) withthese two experts and design a gate neural network with an inserted gateadapter that joins the aggregation every communication round in the secondstage. To further adapt to edge computing scenarios with limited computationalresources, we design a novel Sparsely Activated LoRA (SAL) algorithm thatfreezes the pre-trained foundation model parameters inserts low-rank adaptationmatrices into transformer blocks and activates them progressively during thetraining. We employ extensive experiments to verify the effectiveness of FedMS,results show that FedMS outperforms other SOTA baselines by up to 55.25% indefault settings.</div></details><p><a href=http://arxiv.org/abs/2312.16070v1><strong>Can ChatGPT Read Who You Are?</strong></a></p><p><em>Erik Derner, Dalibor Kuƒçera, Nuria Oliver, Jan Zah√°lka</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The interplay between artificial intelligence (AI) and psychology,particularly in personality assessment, represents an important emerging areaof research. Accurate personality trait estimation is crucial not only forenhancing personalization in human-computer interaction but also for a widevariety of applications ranging from mental health to education. This paperanalyzes the capability of a generic chatbot, ChatGPT, to effectively inferpersonality traits from short texts. We report the results of a comprehensiveuser study featuring texts written in Czech by a representative populationsample of 155 participants. Their self-assessments based on the Big FiveInventory (BFI) questionnaire serve as the ground truth. We compare thepersonality trait estimations made by ChatGPT against those by human raters andreport ChatGPT&rsquo;s competitive performance in inferring personality traits fromtext. We also uncover a &lsquo;positivity bias&rsquo; in ChatGPT&rsquo;s assessments across allpersonality dimensions and explore the impact of prompt composition onaccuracy. This work contributes to the understanding of AI capabilities inpsychological assessment, highlighting both the potential and limitations ofusing large language models for personality inference. Our research underscoresthe importance of responsible AI development, considering ethical implicationssuch as privacy, consent, autonomy, and bias in AI applications.</div></details><blockquote><p><strong><em>2023-12-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.02779v3><strong>Large Language Models Empowered Autonomous Edge AI for Connected Intelligence</strong></a></p><p><em>Yifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang, Khaled B. Letaief</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The evolution of wireless networks gravitates towards connected intelligence,a concept that envisions seamless interconnectivity among humans, objects, andintelligence in a hyper-connected cyber-physical world. Edge artificialintelligence (Edge AI) is a promising solution to achieve connectedintelligence by delivering high-quality, low-latency, and privacy-preserving AIservices at the network edge. This article presents a vision of autonomous edgeAI systems that automatically organize, adapt, and optimize themselves to meetusers&rsquo; diverse requirements, leveraging the power of large language models(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting thepowerful abilities of GPT in language understanding, planning, and codegeneration, as well as incorporating classic wisdom such as task-orientedcommunication and edge federated learning, we present a versatile frameworkthat efficiently coordinates edge AI models to cater to users&rsquo; personal demandswhile automatically generating code to train new models in a privacy-preservingmanner. Experimental results demonstrate the system&rsquo;s remarkable ability toaccurately comprehend user demands, efficiently execute AI models with minimalcost, and effectively create high-performance AI models at edge servers.</div></details><blockquote><p><strong><em>2023-12-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.14211v1><strong>Experimenting with Large Language Models and vector embeddings in NASA SciX</strong></a></p><p><em>Sergi Blanco-Cuaresma, Ioana CiucƒÉ, Alberto Accomazzi, Michael J. Kurtz, Edwin A. Henneken, Kelly E. Lockhart, Felix Grezes, Thomas Allen, Golnaz Shapurian, Carolyn S. Grant, Donna M. Thompson, Timothy W. Hostetler, Matthew R. Templeton, Shinyi Chen, Jennifer Koch, Taylor Jacovich, Daniel Chivvis, Fernanda de Macedo Alves, Jean-Claude Paquin, Jennifer Bartlett, Mugdha Polimera, Stephanie Jarmak</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Open-source Large Language Models enable projects such as NASA SciX (i.e.,NASA ADS) to think out of the box and try alternative approaches forinformation retrieval and data augmentation, while respecting data copyrightand users&rsquo; privacy. However, when large language models are directly promptedwith questions without any context, they are prone to hallucination. At NASASciX we have developed an experiment where we created semantic vectors for ourlarge collection of abstracts and full-text content, and we designed a promptsystem to ask questions using contextual chunks from our system. Based on anon-systematic human evaluation, the experiment shows a lower degree ofhallucination and better responses when using Retrieval Augmented Generation.Further exploration is required to design new features and data augmentationprocesses at NASA SciX that leverages this technology while respecting the highlevel of trust and quality that the project holds.</div></details><p><a href=http://arxiv.org/abs/2309.08173v2><strong>FedJudge: Federated Legal Large Language Model</strong></a></p><p><em>Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, Fangzhou Yao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have gained prominence in the field of LegalIntelligence, offering potential applications in assisting legal professionalsand laymen. However, the centralized training of these Legal LLMs raises dataprivacy concerns, as legal data is distributed among various institutionscontaining sensitive individual information. This paper addresses thischallenge by exploring the integration of Legal LLMs with Federated Learning(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally ondevices or clients, and their parameters are aggregated and distributed on acentral server, ensuring data privacy without directly sharing raw data.However, computation and communication overheads hinder the full fine-tuning ofLLMs under the FL setting. Moreover, the distribution shift of legal datareduces the effectiveness of FL methods. To this end, in this paper, we proposethe first Federated Legal Large Language Model (FedJudge) framework, whichfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudgeutilizes parameter-efficient fine-tuning methods to update only a fewadditional parameters during the FL training. Besides, we explore the continuallearning methods to preserve the global model&rsquo;s important parameters whentraining local clients to mitigate the problem of data shifts. Extensiveexperimental results on three real-world datasets clearly validate theeffectiveness of FedJudge. Code is released athttps://github.com/yuelinan/FedJudge.</div></details><p><a href=http://arxiv.org/abs/2303.11032v2><strong>DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4</strong></a></p><p><em>Zhengliang Liu, Yue Huang, Xiaowei Yu, Lu Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Yiwei Li, Peng Shu, Fang Zeng, Lichao Sun, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, Dajiang Zhu, Xiang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The digitization of healthcare has facilitated the sharing and re-using ofmedical data but has also raised concerns about confidentiality and privacy.HIPAA (Health Insurance Portability and Accountability Act) mandates removingre-identifying information before the dissemination of medical records. Thus,effective and efficient solutions for de-identifying medical data, especiallythose in free-text forms, are highly needed. While various computer-assistedde-identification methods, including both rule-based and learning-based, havebeen developed and used in prior practice, such solutions still lackgeneralizability or need to be fine-tuned according to different scenarios,significantly imposing restrictions in wider use. The advancement of largelanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential inprocessing text data in the medical domain with zero-shot in-context learning,especially in the task of privacy protection, as these models can identifyconfidential information by their powerful named entity recognition (NER)capability. In this work, we developed a novel GPT4-enabled de-identificationframework (``DeID-GPT") to automatically identify and remove the identifyinginformation. Compared to existing commonly used medical text datade-identification methods, our developed DeID-GPT showed the highest accuracyand remarkable reliability in masking private information from the unstructuredmedical text while preserving the original structure and meaning of the text.This study is one of the earliest to utilize ChatGPT and GPT-4 for medical textdata processing and de-identification, which provides insights for furtherresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 inhealthcare. Codes and benchmarking data information are available athttps://github.com/yhydhx/ChatGPT-API.</div></details><blockquote><p><strong><em>2023-12-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.12472v1><strong>A Performance Evaluation of a Quantized Large Language Model on Various Smartphones</strong></a></p><p><em>Tolga √á√∂pl√º, Marc Loedi, Arto Bendiken, Mykhailo Makohin, Joshua J. Bouw, Stephen Cobb</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper explores the feasibility and performance of on-device largelanguage model (LLM) inference on various Apple iPhone models. Amidst the rapidevolution of generative AI, on-device LLMs offer solutions to privacy,security, and connectivity challenges inherent in cloud-based models.Leveraging existing literature on running multi-billion parameter LLMs onresource-limited devices, our study examines the thermal effects andinteraction speeds of a high-performing LLM across different smartphonegenerations. We present real-world performance results, providing insights intoon-device inference capabilities.</div></details><p><a href=http://arxiv.org/abs/2305.18072v2><strong>Image Captioning with Multi-Context Synthetic Data</strong></a></p><p><em>Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Image captioning requires numerous annotated image-text pairs, resulting insubstantial annotation costs. Recently, large models (e.g. diffusion models andlarge language models) have excelled in producing high-quality images and text.This potential can be harnessed to create synthetic image-text pairs fortraining captioning models. Synthetic data can improve cost and time efficiencyin data collection, allow for customization to specific domains, bootstrapgeneralization capability for zero-shot performance, and circumvent privacyconcerns associated with real-world data. However, existing methods struggle toattain satisfactory performance solely through synthetic data. We identify theissue as generated images from simple descriptions mostly capture a solitaryperspective with limited context, failing to align with the intricate scenesprevalent in real-world imagery. To tackle this, we present an innovativepipeline that introduces multi-context data generation. Beginning with aninitial text corpus, our approach employs a large language model to extractmultiple sentences portraying the same scene from diverse viewpoints. Thesesentences are then condensed into a single sentence with multiple contexts.Subsequently, we generate intricate images using the condensed captions throughdiffusion models. Our model is exclusively trained on synthetic image-textpairs crafted through this process. The effectiveness of our pipeline isvalidated through experimental results in both the in-domain and cross-domainsettings, where it achieves state-of-the-art performance on well-known datasetssuch as MSCOCO, Flickr30k, and NoCaps.</div></details><blockquote><p><strong><em>2023-12-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.11701v1><strong>Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview</strong></a></p><p><em>Liang Zhang, Zhelun Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, the rapid advancement and impressive capabilities of LargeLanguage Models (LLMs) have been evident across various domains. This paperexplores the application, implications, and potential of LLMs in buildingenergy efficiency and decarbonization studies. The wide-ranging capabilities ofLLMs are examined in the context of the building energy field, includingintelligent control systems, code generation, data infrastructure, knowledgeextraction, and education. Despite the promising potential of LLMs, challengesincluding complex and expensive computation, data privacy, security andcopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.The paper concludes with a call for future research focused on the enhancementof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative researchbetween AI and energy experts.</div></details><p><a href=http://arxiv.org/abs/2311.02775v3><strong>AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs</strong></a></p><p><em>Yann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Responding to the thousands of student questions on online QA platforms eachsemester has a considerable human cost, particularly in computing courses withrapidly growing enrollments. To address the challenges of scalable andintelligent question-answering (QA), we introduce an innovative solution thatleverages open-source Large Language Models (LLMs) from the LLaMA-2 family toensure data privacy. Our approach combines augmentation techniques such asretrieval augmented generation (RAG), supervised fine-tuning (SFT), andlearning from human preferences data using Direct Preference Optimization(DPO). Through extensive experimentation on a Piazza dataset from anintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs ofpreference data, we demonstrate a significant 30% improvement in the quality ofanswers, with RAG being a particularly impactful addition. Our contributionsinclude the development of a novel architecture for educational QA, extensiveevaluations of LLM performance utilizing both human assessments and LLM-basedmetrics, and insights into the challenges and future directions of educationaldata processing. This work paves the way for the development of AI-TA, anintelligent QA assistant customizable for courses with an online QA platform</div></details><blockquote><p><strong><em>2023-12-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.10645v1><strong>FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph Completion</strong></a></p><p><em>Wei Tang, Zhiqian Wu, Yixin Cao, Yong Liao, Pengyuan Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Knowledge graph completion (KGC) aims to predict missing facts in knowledgegraphs (KGs), which is crucial as modern KGs remain largely incomplete. Whiletraining KGC models on multiple aligned KGs can improve performance, previousmethods that rely on transferring raw data among KGs raise privacy concerns. Toaddress this challenge, we propose a new federated learning framework thatimplicitly aggregates knowledge from multiple KGs without demanding raw dataexchange and entity alignment. We treat each KG as a client that trains a locallanguage model through textbased knowledge representation learning. A centralserver then aggregates the model weights from clients. As natural languageprovides a universal representation, the same knowledge thus has similarsemantic representations across KGs. As such, the aggregated language model canleverage complementary knowledge from multilingual KGs without demanding rawuser data sharing. Extensive experiments on a benchmark dataset demonstratethat our method substantially improves KGC on multilingual KGs, achievingcomparable performance to state-of-the-art alignment-based models withoutrequiring any labeled alignments or raw user data sharing. Our codes will bepublicly available.</div></details><blockquote><p><strong><em>2023-12-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.18396v3><strong>LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers</strong></a></p><p><em>Xuanqi Liu, Zhuotao Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The community explored to build private inference frameworks fortransformer-based large language models (LLMs) in a server-client setting,where the server holds the model parameters and the client inputs its privatedata (or prompt) for inference. However, these frameworks impose significantoverhead when the private inputs are forward propagated through the originalLLMs. In this paper, we show that substituting the computation- andcommunication-heavy operators in the transformer architecture withprivacy-computing friendly approximations can greatly reduce the privateinference costs while incurring very minor impact on model performance.Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computingfriendly model inference pipeline achieves a $5\times$ acceleration incomputation and an 80% reduction in communication overhead, while retainingnearly identical accuracy.</div></details><p><a href=http://arxiv.org/abs/2312.10108v1><strong>Privacy-Aware Document Visual Question Answering</strong></a></p><p><em>Rub√®n Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Document Visual Question Answering (DocVQA) is a fast growing branch ofdocument understanding. Despite the fact that documents contain sensitive orcopyrighted information, none of the current DocVQA methods offers strongprivacy guarantees. In this work, we explore privacy in the domain of DocVQA for the first time.We highlight privacy issues in state of the art multi-modal LLM models used forDocVQA, and explore possible solutions. Specifically, we focus on the invoice processing use case as a realistic,widely used scenario for document understanding, and propose a large scaleDocVQA dataset comprising invoice documents and associated questions andanswers. We employ a federated learning scheme, that reflects the real-lifedistribution of documents in different businesses, and we explore the use casewhere the ID of the invoice issuer is the sensitive information to beprotected. We demonstrate that non-private models tend to memorise, behaviour that canlead to exposing private information. We then evaluate baseline trainingschemes employing federated learning and differential privacy in thismulti-modal scenario, where the sensitive information might be exposed throughany of the two input modalities: vision (document image) or language (OCRtokens). Finally, we design an attack exploiting the memorisation effect of the model,and demonstrate its effectiveness in probing different DocVQA models.</div></details><p><a href=http://arxiv.org/abs/2312.09958v1><strong>Distilling Large Language Models for Matching Patients to Clinical Trials</strong></a></p><p><em>Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The recent success of large language models (LLMs) has paved the way fortheir adoption in the high-stakes domain of healthcare. Specifically, theapplication of LLMs in patient-trial matching, which involves assessing patienteligibility against clinical trial&rsquo;s nuanced inclusion and exclusion criteria,has shown promise. Recent research has shown that GPT-3.5, a widely recognizedLLM developed by OpenAI, can outperform existing methods with minimal &lsquo;variableengineering&rsquo; by simply comparing clinical trial information against patientsummaries. However, there are significant challenges associated with usingclosed-source proprietary LLMs like GPT-3.5 in practical healthcareapplications, such as cost, privacy and reproducibility concerns. To addressthese issues, this study presents the first systematic examination of theefficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA7B,13B, and 70B) for the task of patient-trial matching. Employing amultifaceted evaluation framework, we conducted extensive automated andhuman-centric assessments coupled with a detailed error analysis for eachmodel. To enhance the adaptability of open-source LLMs, we have created aspecialized synthetic dataset utilizing GPT-4, enabling effective fine-tuningunder constrained data conditions. Our findings reveal that open-source LLMs,when fine-tuned on this limited and synthetic dataset, demonstrate performanceparity with their proprietary counterparts. This presents a massive opportunityfor their deployment in real-world healthcare applications. To foster furtherresearch and applications in this field, we release both the annotatedevaluation dataset along with the fine-tuned LLM &ndash; Trial-LLAMA &ndash; for publicuse.</div></details><p><a href=http://arxiv.org/abs/2312.10265v1><strong>VoCopilot: Voice-Activated Tracking of Everyday Interactions</strong></a></p><p><em>Sheen An Goh, Manoj Gulati, Ambuj Varshney</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Voice plays an important role in our lives by facilitating communication,conveying emotions, and indicating health. Therefore, tracking vocalinteractions can provide valuable insight into many aspects of our lives. Thispaper presents our ongoing efforts to design a new vocal tracking system wecall VoCopilot. VoCopilot is an end-to-end system centered around anenergy-efficient acoustic hardware and firmware combined with advanced machinelearning models. As a result, VoCopilot is able to continuously trackconversations, record them, transcribe them, and then extract useful insightsfrom them. By utilizing large language models, VoCopilot ensures the user canextract useful insights from recorded interactions without having to learncomplex machine learning techniques. In order to protect the privacy of endusers, VoCopilot uses a novel wake-up mechanism that only records conversationsof end users. Additionally, all the rest of pipeline can be run on a commoditycomputer (Mac Mini M2). In this work, we show the effectiveness of VoCopilot inreal-world environment for two use cases.</div></details><blockquote><p><strong><em>2023-12-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.08628v3><strong>Recovering from Privacy-Preserving Masking with Large Language Models</strong></a></p><p><em>Arpita Vats, Zhe Liu, Peng Su, Debjyoti Paul, Yingyi Ma, Yutong Pang, Zeeshan Ahmed, Ozlem Kalinli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Model adaptation is crucial to handle the discrepancy between proxy trainingdata and actual users data received. To effectively perform adaptation, textualdata of users is typically stored on servers or their local devices, wheredownstream natural language processing (NLP) models can be directly trainedusing such in-domain data. However, this might raise privacy and securityconcerns due to the extra risks of exposing user information to adversaries.Replacing identifying information in textual data with a generic marker hasbeen recently explored. In this work, we leverage large language models (LLMs)to suggest substitutes of masked tokens and have their effectiveness evaluatedon downstream language modeling tasks. Specifically, we propose multiplepre-trained and fine-tuned LLM-based approaches and perform empirical studieson various datasets for the comparison of these methods. Experimental resultsshow that models trained on the obfuscation corpora are able to achievecomparable performance with the ones trained on the original data withoutprivacy-preserving token masking.</div></details><p><a href=http://arxiv.org/abs/2312.08629v1><strong>ChatSOS: LLM-based knowledge Q&amp;A system for safety engineering</strong></a></p><p><em>Haiyang Tang, Zhenyi Liu, Dongping Chen, Qingzhao Chu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advancements in large language models (LLMs) have notably propellednatural language processing (NLP) capabilities, demonstrating significantpotential in safety engineering applications. Despite these advancements, LLMsface constraints in processing specialized tasks, attributed to factors such ascorpus size, input processing limitations, and privacy concerns. Obtaininguseful information from reliable sources in a limited time is crucial for LLM.Addressing this, our study introduces an LLM-based Q&amp;A system for safetyengineering, enhancing the comprehension and response accuracy of the model. Weemployed prompt engineering to incorporate external knowledge databases, thusenriching the LLM with up-to-date and reliable information. The system analyzeshistorical incident reports through statistical methods, utilizes vectorembedding to construct a vector database, and offers an efficientsimilarity-based search functionality. Our findings indicate that theintegration of external knowledge significantly augments the capabilities ofLLM for in-depth problem analysis and autonomous task assignment. Iteffectively summarizes accident reports and provides pertinent recommendations.This integration approach not only expands LLM applications in safetyengineering but also sets a precedent for future developments towardsautomation and intelligent systems.</div></details><blockquote><p><strong><em>2023-12-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.08143v1><strong>Efficient Representation of the Activation Space in Deep Neural Networks</strong></a></p><p><em>Tanya Akumu, Celia Cintas, Girmaw Abebe Tadesse, Adebayo Oshingbesan, Skyler Speakman, Edward McFowland III</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The representations of the activation space of deep neural networks (DNNs)are widely utilized for tasks like natural language processing, anomalydetection and speech recognition. Due to the diverse nature of these tasks andthe large size of DNNs, an efficient and task-independent representation ofactivations becomes crucial. Empirical p-values have been used to quantify therelative strength of an observed node activation compared to activationscreated by already-known inputs. Nonetheless, keeping raw data for thesecalculations increases memory resource consumption and raises privacy concerns.To this end, we propose a model-agnostic framework for creating representationsof activations in DNNs using node-specific histograms to compute p-values ofobserved activations without retaining already-known inputs. Our proposedapproach demonstrates promising potential when validated with multiple networkarchitectures across various downstream tasks and compared with the kerneldensity estimates and brute-force empirical baselines. In addition, theframework reduces memory usage by 30% with up to 4 times faster p-valuecomputing time while maintaining state of-the-art detection power in downstreamtasks such as the detection of adversarial attacks and synthesized content.Moreover, as we do not persist raw data at inference time, we could potentiallyreduce susceptibility to attacks and privacy issues.</div></details><blockquote><p><strong><em>2023-12-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.06062v2><strong>Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration</strong></a></p><p><em>Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Membership Inference Attacks (MIA) aim to infer whether a target data recordhas been utilized for model training or not. Prior attempts have quantified theprivacy risks of language models (LMs) via MIAs, but there is still noconsensus on whether existing MIA algorithms can cause remarkable privacyleakage on practical Large Language Models (LLMs). Existing MIAs designed forLMs can be classified into two categories: reference-free and reference-basedattacks. They are both based on the hypothesis that training recordsconsistently strike a higher probability of being sampled. Nevertheless, thishypothesis heavily relies on the overfitting of target models, which will bemitigated by multiple regularization methods and the generalization of LLMs.The reference-based attack seems to achieve promising effectiveness in LLMs,which measures a more reliable membership signal by comparing the probabilitydiscrepancy between the target model and the reference model. However, theperformance of reference-based attack is highly dependent on a referencedataset that closely resembles the training dataset, which is usuallyinaccessible in the practical scenario. Overall, existing MIAs are unable toeffectively unveil privacy leakage over practical fine-tuned LLMs that areoverfitting-free and private. We propose a Membership Inference Attack based onSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, sincememorization in LLMs is inevitable during the training process and occursbefore overfitting, we introduce a more reliable membership signal,probabilistic variation, which is based on memorization rather thanoverfitting. Furthermore, we introduce a self-prompt approach, which constructsthe dataset to fine-tune the reference model by prompting the target LLMitself. In this manner, the adversary can collect a dataset with a similardistribution from public APIs.</div></details><blockquote><p><strong><em>2023-12-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.12214v5><strong>InferDPT: Privacy-Preserving Inference for Black-box Large Language Model</strong></a></p><p><em>Meng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang, Nenghai Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs), like ChatGPT, have greatly simplified textgeneration tasks. However, they have also raised concerns about privacy riskssuch as data leakage and unauthorized data collection. Existing solutions forprivacy-preserving inference face practical challenges related to computationtime and communication costs. In this paper, we propose InferDPT, the firstpractical framework for the privacy-preserving Inference of black-box LLMs,implementing Differential Privacy in Text generation. InferDPT comprises twokey modules: the &ldquo;perturbation module&rdquo; utilizes the exponential mechanism togenerate a perturbed prompt, facilitating privacy-preserving inference withblack-box LLMs, and the &ldquo;extraction module&rdquo;, inspired by knowledge distillationand retrieval-augmented generation, extracts coherent and consistent text fromthe perturbed generation result, ensuring successful text generationcompletion. To address privacy concerns related to previous exponentialmechanisms&rsquo; susceptibility to embedding revision attacks, we introduce RANTEXT,a novel differential privacy mechanism integrated into the perturbation moduleof InferDPT, which introduces the concept of &ldquo;RANdom adjacency&rdquo; for TEXTperturbation within the prompt. Experimental results across three datasetsdemonstrate that the text generation quality of InferDPT is comparable to thatof non-private GPT-4, and RANTEXT surpasses existing state-of-the-artmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy andutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achievesan average privacy protection rate exceeding 90% against embedding revisionattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higherthan that of CUSTEXT+.</div></details><blockquote><p><strong><em>2023-12-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.05842v1><strong>Mutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer</strong></a></p><p><em>Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, Yaoxue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While large language models (LLMs) are empowered with broad knowledge, theirtask-specific performance is often suboptimal. It necessitates fine-tuning LLMswith task-specific data, but such data may be inaccessible due to privacyconcerns. In this paper, we propose a novel approach to enhance LLMs withsmaller language models (SLMs) that are trained on clients using their privatetask-specific data. To enable mutual enhancement between LLMs and SLMs, wepropose CrossLM, where the SLMs promote the LLM to generate task-specifichigh-quality data, and both the LLM and SLMs are enhanced with the generateddata. We evaluate CrossLM using publicly accessible language models across arange of benchmark tasks. The results demonstrate that CrossLM significantlyenhances the task-specific performance of SLMs on clients and the LLM on thecloud server simultaneously while preserving the LLM&rsquo;s generalizationcapability.</div></details><blockquote><p><strong><em>2023-12-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.16680v5><strong>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook</strong></a></p><p><em>Mingyuan Fan, Chengyu Wang, Cen Chen, Yang Liu, Jun Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Diffusion models and large language models have emerged as leading-edgegenerative models, revolutionizing various aspects of human life. However, thepractical implementations of these models have also exposed inherent risks,bringing to the forefront their evil sides and sparking concerns regardingtheir trustworthiness. Despite the wealth of literature on this subject, acomprehensive survey specifically delving into the intersection of large-scalegenerative models and their trustworthiness remains largely absent. To bridgethis gap, this paper investigates both the long-standing and emerging threatsassociated with these models across four fundamental dimensions: 1) privacy, 2)security, 3) fairness, and 4) responsibility. Based on the investigationresults, we develop an extensive map outlining the trustworthiness of largegenerative models. After that, we provide practical recommendations andpotential research directions for future secure applications equipped withlarge generative models, ultimately promoting the trustworthiness of the modelsand benefiting the society as a whole.</div></details><p><a href=http://arxiv.org/abs/2312.14950v1><strong>TypeFly: Flying Drones with Large Language Model</strong></a></p><p><em>Guojun Chen, Xiaojing Yu, Lin Zhong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Commanding a drone with a natural language is not only user-friendly but alsoopens the door for emerging language agents to control the drone. Emerginglarge language models (LLMs) provide a previously impossible opportunity toautomatically translate a task description in a natural language to a programthat can be executed by the drone. However, powerful LLMs and their visioncounterparts are limited in three important ways. First, they are onlyavailable as cloud-based services. Sending images to the cloud raises privacyconcerns. Second, they are expensive, costing proportionally to the requestsize. Finally, without expensive fine-tuning, existing LLMs are quite limitedin their capability of writing a program for specialized systems like drones. In this paper, we present a system called TypeFly that tackles the abovethree problems using a combination of edge-based vision intelligence, novelprogramming language design, and prompt engineering. Instead of the familiarPython, TypeFly gets a cloud-based LLM service to write a program in a small,custom language called MiniSpec, based on task and scene descriptions inEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)but also able to consult the LLM during their execution using a special skillcalled query. Using a set of increasingly challenging drone tasks, we show thatdesign choices made by TypeFly can reduce both the cost of LLM service and thetask execution time by more than 2x. More importantly, query and promptengineering techniques contributed by TypeFly significantly improve the chanceof success of complex tasks.</div></details><blockquote><p><strong><em>2023-12-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.14208v2><strong>Domain Private Transformers for Multi-Domain Dialog Systems</strong></a></p><p><em>Anmol Kabra, Ethan R. Elenberg</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large, general purpose language models have demonstrated impressiveperformance across many different conversational domains. While multi-domainlanguage models achieve low overall perplexity, their outputs are notguaranteed to stay within the domain of a given input prompt. This paperproposes domain privacy as a novel way to quantify how likely a conditionallanguage model will leak across domains. We also develop policy functions basedon token-level domain classification, and propose an efficient fine-tuningmethod to improve the trained model&rsquo;s domain privacy. Experiments on membershipinference attacks show that our proposed method has comparable resiliency tomethods adapted from recent literature on differentially private languagemodels.</div></details><blockquote><p><strong><em>2023-12-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.07550v1><strong>Understanding (Un)Intended Memorization in Text-to-Image Generative Models</strong></a></p><p><em>Ali Naseh, Jaechul Roh, Amir Houmansadr</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Multimodal machine learning, especially text-to-image models like StableDiffusion and DALL-E 3, has gained significance for transforming text intodetailed images. Despite their growing use and remarkable generative capabilities, there is apressing need for a detailed examination of these models&rsquo; behavior,particularly with respect to memorization. Historically, memorization inmachine learning has been context-dependent, with diverse definitions emergingfrom classification tasks to complex models like Large Language Models (LLMs)and Diffusion models. Yet, a definitive concept of memorization that alignswith the intricacies of text-to-image synthesis remains elusive. Thisunderstanding is vital as memorization poses privacy risks yet is essential formeeting user expectations, especially when generating representations ofunderrepresented entities. In this paper, we introduce a specialized definitionof memorization tailored to text-to-image models, categorizing it into threedistinct types according to user expectations. We closely examine the subtledistinctions between intended and unintended memorization, emphasizing theimportance of balancing user privacy with the generative quality of the modeloutputs. Using the Stable Diffusion model, we offer examples to validate ourmemorization definitions and clarify their application.</div></details><blockquote><p><strong><em>2023-12-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.20138v2><strong>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</strong></a></p><p><em>Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models pretrained on a huge amount of data capture richknowledge and information in the training data. The ability of datamemorization and regurgitation in pretrained language models, revealed inprevious studies, brings the risk of data leakage. In order to effectivelyreduce these risks, we propose a framework DEPN to Detect and Edit PrivacyNeurons in pretrained language models, partially inspired by knowledge neuronsand model editing. In DEPN, we introduce a novel method, termed as privacyneuron detector, to locate neurons associated with private information, andthen edit these detected privacy neurons by setting their activations to zero.Furthermore, we propose a privacy neuron aggregator dememorize privateinformation in a batch processing manner. Experimental results show that ourmethod can significantly and efficiently reduce the exposure of private dataleakage without deteriorating the performance of the model. Additionally, weempirically demonstrate the relationship between model memorization and privacyneurons, from multiple perspectives, including model size, training time,prompts, privacy neuron distribution, illustrating the robustness of ourapproach.</div></details><blockquote><p><strong><em>2023-12-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.01242v2><strong>Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</strong></a></p><p><em>Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yan Lu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The recent success of Large Language Models (LLMs) signifies an impressivestride towards artificial general intelligence. They have shown a promisingprospect in automatically completing tasks upon user instructions, functioningas brain-like coordinators. The associated risks will be revealed as wedelegate an increasing number of tasks to machines for automated completion. Abig question emerges: how can we make machines behave responsibly when helpinghumans automate tasks as personal copilots? In this paper, we explore thisquestion in depth from the perspectives of feasibility, completeness andsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)as a fundamental framework to facilitate responsible collaboration betweenLLM-based coordinators and executors for task automation with three empoweredcapabilities: 1) predicting the feasibility of the commands for executors; 2)verifying the completeness of executors; 3) enhancing the security (e.g., theprotection of users&rsquo; privacy). We further propose and compare two paradigms forimplementing the first two capabilities. One is to leverage the genericknowledge of LLMs themselves via prompt engineering while the other is to adoptdomain-specific learnable models. Moreover, we introduce a local memorymechanism for achieving the third capability. We evaluate our proposedResponsibleTA on UI task automation and hope it could bring more attentions toensuring LLMs more responsible in diverse scenarios.</div></details><blockquote><p><strong><em>2023-12-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2312.00388v1><strong>LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices</strong></a></p><p><em>Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, Sangeetha Abdu Jyothi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Deploying Large Language Models (LLMs) locally on mobile devices presents asignificant challenge due to their extensive memory requirements. In thispaper, we introduce LinguaLinked, a system for decentralized, distributed LLMinference on mobile devices. LinguaLinked enables collaborative execution ofthe inference task across multiple trusted devices. LinguaLinked ensures dataprivacy by processing information locally. LinguaLinked uses three keystrategies. First, an optimized model assignment technique segments LLMs anduses linear optimization to align segments with each device&rsquo;s capabilities.Second, an optimized data transmission mechanism ensures efficient andstructured data flow between model segments while also maintaining theintegrity of the original model structure. Finally, LinguaLinked incorporates aruntime load balancer that actively monitors and redistributes tasks amongmobile devices to prevent bottlenecks, enhancing the system&rsquo;s overallefficiency and responsiveness. We demonstrate that LinguaLinked facilitatesefficient LLM inference while maintaining consistent throughput and minimallatency through extensive testing across various mobile devices, from high-endto low-end Android devices. In our evaluations, compared to the baseline,LinguaLinked achieves an inference performance acceleration of $1.11\times$ to$1.61\times$ in single-threaded settings, $1.73\times$ to $2.65\times$ withmulti-threading. Additionally, runtime load balancing yields an overallinference acceleration of $1.29\times$ to $1.32\times$.</div></details><blockquote><p><strong><em>2023-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16111v2><strong>Locally Differentially Private Document Generation Using Zero Shot Prompting</strong></a></p><p><em>Saiteja Utpala, Sara Hooker, Pin Yu Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Numerous studies have highlighted the privacy risks associated withpretrained large language models. In contrast, our research offers a uniqueperspective by demonstrating that pretrained large language models caneffectively contribute to privacy preservation. We propose a locallydifferentially private mechanism called DP-Prompt, which leverages the power ofpretrained large language models and zero-shot prompting to counter authorde-anonymization attacks while minimizing the impact on downstream utility.When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),we observe a notable reduction in the success rate of de-anonymization attacks,showing that it surpasses existing approaches by a considerable margin despiteits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt(with ChatGPT) perfectly recovers the clean sentiment F1 score while achievinga 46% reduction in author identification F1 score against static attackers anda 26% reduction against adaptive attackers. We conduct extensive experimentsacross six open-source large language models, ranging up to 7 billionparameters, to analyze various effects of the privacy-utility tradeoff.</div></details><p><a href=http://arxiv.org/abs/2311.18345v1><strong>Situating the social issues of image generation models in the model life cycle: a sociotechnical approach</strong></a></p><p><em>Amelia Katirai, Noa Garcia, Kazuki Ide, Yuta Nakashima, Atsuo Kishimoto</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The race to develop image generation models is intensifying, with a rapidincrease in the number of text-to-image models available. This is coupled withgrowing public awareness of these technologies. Though other generative AImodels&ndash;notably, large language models&ndash;have received recent critical attentionfor the social and other non-technical issues they raise, there has beenrelatively little comparable examination of image generation models. This paperreports on a novel, comprehensive categorization of the social issuesassociated with image generation models. At the intersection of machinelearning and the social sciences, we report the results of a survey of theliterature, identifying seven issue clusters arising from image generationmodels: data issues, intellectual property, bias, privacy, and the impacts onthe informational, cultural, and natural environments. We situate these socialissues in the model life cycle, to aid in considering where potential issuesarise, and mitigation may be needed. We then compare these issue clusters withwhat has been reported for large language models. Ultimately, we argue that therisks posed by image generation models are comparable in severity to the risksposed by large language models, and that the social impact of image generationmodels must be urgently considered.</div></details><blockquote><p><strong><em>2023-11-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16153v2><strong>Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications</strong></a></p><p><em>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo Li, Radha Poovendran</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are increasingly deployed as the service backendfor LLM-integrated applications such as code completion and AI-powered search.LLM-integrated applications serve as middleware to refine users&rsquo; queries withdomain-specific knowledge to better inform LLMs and enhance the responses.Despite numerous opportunities and benefits, LLM-integrated applications alsointroduce new attack surfaces. Understanding, minimizing, and eliminating theseemerging attack surfaces is a new area of research. In this work, we consider asetup where the user and LLM interact via an LLM-integrated application in themiddle. We focus on the communication rounds that begin with user&rsquo;s queries andend with LLM-integrated application returning responses to the queries, poweredby LLMs at the service backend. For this query-response protocol, we identifypotential vulnerabilities that can originate from the malicious applicationdeveloper or from an outsider threat initiator that is able to control thedatabase access, manipulate and poison data that are high-risk for the user.Successful exploits of the identified vulnerabilities result in the usersreceiving responses tailored to the intent of a threat initiator. We assesssuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5and GPT-4. Our empirical results show that the threats can effectively bypassthe restrictions and moderation policies of OpenAI, resulting in usersreceiving responses that contain bias, toxic content, privacy risk, anddisinformation. To mitigate those threats, we identify and define four keyproperties, namely integrity, source identification, attack detectability, andutility preservation, that need to be satisfied by a safe LLM-integratedapplication. Based on these properties, we develop a lightweight,threat-agnostic defense that mitigates both insider and outsider threats.</div></details><blockquote><p><strong><em>2023-11-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16429v1><strong>The Transformative Influence of Large Language Models on Software Development</strong></a></p><p><em>Sajed Jalil</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The increasing adoption and commercialization of generalized Large LanguageModels (LLMs) have profoundly impacted various aspects of our daily lives.Initially embraced by the computer science community, the versatility of LLMshas found its way into diverse domains. In particular, the software engineeringrealm has witnessed the most transformative changes. With LLMs increasinglyserving as AI Pair Programming Assistants spurred the development ofspecialized models aimed at aiding software engineers. Although this newparadigm offers numerous advantages, it also presents critical challenges andopen problems. To identify the potential and prevailing obstacles, wesystematically reviewed contemporary scholarly publications, emphasizing theperspectives of software developers and usability concerns. Preliminaryfindings underscore pressing concerns about data privacy, bias, andmisinformation. Additionally, we identified several usability challenges,including prompt engineering, increased cognitive demands, and mistrust.Finally, we introduce 12 open problems that we have identified through oursurvey, covering these various domains.</div></details><blockquote><p><strong><em>2023-11-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.16060v1><strong>DiffSLVA: Harnessing Diffusion Models for Sign Language Video Anonymization</strong></a></p><p><em>Zhaoyang Xia, Carol Neidle, Dimitris N. Metaxas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Since American Sign Language (ASL) has no standard written form, Deaf signersfrequently share videos in order to communicate in their native language.However, since both hands and face convey critical linguistic information insigned languages, sign language videos cannot preserve signer privacy. Whilesigners have expressed interest, for a variety of applications, in signlanguage video anonymization that would effectively preserve linguisticcontent, attempts to develop such technology have had limited success, giventhe complexity of hand movements and facial expressions. Existing approachesrely predominantly on precise pose estimations of the signer in video footageand often require sign language video datasets for training. These requirementsprevent them from processing videos &lsquo;in the wild,&rsquo; in part because of thelimited diversity present in current sign language video datasets. To addressthese limitations, our research introduces DiffSLVA, a novel methodology thatutilizes pre-trained large-scale diffusion models for zero-shot text-guidedsign language video anonymization. We incorporate ControlNet, which leverageslow-level image features such as HED (Holistically-Nested Edge Detection)edges, to circumvent the need for pose estimation. Additionally, we develop aspecialized module dedicated to capturing facial expressions, which arecritical for conveying essential linguistic information in signed languages. Wethen combine the above methods to achieve anonymization that better preservesthe essential linguistic content of the original signer. This innovativemethodology makes possible, for the first time, sign language videoanonymization that could be used for real-world applications, which would offersignificant benefits to the Deaf and Hard-of-Hearing communities. Wedemonstrate the effectiveness of our approach with a series of signeranonymization experiments.</div></details><blockquote><p><strong><em>2023-11-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.09620v2><strong>AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction</strong></a></p><p><em>Junsol Kim, Byungkyu Lee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) that produce human-like responses have begun torevolutionize research practices in the social sciences. This paper shows howwe can integrate LLMs and social surveys to accurately predict individualresponses to survey questions that were not asked before. We develop a novelmethodological framework to personalize LLMs by considering the meaning ofsurvey questions derived from their text, the latent beliefs of individualsinferred from their response patterns, and the temporal contexts acrossdifferent survey periods through fine-tuning LLMs with survey data. Using theGeneral Social Survey from 1972 to 2021, we show that the fine-tuned modelbased on Alpaca-7b can predict individual responses to survey questions thatare partially missing as well as entirely missing. The remarkable predictioncapabilities allow us to fill in missing trends with high confidence andpinpoint when public attitudes changed, such as the rising support for same-sexmarriage. We discuss practical constraints, socio-demographic representation,and ethical concerns regarding individual autonomy and privacy when using LLMsfor opinion prediction. This study demonstrates that LLMs and surveys canmutually enhance each other&rsquo;s capabilities: LLMs broaden survey potential,while surveys improve the alignment of LLMs.</div></details><blockquote><p><strong><em>2023-11-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.07585v2><strong>Input Reconstruction Attack against Vertical Federated Large Language Models</strong></a></p><p><em>Fei Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, large language models (LLMs) have drawn extensive attention fromacademia and the public, due to the advent of the ChatGPT. While LLMs showtheir astonishing ability in text generation for various tasks, privacyconcerns limit their usage in real-life businesses. More specifically, eitherthe user&rsquo;s inputs (the user sends the query to the model-hosting server) or themodel (the user downloads the complete model) itself will be revealed duringthe usage. Vertical federated learning (VFL) is a promising solution to thiskind of problem. It protects both the user&rsquo;s input and the knowledge of themodel by splitting the model into a bottom part and a top part, which ismaintained by the user and the model provider, respectively. However, in thispaper, we demonstrate that in LLMs, VFL fails to protect the user input sinceit is simple and cheap to reconstruct the input from the intermediateembeddings. Experiments show that even with a commercial GPU, the inputsentence can be reconstructed in only one second. We also discuss severalpossible solutions to enhance the privacy of vertical federated LLMs.</div></details><blockquote><p><strong><em>2023-11-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.14030v1><strong>PrivateLoRA For Efficient Privacy Preserving LLM</strong></a></p><p><em>Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: End users face a choice between privacy and efficiency in current LargeLanguage Model (LLM) service paradigms. In cloud-based paradigms, users areforced to compromise data locality for generation quality and processing speed.Conversely, edge device paradigms maintain data locality but fail to deliversatisfactory performance. In this work, we propose a novel LLM service paradigmthat distributes privacy-sensitive computation on edge devices and sharedcomputation in the cloud. Only activations are transmitted between the centralcloud and edge devices to ensure data locality. Our core innovation,PrivateLoRA, addresses the challenging communication overhead by exploiting thelow rank of residual activations, achieving over 95% communication reduction.Consequently, PrivateLoRA effectively maintains data locality and is extremelyresource efficient. Under standard 5G networks, PrivateLoRA achieves throughputover 300% of device-only solutions for 7B models and over 80% of an A100 GPUfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRAfor advanced personalization. Our approach democratizes access tostate-of-the-art generative AI for edge devices, paving the way for moretailored LLM experiences for the general public. To our knowledge, our proposedframework is the first efficient and privacy-preserving LLM solution in theliterature.</div></details><p><a href=http://arxiv.org/abs/2311.13857v1><strong>Challenges of Large Language Models for Mental Health Counseling</strong></a></p><p><em>Neo Christopher Chung, George Dyer, Lennart Brocki</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The global mental health crisis is looming with a rapid increase in mentaldisorders, limited resources, and the social stigma of seeking treatment. Asthe field of artificial intelligence (AI) has witnessed significantadvancements in recent years, large language models (LLMs) capable ofunderstanding and generating human-like text may be used in supporting orproviding psychological counseling. However, the application of LLMs in themental health domain raises concerns regarding the accuracy, effectiveness, andreliability of the information provided. This paper investigates the majorchallenges associated with the development of LLMs for psychologicalcounseling, including model hallucination, interpretability, bias, privacy, andclinical effectiveness. We explore potential solutions to these challenges thatare practical and applicable to the current paradigm of AI. From our experiencein developing and deploying LLMs for mental health, AI holds a great promisefor improving mental health care, if we can carefully navigate and overcomepitfalls of LLMs.</div></details><blockquote><p><strong><em>2023-11-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.12639v1><strong>KNVQA: A Benchmark for evaluation knowledge-based VQA</strong></a></p><p><em>Sirui Cheng, Siyu Zhang, Jiayi Wu, Muchen Lan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Within the multimodal field, large vision-language models (LVLMs) have madesignificant progress due to their strong perception and reasoning capabilitiesin the visual and language systems. However, LVLMs are still plagued by the twocritical issues of object hallucination and factual accuracy, which limit thepracticality of LVLMs in different scenarios. Furthermore, previous evaluationmethods focus more on the comprehension and reasoning of language content butlack a comprehensive evaluation of multimodal interactions, thereby resultingin potential limitations. To this end, we propose a novel KNVQA-Eval, which isdevoted to knowledge-based VQA task evaluation to reflect the factuality ofmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,we develop a new KNVQA dataset by incorporating human judgment and perception,aiming to evaluate the accuracy of standard answers relative to AI-generatedanswers in knowledge-based VQA. This work not only comprehensively evaluatesthe contextual information of LVLMs using reliable human annotations, but alsofurther analyzes the fine-grained capabilities of current methods to revealpotential avenues for subsequent optimization of LVLMs-based estimators. Ourproposed VQA-Eval and corresponding dataset KNVQA will facilitate thedevelopment of automatic evaluation tools with the advantages of low cost,privacy protection, and reproducibility. Our code will be released uponpublication.</div></details><p><a href=http://arxiv.org/abs/2311.12287v1><strong>Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications</strong></a></p><p><em>Samira Ghodratnama, Mehrdad Zakershahrak</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advent of Large Language Models (LLMs) heralds a pivotal shift in onlineuser interactions with information. Traditional Information Retrieval (IR)systems primarily relied on query-document matching, whereas LLMs excel incomprehending and generating human-like text, thereby enriching the IRexperience significantly. While LLMs are often associated with chatbotfunctionalities, this paper extends the discussion to their explicitapplication in information retrieval. We explore methodologies to optimize theretrieval process, select optimal models, and effectively scale and orchestrateLLMs, aiming for cost-efficiency and enhanced result accuracy. A notablechallenge, model hallucination-where the model yields inaccurate ormisinterpreted data-is addressed alongside other model-specific hurdles. Ourdiscourse extends to crucial considerations including user privacy, dataoptimization, and the necessity for system clarity and interpretability.Through a comprehensive examination, we unveil not only innovative strategiesfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,but also the consequential considerations that underline the need for abalanced approach aligned with user-centric principles.</div></details><p><a href=http://arxiv.org/abs/2311.12955v1><strong>Don&rsquo;t forget private retrieval: distributed private similarity search for large language models</strong></a></p><p><em>Guy Zyskind, Tobin South, Alex Pentland</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While the flexible capabilities of large language models (LLMs) allow them toanswer a range of queries based on existing learned knowledge, informationretrieval to augment generation is an important tool to allow LLMs to answerquestions on information not included in pre-training data. Such privateinformation is increasingly being generated in a wide array of distributedcontexts by organizations and individuals. Performing such informationretrieval using neural embeddings of queries and documents always leakedinformation about queries and database content unless both were stored locally.We present Private Retrieval Augmented Generation (PRAG), an approach that usesmulti-party computation (MPC) to securely transmit queries to a distributed setof servers containing a privately constructed database to return top-k andapproximate top-k documents. This is a first-of-its-kind approach to denseinformation retrieval that ensures no server observes a client&rsquo;s query or cansee the database content. The approach introduces a novel MPC friendly protocolfor inverted file approximate search (IVF) that allows for fast document searchover distributed and private data in sublinear communication complexity. Thiswork presents new avenues through which data for use in LLMs can be accessedand used without needing to centralize or forgo privacy.</div></details><blockquote><p><strong><em>2023-11-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.11161v1><strong>Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&amp;A</strong></a></p><p><em>Zahra Kolagar, Anna Katharina Leschanowsky, Birgit Popp</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy policies play a vital role in safeguarding user privacy as legaljurisdictions worldwide emphasize the need for transparent data processing.While the suitability of privacy policies to enhance transparency has beencritically discussed, employing conversational AI systems presents uniquechallenges in informing users effectively. In this position paper, we propose adynamic workflow for transforming privacy policies into privacyquestion-and-answer (Q&amp;A) pairs to make privacy policies easily accessiblethrough conversational AI. Thereby, we facilitate interdisciplinarycollaboration among legal experts and conversation designers, while alsoconsidering the utilization of large language models&rsquo; generative capabilitiesand addressing associated challenges. Our proposed workflow underscorescontinuous improvement and monitoring throughout the construction of privacyQ&amp;As, advocating for comprehensive review and refinement through anexperts-in-the-loop approach.</div></details><blockquote><p><strong><em>2023-11-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.10785v1><strong>Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models</strong></a></p><p><em>Federico Albanese, Daniel Ciolek, Nicolas D&rsquo;Ippolito</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the context of information systems, text sanitization techniques are usedto identify and remove sensitive data to comply with security and regulatoryrequirements. Even though many methods for privacy preservation have beenproposed, most of them are focused on the detection of entities from specificdomains (e.g., credit card numbers, social security numbers), lackinggenerality and requiring customization for each desirable domain. Moreover,removing words is, in general, a drastic measure, as it can degrade textcoherence and contextual information. Less severe measures include substitutinga word for a safe alternative, yet it can be challenging to automatically findmeaningful substitutions. We present a zero-shot text sanitization techniquethat detects and substitutes potentially sensitive information using LargeLanguage Models. Our evaluation shows that our method excels at protectingprivacy while maintaining text coherence and contextual information, preservingdata utility for downstream tasks.</div></details><blockquote><p><strong><em>2023-11-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.09447v1><strong>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</strong></a></p><p><em>Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an assessment ofopen-source LLMs on trustworthiness, scrutinizing them across eight differentaspects including toxicity, stereotypes, ethics, hallucination, fairness,sycophancy, privacy, and robustness against adversarial demonstrations. Wepropose an enhanced Chain of Utterances-based (CoU) prompting strategy byincorporating meticulously crafted malicious demonstrations for trustworthinessattack. Our extensive experiments encompass recent and representative series ofopen-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. Theempirical outcomes underscore the efficacy of our attack strategy acrossdiverse aspects. More interestingly, our result analysis reveals that modelswith superior performance in general NLP tasks do not always have greatertrustworthiness; in fact, larger models can be more vulnerable to attacks.Additionally, models that have undergone instruction tuning, focusing oninstruction following, tend to be more susceptible, although fine-tuning LLMsfor safety alignment proves effective in mitigating adversarial trustworthinessattacks.</div></details><p><a href=http://arxiv.org/abs/2311.10766v1><strong>Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values</strong></a></p><p><em>Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, Xing Xie</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid advancement of Large Language Models (LLMs) has attracted muchattention to value alignment for their responsible development. However, how todefine values in this context remains a largely unexplored question. Existingwork mainly follows the Helpful, Honest, Harmless principle and specifiesvalues as risk criteria formulated in the AI community, e.g., fairness andprivacy protection, suffering from poor clarity, adaptability and transparency.Inspired by basic values in humanity and social science across cultures, thiswork proposes a novel basic value alignment paradigm and introduces a valuespace spanned by basic value dimensions. All LLMs&rsquo; behaviors can be mapped intothe space by identifying the underlying values, possessing the potential toaddress the three challenges. To foster future research, we apply therepresentative Schwartz&rsquo;s Theory of Basic Values as an initialized example andconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.Our extensive analysis of FULCRA reveals the underlying relation between basicvalues and LLMs&rsquo; behaviors, demonstrating that our approach not only coversexisting mainstream risks but also anticipates possibly unidentified ones.Additionally, we present an initial implementation of the basic valueevaluation and alignment, paving the way for future research in this line.</div></details><blockquote><p><strong><em>2023-11-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.08357v1><strong>Sparsity-Preserving Differentially Private Training of Large Embedding Models</strong></a></p><p><em>Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As the use of large embedding models in recommendation systems and languageapplications increases, concerns over user data privacy have also risen.DP-SGD, a training algorithm that combines differential privacy with stochasticgradient descent, has been the workhorse in protecting user privacy withoutcompromising model accuracy by much. However, applying DP-SGD naively toembedding models can destroy gradient sparsity, leading to reduced trainingefficiency. To address this issue, we present two new algorithms, DP-FEST andDP-AdaFEST, that preserve gradient sparsity during private training of largeembedding models. Our algorithms achieve substantial reductions ($10^6 \times$)in gradient size, while maintaining comparable levels of accuracy, on benchmarkreal-world datasets.</div></details><blockquote><p><strong><em>2023-11-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.06805v1><strong>Tunable Soft Prompts are Messengers in Federated Learning</strong></a></p><p><em>Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) enables multiple participants to collaborativelytrain machine learning models using decentralized data sources, alleviatingprivacy concerns that arise from directly sharing local data. However, the lackof model privacy protection in FL becomes an unneglectable challenge,especially when people want to federally finetune models based on a proprietarylarge language model. In this study, we propose a novel FL training approachthat accomplishes information exchange among participants via tunable softprompts. These soft prompts, updated and transmitted between the server andclients, assume the role of the global model parameters and serve as messengersto deliver useful knowledge from the local data and global model. As the globalmodel itself is not required to be shared and the local training is conductedbased on an auxiliary model with fewer parameters than the global model, theproposed approach provides protection for the global model while reducingcommunication and computation costs in FL. Extensive experiments show theeffectiveness of the proposed approach compared to several baselines. We havereleased the source code at\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.</div></details><p><a href=http://arxiv.org/abs/2311.09243v1><strong>Evaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling</strong></a></p><p><em>Yujin Cho, Mingeon Kim, Seojin Kim, Oyun Kwon, Ryan Donghan Kwon, Yoonha Lee, Dohyun Lim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This study investigates the efficacy of Large Language Models (LLMs) ininteractive language therapy for high-functioning autistic adolescents. Withthe rapid advancement of artificial intelligence, particularly in naturallanguage processing, LLMs present a novel opportunity to augment traditionalpsychological counseling methods. This research primarily focuses on evaluatingthe LLM&rsquo;s ability to engage in empathetic, adaptable, and contextuallyappropriate interactions within a therapeutic setting. A comprehensiveevaluation was conducted by a panel of clinical psychologists and psychiatristsusing a specially developed scorecard. The assessment covered various aspectsof the LLM&rsquo;s performance, including empathy, communication skills,adaptability, engagement, and the ability to establish a therapeutic alliance.The study avoided direct testing with patients, prioritizing privacy andethical considerations, and instead relied on simulated scenarios to gauge theLLM&rsquo;s effectiveness. The results indicate that LLMs hold significant promise assupportive tools in therapy, demonstrating strengths in empathetic engagementand adaptability in conversation. However, challenges in achieving the depth ofpersonalization and emotional understanding characteristic of human therapistswere noted. The study also highlights the importance of ethical considerationsin the application of AI in therapeutic contexts. This research providesvaluable insights into the potential and limitations of using LLMs inpsychological counseling for autistic adolescents. It lays the groundwork forfuture explorations into AI&rsquo;s role in mental health care, emphasizing the needfor ongoing development to enhance the capabilities of these models intherapeutic settings.</div></details><blockquote><p><strong><em>2023-11-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.11254v2><strong>An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing</strong></a></p><p><em>Le Peng, Gaoxiang Luo, sicheng zhou, jiandong chen, Rui Zhang, Ziyue Xu, Ju Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models (LMs) such as BERT and GPT have revolutionized naturallanguage processing (NLP). However, the medical field faces challenges intraining LMs due to limited data access and privacy constraints imposed byregulations like the Health Insurance Portability and Accountability Act(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning(FL) offers a decentralized solution that enables collaborative learning whileensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasksencompassing 8 corpora using 6 LMs. Our results show that: 1) FL modelsconsistently outperformed models trained on individual clients&rsquo; data andsometimes performed comparably with models trained with polled data; 2) withthe fixed number of total data, FL models training with more clients producedinferior performance but pre-trained transformer-based models exhibited greatresilience. 3) FL models significantly outperformed large language models usingzero-/one-shot learning and offered lightning inference speed.</div></details><blockquote><p><strong><em>2023-11-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.05863v1><strong>Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service</strong></a></p><p><em>Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advances in vision-language pre-trained models (VLPs) havesignificantly increased visual understanding and cross-modal analysiscapabilities. Companies have emerged to provide multi-modal Embedding as aService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amountof training data and resources for high-performance service. However, existingstudies indicate that EaaS is vulnerable to model extraction attacks thatinduce great loss for the owners of VLPs. Protecting the intellectual propertyand commercial ownership of VLPs is increasingly crucial yet challenging. Amajor solution of watermarking model for EaaS implants a backdoor in the modelby inserting verifiable trigger embeddings into texts, but it is onlyapplicable for large language models and is unrealistic due to data and modelprivacy. In this paper, we propose a safe and robust backdoor-based embeddingwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embeddingorthogonal transformation to effectively inject triggers into the VLPs withoutinterfering with the model parameters, which achieves high-quality copyrightverification and minimal impact on model performance. To enhance the watermarkrobustness, we further propose a collaborative copyright verification strategybased on both backdoor trigger and embedding distribution, enhancing resilienceagainst various attacks. We increase the watermark practicality via anout-of-distribution trigger selection approach, removing access to the modeltraining data and thus making it possible for many real-world scenarios. Ourextensive experiments on various datasets indicate that the proposedwatermarking approach is effective and safe for verifying the copyright of VLPsfor multi-modal EaaS and robust against model extraction attacks. Our code isavailable at <a href=https://github.com/Pter61/vlpmarker>https://github.com/Pter61/vlpmarker</a>.</div></details><blockquote><p><strong><em>2023-11-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.06294v2><strong>Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT</strong></a></p><p><em>Jingye Yang, Cong Liu, Wendy Deng, Da Wu, Chunhua Weng, Yunyun Zhou, Kai Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.</div></details><blockquote><p><strong><em>2023-11-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.19233v3><strong>Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective</strong></a></p><p><em>Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper studies how to effectively build meeting summarization systems forreal-world usage using large language models (LLMs). For this purpose, weconduct an extensive evaluation and comparison of various closed-source andopen-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findingsreveal that most closed-source LLMs are generally better in terms ofperformance. However, much smaller open-source models like LLaMA- 2 (7B and13B) could still achieve performance comparable to the large closed-sourcemodels even in zero-shot scenarios. Considering the privacy concerns ofclosed-source models for only being accessible via API, alongside the high costassociated with using fine-tuned versions of the closed-source models, theopensource models that can achieve competitive performance are moreadvantageous for industrial use. Balancing performance with associated costsand privacy concerns, the LLaMA-2-7B model looks more promising for industrialusage. In sum, this paper offers practical insights on using LLMs forreal-world business meeting summarization, shedding light on the trade-offsbetween performance and cost.</div></details><blockquote><p><strong><em>2023-11-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.15727v2><strong>Quantifying and Analyzing Entity-level Memorization in Large Language Models</strong></a></p><p><em>Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have been proven capable of memorizing theirtraining data, which can be extracted through specifically designed prompts. Asthe scale of datasets continues to grow, privacy risks arising frommemorization have attracted increasing attention. Quantifying language modelmemorization helps evaluate potential privacy risks. However, prior works onquantifying memorization require access to the precise original data or incursubstantial computational overhead, making it difficult for applications inreal-world language models. To this end, we propose a fine-grained,entity-level definition to quantify memorization with conditions and metricscloser to real-world scenarios. In addition, we also present an approach forefficiently extracting sensitive entities from autoregressive language models.We conduct extensive experiments based on the proposed, probing languagemodels&rsquo; ability to reconstruct sensitive entities under different settings. Wefind that language models have strong memorization at the entity level and areable to reproduce the training data even with partial leakages. The resultsdemonstrate that LLMs not only memorize their training data but also understandassociations between entities. These findings necessitate that trainers of LLMsexercise greater prudence regarding model memorization, adopting memorizationmitigation techniques to preclude privacy violations.</div></details><blockquote><p><strong><em>2023-11-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.02192v1><strong>Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models</strong></a></p><p><em>Jake Chanenson, Madison Pickering, Noah Apthorpe</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Identifying contextual integrity (CI) and governing knowledge commons (GKC)parameters in privacy policy texts can facilitate normative privacy analysis.However, GKC-CI annotation has heretofore required manual or crowdsourcedeffort. This paper demonstrates that high-accuracy GKC-CI parameter annotationof privacy policies can be performed automatically using large language models.We fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotationsfrom 16 ground truth privacy policies. Our best-performing model (fine-tunedGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding theperformance of prior crowdsourcing approaches despite the complexity of privacypolicy texts and the nuance of the GKC-CI annotation task. We apply ourbest-performing model to privacy policies from 164 popular online services,demonstrating the effectiveness of scaling GKC-CI annotation for dataexploration. We make all annotated policies as well as the training data andscripts needed to fine-tune our best-performing model publicly available forfuture research.</div></details><blockquote><p><strong><em>2023-11-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2311.00984v1><strong>Inclusiveness Matters: A Large-Scale Analysis of User Feedback</strong></a></p><p><em>Nowshin Nawar Arony, Ze Shi Li, Bowen Xu, Daniela Damian</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In an era of rapidly expanding software usage, catering to the diverse needsof users from various backgrounds has become a critical challenge.Inclusiveness, representing a core human value, is frequently overlooked duringsoftware development, leading to user dissatisfaction. Users often engage indiscourse on online platforms where they indicate their concerns. In thisstudy, we leverage user feedback from three popular online sources, Reddit,Google Play Store, and Twitter, for 50 of the most popular apps in the world toreveal the inclusiveness-related concerns from end users. Using aSocio-Technical Grounded Theory approach, we analyzed 23,107 posts across thethree sources and identified 1,211 inclusiveness related posts. We organize ourempirical results in a taxonomy for inclusiveness comprising 6 majorcategories: Fairness, Technology, Privacy, Demography, Usability, and OtherHuman Values. To explore automated support to identifying inclusiveness-relatedposts, we experimented with five state-of-the-art pre-trained large languagemodels (LLMs) and found that these models&rsquo; effectiveness is high and yet varieddepending on the data source. GPT-2 performed best on Reddit, BERT on theGoogle Play Store, and BART on Twitter. Our study provides an in-depth view ofinclusiveness-related user feedback from most popular apps and online sources.We provide implications and recommendations that can be used to bridge the gapbetween user expectations and software so that software developers can resonatewith the varied and evolving needs of the wide spectrum of users.</div></details><blockquote><p><strong><em>2023-11-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.05197v3><strong>Multi-step Jailbreaking Privacy Attacks on ChatGPT</strong></a></p><p><em>Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rapid progress of large language models (LLMs), many downstream NLPtasks can be well solved given appropriate prompts. Though model developers andresearchers work hard on dialog safety to avoid generating harmful content fromLLMs, it is still challenging to steer AI-generated content (AIGC) for thehuman good. As powerful LLMs are devouring existing text data from variousdomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whetherthe private information is included in the training data and what privacythreats can these LLMs and their downstream applications bring. In this paper,we study the privacy threats from OpenAI&rsquo;s ChatGPT and the New Bing enhanced byChatGPT and show that application-integrated LLMs may cause new privacythreats. To this end, we conduct extensive experiments to support our claimsand discuss LLMs&rsquo; privacy implications.</div></details><p><a href=http://arxiv.org/abs/2311.00287v1><strong>Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models</strong></a></p><p><em>Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, Carl Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Clinical natural language processing requires methods that can addressdomain-specific challenges, such as complex medical terminology and clinicalcontexts. Recently, large language models (LLMs) have shown promise in thisdomain. Yet, their direct deployment can lead to privacy issues and areconstrained by resources. To address this challenge, we delve into syntheticclinical text generation using LLMs for clinical NLP tasks. We propose aninnovative, resource-efficient approach, ClinGen, which infuses knowledge intothe process. Our model involves clinical knowledge extraction andcontext-informed LLM prompting. Both clinical topics and writing styles aredrawn from external domain-specific knowledge graphs and LLMs to guide datageneration. Our extensive empirical study across 7 clinical NLP tasks and 16datasets reveals that ClinGen consistently enhances performance across varioustasks, effectively aligning the distribution of real datasets and significantlyenriching the diversity of generated training instances. We will publish ourcode and all the generated data in \url{https://github.com/ritaranx/ClinGen}.</div></details><blockquote><p><strong><em>2023-10-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.20150v1><strong>Unlearn What You Want to Forget: Efficient Unlearning for LLMs</strong></a></p><p><em>Jiaao Chen, Diyi Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have achieved significant progress frompre-training on and memorizing a wide range of textual data, however, thisprocess might suffer from privacy issues and violations of data protectionregulations. As a result, the ability to easily remove data related toindividual users from such models while not deteriorating their predictivequality after the removal becomes increasingly important. To address theseissues, in this work, we propose an efficient unlearning framework that couldefficiently update LLMs without having to retrain the whole model after dataremovals, by introducing lightweight unlearning layers learned with a selectiveteacher-student objective into the transformers. In addition, we introduce afusion mechanism to effectively combine different unlearning layers that learnsto forget different sets of data to handle a sequence of forgetting operations.Experiments on classification and generation tasks demonstrate theeffectiveness of our proposed methods compared to the state-of-the-artbaselines.</div></details><p><a href=http://arxiv.org/abs/2310.20111v1><strong>Making Large Language Models Better Data Creators</strong></a></p><p><em>Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although large language models (LLMs) have advanced the state-of-the-art inNLP significantly, deploying them for downstream applications is stillchallenging due to cost, responsiveness, control, or concerns around privacyand security. As such, trainable models are still the preferred option in somecases. However, these models still require human-labeled data for optimalperformance, which is expensive and time-consuming to obtain. In order toaddress this issue, several techniques to reduce human effort involve labelingor generating data using LLMs. Although these methods are effective for certainapplications, in practice they encounter difficulties in real-world scenarios.Labeling data requires careful data selection, while generating datanecessitates task-specific prompt engineering. In this paper, we propose aunified data creation pipeline that requires only a single formatting example,and which is applicable to a broad range of tasks, including traditionallyproblematic ones with semantically devoid label spaces. In our experiments wedemonstrate that instruction-following LLMs are highly cost-effective datacreators, and that models trained with these data exhibit performance betterthan those trained with human-labeled data (by up to 17.5%) onout-of-distribution evaluation, while maintaining comparable performance onin-distribution tasks. These results have important implications for therobustness of NLP systems deployed in the real-world.</div></details><blockquote><p><strong><em>2023-10-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.18395v2><strong>Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks</strong></a></p><p><em>Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have shown promising performance inknowledge-intensive reasoning tasks that require a compound understanding ofknowledge. However, deployment of the LLMs in real-world applications can bechallenging due to their high computational requirements and concerns on dataprivacy. Previous studies have focused on building task-specific small LanguageModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,these approaches are ill-suited for knowledge-intensive reasoning tasks due tothe limited capacity of small LMs in memorizing the knowledge required.Motivated by our theoretical analysis on memorization, we proposeKnowledge-Augmented Reasoning Distillation (KARD), a novel method thatfine-tunes small LMs to generate rationales obtained from LLMs with augmentedknowledge retrieved from an external knowledge base. Moreover, we furtherpropose a neural reranker to obtain documents relevant to rationale generation.We empirically show that KARD significantly improves the performance of smallT5 and GPT models on the challenging knowledge-intensive reasoning datasets,namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the250M T5 models achieve superior performance against the fine-tuned 3B models,having 12 times larger parameters, on both MedQA-USMLE and StrategyQAbenchmarks.</div></details><blockquote><p><strong><em>2023-10-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.17884v1><strong>Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</strong></a></p><p><em>Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The interactive use of large language models (LLMs) in AI assistants (atwork, home, etc.) introduces a new set of inference-time privacy risks: LLMsare fed different types of information from multiple sources in their inputsand are expected to reason about what to share in their outputs, for whatpurpose and with whom, within a given context. In this work, we draw attentionto the highly critical yet overlooked notion of contextual privacy by proposingConfAIde, a benchmark designed to identify critical weaknesses in the privacyreasoning capabilities of instruction-tuned LLMs. Our experiments show thateven the most capable models such as GPT-4 and ChatGPT reveal privateinformation in contexts that humans would not, 39% and 57% of the time,respectively. This leakage persists even when we employ privacy-inducingprompts or chain-of-thought reasoning. Our work underscores the immediate needto explore novel inference-time privacy-preserving approaches, based onreasoning and theory of mind.</div></details><p><a href=http://arxiv.org/abs/2310.18431v1><strong>SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes</strong></a></p><p><em>Adam D. Lelkes, Eric Loreaux, Tal Schuster, Ming-Jun Chen, Alvin Rajkomar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Social and behavioral determinants of health (SDOH) play a significant rolein shaping health outcomes, and extracting these determinants from clinicalnotes is a first step to help healthcare providers systematically identifyopportunities to provide appropriate care and address disparities. Progress onusing NLP methods for this task has been hindered by the lack of high-qualitypublicly available labeled data, largely due to the privacy and regulatoryconstraints on the use of real patients&rsquo; information. This paper introduces anew dataset, SDOH-NLI, that is based on publicly available notes and which werelease publicly. We formulate SDOH extraction as a natural language inference(NLI) task, and provide binary textual entailment labels obtained from humanraters for a cross product of a set of social history snippets as premises andSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks inthat our premises and hypotheses are obtained independently. We evaluate both"off-the-shelf" entailment models as well as models fine-tuned on our data, andhighlight the ways in which our dataset appears more challenging than commonlyused NLI datasets.</div></details><blockquote><p><strong><em>2023-10-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.17752v1><strong>PockEngine: Sparse and Efficient Fine-tuning in a Pocket</strong></a></p><p><em>Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, Song Han</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: On-device learning and efficient fine-tuning enable continuous andprivacy-preserving customization (e.g., locally fine-tuning large languagemodels on personalized data). However, existing training frameworks aredesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) andlack the optimizations for learning on the edge, which faces challenges ofresource limitations and edge hardware diversity. We introduce PockEngine: atiny, sparse and efficient engine to enable fine-tuning on various edgedevices. PockEngine supports sparse backpropagation: it prunes the backwardgraph and sparsely updates the model with measured memory saving and latencyreduction while maintaining the model quality. Secondly, PockEngine iscompilation first: the entire training graph (including forward, backward andoptimization steps) is derived at compile-time, which reduces the runtimeoverhead and brings opportunities for graph transformations. PockEngine alsointegrates a rich set of training graph optimizations, thus can furtheraccelerate the training cost, including operator reordering and backendswitching. PockEngine supports diverse applications, frontends and hardwarebackends: it flexibly compiles and tunes models defined inPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. Weevaluated PockEngine on both vision models and large language models.PockEngine achieves up to 15 $\times$ speedup over off-the-shelf TensorFlow(Raspberry Pi), 5.6 $\times$ memory saving back-propagation (Jetson AGX Orin).Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orinat 550 tokens/s, 7.9$\times$ faster than the PyTorch.</div></details><p><a href=http://arxiv.org/abs/2306.08937v3><strong>DocumentNet: Bridging the Data Gap in Document Pre-Training</strong></a></p><p><em>Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G. Hauptmann, Hanjun Dai, Wei Wei</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER.</div></details><blockquote><p><strong><em>2023-10-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.16538v1><strong>FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning</strong></a></p><p><em>Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho D. Choi, Sung-Ju Lee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Psychiatrists diagnose mental disorders via the linguistic use of patients.Still, due to data privacy, existing passive mental health monitoring systemsuse alternative features such as activity, app usage, and location via mobiledevices. We propose FedTherapist, a mobile mental health monitoring system thatutilizes continuous speech and keyboard input in a privacy-preserving way viafederated learning. We explore multiple model designs by comparing theirperformance and overhead for FedTherapist to overcome the complex nature ofon-device language model training on smartphones. We further propose aContext-Aware Language Learning (CALL) methodology to effectively utilizesmartphones&rsquo; large and noisy text for mental health signal sensing. OurIRB-approved evaluation of the prediction of self-reported depression, stress,anxiety, and mood from 46 participants shows higher accuracy of FedTherapistcompared with the performance with non-language features, achieving 0.15 AUROCimprovement and 8.21% MAE reduction.</div></details><p><a href=http://arxiv.org/abs/2310.16960v1><strong>Privately Aligning Language Models with Reinforcement Learning</strong></a></p><p><em>Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Positioned between pre-training and user deployment, aligning large languagemodels (LLMs) through reinforcement learning (RL) has emerged as a prevailingstrategy for training instruction following-models such as ChatGPT. In thiswork, we initiate the study of privacy-preserving alignment of LLMs throughDifferential Privacy (DP) in conjunction with RL. Following the influentialwork of Ziegler et al. (2020), we study two dominant paradigms: (i) alignmentvia RL without human in the loop (e.g., positive review generation) and (ii)alignment via RL from human feedback (RLHF) (e.g., summarization in ahuman-preferred way). We give a new DP framework to achieve alignment via RL,and prove its correctness. Our experimental results validate the effectivenessof our approach, offering competitive utility while ensuring strong privacyprotections.</div></details><p><a href=http://arxiv.org/abs/2310.16340v1><strong>RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models</strong></a></p><p><em>Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language model (LLM) applications in cloud root cause analysis (RCA)have been actively explored recently. However, current methods are stillreliant on manual workflow settings and do not unleash LLMs&rsquo; decision-makingand environment interaction capabilities. We present RCAgent, a tool-augmentedLLM autonomous agent framework for practical and privacy-aware industrial RCAusage. Running on an internally deployed model rather than GPT families,RCAgent is capable of free-form data collection and comprehensive analysis withtools. Our framework combines a variety of enhancements, including a uniqueSelf-Consistency for action trajectories, and a suite of methods for contextmanagement, stabilization, and importing domain knowledge. Our experiments showRCAgent&rsquo;s evident and consistent superiority over ReAct across all aspects ofRCA &ndash; predicting root causes, solutions, evidence, and responsibilities &ndash; andtasks covered or uncovered by current rules, as validated by both automatedmetrics and human evaluations. Furthermore, RCAgent has already been integratedinto the diagnosis and issue discovery workflow of the Real-time ComputePlatform for Apache Flink of Alibaba Cloud.</div></details><blockquote><p><strong><em>2023-10-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.15469v1><strong>The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks</strong></a></p><p><em>Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, Haixu Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The era post-2018 marked the advent of Large Language Models (LLMs), withinnovations such as OpenAI&rsquo;s ChatGPT showcasing prodigious linguistic prowess.As the industry galloped toward augmenting model parameters and capitalizing onvast swaths of human language data, security and privacy challenges alsoemerged. Foremost among these is the potential inadvertent accrual of PersonalIdentifiable Information (PII) during web-based data acquisition, posing risksof unintended PII disclosure. While strategies like RLHF during training andCatastrophic Forgetting have been marshaled to control the risk of privacyinfringements, recent advancements in LLMs, epitomized by OpenAI&rsquo;s fine-tuninginterface for GPT-3.5, have reignited concerns. One may ask: can thefine-tuning of LLMs precipitate the leakage of personal information embeddedwithin training datasets? This paper reports the first endeavor to seek theanswer to the question, particularly our discovery of a new LLM exploitationavenue, called the Janus attack. In the attack, one can construct a PIIassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,to potentially reinstate and reveal concealed PIIs. Our findings indicate that,with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition frombeing impermeable to PII extraction to a state where they divulge a substantialproportion of concealed PII. This research, through its deep dive into theJanus attack vector, underscores the imperative of navigating the intricateinterplay between LLM utility and privacy preservation.</div></details><p><a href=http://arxiv.org/abs/2310.18362v1><strong>SoK: Memorization in General-Purpose Large Language Models</strong></a></p><p><em>Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, Robert West</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are advancing at a remarkable pace, with myriadapplications under development. Unlike most earlier machine learning models,they are no longer built for one specific application but are designed to excelin a wide range of tasks. A major part of this success is due to their hugetraining datasets and the unprecedented number of model parameters, which allowthem to memorize large amounts of information contained in the training data.This memorization goes beyond mere language, and encompasses information onlypresent in a few documents. This is often desirable since it is necessary forperforming tasks such as question answering, and therefore an important part oflearning, but also brings a whole array of issues, from privacy and security tocopyright and beyond. LLMs can memorize short secrets in the training data, butcan also memorize concepts like facts or writing styles that can be expressedin text in many different ways. We propose a taxonomy for memorization in LLMsthat covers verbatim text, facts, ideas and algorithms, writing styles,distributional properties, and alignment goals. We describe the implications ofeach type of memorization - both positive and negative - for model performance,privacy, security and confidentiality, copyright, and auditing, and ways todetect and prevent memorization. We further highlight the challenges that arisefrom the predominant way of defining memorization with respect to modelbehavior instead of model weights, due to LLM-specific phenomena such asreasoning capabilities or differences between decoding algorithms. Throughoutthe paper, we describe potential risks and opportunities arising frommemorization in LLMs that we hope will motivate new research directions.</div></details><p><a href=http://arxiv.org/abs/2310.15477v1><strong>CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model</strong></a></p><p><em>Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Instruction tuning has recently been recognized as an effective way ofaligning Large Language Models (LLMs) to enhance their generalization abilityacross various tasks. However, when tuning publicly accessible, centralizedLLMs with private instruction data, privacy concerns are inevitable. Whiledirect transfer of parameterized modules between models is a plausible approachto address this, its implications and effectiveness need further exploration.This paper focuses on Offsite-Tuning (OFT), a representative technique thattransfers transformer blocks between centralized LLMs and downstream emulators.Given the limited understanding of the underlying mechanism of OFT, we performan empirical analysis on LLMs from the perspectives of representation andfunctional similarity. Interestingly, our findings reveal a unique modularstructure within the layers of LLMs that appears to emerge as the model sizeexpands. Simultaneously, we note subtle but potentially significant changes inrepresentation and intermediate predictions across the layers. Inspired bythese observations, we propose CRaSh, involving Clustering, Removing, andSharing, a training-free strategy to derive improved emulators from LLMs. CRaShsignificantly boosts performance of OFT with billions of parameters.Furthermore, we investigate the optimal solutions yielded by fine-tuning withand without full model through the lens of loss landscape. Our findingsdemonstrate a linear connectivity among these optima falling over the samebasin, thereby highlighting the effectiveness of CRaSh and OFT. The source codeis publicly available at <a href=https://github.com/TsinghuaC3I/CRaSh>https://github.com/TsinghuaC3I/CRaSh</a>.</div></details><blockquote><p><strong><em>2023-10-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.14651v1><strong>$Œõ$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative AI</strong></a></p><p><em>Shoki Ohta, Takayuki Nishio</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the wake of the burgeoning expansion of generative artificial intelligence(AI) services, the computational demands inherent to these technologiesfrequently necessitate cloud-powered computational offloading, particularly forresource-constrained mobile devices. These services commonly employ prompts tosteer the generative process, and both the prompts and the resultant content,such as text and images, may harbor privacy-sensitive or confidentialinformation, thereby elevating security and privacy risks. To mitigate theseconcerns, we introduce $\Lambda$-Split, a split computing framework tofacilitate computational offloading while simultaneously fortifying dataprivacy against risks such as eavesdropping and unauthorized access. In$\Lambda$-Split, a generative model, usually a deep neural network (DNN), ispartitioned into three sub-models and distributed across the user&rsquo;s localdevice and a cloud server: the input-side and output-side sub-models areallocated to the local, while the intermediate, computationally-intensivesub-model resides on the cloud server. This architecture ensures that only thehidden layer outputs are transmitted, thereby preventing the externaltransmission of privacy-sensitive raw input and output data. Given theblack-box nature of DNNs, estimating the original input or output fromintercepted hidden layer outputs poses a significant challenge for maliciouseavesdroppers. Moreover, $\Lambda$-Split is orthogonal to traditionalencryption-based security mechanisms, offering enhanced security when deployedin conjunction. We empirically validate the efficacy of the $\Lambda$-Splitframework using Llama 2 and Stable Diffusion XL, representative large languageand diffusion models developed by Meta and Stability AI, respectively. Our$\Lambda$-Split implementation is publicly accessible athttps://github.com/nishio-laboratory/lambda_split.</div></details><p><a href=http://arxiv.org/abs/2310.18355v1><strong>Health Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model</strong></a></p><p><em>Yohn Jairo Parra Bautista, Vinicious Lima, Carlos Theran, Richard Alo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Health disparities are differences in health outcomes and access tohealthcare between different groups, including racial and ethnic minorities,low-income people, and rural residents. An artificial intelligence (AI) programcalled large language models (LLMs) can understand and generate human language,improving health communication and reducing health disparities. There are manychallenges in using LLMs in human-doctor interaction, including the need fordiverse and representative data, privacy concerns, and collaboration betweenhealthcare providers and technology experts. We introduce the comparativeinvestigation of domain-specific large language models such as SciBERT with amulti-purpose LLMs BERT. We used cosine similarity to analyze text queriesabout health disparities in exam rooms when factors such as race are usedalone. Using text queries, SciBERT fails when it doesn&rsquo;t differentiate betweenqueries text: &ldquo;race&rdquo; alone and &ldquo;perpetuates health disparities.&rdquo; We believeclinicians can use generative AI to create a draft response when communicatingasynchronously with patients. However, careful attention must be paid to ensurethey are developed and implemented ethically and equitably.</div></details><p><a href=http://arxiv.org/abs/2305.14536v2><strong>MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems</strong></a></p><p><em>Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: While automatic dialogue tutors hold great potential in making educationpersonalized and more accessible, research on such systems has been hampered bya lack of sufficiently large and high-quality datasets. Collecting suchdatasets remains challenging, as recording tutoring sessions raises privacyconcerns and crowdsourcing leads to insufficient data quality. To address this,we propose a framework to generate such dialogues by pairing human teacherswith a Large Language Model (LLM) prompted to represent common student errors.We describe how we use this framework to collect MathDial, a dataset of 3kone-to-one teacher-student tutoring dialogues grounded in multi-step mathreasoning problems. While models like GPT-3 are good problem solvers, they failat tutoring because they generate factually incorrect feedback or are prone torevealing solutions to students too early. To overcome this, we let teachersprovide learning opportunities to students by guiding them using variousscaffolding questions according to a taxonomy of teacher moves. We demonstrateMathDial and its extensive annotations can be used to finetune models to bemore effective tutors (and not just solvers). We confirm this by automatic andhuman evaluation, notably in an interactive setting that measures the trade-offbetween student solving success and telling solutions. The dataset is releasedpublicly.</div></details><p><a href=http://arxiv.org/abs/2310.14970v1><strong>Towards LLM-driven Dialogue State Tracking</strong></a></p><p><em>Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, Xiao-Ming Wu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Dialogue State Tracking (DST) is of paramount importance in ensuring accuratetracking of user goals and system actions within task-oriented dialoguesystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPThas sparked considerable interest in assessing their efficacy across diverseapplications. In this study, we conduct an initial examination of ChatGPT&rsquo;scapabilities in DST. Our evaluation uncovers the exceptional performance ofChatGPT in this task, offering valuable insights to researchers regarding itscapabilities and providing useful directions for designing and enhancingdialogue systems. Despite its impressive performance, ChatGPT has significantlimitations including its closed-source nature, request restrictions, raisingdata privacy concerns, and lacking local deployment capabilities. To addressthese concerns, we present LDST, an LLM-driven DST framework based on smaller,open-source foundation models. By utilizing a novel domain-slot instructiontuning method, LDST achieves performance on par with ChatGPT. Comprehensiveevaluations across three distinct experimental settings, we find that LDSTexhibits remarkable performance improvements in both zero-shot and few-shotsetting compared to previous SOTA methods. The source code is provided forreproducibility.</div></details><p><a href=http://arxiv.org/abs/2310.15007v1><strong>Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models</strong></a></p><p><em>Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With large language models (LLMs) poised to become embedded in our dailylives, questions are starting to be raised about the dataset(s) they learnedfrom. These questions range from potential bias or misinformation LLMs couldretain from their training data to questions of copyright and fair use ofhuman-generated text. However, while these questions emerge, developers of therecent state-of-the-art LLMs become increasingly reluctant to disclose detailson their training corpus. We here introduce the task of document-levelmembership inference for real-world LLMs, i.e. inferring whether the LLM hasseen a given document during training or not. First, we propose a procedure forthe development and evaluation of document-level membership inference for LLMsby leveraging commonly used data sources for training and the model releasedate. We then propose a practical, black-box method to predict document-levelmembership and instantiate it on OpenLLaMA-7B with both books and academicpapers. We show our methodology to perform very well, reaching an impressiveAUC of 0.856 for books and 0.678 for papers. We then show our approach tooutperform the sentence-level membership inference attacks used in the privacyliterature for the document-level membership task. We finally evaluate whethersmaller models might be less sensitive to document-level inference and showOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.Taken together, our results show that accurate document-level membership can beinferred for LLMs, increasing the transparency of technology poised to changeour lives.</div></details><blockquote><p><strong><em>2023-10-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.13291v1><strong>Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks</strong></a></p><p><em>Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan Kulkarni, Xia Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models have revolutionized the field of NLP by achievingstate-of-the-art performance on various tasks. However, there is a concern thatthese models may disclose information in the training data. In this study, wefocus on the summarization task and investigate the membership inference (MI)attack: given a sample and black-box access to a model&rsquo;s API, it is possible todetermine if the sample was part of the training data. We exploit textsimilarity and the model&rsquo;s resistance to document modifications as potential MIsignals and evaluate their effectiveness on widely used datasets. Our resultsdemonstrate that summarization models are at risk of exposing data membership,even in cases where the reference summary is not available. Furthermore, wediscuss several safeguards for training summarization models to protect againstMI attacks and discuss the inherent trade-off between privacy and utility.</div></details><p><a href=http://arxiv.org/abs/2310.13315v1><strong>Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models</strong></a></p><p><em>Miaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Quantization is a promising approach for reducing memory overhead andaccelerating inference, especially in large pre-trained language model (PLM)scenarios. While having no access to original training data due to security andprivacy concerns has emerged the demand for zero-shot quantization. Most of thecutting-edge zero-shot quantization methods primarily 1) apply to computervision tasks, and 2) neglect of overfitting problem in the generativeadversarial learning process, leading to sub-optimal performance. Motivated bythis, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)framework for the zero-shot quantization of various PLMs. The key algorithm insolving ZSAQ is the SAM-SGA optimization, which aims to improve thequantization accuracy and model generalization via optimizing a minimaxproblem. We theoretically prove the convergence rate for the minimaxoptimization problem and this result can be applied to other nonconvex-PLminimax optimization frameworks. Extensive experiments on 11 tasks demonstratethat our method brings consistent and significant performance gains on bothdiscriminative and generative PLMs, i.e., up to +6.98 average score.Furthermore, we empirically validate that our method can effectively improvethe model generalization.</div></details><blockquote><p><strong><em>2023-10-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.12523v1><strong>Privacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework</strong></a></p><p><em>Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The generative Artificial Intelligence (AI) tools based on Large LanguageModels (LLMs) use billions of parameters to extensively analyse large datasetsand extract critical private information such as, context, specific details,identifying information etc. This have raised serious threats to user privacyand reluctance to use such tools. This article proposes the conceptual modelcalled PrivChatGPT, a privacy-preserving model for LLMs that consists of twomain components i.e., preserving user privacy during the datacuration/pre-processing together with preserving private context and theprivate training process for large-scale data. To demonstrate itsapplicability, we show how a private mechanism could be integrated into theexisting model for training LLMs to protect user privacy; specifically, weemployed differential privacy and private training using Reinforcement Learning(RL). We measure the privacy loss and evaluate the measure of uncertainty orrandomness once differential privacy is applied. It further recursivelyevaluates the level of privacy guarantees and the measure of uncertainty ofpublic database and resources, during each update when new information is addedfor training purposes. To critically evaluate the use of differential privacyfor private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,private information retrieval, randomisation, for various performance measuressuch as the model performance and accuracy, computational complexity, privacyvs. utility etc. We conclude that differential privacy, randomisation, andobfuscation can impact utility and performance of trained models, conversely,the use of ToR, Blockchain, and PIR may introduce additional computationalcomplexity and high training latency. We believe that the proposed model couldbe used as a benchmark for proposing privacy preserving LLMs for generative AItools.</div></details><p><a href=http://arxiv.org/abs/2310.12746v1><strong>TabuLa: Harnessing Language Models for Tabular Data Synthesis</strong></a></p><p><em>Zilong Zhao, Robert Birke, Lydia Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Given the ubiquitous use of tabular data in industries and the growingconcerns in data privacy and security, tabular data synthesis emerges as acritical research area. The recent state-of-the-art methods show that largelanguage models (LLMs) can be adopted to generate realistic tabular data. AsLLMs pre-process tabular data as full text, they have the advantage of avoidingthe curse of dimensionality associated with one-hot encoding high-dimensionaldata. However, their long training time and limited re-usability on new tasksprevent them from replacing exiting tabular generative models. In this paper,we propose Tabula, a tabular data synthesizer based on the language modelstructure. Through Tabula, we demonstrate the inherent limitation of employingpre-trained language models designed for natural language processing (NLP) inthe context of tabular data synthesis. Our investigation delves into thedevelopment of a dedicated foundational model tailored specifically for tabulardata synthesis. Additionally, we propose a token sequence compression strategyto significantly reduce training time while preserving the quality of syntheticdata. Extensive experiments on six datasets demonstrate that using a languagemodel structure without loading the well-trained model weights yields a betterstarting model for tabular data synthesis. Moreover, the Tabula model,previously trained on other tabular data, serves as an excellent foundationmodel for new tabular data synthesis tasks. Additionally, the token sequencecompression method substantially reduces the model&rsquo;s training time. Resultsshow that Tabula averagely reduces 46.2% training time per epoch comparing tocurrent LLMs-based state-of-the-art algorithm and consistently achieves evenhigher synthetic data utility.</div></details><blockquote><p><strong><em>2023-10-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.11397v1><strong>Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</strong></a></p><p><em>Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are powerful tools for natural languageprocessing, enabling novel applications and user experiences. However, toachieve optimal performance, LLMs often require adaptation with private data,which poses privacy and security challenges. Several techniques have beenproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparativeprivacy and security properties have not been systematically investigated. Inthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICLagainst three types of well-established attacks: membership inference, whichexposes data leakage (privacy); backdoor, which injects malicious behavior(security); and model stealing, which can violate intellectual property(privacy and security). Our results show that there is no silver bullet forprivacy and security in LLM adaptation and each technique has differentstrengths and weaknesses.</div></details><p><a href=http://arxiv.org/abs/2306.10070v2><strong>Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health</strong></a></p><p><em>Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C. Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, Zhiyong Lu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: ChatGPT has drawn considerable attention from both the general public anddomain experts with its remarkable text generation capabilities. This hassubsequently led to the emergence of diverse applications in the field ofbiomedicine and health. In this work, we examine the diverse applications oflarge language models (LLMs), such as ChatGPT, in biomedicine and health.Specifically we explore the areas of biomedical information retrieval, questionanswering, medical text summarization, information extraction, and medicaleducation, and investigate whether LLMs possess the transformative power torevolutionize these tasks or whether the distinct complexities of biomedicaldomain presents unique challenges. Following an extensive literature survey, wefind that significant advances have been made in the field of text generationtasks, surpassing the previous state-of-the-art methods. For otherapplications, the advances have been modest. Overall, LLMs have not yetrevolutionized biomedicine, but recent rapid progress indicates that suchmethods hold great potential to provide valuable means for acceleratingdiscovery and improving health. We also find that the use of LLMs, likeChatGPT, in the fields of biomedicine and health entails various risks andchallenges, including fabricated information in its generated responses, aswell as legal and privacy concerns associated with sensitive patient data. Webelieve this survey can provide a comprehensive and timely overview tobiomedical researchers and healthcare practitioners on the opportunities andchallenges associated with using ChatGPT and other LLMs for transformingbiomedicine and health.</div></details><blockquote><p><strong><em>2023-10-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.10383v1><strong>Privacy in Large Language Models: Attacks, Defenses and Future Directions</strong></a></p><p><em>Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The advancement of large language models (LLMs) has significantly enhancedthe ability to effectively tackle various downstream NLP tasks and unify thesetasks into generative pipelines. On the one hand, powerful language models,trained on massive textual data, have brought unparalleled accessibility andusability for both models and users. On the other hand, unrestricted access tothese models can also introduce potential malicious and unintentional privacyrisks. Despite ongoing efforts to address the safety and privacy concernsassociated with LLMs, the problem remains unresolved. In this paper, we providea comprehensive analysis of the current privacy attacks targeting LLMs andcategorize them according to the adversary&rsquo;s assumed capabilities to shed lighton the potential vulnerabilities present in LLMs. Then, we present a detailedoverview of prominent defense strategies that have been developed to counterthese privacy attacks. Beyond existing works, we identify upcoming privacyconcerns as LLMs evolve. Lastly, we point out several potential avenues forfuture exploration.</div></details><p><a href=http://arxiv.org/abs/2310.10049v1><strong>FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</strong></a></p><p><em>Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, haveexhibited remarkable performances across various tasks in recent years.However, LLMs face two main challenges in real-world applications. Onechallenge is that training LLMs consumes vast computing resources, preventingLLMs from being adopted by small and medium-sized enterprises with limitedcomputing resources. Another is that training LLM requires a large amount ofhigh-quality data, which are often scattered among enterprises. To addressthese challenges, we propose FATE-LLM, an industrial-grade federated learningframework for large language models. FATE-LLM (1) facilitates federatedlearning for large language models (coined FedLLM); (2) promotes efficienttraining of FedLLM using parameter-efficient fine-tuning methods; (3) protectsthe intellectual property of LLMs; (4) preserves data privacy during trainingand inference through privacy-preserving mechanisms. We release the code ofFATE-LLM at <a href=https://github.com/FederatedAI/FATE-LLM>https://github.com/FederatedAI/FATE-LLM</a> to facilitate the researchof FedLLM and enable a broad range of industrial applications.</div></details><p><a href=http://arxiv.org/abs/2306.00980v3><strong>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</strong></a></p><p><em>Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Text-to-image diffusion models can create stunning images from naturallanguage descriptions that rival the work of professional artists andphotographers. However, these models are large, with complex networkarchitectures and tens of denoising iterations, making them computationallyexpensive and slow to run. As a result, high-end GPUs and cloud-based inferenceare required to run diffusion models at scale. This is costly and has privacyimplications, especially when user data is sent to a third party. To overcomethese challenges, we present a generic approach that, for the first time,unlocks running text-to-image diffusion models on mobile devices in less than$2$ seconds. We achieve so by introducing efficient network architecture andimproving step distillation. Specifically, we propose an efficient UNet byidentifying the redundancy of the original model and reducing the computationof the image decoder via data distillation. Further, we enhance the stepdistillation by exploring training strategies and introducing regularizationfrom classifier-free guidance. Our extensive experiments on MS-COCO show thatour model with $8$ denoising steps achieves better FID and CLIP scores thanStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creationby bringing powerful text-to-image diffusion models to the hands of users.</div></details><blockquote><p><strong><em>2023-10-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.07282v2><strong>An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT</strong></a></p><p><em>Shyni Sharaf, V. S. Anoop</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper conducts a comprehensive investigation into applying largelanguage models, particularly on BioBERT, in healthcare. It begins withthoroughly examining previous natural language processing (NLP) approaches inhealthcare, shedding light on the limitations and challenges these methodsface. Following that, this research explores the path that led to theincorporation of BioBERT into healthcare applications, highlighting itssuitability for addressing the specific requirements of tasks related tobiomedical text mining. The analysis outlines a systematic methodology forfine-tuning BioBERT to meet the unique needs of the healthcare domain. Thisapproach includes various components, including the gathering of data from awide range of healthcare sources, data annotation for tasks like identifyingmedical entities and categorizing them, and the application of specializedpreprocessing techniques tailored to handle the complexities found inbiomedical texts. Additionally, the paper covers aspects related to modelevaluation, with a focus on healthcare benchmarks and functions like processingof natural language in biomedical, question-answering, clinical documentclassification, and medical entity recognition. It explores techniques toimprove the model&rsquo;s interpretability and validates its performance compared toexisting healthcare-focused language models. The paper thoroughly examinesethical considerations, particularly patient privacy and data security. Ithighlights the benefits of incorporating BioBERT into healthcare contexts,including enhanced clinical decision support and more efficient informationretrieval. Nevertheless, it acknowledges the impediments and complexities ofthis integration, encompassing concerns regarding data privacy, transparency,resource-intensive requirements, and the necessity for model customization toalign with diverse healthcare domains.</div></details><blockquote><p><strong><em>2023-10-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.07298v1><strong>Beyond Memorization: Violating Privacy Via Inference with Large Language Models</strong></a></p><p><em>Robin Staab, Mark Vero, Mislav Balunoviƒá, Martin Vechev</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Current privacy research on large language models (LLMs) primarily focuses onthe issue of extracting memorized training data. At the same time, models&rsquo;inference capabilities have increased drastically. This raises the key questionof whether current LLMs could violate individuals&rsquo; privacy by inferringpersonal attributes from text given at inference time. In this work, we presentthe first comprehensive study on the capabilities of pretrained LLMs to inferpersonal attributes from text. We construct a dataset consisting of real Redditprofiles, and show that current LLMs can infer a wide range of personalattributes (e.g., location, income, sex), achieving up to $85%$ top-1 and$95.8%$ top-3 accuracy at a fraction of the cost ($100\times$) and time($240\times$) required by humans. As people increasingly interact withLLM-powered chatbots across all aspects of life, we also explore the emergingthreat of privacy-invasive chatbots trying to extract personal informationthrough seemingly benign questions. Finally, we show that common mitigations,i.e., text anonymization and model alignment, are currently ineffective atprotecting user privacy against LLM inference. Our findings highlight thatcurrent LLMs can infer personal data at a previously unattainable scale. In theabsence of working defenses, we advocate for a broader discussion around LLMprivacy implications beyond memorization, striving for a wider privacyprotection.</div></details><blockquote><p><strong><em>2023-10-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.03749v2><strong>Memorization of Named Entities in Fine-tuned BERT Models</strong></a></p><p><em>Andor Diera, Nicolas Lell, Aygul Garifullina, Ansgar Scherp</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy preserving deep learning is an emerging field in machine learningthat aims to mitigate the privacy risks in the use of deep neural networks. Onesuch risk is training data extraction from language models that have beentrained on datasets, which contain personal and privacy sensitive information.In our study, we investigate the extent of named entity memorization infine-tuned BERT models. We use single-label text classification asrepresentative downstream task and employ three different fine-tuning setups inour experiments, including one with Differentially Privacy (DP). We create alarge number of text samples from the fine-tuned BERT models utilizing a customsequential sampling strategy with two prompting strategies. We search in thesesamples for named entities and check if they are also present in thefine-tuning datasets. We experiment with two benchmark datasets in the domainsof emails and blogs. We show that the application of DP has a detrimentaleffect on the text generation capabilities of BERT. Furthermore, we show that afine-tuned BERT does not generate more named entities specific to thefine-tuning dataset than a BERT model that is pre-trained only. This suggeststhat BERT is unlikely to emit personal or privacy sensitive named entities.Overall, our results are important to understand to what extent BERT-basedservices are prone to training data extraction attacks.</div></details><p><a href=http://arxiv.org/abs/2308.15126v3><strong>Evaluation and Analysis of Hallucination in Large Vision-Language Models</strong></a></p><p><em>Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Vision-Language Models (LVLMs) have recently achieved remarkablesuccess. However, LVLMs are still plagued by the hallucination problem, whichlimits the practicality in many scenarios. Hallucination refers to theinformation of LVLMs&rsquo; responses that does not exist in the visual input, whichposes potential risks of substantial consequences. There has been limited workstudying hallucination evaluation in LVLMs. In this paper, we proposeHallucination Evaluation based on Large Language Models (HaELM), an LLM-basedhallucination evaluation framework. HaELM achieves an approximate 95%performance comparable to ChatGPT and has additional advantages including lowcost, reproducibility, privacy preservation and local deployment. Leveragingthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, weanalyze the factors contributing to hallucination in LVLMs and offer helpfulsuggestions to mitigate the hallucination problem. Our training data and humanannotation hallucination data will be made public soon.</div></details><p><a href=http://arxiv.org/abs/2310.06278v1><strong>BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models</strong></a></p><p><em>Haoxiang Luo, Jian Luo, Athanasios V. Vasilakos</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, artificial intelligence (AI) and machine learning (ML) arereshaping society&rsquo;s production methods and productivity, and also changing theparadigm of scientific research. Among them, the AI language model representedby ChatGPT has made great progress. Such large language models (LLMs) servepeople in the form of AI-generated content (AIGC) and are widely used inconsulting, healthcare, and education. However, it is difficult to guaranteethe authenticity and reliability of AIGC learning data. In addition, there arealso hidden dangers of privacy disclosure in distributed AI training. Moreover,the content generated by LLMs is difficult to identify and trace, and it isdifficult to cross-platform mutual recognition. The above information securityissues in the coming era of AI powered by LLMs will be infinitely amplified andaffect everyone&rsquo;s life. Therefore, we consider empowering LLMs using blockchaintechnology with superior security features to propose a vision for trusted AI.This paper mainly introduces the motivation and technical route of blockchainfor LLM (BC4LLM), including reliable learning corpus, secure training process,and identifiable generated content. Meanwhile, this paper also reviews thepotential applications and future challenges, especially in the frontiercommunication networks field, including network resource allocation, dynamicspectrum sharing, and semantic communication. Based on the above work combinedand the prospect of blockchain and LLMs, it is expected to help the earlyrealization of trusted AI and provide guidance for the academic community.</div></details><p><a href=http://arxiv.org/abs/2305.13257v3><strong>Watermarking Classification Dataset for Copyright Protection</strong></a></p><p><em>Yixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Substantial research works have shown that deep models, e.g., pre-trainedmodels, on the large corpus can learn universal language representations, whichare beneficial for downstream NLP tasks. However, these powerful models arealso vulnerable to various privacy attacks, while much sensitive informationexists in the training dataset. The attacker can easily steal sensitiveinformation from public models, e.g., individuals&rsquo; email addresses and phonenumbers. In an attempt to address these issues, particularly the unauthorizeduse of private data, we introduce a novel watermarking technique via abackdoor-based membership inference approach named TextMarker, which cansafeguard diverse forms of private information embedded in the training textdata. Specifically, TextMarker only requires data owners to mark a small numberof samples for data copyright protection under the black-box access assumptionto the target model. Through extensive evaluation, we demonstrate theeffectiveness of TextMarker on various real-world datasets, e.g., marking only0.1% of the training dataset is practically sufficient for effective membershipinference with negligible effect on model utility. We also discuss potentialcountermeasures and show that TextMarker is stealthy enough to bypass them.</div></details><blockquote><p><strong><em>2023-10-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.01002v3><strong>Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?</strong></a></p><p><em>Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao &lsquo;Kenneth&rsquo; Huang, Dongwon Lee</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved thegeneration of coherent sentences resembling human writing on a large scale,resulting in the creation of so-called deepfake texts. However, this progressposes security and privacy concerns, necessitating effective solutions fordistinguishing deepfake texts from human-written ones. Although prior worksstudied humans&rsquo; ability to detect deepfake texts, none has examined whether"collaboration" among humans improves the detection of deepfake texts. In thisstudy, to address this gap of understanding on deepfake texts, we conductedexperiments with two groups: (1) nonexpert individuals from the AMT platformand (2) writing experts from the Upwork platform. The results demonstrate thatcollaboration among humans can potentially improve the detection of deepfaketexts for both groups, increasing detection accuracies by 6.36% for non-expertsand 12.76% for experts, respectively, compared to individuals&rsquo; detectionaccuracies. We further analyze the explanations that humans used for detectinga piece of text as deepfake text, and find that the strongest indicator ofdeepfake texts is their lack of coherence and consistency. Our study providesuseful insights for future tools and framework designs to facilitate thecollaborative human detection of deepfake texts. The experiment datasets andAMT implementations are available at:https://github.com/huashen218/llm-deepfake-human-study.git</div></details><blockquote><p><strong><em>2023-10-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.04875v1><strong>Prompt-to-OS (P2OS): Revolutionizing Operating Systems and Human-Computer Interaction with Integrated AI Generative Models</strong></a></p><p><em>Gabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, Giovanni Trappolini</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we present a groundbreaking paradigm for human-computerinteraction that revolutionizes the traditional notion of an operating system. Within this innovative framework, user requests issued to the machine arehandled by an interconnected ecosystem of generative AI models that seamlesslyintegrate with or even replace traditional software applications. At the coreof this paradigm shift are large generative models, such as language anddiffusion models, which serve as the central interface between users andcomputers. This pioneering approach leverages the abilities of advancedlanguage models, empowering users to engage in natural language conversationswith their computing devices. Users can articulate their intentions, tasks, andinquiries directly to the system, eliminating the need for explicit commands orcomplex navigation. The language model comprehends and interprets the user&rsquo;sprompts, generating and displaying contextual and meaningful responses thatfacilitate seamless and intuitive interactions. This paradigm shift not only streamlines user interactions but also opens upnew possibilities for personalized experiences. Generative models can adapt toindividual preferences, learning from user input and continuously improvingtheir understanding and response generation. Furthermore, it enables enhancedaccessibility, as users can interact with the system using speech or text,accommodating diverse communication preferences. However, this visionary concept raises significant challenges, includingprivacy, security, trustability, and the ethical use of generative models.Robust safeguards must be in place to protect user data and prevent potentialmisuse or manipulation of the language model. While the full realization of this paradigm is still far from being achieved,this paper serves as a starting point for envisioning this transformativepotential.</div></details><blockquote><p><strong><em>2023-10-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.03971v1><strong>Quantized Transformer Language Model Implementations on Edge Devices</strong></a></p><p><em>Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, Soheil Salehi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale transformer-based models like the Bidirectional EncoderRepresentations from Transformers (BERT) are widely used for Natural LanguageProcessing (NLP) applications, wherein these models are initially pre-trainedwith a large corpus with millions of parameters and then fine-tuned for adownstream NLP task. One of the major limitations of these large-scale modelsis that they cannot be deployed on resource-constrained devices due to theirlarge model size and increased inference latency. In order to overcome theselimitations, such large-scale models can be converted to an optimizedFlatBuffer format, tailored for deployment on resource-constrained edgedevices. Herein, we evaluate the performance of such FlatBuffer transformedMobileBERT models on three different edge devices, fine-tuned for Reputationanalysis of English language tweets in the RepLab 2013 dataset. In addition,this study encompassed an evaluation of the deployed models, wherein theirlatency, performance, and resource efficiency were meticulously assessed. Ourexperiment results show that, compared to the original BERT large model, theconverted and quantized MobileBERT models have 160$\times$ smaller footprintsfor a 4.1% drop in accuracy while analyzing at least one tweet per second onedge devices. Furthermore, our study highlights the privacy-preserving aspectof TinyML systems as all data is processed locally within a serverlessenvironment.</div></details><blockquote><p><strong><em>2023-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.08247v2><strong>MedAlpaca &ndash; An Open-Source Collection of Medical Conversational AI Models and Training Data</strong></a></p><p><em>Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L√∂ser, Daniel Truhn, Keno K. Bressem</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As large language models (LLMs) like OpenAI&rsquo;s GPT series continue to makestrides, we witness the emergence of artificial intelligence applications in anever-expanding range of fields. In medicine, these LLMs hold considerablepromise for improving medical workflows, diagnostics, patient care, andeducation. Yet, there is an urgent need for open-source models that can bedeployed on-premises to safeguard patient privacy. In our work, we present aninnovative dataset consisting of over 160,000 entries, specifically crafted tofine-tune LLMs for effective medical applications. We investigate the impact offine-tuning these datasets on publicly accessible pre-trained LLMs, andsubsequently, we juxtapose the performance of pre-trained-only models againstthe fine-tuned models concerning the examinations that future medical doctorsmust pass to achieve certification.</div></details><p><a href=http://arxiv.org/abs/2310.03104v1><strong>DP-SGD for non-decomposable objective functions</strong></a></p><p><em>William Kong, Andr√©s Mu√±oz Medina, M√≥nica Ribero</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Unsupervised pre-training is a common step in developing computer visionmodels and large language models. In this setting, the absence of labelsrequires the use of similarity-based loss functions, such as contrastive loss,that favor minimizing the distance between similar inputs and maximizing thedistance between distinct inputs. As privacy concerns mount, training thesemodels using differential privacy has become more important. However, due tohow inputs are generated for these losses, one of their undesirable propertiesis that their $L_2$ sensitivity can grow with increasing batch size. Thisproperty is particularly disadvantageous for differentially private trainingmethods, such as DP-SGD. To overcome this issue, we develop a new DP-SGDvariant for similarity based loss functions &ndash; in particular the commonly usedcontrastive loss &ndash; that manipulates gradients of the objective function in anovel way to obtain a senstivity of the summed gradient that is $O(1)$ forbatch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, ourmethod&rsquo;s performance comes close to that of a non-private model and generallyoutperforms DP-SGD applied directly to the contrastive loss.</div></details><blockquote><p><strong><em>2023-10-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.02469v1><strong>Large Language Models Can Be Good Privacy Protection Learners</strong></a></p><p><em>Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The proliferation of Large Language Models (LLMs) has driven considerableinterest in fine-tuning them with domain-specific data to create specializedlanguage models. Nevertheless, such domain-specific fine-tuning data oftencontains sensitive personally identifiable information (PII). Directfine-tuning LLMs on this data without privacy protection poses a risk ofleakage. To address this challenge, we introduce Privacy Protection LanguageModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injectsdomain-specific knowledge while safeguarding data privacy. Our work offers atheoretical analysis for model design and delves into various techniques suchas corpus curation, penalty-based unlikelihood in training loss, andinstruction-based tuning, etc. Extensive experiments across diverse datasetsand scenarios demonstrate the effectiveness of our approaches. In particular,instruction tuning with both positive and negative examples, stands out as apromising method, effectively protecting private data while enhancing themodel&rsquo;s knowledge. Our work underscores the potential for Large Language Modelsas robust privacy protection learners.</div></details><p><a href=http://arxiv.org/abs/2310.02224v1><strong>Can Language Models be Instructed to Protect Personal Information?</strong></a></p><p><em>Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large multimodal language models have proven transformative in numerousapplications. However, these models have been shown to memorize and leakpre-training data, raising serious user privacy and information securityconcerns. While data leaks should be prevented, it is also crucial to examinethe trade-off between the privacy protection and model utility of proposedapproaches. In this paper, we introduce PrivQA &ndash; a multimodal benchmark toassess this privacy/utility trade-off when a model is instructed to protectspecific categories of personal information in a simulated scenario. We alsopropose a technique to iteratively self-moderate responses, which significantlyimproves privacy. However, through a series of red-teaming experiments, we findthat adversaries can also easily circumvent these protections with simplejailbreaking methods through textual and/or image inputs. We believe PrivQA hasthe potential to support the development of new models with improved privacyprotections, as well as the adversarial robustness of these protections. Werelease the entire PrivQA dataset at <a href=https://llm-access-control.github.io/>https://llm-access-control.github.io/</a>.</div></details><p><a href=http://arxiv.org/abs/2310.02431v1><strong>Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions</strong></a></p><p><em>Yufan Chen, Arjun Arunasalam, Z. Berkay Celik</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Users seek security & privacy (S&amp;P) advice from online resources, includingtrusted websites and content-sharing platforms. These resources help usersunderstand S&amp;P technologies and tools and suggest actionable strategies. LargeLanguage Models (LLMs) have recently emerged as trusted information sources.However, their accuracy and correctness have been called into question. Priorresearch has outlined the shortcomings of LLMs in answering multiple-choicequestions and user ability to inadvertently circumvent model restrictions(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliableS&amp;P advice is not well-explored. In this paper, we measure their ability torefute popular S&amp;P misconceptions that the general public holds. We first studyrecent academic literature to curate a dataset of over a hundred S&amp;P-relatedmisconceptions across six different topics. We then query two popular LLMs(Bard and ChatGPT) and develop a labeling guide to evaluate their responses tothese misconceptions. To comprehensively evaluate their responses, we furtherapply three strategies: query each misconception multiple times, generate andquery their paraphrases, and solicit source URLs of the responses. Both modelsdemonstrate, on average, a 21.3% non-negligible error rate, incorrectlysupporting popular S&amp;P misconceptions. The error rate increases to 32.6% whenwe repeatedly query LLMs with the same or paraphrased misconceptions. We alsoexpose that models may partially support a misconception or remainnoncommittal, refusing a firm stance on misconceptions. Our exploration ofinformation sources for responses revealed that LLMs are susceptible toproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point tounrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).</div></details><blockquote><p><strong><em>2023-10-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.01329v1><strong>BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models</strong></a></p><p><em>Qingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Retrieval augmentation addresses many critical problems in large languagemodels such as hallucination, staleness, and privacy leaks. However, runningretrieval-augmented language models (LMs) is slow and difficult to scale due toprocessing large amounts of retrieved text. We introduce binary tokenrepresentations (BTR), which use 1-bit vectors to precompute every token inpassages, significantly reducing computation during inference. Despite thepotential loss of accuracy, our new calibration techniques and trainingobjectives restore performance. Combined with offline and runtime compression,this only requires 127GB of disk space for encoding 3 billion tokens inWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTRaccelerates state-of-the-art inference by up to 4x and reduces storage by over100x while maintaining over 95% task performance.</div></details><p><a href=http://arxiv.org/abs/2310.01467v1><strong>FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models</strong></a></p><p><em>Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, Holger R. Roth</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained language models (PLM) have revolutionized the NLP landscape,achieving stellar performances across diverse tasks. These models, whilebenefiting from vast training data, often require fine-tuning on specific datato cater to distinct downstream tasks. However, this data adaptation processhas inherent security and privacy concerns, primarily when leveraginguser-generated, device-residing data. Federated learning (FL) provides asolution, allowing collaborative model fine-tuning without centralized datacollection. However, applying FL to finetune PLMs is hampered by challenges,including restricted model parameter access, high computational requirements,and communication overheads. This paper introduces Federated Black-box PromptTuning (FedBPT), a framework designed to address these challenges. FedBPT doesnot require the clients to access the model parameters. By focusing on trainingoptimal prompts and utilizing gradient-free optimization methods, FedBPTreduces the number of exchanged variables, boosts communication efficiency, andminimizes computational and storage costs. Experiments highlight theframework&rsquo;s ability to drastically cut communication and memory costs whilemaintaining competitive performance. Ultimately, FedBPT presents a promisingsolution for efficient, privacy-preserving fine-tuning of PLM in the age oflarge language models.</div></details><p><a href=http://arxiv.org/abs/2310.01304v1><strong>Coupling public and private gradient provably helps optimization</strong></a></p><p><em>Ruixuan Liu, Zhiqi Bu, Yu-xiang Wang, Sheng Zha, George Karypis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The success of large neural networks is crucially determined by theavailability of data. It has been observed that training only on a small amountof public data, or privately on the abundant private data can lead toundesirable degradation of accuracy. In this work, we leverage both private andpublic data to improve the optimization, by coupling their gradients via aweighted linear combination. We formulate an optimal solution for the optimalweight in the convex setting to indicate that the weighting coefficient shouldbe hyperparameter-dependent. Then, we prove the acceleration in the convergenceof non-convex loss and the effects of hyper-parameters such as privacy budget,number of iterations, batch size, and model size on the choice of the weightingcoefficient. We support our analysis with empirical experiments across languageand vision benchmarks, and provide a guideline for choosing the optimal weightof the gradient coupling.</div></details><p><a href=http://arxiv.org/abs/2310.01166v1><strong>Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models</strong></a></p><p><em>Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsum Kim, Donggyun Han, David Lo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Given large-scale source code datasets available in open-source projects andadvanced large language models, recent code models have been proposed toaddress a series of critical software engineering tasks, such as program repairand code completion. The training data of the code models come from varioussources, not only the publicly available source code, e.g., open-sourceprojects on GitHub but also the private data such as the confidential sourcecode from companies, which may contain sensitive information (for example, SSHkeys and personal information). As a result, the use of these code models mayraise new privacy concerns. In this paper, we focus on a critical yet not well-explored question on usingcode models: what is the risk of membership information leakage in code models?Membership information leakage refers to the risk that an attacker can inferwhether a given data point is included in (i.e., a member of) the trainingdata. To answer this question, we propose Gotcha, a novel membership inferenceattack method specifically for code models. We investigate the membershipleakage risk of code models. Our results reveal a worrying fact that the riskof membership leakage is high: although the previous attack methods are closeto random guessing, Gotcha can predict the data membership with a high truepositive rate of 0.95 and a low false positive rate of 0.10. We also show thatthe attacker&rsquo;s knowledge of the victim model (e.g., the model architecture andthe pre-training data) impacts the success rate of attacks. Further analysisdemonstrates that changing the decoding strategy can mitigate the risk ofmembership leakage. This study calls for more attention to understanding theprivacy of code models and developing more effective countermeasures againstsuch attacks.</div></details><blockquote><p><strong><em>2023-09-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.01639v2><strong>Privacy-Preserving In-Context Learning for Large Language Models</strong></a></p><p><em>Tong Wu, Ashwinee Panda, Jiachen T. Wang, Prateek Mittal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In-context learning (ICL) is an important capability of Large Language Models(LLMs), enabling these models to dynamically adapt based on specific,in-context exemplars, thereby improving accuracy and relevance. However, LLM&rsquo;sresponses may leak the sensitive private information contained in in-contextexemplars. To address this challenge, we propose Differentially PrivateIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. Thekey idea for DP-ICL paradigm is generating differentially private responsesthrough a noisy consensus among an ensemble of LLM&rsquo;s responses based ondisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiateseveral techniques showing how to privatize ICL for text classification andlanguage generation. We evaluate DP-ICL on four text classification benchmarksand two language generation tasks, and our empirical results show that DP-ICLachieves a strong utility-privacy tradeoff.</div></details><p><a href=http://arxiv.org/abs/2310.00272v1><strong>Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting</strong></a></p><p><em>Baphumelele Masikisiki, Vukosi Marivate, Yvette Hlope</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models, such as Generative Pre-trained Transformer 3 (aka.GPT-3), have been developed to understand language through the analysis ofextensive text data, allowing them to identify patterns and connections betweenwords. While LLMs have demonstrated impressive performance across varioustext-related tasks, they encounter challenges in tasks associated withreasoning. To address this challenge, Chain of Thought(CoT) prompting methodhas been proposed as a means to enhance LLMs&rsquo; proficiency in complex reasoningtasks like solving math word problems and answering questions based on logicalargumentative reasoning. The primary aim of this research is to assess how wellfour language models can grade reflective essays of third-year medicalstudents. The assessment will specifically target the evaluation of criticalthinking skills using CoT prompting. The research will provide the following contributions; to introduce andeducate on the process of instructing models to evaluate reflective essays froma dataset they have not been previously trained on; to illustrate the use ofCoT prompting as an instructional approach for training large models to carryout particular tasks. Our results suggest that among all the models, Llama-7bperforms the least effectively, displaying the highest mean squared error.Conversely, ChatGPT emerges as the superior model, boasting a higher Cohenkappa score value of 0.53. Lastly, it&rsquo;s important to note that the selectedmodels do prioritise user privacy by allowing users to delete their ownconducted conversations.</div></details><blockquote><p><strong><em>2023-09-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2310.01434v1><strong>Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile</strong></a></p><p><em>Samuel Carreira, Tom√°s Marques, Jos√© Ribeiro, Carlos Grilo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The field of Artificial Intelligence has witnessed remarkable progress inrecent years, especially with the emergence of powerful large language models(LLMs) based on the transformer architecture. Cloud-based LLMs, such asOpenAI&rsquo;s ChatGPT, offer impressive capabilities but come with concernsregarding latency and privacy due to network dependencies. This articlepresents an innovative approach to LLM inference, envisioning a future whereLLMs with billions of parameters can be executed directly on mobile deviceswithout network connectivity. The article showcases a fine-tuned GPT LLM with 3billion parameters that can operate smoothly on devices with as low as 4GB ofmemory. Through the integration of native code and model quantizationtechniques, the application not only serves as a general-purpose assistant butalso facilitates seamless mobile interactions with text-to-actions features.The article provides insights into the training pipeline, implementationdetails, test results, and future directions of on-device LLM inference. Thisbreakthrough technology opens up possibilities for empowering users withsophisticated AI capabilities while preserving their privacy and eliminatinglatency concerns.</div></details><blockquote><p><strong><em>2023-09-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.14726v1><strong>PLMM: Personal Large Models on Mobile Devices</strong></a></p><p><em>Yuanhao Gong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Inspired by Federated Learning, in this paper, we propose personal largemodels that are distilled from traditional large language models but moreadaptive to local users&rsquo; personal information such as education background andhobbies. We classify the large language models into three levels: the personallevel, expert level and traditional level. The personal level models areadaptive to users&rsquo; personal information. They encrypt the users&rsquo; input andprotect their privacy. The expert level models focus on merging specificknowledge such as finance, IT and art. The traditional models focus on theuniversal knowledge discovery and upgrading the expert models. In suchclassifications, the personal models directly interact with the user. For thewhole system, the personal models have users&rsquo; (encrypted) personal information.Moreover, such models must be small enough to be performed on personalcomputers or mobile devices. Finally, they also have to response in real-timefor better user experience and produce high quality results. The proposedpersonal large models can be applied in a wide range of applications such aslanguage and vision tasks.</div></details><blockquote><p><strong><em>2023-09-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.14510v1><strong>An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox</strong></a></p><p><em>Chaoran Chen, Weijun Li, Wenxin Song, Yanfang Ye, Yaxing Yao, Toby Jia-jun Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The &ldquo;privacy paradox&rdquo; describes the discrepancy between users&rsquo; privacyattitudes and their actual behaviors. Mitigating this discrepancy requiressolutions that account for both system opaqueness and users&rsquo; hesitations intesting different privacy settings due to fears of unintended data exposure. Weintroduce an empathy-based approach that allows users to experience how privacybehaviors may alter system outcomes in a risk-free sandbox environment from theperspective of artificially generated personas. To generate realistic personas,we introduce a novel pipeline that augments the outputs of large languagemodels using few-shot learning, contextualization, and chain of thoughts. Ourempirical studies demonstrated the adequate quality of generated personas andhighlighted the changes in privacy-related applications (e.g., onlineadvertising) caused by different personas. Furthermore, users demonstratedcognitive and emotional empathy towards the personas when interacting with oursandbox. We offered design implications for downstream applications inimproving user privacy literacy and promoting behavior changes.</div></details><blockquote><p><strong><em>2023-09-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.03941v3><strong>Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions</strong></a></p><p><em>Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The Right to be Forgotten (RTBF) was first established as the result of theruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz'alez, andwas later included as the Right to Erasure under the General Data ProtectionRegulation (GDPR) of European Union to allow individuals the right to requestpersonal data be deleted by organizations. Specifically for search engines,individuals can send requests to organizations to exclude their informationfrom the query results. It was a significant emergent right as the result ofthe evolution of technology. With the recent development of Large LanguageModels (LLMs) and their use in chatbots, LLM-enabled software systems havebecome popular. But they are not excluded from the RTBF. Compared with theindexing approach used by search engines, LLMs store, and process informationin a completely different way. This poses new challenges for compliance withthe RTBF. In this paper, we explore these challenges and provide our insightson how to implement technical solutions for the RTBF, including the use ofdifferential privacy, machine unlearning, model editing, and promptengineering. With the rapid advancement of AI and the increasing need ofregulating this powerful technology, learning from the case of RTBF can providevaluable lessons for technical practitioners, legal experts, organizations, andauthorities.</div></details><blockquote><p><strong><em>2023-09-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.11653v1><strong>&ldquo;It&rsquo;s a Fair Game&rsquo;&rsquo;, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</strong></a></p><p><em>Zhiping Zhang, Michelle Jia, Hao-Ping, Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The widespread use of Large Language Model (LLM)-based conversational agents(CAs), especially in high-stakes domains, raises many privacy concerns.Building ethical LLM-based CAs that respect user privacy requires an in-depthunderstanding of the privacy risks that concern users the most. However,existing research, primarily model-centered, does not provide insight intousers&rsquo; perspectives. To bridge this gap, we analyzed sensitive disclosures inreal-world ChatGPT conversations and conducted semi-structured interviews with19 LLM-based CA users. We found that users are constantly faced with trade-offsbetween privacy, utility, and convenience when using LLM-based CAs. However,users&rsquo; erroneous mental models and the dark patterns in system design limitedtheir awareness and comprehension of the privacy risks. Additionally, thehuman-like interactions encouraged more sensitive disclosures, whichcomplicated users&rsquo; ability to navigate the trade-offs. We discuss practicaldesign guidelines and the needs for paradigmatic shifts to protect the privacyof LLM-based CA users.</div></details><blockquote><p><strong><em>2023-09-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.10238v1><strong>PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models</strong></a></p><p><em>Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy policies serve as the primary conduit through which online serviceproviders inform users about their data collection and usage procedures.However, in a bid to be comprehensive and mitigate legal risks, these policydocuments are often quite verbose. In practical use, users tend to click theAgree button directly rather than reading them carefully. This practice exposesusers to risks of privacy leakage and legal issues. Recently, the advent ofLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened newpossibilities for text analysis, especially for lengthy documents like privacypolicies. In this study, we investigate a privacy policy text analysisframework PolicyGPT based on the LLM. This framework was tested using twodatasets. The first dataset comprises of privacy policies from 115 websites,which were meticulously annotated by legal experts, categorizing each segmentinto one of 10 classes. The second dataset consists of privacy policies from304 popular mobile applications, with each sentence manually annotated andclassified into one of another 10 categories. Under zero-shot learningconditions, PolicyGPT demonstrated robust performance. For the first dataset,it achieved an accuracy rate of 97%, while for the second dataset, it attainedan 87% accuracy rate, surpassing that of the baseline machine learning andneural network models.</div></details><p><a href=http://arxiv.org/abs/2309.10929v1><strong>Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training</strong></a></p><p><em>Ruiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this work, we introduce the concept of complex text style transfer tasks,and constructed complex text datasets based on two widely applicable scenarios.Our dataset is the first large-scale data set of its kind, with 700 rephrasedsentences and 1,000 sentences from the game Genshin Impact. While largelanguage models (LLM) have shown promise in complex text style transfer, theyhave drawbacks such as data privacy concerns, network instability, and highdeployment costs. To address these issues, we explore the effectiveness ofsmall models (less than T5-3B) with implicit style pre-training throughcontrastive learning. We also propose a method for automated evaluation of textgeneration quality based on alignment with human evaluations using ChatGPT.Finally, we compare our approach with existing methods and show that our modelachieves state-of-art performances of few-shot text style transfer models.</div></details><p><a href=http://arxiv.org/abs/2309.10254v1><strong>LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI&rsquo;s ChatGPT Plugins</strong></a></p><p><em>Umar Iqbal, Tadayoshi Kohno, Franziska Roesner</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language model (LLM) platforms, such as ChatGPT, have recently begunoffering a plugin ecosystem to interface with third-party services on theinternet. While these plugins extend the capabilities of LLM platforms, theyare developed by arbitrary third parties and thus cannot be implicitly trusted.Plugins also interface with LLM platforms and users using natural language,which can have imprecise interpretations. In this paper, we propose a frameworkthat lays a foundation for LLM platform designers to analyze and improve thesecurity, privacy, and safety of current and future plugin-integrated LLMplatforms. Our framework is a formulation of an attack taxonomy that isdeveloped by iteratively exploring how LLM platform stakeholders could leveragetheir capabilities and responsibilities to mount attacks against each other. Aspart of our iterative process, we apply our framework in the context ofOpenAI&rsquo;s plugin ecosystem. We uncover plugins that concretely demonstrate thepotential for the types of issues that we outline in our attack taxonomy. Weconclude by discussing novel challenges and by providing recommendations toimprove the security, privacy, and safety of present and future LLM-basedcomputing platforms.</div></details><p><a href=http://arxiv.org/abs/2210.00038v2><strong>Differentially Private Optimization on Large Model at Small Cost</strong></a></p><p><em>Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Differentially private (DP) optimization is the standard paradigm to learnlarge neural networks that are accurate and privacy-preserving. Thecomputational cost for DP deep learning, however, is notoriously heavy due tothe per-sample gradient clipping. Existing DP implementations are 2-1000X morecostly in time and space complexity than the standard (non-private) training.In this work, we develop a novel Book-Keeping (BK) technique that implementsexisting DP optimizers (thus achieving the same accuracy), with a substantialimprovement on the computational cost. Specifically, BK enables DP training onlarge models and high dimensional data to be roughly as fast and memory-savingas the standard training, whereas previous DP algorithms can be inefficient orincapable of training due to memory error. The computational advantage of BK issupported by the complexity analysis as well as extensive experiments on visionand language tasks. Our implementation achieves state-of-the-art (SOTA)accuracy with very small extra cost: on GPT2 and at almost the same memory cost(&lt;1% overhead), BK has 1.03X the time complexity of the standard training(0.83X training speed in practice), and 0.61X the time complexity of the mostefficient DP implementation (1.36X training speed in practice). We open-sourcethe codebase for the BK algorithm at the FastDP library(<a href=https://github.com/awslabs/fast-differential-privacy%29>https://github.com/awslabs/fast-differential-privacy)</a>.</div></details><p><a href=http://arxiv.org/abs/2309.06746v2><strong>DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass</strong></a></p><p><em>Minxin Du, Xiang Yue, Sherman S. M. Chow, Tianhao Wang, Chenyu Huang, Huan Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Differentially private stochastic gradient descent (DP-SGD) adds noise togradients in back-propagation, safeguarding training data from privacy leakage,particularly membership inference. It fails to cover (inference-time) threatslike embedding inversion and sensitive attribute inference. It is also costlyin storage and computation when used to fine-tune large pre-trained languagemodels (LMs). We propose DP-Forward, which directly perturbs embedding matrices in theforward pass of LMs. It satisfies stringent local DP requirements for trainingand inference data. To instantiate it using the smallest matrix-valued noise,we devise an analytic matrix Gaussian~mechanism (aMGM) by drawing possiblynon-i.i.d. noise from a matrix Gaussian distribution. We then investigateperturbing outputs from different hidden (sub-)layers of LMs with aMGM noises.Its utility on three typical tasks almost hits the non-private baseline andoutperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves3$\times$ time and memory costs compared to DP-SGD with the latest high-speedlibrary. It also reduces the average success rates of embedding inversion andsensitive attribute inference by up to 88pp and 41pp, respectively, whereasDP-SGD fails.</div></details><blockquote><p><strong><em>2023-09-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.09843v1><strong>Instruction-Following Speech Recognition</strong></a></p><p><em>Cheng-I Jeff Lai, Zhiyun Lu, Liangliang Cao, Ruoming Pang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Conventional end-to-end Automatic Speech Recognition (ASR) models primarilyfocus on exact transcription tasks, lacking flexibility for nuanced userinteractions. With the advent of Large Language Models (LLMs) in speechprocessing, more organic, text-prompt-based interactions have become possible.However, the mechanisms behind these models&rsquo; speech understanding and"reasoning" capabilities remain underexplored. To study this question from thedata perspective, we introduce instruction-following speech recognition,training a Listen-Attend-Spell model to understand and execute a diverse set offree-form text instructions. This enables a multitude of speech recognitiontasks &ndash; ranging from transcript manipulation to summarization &ndash; withoutrelying on predefined command sets. Remarkably, our model, trained from scratchon Librispeech, interprets and executes simple instructions without requiringLLMs or pre-trained speech modules. It also offers selective transcriptionoptions based on instructions like &ldquo;transcribe first half and then turn offlistening,&rdquo; providing an additional layer of privacy and safety compared toexisting LLMs. Our findings highlight the significant potential ofinstruction-following training to advance speech foundation models.</div></details><blockquote><p><strong><em>2023-09-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.09273v2><strong>Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning</strong></a></p><p><em>Miao Li, Wenhao Ding, Ding Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The prominence of embodied Artificial Intelligence (AI), which empowersrobots to navigate, perceive, and engage within virtual environments, hasattracted significant attention, owing to the remarkable advancements incomputer vision and large language models. Privacy emerges as a pivotal concernwithin the realm of embodied AI, as the robot accesses substantial personalinformation. However, the issue of privacy leakage in embodied AI tasks,particularly in relation to reinforcement learning algorithms, has not receivedadequate consideration in research. This paper aims to address this gap byproposing an attack on the value-based algorithm and the gradient-basedalgorithm, utilizing gradient inversion to reconstruct states, actions, andsupervision signals. The choice of using gradients for the attack is motivatedby the fact that commonly employed federated learning techniques solely utilizegradients computed based on private user data to optimize models, withoutstoring or transmitting the data to public servers. Nevertheless, thesegradients contain sufficient information to potentially expose private data. Tovalidate our approach, we conduct experiments on the AI2THOR simulator andevaluate our algorithm on active perception, a prevalent task in embodied AI.The experimental results demonstrate the effectiveness of our method insuccessfully reconstructing all information from the data across 120 roomlayouts.</div></details><blockquote><p><strong><em>2023-09-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.00964v2><strong>eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models</strong></a></p><p><em>Minsik Cho, Keivan A. Vahid, Qichen Fu, Saurabh Adya, Carlo C Del Mundo, Mohammad Rastegari, Devang Naik, Peter Zatloukal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Since Large Language Models or LLMs have demonstrated high-qualityperformance on many complex language tasks, there is a great interest inbringing these LLMs to mobile devices for faster responses and better privacyprotection. However, the size of LLMs (i.e., billions of parameters) requireshighly effective compression to fit into storage-limited devices. Among manycompression techniques, weight-clustering, a form of non-linear quantization,is one of the leading candidates for LLM compression, and supported by modernsmartphones. Yet, its training overhead is prohibitively significant for LLMfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shownthe state-of-the-art trade-off between compression ratio and accuracyregression, but its large memory complexity makes it nearly impossible to applyto train-time LLM compression. In this paper, we propose a memory-efficient DKMimplementation, eDKM powered by novel techniques to reduce the memory footprintof DKM by orders of magnitudes. For a given tensor to be saved on CPU for thebackward pass of DKM, we compressed the tensor by applying uniquification andsharding after checking if there is no duplicated tensor previously copied toCPU. Our experimental results demonstrate that \prjname can fine-tune andcompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) withthe Alpaca dataset by reducing the train-time memory footprint of a decoderlayer by 130$\times$, while delivering good accuracy on broader LLM benchmarks(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).</div></details><blockquote><p><strong><em>2023-09-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.04255v1><strong>LLMCad: Fast and Scalable On-device Large Language Model Inference</strong></a></p><p><em>Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, Xuanzhe Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Generative tasks, such as text generation and question answering, hold acrucial position in the realm of mobile applications. Due to their sensitivityto privacy concerns, there is a growing demand for their execution directly onmobile devices. Currently, the execution of these generative tasks heavilydepends on Large Language Models (LLMs). Nevertheless, the limited memorycapacity of these devices presents a formidable challenge to the scalability ofsuch models. In our research, we introduce LLMCad, an innovative on-device inferenceengine specifically designed for efficient generative Natural LanguageProcessing (NLP) tasks. The core idea behind LLMCad revolves around modelcollaboration: a compact LLM, residing in memory, takes charge of generatingthe most straightforward tokens, while a high-precision LLM steps in tovalidate these tokens and rectify any identified errors. LLMCad incorporatesthree novel techniques: (1) Instead of generating candidate tokens in asequential manner, LLMCad employs the smaller LLM to construct a token tree,encompassing a wider range of plausible token pathways. Subsequently, thelarger LLM can efficiently validate all of these pathways simultaneously. (2)It employs a self-adjusting fallback strategy, swiftly initiating theverification process whenever the smaller LLM generates an erroneous token. (3)To ensure a continuous flow of token generation, LLMCad speculatively generatestokens during the verification process by implementing a compute-IO pipeline.Through an extensive series of experiments, LLMCad showcases an impressivetoken generation speed, achieving rates up to 9.3x faster than existinginference engines.</div></details><blockquote><p><strong><em>2023-09-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.03748v1><strong>Enhancing Pipeline-Based Conversational Agents with Large Language Models</strong></a></p><p><em>Mina Foosherian, Hendrik Purwins, Purna Rathnayake, Touhidul Alam, Rui Teimao, Klaus-Dieter Thoben</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The latest advancements in AI and deep learning have led to a breakthrough inlarge language model (LLM)-based agents such as GPT-4. However, many commercialconversational agent development tools are pipeline-based and have limitationsin holding a human-like conversation. This paper investigates the capabilitiesof LLMs to enhance pipeline-based conversational agents during two phases: 1)in the design and development phase and 2) during operations. In 1) LLMs canaid in generating training data, extracting entities and synonyms,localization, and persona design. In 2) LLMs can assist in contextualization,intent classification to prevent conversational breakdown and handleout-of-scope questions, auto-correcting utterances, rephrasing responses,formulating disambiguation questions, summarization, and enabling closedquestion-answering capabilities. We conducted informal experiments with GPT-4in the private banking domain to demonstrate the scenarios above with apractical example. Companies may be hesitant to replace their pipeline-basedagents with LLMs entirely due to privacy concerns and the need for deepintegration within their existing ecosystems. A hybrid approach in which LLMs&rsquo;are integrated into the pipeline-based agents allows them to save time andcosts of building and running agents by capitalizing on the capabilities ofLLMs while retaining the integration and privacy safeguards of their existingsystems.</div></details><blockquote><p><strong><em>2023-09-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.00237v2><strong>Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes</strong></a></p><p><em>Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The development of large language models tailored for handling patients&rsquo;clinical notes is often hindered by the limited accessibility and usability ofthese notes due to strict privacy regulations. To address these challenges, wefirst create synthetic large-scale clinical notes using publicly available casereports extracted from biomedical literature. We then use these synthetic notesto train our specialized clinical large language model, Asclepius. WhileAsclepius is trained on synthetic data, we assess its potential performance inreal-world applications by evaluating it using real clinical notes. Webenchmark Asclepius against several other large language models, includingGPT-3.5-turbo and other open-source alternatives. To further validate ourapproach using synthetic notes, we also compare Asclepius with its variantstrained on real clinical notes. Our findings convincingly demonstrate thatsynthetic clinical notes can serve as viable substitutes for real ones whenconstructing high-performing clinical language models. This conclusion issupported by detailed evaluations conducted by both GPT-4 and medicalprofessionals. All resources including weights, codes, and data used in thedevelopment of Asclepius are made publicly accessible for future research.</div></details><p><a href=http://arxiv.org/abs/2309.03057v1><strong>Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection</strong></a></p><p><em>Yu Chen, Tingxin Li, Huiming Liu, Yang Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Numerous companies have started offering services based on large languagemodels (LLM), such as ChatGPT, which inevitably raises privacy concerns asusers&rsquo; prompts are exposed to the model provider. Previous research on securereasoning using multi-party computation (MPC) has proven to be impractical forLLM applications due to its time-consuming and communication-intensive nature.While lightweight anonymization techniques can protect private information inprompts through substitution or masking, they fail to recover sensitive datareplaced in the LLM-generated results. In this paper, we expand the applicationscenarios of anonymization techniques by training a small local model tode-anonymize the LLM&rsquo;s returned results with minimal computational overhead. Weintroduce the HaS framework, where &ldquo;H(ide)&rdquo; and &ldquo;S(eek)&rdquo; represent its two coreprocesses: hiding private entities for anonymization and seeking privateentities for de-anonymization, respectively. To quantitatively assess HaS&rsquo;sprivacy protection performance, we propose both black-box and white-boxadversarial models. Furthermore, we conduct experiments to evaluate HaS&rsquo;susability in translation and classification tasks. The experimental findingsdemonstrate that the HaS framework achieves an optimal balance between privacyprotection and utility.</div></details><p><a href=http://arxiv.org/abs/2309.03242v1><strong>Automated Bioinformatics Analysis via AutoBA</strong></a></p><p><em>Juexiao Zhou, Bin Zhang, Xiuying Chen, Haoyang Li, Xiaopeng Xu, Siyuan Chen, Xin Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the fast-growing and evolving omics data, the demand for streamlined andadaptable tools to handle the analysis continues to grow. In response to thisneed, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AIagent based on a large language model designed explicitly for conventionalomics data analysis. AutoBA simplifies the analytical process by requiringminimal user input while delivering detailed step-by-step plans for variousbioinformatics tasks. Through rigorous validation by expert bioinformaticians,AutoBA&rsquo;s robustness and adaptability are affirmed across a diverse range ofomics analysis cases, including whole genome sequencing (WGS), RNA sequencing(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA&rsquo;sunique capacity to self-design analysis processes based on input datavariations further underscores its versatility. Compared with onlinebioinformatic services, AutoBA deploys the analysis locally, preserving dataprivacy. Moreover, different from the predefined pipeline, AutoBA hasadaptability in sync with emerging bioinformatics tools. Overall, AutoBArepresents a convenient tool, offering robustness and adaptability for complexomics data analysis.</div></details><p><a href=http://arxiv.org/abs/2311.06251v1><strong>AI for Investment: A Platform Disruption</strong></a></p><p><em>Mohammad Rasouli, Ravi Chiruvolu, Ali Risheh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the investment landscape becoming more competitive, efficiently scalingdeal sourcing and improving deal insights have become a dominant strategy forfunds. While funds are already spending significant efforts on these two tasks,they cannot be scaled with traditional approaches; hence, there is a surge inautomating them. Many third party software providers have emerged recently toaddress this need with productivity solutions, but they fail due to a lack ofpersonalization for the fund, privacy constraints, and natural limits ofsoftware use cases. Therefore, most major funds and many smaller funds havestarted developing their in-house AI platforms: a game changer for theindustry. These platforms grow smarter by direct interactions with the fund andcan be used to provide personalized use cases. Recent developments in largelanguage models, e.g. ChatGPT, have provided an opportunity for other funds toalso develop their own AI platforms. While not having an AI platform now is nota competitive disadvantage, it will be in two years. Funds require a practicalplan and corresponding risk assessments for such AI platforms.</div></details><blockquote><p><strong><em>2023-09-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2309.01172v1><strong>FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs</strong></a></p><p><em>Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid growth of memory and computation requirements of large languagemodels (LLMs) has outpaced the development of hardware, hindering people wholack large-scale high-end GPUs from training or deploying LLMs. However,consumer-level GPUs, which constitute a larger market share, are typicallyoverlooked in LLM due to their weaker computing performance, smaller storagecapacity, and lower communication bandwidth. Additionally, users may haveprivacy concerns when interacting with remote LLMs. In this paper, we envisiona decentralized system unlocking the potential vast untapped consumer-levelGPUs in pre-training, inference and fine-tuning of LLMs with privacyprotection. However, this system faces critical challenges, including limitedCPU and GPU memory, low network bandwidth, the variability of peer and deviceheterogeneity. To address these challenges, our system design incorporates: 1)a broker with backup pool to implement dynamic join and quit of computingproviders; 2) task scheduling with hardware performance to improve systemefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) toachieve model and task universality; 4) abstracting intermediate representionand execution planes to ensure compatibility of various devices and deeplearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which aresignificantly more expensive.</div></details><blockquote><p><strong><em>2023-09-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.10802v3><strong>Combing for Credentials: Active Pattern Extraction from Smart Reply</strong></a></p><p><em>Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, David Evans</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, areoften fine-tuned to achieve state-of-the-art performance on a downstream task.One natural example is the ``Smart Reply&rsquo;&rsquo; application where a pre-trainedmodel is tuned to provide suggested responses for a given query message. Sincethe tuning data is often sensitive data such as emails or chat transcripts, itis important to understand and mitigate the risk that the model leaks itstuning data. We investigate potential information leakage vulnerabilities in atypical Smart Reply pipeline. We consider a realistic setting where theadversary can only interact with the underlying model through a front-endinterface that constrains what types of queries can be sent to the model.Previous attacks do not work in these settings, but require the ability to sendunconstrained queries directly to the model. Even when there are no constraintson the queries, previous attacks typically require thousands, or even millions,of queries to extract useful information, while our attacks can extractsensitive data in just a handful of queries. We introduce a new type of activeextraction attack that exploits canonical patterns in text containing sensitivedata. We show experimentally that it is possible for an adversary to extractsensitive user information present in the training data, even in realisticsettings where all interactions with the model must go through a front-end thatlimits the types of queries. We explore potential mitigation strategies anddemonstrate empirically how differential privacy appears to be a reasonablyeffective defense mechanism to such pattern extraction attacks.</div></details><blockquote><p><strong><em>2023-09-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.09399v3><strong>Designing a realistic peer-like embodied conversational agent for supporting children&rsquo;s storytelling</strong></a></p><p><em>Zhixin Li, Ying Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Advances in artificial intelligence have facilitated the use of largelanguage models (LLMs) and AI-generated synthetic media in education, which mayinspire HCI researchers to develop technologies, in particular, embodiedconversational agents (ECAs) to simulate the kind of scaffolding children mightreceive from a human partner. In this paper, we will propose a design prototypeof a peer-like ECA named STARie that integrates multiple AI models - GPT-3,Speech Synthesis (Real-time Voice Cloning), VOCA (Voice Operated CharacterAnimation), and FLAME (Faces Learned with an Articulated Model and Expressions)that aims to support narrative production in collaborative storytelling,specifically for children aged 4-8. However, designing a child-centered ECAraises concerns about age appropriateness, children privacy, gender choices ofECAs, and the uncanny valley effect. Thus, this paper will also discussconsiderations and ethical concerns that must be taken into account whendesigning such an ECA. This proposal offers insights into the potential use ofAI-generated synthetic media in child-centered AI design and how peer-like AIembodiment may support children\textquotesingle s storytelling.</div></details><blockquote><p><strong><em>2023-08-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.15827v1><strong>Introducing Language Guidance in Prompt-based Continual Learning</strong></a></p><p><em>Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier Stricker, Federico Tombari, Muhammad Zeshan Afzal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters.</div></details><blockquote><p><strong><em>2023-08-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.14965v1><strong>CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction</strong></a></p><p><em>Umar Khalid, Hasan Iqbal, Saeed Vahidian, Jing Hua, Chen Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human-robot interaction (HRI) is a rapidly growing field that encompassessocial and industrial applications. Machine learning plays a vital role inindustrial HRI by enhancing the adaptability and autonomy of robots in complexenvironments. However, data privacy is a crucial concern in the interactionbetween humans and robots, as companies need to protect sensitive data whilemachine learning algorithms require access to large datasets. FederatedLearning (FL) offers a solution by enabling the distributed training of modelswithout sharing raw data. Despite extensive research on Federated learning (FL)for tasks such as natural language processing (NLP) and image classification,the question of how to use FL for HRI remains an open research problem. Thetraditional FL approach involves transmitting large neural network parametermatrices between the server and clients, which can lead to high communicationcosts and often becomes a bottleneck in FL. This paper proposes acommunication-efficient FL framework for human-robot interaction (CEFHRI) toaddress the challenges of data heterogeneity and communication costs. Theframework leverages pre-trained models and introduces a trainablespatiotemporal adapter for video understanding tasks in HRI. Experimentalresults on three human-robot interaction benchmark datasets: HRI30, InHARD, andCOIN demonstrate the superiority of CEFHRI over full fine-tuning in terms ofcommunication costs. The proposed methodology provides a secure and efficientapproach to HRI federated learning, particularly in industrial environmentswith data privacy concerns and limited communication bandwidth. Our code isavailable athttps://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.</div></details><p><a href=http://arxiv.org/abs/2308.15010v1><strong>TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification</strong></a></p><p><em>Jianing Wang, Chengyu Wang, Cen Chen, Ming Gao, Jun Huang, Aoying Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Text classification is one of the most imperative tasks in natural languageprocessing (NLP). Recent advances with pre-trained language models (PLMs) haveshown remarkable success on this task. However, the satisfying results obtainedby PLMs heavily depend on the large amounts of task-specific labeled data,which may not be feasible in many application scenarios due to data access andprivacy constraints. The recently-proposed prompt-based fine-tuning paradigmimproves the performance of PLMs for few-shot text classification withtask-specific templates. Yet, it is unclear how the prompting knowledge can betransferred across tasks, for the purpose of mutual reinforcement. We proposeTransPrompt v2, a novel transferable prompting framework for few-shot learningacross similar or distant text classification tasks. For learning acrosssimilar tasks, we employ a multi-task meta-knowledge acquisition (MMA)procedure to train a meta-learner that captures the cross-task transferableknowledge. For learning across distant tasks, we further inject the task typedescriptions into the prompt, and capture the intra-type and inter-type promptembeddings among multiple distant tasks. Additionally, two de-biasingtechniques are further designed to make the trained meta-learner moretask-agnostic and unbiased towards any tasks. After that, the meta-learner canbe adapted to each specific task with better parameters initialization.Extensive experiments show that TransPrompt v2 outperforms single-task andcross-task strong baselines over multiple NLP tasks and datasets. We furthershow that the meta-learner can effectively improve the performance of PLMs onpreviously unseen tasks. In addition, TransPrompt v2 also outperforms strongfine-tuning baselines when learning with full training sets.</div></details><blockquote><p><strong><em>2023-08-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.14352v1><strong>EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models</strong></a></p><p><em>Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) such as GPTs and LLaMa have ushered in arevolution in machine intelligence, owing to their exceptional capabilities ina wide range of machine learning tasks. However, the transition of LLMs fromdata centers to edge devices presents a set of challenges and opportunities.While this shift can enhance privacy and availability, it is hampered by theenormous parameter sizes of these models, leading to impractical runtime costs.In light of these considerations, we introduce EdgeMoE, the first on-deviceinference engine tailored for mixture-of-expert (MoE) LLMs, a popular variantof sparse LLMs that exhibit nearly constant computational complexity as theirparameter size scales. EdgeMoE achieves both memory and computationalefficiency by strategically partitioning the model across the storagehierarchy. Specifically, non-expert weights are stored in the device&rsquo;s memory,while expert weights are kept in external storage and are fetched into memoryonly when they are activated. This design is underpinned by a crucial insightthat expert weights, though voluminous, are infrequently accessed due to sparseactivation patterns. To further mitigate the overhead associated with expertI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wisebitwidth adaptation: This method reduces the size of expert weights with anacceptable level of accuracy loss. (2) Expert management: It predicts theexperts that will be activated in advance and preloads them into thecompute-I/O pipeline, thus further optimizing the process. In empiricalevaluations conducted on well-established MoE LLMs and various edge devices,EdgeMoE demonstrates substantial memory savings and performance improvementswhen compared to competitive baseline solutions.</div></details><blockquote><p><strong><em>2023-08-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.11807v1><strong>Towards an On-device Agent for Text Rewriting</strong></a></p><p><em>Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu-hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, Lei Meng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have demonstrated impressive capabilities fortext rewriting. Nonetheless, the large sizes of these models make themimpractical for on-device inference, which would otherwise allow for enhancedprivacy and economical inference. Creating a smaller yet potent language modelfor text rewriting presents a formidable challenge because it requiresbalancing the need for a small size with the need to retain the emergentcapabilities of the LLM, that requires costly data collection. To address theabove challenge, we introduce a new instruction tuning approach for building amobile-centric text rewriting model. Our strategies enable the generation ofhigh quality training data without any human labeling. In addition, we proposea heuristic reinforcement learning framework which substantially enhancesperformance without requiring preference data. To further bridge theperformance gap with the larger server-side model, we propose an effectiveapproach that combines the mobile rewrite agent with the server model using acascade. To tailor the text rewriting tasks to mobile scenarios, we introduceMessageRewriteEval, a benchmark that focuses on text rewriting for messagesthrough natural language instructions. Through empirical experiments, wedemonstrate that our on-device model surpasses the current state-of-the-artLLMs in text rewriting while maintaining a significantly reduced model size.Notably, we show that our proposed cascading approach improves modelperformance.</div></details><blockquote><p><strong><em>2023-08-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.11284v2><strong>Federated learning for secure development of AI models for Parkinson&rsquo;s disease detection using speech from different languages</strong></a></p><p><em>Soroosh Tayebi Arasteh, Cristian David Rios-Urrego, Elmar Noeth, Andreas Maier, Seung Hee Yang, Jan Rusz, Juan Rafael Orozco-Arroyave</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Parkinson&rsquo;s disease (PD) is a neurological disorder impacting a person&rsquo;sspeech. Among automatic PD assessment methods, deep learning models have gainedparticular interest. Recently, the community has explored cross-pathology andcross-language models which can improve diagnostic accuracy even further.However, strict patient data privacy regulations largely prevent institutionsfrom sharing patient speech data with each other. In this paper, we employfederated learning (FL) for PD detection using speech signals from 3 real-worldlanguage corpora of German, Spanish, and Czech, each from a separateinstitution. Our results indicate that the FL model outperforms all the localmodels in terms of diagnostic accuracy, while not performing very differentlyfrom the model based on centrally combined training sets, with the advantage ofnot requiring any data sharing among collaborators. This will simplifyinter-institutional collaborations, resulting in enhancement of patientoutcomes.</div></details><blockquote><p><strong><em>2023-08-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.09376v1><strong>Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks</strong></a></p><p><em>Abubakar S. Ali, Dimitrios Michael Manias, Abdallah Shami, Sami Muhaidat</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As the dawn of sixth-generation (6G) networking approaches, it promisesunprecedented advancements in communication and automation. Among the leadinginnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming toachieve fully automated, self-optimizing networks with minimal humanintervention. Despite the advantages ZTNs offer in terms of efficiency andscalability, challenges surrounding transparency, adaptability, and human trustremain prevalent. Concurrently, the advent of Large Language Models (LLMs)presents an opportunity to elevate the ZTN framework by bridging the gapbetween automated processes and human-centric interfaces. This paper exploresthe integration of LLMs into ZTNs, highlighting their potential to enhancenetwork transparency and improve user interactions. Through a comprehensivecase study on deep reinforcement learning (DRL)-based anti-jamming technique,we demonstrate how LLMs can distill intricate network operations intointuitive, human-readable reports. Additionally, we address the technical andethical intricacies of melding LLMs with ZTNs, with an emphasis on dataprivacy, transparency, and bias reduction. Looking ahead, we identify emergingresearch avenues at the nexus of LLMs and ZTNs, advocating for sustainedinnovation and interdisciplinary synergy in the domain of automated networks.</div></details><blockquote><p><strong><em>2023-08-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.08774v1><strong>Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models</strong></a></p><p><em>Phillip Rust, Anders S√∏gaard</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingualgeneralization or compression to facilitate transfer to a large number of(potentially unseen) languages. However, these models should ideally also beprivate, linguistically fair, and transparent, by relating their predictions totraining data. Can these requirements be simultaneously satisfied? We show thatmultilingual compression and linguistic fairness are compatible withdifferential privacy, but that differential privacy is at odds with trainingdata influence sparsity, an objective for transparency. We further present aseries of experiments on two common NLP tasks and evaluate multilingualcompression and training data influence sparsity under different privacyguarantees, exploring these trade-offs in more detail. Our results suggest thatwe need to develop ways to jointly optimize for these objectives in order tofind practical trade-offs.</div></details><blockquote><p><strong><em>2023-08-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.07847v1><strong>Robustness Over Time: Understanding Adversarial Examples&rsquo; Effectiveness on Longitudinal Versions of Large Language Models</strong></a></p><p><em>Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have led to significant improvements in manytasks across various domains, such as code interpretation, response generation,and ambiguity handling. These LLMs, however, when upgrading, primarilyprioritize enhancing user experience while neglecting security, privacy, andsafety implications. Consequently, unintended vulnerabilities or biases can beintroduced. Previous studies have predominantly focused on specific versions ofthe models and disregard the potential emergence of new attack vectorstargeting the updated versions. Through the lens of adversarial examples withinthe in-context learning framework, this longitudinal study addresses this gapby conducting a comprehensive assessment of the robustness of successiveversions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments toanalyze and understand the impact of the robustness in two distinct learningcategories: zero-shot learning and few-shot learning. Our findings indicatethat, in comparison to earlier versions of LLMs, the updated versions do notexhibit the anticipated level of robustness against adversarial attacks. Inaddition, our study emphasizes the increased effectiveness of synergizedadversarial queries in most zero-shot learning and few-shot learning cases. Wehope that our study can lead to a more refined assessment of the robustness ofLLMs over time and provide valuable insights of these models for bothdevelopers and users.</div></details><blockquote><p><strong><em>2023-08-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.02898v4><strong>Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark</strong></a></p><p><em>Shuyu Yang, Yinan Zhou, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In this paper, we introduce a large Multi-Attribute and Language Searchdataset for text-based person retrieval, called MALS, and explore thefeasibility of performing pre-training on both attribute recognition andimage-text matching tasks in one stone. In particular, MALS contains 1,510,330image-text pairs, which is about 37.5 times larger than prevailing CUHK-PEDES,and all images are annotated with 27 attributes. Considering the privacyconcerns and annotation costs, we leverage the off-the-shelf diffusion modelsto generate the dataset. To verify the feasibility of learning from thegenerated data, we develop a new joint Attribute Prompt Learning and TextMatching Learning (APTM) framework, considering the shared knowledge betweenattribute and text. As the name implies, APTM contains an attribute promptlearning stream and a text matching learning stream. (1) The attribute promptlearning leverages the attribute prompts for image-attribute alignment, whichenhances the text matching learning. (2) The text matching learning facilitatesthe representation learning on fine-grained details, and in turn, boosts theattribute prompt learning. Extensive experiments validate the effectiveness ofthe pre-training on MALS, achieving state-of-the-art retrieval performance viaAPTM on three challenging real-world benchmarks. In particular, APTM achieves aconsistent improvement of +6.96%, +7.68%, and +16.95% Recall@1 accuracy onCUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively.</div></details><blockquote><p><strong><em>2023-08-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.06739v1><strong>Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks</strong></a></p><p><em>David Junhao Zhang, Mutian Xu, Chuhui Xue, Wenqing Zhang, Xiaoguang Han, Song Bai, Mike Zheng Shou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Despite the rapid advancement of unsupervised learning in visualrepresentation, it requires training on large-scale datasets that demand costlydata collection, and pose additional challenges due to concerns regarding dataprivacy. Recently, synthetic images generated by text-to-image diffusionmodels, have shown great potential for benefiting image recognition. Althoughpromising, there has been inadequate exploration dedicated to unsupervisedlearning on diffusion-generated images. To address this, we start by uncoveringthat diffusion models&rsquo; cross-attention layers inherently provideannotation-free attention masks aligned with corresponding text inputs ongenerated images. We then investigate the problems of three prevalentunsupervised learning techniques ( i.e., contrastive learning, masked modeling,and vision-language pretraining) and introduce customized solutions by fullyexploiting the aforementioned free attention masks. Our approach is validatedthrough extensive experiments that show consistent improvements in baselinemodels across various downstream tasks, including image classification,detection, segmentation, and image-text retrieval. By utilizing our method, itis possible to close the performance gap between unsupervised pretraining onsynthetic data and real-world scenarios.</div></details><blockquote><p><strong><em>2023-08-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.06261v1><strong>Enhancing Network Management Using Code Generated by Large Language Models</strong></a></p><p><em>Sathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Ranveer Chandra, Srikanth Kandula</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Analyzing network topologies and communication graphs plays a crucial role incontemporary network management. However, the absence of a cohesive approachleads to a challenging learning curve, heightened errors, and inefficiencies.In this paper, we introduce a novel approach to facilitate anatural-language-based network management experience, utilizing large languagemodels (LLMs) to generate task-specific code from natural language queries.This method tackles the challenges of explainability, scalability, and privacyby allowing network operators to inspect the generated code, eliminating theneed to share network data with LLMs, and concentrating on application-specificrequests combined with general program synthesis techniques. We design andevaluate a prototype system using benchmark applications, showcasing highaccuracy, cost-effectiveness, and the potential for further enhancements usingcomplementary program synthesis techniques.</div></details><blockquote><p><strong><em>2023-08-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.04913v1><strong>LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following</strong></a></p><p><em>Kaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, Qing Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: E-commerce authoring involves creating attractive, abundant, and targetedpromotional content to drive product sales. The emergence of large languagemodels (LLMs) introduces an innovative paradigm, offering a unified solution toaddress various authoring tasks within this scenario. However, mainstream LLMstrained on general corpora with common sense knowledge reveal limitations infitting complex and personalized features unique to e-commerce products andcustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,raising concerns about safeguarding voluminous customer privacy data duringtransmission. This paper proposes the LLaMA-E, the unified and customizedinstruction-following language models focusing on diverse e-commerce authoringtasks. Specifically, the domain experts create the seed instruction set fromthe tasks of ads generation, query-enhanced product title rewriting, productclassification, purchase intent speculation, and general Q&amp;A. These tasksenable the models to comprehensively understand precise e-commerce authoringknowledge by interleaving features covering typical service aspects ofcustomers, sellers, and platforms. The GPT-3.5 is introduced as a teachermodel, which expands the seed instructions to form a training set for theLLaMA-E models with various scales. The experimental results show that theproposed LLaMA-E models achieve state-of-the-art results in quantitative andqualitative evaluations, also exhibiting the advantage in zero-shot scenes. Tothe best of our knowledge, this study is the first to serve the LLMs tospecific e-commerce authoring scenarios.</div></details><blockquote><p><strong><em>2023-08-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2308.03983v1><strong>SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool</strong></a></p><p><em>Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka, Osamu Torii, Tomoya Kodama, Jun Deguchi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Model (LLM) based Generative AI systems have seen significantprogress in recent years. Integrating a knowledge retrieval architecture allowsfor seamless integration of private data into publicly available Generative AIsystems using pre-trained LLM without requiring additional model fine-tuning.Moreover, Retrieval-Centric Generation (RCG) approach, a promising futureresearch direction that explicitly separates roles of LLMs and retrievers incontext interpretation and knowledge memorization, potentially leads to moreefficient implementation. SimplyRetrieve is an open-source tool with the goalof providing a localized, lightweight, and user-friendly interface to thesesophisticated advancements to the machine learning community. SimplyRetrievefeatures a GUI and API based RCG platform, assisted by a Private Knowledge BaseConstructor and a Retrieval Tuning Module. By leveraging these capabilities,users can explore the potential of RCG for improving generative AI performancewhile maintaining privacy standards. The tool is available athttps://github.com/RCGAI/SimplyRetrieve with an MIT license.</div></details><blockquote><p><strong><em>2023-08-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.08674v3><strong>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</strong></a></p><p><em>Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Tables are prevalent in real-world databases, requiring significant time andeffort for humans to analyze and manipulate. The advancements in large languagemodels (LLMs) have made it possible to interact with tables using naturallanguage input, bringing this capability closer to reality. In this paper, wepresent TableGPT, a unified fine-tuned framework that enables LLMs tounderstand and operate on tables using external functional commands. Itintroduces the capability to seamlessly interact with tables, enabling a widerange of functionalities such as question answering, data manipulation (e.g.,insert, delete, query, and modify operations), data visualization, analysisreport generation, and automated prediction. TableGPT aims to provideconvenience and accessibility to users by empowering them to effortlesslyleverage tabular data. At the core of TableGPT lies the novel concept of globaltabular representations, which empowers LLMs to gain a comprehensiveunderstanding of the entire table beyond meta-information. By jointly trainingLLMs on both table and text modalities, TableGPT achieves a deep understandingof tabular data and the ability to perform complex operations on tables throughchain-of-command instructions. Importantly, TableGPT offers the advantage ofbeing a self-contained system rather than relying on external API interfaces.Moreover, it supports efficient data process flow, query rejection (whenappropriate) and private deployment, enabling faster domain data fine-tuningand ensuring data privacy, which enhances the framework&rsquo;s adaptability tospecific use cases.</div></details><blockquote><p><strong><em>2023-08-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.16756v2><strong>Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching</strong></a></p><p><em>Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, Xia Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The process of matching patients with suitable clinical trials is essentialfor advancing medical research and providing optimal care. However, currentapproaches face challenges such as data standardization, ethicalconsiderations, and a lack of interoperability between Electronic HealthRecords (EHRs) and clinical trial criteria. In this paper, we explore thepotential of large language models (LLMs) to address these challenges byleveraging their advanced natural language generation capabilities to improvecompatibility between EHRs and clinical trial descriptions. We propose aninnovative privacy-aware data augmentation approach for LLM-based patient-trialmatching (LLM-PTM), which balances the benefits of LLMs while ensuring thesecurity and confidentiality of sensitive patient data. Our experimentsdemonstrate a 7.32% average improvement in performance using the proposedLLM-PTM method, and the generalizability to new data is improved by 12.12%.Additionally, we present case studies to further illustrate the effectivenessof our approach and provide a deeper understanding of its underlyingprinciples.</div></details><blockquote><p><strong><em>2023-07-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.03086v2><strong>ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model</strong></a></p><p><em>Hanyao Huang, Ou Zheng, Dongdong Wang, Jiayi Yin, Zijin Wang, Shengxuan Ding, Heng Yin, Chuan Xu, Renjie Yang, Qian Zheng, Bing Shi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ChatGPT, a lite and conversational variant of Generative PretrainedTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone LargeLanguage Models (LLMs) with billions of parameters. LLMs have stirred up muchinterest among researchers and practitioners in their impressive skills innatural language processing tasks, which profoundly impact various fields. Thispaper mainly discusses the future applications of LLMs in dentistry. Weintroduce two primary LLM deployment methods in dentistry, including automateddental diagnosis and cross-modal dental diagnosis, and examine their potentialapplications. Especially, equipped with a cross-modal encoder, a single LLM canmanage multi-source data and conduct advanced natural language reasoning toperform complex clinical operations. We also present cases to demonstrate thepotential of a fully automatic Multi-Modal LLM AI system for dentistry clinicalapplication. While LLMs offer significant potential benefits, the challenges,such as data privacy, data quality, and model bias, need further study.Overall, LLMs have the potential to revolutionize dental diagnosis andtreatment, which indicates a promising avenue for clinical application andresearch in dentistry.</div></details><blockquote><p><strong><em>2023-07-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.15838v1><strong>Holistic Survey of Privacy and Fairness in Machine Learning</strong></a></p><p><em>Sina Shaham, Arash Hajisafi, Minh K Quan, Dinh C Nguyen, Bhaskar Krishnamachari, Charith Peris, Gabriel Ghinita, Cyrus Shahabi, Pubudu N. Pathirana</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy and fairness are two crucial pillars of responsible ArtificialIntelligence (AI) and trustworthy Machine Learning (ML). Each objective hasbeen independently studied in the literature with the aim of reducing utilityloss in achieving them. Despite the significant interest attracted from bothacademia and industry, there remains an immediate demand for more in-depthresearch to unravel how these two objectives can be simultaneously integratedinto ML models. As opposed to well-accepted trade-offs, i.e., privacy-utilityand fairness-utility, the interrelation between privacy and fairness is notwell-understood. While some works suggest a trade-off between the two objectivefunctions, there are others that demonstrate the alignment of these functionsin certain scenarios. To fill this research gap, we provide a thorough reviewof privacy and fairness in ML, including supervised, unsupervised,semi-supervised, and reinforcement learning. After examining and consolidatingthe literature on both objectives, we present a holistic survey on the impactof privacy on fairness, the impact of fairness on privacy, existingarchitectures, their interaction in application domains, and algorithms thataim to achieve both objectives while minimizing the utility sacrificed.Finally, we identify research challenges in achieving privacy and fairnessconcurrently in ML, particularly focusing on large language models.</div></details><blockquote><p><strong><em>2023-07-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.14192v1><strong>Unveiling Security, Privacy, and Ethical Concerns of ChatGPT</strong></a></p><p><em>Xiaodong Wu, Ran Duan, Jianbing Ni</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This paper delves into the realm of ChatGPT, an AI-powered chatbot thatutilizes topic modeling and reinforcement learning to generate naturalresponses. Although ChatGPT holds immense promise across various industries,such as customer service, education, mental health treatment, personalproductivity, and content creation, it is essential to address its security,privacy, and ethical implications. By exploring the upgrade path from GPT-1 toGPT-4, discussing the model&rsquo;s features, limitations, and potentialapplications, this study aims to shed light on the potential risks ofintegrating ChatGPT into our daily lives. Focusing on security, privacy, andethics issues, we highlight the challenges these concerns pose for widespreadadoption. Finally, we analyze the open problems in these areas, calling forconcerted efforts to ensure the development of secure and ethically sound largelanguage models.</div></details><blockquote><p><strong><em>2023-07-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.13221v1><strong>Multilevel Large Language Models for Everyone</strong></a></p><p><em>Yuanhao Gong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models have made significant progress in the past few years.However, they are either generic {\it or} field specific, splitting thecommunity into different groups. In this paper, we unify these large languagemodels into a larger map, where the generic {\it and} specific models arelinked together and can improve each other, based on the user personal inputand information from the internet. The idea of linking several large languagemodels together is inspired by the functionality of human brain. The specificregions on the brain cortex are specific for certain low level functionality.And these regions can jointly work together to achieve more complex high levelfunctionality. Such behavior on human brain cortex sheds the light to designthe multilevel large language models that contain global level, field level anduser level models. The user level models run on local machines to achieveefficient response and protect the user&rsquo;s privacy. Such multilevel modelsreduce some redundancy and perform better than the single level models. Theproposed multilevel idea can be applied in various applications, such asnatural language processing, computer vision tasks, professional assistant,business and healthcare.</div></details><blockquote><p><strong><em>2023-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.12181v1><strong>Security and Privacy Issues of Federated Learning</strong></a></p><p><em>Jahid Hasan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) has emerged as a promising approach to address dataprivacy and confidentiality concerns by allowing multiple participants toconstruct a shared model without centralizing sensitive data. However, thisdecentralized paradigm introduces new security challenges, necessitating acomprehensive identification and classification of potential risks to ensureFL&rsquo;s security guarantees. This paper presents a comprehensive taxonomy ofsecurity and privacy challenges in Federated Learning (FL) across variousmachine learning models, including large language models. We specificallycategorize attacks performed by the aggregator and participants, focusing onpoisoning attacks, backdoor attacks, membership inference attacks, generativeadversarial network (GAN) based attacks, and differential privacy attacks.Additionally, we propose new directions for future research, seeking innovativesolutions to fortify FL systems against emerging security risks and upholdsensitive data confidentiality in distributed learning environments.</div></details><p><a href=http://arxiv.org/abs/2303.13379v2><strong>Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review</strong></a></p><p><em>Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin, Dragan Ga≈°eviƒá</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Educational technology innovations leveraging large language models (LLMs)have shown the potential to automate the laborious process of generating andanalysing textual content. While various innovations have been developed toautomate a range of educational tasks (e.g., question generation, feedbackprovision, and essay grading), there are concerns regarding the practicalityand ethicality of these innovations. Such concerns may hinder future researchand the adoption of LLMs-based innovations in authentic educational contexts.To address this, we conducted a systematic scoping review of 118 peer-reviewedpapers published since 2017 to pinpoint the current state of research on usingLLMs to automate and support educational tasks. The findings revealed 53 usecases for LLMs in automating education tasks, categorised into nine maincategories: profiling/labelling, detection, grading, teaching support,prediction, knowledge representation, feedback, content generation, andrecommendation. Additionally, we also identified several practical and ethicalchallenges, including low technological readiness, lack of replicability andtransparency, and insufficient privacy and beneficence considerations. Thefindings were summarised into three recommendations for future studies,including updating existing innovations with state-of-the-art models (e.g.,GPT-3/4), embracing the initiative of open-sourcing models/systems, andadopting a human-centred approach throughout the developmental process. As theintersection of AI and education is continuously evolving, the findings of thisstudy can serve as an essential reference point for researchers, allowing themto leverage the strengths, learn from the limitations, and uncover potentialresearch opportunities enabled by ChatGPT and other generative AI models.</div></details><blockquote><p><strong><em>2023-07-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.11899v1><strong>Project Florida: Federated Learning Made Easy</strong></a></p><p><em>Daniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We present Project Florida, a system architecture and software developmentkit (SDK) enabling deployment of large-scale Federated Learning (FL) solutionsacross a heterogeneous device ecosystem. Federated learning is an approach tomachine learning based on a strong data sovereignty principle, i.e., thatprivacy and security of data is best enabled by storing it at its origin,whether on end-user devices or in segregated cloud storage silos. Federatedlearning enables model training across devices and silos while the trainingdata remains within its security boundary, by distributing a model snapshot toa client running inside the boundary, running client code to update the model,and then aggregating updated snapshots across many clients in a centralorchestrator. Deploying a FL solution requires implementation of complexprivacy and security mechanisms as well as scalable orchestrationinfrastructure. Scale and performance is a paramount concern, as the modeltraining process benefits from full participation of many client devices, whichmay have a wide variety of performance characteristics. Project Florida aims tosimplify the task of deploying cross-device FL solutions by providingcloud-hosted infrastructure and accompanying task management interfaces, aswell as a multi-platform SDK supporting most major programming languagesincluding C++, Java, and Python, enabling FL training across a wide range ofoperating system (OS) and hardware specifications. The architecture decouplesservice management from the FL workflow, enabling a cloud service provider todeliver FL-as-a-service (FLaaS) to ML engineers and application developers. Wepresent an overview of Florida, including a description of the architecture,sample code, and illustrative experiments demonstrating system capabilities.</div></details><blockquote><p><strong><em>2023-07-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.00470v4><strong>PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation</strong></a></p><p><em>Le Xiao, Xin Shan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models(LLMS)have shown excellent text generation capabilities,capable of generating fluent human-like responses for many downstream tasks.However, applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To cope with the above challenges, this paperproposes PatternGPT, a pattern-driven text generation framework for LargeLanguage Models. Firstly, the framework utilizes the extraction capability ofLarge Language Models to generate rich and diversified structured andformalized patterns, which facilitates the introduction of external knowledgeto do the computation, and then draws on the idea of federated learning to usemultiple agents to achieve the sharing in order to obtain more diversifiedpatterns, and finally uses judgment criteria and optimization algorithm tosearch for high-quality patterns to guide the generation of models. Finally,external knowledge such as judgment criteria and optimization algorithms areused to search for high-quality patterns, and the searched patterns are used toguide model generation. This framework has the advantages of generatingdiversified patterns, protecting data privacy, combining external knowledge,and improving the quality of generation, which provides an effective method tooptimize the text generation capability of large language models, and make itbetter applied to the field of intelligent dialogue and content generation.</div></details><blockquote><p><strong><em>2023-07-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.10476v1><strong>What can we learn from Data Leakage and Unlearning for Law?</strong></a></p><p><em>Jaydeep Borkar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have a privacy concern because they memorizetraining data (including personally identifiable information (PII) like emailsand phone numbers) and leak it during inference. A company can train an LLM onits domain-customized data which can potentially also include their users&rsquo; PII.In order to comply with privacy laws such as the &ldquo;right to be forgotten&rdquo;, thedata points of users that are most vulnerable to extraction could be deleted.We find that once the most vulnerable points are deleted, a new set of pointsbecome vulnerable to extraction. So far, little attention has been given tounderstanding memorization for fine-tuned models. In this work, we also showthat not only do fine-tuned models leak their training data but they also leakthe pre-training data (and PII) memorized during the pre-training phase. Theproperty of new data points becoming vulnerable to extraction after unlearningand leakage of pre-training data through fine-tuned models can pose significantprivacy and legal concerns for companies that use LLMs to offer services. Wehope this work will start an interdisciplinary discussion within AI and lawcommunities regarding the need for policies to tackle these issues.</div></details><blockquote><p><strong><em>2023-07-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.08925v1><strong>Federated Large Language Model: A Position Paper</strong></a></p><p><em>Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large scale language models (LLM) have received significant attention andfound diverse applications across various domains, but their developmentencounters challenges in real-world scenarios. These challenges arise due tothe scarcity of public domain data availability and the need to maintainprivacy with respect to private domain data. To address these issues, federatedlearning (FL) has emerged as a promising technology that enables collaborativetraining of shared models while preserving decentralized data. We propose theconcept of federated LLM, which comprises three key components, i.e., federatedLLM pre-training, federated LLM fine-tuning, and federated LLM promptengineering. For each component, we discuss its advantage over traditional LLMtraining methods and propose specific engineering strategies forimplementation. Furthermore, we explore the novel challenges introduced by theintegration of FL and LLM. We analyze existing solutions and identify potentialobstacles faced by these solutions within the context of federated LLM.</div></details><blockquote><p><strong><em>2023-07-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.18465v2><strong>Federated Learning of Gboard Language Models with Differential Privacy</strong></a></p><p><em>Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A. Choquette-Choo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, Yuanbo Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We train language models (LMs) with federated learning (FL) and differentialprivacy (DP) in the Google Keyboard (Gboard). We apply theDP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm toachieve meaningfully formal DP guarantees without requiring uniform sampling ofclient devices. To provide favorable privacy-utility trade-offs, we introduce anew client participation criterion and discuss the implication of itsconfiguration in large scale systems. We show how quantile-based clipestimation~\citep{andrew2019differentially} can be combined with DP-FTRL toadaptively choose the clip norm during training or reduce the hyperparametertuning in preparation for training. With the help of pretraining on publicdata, we train and deploy more than twenty Gboard LMs that achieve high utilityand $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two modelsadditionally trained with secure aggregation~\citep{bonawitz2017practical}. Weare happy to announce that all the next word prediction neural network LMs inGboard now have DP guarantees, and all future launches of Gboard neural networkLMs will require DP guarantees. We summarize our experience and provideconcrete suggestions on DP training for practitioners.</div></details><blockquote><p><strong><em>2023-07-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.07477v1><strong>Population Expansion for Training Language Models with Private Federated Learning</strong></a></p><p><em>Tatsuki Koga, Congzheng Song, Martin Pelikan, Mona Chitnis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) combined with differential privacy (DP) offersmachine learning (ML) training with distributed devices and with a formalprivacy guarantee. With a large population of devices, FL with DP produces aperformant model in a timely manner. However, for applications with a smallerpopulation, not only does the model utility degrade as the DP noise isinversely proportional to population, but also the training latency increasessince waiting for enough clients to become available from a smaller pool isslower. In this work, we thus propose expanding the population based on domainadaptation techniques to speed up the training and improves the final modelquality when training with small populations. We empirically demonstrate thatour techniques can improve the utility by 13% to 30% on real-world languagemodeling datasets.</div></details><blockquote><p><strong><em>2023-07-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.03875v2><strong>Large Language Models for Supply Chain Optimization</strong></a></p><p><em>Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide &ndash; a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft&rsquo;s cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.</div></details><blockquote><p><strong><em>2023-07-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.05920v1><strong>Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt</strong></a></p><p><em>Yuhao Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Contrastive language-image Pre-training (CLIP) [13] can leverage largedatasets of unlabeled Image-Text pairs, which have demonstrated impressiveperformance in various downstream tasks. Given that annotating medical data istime-consuming and laborious, Image-Text Pre-training has promisingapplications in exploiting large-scale medical image and radiology reportdatasets. However, medical Image-Text Pre-training faces several challenges, asfollows: (1) Due to privacy concerns, the amount of available medical data isrelatively small compared to natural data, leading to weaker generalizationability of the model. (2) Medical images are highly similar with onlyfine-grained differences in subtleties, resulting in a large number offalse-negative sample pairs in comparison learning. (3) The hand-crafted Promptusually differs from the natural medical image report, Subtle changes inwording can lead to significant differences in performance. In this paper, wepropose a unified Image-Text-Label contrastive learning framework based oncontinuous prompts, with three main contributions. First, We unified the dataof images, text, and labels, which greatly expanded the training data that themodel could utilize. Second, we address the issue of data diversity and theimpact of hand-crafted prompts on model performance by introducing continuousimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training tomitigate the problem of too many false-negative samples. We demonstrate throughsufficient experiments that the Unified Medical Contrastive Learning (UMCL)framework exhibits excellent performance on several downstream tasks.</div></details><blockquote><p><strong><em>2023-07-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.04401v1><strong>Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation</strong></a></p><p><em>Zhexin Zhang, Jiaxin Wen, Minlie Huang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large pre-trained language models achieve impressive results across manytasks. However, recent works point out that pre-trained language models maymemorize a considerable fraction of their training data, leading to the privacyrisk of information leakage. In this paper, we propose a method named Ethicistfor targeted training data extraction through loss smoothed soft prompting andcalibrated confidence estimation, investigating how to recover the suffix inthe training data when given a prefix. To elicit memorization in the attackedmodel, we tune soft prompt embeddings while keeping the model fixed. We furtherpropose a smoothing loss that smooths the loss distribution of the suffixtokens to make it easier to sample the correct suffix. In order to select themost probable suffix from a collection of sampled suffixes and estimate theprediction confidence, we propose a calibrated confidence estimation method,which normalizes the confidence of the generated suffixes with a localestimation. We show that Ethicist significantly improves the extractionperformance on a recently proposed public benchmark. We also investigateseveral factors influencing the data extraction performance, including decodingstrategy, model scale, prefix length, and suffix length. Our code is availableat <a href=https://github.com/thu-coai/Targeted-Data-Extraction>https://github.com/thu-coai/Targeted-Data-Extraction</a>.</div></details><blockquote><p><strong><em>2023-07-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.04280v1><strong>Shaping the Emerging Norms of Using Large Language Models in Social Computing Research</strong></a></p><p><em>Hong Shen, Tianshi Li, Toby Jia-Jun Li, Joon Sung Park, Diyi Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The emergence of Large Language Models (LLMs) has brought both excitement andconcerns to social computing research. On the one hand, LLMs offerunprecedented capabilities in analyzing vast amounts of textual data andgenerating human-like responses, enabling researchers to delve into complexsocial phenomena. On the other hand, concerns are emerging regarding thevalidity, privacy, and ethics of the research when LLMs are involved. This SIGaims at offering an open space for social computing researchers who areinterested in understanding the impacts of LLMs to discuss their currentpractices, perspectives, challenges when engaging with LLMs in their everydaywork and collectively shaping the emerging norms of using LLMs in socialcomputing research.</div></details><blockquote><p><strong><em>2023-07-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2307.01881v1><strong>ProPILE: Probing Privacy Leakage in Large Language Models</strong></a></p><p><em>Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The rapid advancement and widespread use of large language models (LLMs) haveraised significant concerns regarding the potential leakage of personallyidentifiable information (PII). These models are often trained on vastquantities of web-collected data, which may inadvertently include sensitivepersonal data. This paper presents ProPILE, a novel probing tool designed toempower data subjects, or the owners of the PII, with awareness of potentialPII leakage in LLM-based services. ProPILE lets data subjects formulate promptsbased on their own PII to evaluate the level of privacy intrusion in LLMs. Wedemonstrate its application on the OPT-1.3B model trained on the publiclyavailable Pile dataset. We show how hypothetical data subjects may assess thelikelihood of their PII being included in the Pile dataset being revealed.ProPILE can also be leveraged by LLM service providers to effectively evaluatetheir own levels of PII leakage with more powerful prompts specifically tunedfor their in-house models. This tool represents a pioneering step towardsempowering the data subjects for their awareness and control over their owndata on the web.</div></details><blockquote><p><strong><em>2023-06-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.14504v1><strong>ChatIDS: Explainable Cybersecurity Using Generative AI</strong></a></p><p><em>Victor J√ºttner, Martin Grimmer, Erik Buchmann</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Intrusion Detection Systems (IDS) are a proven approach to secure networks.However, in a privately used network, it is difficult for users withoutcybersecurity expertise to understand IDS alerts, and to respond in time withadequate measures. This puts the security of home networks, smart homeinstallations, home-office workers, etc. at risk, even if an IDS is correctlyinstalled and configured. In this work, we propose ChatIDS, our approach toexplain IDS alerts to non-experts by using large language models. We evaluatethe feasibility of ChatIDS by using ChatGPT, and we identify open researchissues with the help of interdisciplinary experts in artificial intelligence.Our results show that ChatIDS has the potential to increase network security byproposing meaningful security measures in an intuitive language from IDSalerts. Nevertheless, some potential issues in areas such as trust, privacy,ethics, etc. need to be resolved, before ChatIDS might be put into practice.</div></details><blockquote><p><strong><em>2023-06-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.14070v5><strong>ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge</strong></a></p><p><em>Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The primary aim of this research was to address the limitations observed inthe medical knowledge of prevalent large language models (LLMs) such asChatGPT, by creating a specialized language model with enhanced accuracy inmedical advice. We achieved this by adapting and refining the large languagemodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialoguessourced from a widely used online medical consultation platform. Theseconversations were cleaned and anonymized to respect privacy concerns. Inaddition to the model refinement, we incorporated a self-directed informationretrieval mechanism, allowing the model to access and utilize real-timeinformation from online sources like Wikipedia and data from curated offlinemedical databases. The fine-tuning of the model with real-world patient-doctorinteractions significantly improved the model&rsquo;s ability to understand patientneeds and provide informed advice. By equipping the model with self-directedinformation retrieval from reliable online and offline sources, we observedsubstantial improvements in the accuracy of its responses. Our proposedChatDoctor, represents a significant advancement in medical LLMs, demonstratinga significant improvement in understanding patient inquiries and providingaccurate advice. Given the high stakes and low error tolerance in the medicalfield, such enhancements in providing accurate and reliable information are notonly beneficial but essential.</div></details><blockquote><p><strong><em>2023-06-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.13700v1><strong>Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT</strong></a></p><p><em>Ryan Lingo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: This research delves into the construction and utilization of syntheticdatasets, specifically within the telematics sphere, leveraging OpenAI&rsquo;spowerful language model, ChatGPT. Synthetic datasets present an effectivesolution to challenges pertaining to data privacy, scarcity, and control overvariables - characteristics that make them particularly valuable for researchpursuits. The utility of these datasets, however, largely depends on theirquality, measured through the lenses of diversity, relevance, and coherence. Toillustrate this data creation process, a hands-on case study is conducted,focusing on the generation of a synthetic telematics dataset. The experimentinvolved an iterative guidance of ChatGPT, progressively refining prompts andculminating in the creation of a comprehensive dataset for a hypothetical urbanplanning scenario in Columbus, Ohio. Upon generation, the synthetic dataset wassubjected to an evaluation, focusing on the previously identified qualityparameters and employing descriptive statistics and visualization techniquesfor a thorough analysis. Despite synthetic datasets not serving as perfectreplacements for actual world data, their potential in specific use-cases, whenexecuted with precision, is significant. This research underscores thepotential of AI models like ChatGPT in enhancing data availability for complexsectors like telematics, thus paving the way for a myriad of new researchopportunities.</div></details><blockquote><p><strong><em>2023-06-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.09486v2><strong>FedMultimodal: A Benchmark For Multimodal Federated Learning</strong></a></p><p><em>Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, Mi Zhang, Salman Avestimehr, Shrikanth Narayanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Over the past few years, Federated Learning (FL) has become an emergingmachine learning technique to tackle data privacy challenges throughcollaborative training. In the Federated Learning algorithm, the clients submita locally trained model, and the server aggregates these parameters untilconvergence. Despite significant efforts that have been made to FL in fieldslike computer vision, audio, and natural language processing, the FLapplications utilizing multimodal data streams remain largely unexplored. It isknown that multimodal learning has broad real-world applications in emotionrecognition, healthcare, multimedia, and social media, while user privacypersists as a critical concern. Specifically, there are no existing FLbenchmarks targeting multimodal applications or related tasks. In order tofacilitate the research in multimodal FL, we introduce FedMultimodal, the firstFL benchmark for multimodal learning covering five representative multimodalapplications from ten commonly used datasets with a total of eight uniquemodalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-endmodeling framework ranging from data partition and feature extraction to FLbenchmark algorithms and model evaluation. Unlike existing FL benchmarks,FedMultimodal provides a standardized approach to assess the robustness of FLagainst three common data corruptions in real-life multimodal applications:missing modalities, missing labels, and erroneous labels. We hope thatFedMultimodal can accelerate numerous future research directions, includingdesigning multimodal FL algorithms toward extreme data heterogeneity,robustness multimodal FL, and efficient multimodal FL. The datasets andbenchmark results can be accessed at:https://github.com/usc-sail/fed-multimodal.</div></details><blockquote><p><strong><em>2023-06-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.10765v1><strong>Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost</strong></a></p><p><em>Juexiao Zhou, Xiuying Chen, Xin Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Medical artificial general intelligence (AGI) is an emerging field that aimsto develop systems specifically designed for medical applications that possessthe ability to understand, learn, and apply knowledge across a wide range oftasks and domains. Large language models (LLMs) represent a significant steptowards AGI. However, training cross-domain LLMs in the medical field posessignificant challenges primarily attributed to the requirement of collectingdata from diverse domains. This task becomes particularly difficult due toprivacy restrictions and the scarcity of publicly available medical datasets.Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specificmedical LLMs with the lowest cost, and suggest a possible path to achievemedical AGI. With an increasing number of domain-specific professionalmultimodal LLMs in the medical field being developed, MedAGI is designed toautomatically select appropriate medical models by analyzing users&rsquo; questionswith our novel adaptive expert selection algorithm. It offers a unifiedapproach to existing LLMs in the medical field, eliminating the need forretraining regardless of the introduction of new models. This characteristicrenders it a future-proof solution in the dynamically advancing medical domain.To showcase the resilience of MedAGI, we conducted an evaluation across threedistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysisof pathology pictures. The results demonstrated that MedAGI exhibitedremarkable versatility and scalability, delivering exceptional performanceacross diverse domains. Our code is publicly available to facilitate furtherresearch at <a href=https://github.com/JoshuaChou2018/MedAGI>https://github.com/JoshuaChou2018/MedAGI</a>.</div></details><blockquote><p><strong><em>2023-06-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.08223v1><strong>Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization</strong></a></p><p><em>Zhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are gaining increasing attention due to theirexceptional performance across numerous tasks. As a result, the general publicutilize them as an influential tool for boosting their productivity whilenatural language processing researchers endeavor to employ them in solvingexisting or new research problems. Unfortunately, individuals can only accesssuch powerful AIs through APIs, which ultimately leads to the transmission ofraw data to the models&rsquo; providers and increases the possibility of privacy dataleakage. Current privacy-preserving methods for cloud-deployed language modelsaim to protect privacy information in the pre-training dataset or during themodel training phase. However, they do not meet the specific challengespresented by the remote access approach of new large-scale language models. This paper introduces a novel task, &ldquo;User Privacy Protection for DialogueModels,&rdquo; which aims to safeguard sensitive user information from any possibledisclosure while conversing with chatbots. We also present an evaluation schemefor this task, which covers evaluation metrics for privacy protection, dataavailability, and resistance to simulation attacks. Moreover, we propose thefirst framework for this task, namely privacy protection through textsanitization. Before sending the input to remote large models, it filters outthe sensitive information, using several rounds of text sanitization based onprivacy types that users define. Upon receiving responses from the largermodel, our framework automatically restores privacy to ensure that theconversation goes smoothly, without intervention from the privacy filter.Experiments based on real-world datasets demonstrate the efficacy of ourprivacy-preserving approach against eavesdropping from potential attackers.</div></details><blockquote><p><strong><em>2023-06-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.08126v1><strong>PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer</strong></a></p><p><em>Xu Han, Bin Guo, Yoon Jung, Benjamin Yao, Yu Zhang, Xiaohu Liu, Chenlei Guo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Personalized dialogue agents (DAs) powered by large pre-trained languagemodels (PLMs) often rely on explicit persona descriptions to maintainpersonality consistency. However, such descriptions may not always be availableor may pose privacy concerns. To tackle this bottleneck, we introducePersonaPKT, a lightweight transfer learning approach that can buildpersona-consistent dialogue models without explicit persona descriptions. Byrepresenting each persona as a continuous vector, PersonaPKT learns implicitpersona-specific features directly from a small number of dialogue samplesproduced by the same persona, adding less than 0.1% trainable parameters foreach persona on top of the PLM backbone. Empirical results demonstrate thatPersonaPKT effectively builds personalized DAs with high storage efficiency,outperforming various baselines in terms of persona consistency whilemaintaining good response generation quality. In addition, it enhances privacyprotection by avoiding explicit persona descriptions. Overall, PersonaPKT is aneffective solution for creating personalized DAs that respect user privacy.</div></details><blockquote><p><strong><em>2023-06-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.05652v1><strong>Privacy Aware Question-Answering System for Online Mental Health Risk Assessment</strong></a></p><p><em>Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Social media platforms have enabled individuals suffering from mentalillnesses to share their lived experiences and find the online supportnecessary to cope. However, many users fail to receive genuine clinicalsupport, thus exacerbating their symptoms. Screening users based on what theypost online can aid providers in administering targeted healthcare and minimizefalse positives. Pre-trained Language Models (LMs) can assess users&rsquo; socialmedia data and classify them in terms of their mental health risk. We propose aQuestion-Answering (QA) approach to assess mental health risk using theUnified-QA model on two large mental health datasets. To protect user data, weextend Unified-QA by anonymizing the model training process using differentialprivacy. Our results demonstrate the effectiveness of modeling risk assessmentas a QA task, specifically for mental health use cases. Furthermore, themodel&rsquo;s performance decreases by less than 1% with the inclusion ofdifferential privacy. The proposed system&rsquo;s performance is indicative of apromising research direction that will lead to the development of privacy-awarediagnostic systems.</div></details><blockquote><p><strong><em>2023-06-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.05087v1><strong>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization</strong></a></p><p><em>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Instruction tuning large language models (LLMs) remains a challenging task,owing to the complexity of hyperparameter selection and the difficulty involvedin evaluating the tuned models. To determine the optimal hyperparameters, anautomatic, robust, and reliable evaluation benchmark is essential. However,establishing such a benchmark is not a trivial task due to the challengesassociated with evaluation accuracy and privacy protection. In response tothese challenges, we introduce a judge large language model, named PandaLM,which is trained to distinguish the superior model given several LLMs.PandaLM&rsquo;s focus extends beyond just the objective correctness of responses,which is the main focus of traditional evaluation datasets. It addresses vitalsubjective factors such as relative conciseness, clarity, adherence toinstructions, comprehensiveness, and formality. To ensure the reliability ofPandaLM, we collect a diverse human-annotated test dataset, where all contextsare generated by humans and labels are aligned with human preferences. Ourresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5&rsquo;s evaluationability and 88.28% of GPT-4&rsquo;s in terms of F1-score on our test dataset. PandaLMenables the evaluation of LLM to be fairer but with less cost, evidenced bysignificant improvements achieved by models tuned through PandaLM compared totheir counterparts trained with default Alpaca&rsquo;s hyperparameters. In addition,PandaLM does not depend on API-based evaluations, thus avoiding potential dataleakage. All resources of PandaLM are released athttps://github.com/WeOpenML/PandaLM.</div></details><p><a href=http://arxiv.org/abs/2304.10691v2><strong>SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model</strong></a></p><p><em>Juexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, Xin Gao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Skin and subcutaneous diseases rank high among the leading contributors tothe global burden of nonfatal diseases, impacting a considerable portion of thepopulation. Nonetheless, the field of dermatology diagnosis faces threesignificant hurdles. Firstly, there is a shortage of dermatologists accessibleto diagnose patients, particularly in rural regions. Secondly, accuratelyinterpreting skin disease images poses a considerable challenge. Lastly,generating patient-friendly diagnostic reports is usually a time-consuming andlabor-intensive task for dermatologists. To tackle these challenges, we presentSkinGPT-4, which is the world&rsquo;s first interactive dermatology diagnostic systempowered by an advanced visual large language model. SkinGPT-4 leverages afine-tuned version of MiniGPT-4, trained on an extensive collection of skindisease images (comprising 52,929 publicly available and proprietary images)along with clinical concepts and doctors&rsquo; notes. We designed a two-steptraining process to allow SkinGPT to express medical features in skin diseaseimages with natural language and make accurate diagnoses of the types of skindiseases. With SkinGPT-4, users could upload their own skin photos fordiagnosis, and the system could autonomously evaluate the images, identifiesthe characteristics and categories of the skin conditions, performs in-depthanalysis, and provides interactive treatment recommendations. Meanwhile,SkinGPT-4&rsquo;s local deployment capability and commitment to user privacy alsorender it an appealing choice for patients in search of a dependable andprecise diagnosis of their skin ailments. To demonstrate the robustness ofSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, whichwere independently reviewed by certified dermatologists, and showed thatSkinGPT-4 could provide accurate diagnoses of skin diseases.</div></details><blockquote><p><strong><em>2023-06-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.10025v2><strong>When Federated Learning Meets Pre-trained Language Models&rsquo; Parameter-Efficient Tuning Methods</strong></a></p><p><em>Zhuo Zhang, Yuanhang Yang, Yong Dai, Lizhen Qu, Zenglin Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With increasing privacy concerns on data, recent studies have madesignificant progress using federated learning (FL) on privacy-sensitive naturallanguage processing (NLP) tasks. Much literature suggests fully fine-tuningpre-trained language models (PLMs) in the FL paradigm can mitigate the dataheterogeneity problem and close the performance gap with centralized training.However, large PLMs bring the curse of prohibitive communication overhead andlocal model adaptation costs for the FL system. To this end, we introducevarious parameter-efficient tuning (PETuning) methods into federated learning.Specifically, we provide a holistic empirical study of representative PLMstuning methods in FL. The experimental results cover the analysis of dataheterogeneity levels, data scales, and different FL scenarios. Overallcommunication overhead can be significantly reduced by locally tuning andglobally aggregating lightweight model parameters while maintaining acceptableperformance in various FL settings. To facilitate the research of PETuning inFL, we also develop a federated tuning framework FedPETuning, which allowspractitioners to exploit different PETuning methods under the FL trainingparadigm conveniently. The source code is available at\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.</div></details><blockquote><p><strong><em>2023-06-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.04460v2><strong>Bag of Tricks for Training Data Extraction from Language Models</strong></a></p><p><em>Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, Shuicheng Yan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the advance of language models, privacy protection is receiving moreattention. Training data extraction is therefore of great importance, as it canserve as a potential tool to assess privacy leakage. However, due to thedifficulty of this task, most of the existing methods are proof-of-concept andstill not effective enough. In this paper, we investigate and benchmark tricksfor improving training data extraction using a publicly available dataset.Because most existing extraction methods use a pipeline ofgenerating-then-ranking, i.e., generating text candidates as potential trainingdata and then ranking them based on specific criteria, our research focuses onthe tricks for both text generation (e.g., sampling strategy) and text ranking(e.g., token-level criteria). The experimental results show that severalpreviously overlooked tricks can be crucial to the success of training dataextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricksoutperform the baseline by a large margin in most cases, providing a muchstronger baseline for future research. The code is available athttps://github.com/weichen-yu/LM-Extraction.</div></details><blockquote><p><strong><em>2023-05-31</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.08196v2><strong>Deep Regression Unlearning</strong></a></p><p><em>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the introduction of data protection and privacy regulations, it hasbecome crucial to remove the lineage of data on demand from a machine learning(ML) model. In the last few years, there have been notable developments inmachine unlearning to remove the information of certain training dataefficiently and effectively from ML models. In this work, we explore unlearningfor the regression problem, particularly in deep learning models. Unlearning inclassification and simple linear regression has been considerably investigated.However, unlearning in deep regression models largely remains an untouchedproblem till now. In this work, we introduce deep regression unlearning methodsthat generalize well and are robust to privacy attacks. We propose theBlindspot unlearning method which uses a novel weight optimization process. Arandomly initialized model, partially exposed to the retain samples and a copyof the original model are used together to selectively imprint knowledge aboutthe data that we wish to keep and scrub off the information of the data we wishto forget. We also propose a Gaussian fine tuning method for regressionunlearning. The existing unlearning metrics for classification are not directlyapplicable to regression unlearning. Therefore, we adapt these metrics for theregression setting. We conduct regression unlearning experiments for computervision, natural language processing and forecasting applications. Our methodsshow excellent performance for all these datasets across all the metrics.Source code: <a href=https://github.com/ayu987/deep-regression-unlearning>https://github.com/ayu987/deep-regression-unlearning</a></div></details><p><a href=http://arxiv.org/abs/2212.09864v2><strong>Synthetic Pre-Training Tasks for Neural Machine Translation</strong></a></p><p><em>Zexue He, Graeme Blackwood, Rameswar Panda, Julian McAuley, Rogerio Feris</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-training models with large crawled corpora can lead to issues such astoxicity and bias, as well as copyright and privacy concerns. A promising wayof alleviating such concerns is to conduct pre-training with synthetic tasksand data, since no real-world information is ingested by the model. Our goal inthis paper is to understand the factors that contribute to the effectiveness ofpre-training models when using synthetic resources, particularly in the contextof neural machine translation. We propose several novel approaches topre-training translation models that involve different levels of lexical andstructural knowledge, including: 1) generating obfuscated data from a largeparallel corpus 2) concatenating phrase pairs extracted from a smallword-aligned corpus, and 3) generating synthetic parallel data without realhuman language corpora. Our experiments on multiple language pairs reveal thatpre-training benefits can be realized even with high levels of obfuscation orpurely synthetic parallel data. We hope the findings from our comprehensiveempirical analysis will shed light on understanding what matters for NMTpre-training, as well as pave the way for the development of more efficient andless toxic models.</div></details><blockquote><p><strong><em>2023-05-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2209.07341v3><strong>Does CLIP Know My Face?</strong></a></p><p><em>Dominik Hintersdorf, Lukas Struppek, Manuel Brack, Felix Friedrich, Patrick Schramowski, Kristian Kersting</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: With the rise of deep learning in various applications, privacy concernsaround the protection of training data has become a critical area of research.Whereas prior studies have focused on privacy risks in single-modal models, weintroduce a novel method to assess privacy for multi-modal models, specificallyvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)reveals whether an individual was included in the training data by querying themodel with images of the same person. Letting the model choose from a widevariety of possible text labels, the model reveals whether it recognizes theperson and, therefore, was used for training. Our large-scale experiments onCLIP demonstrate that individuals used for training can be identified with veryhigh accuracy. We confirm that the model has learned to associate names withdepicted individuals, implying the existence of sensitive information that canbe extracted by adversaries. Our results highlight the need for strongerprivacy protection in large-scale models and suggest that IDIAs can be used toprove the unauthorized use of data for training and to enforce privacy laws.</div></details><blockquote><p><strong><em>2023-05-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.15242v1><strong>Machine Unlearning: its nature, scope, and importance for a &ldquo;delete culture&rdquo;</strong></a></p><p><em>Luciano Floridi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The article explores the cultural shift from recording to deletinginformation in the digital age and its implications on privacy, intellectualproperty (IP), and Large Language Models like ChatGPT. It begins by defining adelete culture where information, in principle legal, is made unavailable orinaccessible because unacceptable or undesirable, especially but not only dueto its potential to infringe on privacy or IP. Then it focuses on twostrategies in this context: deleting, to make information unavailable; andblocking, to make it inaccessible. The article argues that both strategies havesignificant implications, particularly for machine learning (ML) models whereinformation is not easily made unavailable. However, the emerging research areaof Machine Unlearning (MU) is highlighted as a potential solution. MU, still inits infancy, seeks to remove specific data points from ML models, effectivelymaking them &lsquo;forget&rsquo; completely specific information. If successful, MU couldprovide a feasible means to manage the overabundance of information and ensurea better protection of privacy and IP. However, potential ethical risks, suchas misuse, overuse, and underuse of MU, should be systematically studied todevise appropriate policies.</div></details><p><a href=http://arxiv.org/abs/2305.15594v1><strong>Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</strong></a></p><p><em>Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock&rsquo;s knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.</div></details><p><a href=http://arxiv.org/abs/2301.04283v2><strong>MGeo: Multi-Modal Geographic Pre-Training Method</strong></a></p><p><em>Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, Yao Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: As a core task in location-based services (LBS) (e.g., navigation maps),query and point of interest (POI) matching connects users&rsquo; intent withreal-world geographic information. Recently, pre-trained models (PTMs) havemade advancements in many natural language processing (NLP) tasks. Generictext-based PTMs do not have enough geographic knowledge for query-POI matching.To overcome this limitation, related literature attempts to employdomain-adaptive pre-training based on geo-related corpus. However, a querygenerally contains mentions of multiple geographic objects, such as nearbyroads and regions of interest (ROIs). The geographic context (GC), i.e., thesediverse geographic objects and their relationships, is therefore pivotal toretrieving the most relevant POI. Single-modal PTMs can barely make use of theimportant GC and therefore have limited performance. In this work, we propose anovel query-POI matching method Multi-modal Geographic language model (MGeo),which comprises a geographic encoder and a multi-modal interaction module. MGeorepresents GC as a new modality and is able to fully extract multi-modalcorrelations for accurate query-POI matching. Besides, there is no publiclyavailable benchmark for this topic. In order to facilitate further research, webuild a new open-source large-scale benchmark Geographic TExtual Similarity(GeoTES). The POIs come from an open-source geographic information system(GIS). The queries are manually generated by annotators to prevent privacyissues. Compared with several strong baselines, the extensive experimentresults and detailed ablation analyses on GeoTES demonstrate that our proposedmulti-modal pre-training method can significantly improve the query-POImatching capability of generic PTMs, even when the queries&rsquo; GC is not provided.Our code and dataset are publicly available athttps://github.com/PhantomGrapes/MGeo.</div></details><blockquote><p><strong><em>2023-05-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.12723v1><strong>Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting</strong></a></p><p><em>Xinlu Zhang, Shiyang Li, Xianjun Yang, Chenxin Tian, Yao Qin, Linda Ruth Petzold</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) demonstrate remarkable medical expertise, butdata privacy concerns impede their direct use in healthcare environments.Although offering improved data privacy protection, domain-specific smalllanguage models (SLMs) often underperform LLMs, emphasizing the need formethods that reduce this performance gap while alleviating privacy concerns. Inthis paper, we present a simple yet effective method that harnesses LLMs&rsquo;medical proficiency to boost SLM performance in medical tasks underprivacy-restricted scenarios. Specifically, we mitigate patient privacy issuesby extracting keywords from medical data and prompting the LLM to generate amedical knowledge-intensive context by simulating clinicians&rsquo; thoughtprocesses. This context serves as additional input for SLMs, augmenting theirdecision-making capabilities. Our method significantly enhances performance inboth few-shot and full training settings across three medicalknowledge-intensive tasks, achieving up to a 22.57% increase in absoluteaccuracy compared to SLM fine-tuning without context, and sets newstate-of-the-art results in two medical tasks within privacy-restrictedscenarios. Further out-of-domain testing and experiments in two general domaindatasets showcase its generalizability and broad applicability.</div></details><blockquote><p><strong><em>2023-05-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.12132v1><strong>Can Public Large Language Models Help Private Cross-device Federated Learning?</strong></a></p><p><em>Boxin Wang, Yibo Jacky Zhang, Yuan Cao, Bo Li, H. Brendan McMahan, Sewoong Oh, Zheng Xu, Manzil Zaheer</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study (differentially) private federated learning (FL) of language models.The language models in cross-device FL are relatively small, which can betrained with meaningful formal user-level differential privacy (DP) guaranteeswhen massive parallelism in training is enabled by the participation of amoderate size of users. Recently, public data has been used to improveprivacy-utility trade-offs for both large and small language models. In thiswork, we provide a systematic study of using large-scale public data and LLMsto help differentially private training of on-device FL models, and furtherimprove the privacy-utility tradeoff by techniques of distillation. Moreover,we propose a novel distribution matching algorithm with theoretical groundingto sample public data close to private data distribution, which significantlyimproves the sample efficiency of (pre-)training on public data. The proposedmethod is efficient and effective for training private model by takingadvantage of public data, especially for customized on-device architecturesthat do not have ready-to-use pre-trained models.</div></details><blockquote><p><strong><em>2023-05-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2306.05552v1><strong>ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery</strong></a></p><p><em>Anaelia Ovalle, Mehrab Beikzadeh, Parshan Teimouri, Kai-Wei Chang, Majid Sarrafzadeh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models have been useful in expanding mental health caredelivery. ChatGPT, in particular, has gained popularity for its ability togenerate human-like dialogue. However, data-sensitive domains &ndash; including butnot limited to healthcare &ndash; face challenges in using ChatGPT due to privacyand data-ownership concerns. To enable its utilization, we propose a textambiguation framework that preserves user privacy. We ground this in the taskof addressing stress prompted by user-provided texts to demonstrate theviability and helpfulness of privacy-preserved generations. Our results suggestthat chatGPT recommendations are still able to be moderately helpful andrelevant, even when the original user text is not provided.</div></details><p><a href=http://arxiv.org/abs/2305.11759v1><strong>Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning</strong></a></p><p><em>Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) are known to memorize significant portions oftheir training data. Parts of this memorized content have been shown to beextractable by simply querying the model, which poses a privacy risk. Wepresent a novel approach which uses prompt-tuning to control the extractionrates of memorized content in LLMs. We present two prompt training strategiesto increase and decrease extraction rates, which correspond to an attack and adefense, respectively. We demonstrate the effectiveness of our techniques byusing models from the GPT-Neo family on a public benchmark. For the 1.3Bparameter GPT-Neo model, our attack yields a 9.3 percentage point increase inextraction rate compared to our baseline. Our defense can be tuned to achievedifferent privacy-utility trade-offs by a user-specified hyperparameter. Weachieve an extraction rate reduction of up to 97.7% relative to our baseline,with a perplexity increase of 16.9%.</div></details><p><a href=http://arxiv.org/abs/2305.11418v1><strong>Towards Human-AI Collaborative Urban Science Research Enabled by Pre-trained Large Language Models</strong></a></p><p><em>Jiayi Fu, Haoying Han, Xing Su, Chao Fan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained large language models (PLMs) have the potential to support urbanscience research through content creation, information extraction, assistedprogramming, text classification, and other technical advances. In thisresearch, we explored the opportunities, challenges, and prospects of PLMs inurban science research. Specifically, we discussed potential applications ofPLMs to urban institution, urban space, urban information, and citizenbehaviors research through seven examples using ChatGPT. We also examined thechallenges of PLMs in urban science research from both technical and socialperspectives. The prospects of the application of PLMs in urban scienceresearch were then proposed. We found that PLMs can effectively aid inunderstanding complex concepts in urban science, facilitate urban spatial formidentification, assist in disaster monitoring, and sense public sentiment. Atthe same time, however, the applications of PLMs in urban science research faceevident threats, such as technical limitations, security, privacy, and socialbias. The development of fundamental models based on domain knowledge andhuman-AI collaboration may help improve PLMs to support urban science researchin future.</div></details><blockquote><p><strong><em>2023-05-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.10646v1><strong>Ethical ChatGPT: Concerns, Challenges, and Commandments</strong></a></p><p><em>Jianlong Zhou, Heimo M√ºller, Andreas Holzinger, Fang Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models, e.g. ChatGPT are currently contributing enormously tomake artificial intelligence even more popular, especially among the generalpopulation. However, such chatbot models were developed as tools to supportnatural language communication between humans. Problematically, it is very mucha ``statistical correlation machine" (correlation instead of causality) andthere are indeed ethical concerns associated with the use of AI language modelssuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlightsspecific ethical concerns on ChatGPT and articulates key challenges whenChatGPT is used in various applications. Practical commandments for differentstakeholders of ChatGPT are also proposed that can serve as checklistguidelines for those applying ChatGPT in their applications. These commandmentexamples are expected to motivate the ethical use of ChatGPT.</div></details><p><a href=http://arxiv.org/abs/2305.04757v2><strong>Augmented Large Language Models with Parametric Knowledge Guiding</strong></a></p><p><em>Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language Models (LLMs) have significantly advanced natural languageprocessing (NLP) with their impressive language understanding and generationcapabilities. However, their performance may be suboptimal for domain-specifictasks that require specialized knowledge due to limited exposure to the relateddata. Additionally, the lack of transparency of most state-of-the-art (SOTA)LLMs, which can only be accessed via APIs, impedes further fine-tuning withdomain custom data. Moreover, providing private data to the LLMs&rsquo; owner leadsto data privacy problems. To address these challenges, we propose the novelParametric Knowledge Guiding (PKG) framework, which equips LLMs with aknowledge-guiding module to access relevant knowledge without altering theLLMs&rsquo; parameters. Our PKG is based on open-source &ldquo;white-box&rdquo; language models,allowing offline memory of any knowledge that LLMs require. We demonstrate thatour PKG framework can enhance the performance of &ldquo;black-box&rdquo; LLMs on a range ofdomain knowledge-intensive tasks that require factual (+7.9%), tabular(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.</div></details><blockquote><p><strong><em>2023-05-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.09550v2><strong>Life of PII &ndash; A PII Obfuscation Transformer</strong></a></p><p><em>Ajinkya Deshmukh, Saumya Banthia, Anantha Sharma</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Protecting sensitive information is crucial in today&rsquo;s world of LargeLanguage Models (LLMs) and data-driven services. One common method used topreserve privacy is by using data perturbation techniques to reduceoverreaching utility of (sensitive) Personal Identifiable Information (PII)data while maintaining its statistical and semantic properties. Dataperturbation methods often result in significant information loss, making themimpractical for use. In this paper, we propose &lsquo;Life of PII&rsquo;, a novelObfuscation Transformer framework for transforming PII into faux-PII whilepreserving the original information, intent, and context as much as possible.Our approach includes an API to interface with the given document, aconfiguration-based obfuscator, and a model based on the Transformerarchitecture, which has shown high context preservation and performance innatural language processing tasks and LLMs. Our Transformer-based approach learns mapping between the original PII andits transformed faux-PII representation, which we call &ldquo;obfuscated&rdquo; data. Ourexperiments demonstrate that our method, called Life of PII, outperformstraditional data perturbation techniques in terms of both utility preservationand privacy protection. We show that our approach can effectively reduceutility loss while preserving the original information, offering greaterflexibility in the trade-off between privacy protection and data utility. Ourwork provides a solution for protecting PII in various real-world applications.</div></details><p><a href=http://arxiv.org/abs/2304.09513v2><strong>NetGPT: Generative Pretrained Transformer for Network Traffic</strong></a></p><p><em>Xuying Meng, Chungang Lin, Yequan Wang, Yujun Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: All data on the Internet are transferred by network traffic, thus accuratelymodeling network traffic can help improve network services quality and protectdata privacy. Pretrained models for network traffic can utilize large-scale rawdata to learn the essential characteristics of network traffic, and generatedistinguishable results for input traffic without considering specificdownstream tasks. Effective pretrained models can significantly optimize thetraining efficiency and effectiveness of downstream tasks, such as applicationclassification, attack detection and traffic generation. Despite the greatsuccess of pretraining in natural language processing, there is no work in thenetwork field. Considering the diverse demands and characteristics of networktraffic and network tasks, it is non-trivial to build a pretrained model fornetwork traffic and we face various challenges, especially the heterogeneousheaders and payloads in the multi-pattern network traffic and the differentdependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt toprovide a generative pretrained model NetGPT for both traffic understanding andgeneration tasks. We propose the multi-pattern network traffic modeling toconstruct unified text inputs and support both traffic understanding andgeneration tasks. We further optimize the adaptation effect of the pretrainedmodel to diversified tasks by shuffling header fields, segmenting packets inflows, and incorporating diverse task labels with prompts. With diverse trafficdatasets from encrypted software, DNS, private industrial protocols andcryptocurrency mining, expensive experiments demonstrate the effectiveness ofour NetGPT in a range of traffic understanding and generation tasks on trafficdatasets, and outperform state-of-the-art baselines by a wide margin.</div></details><blockquote><p><strong><em>2023-05-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.10011v2><strong>PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English</strong></a></p><p><em>Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, Kai-Wei Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy policies provide individuals with information about their rights andhow their personal information is handled. Natural language understanding (NLU)technologies can support individuals and practitioners to understand betterprivacy practices described in lengthy and complex documents. However, existingefforts that use NLU technologies are limited by processing the language in away exclusive to a single task focusing on certain privacy practices. To thisend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)benchmark, a multi-task benchmark for evaluating the privacy policy languageunderstanding across various tasks. We also collect a large corpus of privacypolicies to enable privacy policy domain-specific language model pre-training.We evaluate several generic pre-trained language models and continuepre-training them on the collected corpus. We demonstrate that domain-specificcontinual pre-training offers performance improvements across all tasks.</div></details><blockquote><p><strong><em>2023-05-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.06212v1><strong>Privacy-Preserving Prompt Tuning for Large Language Model Services</strong></a></p><p><em>Yansong Li, Zhixing Tan, Yang Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prompt tuning provides an efficient way for users to customize Large LanguageModels (LLMs) with their private data in the emerging LLM service scenario.However, the sensitive nature of private data brings the need for privacypreservation in LLM service customization. Based on prompt tuning, we proposePrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacyguarantees for LLM services. \textsc{rapt} adopts a local privacy setting,allowing users to privatize their data locally with local differential privacy.As prompt tuning performs poorly when directly trained on privatized data, weintroduce a novel privatized token reconstruction task that is trained jointlywith the downstream task, allowing LLMs to learn better task-dependentrepresentations. Despite the simplicity of our framework, experiments show thatRAPT achieves competitive performance across tasks while providing privacyguarantees against adversaries.</div></details><blockquote><p><strong><em>2023-05-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.00099v2><strong>Measuring Forgetting of Memorized Training Examples</strong></a></p><p><em>Matthew Jagielski, Om Thakkar, Florian Tram√®r, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine learning models exhibit two seemingly contradictory phenomena:training data memorization, and various forms of forgetting. In memorization,models overfit specific training examples and become susceptible to privacyattacks. In forgetting, examples which appeared early in training are forgottenby the end. In this work, we connect these phenomena. We propose a technique tomeasure to what extent models &ldquo;forget&rdquo; the specifics of training examples,becoming less susceptible to privacy attacks on examples they have not seenrecently. We show that, while non-convex models can memorize data forever inthe worst-case, standard image, speech, and language models empirically doforget examples over time. We identify nondeterminism as a potentialexplanation, showing that deterministically trained models do not forget. Ourresults suggest that examples seen early when training with extremely largedatasets - for instance those examples used to pre-train a model - may observeprivacy benefits at the expense of examples seen later.</div></details><blockquote><p><strong><em>2023-05-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.04701v1><strong>Differentially Private Attention Computation</strong></a></p><p><em>Yeqi Gao, Zhao Song, Xin Yang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have had a profound impact on numerous aspectsof daily life including natural language processing, content generation,research methodologies and so on. However, one crucial issue concerning theinference results of large language models is security and privacy. In manyscenarios, the results generated by LLMs could possibly leak many confidentialor copyright information. A recent beautiful and breakthrough work [Vyas,Kakade and Barak 2023] focus on such privacy issue of the LLMs from theoreticalperspective. It is well-known that computing the attention matrix is one of themajor task during the LLMs computation. Thus, how to give a provable privatelyguarantees of computing the attention matrix is an important researchdirection. Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposedprovable tight result for fast computation of attention without consideringprivacy concerns. One natural mathematical formulation to quantity the privacyin theoretical computer science graduate school textbook is differentialprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide aprovable result for showing how to differentially private approximate theattention matrix. From technique perspective, our result replies on a pioneering work in thearea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang2022].</div></details><blockquote><p><strong><em>2023-05-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.01550v1><strong>Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy</strong></a></p><p><em>Aly M. Kassem</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Language models (LLMs) are trained on large amounts of data, which caninclude sensitive information that may compromise personal privacy. LLMs showedto memorize parts of the training data and emit those data verbatim when anadversary prompts appropriately. Previous research has primarily focused ondata preprocessing and differential privacy techniques to address memorizationor prevent verbatim memorization exclusively, which can give a false sense ofprivacy. However, these methods rely on explicit and implicit assumptions aboutthe structure of the data to be protected, which often results in an incompletesolution to the problem. To address this, we propose a novel framework thatutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigateapproximate memorization. Our approach utilizes a negative similarity score,such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilaritypolicy. Our results demonstrate that this framework effectively mitigatesapproximate memorization while maintaining high levels of coherence and fluencyin the generated samples. Furthermore, our framework is robust in mitigatingapproximate memorization across various circumstances, including longercontext, which is known to increase memorization in LLMs.</div></details><blockquote><p><strong><em>2023-05-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.09419v3><strong>A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT</strong></a></p><p><em>Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, Lichao Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pretrained Foundation Models (PFMs) are regarded as the foundation forvarious downstream tasks with different data modalities. A PFM (e.g., BERT,ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonableparameter initialization for a wide range of downstream applications. BERTlearns bidirectional encoder representations from Transformers, which aretrained on large datasets as contextual language models. Similarly, thegenerative pretrained transformer (GPT) method employs Transformers as thefeature extractor and is trained using an autoregressive paradigm on largedatasets. Recently, ChatGPT shows promising success on large language models,which applies an autoregressive language model with zero shot or few shotprompting. The remarkable achievements of PFM have brought significantbreakthroughs to various fields of AI. Numerous studies have proposed differentmethods, raising the demand for an updated survey. This study provides acomprehensive review of recent research advancements, challenges, andopportunities for PFMs in text, image, graph, as well as other data modalities.The review covers the basic components and existing pretraining methods used innatural language processing, computer vision, and graph learning. Additionally,it explores advanced PFMs used for different data modalities and unified PFMsthat consider data quality and quantity. The review also discusses researchrelated to the fundamentals of PFMs, such as model efficiency and compression,security, and privacy. Finally, the study provides key implications, futureresearch directions, challenges, and open problems in the field of PFMs.Overall, this survey aims to shed light on the research of the PFMs onscalability, security, logical reasoning ability, cross-domain learningability, and the user-friendly interactive ability for artificial generalintelligence.</div></details><blockquote><p><strong><em>2023-04-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.00593v1><strong>Reliable Gradient-free and Likelihood-free Prompt Tuning</strong></a></p><p><em>Maohao Shen, Soumya Ghosh, Prasanna Sattigeri, Subhro Das, Yuheng Bu, Gregory Wornell</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Due to privacy or commercial constraints, large pre-trained language models(PLMs) are often offered as black-box APIs. Fine-tuning such models todownstream tasks is challenging because one can neither access the model&rsquo;sinternal representations nor propagate gradients through it. This paperaddresses these challenges by developing techniques for adapting PLMs with onlyAPI access. Building on recent work on soft prompt tuning, we develop methodsto tune the soft prompts without requiring gradient computation. Further, wedevelop extensions that in addition to not requiring gradients also do not needto access any internal representation of the PLM beyond the input embeddings.Moreover, instead of learning a single prompt, our methods learn a distributionover prompts allowing us to quantify predictive uncertainty. Ours is the firstwork to consider uncertainty in prompts when only having API access to the PLM.Finally, through extensive experiments, we carefully vet the proposed methodsand find them competitive with (and sometimes even improving on) gradient-basedapproaches with full access to the PLM.</div></details><blockquote><p><strong><em>2023-04-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.13188v1><strong>TABLET: Learning From Instructions For Tabular Data</strong></a></p><p><em>Dylan Slack, Sameer Singh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Acquiring high-quality data is often a significant challenge in trainingmachine learning (ML) models for tabular prediction, particularly inprivacy-sensitive and costly domains like medicine and finance. Providingnatural language instructions to large language models (LLMs) offers analternative solution. However, it is unclear how effectively instructionsleverage the knowledge in LLMs for solving tabular prediction problems. Toaddress this gap, we introduce TABLET, a benchmark of 20 diverse tabulardatasets annotated with instructions that vary in their phrasing, granularity,and technicality. Additionally, TABLET includes the instructions&rsquo; logic andstructured modifications to the instructions. We find in-context instructionsincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% forChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabularprediction in our benchmark by evaluating instruction faithfulness. We findLLMs often ignore instructions and fail to predict specific instancescorrectly, even with examples. Our analysis on TABLET shows that, whileinstructions help LLM performance, learning from instructions for tabular datarequires new capabilities.</div></details><blockquote><p><strong><em>2023-04-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.08952v3><strong>Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies</strong></a></p><p><em>Md Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, Kai-Wei Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prior studies in privacy policies frame the question answering (QA) task asidentifying the most relevant text segment or a list of sentences from a policydocument given a user query. Existing labeled datasets are heavily imbalanced(only a few relevant segments), limiting the QA performance in this domain. Inthis paper, we develop a data augmentation framework based on ensemblingretriever models that captures the relevant text segments from unlabeled policydocuments and expand the positive examples in the training set. In addition, toimprove the diversity and quality of the augmented data, we leverage multiplepre-trained language models (LMs) and cascade them with noise reduction filtermodels. Using our augmented data on the PrivacyQA benchmark, we elevate theexisting baseline by a large margin (10% F1) and achieve a newstate-of-the-art F1 score of 50%. Our ablation studies provide furtherinsights into the effectiveness of our approach.</div></details><blockquote><p><strong><em>2023-04-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2304.03472v2><strong>Does Prompt-Tuning Language Model Ensure Privacy?</strong></a></p><p><em>Shangyu Xie, Wei Dai, Esha Ghosh, Sambuddha Roy, Dan Schwartz, Kim Laine</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Prompt-tuning has received attention as an efficient tuning method in thelanguage domain, i.e., tuning a prompt that is a few tokens long, while keepingthe large language model frozen, yet achieving comparable performance withconventional fine-tuning. Considering the emerging privacy concerns withlanguage models, we initiate the study of privacy leakage in the setting ofprompt-tuning. We first describe a real-world email service pipeline to providecustomized output for various users via prompt-tuning. Then we propose a novelprivacy attack framework to infer users&rsquo; private information by exploiting theprompt module with user-specific signals. We conduct a comprehensive privacyevaluation on the target pipeline to demonstrate the potential leakage fromprompt-tuning. The results also demonstrate the effectiveness of the proposedattack.</div></details><blockquote><p><strong><em>2023-04-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2305.03123v1><strong>ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review</strong></a></p><p><em>Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: ChatGPT is another large language model (LLM) inline but due to itsperformance and ability to converse effectively, it has gained a hugepopularity amongst research as well as industrial community. Recently, manystudies have been published to show the effectiveness, efficiency, integration,and sentiments of chatGPT and other LLMs. In contrast, this study focuses onthe important aspects that are mostly overlooked, i.e. sustainability, privacy,digital divide, and ethics and suggests that not only chatGPT but everysubsequent entry in the category of conversational bots should undergoSustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. Thispaper discusses in detail about the issues and concerns raised over chatGPT inline with aforementioned characteristics. We support our hypothesis by somepreliminary data collection and visualizations along with hypothesized facts.We also suggest mitigations and recommendations for each of the concerns.Furthermore, we also suggest some policies and recommendations for AI policyact, if designed by the governments.</div></details><blockquote><p><strong><em>2023-04-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.13856v2><strong>Unleashing ChatGPT on the Metaverse: Savior or Destroyer?</strong></a></p><p><em>Pengyuan Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The incorporation of artificial intelligence (AI) technology, and inparticular natural language processing (NLP), is becoming increasingly vitalfor the development of immersive and interactive metaverse experiences. Onesuch artificial intelligence tool that is gaining traction in the metaverse isChatGPT, a large language model trained by OpenAI. The article delves into thepros and cons of utilizing ChatGPT for metaverse-based education,entertainment, personalization, and support. Dynamic and personalizedexperiences are possible with this technology, but there are also legitimateprivacy, bias, and ethical issues to consider. This article aims to helpreaders understand the possible influence of ChatGPT on the metaverse and howit may be used to effectively create a more immersive and engaging virtualenvironment by evaluating these opportunities and obstacles.</div></details><blockquote><p><strong><em>2023-04-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.04360v2><strong>Does Synthetic Data Generation of LLMs Help Clinical Text Mining?</strong></a></p><p><em>Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent advancements in large language models (LLMs) have led to thedevelopment of highly potent models like OpenAI&rsquo;s ChatGPT. These models haveexhibited exceptional performance in a variety of tasks, such as questionanswering, essay composition, and code generation. However, their effectivenessin the healthcare sector remains uncertain. In this study, we seek toinvestigate the potential of ChatGPT to aid in clinical text mining byexamining its ability to extract structured information from unstructuredhealthcare texts, with a focus on biological named entity recognition andrelation extraction. However, our preliminary results indicate that employingChatGPT directly for these tasks resulted in poor performance and raisedprivacy concerns associated with uploading patients&rsquo; information to the ChatGPTAPI. To overcome these limitations, we propose a new training paradigm thatinvolves generating a vast quantity of high-quality synthetic data with labelsutilizing ChatGPT and fine-tuning a local model for the downstream task. Ourmethod has resulted in significant improvements in the performance ofdownstream tasks, improving the F1-score from 23.37% to 63.99% for the namedentity recognition task and from 75.86% to 83.59% for the relation extractiontask. Furthermore, generating data using ChatGPT can significantly reduce thetime and effort required for data collection and labeling, as well as mitigatedata privacy concerns. In summary, the proposed framework presents a promisingsolution to enhance the applicability of LLM models to clinical text mining.</div></details><blockquote><p><strong><em>2023-03-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.09790v2><strong>ConStruct-VL: Data-Free Continual Structured VL Concepts Learning</strong></a></p><p><em>James Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim, Rameswar Panda, David Cox, Diyi Yang, Zsolt Kira, Rogerio Feris, Leonid Karlinsky</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recently, large-scale pre-trained Vision-and-Language (VL) foundation modelshave demonstrated remarkable capabilities in many zero-shot downstream tasks,achieving competitive results for recognizing objects defined by as little asshort text prompts. However, it has also been shown that VL models are stillbrittle in Structured VL Concept (SVLC) reasoning, such as the ability torecognize object attributes, states, and inter-object relations. This leads toreasoning mistakes, which need to be corrected as they occur by teaching VLmodels the missing SVLC skills; often this must be done using private datawhere the issue was found, which naturally leads to a data-free continual (notask-id) VL learning setting. In this work, we introduce the first ContinualData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show itis challenging for many existing data-free CL strategies. We, therefore,propose a data-free method comprised of a new approach of AdversarialPseudo-Replay (APR) which generates adversarial reminders of past tasks frompast task models. To use this method efficiently, we also propose a continualparameter-efficient Layered-LoRA (LaLo) neural architecture allowingno-memory-cost access to all past models at train time. We show this approachoutperforms all data-free methods by as much as ~7% while even matching somelevels of experience-replay (prohibitive for applications where data-privacymust be preserved). Our code is publicly available athttps://github.com/jamessealesmith/ConStruct-VL</div></details><blockquote><p><strong><em>2023-03-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.16028v1><strong>Synthetically generated text for supervised text analysis</strong></a></p><p><em>Andrew Halterman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Supervised text models are a valuable tool for political scientists butpresent several obstacles to their use, including the expense of hand-labelingdocuments, the difficulty of retrieving rare relevant documents for annotation,and copyright and privacy concerns involved in sharing annotated documents.This article proposes a partial solution to these three issues, in the form ofcontrolled generation of synthetic text with large language models. I provide aconceptual overview of text generation, guidance on when researchers shouldprefer different techniques for generating synthetic text, a discussion ofethics, and a simple technique for improving the quality of synthetic text. Idemonstrate the usefulness of synthetic text with three applications:generating synthetic tweets describing the fighting in Ukraine, synthetic newsarticles describing specified political events for training an event detectionsystem, and a multilingual corpus of populist manifesto statements for traininga sentence-level populism classifier.</div></details><blockquote><p><strong><em>2023-03-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.12429v1><strong>Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models</strong></a></p><p><em>Constantinos Patsakis, Nikolaos Lykousas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The collection and use of personal data are becoming more common in today&rsquo;sdata-driven culture. While there are many advantages to this, including betterdecision-making and service delivery, it also poses significant ethical issuesaround confidentiality and privacy. Text anonymisation tries to prune and/ormask identifiable information from a text while keeping the remaining contentintact to alleviate privacy concerns. Text anonymisation is especiallyimportant in industries like healthcare, law, as well as research, wheresensitive and personal information is collected, processed, and exchanged underhigh legal and ethical standards. Although text anonymization is widely adopted in practice, it continues toface considerable challenges. The most significant challenge is striking abalance between removing information to protect individuals&rsquo; privacy whilemaintaining the text&rsquo;s usability for future purposes. The question is whetherthese anonymisation methods sufficiently reduce the risk of re-identification,in which an individual can be identified based on the remaining information inthe text. In this work, we challenge the effectiveness of these methods and how weperceive identifiers. We assess the efficacy of these methods against theelephant in the room, the use of AI over big data. While most of the researchis focused on identifying and removing personal information, there is limiteddiscussion on whether the remaining information is sufficient to deanonymiseindividuals and, more precisely, who can do it. To this end, we conduct anexperiment using GPT over anonymised texts of famous people to determinewhether such trained networks can deanonymise them. The latter allows us torevise these methods and introduce a novel methodology that employs LargeLanguage Models to improve the anonymity of texts.</div></details><blockquote><p><strong><em>2023-03-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.15042v3><strong>Privately Fine-Tuning Large Language Models with Differential Privacy</strong></a></p><p><em>Rouzbeh Behnia, Mohamamdreza Ebrahimi, Jason Pacheco, Balaji Padmanabhan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-trained Large Language Models (LLMs) are an integral part of modern AIthat have led to breakthrough performances in complex AI tasks. Major AIcompanies with expensive infrastructures are able to develop and train theselarge models with billions and millions of parameters from scratch. Thirdparties, researchers, and practitioners are increasingly adopting thesepre-trained models and fine-tuning them on their private data to accomplishtheir downstream AI tasks. However, it has been shown that an adversary canextract/reconstruct the exact training samples from these LLMs, which can leadto revealing personally identifiable information. The issue has raised deepconcerns about the privacy of LLMs. Differential privacy (DP) provides arigorous framework that allows adding noise in the process of training orfine-tuning LLMs such that extracting the training data becomes infeasible(i.e., with a cryptographically small success probability). While thetheoretical privacy guarantees offered in most extant studies assume learningmodels from scratch through many training iterations in an asymptotic setting,this assumption does not hold in fine-tuning scenarios in which the number oftraining iterations is significantly smaller. To address the gap, we present\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant withfinite-sample privacy guarantees. Our results across four well-establishednatural language understanding (NLU) tasks show that while \ewtune~adds privacyguarantees to LLM fine-tuning process, it directly contributes to decreasingthe induced noise to up to 5.6% and improves the state-of-the-art LLMsperformance by up to 1.1% across all NLU tasks. We have open-sourced ourimplementations for wide adoption and public testing purposes.</div></details><blockquote><p><strong><em>2023-03-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.09136v1><strong>A Short Survey of Viewing Large Language Models in Legal Aspect</strong></a></p><p><em>Zhongxiang Sun</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LLMs) have transformed many fields, including naturallanguage processing, computer vision, and reinforcement learning. These modelshave also made a significant impact in the field of law, where they are beingincreasingly utilized to automate various legal tasks, such as legal judgementprediction, legal document analysis, and legal document writing. However, theintegration of LLMs into the legal field has also raised several legalproblems, including privacy concerns, bias, and explainability. In this survey,we explore the integration of LLMs into the field of law. We discuss thevarious applications of LLMs in legal tasks, examine the legal challenges thatarise from their use, and explore the data resources that can be used tospecialize LLMs in the legal domain. Finally, we discuss several promisingdirections and conclude this paper. By doing so, we hope to provide an overviewof the current state of LLMs in law and highlight the potential benefits andchallenges of their integration.</div></details><blockquote><p><strong><em>2023-03-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.07646v3><strong>Quantifying Memorization Across Neural Language Models</strong></a></p><p><em>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models (LMs) have been shown to memorize parts of theirtraining data, and when prompted appropriately, they will emit the memorizedtraining data verbatim. This is undesirable because memorization violatesprivacy (exposing user data), degrades utility (repeated easy-to-memorize textis often low quality), and hurts fairness (some texts are memorized overothers). We describe three log-linear relationships that quantify the degree to whichLMs emit memorized training data. Memorization significantly grows as weincrease (1) the capacity of a model, (2) the number of times an example hasbeen duplicated, and (3) the number of tokens of context used to prompt themodel. Surprisingly, we find the situation becomes more complicated whengeneralizing these results across model families. On the whole, we find thatmemorization in LMs is more prevalent than previously believed and will likelyget worse as models continues to scale, at least without active mitigations.</div></details><blockquote><p><strong><em>2023-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.00171v1><strong>DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction</strong></a></p><p><em>Raviteja Anantha, Kriti Bhasin, Daniela de la Parra Aguilar, Prabal Vashisht, Becci Williamson, Srinivas Chappidi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Personal Digital Assistants (PDAs) - such as Siri, Alexa and GoogleAssistant, to name a few - play an increasingly important role to accessinformation and complete tasks spanning multiple domains, and by diverse groupsof users. A text-to-speech (TTS) module allows PDAs to interact in a natural,human-like manner, and play a vital role when the interaction involves peoplewith visual impairments or other disabilities. To cater to the needs of adiverse set of users, inclusive TTS is important to recognize and pronouncecorrectly text in different languages and dialects. Despite great progress inspeech synthesis, the pronunciation accuracy of named entities in amulti-lingual setting still has a large room for improvement. Existingapproaches to correct named entity (NE) mispronunciations, like retrainingGrapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciationdictionary, require expensive annotation of the ground truth pronunciation,which is also time consuming. In this work, we present a highly-precise,PDA-compatible pronunciation learning framework for the task of TTSmispronunciation detection and correction. In addition, we also propose a novelmispronunciation detection model called DTW-SiameseNet, which employs metriclearning with a Siamese architecture for Dynamic Time Warping (DTW) withtriplet loss. We demonstrate that a locale-agnostic, privacy-preservingsolution to the problem of TTS mispronunciation detection is feasible. Weevaluate our approach on a real-world dataset, and a corpus of NEpronunciations of an anonymized audio dataset of person names recorded byparticipants from 10 different locales. Human evaluation shows our proposedapproach improves pronunciation accuracy on average by ~6% compared to strongphoneme-based and audio-based baselines.</div></details><blockquote><p><strong><em>2023-02-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.13681v2><strong>The (ab)use of Open Source Code to Train Large Language Models</strong></a></p><p><em>Ali Al-Kaswan, Maliheh Izadi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In recent years, Large Language Models (LLMs) have gained significantpopularity due to their ability to generate human-like text and their potentialapplications in various fields, such as Software Engineering. LLMs for Code arecommonly trained on large unsanitized corpora of source code scraped from theInternet. The content of these datasets is memorized and emitted by the models,often in a verbatim manner. In this work, we will discuss the security,privacy, and licensing implications of memorization. We argue why the use ofcopyleft code to train LLMs is a legal and ethical dilemma. Finally, we providefour actionable recommendations to address this issue.</div></details><blockquote><p><strong><em>2023-02-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2303.17511v1><strong>On pitfalls (and advantages) of sophisticated large language models</strong></a></p><p><em>Anna Strasser</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Natural language processing based on large language models (LLMs) is abooming field of AI research. After neural networks have proven to outperformhumans in games and practical domains based on pattern recognition, we mightstand now at a road junction where artificial entities might eventually enterthe realm of human communication. However, this comes with serious risks. Dueto the inherent limitations regarding the reliability of neural networks,overreliance on LLMs can have disruptive consequences. Since it will beincreasingly difficult to distinguish between human-written andmachine-generated text, one is confronted with new ethical challenges. Thisbegins with the no longer undoubtedly verifiable human authorship and continueswith various types of fraud, such as a new form of plagiarism. This alsoconcerns the violation of privacy rights, the possibility of circulatingcounterfeits of humans, and, last but not least, it makes a massive spread ofmisinformation possible.</div></details><blockquote><p><strong><em>2023-02-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.09042v2><strong>Privately Customizing Prefinetuning to Better Match User Data in Federated Learning</strong></a></p><p><em>Charlie Hou, Hongyuan Zhan, Akshat Shrivastava, Sid Wang, Aleksandr Livshits, Giulia Fanti, Daniel Lazar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In Federated Learning (FL), accessing private client data incurscommunication and privacy costs. As a result, FL deployments commonlyprefinetune pretrained foundation models on a (large, possibly public) datasetthat is held by the central server; they then FL-finetune the model on aprivate, federated dataset held by clients. Evaluating prefinetuning datasetquality reliably and privately is therefore of high importance. To this end, wepropose FreD (Federated Private Fr'echet Distance) &ndash; a privately computeddistance between a prefinetuning dataset and federated datasets. Intuitively,it privately computes and compares a Fr'echet distance between embeddingsgenerated by a large language model on both the central (public) dataset andthe federated private client data. To make this computation privacy-preserving,we use distributed, differentially-private mean and covariance estimators. Weshow empirically that FreD accurately predicts the best prefinetuning datasetat minimal privacy cost. Altogether, using FreD we demonstrate aproof-of-concept for a new approach in private FL training: (1) customize aprefinetuning dataset to better match user data (2) prefinetune (3) performFL-finetuning.</div></details><blockquote><p><strong><em>2023-02-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.03269v3><strong>PLACES: Prompting Language Models for Social Conversation Synthesis</strong></a></p><p><em>Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, Dilek Hakkani-Tur</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Collecting high quality conversational data can be very expensive for mostapplications and infeasible for others due to privacy, ethical, or similarconcerns. A promising direction to tackle this problem is to generate syntheticdialogues by prompting large language models. In this work, we use a small setof expert-written conversations as in-context examples to synthesize a socialconversation dataset using prompting. We perform several thorough evaluationsof our synthetic conversations compared to human-collected conversations. Thisincludes various dimensions of conversation quality with human evaluationdirectly on the synthesized conversations, and interactive human evaluation ofchatbots fine-tuned on the synthetically generated dataset. We additionallydemonstrate that this prompting approach is generalizable to multi-partyconversations, providing potential to create new synthetic data for multi-partytasks. Our synthetic multi-party conversations were rated more favorably acrossall measured dimensions compared to conversation excerpts sampled from ahuman-collected multi-party dataset.</div></details><p><a href=http://arxiv.org/abs/2302.08659v1><strong>Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling</strong></a></p><p><em>Jianing Wang, Chengyu Wang, Jun Huang, Ming Gao, Aoying Zhou</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural sequence labeling (NSL) aims at assigning labels for input languagetokens, which covers a broad range of applications, such as named entityrecognition (NER) and slot filling, etc. However, the satisfying resultsachieved by traditional supervised-based approaches heavily depend on the largeamounts of human annotation data, which may not be feasible in real-worldscenarios due to data privacy and computation efficiency issues. This paperpresents SeqUST, a novel uncertain-aware self-training framework for NSL toaddress the labeled data scarcity issue and to effectively utilize unlabeleddata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neuralnetwork (BNN) to perform uncertainty estimation at the token level and thenselect reliable language tokens from unlabeled data based on the modelconfidence and certainty. A well-designed masked sequence labeling task with anoise-robust loss supports robust training, which aims to suppress the problemof noisy pseudo labels. In addition, we develop a Gaussian-based consistencyregularization technique to further improve the model robustness onGaussian-distributed perturbed representations. This effectively alleviates theover-fitting dilemma originating from pseudo-labeled augmented data. Extensiveexperiments over six benchmarks demonstrate that our SeqUST frameworkeffectively improves the performance of self-training, and consistentlyoutperforms strong baselines by a large margin in low-resource scenarios</div></details><blockquote><p><strong><em>2023-02-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.07735v1><strong>Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge</strong></a></p><p><em>Ali Al-Kaswan, Maliheh Izadi, Arie van Deursen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Previous work has shown that Large Language Models are susceptible toso-called data extraction attacks. This allows an attacker to extract a samplethat was contained in the training data, which has massive privacyimplications. The construction of data extraction attacks is challenging,current attacks are quite inefficient, and there exists a significant gap inthe extraction capabilities of untargeted attacks and memorization. Thus,targeted attacks are proposed, which identify if a given sample from thetraining data, is extractable from a model. In this work, we apply a targeteddata extraction attack to the SATML2023 Language Model Training Data ExtractionChallenge. We apply a two-step approach. In the first step, we maximise therecall of the model and are able to extract the suffix for 69% of the samples.In the second step, we use a classifier-based Membership Inference Attack onthe generations. Our AutoSklearn classifier achieves a precision of 0.841. Thefull approach reaches a score of 0.405 recall at a 10% false positive rate,which is an improvement of 34% over the baseline of 0.301.</div></details><blockquote><p><strong><em>2023-02-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.02094v2><strong>Chat2VIS: Generating Data Visualisations via Natural Language using ChatGPT, Codex and GPT-3 Large Language Models</strong></a></p><p><em>Paula Maddigan, Teo Susnjak</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The field of data visualisation has long aimed to devise solutions forgenerating visualisations directly from natural language text. Research inNatural Language Interfaces (NLIs) has contributed towards the development ofsuch techniques. However, the implementation of workable NLIs has always beenchallenging due to the inherent ambiguity of natural language, as well as inconsequence of unclear and poorly written user queries which pose problems forexisting language models in discerning user intent. Instead of pursuing theusual path of developing new iterations of language models, this study uniquelyproposes leveraging the advancements in pre-trained large language models(LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directlyinto code for appropriate visualisations. This paper presents a novel system,Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrateshow, with effective prompt engineering, the complex problem of languageunderstanding can be solved more efficiently, resulting in simpler and moreaccurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMstogether with the proposed prompts offer a reliable approach to renderingvisualisations from natural language queries, even when queries are highlymisspecified and underspecified. This solution also presents a significantreduction in costs for the development of NLI systems, while attaining greatervisualisation inference abilities compared to traditional NLP approaches thatuse hand-crafted grammar rules and tailored models. This study also presentshow LLM prompts can be constructed in a way that preserves data security andprivacy while being generalisable to different datasets. This work compares theperformance of GPT-3, Codex and ChatGPT across a number of case studies andcontrasts the performances with prior studies.</div></details><blockquote><p><strong><em>2023-02-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2302.04870v1><strong>Offsite-Tuning: Transfer Learning without Full Model</strong></a></p><p><em>Guangxuan Xiao, Ji Lin, Song Han</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Transfer learning is important for foundation models to adapt to downstreamtasks. However, many foundation models are proprietary, so users must sharetheir data with model owners to fine-tune the models, which is costly and raiseprivacy concerns. Moreover, fine-tuning large foundation models iscomputation-intensive and impractical for most downstream users. In this paper,we propose Offsite-Tuning, a privacy-preserving and efficient transfer learningframework that can adapt billion-parameter foundation models to downstream datawithout access to the full model. In offsite-tuning, the model owner sends alight-weight adapter and a lossy compressed emulator to the data owner, whothen fine-tunes the adapter on the downstream data with the emulator&rsquo;sassistance. The fine-tuned adapter is then returned to the model owner, whoplugs it into the full model to create an adapted foundation model.Offsite-tuning preserves both parties&rsquo; privacy and is computationally moreefficient than the existing fine-tuning methods that require access to the fullmodel weights. We demonstrate the effectiveness of offsite-tuning on variouslarge language and vision foundation models. Offsite-tuning can achievecomparable accuracy as full model fine-tuning while being privacy-preservingand efficient, achieving 6.5x speedup and 5.6x memory reduction. Code isavailable at <a href=https://github.com/mit-han-lab/offsite-tuning>https://github.com/mit-han-lab/offsite-tuning</a>.</div></details><blockquote><p><strong><em>2023-01-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1602.05629v4><strong>Communication-Efficient Learning of Deep Networks from Decentralized Data</strong></a></p><p><em>H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag√ºera y Arcas</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksbased on iterative model averaging, and conduct an extensive empiricalevaluation, considering five different model architectures and four datasets.These experiments demonstrate the approach is robust to the unbalanced andnon-IID data distributions that are a defining characteristic of this setting.Communication costs are the principal constraint, and we show a reduction inrequired communication rounds by 10-100x as compared to synchronized stochasticgradient descent.</div></details><blockquote><p><strong><em>2023-01-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2208.12268v3><strong>FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning</strong></a></p><p><em>Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, Gongshen Liu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) has enabled global model training on decentralizeddata in a privacy-preserving way by aggregating model updates. However, formany natural language processing (NLP) tasks that utilize pre-trained languagemodels (PLMs) with large numbers of parameters, there are considerablecommunication costs associated with FL. Recently, prompt tuning, which tunessome soft prompts without modifying PLMs, has achieved excellent performance asa new learning paradigm. Therefore we want to combine the two methods andexplore the effect of prompt tuning under FL. In this paper, we propose"FedPrompt" to study prompt tuning in a model split aggregation way using FL,and prove that split aggregation greatly reduces the communication cost, only0.01% of the PLMs&rsquo; parameters, with little decrease on accuracy both on IID andNon-IID data distribution. This improves the efficiency of FL method while alsoprotecting the data privacy in prompt tuning. In addition, like PLMs, promptsare uploaded and downloaded between public platforms and personal users, so wetry to figure out whether there is still a backdoor threat using only softprompts in FL scenarios. We further conduct backdoor attacks by data poisoningon FedPrompt. Our experiments show that normal backdoor attack can not achievea high attack success rate, proving the robustness of FedPrompt. We hope thiswork can promote the application of prompt in FL and raise the awareness of thepossible security threats.</div></details><blockquote><p><strong><em>2022-12-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.06539v3><strong>Deduplicating Training Data Mitigates Privacy Risks in Language Models</strong></a></p><p><em>Nikhil Kandpal, Eric Wallace, Colin Raffel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Past work has shown that large language models are susceptible to privacyattacks, where adversaries generate sequences from a trained model and detectwhich sequences are memorized from the training set. In this work, we show thatthe success of these attacks is largely due to duplication in commonly usedweb-scraped training sets. We first show that the rate at which language modelsregenerate training sequences is superlinearly related to a sequence&rsquo;s count inthe training set. For instance, a sequence that is present 10 times in thetraining data is on average generated ~1000 times more often than a sequencethat is present only once. We next show that existing methods for detectingmemorized sequences have near-chance accuracy on non-duplicated trainingsequences. Finally, we find that after applying methods to deduplicate trainingdata, language models are considerably more secure against these types ofprivacy attacks. Taken together, our results motivate an increased focus ondeduplication in privacy-sensitive applications and a reevaluation of thepracticality of existing privacy attacks.</div></details><blockquote><p><strong><em>2022-12-16</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2212.08619v1><strong>Planting and Mitigating Memorized Content in Predictive-Text Language Models</strong></a></p><p><em>C. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Language models are widely deployed to provide automatic text completionservices in user products. However, recent research has revealed that languagemodels (especially large ones) bear considerable risk of memorizing privatetraining data, which is then vulnerable to leakage and extraction byadversaries. In this study, we test the efficacy of a range ofprivacy-preserving techniques to mitigate unintended memorization of sensitiveuser text, while varying other factors such as model size and adversarialconditions. We test both &ldquo;heuristic&rdquo; mitigations (those without formal privacyguarantees) and Differentially Private training, which provides provable levelsof privacy at the cost of some model performance. Our experiments show that(with the exception of L2 regularization), heuristic mitigations are largelyineffective in preventing memorization in our test suite, possibly because theymake too strong of assumptions about the characteristics that define"sensitive" or &ldquo;private&rdquo; text. In contrast, Differential Privacy reliablyprevents memorization in our experiments, despite its computational andmodel-performance costs.</div></details><p><a href=http://arxiv.org/abs/2212.08354v1><strong>FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP Tasks</strong></a></p><p><em>Weilong Dong, Xinwei Wu, Junzhuo Li, Shuangzhi Wu, Chao Bian, Deyi Xiong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Massively multi-task learning with large language models has recently madesubstantial progress on few-shot generalization. However, this is usuallyperformed in a centralized learning fashion, ignoring the privacy sensitivityissue of (annotated) data used in multiple tasks. To mitigate this issue, wepropose FewFedWeight, a few-shot federated learning framework across multipletasks, to achieve the best of both worlds: privacy preservation and cross-taskgeneralization. FewFedWeight trains client models in isolated devices withoutsharing data. It broadcasts the global model in the server to each client andproduces pseudo data for clients so that knowledge from the global model can beexplored to enhance few-shot learning of each client model. An energy-basedalgorithm is further proposed to weight pseudo samples in order to reduce thenegative impact of noise from the generated pseudo data. Adaptive model weightsof client models are also tuned according to their performance. We use thesemodel weights to dynamically aggregate client models to update the globalmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantlyimprove the performance of client models on 61% tasks with an averageperformance improvement rate of 30.5% over the baseline and substantiallyoutperform FedAvg and other decentralized learning methods.</div></details><blockquote><p><strong><em>2022-12-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.00965v2><strong>Semantics-Preserved Distortion for Personal Privacy Protection in Information Management</strong></a></p><p><em>Jiajia Li, Letian Peng, Ping Wang, Zuchao Li, Xueyi Li, Hai Zhao</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although machine learning and especially deep learning methods have played animportant role in the field of information management, privacy protection is animportant and concerning topic in current machine learning models. Ininformation management field, a large number of texts containing personalinformation are produced by users every day. As the model training oninformation from users is likely to invade personal privacy, many methods havebeen proposed to block the learning and memorizing of the sensitive data in rawtexts. In this paper, we try to do this more linguistically via distorting thetext while preserving the semantics. In practice, we leverage a recently ourproposed metric, Neighboring Distribution Divergence, to evaluate the semanticpreservation during the distortion. Based on the metric, we propose twoframeworks for semantics-preserved distortion, a generative one and asubstitutive one. We conduct experiments on named entity recognition,constituency parsing, and machine reading comprehension tasks. Results from ourexperiments show the plausibility and efficiency of our distortion as a methodfor personal privacy protection. Moreover, we also evaluate the attributeattack on three privacy-related tasks in the current natural languageprocessing field, and the results show the simplicity and effectiveness of ourdata-based improvement approach compared to the structural improvementapproach. Further, we also investigate the effects of privacy protection inspecific medical information management in this work and show that the medicalinformation pre-training model using our approach can effectively reduce thememory of patients and symptoms, which fully demonstrates the practicality ofour approach.</div></details><blockquote><p><strong><em>2022-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.14739v2><strong>A Case for Business Process-Specific Foundation Models</strong></a></p><p><em>Yara Rizk, Praveen Venkateswaran, Vatche Isahagian, Vinod Muthusamy</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The inception of large language models has helped advance state-of-the-artperformance on numerous natural language tasks. This has also opened the doorfor the development of foundation models for other domains and data modalitiessuch as images, code, and music. In this paper, we argue that business processdata representations have unique characteristics that warrant the developmentof a new class of foundation models to handle tasks like process mining,optimization, and decision making. These models should also tackle the uniquechallenges of applying AI to business processes which include data scarcity,multi-modal representations, domain specific terminology, and privacy concerns.</div></details><blockquote><p><strong><em>2022-11-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.12360v1><strong>GDPR Compliant Collection of Therapist-Patient-Dialogues</strong></a></p><p><em>Tobias Mayer, Neha Warikoo, Oliver Grimm, Andreas Reif, Iryna Gurevych</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: According to the Global Burden of Disease list provided by the World HealthOrganization (WHO), mental disorders are among the most debilitatingdisorders.To improve the diagnosis and the therapy effectiveness in recentyears, researchers have tried to identify individual biomarkers. Gatheringneurobiological data however, is costly and time-consuming. Another potentialsource of information, which is already part of the clinical routine, aretherapist-patient dialogues. While there are some pioneering worksinvestigating the role of language as predictors for various therapeuticparameters, for example patient-therapist alliance, there are no large-scalestudies. A major obstacle to conduct these studies is the availability ofsizeable datasets, which are needed to train machine learning models. Whilethese conversations are part of the daily routine of clinicians, gathering themis usually hindered by various ethical (purpose of data usage), legal (dataprivacy) and technical (data formatting) limitations. Some of these limitationsare particular to the domain of therapy dialogues, like the increaseddifficulty in anonymisation, or the transcription of the recordings. In thispaper, we elaborate on the challenges we faced in starting our collection oftherapist-patient dialogues in a psychiatry clinic under the General DataPrivacy Regulation of the European Union with the goal to use the data forNatural Language Processing (NLP) research. We give an overview of each step inour procedure and point out the potential pitfalls to motivate further researchin this field.</div></details><blockquote><p><strong><em>2022-11-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.08248v2><strong>A Closer Look at the Calibration of Differentially Private Learners</strong></a></p><p><em>Hanlin Zhang, Xuechen Li, Prithviraj Sen, Salim Roukos, Tatsunori Hashimoto</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We systematically study the calibration of classifiers trained withdifferentially private stochastic gradient descent (DP-SGD) and observemiscalibration across a wide range of vision and language tasks. Our analysisidentifies per-example gradient clipping in DP-SGD as a major cause ofmiscalibration, and we show that existing approaches for improving calibrationwith differential privacy only provide marginal improvements in calibrationerror while occasionally causing large degradations in accuracy. As a solution,we show that differentially private variants of post-processing calibrationmethods such as temperature scaling and Platt scaling are surprisinglyeffective and have negligible utility cost to the overall model. Across 7tasks, temperature scaling and Platt scaling with DP-SGD result in an average3.1-fold reduction in the in-domain expected calibration error and only incurat most a minor percent drop in accuracy.</div></details><blockquote><p><strong><em>2022-11-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.05679v6><strong>Large Language Models Can Be Strong Differentially Private Learners</strong></a></p><p><em>Xuechen Li, Florian Tram√®r, Percy Liang, Tatsunori Hashimoto</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Differentially Private (DP) learning has seen limited success for buildinglarge deep learning models of text, and straightforward attempts at applyingDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks haveresulted in large performance drops and high computational overhead. We showthat this performance drop can be mitigated with (1) the use of largepretrained language models; (2) non-standard hyperparameters that suit DPoptimization; and (3) fine-tuning objectives which are aligned with thepretraining procedure. With the above, we obtain NLP models that outperformstate-of-the-art DP-trained models under the same privacy budget and strongnon-private baselines &ndash; by directly fine-tuning pretrained models with DPoptimization on moderately-sized corpora. To address the computationalchallenge of running DP-SGD with large Transformers, we propose a memory savingtechnique that allows clipping in DP-SGD to run without instantiatingper-example gradients for any linear layer in the model. The technique enablesprivately training Transformers with almost the same memory cost as non-privatetraining at a modest run-time overhead. Contrary to conventional wisdom that DPoptimization fails at learning high-dimensional models (due to noise thatscales with dimension) empirical results reveal that private learning withpretrained language models doesn&rsquo;t tend to suffer from dimension-dependentperformance degradation. Code to reproduce results can be found athttps://github.com/lxuechen/private-transformers.</div></details><blockquote><p><strong><em>2022-11-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2211.02956v1><strong>Privacy-Preserving Models for Legal Natural Language Processing</strong></a></p><p><em>Ying Yin, Ivan Habernal</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Pre-training large transformer models with in-domain data improves domainadaptation and helps gain performance on the domain-specific downstream tasks.However, sharing models pre-trained on potentially sensitive data is prone toadversarial privacy attacks. In this paper, we asked to which extent we canguarantee privacy of pre-training data and, at the same time, achieve betterdownstream performance on legal tasks without the need of additional labeleddata. We extensively experiment with scalable self-supervised learning oftransformer models under the formal paradigm of differential privacy and showthat under specific training configurations we can improve downstreamperformance without sacrifying privacy protection for the in-domain data. Ourmain contribution is utilizing differential privacy for large-scalepre-training of transformer language models in the legal NLP domain, which, tothe best of our knowledge, has not been addressed before.</div></details><blockquote><p><strong><em>2022-11-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.12506v2><strong>Memorization in NLP Fine-tuning Methods</strong></a></p><p><em>Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, Taylor Berg-Kirkpatrick</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models are shown to present privacy risks through memorizationof training data, and several recent works have studied such risks for thepre-training phase. Little attention, however, has been given to thefine-tuning phase and it is not well understood how different fine-tuningmethods (such as fine-tuning the full model, the model head, and adapter)compare in terms of memorization risk. This presents increasing concern as the"pre-train and fine-tune" paradigm proliferates. In this paper, we empiricallystudy memorization of fine-tuning methods using membership inference andextraction attacks, and show that their susceptibility to attacks is verydifferent. We observe that fine-tuning the head of the model has the highestsusceptibility to attacks, whereas fine-tuning smaller adapters appears to beless vulnerable to known extraction attacks.</div></details><blockquote><p><strong><em>2022-10-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.07667v3><strong>Just Fine-tune Twice: Selective Differential Privacy for Large Language Models</strong></a></p><p><em>Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, Zhou Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Protecting large language models from privacy leakage is becomingincreasingly crucial with their wide adoption in real-world products. Yetapplying differential privacy (DP), a canonical notion with provable privacyguarantees for machine learning models, to those models remains challenging dueto the trade-off between model utility and privacy loss. Utilizing the factthat sensitive information in language data tends to be sparse, Shi et al.(2021) formalized a DP notion extension called Selective Differential Privacy(SDP) to protect only the sensitive tokens defined by a policy function.However, their algorithm only works for RNN-based models. In this paper, wedevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP forstate-of-the-art large transformer-based models. Our method is easy toimplement: it first fine-tunes the model with redacted in-domain data, and thenfine-tunes it again with the original in-domain data using a private trainingmechanism. Furthermore, we study the scenario of imperfect implementation ofpolicy functions that misses sensitive tokens and develop systematic methods tohandle it. Experiments show that our method achieves strong utility compared toprevious baselines. We also analyze the SDP privacy guarantee empirically withthe canary insertion attack.</div></details><blockquote><p><strong><em>2022-10-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.00160v4><strong>When Does Differentially Private Learning Not Suffer in High Dimensions?</strong></a></p><p><em>Xuechen Li, Daogao Liu, Tatsunori Hashimoto, Huseyin A. Inan, Janardhan Kulkarni, Yin Tat Lee, Abhradeep Guha Thakurta</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large pretrained models can be privately fine-tuned to achieve performanceapproaching that of non-private models. A common theme in these results is thesurprising observation that high-dimensional models can achieve favorableprivacy-utility trade-offs. This seemingly contradicts known results on themodel-size dependence of differentially private convex learning and raises thefollowing research question: When does the performance of differentiallyprivate learning not degrade with increasing model size? We identify that themagnitudes of gradients projected onto subspaces is a key factor thatdetermines performance. To precisely characterize this for private convexlearning, we introduce a condition on the objective that we term\emph{restricted Lipschitz continuity} and derive improved bounds for theexcess empirical and population risks that are dimension-independent underadditional conditions. We empirically show that in private fine-tuning of largelanguage models, gradients obtained during fine-tuning are mostly controlled bya few principal components. This behavior is similar to conditions under whichwe obtain dimension-independent bounds in convex settings. Our theoretical andempirical results together provide a possible explanation for recent successesin large-scale private fine-tuning. Code to reproduce our results can be foundat\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.</div></details><blockquote><p><strong><em>2022-10-25</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.14254v1><strong>Leveraging Open Data and Task Augmentation to Automated Behavioral Coding of Psychotherapy Conversations in Low-Resource Scenarios</strong></a></p><p><em>Zhuohao Chen, Nikolaos Flemotomos, Zac E. Imel, David C. Atkins, Shrikanth Narayanan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In psychotherapy interactions, the quality of a session is assessed bycodifying the communicative behaviors of participants during the conversationthrough manual observation and annotation. Developing computational approachesfor automated behavioral coding can reduce the burden on human coders andfacilitate the objective evaluation of the intervention. In the real world,however, implementing such algorithms is associated with data sparsitychallenges since privacy concerns lead to limited available in-domain data. Inthis paper, we leverage a publicly available conversation-based dataset andtransfer knowledge to the low-resource behavioral coding task by performing anintermediate language model training via meta-learning. We introduce a taskaugmentation method to produce a large number of &ldquo;analogy tasks&rdquo; - taskssimilar to the target one - and demonstrate that the proposed frameworkpredicts target behaviors more accurately than all the other baseline models.</div></details><blockquote><p><strong><em>2022-10-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.12628v2><strong>Are Large Pre-Trained Language Models Leaking Your Personal Information?</strong></a></p><p><em>Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Are Large Pre-Trained Language Models Leaking Your Personal Information? Inthis paper, we analyze whether Pre-Trained Language Models (PLMs) are prone toleaking personal information. Specifically, we query PLMs for email addresseswith contexts of the email address or prompts containing the owner&rsquo;s name. Wefind that PLMs do leak personal information due to memorization. However, sincethe models are weak at association, the risk of specific personal informationbeing extracted by attackers is low. We hope this work could help the communityto better understand the privacy risk of PLMs and bring new insights to makePLMs safe.</div></details><blockquote><p><strong><em>2022-10-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.08514v2><strong>Recovering Private Text in Federated Learning of Language Models</strong></a></p><p><em>Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, Danqi Chen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning allows distributed users to collaboratively train a modelwhile keeping each user&rsquo;s data private. Recently, a growing body of work hasdemonstrated that an eavesdropping attacker can effectively recover image datafrom gradients transmitted during federated learning. However, little progresshas been made in recovering text data. In this paper, we present a novel attackmethod FILM for federated learning of language models (LMs). For the firsttime, we show the feasibility of recovering text from large batch sizes of upto 128 sentences. Unlike image-recovery methods that are optimized to matchgradients, we take a distinct approach that first identifies a set of wordsfrom gradients and then directly reconstructs sentences based on beam searchand a prior-based reordering strategy. We conduct the FILM attack on severallarge-scale datasets and show that it can successfully reconstruct singlesentences with high fidelity for large batch sizes and even multiple sentencesif applied iteratively. We evaluate three defense methods: gradient pruning,DPSGD, and a simple approach to freeze word embeddings that we propose. We showthat both gradient pruning and DPSGD lead to a significant drop in utility.However, if we fine-tune a public pre-trained LM on private text withoutupdating word embeddings, it can effectively defend the attack with minimaldata utility loss. Together, we hope that our results can encourage thecommunity to rethink the privacy concerns of LM training and its standardpractices in the future.</div></details><blockquote><p><strong><em>2022-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2210.00036v2><strong>Differentially Private Bias-Term only Fine-tuning of Foundation Models</strong></a></p><p><em>Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the problem of differentially private (DP) fine-tuning of largepre-trained models &ndash; a recent privacy-preserving approach suitable for solvingdownstream tasks with sensitive data. Existing work has demonstrated that highaccuracy is possible under strong privacy constraint, yet requires significantcomputational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), whichmatches the state-of-the-art accuracy for DP algorithms and the efficiency ofthe standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the networkarchitecture), parameter efficient (only training about $0.1%$ of theparameters), and computation efficient (almost removing the overhead caused byDP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiTis $2\sim 30\times$ faster and uses $2\sim 8\times$ less memory than DP fullfine-tuning, even faster than the standard full fine-tuning. This amazingefficiency enables us to conduct DP fine-tuning on language and vision taskswith long-sequence texts and high-resolution images, which were computationallydifficult using existing methods.</div></details><blockquote><p><strong><em>2022-09-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.13621v2><strong>Differentially Private Decoding in Large Language Models</strong></a></p><p><em>Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami Smaili, Rahul Gupta, Richard Zemel</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent large-scale natural language processing (NLP) systems use apre-trained Large Language Model (LLM) on massive and diverse corpora as aheadstart. In practice, the pre-trained model is adapted to a wide array oftasks via fine-tuning on task-specific datasets. LLMs, while effective, havebeen shown to memorize instances of training data thereby potentially revealingprivate information processed during pre-training. The potential leakage mightfurther propagate to the downstream tasks for which LLMs are fine-tuned. On theother hand, privacy-preserving algorithms usually involve retraining fromscratch, which is prohibitively expensive for LLMs. In this work, we propose asimple, easy to interpret, and computationally lightweight perturbationmechanism to be applied to an already trained model at the decoding stage. Ourperturbation mechanism is model-agnostic and can be used in conjunction withany LLM. We provide theoretical analysis showing that the proposed mechanism isdifferentially private, and experimental results showing a privacy-utilitytrade-off.</div></details><blockquote><p><strong><em>2022-07-18</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2207.08988v1><strong>Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices</strong></a></p><p><em>Mingbin Xu, Congzheng Song, Ye Tian, Neha Agrawal, Filip Granqvist, Rogier van Dalen, Xiao Zhang, Arturo Argueta, Shiyi Han, Yaqiao Deng, Leo Liu, Anmol Walia, Alex Jin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning (FL) is a technique to train models using data distributedacross devices. Differential Privacy (DP) provides a formal privacy guaranteefor sensitive data. Our goal is to train a large neural network language model(NNLM) on compute-constrained devices while preserving privacy using FL and DP.However, the DP-noise introduced to the model increases as the model sizegrows, which often prevents convergence. We propose Partial Embedding Updates(PEU), a novel technique to decrease noise by decreasing payload size.Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise ContrastiveEstimation (NCE) to reduce the memory demands of large models oncompute-constrained devices. This combination of techniques makes it possibleto train large-vocabulary language models while preserving accuracy andprivacy.</div></details><blockquote><p><strong><em>2022-07-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.06500v2><strong>Differentially Private Fine-tuning of Language Models</strong></a></p><p><em>Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We give simpler, sparser, and faster algorithms for differentially privatefine-tuning of large-scale pre-trained language models, which achieve thestate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.We propose a meta-framework for this problem, inspired by the recent success ofhighly parameter-efficient methods for fine-tuning. Our experiments show thatdifferentially private adaptations of these approaches outperform previousprivate algorithms in three important dimensions: utility, privacy, and thecomputational and memory cost of private training. On many commonly studieddatasets, the utility of private models approaches that of non-private models.For example, on the MNLI dataset we achieve an accuracy of $87.8%$ usingRoBERTa-Large and $83.5%$ using RoBERTa-Base with a privacy budget of$\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Largeachieves an accuracy of $90.2%$. Our findings are similar for natural languagegeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8respectively (privacy budget of $\epsilon = 6.8,\delta=$ 1e-5) whereas thenon-private baseline is $48.1$. All our experiments suggest that larger modelsare better suited for private fine-tuning: while they are well known to achievesuperior accuracy non-privately, we find that they also better maintain theiraccuracy when privacy is introduced.</div></details><blockquote><p><strong><em>2022-06-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.05854v5><strong>NegDL: Privacy-Preserving Deep Learning Based on Negative Database</strong></a></p><p><em>Dongdong Zhao, Pingchuan Zhang, Jianwen Xiang, Jing Tian</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In the era of big data, deep learning has become an increasingly populartopic. It has outstanding achievements in the fields of image recognition,object detection, and natural language processing et al. The first priority ofdeep learning is exploiting valuable information from a large amount of data,which will inevitably induce privacy issues that are worthy of attention.Presently, several privacy-preserving deep learning methods have been proposed,but most of them suffer from a non-negligible degradation of either efficiencyor accuracy. Negative database (\textit{NDB}) is a new type of datarepresentation which can protect data privacy by storing and utilizing thecomplementary form of original data. In this paper, we propose aprivacy-preserving deep learning method named NegDL based on \textit{NDB}.Specifically, private data are first converted to \textit{NDB} as the input ofdeep learning models by a generation algorithm called \textit{QK}-hiddenalgorithm, and then the sketches of \textit{NDB} are extracted for training andinference. We demonstrate that the computational complexity of NegDL is thesame as the original deep learning model without privacy protection.Experimental results on Breast Cancer, MNIST, and CIFAR-10 benchmark datasetsdemonstrate that the accuracy of NegDL could be comparable to the original deeplearning model in most cases, and it performs better than the method based ondifferential privacy.</div></details><blockquote><p><strong><em>2022-06-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.01863v2><strong>Provably Confidential Language Modelling</strong></a></p><p><em>Xuandong Zhao, Lei Li, Yu-Xiang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large language models are shown to memorize privacy information such associal security numbers in training data. Given the sheer scale of the trainingcorpus, it is challenging to screen and filter these privacy data, eithermanually or automatically. In this paper, we propose Confidentially RedactedTraining (CRT), a method to train language generation models while protectingthe confidential segments. We borrow ideas from differential privacy (whichsolves a related but distinct problem) and show that our method is able toprovably prevent unintended memorization by randomizing parts of the trainingprocess. Moreover, we show that redaction with an approximately correctscreening policy amplifies the confidentiality guarantee. We implement themethod for both LSTM and GPT language models. Our experimental results showthat the models trained by CRT obtain almost the same perplexity whilepreserving strong confidentiality.</div></details><blockquote><p><strong><em>2022-06-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2206.01838v1><strong>Differentially Private Model Compression</strong></a></p><p><em>Fatemehsadat Mireshghallah, Arturs Backurs, Huseyin A Inan, Lukas Wutschitz, Janardhan Kulkarni</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent papers have shown that large pre-trained language models (LLMs) suchas BERT, GPT-2 can be fine-tuned on private data to achieve performancecomparable to non-private models for many downstream Natural LanguageProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.The inference cost of these models &ndash; which consist of hundreds of millions ofparameters &ndash; however, can be prohibitively large. Hence, often in practice,LLMs are compressed before they are deployed in specific applications. In thispaper, we initiate the study of differentially private model compression andpropose frameworks for achieving 50% sparsity levels while maintaining nearlyfull performance. We demonstrate these ideas on standard GLUE benchmarks usingBERT models, setting benchmarks for future research on this topic.</div></details><blockquote><p><strong><em>2022-05-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.14727v1><strong>CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI</strong></a></p><p><em>Yirong Chen, Weiquan Fan, Xiaofen Xing, Jianxin Pang, Minlie Huang, Wenjing Han, Qianfeng Tie, Xiangmin Xu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Human language expression is based on the subjective construal of thesituation instead of the objective truth conditions, which means that speakers&rsquo;personalities and emotions after cognitive processing have an importantinfluence on conversation. However, most existing datasets for conversationalAI ignore human personalities and emotions, or only consider part of them. It&rsquo;sdifficult for dialogue systems to understand speakers&rsquo; personalities andemotions although large-scale pre-training language models have been widelyused. In order to consider both personalities and emotions in the process ofconversation generation, we propose CPED, a large-scale Chinese personalizedand emotional dialogue dataset, which consists of multi-source knowledgerelated to empathy and personal characteristic. These knowledge covers gender,Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPEDcontains more than 12K dialogues of 392 speakers from 40 TV shows. We releasethe textual dataset with audio features and video features according to thecopyright claims, privacy issues, terms of service of video platforms. Weprovide detailed description of the CPED construction process and introducethree tasks for conversational AI, including personality recognition, emotionrecognition in conversations as well as personalized and emotional conversationgeneration. Finally, we provide baseline systems for these tasks and considerthe function of speakers&rsquo; personalities and emotions on conversation. Ourmotivation is to propose a dataset to be widely adopted by the NLP community asa new open benchmark for conversational AI research. The full dataset isavailable at <a href=https://github.com/scutcyr/CPED>https://github.com/scutcyr/CPED</a>.</div></details><blockquote><p><strong><em>2022-05-19</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.09470v1><strong>Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters</strong></a></p><p><em>Yang Xiang, Zhihua Wu, Weibao Gong, Siyu Ding, Xianjie Mo, Yuang Liu, Shuohuan Wang, Peng Liu, Yongshuai Hou, Long Li, Bin Wang, Shaohuai Shi, Yaqian Han, Yue Yu, Ge Li, Yu Sun, Yanjun Ma, Dianhai Yu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ever-growing model size and scale of compute have attracted increasinginterests in training deep learning models over multiple nodes. However, whenit comes to training on cloud clusters, especially across remote clusters, hugechallenges are faced. In this work, we introduce a general framework, Nebula-I,for collaboratively training deep learning models over remote heterogeneousclusters, the connections between which are low-bandwidth wide area networks(WANs). We took natural language processing (NLP) as an example to show howNebula-I works in different training phases that include: a) pre-training amultilingual language model using two remote clusters; and b) fine-tuning amachine translation model using knowledge distilled from pre-trained models,which run through the most popular paradigm of recent deep learning. To balancethe accuracy and communication efficiency, in Nebula-I, parameter-efficienttraining strategies, hybrid parallel computing methods and adaptivecommunication acceleration techniques are jointly applied. Meanwhile, securitystrategies are employed to guarantee the safety, reliability and privacy inintra-cluster computation and inter-cluster communication. Nebula-I isimplemented with the PaddlePaddle deep learning framework, which can supportcollaborative training over heterogeneous hardware, e.g. GPU and NPU.Experiments demonstrate that the proposed framework could substantiallymaximize the training efficiency while preserving satisfactory NLP performance.By using Nebula-I, users can run large-scale training tasks over cloud clusterswith minimum developments, and the utility of existed large pre-trained modelscould be further promoted. We also introduced new state-of-the-art results oncross-lingual natural language inference tasks, which are generated based upona novel learning framework and Nebula-I.</div></details><blockquote><p><strong><em>2022-05-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2107.08661v5><strong>Translatotron 2: High-quality direct speech-to-speech translation with voice preservation</strong></a></p><p><em>Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We present Translatotron 2, a neural direct speech-to-speech translationmodel that can be trained end-to-end. Translatotron 2 consists of a speechencoder, a linguistic decoder, an acoustic synthesizer, and a single attentionmodule that connects them together. Experimental results on three datasetsconsistently show that Translatotron 2 outperforms the original Translatotronby a large margin on both translation quality (up to +15.5 BLEU) and speechgeneration quality, and approaches the same of cascade systems. In addition, wepropose a simple method for preserving speakers&rsquo; voices from the source speechto the translation speech in a different language. Unlike existing approaches,the proposed method is able to preserve each speaker&rsquo;s voice on speaker turnswithout requiring for speaker segmentation. Furthermore, compared to existingapproaches, it better preserves speaker&rsquo;s privacy and mitigates potentialmisuse of voice cloning for creating spoofing audio artifacts.</div></details><blockquote><p><strong><em>2022-05-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.03824v1><strong>A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges</strong></a></p><p><em>Zhenghua Chen, Min Wu, Alvin Chan, Xiaoli Li, Yew-Soon Ong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Artificial Intelligence (AI) is a fast-growing research and development (R&amp;D)discipline which is attracting increasing attention because of its promises tobring vast benefits for consumers and businesses, with considerable benefitspromised in productivity growth and innovation. To date it has reportedsignificant accomplishments in many areas that have been deemed as challengingfor machines, ranging from computer vision, natural language processing, audioanalysis to smart sensing and many others. The technical trend in realizing thesuccesses has been towards increasing complex and large size AI models so as tosolve more complex problems at superior performance and robustness. This rapidprogress, however, has taken place at the expense of substantial environmentalcosts and resources. Besides, debates on the societal impacts of AI, such asfairness, safety and privacy, have continued to grow in intensity. These issueshave presented major concerns pertaining to the sustainable development of AI.In this work, we review major trends in machine learning approaches that canaddress the sustainability problem of AI. Specifically, we examine emerging AImethodologies and algorithms for addressing the sustainability issue of AI intwo major aspects, i.e., environmental sustainability and social sustainabilityof AI. We will also highlight the major limitations of existing studies andpropose potential research challenges and directions for the development ofnext generation of sustainable AI techniques. We believe that this technicalreview can help to promote a sustainable development of AI R&amp;D activities forthe research community.</div></details><blockquote><p><strong><em>2022-05-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.08815v3><strong>FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks</strong></a></p><p><em>Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Increasing concerns and regulations about data privacy and sparsitynecessitate the study of privacy-preserving, decentralized learning methods fornatural language processing (NLP) tasks. Federated learning (FL) providespromising approaches for a large number of clients (e.g., personal devices ororganizations) to collaboratively learn a shared global model to benefit allclients while allowing users to keep their data locally. Despite interest instudying FL methods for NLP tasks, a systematic comparison and analysis islacking in the literature. Herein, we present the FedNLP, a benchmarkingframework for evaluating federated learning methods on four different taskformulations: text classification, sequence tagging, question answering, andseq2seq. We propose a universal interface between Transformer-based languagemodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) undervarious non-IID partitioning strategies. Our extensive experiments with FedNLPprovide empirical comparisons between FL methods and helps us better understandthe inherent challenges of this direction. The comprehensive analysis points tointriguing and exciting future research aimed at developing FL methods for NLPtasks.</div></details><blockquote><p><strong><em>2022-04-26</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2205.10228v1><strong>You Don&rsquo;t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers&rsquo; Private Personas</strong></a></p><p><em>Haoran Li, Yangqiu Song, Lixin Fan</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Social chatbots, also known as chit-chat chatbots, evolve rapidly with largepretrained language models. Despite the huge progress, privacy concerns havearisen recently: training data of large language models can be extracted viamodel inversion attacks. On the other hand, the datasets used for trainingchatbots contain many private conversations between two individuals. In thiswork, we further investigate the privacy leakage of the hidden states ofchatbots trained by language modeling which has not been well studied yet. Weshow that speakers&rsquo; personas can be inferred through a simple neural networkwith high accuracy. To this end, we propose effective defense objectives toprotect persona leakage from hidden states. We conduct extensive experiments todemonstrate that our proposed defense objectives can greatly reduce the attackaccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preservelanguage models&rsquo; powerful generation ability.</div></details><blockquote><p><strong><em>2022-04-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.09391v1><strong>You Are What You Write: Preserving Privacy in the Era of Large Language Models</strong></a></p><p><em>Richard Plant, Valerio Giuffrida, Dimitra Gkatzia</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large scale adoption of large language models has introduced a new era ofconvenient knowledge transfer for a slew of natural language processing tasks.However, these models also run the risk of undermining user trust by exposingunwanted information about the data subjects, which may be extracted by amalicious party, e.g. through adversarial attacks. We present an empiricalinvestigation into the extent of the personal information encoded intopre-trained representations by a range of popular models, and we show apositive correlation between the complexity of a model, the amount of data usedin pre-training, and data leakage. In this paper, we present the first widecoverage evaluation and comparison of some of the most popularprivacy-preserving algorithms, on a large, multi-lingual dataset on sentimentanalysis annotated with demographic information (location, age and gender). Theresults show since larger and more complex models are more prone to leakingprivate information, use of privacy-preserving methods is highly desirable. Wealso find that highly privacy-preserving technologies like differential privacy(DP) can have serious model utility effects, which can be ameliorated usinghybrid or metric-DP techniques.</div></details><blockquote><p><strong><em>2022-04-06</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2002.09745v2><strong>Differentially Private Set Union</strong></a></p><p><em>Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, Sergey Yekhanin</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We study the basic operation of set union in the global model of differentialprivacy. In this problem, we are given a universe $U$ of items, possibly ofinfinite size, and a database $D$ of users. Each user $i$ contributes a subset$W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentiallyprivate algorithm which outputs a subset $S \subset \cup_i W_i$ such that thesize of $S$ is as large as possible. The problem arises in countless real worldapplications; it is particularly ubiquitous in natural language processing(NLP) applications as vocabulary extraction. For example, discovering words,sentences, $n$-grams etc., from private text data belonging to users is aninstance of the set union problem. Known algorithms for this problem proceed by collecting a subset of itemsfrom each user, taking the union of such subsets, and disclosing the itemswhose noisy counts fall above a certain threshold. Crucially, in the aboveprocess, the contribution of each individual user is always independent of theitems held by other users, resulting in a wasteful aggregation process, wheresome item counts happen to be way above the threshold. We deviate from theabove paradigm by allowing users to contribute their items in a$\textit{dependent fashion}$, guided by a $\textit{policy}$. In this newsetting ensuring privacy is significantly delicate. We prove that any policywhich has certain $\textit{contractive}$ properties would result in adifferentially private algorithm. We design two new algorithms, one usingLaplace noise and other Gaussian noise, as specific instances of policiessatisfying the contractive properties. Our experiments show that the newalgorithms significantly outperform previously known mechanisms for theproblem.</div></details><blockquote><p><strong><em>2022-03-28</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2203.11481v2><strong>Mixed Differential Privacy in Computer Vision</strong></a></p><p><em>Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns, Stefano Soatto</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce AdaMix, an adaptive differentially private algorithm fortraining deep neural network classifiers using both private and public imagedata. While pre-training language models on large public datasets has enabledstrong differential privacy (DP) guarantees with minor loss of accuracy, asimilar practice yields punishing trade-offs in vision tasks. A few-shot oreven zero-shot learning baseline that ignores private data can outperformfine-tuning on a large private dataset. AdaMix incorporates few-shot training,or cross-modal zero-shot learning, on public data prior to private fine-tuning,to improve the trade-off. AdaMix reduces the error increase from thenon-private upper bound from the 167-311% of the baseline, on average across 6datasets, to 68-92% depending on the desired privacy level selected by theuser. AdaMix tackles the trade-off arising in visual classification, wherebythe most privacy sensitive data, corresponding to isolated points inrepresentation space, are also critical for high classification accuracy. Inaddition, AdaMix comes with strong theoretical privacy guarantees andconvergence analysis.</div></details><blockquote><p><strong><em>2022-03-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2204.01845v1><strong>Compliance Checking with NLI: Privacy Policies vs. Regulations</strong></a></p><p><em>Amin Rabinia, Zane Nygaard</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: A privacy policy is a document that states how a company intends to handleand manage their customers&rsquo; personal data. One of the problems that arises withthese privacy policies is that their content might violate data privacyregulations. Because of the enormous number of privacy policies that exist, theonly realistic way to check for legal inconsistencies in all of them is throughan automated method. In this work, we use Natural Language Inference (NLI)techniques to compare privacy regulations against sections of privacy policiesfrom a selection of large companies. Our NLI model uses pre-trained embeddings,along with BiLSTM in its attention mechanism. We tried two versions of ourmodel: one that was trained on the Stanford Natural Language Inference (SNLI)and the second on the Multi-Genre Natural Language Inference (MNLI) dataset. Wefound that our test accuracy was higher on our model trained on the SNLI, butwhen actually doing NLI tasks on real world privacy policies, the model trainedon MNLI generalized and performed much better.</div></details><blockquote><p><strong><em>2022-02-15</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.07623v1><strong>Defending against Reconstruction Attacks with R√©nyi Differential Privacy</strong></a></p><p><em>Pierre Stock, Igor Shilov, Ilya Mironov, Alexandre Sablayrolles</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Reconstruction attacks allow an adversary to regenerate data samples of thetraining set using access to only a trained model. It has been recently shownthat simple heuristics can reconstruct data samples from language models,making this threat scenario an important aspect of model release. Differentialprivacy is a known solution to such attacks, but is often used with arelatively large privacy budget (epsilon > 8) which does not translate tomeaningful guarantees. In this paper we show that, for a same mechanism, we canderive privacy guarantees for reconstruction attacks that are better than thetraditional ones from the literature. In particular, we show that largerprivacy budgets do not protect against membership inference, but can stillprotect extraction of rare secrets. We show experimentally that our guaranteeshold against various language models, including GPT-2 finetuned onWikitext-103.</div></details><blockquote><p><strong><em>2022-02-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.05993v1><strong>Wav2Vec2.0 on the Edge: Performance Evaluation</strong></a></p><p><em>Santosh Gondi</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Wav2Vec2.0 is a state-of-the-art model which learns speech representationsthrough unlabeled speech data, aka, self supervised learning. The pretrainedmodel is then fine tuned on small amounts of labeled data to use it forspeech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformativesolution for low resource languages as it is mainly developed using unlabeledaudio data. Getting large amounts of labeled data is resource intensive andespecially challenging to do for low resource languages such as Swahilli,Tatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpassesthe very recent supervised learning algorithms while using 100x less labeleddata. Given its importance and enormous potential in enabling speech basedtasks on world&rsquo;s 7000 languages, it is key to evaluate the accuracy, latencyand efficiency of this model on low resource and low power edge devices andinvestigate the feasibility of using it in such devices for private, secure andreliable speech based tasks. On-device speech tasks preclude sending audio datato the server hence inherently providing privacy, reduced latency and enhancedreliability. In this paper, Wav2Vec2.0 model&rsquo;s accuracy and latency has beenevaluated on Raspberry Pi along with the KenLM language model for speechrecognition tasks. How to tune certain parameters to achieve desired level ofWER rate and latency while meeting the CPU, memory and energy budgets of theproduct has been discussed.</div></details><blockquote><p><strong><em>2022-02-09</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2202.04742v1><strong>FedQAS: Privacy-aware machine reading comprehension with federated learning</strong></a></p><p><em>Addi Ait-Mlouk, Sadi Alawadi, Salman Toor, Andreas Hellander</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Machine reading comprehension (MRC) of text data is one important task inNatural Language Understanding. It is a complex NLP problem with a lot ofongoing research fueled by the release of the Stanford Question AnsweringDataset (SQuAD) and Conversational Question Answering (CoQA). It is consideredto be an effort to teach computers how to &ldquo;understand&rdquo; a text, and then to beable to answer questions about it using deep learning. However, until nowlarge-scale training on private text data and knowledge sharing has beenmissing for this NLP task. Hence, we present FedQAS, a privacy-preservingmachine reading system capable of leveraging large-scale private data withoutthe need to pool those datasets in a central location. The proposed approachcombines transformer models and federated learning technologies. The system isdeveloped using the FEDn framework and deployed as a proof-of-concept allianceinitiative. FedQAS is flexible, language-agnostic, and allows intuitiveparticipation and execution of local model training. In addition, we presentthe architecture and implementation of the system, as well as provide areference evaluation based on the SQUAD dataset, to showcase how it overcomesdata privacy issues and enables knowledge sharing between alliance members in aFederated learning setting.</div></details><blockquote><p><strong><em>2022-01-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2201.00971v1><strong>Submix: Practical Private Prediction for Large-Scale Language Models</strong></a></p><p><em>Antonio Ginart, Laurens van der Maaten, James Zou, Chuan Guo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent data-extraction attacks have exposed that language models can memorizesome training samples verbatim. This is a vulnerability that can compromise theprivacy of the model&rsquo;s training data. In this work, we introduce SubMix: apractical protocol for private next-token prediction designed to preventprivacy violations by language models that were fine-tuned on a private corpusafter pre-training on a public corpus. We show that SubMix limits the leakageof information that is unique to any individual user in the private corpus viaa relaxation of group differentially private prediction. Importantly, SubMixadmits a tight, data-dependent privacy accounting mechanism, which allows it tothwart existing data-extraction attacks while maintaining the utility of thelanguage model. SubMix is the first protocol that maintains privacy even whenpublicly releasing tens of thousands of next-token predictions made by largetransformer-based models such as GPT-2.</div></details><blockquote><p><strong><em>2021-11-30</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2111.12182v2><strong>Identifying Terms and Conditions Important to Consumers using Crowdsourcing</strong></a></p><p><em>Xingyu Liu, Annabel Sun, Jason I. Hong</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Terms and conditions (T&amp;Cs) are pervasive on the web and often containimportant information for consumers, but are rarely read. Previous research hasexplored methods to surface alarming privacy policies using manual labelers,natural language processing, and deep learning techniques. However, this priorwork used pre-determined categories for annotations, and did not investigatewhat consumers really deem as important from their perspective. In this paper,we instead combine crowdsourcing with an open definition of &ldquo;what is important"in T&amp;Cs. We present a workflow consisting of pairwise comparisons, agreementvalidation, and Bradley-Terry rank modeling, to effectively establish rankingsof T&amp;C statements from non-expert crowdworkers on this open definition, andfurther analyzed consumers&rsquo; preferences. We applied this workflow to 1,551 T&amp;Cstatements from 27 e-commerce websites, contributed by 3,462 unique crowdworkers doing 203,068 pairwise comparisons, and conducted thematic andreadability analysis on the statements considered as important/unimportant. Wefound that consumers especially cared about policies related to after-sales andmoney, and tended to regard harder-to-understand statements as more important.We also present machine learning models to identify T&amp;C clauses that consumersconsidered important, achieving at best a 92.7% balanced accuracy, 91.6%recall, and 89.2% precision. We foresee using our workflow and model toefficiently and reliably highlight important T&amp;Cs on websites at a large scale,improving consumers&rsquo; awareness</div></details><blockquote><p><strong><em>2021-11-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2105.09680v4><strong>KLUE: Korean Language Understanding Evaluation</strong></a></p><p><em>Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, Kyunghyun Cho</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUEis a collection of 8 Korean natural language understanding (NLU) tasks,including Topic Classification, SemanticTextual Similarity, Natural LanguageInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,Machine Reading Comprehension, and Dialogue State Tracking. We build all of thetasks from scratch from diverse source corpora while respecting copyrights, toensure accessibility for anyone without any restrictions. With ethicalconsiderations in mind, we carefully design annotation protocols. Along withthe benchmark tasks and data, we provide suitable evaluation metrics andfine-tuning recipes for pretrained language models for each task. Wefurthermore release the pretrained language models (PLM), KLUE-BERT andKLUE-RoBERTa, to help reproducing baseline models on KLUE and therebyfacilitate future research. We make a few interesting observations from thepreliminary experiments using the proposed KLUE benchmark suite, alreadydemonstrating the usefulness of this new benchmark suite. First, we findKLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs andexisting open-source Korean PLMs. Second, we see minimal degradation inperformance even when we replace personally identifiable information from thepretraining corpus, suggesting that privacy and NLU capability are not at oddswith each other. Lastly, we find that using BPE tokenization in combinationwith morpheme-level pre-tokenization is effective in tasks involvingmorpheme-level tagging, detection and generation. In addition to acceleratingKorean NLP research, our comprehensive documentation on creating KLUE willfacilitate creating similar resources for other languages in the future. KLUEis available at <a href=https://klue-benchmark.com>https://klue-benchmark.com</a>.</div></details><blockquote><p><strong><em>2021-10-13</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.06419v1><strong>Federated Natural Language Generation for Personalized Dialogue System</strong></a></p><p><em>Yujie Lu, Chao Huang, Huanli Zhan, Yong Zhuang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Neural conversational models have long suffered from the problem ofinconsistency and lacking coherent personality. To address the issue,persona-based models capturing individual characteristics have been proposed,but they still face the dilemma of model adaption and data privacy. To breakthis dilemma, we propose a novel Federated Natural Language Generation (FedNLG)framework, which learns personalized representations from various dataset ondistributed devices, and thus implements the personalized dialogue systemefficiently and safely. FedNLG first pre-trains parameters of standard neuralconversational model over a large dialogue corpus, and then fine-tune the modelparameters and persona embeddings on specific datasets, in a federated manner.Thus, the model could simultaneously learn the persona embeddings in localclients and learn shared model parameters by federated aggregation, whichachieves accuracyprivacy balance. By conducting extensive experiments, wedemonstrate the effectiveness of our model by pre-training model over CornellMovie-Dialogs Corpus and fine-tuning the model over two TV series dataset.</div></details><blockquote><p><strong><em>2021-10-05</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.05688v2><strong>AI-enabled Automation for Completeness Checking of Privacy Policies</strong></a></p><p><em>Orlando Amaral, Sallam Abualhaija, Damiano Torre, Mehrdad Sabetzadeh, Lionel C. Briand</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Technological advances in information sharing have raised concerns about dataprotection. Privacy policies contain privacy-related requirements about how thepersonal data of individuals will be handled by an organization or a softwaresystem (e.g., a web service or an app). In Europe, privacy policies are subjectto compliance with the General Data Protection Regulation (GDPR). Aprerequisite for GDPR compliance checking is to verify whether the content of aprivacy policy is complete according to the provisions of GDPR. Incompleteprivacy policies might result in large fines on violating organization as wellas incomplete privacy-related software specifications. Manual completenesschecking is both time-consuming and error-prone. In this paper, we proposeAI-based automation for the completeness checking of privacy policies. Throughsystematic qualitative methods, we first build two artifacts to characterizethe privacy-related provisions of GDPR, namely a conceptual model and a set ofcompleteness criteria. Then, we develop an automated solution on top of theseartifacts by leveraging a combination of natural language processing andsupervised machine learning. Specifically, we identify the GDPR-relevantinformation content in privacy policies and subsequently check them against thecompleteness criteria. To evaluate our approach, we collected 234 real privacypolicies from the fund industry. Over a set of 48 unseen privacy policies, ourapproach detected 300 of the total of 334 violations of some completenesscriteria correctly, while producing 23 false positives. The approach thus has aprecision of 92.9% and recall of 89.8%. Compared to a baseline that applieskeyword search only, our approach results in an improvement of 24.5% inprecision and 38% in recall.</div></details><blockquote><p><strong><em>2021-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2110.01643v1><strong>Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning</strong></a></p><p><em>Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Privacy is important considering the financial Domain as such data is highlyconfidential and sensitive. Natural Language Processing (NLP) techniques can beapplied for text classification and entity detection purposes in financialdomains such as customer feedback sentiment analysis, invoice entity detection,categorisation of financial documents by type etc. Due to the sensitive natureof such data, privacy measures need to be taken for handling and training largemodels with such data. In this work, we propose a contextualized transformer(BERT and RoBERTa) based text classification model integrated with privacyfeatures such as Differential Privacy (DP) and Federated Learning (FL). Wepresent how to privately train NLP models and desirable privacy-utilitytradeoffs and evaluate them on the Financial Phrase Bank dataset.</div></details><blockquote><p><strong><em>2021-09-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2103.06819v6><strong>TAG: Gradient Attack on Transformer-based Language Models</strong></a></p><p><em>Jieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, Caiwen Ding</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Although federated learning has increasingly gained attention in terms ofeffectively utilizing local devices for data privacy enhancement, recentstudies show that publicly shared gradients in the training process can revealthe private training images (gradient leakage) to a third-party in computervision. We have, however, no systematic understanding of the gradient leakagemechanism on the Transformer based language models. In this paper, as the firstattempt, we formulate the gradient attack problem on the Transformer-basedlanguage models and propose a gradient attack algorithm, TAG, to reconstructthe local training data. We develop a set of metrics to evaluate theeffectiveness of the proposed attack algorithm quantitatively. Experimentalresults on Transformer, TinyBERT$<em>{4}$, TinyBERT$</em>{6}$, BERT$<em>{BASE}$, andBERT$</em>{LARGE}$ using GLUE benchmark show that TAG works well on more weightdistributions in reconstructing training data and achieves 1.5$\times$ recoverrate and 2.5$\times$ ROUGE-2 over prior methods without the need of groundtruth label. TAG can obtain up to 90$%$ data by attacking gradients in CoLAdataset. In addition, TAG has a stronger adversary on large models, smalldictionary size, and small input length. We hope the proposed TAG will shedsome light on the privacy leakage problem in Transformer-based NLP models.</div></details><blockquote><p><strong><em>2021-09-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2109.04400v2><strong>Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph</strong></a></p><p><em>Nuttapong Chairatanakul, Noppayut Sriwatanasakdi, Nontawat Charoenphakdee, Xin Liu, Tsuyoshi Murata</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In cross-lingual text classification, it is required that task-specifictraining data in high-resource source languages are available, where the taskis identical to that of a low-resource target language. However, collectingsuch training data can be infeasible because of the labeling cost, taskcharacteristics, and privacy concerns. This paper proposes an alternativesolution that uses only task-independent word embeddings of high-resourcelanguages and bilingual dictionaries. First, we construct a dictionary-basedheterogeneous graph (DHG) from bilingual dictionaries. This opens thepossibility to use graph neural networks for cross-lingual transfer. Theremaining challenge is the heterogeneity of DHG because multiple languages areconsidered. To address this challenge, we propose dictionary-basedheterogeneous graph neural network (DHGNet) that effectively handles theheterogeneity of DHG by two-step aggregations, which are word-level andlanguage-level aggregations. Experimental results demonstrate that our methodoutperforms pretrained models even though it does not access to large corpora.Furthermore, it can perform well even though dictionaries contain manyincorrect translations. Its robustness allows the usage of a wider range ofdictionaries such as an automatically constructed dictionary and crowdsourceddictionary, which are convenient for real-world applications.</div></details><blockquote><p><strong><em>2021-09-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2104.07145v2><strong>FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks</strong></a></p><p><em>Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao Sun, Lifang He, Liangwei Yang, Philip S. Yu, Yu Rong, Peilin Zhao, Junzhou Huang, Murali Annavaram, Salman Avestimehr</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Graph Neural Network (GNN) research is rapidly growing thanks to the capacityof GNNs in learning distributed representations from graph-structured data.However, centralizing a massive amount of real-world graph data for GNNtraining is prohibitive due to privacy concerns, regulation restrictions, andcommercial competitions. Federated learning (FL), a trending distributedlearning paradigm, provides possibilities to solve this challenge whilepreserving data privacy. Despite recent advances in vision and languagedomains, there is no suitable platform for the FL of GNNs. To this end, weintroduce FedGraphNN, an open FL benchmark system that can facilitate researchon federated GNNs. FedGraphNN is built on a unified formulation of graph FL andcontains a wide range of datasets from different domains, popular GNN models,and FL algorithms, with secure and efficient system support. Particularly forthe datasets, we collect, preprocess, and partition 36 datasets from 7 domains,including both publicly available ones and specifically obtained ones such ashERG and Tencent. Our empirical analysis showcases the utility of our benchmarksystem, while exposing significant challenges in graph FL: federated GNNsperform worse in most datasets with a non-IID split than centralized GNNs; theGNN model that attains the best result in the centralized setting may notmaintain its advantage in the FL setting. These results imply that moreresearch efforts are needed to unravel the mystery behind federated GNNs.Moreover, our system performance analysis demonstrates that the FedGraphNNsystem is computationally efficient and secure to large-scale graphs datasets.We maintain the source code at <a href=https://github.com/FedML-AI/FedGraphNN>https://github.com/FedML-AI/FedGraphNN</a>.</div></details><blockquote><p><strong><em>2021-07-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2107.12603v1><strong>Federated Learning Meets Natural Language Processing: A Survey</strong></a></p><p><em>Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated Learning aims to learn machine learning models from multipledecentralized edge devices (e.g. mobiles) or servers without sacrificing localdata privacy. Recent Natural Language Processing techniques rely on deeplearning and large pre-trained language models. However, both big deep neuraland language models are trained with huge amounts of data which often lies onthe server side. Since text data is widely originated from end users, in thiswork, we look into recent NLP models and techniques which use federatedlearning as the learning framework. Our survey discusses major challenges infederated natural language processing, including the algorithm challenges,system challenges as well as the privacy issues. We also provide a criticalreview of the existing Federated NLP evaluation methods and tools. Finally, wehighlight the current research gaps and future directions.</div></details><blockquote><p><strong><em>2021-07-22</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.12305v2><strong>Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective</strong></a></p><p><em>Svetlana Kiritchenko, Isar Nejadgholi, Kathleen C. Fraser</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The pervasiveness of abusive content on the internet can lead to severepsychological and physical harm. Significant effort in Natural LanguageProcessing (NLP) research has been devoted to addressing this problem throughabusive content detection and related sub-areas, such as the detection of hatespeech, toxicity, cyberbullying, etc. Although current technologies achievehigh classification performance in research studies, it has been observed thatthe real-life application of this technology can cause unintended harms, suchas the silencing of under-represented groups. We review a large body of NLPresearch on automatic abuse detection with a new focus on ethical challenges,organized around eight established ethical principles: privacy, accountability,safety and security, transparency and explainability, fairness andnon-discrimination, human control of technology, professional responsibility,and promotion of human values. In many cases, these principles relate not onlyto situational ethical codes, which may be context-dependent, but are in factconnected to universal human rights, such as the right to privacy, freedom fromdiscrimination, and freedom of expression. We highlight the need to examine thebroad social impacts of this technology, and to bring ethical and human rightsconsiderations to every stage of the application life-cycle, from taskformulation and dataset design, to model training and evaluation, toapplication deployment. Guided by these principles, we identify severalopportunities for rights-respecting, socio-technical solutions to detect andconfront online abuse, including <code>nudging', </code>quarantining&rsquo;, value sensitivedesign, counter-narratives, style transfer, and AI-driven public educationapplications.</div></details><blockquote><p><strong><em>2021-06-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.04716v1><strong>Labeled Data Generation with Inexact Supervision</strong></a></p><p><em>Enyan Dai, Kai Shu, Yiwei Sun, Suhang Wang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The recent advanced deep learning techniques have shown the promising resultsin various domains such as computer vision and natural language processing. Thesuccess of deep neural networks in supervised learning heavily relies on alarge amount of labeled data. However, obtaining labeled data with targetlabels is often challenging due to various reasons such as cost of labeling andprivacy issues, which challenges existing deep models. In spite of that, it isrelatively easy to obtain data with \textit{inexact supervision}, i.e., havinglabels/tags related to the target task. For example, social media platforms areoverwhelmed with billions of posts and images with self-customized tags, whichare not the exact labels for target classification tasks but are usuallyrelated to the target labels. It is promising to leverage these tags (inexactsupervision) and their relations with target classes to generate labeled datato facilitate the downstream classification tasks. However, the work on this israther limited. Therefore, we study a novel problem of labeled data generationwith inexact supervision. We propose a novel generative framework named asADDES which can synthesize high-quality labeled data for target classificationtasks by learning from data with inexact supervision and the relations betweeninexact supervision and target classes. Experimental results on image and textdatasets demonstrate the effectiveness of the proposed ADDES for generatingrealistic labeled data from inexact supervision to facilitate the targetclassification task.</div></details><blockquote><p><strong><em>2021-06-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2106.01499v1><strong>Personalizing Pre-trained Models</strong></a></p><p><em>Mina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali Hazariwala, Pattie Maes</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Self-supervised or weakly supervised models trained on large-scale datasetshave shown sample-efficient transfer to diverse datasets in few-shot settings.We consider how upstream pretrained models can be leveraged for downstreamfew-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIPPERsonalized) uses image representations from CLIP, a large-scale imagerepresentation learning model trained using weak natural language supervision.We developed a technique, called Multi-label Weight Imprinting (MWI), formulti-label, continual, and few-shot learning, and CLIPPER uses MWI with imagerepresentations from CLIP. We evaluated CLIPPER on 10 single-label and 5multi-label datasets. Our model shows robust and competitive performance, andwe set new benchmarks for few-shot, multi-label, and continual learning. Ourlightweight technique is also compute-efficient and enables privacy-preservingapplications as the data is not sent to the upstream model for fine-tuning.</div></details><p><a href=http://arxiv.org/abs/2106.01251v1><strong>Multilingual Medical Question Answering and Information Retrieval for Rural Health Intelligence Access</strong></a></p><p><em>Vishal Vinod, Susmit Agrawal, Vipul Gaurav, Pallavi R, Savita Choudhary</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In rural regions of several developing countries, access to qualityhealthcare, medical infrastructure, and professional diagnosis is largelyunavailable. Many of these regions are gradually gaining access to internetinfrastructure, although not with a strong enough connection to allow forsustained communication with a medical practitioner. Several deaths resultingfrom this lack of medical access, absence of patient&rsquo;s previous health records,and the unavailability of information in indigenous languages can be easilyprevented. In this paper, we describe an approach leveraging the phenomenalprogress in Machine Learning and NLP (Natural Language Processing) techniquesto design a model that is low-resource, multilingual, and a preliminaryfirst-point-of-contact medical assistant. Our contribution includes definingthe NLP pipeline required for named-entity-recognition, language-agnosticsentence embedding, natural language translation, information retrieval,question answering, and generative pre-training for final query processing. Weobtain promising results for this pipeline and preliminary results for EHR(Electronic Health Record) analysis with text summarization for medicalpractitioners to peruse for their diagnosis. Through this NLP pipeline, we aimto provide preliminary medical information to the user and do not claim tosupplant diagnosis from qualified medical practitioners. Using the input fromsubject matter experts, we have compiled a large corpus to pre-train andfine-tune our BioBERT based NLP model for the specific tasks. We expect recentadvances in NLP architectures, several of which are efficient andprivacy-preserving models, to further the impact of our solution and improve onindividual task performance.</div></details><blockquote><p><strong><em>2021-02-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2102.00875v1><strong>Scaling Federated Learning for Fine-tuning of Large Language Models</strong></a></p><p><em>Agrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon Ren√© S√ºtfeld, Edvin Listo Zec, Olof Mogren</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) is a promising approach to distributed compute, aswell as distributed data, and provides a level of privacy and compliance tolegal frameworks. This makes FL attractive for both consumer and healthcareapplications. While the area is actively being explored, few studies haveexamined FL in the context of larger language models and there is a lack ofcomprehensive reviews of robustness across tasks, architectures, numbers ofclients, and other relevant factors. In this paper, we explore the fine-tuningof Transformer-based language models in a federated learning setting. Weevaluate three popular BERT-variants of different sizes (BERT, ALBERT, andDistilBERT) on a number of text classification tasks such as sentiment analysisand author identification. We perform an extensive sweep over the number ofclients, ranging up to 32, to evaluate the impact of distributed compute ontask performance in the federated averaging setting. While our findings suggestthat the large sizes of the evaluated models are not generally prohibitive tofederated training, we found that the different models handle federatedaveraging to a varying degree. Most notably, DistilBERT converges significantlyslower with larger numbers of clients, and under some circumstances, evencollapses to chance level performance. Investigating this issue presents aninteresting perspective for future research.</div></details><blockquote><p><strong><em>2021-01-29</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2101.12736v1><strong>N-grams Bayesian Differential Privacy</strong></a></p><p><em>Osman Ramadan, James Withers, Douglas Orr</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Differential privacy has gained popularity in machine learning as a strongprivacy guarantee, in contrast to privacy mitigation techniques such ask-anonymity. However, applying differential privacy to n-gram countssignificantly degrades the utility of derived language models due to theirlarge vocabularies. We propose a differential privacy mechanism that usespublic data as a prior in a Bayesian setup to provide tighter bounds on theprivacy loss metric epsilon, and thus better privacy-utility trade-offs. Itfirst transforms the counts to log space, approximating the distribution of thepublic and private data as Gaussian. The posterior distribution is thenevaluated and softmax is applied to produce a probability distribution. Thistechnique achieves up to 85% reduction in KL divergence compared to previouslyknown mechanisms at epsilon equals 0.1. We compare our mechanism to k-anonymityin a n-gram language modelling task and show that it offers competitiveperformance at large vocabulary sizes, while also providing superior privacyprotection.</div></details><blockquote><p><strong><em>2020-12-11</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.03680v2><strong>Adaptive Self-training for Few-shot Neural Sequence Labeling</strong></a></p><p><em>Yaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu, Jing Gao, Ahmed Hassan Awadallah</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Sequence labeling is an important technique employed for many NaturalLanguage Processing (NLP) tasks, such as Named Entity Recognition (NER), slottagging for dialog systems and semantic parsing. Large-scale pre-trainedlanguage models obtain very good performance on these tasks when fine-tuned onlarge amounts of task-specific labeled data. However, such large-scale labeleddatasets are difficult to obtain for several tasks and domains due to the highcost of human annotation as well as privacy and data access constraints forsensitive user applications. This is exacerbated for sequence labeling tasksrequiring such annotations at token-level. In this work, we develop techniquesto address the label scarcity challenge for neural sequence labeling models.Specifically, we develop self-training and meta-learning techniques fortraining neural sequence taggers with few labels. While self-training serves asan effective mechanism to learn from large amounts of unlabeled data &ndash;meta-learning helps in adaptive sample re-weighting to mitigate errorpropagation from noisy pseudo-labels. Extensive experiments on six benchmarkdatasets including two for massive multilingual NER and four slot taggingdatasets for task-oriented dialog systems demonstrate the effectiveness of ourmethod. With only 10 labeled examples for each class for each task, our methodobtains 10% improvement over state-of-the-art systems demonstrating itseffectiveness for the low-resource setting.</div></details><blockquote><p><strong><em>2020-12-01</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2012.00363v1><strong>Modifying Memories in Transformer Models</strong></a></p><p><em>Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large Transformer models have achieved impressive performance in many naturallanguage tasks. In particular, Transformer based language models have beenshown to have great capabilities in encoding factual knowledge in their vastamount of parameters. While the tasks of improving the memorization andgeneralization of Transformers have been widely studied, it is not well knownhow to make transformers forget specific old facts and memorize new ones. Inthis paper, we propose a new task of \emph{explicitly modifying specificfactual knowledge in Transformer models while ensuring the model performancedoes not degrade on the unmodified facts}. This task is useful in manyscenarios, such as updating stale knowledge, protecting privacy, andeliminating unintended biases stored in the models. We benchmarked severalapproaches that provide natural baseline performances on this task. This leadsto the discovery of key components of a Transformer model that are especiallyeffective for knowledge modifications. The work also provides insights into therole that different training phases (such as pretraining and fine-tuning) playtowards memorization and knowledge modification.</div></details><blockquote><p><strong><em>2020-11-07</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.12254v5><strong>Privacy in Deep Learning: A Survey</strong></a></p><p><em>Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The ever-growing advances of deep learning in many areas including vision,recommendation systems, natural language processing, etc., have led to theadoption of Deep Neural Networks (DNNs) in production systems. The availabilityof large datasets and high computational power are the main contributors tothese advances. The datasets are usually crowdsourced and may contain sensitiveinformation. This poses serious privacy concerns as this data can be misused orleaked through various vulnerabilities. Even if the cloud provider and thecommunication link is trusted, there are still threats of inference attackswhere an attacker could speculate properties of the data used for training, orfind the underlying model architecture and parameters. In this survey, wereview the privacy concerns brought by deep learning, and the mitigatingtechniques introduced to tackle these issues. We also show that there is a gapin the literature regarding test-time inference privacy, and propose possiblefuture research directions.</div></details><blockquote><p><strong><em>2020-10-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2007.15197v2><strong>Communication-Efficient Federated Learning via Optimal Client Sampling</strong></a></p><p><em>Monica Ribero, Haris Vikalo</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Federated learning (FL) ameliorates privacy concerns in settings where acentral server coordinates learning from data distributed across many clients.The clients train locally and communicate the models they learn to the server;aggregation of local models requires frequent communication of large amounts ofinformation between the clients and the central server. We propose a novel,simple and efficient way of updating the central model incommunication-constrained settings based on collecting models from clients withinformative updates and estimating local updates that were not communicated. Inparticular, modeling the progression of model&rsquo;s weights by anOrnstein-Uhlenbeck process allows us to derive an optimal sampling strategy forselecting a subset of clients with significant weight updates. The centralserver collects updated local models from only the selected clients andcombines them with estimated model updates of the clients that were notselected for communication. We test this policy on a synthetic dataset forlogistic regression and two FL benchmarks, namely, a classification task onEMNIST and a realistic language modeling task using the Shakespeare dataset.The results demonstrate that the proposed framework provides significantreduction in communication while maintaining competitive or achieving superiorperformance compared to a baseline. Our method represents a new line ofstrategies for communication-efficient FL that is orthogonal to the existinguser-local methods such as quantization or sparsification, thus complementingrather than aiming to replace those existing methods.</div></details><blockquote><p><strong><em>2020-10-10</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2010.04883v1><strong>Adversarial Self-Supervised Data-Free Distillation for Text Classification</strong></a></p><p><em>Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large pre-trained transformer-based language models have achieved impressiveresults on a wide range of NLP tasks. In the past few years, KnowledgeDistillation(KD) has become a popular paradigm to compress a computationallyexpensive model to a resource-efficient lightweight model. However, most KDalgorithms, especially in NLP, rely on the accessibility of the originaltraining dataset, which may be unavailable due to privacy issues. To tacklethis problem, we propose a novel two-stage data-free distillation method, namedAdversarial self-Supervised Data-Free Distillation (AS-DFD), which is designedfor compressing large-scale transformer-based models (e.g., BERT). To avoidtext generation in discrete space, we introduce a Plug & Play EmbeddingGuessing method to craft pseudo embeddings from the teacher&rsquo;s hidden knowledge.Meanwhile, with a self-supervised module to quantify the student&rsquo;s ability, weadapt the difficulty of pseudo embeddings in an adversarial training manner. Tothe best of our knowledge, our framework is the first data-free distillationframework designed for NLP tasks. We verify the effectiveness of our method onseveral text classification datasets.</div></details><blockquote><p><strong><em>2020-06-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2006.07490v1><strong>Understanding Unintended Memorization in Federated Learning</strong></a></p><p><em>Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Fran√ßoise Beaufays</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Recent works have shown that generative sequence models (e.g., languagemodels) have a tendency to memorize rare or unique sequences in the trainingdata. Since useful models are often trained on sensitive data, to ensure theprivacy of the training data it is critical to identify and mitigate suchunintended memorization. Federated Learning (FL) has emerged as a novelframework for large-scale distributed learning tasks. However, it differs inmany aspects from the well-studied central learning setting where all the datais stored at the central server. In this paper, we initiate a formal study tounderstand the effect of different components of canonical FL on unintendedmemorization in trained models, comparing with the central learning setting.Our results show that several differing components of FL play an important rolein reducing unintended memorization. Specifically, we observe that theclustering of data according to users&mdash;which happens by design in FL&mdash;has asignificant effect in reducing such memorization, and using the method ofFederated Averaging for training causes a further reduction. We also show thattraining with a strong user-level differential privacy guarantee results inmodels that exhibit the least amount of unintended memorization.</div></details><blockquote><p><strong><em>2020-04-23</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2004.11131v1><strong>Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies</strong></a></p><p><em>Mukund Srinath, Shomir Wilson, C. Lee Giles</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Organisations disclose their privacy practices by posting privacy policies ontheir website. Even though users often care about their digital privacy, theyoften don&rsquo;t read privacy policies since they require a significant investmentin time and effort. Although natural language processing can help in privacypolicy understanding, there has been a lack of large scale privacy policycorpora that could be used to analyse, understand, and simplify privacypolicies. Thus, we create PrivaSeer, a corpus of over one million Englishlanguage website privacy policies, which is significantly larger than anypreviously available corpus. We design a corpus creation pipeline whichconsists of crawling the web followed by filtering documents using languagedetection, document classification, duplicate and near-duplication removal, andcontent extraction. We investigate the composition of the corpus and showresults from readability tests, document similarity, keyphrase extraction, andexplored the corpus through topic modeling.</div></details><blockquote><p><strong><em>2020-03-17</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2003.03106v2><strong>Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT</strong></a></p><p><em>Aitor Garc√≠a-Pablos, Naiara Perez, Montse Cuadros</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Massive digital data processing provides a wide range of opportunities andbenefits, but at the cost of endangering personal data privacy. Anonymisationconsists in removing or replacing sensitive information from data, enabling itsexploitation for different purposes while preserving the privacy ofindividuals. Over the years, a lot of automatic anonymisation systems have beenproposed; however, depending on the type of data, the target language or theavailability of training documents, the task remains challenging still. Theemergence of novel deep-learning models during the last two years has broughtlarge improvements to the state of the art in the field of Natural LanguageProcessing. These advancements have been most noticeably led by BERT, a modelproposed by Google in 2018, and the shared language models pre-trained onmillions of documents. In this paper, we use a BERT-based sequence labellingmodel to conduct a series of anonymisation experiments on several clinicaldatasets in Spanish. We also compare BERT to other algorithms. The experimentsshow that a simple BERT-based model with general-domain pre-training obtainshighly competitive results without any domain specific feature engineering.</div></details><blockquote><p><strong><em>2020-02-20</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/2002.08562v1><strong>Federated pretraining and fine tuning of BERT using clinical notes from multiple silos</strong></a></p><p><em>Dianbo Liu, Tim Miller</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large scale contextual representation models, such as BERT, havesignificantly advanced natural language processing (NLP) in recently years.However, in certain area like healthcare, accessing diverse large scale textdata from multiple institutions is extremely challenging due to privacy andregulatory reasons. In this article, we show that it is possible to bothpretrain and fine tune BERT models in a federated manner using clinical textsfrom different silos without moving the data.</div></details><blockquote><p><strong><em>2020-01-02</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1901.09672v2><strong>Personalized Dialogue Generation with Diversified Traits</strong></a></p><p><em>Yinhe Zheng, Guanyi Chen, Minlie Huang, Song Liu, Xuan Zhu</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Endowing a dialogue system with particular personality traits is essential todeliver more human-like conversations. However, due to the challenge ofembodying personality via language expression and the lack of large-scalepersona-labeled dialogue data, this research problem is still far fromwell-studied. In this paper, we investigate the problem of incorporatingexplicit personality traits in dialogue generation to deliver personalizeddialogues. To this end, firstly, we construct PersonalDialog, a large-scale multi-turndialogue dataset containing various traits from a large number of speakers. Thedataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.Each utterance is associated with a speaker who is marked with traits like Age,Gender, Location, Interest Tags, etc. Several anonymization schemes aredesigned to protect the privacy of each speaker. This large-scale dataset willfacilitate not only the study of personalized dialogue generation, but alsoother researches on sociolinguistics or social science. Secondly, to study how personality traits can be captured and addressed indialogue generation, we propose persona-aware dialogue generation models withinthe sequence to sequence learning framework. Explicit personality traits(structured by key-value pairs) are embedded using a trait fusion module.During the decoding process, two techniques, namely persona-aware attention andpersona-aware bias, are devised to capture and address trait-relatedinformation. Experiments demonstrate that our model is able to address propertraits in different contexts. Case studies also show interesting results forthis challenging research problem.</div></details><blockquote><p><strong><em>2019-10-08</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1910.03432v1><strong>Federated Learning of N-gram Language Models</strong></a></p><p><em>Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril Allauzen, Fran√ßoise Beaufays, Michael Riley</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We propose algorithms to train production-quality n-gram language modelsusing federated learning. Federated learning is a distributed computationplatform that can be used to train global models for portable devices such assmart phones. Federated learning is especially relevant for applicationshandling privacy-sensitive data, such as virtual keyboards, because training isperformed without the users&rsquo; data ever leaving their devices. While theprinciples of federated learning are fairly generic, its methodology assumesthat the underlying models are neural networks. However, virtual keyboards aretypically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using thedecentralized FederatedAveraging algorithm and to approximate this federatedmodel server-side with an n-gram model that can be deployed to devices for fastinference. Our technical contributions include ways of handling largevocabularies, algorithms to correct capitalization errors in user data, andefficient finite state transducer algorithms to convert word language models toword-piece language models and vice versa. The n-gram language models trainedwith federated learning are compared to n-grams trained with traditionalserver-based algorithms using A/B tests on tens of millions of users of virtualkeyboard. Results are presented for two languages, American English andBrazilian Portuguese. This work demonstrates that high-quality n-gram languagemodels can be trained directly on client mobile devices without sensitivetraining data ever leaving the devices.</div></details><blockquote><p><strong><em>2019-10-04</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1910.03033v1><strong>Synthesizing Credit Card Transactions</strong></a></p><p><em>Erik R. Altman</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Two elements have been essential to AI&rsquo;s recent boom: (1) deep neural netsand the theory and practice behind them; and (2) cloud computing with itsabundant labeled data and large computing resources. Abundant labeled data is available for key domains such as images, speech,natural language processing, and recommendation engines. However, there aremany other domains where such data is not available, or access to it is highlyrestricted for privacy reasons, as with health and financial data. Even whenabundant data is available, it is often not labeled. Doing such labeling islabor-intensive and non-scalable. As a result, to the best of our knowledge, key domains still lack labeleddata or have at most toy data; or the synthetic data must have access to realdata from which it can mimic new data. This paper outlines work to generaterealistic synthetic data for an important domain: credit card transactions. Some challenges: there are many patterns and correlations in real purchases.There are millions of merchants and innumerable locations. Those merchantsoffer a wide variety of goods. Who shops where and when? How much do peoplepay? What is a realistic fraudulent transaction? We use a mixture of technical approaches and domain knowledge includingmechanics of credit card processing, a broad set of consumer domains:electronics, clothing, hair styling, etc. Connecting everything is a virtualworld. This paper outlines some of our key techniques and provides evidencethat the data generated is indeed realistic. Beyond the scope of this paper: (1) use of our data to develop and trainmodels to predict fraud; (2) coupling models and the synthetic dataset toassess performance in designing accelerators such as GPUs and TPUs.</div></details><blockquote><p><strong><em>2019-06-03</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1812.00984v2><strong>Protection Against Reconstruction and Its Applications in Private Federated Learning</strong></a></p><p><em>Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, Ryan Rogers</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: In large-scale statistical learning, data collection and model fitting aremoving increasingly toward peripheral devices&mdash;phones, watches, fitnesstrackers&mdash;away from centralized data collection. Concomitant with this rise indecentralized data are increasing challenges of maintaining privacy whileallowing enough information to fit accurate, useful statistical models. Thismotivates local notions of privacy&mdash;most significantly, local differentialprivacy, which provides strong protections against sensitive datadisclosures&mdash;where data is obfuscated before a statistician or learner caneven observe it, providing strong protections to individuals&rsquo; data. Yet localprivacy as traditionally employed may prove too stringent for practical use,especially in modern high-dimensional statistical and machine learningproblems. Consequently, we revisit the types of disclosures and adversariesagainst which we provide protections, considering adversaries with limitedprior information and ensuring that with high probability, ensuring they cannotreconstruct an individual&rsquo;s data within useful tolerances. By reconceptualizingthese protections, we allow more useful data release&mdash;large privacy parametersin local differential privacy&mdash;and we design new (minimax) optimal locallydifferentially private mechanisms for statistical learning problems for\emph{all} privacy levels. We thus present practicable approaches tolarge-scale locally private model training that were previously impossible,showing theoretically and empirically that we can fit large-scale imageclassification and language models with little degradation in utility.</div></details><blockquote><p><strong><em>2019-05-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1905.07002v2><strong>Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models</strong></a></p><p><em>Oren Melamud, Chaitanya Shivade</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Large-scale clinical data is invaluable to driving many computationalscientific advances today. However, understandable concerns regarding patientprivacy hinder the open dissemination of such data and give rise to suboptimalsiloed research. De-identification methods attempt to address these concernsbut were shown to be susceptible to adversarial attacks. In this work, we focuson the vast amounts of unstructured natural language data stored in clinicalnotes and propose to automatically generate synthetic clinical notes that aremore amenable to sharing using generative models trained on real de-identifiedrecords. To evaluate the merit of such notes, we measure both their privacypreservation properties as well as utility in training clinical NLP models.Experiments using neural language models yield notes whose utility is close tothat of the real ones in some clinical NLP tasks, yet leave ample room forfuture improvements.</div></details><blockquote><p><strong><em>2019-02-27</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1812.06034v2><strong>Scalable Privacy-Compliant Virality Prediction on Twitter</strong></a></p><p><em>Damian Konrad Kowalczyk, Jan Larsen</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: The digital town hall of Twitter becomes a preferred medium of communicationfor individuals and organizations across the globe. Some of them reachaudiences of millions, while others struggle to get noticed. Given the impactof social media, the question remains more relevant than ever: how to model thedynamics of attention in Twitter. Researchers around the world turn to machinelearning to predict the most influential tweets and authors, navigating thevolume, velocity, and variety of social big data, with many compromises. Inthis paper, we revisit content popularity prediction on Twitter. We argue thatstrict alignment of data acquisition, storage and analysis algorithms isnecessary to avoid the common trade-offs between scalability, accuracy andprivacy compliance. We propose a new framework for the rapid acquisition oflarge-scale datasets, high accuracy supervisory signal and multilanguagesentiment prediction while respecting every privacy request applicable. We thenapply a novel gradient boosting framework to achieve state-of-the-art resultsin virality ranking, already before including tweet&rsquo;s visual or propagationfeatures. Our Gradient Boosted Regression Tree is the first to offerexplainable, strong ranking performance on benchmark datasets. Since theanalysis focused on features available early, the model is immediatelyapplicable to incoming tweets in 18 languages.</div></details><blockquote><p><strong><em>2018-11-12</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1811.04720v1><strong>Automatically Generate Steganographic Text Based on Markov Model and Huffman Coding</strong></a></p><p><em>Zhongliang Yang, Shuyu Jin, Yongfeng Huang, Yujin Zhang, Hui Li</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Steganography, as one of the three basic information security systems, haslong played an important role in safeguarding the privacy and confidentialityof data in cyberspace. The text is the most widely used information carrier inpeople&rsquo;s daily life, using text as a carrier for information hiding has broadresearch prospects. However, due to the high coding degree and less informationredundancy in the text, it has been an extremely challenging problem to hideinformation in it for a long time. In this paper, we propose a steganographymethod which can automatically generate steganographic text based on the Markovchain model and Huffman coding. It can automatically generate fluent textcarrier in terms of secret information which need to be embedded. The proposedmodel can learn from a large number of samples written by people and obtain agood estimate of the statistical language model. We evaluated the proposedmodel from several perspectives. Experimental results show that the performanceof the proposed model is superior to all the previous related methods in termsof information imperceptibility and information hidden capacity.</div></details><blockquote><p><strong><em>2018-02-24</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1710.06963v3><strong>Learning Differentially Private Recurrent Language Models</strong></a></p><p><em>H. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: We demonstrate that it is possible to train large recurrent language modelswith user-level differential privacy guarantees with only a negligible cost inpredictive accuracy. Our work builds on recent advances in the training of deepnetworks on user-partitioned data and privacy accounting for stochasticgradient descent. In particular, we add user-level privacy protection to thefederated averaging algorithm, which makes &ldquo;large step&rdquo; updates from user-leveldata. Our work demonstrates that given a dataset with a sufficiently largenumber of users (a requirement easily met by even small internet-scaledatasets), achieving differential privacy comes at the cost of increasedcomputation, rather than in decreased utility as in most prior work. We findthat our private LSTM language models are quantitatively and qualitativelysimilar to un-noised models when trained on a large dataset.</div></details><blockquote><p><strong><em>2018-02-14</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1607.08723v4><strong>Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner</strong></a></p><p><em>Emmanuel Dupoux</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: During their first years of life, infants learn the language(s) of theirenvironment at an amazing speed despite large cross cultural variations inamount and complexity of the available language input. Understanding thissimple fact still escapes current cognitive and linguistic theories. Recently,spectacular progress in the engineering science, notably, machine learning andwearable technology, offer the promise of revolutionizing the study ofcognitive development. Machine learning offers powerful learning algorithmsthat can achieve human-like performance on many linguistic tasks. Wearablesensors can capture vast amounts of data, which enable the reconstruction ofthe sensory experience of infants in their natural environment. The project of&rsquo;reverse engineering&rsquo; language development, i.e., of building an effectivesystem that mimics infant&rsquo;s achievements appears therefore to be within reach.Here, we analyze the conditions under which such a project can contribute toour scientific understanding of early language development. We argue thatinstead of defining a sub-problem or simplifying the data, computational modelsshould address the full complexity of the learning situation, and take as inputthe raw sensory signals available to infants. This implies that (1) accessiblebut privacy-preserving repositories of home data be setup and widely shared,and (2) models be evaluated at different linguistic levels through a benchmarkof psycholinguist tests that can be passed by machines and humans alike, (3)linguistically and psychologically plausible learning architectures be scaledup to real data using probabilistic/optimization principles from machinelearning. We discuss the feasibility of this approach and present preliminaryresults.</div></details><blockquote><p><strong><em>2014-10-21</em></strong></p></blockquote><p><a href=http://arxiv.org/abs/1410.5815v1><strong>OHMF: A Query Based Optimal Healthcare Medication Framework</strong></a></p><p><em>Santosh Kumar Majhi, Padmalochan Bera</em></p><details><summary>abstract</summary><div class=markdown-inner>abstract: Today cloud computing infrastructure is largely being deployed in healthcareto access various healthcare services easily over the Internet on an as neededbasis. The main advantage of healthcare cloud is that it can be used as a toolfor patients, medical professionals and insurance providers, to query andcoordinate among medical departments, organizations and other healthcarerelated hubs. Although healthcare cloud services can enable better medicationprocess with high responsiveness, but the privacy and other requirements of thepatients need to be ensured in the process. Patients medical data may berequired by the medical professionals, hospitals, diagnostic centers foranalysis and diagnosis. However, data privacy and service quality cannot becompromised. In other words, there may exist various service providerscorresponding to a specific healthcare service. The main challenge is to findthe appropriate providers that comply best with patients requirement. In thispaper, we propose a query based optimal medication framework to support thepatients healthcare service accessibility comprehensively with considerableresponse time. The framework accepts related healthcare queries in naturallanguage through a comprehensive user-interface and then processes the inputquery through a first order logic based evaluation engine and finds allpossible services satisfying the requirements. First order logic is used formodeling of user requirements and queries. The query evaluation engine is builtusing zChaff, a Boolean logic satisfiability solver. The efficacy and usabilityof the framework is evaluated with initial case studies on synthetic and reallife healthcare cloud.</div></details></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>