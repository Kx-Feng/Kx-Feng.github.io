<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Xiaoyan Feng</title><link>http://localhost:1313/docs/arxiv_papers/</link><description>Recent content on Xiaoyan Feng</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://localhost:1313/docs/arxiv_papers/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM with Copyright</title><link>http://localhost:1313/docs/arxiv_papers/llm_copyright/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/llm_copyright/</guid><description>Arxiv Papers: LLM with Copyright#2024-03-15
Lost in Overlap: Exploring Watermark Collision in LLMs
Yiyang Luo, Ke Lin, Chao Gu
abstractabstract: The proliferation of large language models (LLMs) in generating contentraises concerns about text copyright. Watermarking methods, particularlylogit-based approaches, embed imperceptible identifiers into text to addressthese challenges. However, the widespread use of watermarking across diverseLLMs has led to an inevitable issue known as watermark collision during commontasks like question answering and paraphrasing.</description></item><item><title>LLM with Privacy</title><link>http://localhost:1313/docs/arxiv_papers/llm_privacy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/llm_privacy/</guid><description>Arxiv Papers: LLM with Privacy#2024-03-20
MELTing point: Mobile Evaluation of Language Transformers
Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
abstractabstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence&amp;rsquo;&amp;rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs).</description></item><item><title>Machine Unlearning</title><link>http://localhost:1313/docs/arxiv_papers/machine_unlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/machine_unlearning/</guid><description>Arxiv Papers: Machine Unlearning#2024-03-20
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam
abstractabstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF).</description></item><item><title>Vulnerable LLM</title><link>http://localhost:1313/docs/arxiv_papers/vulnerable_llm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/vulnerable_llm/</guid><description>content</description></item></channel></rss>