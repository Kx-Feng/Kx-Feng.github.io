<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hello! on Xiaoyan Feng</title><link>http://localhost:1313/</link><description>Recent content in Hello! on Xiaoyan Feng</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml"/><item><title>Home</title><link>http://localhost:1313/docs/about_me/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/about_me/</guid><description>About Me#</description></item><item><title>LLM with Copyright</title><link>http://localhost:1313/docs/arxiv_papers/llm_copyright/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/llm_copyright/</guid><description>Arxiv Papers: LLM with Copyright#2024-04-17
Sampling-based Pseudo-Likelihood for Membership Inference Attacks
Masahiro Kaneko, Youmi Ma, Yuki Wata, Naoaki Okazaki
abstractabstract: Large Language Models (LLMs) are trained on large-scale web data, which makesit difficult to grasp the contribution of each text. This poses the risk ofleaking inappropriate data such as benchmarks, personal information, andcopyrighted texts in the training data. Membership Inference Attacks (MIA),which determine whether a given text is included in the model&amp;rsquo;s training data,have been attracting attention.</description></item><item><title>LLM with Privacy</title><link>http://localhost:1313/docs/arxiv_papers/llm_privacy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/llm_privacy/</guid><description>Arxiv Papers: LLM with Privacy#2024-03-20
MELTing point: Mobile Evaluation of Language Transformers
Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
abstractabstract: Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with ``sparksof intelligence&amp;rsquo;&amp;rsquo;. However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs).</description></item><item><title>Machine Unlearning</title><link>http://localhost:1313/docs/arxiv_papers/machine_unlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/machine_unlearning/</guid><description>Arxiv Papers: Machine Unlearning#2024-03-20
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam
abstractabstract: Recently, Machine Unlearning (MU) has gained considerable attention for itspotential to improve AI safety by removing the influence of specific data fromtrained Machine Learning (ML) models. This process, known as knowledge removal,addresses concerns about data such as sensitivity, copyright restrictions,obsolescence, or low quality. This capability is also crucial for ensuringcompliance with privacy regulations such as the Right To Be Forgotten (RTBF).</description></item><item><title>Vulnerable LLM</title><link>http://localhost:1313/docs/arxiv_papers/vulnerable_llm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/arxiv_papers/vulnerable_llm/</guid><description>Arxiv Papers: Vulnerable LLM#2024-03-20
Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal
Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel
abstractabstract: The rapid integration of Large Language Models (LLMs) across diverse sectorshas marked a transformative era, showcasing remarkable capabilities in textgeneration and problem-solving tasks. However, this technological advancementis accompanied by significant risks and vulnerabilities. Despite ongoingsecurity enhancements, attackers persistently exploit these weaknesses, castingdoubts on the overall trustworthiness of LLMs.</description></item></channel></rss>